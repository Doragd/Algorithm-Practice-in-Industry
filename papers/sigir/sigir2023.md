# SIGIR2023 Paper List

|论文|作者|组织|摘要|翻译|代码|引用数|
|---|---|---|---|---|---|---|
|[A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering](https://doi.org/10.1145/3539618.3591629)|Alireza Salemi, Juan Altmayer Pizzorno, Hamed Zamani||Knowledge-Intensive Visual Question Answering (KI-VQA) refers to answering a question about an image whose answer does not lie in the image. This paper presents a new pipeline for KI-VQA tasks, consisting of a retriever and a reader. First, we introduce DEDR, a symmetric dual encoding dense retrieval framework in which documents and queries are encoded into a shared embedding space using uni-modal (textual) and multi-modal encoders. We introduce an iterative knowledge distillation approach that bridges the gap between the representation spaces in these two encoders. Extensive evaluation on two well-established KI-VQA datasets, i.e., OK-VQA and FVQA, suggests that DEDR outperforms state-of-the-art baselines by 11.6% and 30.9% on OK-VQA and FVQA, respectively. Utilizing the passages retrieved by DEDR, we further introduce MM-FiD, an encoder-decoder multi-modal fusion-in-decoder model, for generating a textual answer for KI-VQA tasks. MM-FiD encodes the question, the image, and each retrieved passage separately and uses all passages jointly in its decoder. Compared to competitive baselines in the literature, this approach leads to 5.5% and 8.5% improvements in terms of question answering accuracy on OK-VQA and FVQA, respectively.|知识密集型视觉问题回答(KI-VQA)是指回答一个关于图像的问题，而这个问题的答案并不在图像中。本文提出了一种新的 KI-VQA 任务流水线，它由一个检索器和一个读取器组成。首先，我们介绍了 DEDR，一个对称的双重编码密集检索框架，其中文档和查询被编码到一个共享的嵌入空间使用单模态(文本)和多模态编码器。我们介绍了一种迭代的知识提取方法，它弥补了这两个编码器中表示空间之间的差距。对两个行之有效的 KI-VQA 数据集(即 OK-VQA 和 FVQA)的广泛评估表明，在 OK-VQA 和 FVQA 上，DEDR 的表现分别比最先进的基线水平高出11.6% 和30.9% 。利用 DEDR 检索到的段落，我们进一步介绍了 MM-FiD，一种编码器-解码器多模态融合-解码器模型，用于生成 KI-VQA 任务的文本答案。MM-FiD 分别对问题、图像和每个检索到的段落进行编码，并在其解码器中联合使用所有段落。与文献中的竞争性基线相比，这种方法在 OK-VQA 和 FVQA 上的问答准确率分别提高了5.5% 和8.5% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Symmetric+Dual+Encoding+Dense+Retrieval+Framework+for+Knowledge-Intensive+Visual+Question+Answering)|2|
|[Frequency Enhanced Hybrid Attention Network for Sequential Recommendation](https://doi.org/10.1145/3539618.3591689)|Xinyu Du, Huanhuan Yuan, Pengpeng Zhao, Jianfeng Qu, Fuzhen Zhuang, Guanfeng Liu, Yanchi Liu, Victor S. Sheng||The self-attention mechanism, which equips with a strong capability of modeling long-range dependencies, is one of the extensively used techniques in the sequential recommendation field. However, many recent studies represent that current self-attention based models are low-pass filters and are inadequate to capture high-frequency information. Furthermore, since the items in the user behaviors are intertwined with each other, these models are incomplete to distinguish the inherent periodicity obscured in the time domain. In this work, we shift the perspective to the frequency domain, and propose a novel Frequency Enhanced Hybrid Attention Network for Sequential Recommendation, namely FEARec. In this model, we firstly improve the original time domain self-attention in the frequency domain with a ramp structure to make both low-frequency and high-frequency information could be explicitly learned in our approach. Moreover, we additionally design a similar attention mechanism via auto-correlation in the frequency domain to capture the periodic characteristics and fuse the time and frequency level attention in a union model. Finally, both contrastive learning and frequency regularization are utilized to ensure that multiple views are aligned in both the time domain and frequency domain. Extensive experiments conducted on four widely used benchmark datasets demonstrate that the proposed model performs significantly better than the state-of-the-art approaches.|自注意机制具有很强的远程依赖建模能力，是顺序推荐领域中广泛应用的技术之一。然而，许多最近的研究表明，目前基于自我注意的模型是低通滤波器，不足以捕获高频信息。此外，由于用户行为中的项目相互交织在一起，这些模型不能完全区分隐藏在时域中的固有周期性。在这项工作中，我们将视角转移到频域，并提出了一种新的频率增强的混合注意网络的顺序推荐，即 FEARec。在这个模型中，我们首先在频率域改进了原有的时域自注意，使得低频和高频信息都可以在我们的方法中显式地学习。此外，我们还设计了一个类似的注意机制，通过在频域中的自相关来捕捉周期特性，并将时间和频率水平的注意融合到一个联合模型中。最后，利用对比学习和频率正则化技术保证了多视图在时域和频域的对齐。在四个广泛使用的基准数据集上进行的大量实验表明，所提出的模型的性能明显优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Frequency+Enhanced+Hybrid+Attention+Network+for+Sequential+Recommendation)|1|
|[Reformulating CTR Prediction: Learning Invariant Feature Interactions for Recommendation](https://doi.org/10.1145/3539618.3591755)|Yang Zhang, Tianhao Shi, Fuli Feng, Wenjie Wang, Dingxian Wang, Xiangnan He, Yongdong Zhang||Click-Through Rate (CTR) prediction plays a core role in recommender systems, serving as the final-stage filter to rank items for a user. The key to addressing the CTR task is learning feature interactions that are useful for prediction, which is typically achieved by fitting historical click data with the Empirical Risk Minimization (ERM) paradigm. Representative methods include Factorization Machines and Deep Interest Network, which have achieved wide success in industrial applications. However, such a manner inevitably learns unstable feature interactions, i.e., the ones that exhibit strong correlations in historical data but generalize poorly for future serving. In this work, we reformulate the CTR task -- instead of pursuing ERM on historical data, we split the historical data chronologically into several periods (a.k.a, environments), aiming to learn feature interactions that are stable across periods. Such feature interactions are supposed to generalize better to predict future behavior data. Nevertheless, a technical challenge is that existing invariant learning solutions like Invariant Risk Minimization are not applicable, since the click data entangles both environment-invariant and environment-specific correlations. To address this dilemma, we propose Disentangled Invariant Learning (DIL) which disentangles feature embeddings to capture the two types of correlations separately. To improve the modeling efficiency, we further design LightDIL which performs the disentanglement at the higher level of the feature field. Extensive experiments demonstrate the effectiveness of DIL in learning stable feature interactions for CTR. We release the code at https://github.com/zyang1580/DIL.|点进率(ctrl)预测在推荐系统中扮演着核心角色，充当为用户排序项目的最终过滤器。解决 CTR 任务的关键是学习对预测有用的特征交互，这通常是通过将历史点击数据与经验风险最小化(ERM)范式相匹配来实现的。代表性的方法包括因子分解机和深度兴趣网络，它们在工业应用中取得了广泛的成功。然而，这种方式不可避免地会学习到不稳定的特征交互，即在历史数据中表现出强相关性的特征交互，但对于未来服务的推广很差。在这项工作中，我们重新规划了 CTR 任务——而不是追求历史数据的 ERM，我们按时间顺序将历史数据分成几个时期(也就是环境) ，目的是学习跨时期稳定的特性交互。这样的特征交互被认为可以更好地推广以预测未来的行为数据。然而，一个技术挑战是，现有的不变学习解决方案，如不变风险最小化是不适用的，因为点击数据纠缠环境不变和环境特定的相关性。为了解决这一难题，我们提出了解除特征嵌入的不变学习(DIL)算法，分别捕获两种类型的相关性。为了提高建模效率，我们进一步设计了 LightDIL，它在特征字段的更高层次上执行分离。大量的实验证明了 DIL 在 CTR 中学习稳定特征交互的有效性。我们在 https://github.com/zyang1580/dil 公布密码。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reformulating+CTR+Prediction:+Learning+Invariant+Feature+Interactions+for+Recommendation)|1|
|[How Well do Offline Metrics Predict Online Performance of Product Ranking Models?](https://doi.org/10.1145/3539618.3591865)|Xiaojie Wang, Ruoyuan Gao, Anoop Jain, Graham Edge, Sachin Ahuja|Amazon.com, Palo Alto, CA, USA; Amazon.com, Seattle, WA, USA|Online evaluation techniques are widely adopted by industrial search engines to determine which ranking models perform better under a certain business metric. However, online evaluation can only evaluate a small number of rankers and people resort to offline evaluation to select rankers that are likely to yield good online performance. To use offline metrics for effective model selection, a major challenge is to understand how well offline metrics predict which ranking models perform better in online experiments. This paper aims to address this challenge in product search ranking. Towards this end, we collect gold data in the form of preferences over ranker pairs under a business metric in e-commerce search engine. For the first time, we use such gold data to evaluate offline metrics in terms of directional agreement with the business metric. Furthermore, we analyze offline metrics in terms of discriminative power through paired sample t-test and rank correlations among offline metrics. Through extensive online and offline experiments, we studied 36 offline metrics and observed that: (1) Offline metrics align well with online metrics: they agree on which one of two ranking models is better up to 97% of times; (2) Offline metrics are highly discriminative on large-scale search ranking data, especially NDCG (Normalized Discounted Cumulative Gain) which has a discriminative power over 99%.|在线评估技术被工业搜索引擎广泛采用，以确定哪些排名模型在一定的业务指标下表现更好。然而，在线评价只能评价少量的排名者，人们通过离线评价来选择有可能产生良好在线表现的排名者。为了使用离线指标进行有效的模型选择，一个主要的挑战是了解离线指标如何很好地预测哪些排名模型在在线实验中表现得更好。本文旨在解决这一挑战的产品搜索排名。为此，我们在电子商务搜索引擎的商业度量下，以优先于排名对的形式收集黄金数据。我们第一次使用这样的黄金数据来评估与业务度量方向一致的离线度量。此外，我们还通过配对样本 t 检验和离线指标之间的等级相关性来分析离线指标的判别能力。通过大量的在线和离线实验，我们研究了36个离线指标，并观察到: (1)离线指标与在线指标很好地一致，他们对两个排名模型中的哪一个更好达到97% 的时间; (2)离线指标对大规模搜索排名数据具有高度歧视性，特别是 NDCG (标准化折扣累积收益) ，其歧视性超过99% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+Well+do+Offline+Metrics+Predict+Online+Performance+of+Product+Ranking+Models?)|1|
|[pybool_ir: A Toolkit for Domain-Specific Search Experiments](https://doi.org/10.1145/3539618.3591819)|Harrisen Scells, Martin Potthast|Leipzig University and ScaDS.AI, Leipzig, Germany; Leipzig University, Leipzig, Germany|Undertaking research in domain-specific scenarios such as systematic review literature search, legal search, and patent search can often have a high barrier of entry due to complicated indexing procedures and complex Boolean query syntax. Indexing and searching document collections like PubMed in off-the-shelf tools such as Elasticsearch and Lucene often yields less accurate (and less effective) results than the PubMed search engine, i.e., retrieval results do not match what would be retrieved if one issued the same query to PubMed. Furthermore, off-the-shelf tools have their own nuanced query languages and do not allow directly using the often large and complicated Boolean queries seen in domain-specific search scenarios. The pybool_ir toolkit aims to address these problems and to lower the barrier to entry for developing new methods for domain-specific search. The toolkit is an open source package available at https://github.com/hscells/pybool_ir.|由于复杂的索引程序和复杂的布尔查询语法，在特定领域进行研究，如系统综述文献检索、法律检索和专利检索，往往会遇到很高的入门门槛。在诸如 Elasticsearch 和 Lucene 这样的现成工具中，对像 PubMed 这样的文档集合进行索引和搜索，通常会得到比 PubMed 搜索引擎更不准确(也更不有效)的结果，也就是说，如果向 PubMed 发出同样的查询，检索结果与检索结果不匹配。此外，现成的工具有自己的微妙查询语言，不允许直接使用特定领域搜索场景中常见的大型和复杂的布尔查询。Pybool _ ir 工具包旨在解决这些问题，降低开发特定领域搜索新方法的门槛。该工具包是一个开源软件包，可在 https://github.com/hscells/pybool_ir 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=pybool_ir:+A+Toolkit+for+Domain-Specific+Search+Experiments)|1|
|[Lexically-Accelerated Dense Retrieval](https://doi.org/10.1145/3539618.3591715)|Hrishikesh Kulkarni, Sean MacAvaney, Nazli Goharian, Ophir Frieder||Retrieval approaches that score documents based on learned dense vectors (i.e., dense retrieval) rather than lexical signals (i.e., conventional retrieval) are increasingly popular. Their ability to identify related documents that do not necessarily contain the same terms as those appearing in the user's query (thereby improving recall) is one of their key advantages. However, to actually achieve these gains, dense retrieval approaches typically require an exhaustive search over the document collection, making them considerably more expensive at query-time than conventional lexical approaches. Several techniques aim to reduce this computational overhead by approximating the results of a full dense retriever. Although these approaches reasonably approximate the top results, they suffer in terms of recall -- one of the key advantages of dense retrieval. We introduce 'LADR' (Lexically-Accelerated Dense Retrieval), a simple-yet-effective approach that improves the efficiency of existing dense retrieval models without compromising on retrieval effectiveness. LADR uses lexical retrieval techniques to seed a dense retrieval exploration that uses a document proximity graph. We explore two variants of LADR: a proactive approach that expands the search space to the neighbors of all seed documents, and an adaptive approach that selectively searches the documents with the highest estimated relevance in an iterative fashion. Through extensive experiments across a variety of dense retrieval models, we find that LADR establishes a new dense retrieval effectiveness-efficiency Pareto frontier among approximate k nearest neighbor techniques. Further, we find that when tuned to take around 8ms per query in retrieval latency on our hardware, LADR consistently achieves both precision and recall that are on par with an exhaustive search on standard benchmarks.|基于学习密集向量(即密集检索)而非词汇信号(即常规检索)对文档进行评分的检索方法越来越流行。它们能够识别出与用户查询中出现的不一定包含相同术语的相关文档(从而提高召回率) ，这是它们的主要优势之一。然而，为了实际获得这些收益，密集检索方法通常需要对文档集进行彻底搜索，这使得它们在查询时比传统的词法方法昂贵得多。一些技术旨在通过近似一个完全密集的检索器的结果来减少这种计算开销。尽管这些方法可以合理地接近最高的结果，但它们在回忆方面受到影响——这是密集检索的关键优势之一。我们介绍了“ LADR”(词汇加速密集检索) ，一个简单而有效的方法，提高了现有的密集检索模型的效率，而不影响检索的有效性。LADR 使用词汇检索技术来引导使用文档接近图的密集检索探索。我们探索了 LADR 的两种变体: 一种积极主动的方法，将搜索空间扩展到所有种子文档的邻居，以及一种自适应的方法，以迭代的方式选择性地搜索具有最高估计相关性的文档。通过对各种密集检索模型的大量实验，我们发现 LADR 在近似 k 最近邻技术中建立了一种新的密集检索有效性——帕累托前沿。此外，我们发现，当硬件上的检索延迟调整到每个查询大约需要8毫秒时，LADR 始终如一地实现了这两个准确率召回率，与标准基准的详尽搜索相当。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Lexically-Accelerated+Dense+Retrieval)|1|
|[Safe Deployment for Counterfactual Learning to Rank with Exposure-Based Risk Minimization](https://doi.org/10.1145/3539618.3591760)|Shashank Gupta, Harrie Oosterhuis, Maarten de Rijke||Counterfactual learning to rank (CLTR) relies on exposure-based inverse propensity scoring (IPS), a LTR-specific adaptation of IPS to correct for position bias. While IPS can provide unbiased and consistent estimates, it often suffers from high variance. Especially when little click data is available, this variance can cause CLTR to learn sub-optimal ranking behavior. Consequently, existing CLTR methods bring significant risks with them, as naively deploying their models can result in very negative user experiences. We introduce a novel risk-aware CLTR method with theoretical guarantees for safe deployment. We apply a novel exposure-based concept of risk regularization to IPS estimation for LTR. Our risk regularization penalizes the mismatch between the ranking behavior of a learned model and a given safe model. Thereby, it ensures that learned ranking models stay close to a trusted model, when there is high uncertainty in IPS estimation, which greatly reduces the risks during deployment. Our experimental results demonstrate the efficacy of our proposed method, which is effective at avoiding initial periods of bad performance when little data is available, while also maintaining high performance at convergence. For the CLTR field, our novel exposure-based risk minimization method enables practitioners to adopt CLTR methods in a safer manner that mitigates many of the risks attached to previous methods.|反事实学习排名(CLTR)依赖于基于暴露的逆倾向评分(IPS) ，IPS 的一种 LTR 特异性适应，以纠正位置偏差。虽然 IPS 可以提供无偏和一致的估计，但它经常受到高方差的影响。特别是当很少的点击数据可用时，这种方差会导致 CLTR 学习次优排序行为。因此，现有的 CLTR 方法带来了巨大的风险，因为天真地部署它们的模型可能会导致非常负面的用户体验。我们介绍了一种新的风险意识的 CLTR 方法与安全部署的理论保证。我们将一种新的基于暴露的风险正则化概念应用于长期寿命周期的 IPS 估计。我们的风险正则化惩罚了学习模型的排序行为和给定的安全模型之间的不匹配。因此，当 IPS 估计存在较高的不确定性时，学习的排序模型能够保持接近可信模型，从而大大降低了部署过程中的风险。实验结果证明了该方法的有效性，该方法在保证收敛性能的同时，能有效地避免初始阶段性能不佳的情况。对于 CLTR 领域，我们新颖的基于暴露的风险最小化方法使从业者能够以更安全的方式采用 CLTR 方法，从而减轻了许多与以前的方法相关的风险。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Safe+Deployment+for+Counterfactual+Learning+to+Rank+with+Exposure-Based+Risk+Minimization)|1|
|[Disentangled Contrastive Collaborative Filtering](https://doi.org/10.1145/3539618.3591665)|Xubin Ren, Lianghao Xia, Jiashu Zhao, Dawei Yin, Chao Huang||Recent studies show that graph neural networks (GNNs) are prevalent to model high-order relationships for collaborative filtering (CF). Towards this research line, graph contrastive learning (GCL) has exhibited powerful performance in addressing the supervision label shortage issue by learning augmented user and item representations. While many of them show their effectiveness, two key questions still remain unexplored: i) Most existing GCL-based CF models are still limited by ignoring the fact that user-item interaction behaviors are often driven by diverse latent intent factors (e.g., shopping for family party, preferred color or brand of products); ii) Their introduced non-adaptive augmentation techniques are vulnerable to noisy information, which raises concerns about the model's robustness and the risk of incorporating misleading self-supervised signals. In light of these limitations, we propose a Disentangled Contrastive Collaborative Filtering framework (DCCF) to realize intent disentanglement with self-supervised augmentation in an adaptive fashion. With the learned disentangled representations with global context, our DCCF is able to not only distill finer-grained latent factors from the entangled self-supervision signals but also alleviate the augmentation-induced noise. Finally, the cross-view contrastive learning task is introduced to enable adaptive augmentation with our parameterized interaction mask generator. Experiments on various public datasets demonstrate the superiority of our method compared to existing solutions. Our model implementation is released at the link https://github.com/HKUDS/DCCF.|最近的研究表明，图形神经网络(GNN)普遍用于模拟协同过滤(CF)的高阶关系。针对这一研究方向，图形对比学习(GCL)通过学习增强用户和项目表示，在解决监督标签短缺问题方面表现出强大的性能。虽然其中许多显示了它们的有效性，但有两个关键问题仍然没有得到探索: i)大多数现有的基于 GCL 的 CF 模型仍然受到限制，因为忽略了用户项目交互行为通常由不同的潜在意图因素驱动(例如，为家庭聚会，首选颜色或产品品牌) ; ii)它们引入的非自适应增强技术易受噪声信息的影响，这引起了对模型的稳健性和纳入误导性自我监督信号的风险的担忧。鉴于这些局限性，我们提出了一个自适应增强的协同过滤对比度分离框架(dCCF)来实现意图分离。利用学习的全局解纠缠表示，我们的 DCCF 不仅能够从纠缠的自我监督信号中提取出更细粒度的潜在因子，而且能够减轻增强引起的噪声。最后，引入横向视图对比学习任务，利用参数化交互掩模生成器实现自适应增强。在各种公共数据集上的实验表明了该方法相对于现有解的优越性。我们的模型实现在链接 https://github.com/hkuds/dccf 发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Disentangled+Contrastive+Collaborative+Filtering)|1|
|[A Unified Generative Retriever for Knowledge-Intensive Language Tasks via Prompt Learning](https://doi.org/10.1145/3539618.3591631)|Jiangui Chen, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yiqun Liu, Yixing Fan, Xueqi Cheng||Knowledge-intensive language tasks (KILTs) benefit from retrieving high-quality relevant contexts from large external knowledge corpora. Learning task-specific retrievers that return relevant contexts at an appropriate level of semantic granularity, such as a document retriever, passage retriever, sentence retriever, and entity retriever, may help to achieve better performance on the end-to-end task. But a task-specific retriever usually has poor generalization ability to new domains and tasks, and it may be costly to deploy a variety of specialised retrievers in practice. We propose a unified generative retriever (UGR) that combines task-specific effectiveness with robust performance over different retrieval tasks in KILTs. To achieve this goal, we make two major contributions: (i) To unify different retrieval tasks into a single generative form, we introduce an n-gram-based identifier for relevant contexts at different levels of granularity in KILTs. And (ii) to address different retrieval tasks with a single model, we employ a prompt learning strategy and investigate three methods to design prompt tokens for each task. In this way, the proposed UGR model can not only share common knowledge across tasks for better generalization, but also perform different retrieval tasks effectively by distinguishing task-specific characteristics. We train UGR on a heterogeneous set of retrieval corpora with well-designed prompts in a supervised and multi-task fashion. Experimental results on the KILT benchmark demonstrate the effectiveness of UGR on in-domain datasets, out-of-domain datasets, and unseen tasks.|知识密集型语言任务(KILT)受益于从大型外部知识库中检索高质量的相关上下文。学习以适当的语义粒度返回相关上下文的特定任务检索器，如文档检索器、文章检索器、句子检索器和实体检索器，可能有助于在端到端任务中获得更好的性能。但是特定于任务的检索器通常对新领域和任务的泛化能力较差，而且在实践中部署各种专门的检索器可能成本较高。我们提出了一个统一的生成检索器(UGR) ，它结合了任务特定的有效性和鲁棒性能对不同的检索任务在 KILT。为了实现这一目标，我们做出了两个主要贡献: (i)为了将不同的检索任务统一到一个单一的生成形式，我们在 KILT 中引入了一个基于 n-gram 的标识符来标识不同粒度级别的相关上下文。为了解决不同的检索任务，我们采用了快速学习策略，并研究了三种方法为每个任务设计快速令牌。这样，该模型不仅可以实现任务之间的共同知识共享以便更好地泛化，而且可以通过区分任务特定的特征来有效地执行不同的检索任务。我们在一个异构的检索语料集上训练 UGR，这些检索语料集以监督和多任务的方式使用精心设计的提示。在 KILT 基准上的实验结果证明了 UGR 在域内数据集、域外数据集和未知任务上的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Unified+Generative+Retriever+for+Knowledge-Intensive+Language+Tasks+via+Prompt+Learning)|1|
|[ADL: Adaptive Distribution Learning Framework for Multi-Scenario CTR Prediction](https://doi.org/10.1145/3539618.3591944)|Jinyun Li, Huiwen Zheng, Yuanlin Liu, Minfang Lu, Lixia Wu, Haoyuan Hu|Cainiao Network, Hangzhou, China|Large-scale commercial platforms usually involve numerous business scenarios for diverse business strategies. To provide click-through rate (CTR) predictions for multiple scenarios simultaneously, existing promising multi-scenario models explicitly construct scenario-specific networks by manually grouping scenarios based on particular business strategies. Nonetheless, this pre-defined data partitioning process heavily relies on prior knowledge, and it may neglect the underlying data distribution of each scenario, hence limiting the model's representation capability. Regarding the above issues, we propose Adaptive Distribution Learning (ADL): an end-to-end optimization distribution framework which is composed of a clustering process and classification process. Specifically, we design a distribution adaptation module with a customized dynamic routing mechanism. Instead of introducing prior knowledge for pre-defined data allocation, this routing algorithm adaptively provides a distribution coefficient for each sample to determine which cluster it belongs to. Each cluster corresponds to a particular distribution so that the model can sufficiently capture the commonalities and distinctions between these distinct clusters. Our results on both public and large-scale industrial datasets show the effectiveness and efficiency of ADL: the model yields impressive prediction accuracy with more than 50% reduction in time cost during the training phase when compared to other methods.|大型商业平台通常涉及多种不同业务策略的多个业务场景。为了同时提供多个场景的点进率预测，现有有前途的多场景模型通过基于特定业务策略的手动分组场景，明确地构建特定场景的网络。尽管如此，这个预定义的数据分区过程严重依赖于先前的知识，并且它可能会忽略每个场景的底层数据分布，从而限制模型的表示能力。针对以上问题，本文提出了自适应分布学习(ADL) : 一种由聚类过程和分类过程组成的端到端优化分布框架。具体来说，我们设计了一个具有自定义动态路由机制的分布式自适应模块。这种路由算法不需要为预定义的数据分配引入先验知识，而是自适应地为每个样本提供一个分布系数，以确定它属于哪个集群。每个集群对应于一个特定的分布，以便模型能够充分捕获这些不同集群之间的共性和区别。我们在公共和大规模工业数据集上的结果显示了 ADL 的有效性和效率: 与其他方法相比，该模型在训练阶段的时间成本降低了50% 以上，预测精度令人印象深刻。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ADL:+Adaptive+Distribution+Learning+Framework+for+Multi-Scenario+CTR+Prediction)|1|
|[FINAL: Factorized Interaction Layer for CTR Prediction](https://doi.org/10.1145/3539618.3591988)|Jieming Zhu, Qinglin Jia, Guohao Cai, Quanyu Dai, Jingjie Li, Zhenhua Dong, Ruiming Tang, Rui Zhang|Huawei Noah's Ark Lab, Shenzhen, China; ruizhang.info, Shenzhen, China|Multi-layer perceptron (MLP) serves as a core component in many deep models for click-through rate (CTR) prediction. However, vanilla MLP networks are inefficient in learning multiplicative feature interactions, making feature interaction learning an essential topic for CTR prediction. Existing feature interaction networks are effective in complementing the learning of MLPs, but they often fall short of the performance of MLPs when applied alone. Thus, their integration with MLP networks is necessary to achieve improved performance. This situation motivates us to explore a better alternative to the MLP backbone that could potentially replace MLPs. Inspired by factorization machines, in this paper, we propose FINAL, a factorized interaction layer that extends the widely-used linear layer and is capable of learning 2nd-order feature interactions. Similar to MLPs, multiple FINAL layers can be stacked into a FINAL block, yielding feature interactions with an exponential degree growth. We unify feature interactions and MLPs into a single FINAL block and empirically show its effectiveness as a replacement for the MLP block. Furthermore, we explore the ensemble of two FINAL blocks as an enhanced two-stream CTR model, setting a new state-of-the-art on open benchmark datasets. FINAL can be easily adopted as a building block and has achieved business metric gains in multiple applications at Huawei. Our source code will be made available at MindSpore/models and FuxiCTR/model_zoo.|多层感知器(MLP)是许多深度点进率(CTR)预测模型的核心部件。然而，普通 MLP 网络在学习乘法特征交互方面效率低下，使得特征交互学习成为 CTR 预测的重要课题。现有的特征交互网络在补充 MLP 学习方面是有效的，但在单独应用时往往不能满足 MLP 的性能要求。因此，它们与 MLP 网络的集成对于提高性能是必要的。这种情况促使我们探索可能取代 MLP 的更好的替代 MLP 骨干。受因子分解机器的启发，本文提出了一种新的因子分解交互层 FINAL，它扩展了广泛使用的线性层，能够学习二阶特征交互。与 MLP 类似，多个 FINAL 层可以堆叠成一个 FINAL 块，产生指数级增长的特征交互。我们将特征交互和 MLP 统一到一个 FINAL 块中，并通过实验证明了它作为 MLP 块的替代品的有效性。此外，我们探讨了两个 FINAL 块的集成作为一个增强的双流 CTR 模型，设置了一个新的国家的最先进的开放基准数据集。FINAL 可以很容易地作为一个组成部分使用，并且在华为的多个应用程序中取得了业务指标收益。我们的源代码将在 MindSpore/model 和 FuxiCTR/model _ zoo 提供。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FINAL:+Factorized+Interaction+Layer+for+CTR+Prediction)|1|
|[The Dark Side of Explanations: Poisoning Recommender Systems with Counterfactual Examples](https://doi.org/10.1145/3539618.3592070)|Ziheng Chen, Fabrizio Silvestri, Jia Wang, Yongfeng Zhang, Gabriele Tolomei|Sapienza University of Rome, ROME, Italy; Rutgers University, New Brunswick, NJ, USA; Stony Brook University, Stony Brook, NY, USA; Xi'an Jiaotong-Liverpool University, Suzhou, China|Deep learning-based recommender systems have become an integral part of several online platforms. However, their black-box nature emphasizes the need for explainable artificial intelligence (XAI) approaches to provide human-understandable reasons why a specific item gets recommended to a given user. One such method is counterfactual explanation (CF). While CFs can be highly beneficial for users and system designers, malicious actors may also exploit these explanations to undermine the system's security. In this work, we propose H-CARS, a novel strategy to poison recommender systems via CFs. Specifically, we first train a logical-reasoning-based surrogate model on training data derived from counterfactual explanations. By reversing the learning process of the recommendation model, we thus develop a proficient greedy algorithm to generate fabricated user profiles and their associated interaction records for the aforementioned surrogate model. Our experiments, which employ a well-known CF generation method and are conducted on two distinct datasets, show that H-CARS yields significant and successful attack performance.|基于深度学习的推荐系统已经成为几个在线平台的组成部分。然而，它们的黑匣子特性强调了对可解释的人工智能(XAI)方法的需要，以便提供人类可以理解的理由，说明为什么某个特定项目会被推荐给给定的用户。其中一种方法是反事实解释(CF)。虽然 CF 对用户和系统设计人员非常有益，但恶意参与者也可能利用这些解释来破坏系统的安全性。在这项工作中，我们提出了 H-CARS，一种新的策略，毒害推荐系统通过 CFs。具体来说，我们首先训练了一个基于逻辑推理的代理模型的训练数据来源于反事实的解释。通过逆推荐模型的学习过程，我们开发了一个熟练的贪婪算法来为上述代理模型生成虚构的用户配置文件及其相关的交互记录。我们的实验采用了著名的 CF 生成方法，并在两个不同的数据集上进行，结果表明 H-CARS 产生了显著的成功的攻击性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Dark+Side+of+Explanations:+Poisoning+Recommender+Systems+with+Counterfactual+Examples)|1|
|[Weakly-Supervised Scientific Document Classification via Retrieval-Augmented Multi-Stage Training](https://doi.org/10.1145/3539618.3592085)|Ran Xu, Yue Yu, Joyce C. Ho, Carl Yang|Emory University, Atlanta, USA; Georgia Institute of Technology, Atlanta, USA|Scientific document classification is a critical task for a wide range of applications, but the cost of obtaining massive amounts of human-labeled data can be prohibitive. To address this challenge, we propose a weakly-supervised approach for scientific document classification using label names only. In scientific domains, label names often include domain-specific concepts that may not appear in the document corpus, making it difficult to match labels and documents precisely. To tackle this issue, we propose WANDER, which leverages dense retrieval to perform matching in the embedding space to capture the semantics of label names. We further design the label name expansion module to enrich the label name representations. Lastly, a self-training step is used to refine the predictions. The experiments on three datasets show that WANDER outperforms the best baseline by 11.9% on average. Our code will be published at https://github.com/ritaranx/wander.|科学文档分类对于广泛的应用来说是一个关键的任务，但是获取大量的人类标记数据的成本可能是高昂的。为了应对这一挑战，我们提出了一种弱监督的方法，用于只使用标签名称的科学文档分类。在科学领域，标签名称往往包括特定领域的概念，这些概念可能不会出现在文档语料库中，因此难以精确匹配标签和文档。为了解决这个问题，我们提出了 WANDER，它利用密集检索在嵌入空间中执行匹配以捕获标签名的语义。进一步设计了标签名扩展模块，丰富了标签名表示。最后，使用一个自我训练步骤来完善预测。在三个数据集上的实验结果表明，WANDER 平均比最佳基准线高出11.9% 。我们的代码会在 https://github.com/ritaranx/wander 公布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Weakly-Supervised+Scientific+Document+Classification+via+Retrieval-Augmented+Multi-Stage+Training)|1|
|[Click-Conversion Multi-Task Model with Position Bias Mitigation for Sponsored Search in eCommerce](https://doi.org/10.1145/3539618.3591963)|Yibo Wang, Yanbing Xue, Bo Liu, Musen Wen, Wenting Zhao, Stephen Guo, Philip S. Yu||Position bias, the phenomenon whereby users tend to focus on higher-ranked items of the search result list regardless of the actual relevance to queries, is prevailing in many ranking systems. Position bias in training data biases the ranking model, leading to increasingly unfair item rankings, click-through-rate (CTR), and conversion rate (CVR) predictions. To jointly mitigate position bias in both item CTR and CVR prediction, we propose two position-bias-free CTR and CVR prediction models: Position-Aware Click-Conversion (PACC) and PACC via Position Embedding (PACC-PE). PACC is built upon probability decomposition and models position information as a probability. PACC-PE utilizes neural networks to model product-specific position information as embedding. Experiments on the E-commerce sponsored product search dataset show that our proposed models have better ranking effectiveness and can greatly alleviate position bias in both CTR and CVR prediction.|排名偏差是指用户倾向于关注搜索结果列表中排名较高的项目，而不考虑与查询的实际相关性，这种现象在许多排名系统中普遍存在。训练数据中的位置偏差使排名模型产生偏差，导致项目排名、点击率(CTR)和转换率(CVR)预测日益不公平。为了共同减轻项目 CTR 和 CVR 预测中的位置偏差，我们提出了两种无位置偏差的 CTR 和 CVR 预测模型: 位置感知点击转换(PACC)和通过位置嵌入(PACC-PE)的 PACC。PACC 是建立在概率分解和模型位置信息作为一个概率。PACC-PE 利用神经网络将特定产品的位置信息作为嵌入信息进行建模。对电子商务赞助商产品搜索数据集的实验结果表明，该模型具有较好的排序效果，可以大大减轻 CTR 和 CVR 预测中的位置偏差。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Click-Conversion+Multi-Task+Model+with+Position+Bias+Mitigation+for+Sponsored+Search+in+eCommerce)|0|
|[Beyond Two-Tower Matching: Learning Sparse Retrievable Cross-Interactions for Recommendation](https://doi.org/10.1145/3539618.3591643)|Liangcai Su, Fan Yan, Jieming Zhu, Xi Xiao, Haoyi Duan, Zhou Zhao, Zhenhua Dong, Ruiming Tang|Huawei Noah's Ark Lab, Shenzhen, China; Zhejiang University, Hangzhou, China; Shenzhen International Graduate School, Tsinghua University, Shenzhen, China|Two-tower models are a prevalent matching framework for recommendation, which have been widely deployed in industrial applications. The success of two-tower matching attributes to its efficiency in retrieval among a large number of items, since the item tower can be precomputed and used for fast Approximate Nearest Neighbor (ANN) search. However, it suffers two main challenges, including limited feature interaction capability and reduced accuracy in online serving. Existing approaches attempt to design novel late interactions instead of dot products, but they still fail to support complex feature interactions or lose retrieval efficiency. To address these challenges, we propose a new matching paradigm named SparCode, which supports not only sophisticated feature interactions but also efficient retrieval. Specifically, SparCode introduces an all-to-all interaction module to model fine-grained query-item interactions. Besides, we design a discrete code-based sparse inverted index jointly trained with the model to achieve effective and efficient model inference. Extensive experiments have been conducted on open benchmark datasets to demonstrate the superiority of our framework. The results show that SparCode significantly improves the accuracy of candidate item matching while retaining the same level of retrieval efficiency with two-tower models.|双塔模型是一种流行的推荐匹配框架，在工业应用中得到了广泛的应用。双塔匹配属性的成功在于它能够有效地检索大量的项目，因为项目塔可以预先计算并用于快速近似最近邻(ANN)搜索。然而，它面临两个主要的挑战，包括有限的特征交互能力和降低在线服务的准确性。现有的方法试图设计新的后期交互而不是点积，但仍然不能支持复杂的特征交互或失去检索效率。为了应对这些挑战，我们提出了一种新的匹配范例 SparCode，它不仅支持复杂的特征交互，而且支持高效的检索。具体来说，SparCode 引入了一个全对全交互模块来对细粒度的查询项交互进行建模。此外，我们设计了一个基于离散编码的稀疏倒排索引与模型联合训练，以实现有效和高效的模型推理。在开放的基准数据集上进行了大量的实验，验证了该框架的优越性。结果表明，SparCode 在保持双塔模型检索效率不变的情况下，显著提高了候选项匹配的准确率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Beyond+Two-Tower+Matching:+Learning+Sparse+Retrievable+Cross-Interactions+for+Recommendation)|0|
|[News Popularity Beyond the Click-Through-Rate for Personalized Recommendations](https://doi.org/10.1145/3539618.3591741)|Ashutosh Nayak, Mayur Garg, Rajasekhara Reddy Duvvuru Muni|Samsung R&D Institute, Bangalore, India|Popularity detection of news articles is critical for making relevant recommendations for users and drive user engagement for maximum business value. Among several well-known metrics such as likes, shares, comments, Click-Through-Rate (CTR) has evolved as a default metric of popularity. However, CTR is highly influenced by the probability of news articles getting an impression, which in turn depends on the recommendation algorithm. Furthermore, it does not consider the age of the news articles, which are highly perishable and also misses out on human contextual behavioral preferences towards news. Here, we use the MIND dataset, open sourced by Microsoft to investigate the existing metrics of popularity and propose six new metrics. Our aim is to create awareness about the different perspectives of measuring popularity while discussing the advantages and disadvantages of the proposed metrics with respect to the human click behavior. We evaluated the predictability of the proposed metrics in comparison to CTR prediction. We further evaluated the utility of the proposed metrics through different test cases. Our results indicate that by using appropriate popularity metrics, we can reduce the initial news corpus (item set) by 50% and still could achieve 99% of the total clicks as compared to unfiltered news corpus based recommender systems. Similarly, our results show that we can reduce the effective number of articles recommended per impression that could improve user experience with the news platforms. The metrics proposed in this paper can be useful in other contexts, especially in recommenders with perishable items e.g. video reels or blogs.|新闻文章的流行度检测对于向用户提供相关建议和推动用户参与以获得最大商业价值至关重要。在一些众所周知的指标，如喜欢，分享，评论，点击率(CTR)已经发展成为一个默认的流行度量。然而，点击率受新闻文章获得印象的概率的影响很大，而这又取决于推荐算法。此外，它没有考虑到新闻文章的年龄，这些文章是高度容易腐烂的，也错过了人类对新闻的语境行为偏好。在这里，我们使用由微软开源的 MIND 数据集来调查现有的流行度量标准，并提出六个新的度量标准。我们的目标是在讨论与人类点击行为相关的度量标准的优缺点时，建立对测量流行度的不同视角的认识。与 CTR 预测相比，我们评估了提出的指标的可预测性。我们通过不同的测试用例进一步评估了提出的度量标准的效用。我们的研究结果表明，通过使用合适的流行度指标，我们可以减少50% 的初始新闻语料库(项目集) ，仍然可以实现99% 的总点击量相比，未过滤的新闻语料库为基础的推荐系统。同样，我们的研究结果表明，我们可以减少有效数量的文章推荐每个印象，可以改善用户体验的新闻平台。本文中提出的度量标准在其他情况下也是有用的，特别是对于那些容易变质的项目(如视频卷轴或博客)的推荐者。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=News+Popularity+Beyond+the+Click-Through-Rate+for+Personalized+Recommendations)|0|
|[Learning Query-aware Embedding Index for Improving E-commerce Dense Retrieval](https://doi.org/10.1145/3539618.3591834)|Mingming Li, Chunyuan Yuan, Binbin Wang, Jingwei Zhuo, Songlin Wang, Lin Liu, Sulong Xu|JD.com, Beijing, China|The embedding index has become an essential part of the dense retrieval (DR) system, which enables a fast search for billion of items in online E-commerce applications. To accelerate the retrieval process in industrial scenarios, most of the previous studies only utilize item embeddings. However, the product quantization process without query embeddings will lead to inconsistency between queries and items. A straightforward solution is to put query embedding into the product quantization process. But we found that the distance of the positive query and item embedding pairs is too large, which means the query and item embeddings learned by the two-tower are not fully aligned. This problem would lead to performance decay when directly putting query embeddings into the product quantization. In this paper, we propose a novel query-aware embedding Index framework, which aligns the query and item embedding space to reduce the distance between positive pairs, thereby mixing the query and item embeddings to learn better cluster centers for product quantization. Specifically, we first propose s symmetric loss to train a better two-tower to achieve space alignment. Subsequently, we propose a mixed quantization strategy to put the query embeddings into the product quantization process for bridging the gap between queries and compressed item embeddings. Extensive experiments show that our framework significantly outperforms previous models on a real-world dataset, which demonstrates the superiority and effectiveness of the framework.|嵌入索引已经成为密集检索(DR)系统的重要组成部分，可以实现在线电子商务应用中对数十亿条目的快速搜索。为了加速工业场景中的检索过程，以往的研究大多只使用项目嵌入。但是，没有嵌入查询的产品量化过程会导致查询和项目之间的不一致。一个直接的解决方案是将查询嵌入到产品量化过程中。但是我们发现正查询和项目嵌入对之间的距离太大，这意味着双塔学习的查询和项目嵌入没有完全对齐。当直接将查询嵌入到产品量化中时，这个问题会导致性能下降。本文提出了一种新的查询感知嵌入索引框架，该框架通过对查询和项目嵌入空间进行对齐来减少正对之间的距离，从而混合查询和项目嵌入来学习更好的产品量化聚类中心。具体来说，我们首先提出了对称损失训练一个更好的双塔实现空间对准。随后，我们提出了一种混合量化策略，将查询嵌入放入产品量化过程中，以缩小查询与压缩项嵌入之间的差距。大量实验表明，该框架在实际数据集上的性能明显优于以往的模型，证明了该框架的优越性和有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Query-aware+Embedding+Index+for+Improving+E-commerce+Dense+Retrieval)|0|
|[Exploiting Simulated User Feedback for Conversational Search: Ranking, Rewriting, and Beyond](https://doi.org/10.1145/3539618.3591683)|Paul Owoicho, Ivan Sekulic, Mohammad Aliannejadi, Jeffrey Dalton, Fabio Crestani||This research aims to explore various methods for assessing user feedback in mixed-initiative conversational search (CS) systems. While CS systems enjoy profuse advancements across multiple aspects, recent research fails to successfully incorporate feedback from the users. One of the main reasons for that is the lack of system-user conversational interaction data. To this end, we propose a user simulator-based framework for multi-turn interactions with a variety of mixed-initiative CS systems. Specifically, we develop a user simulator, dubbed ConvSim, that, once initialized with an information need description, is capable of providing feedback to a system's responses, as well as answering potential clarifying questions. Our experiments on a wide variety of state-of-the-art passage retrieval and neural re-ranking models show that effective utilization of user feedback can lead to 16% retrieval performance increase in terms of nDCG@3. Moreover, we observe consistent improvements as the number of feedback rounds increases (35% relative improvement in terms of nDCG@3 after three rounds). This points to a research gap in the development of specific feedback processing modules and opens a potential for significant advancements in CS. To support further research in the topic, we release over 30,000 transcripts of system-simulator interactions based on well-established CS datasets.|本研究旨在探讨混合主动式会话搜寻系统中评估使用者反馈的各种方法。虽然 CS 系统在多个方面取得了长足的进步，但最近的研究未能成功地整合来自用户的反馈。其中一个主要原因是缺乏系统-用户交互数据。为此，我们提出了一个基于用户模拟器的框架，用于多个混合主动 CS 系统的多回合交互。具体来说，我们开发了一个用户模拟器，称为 ConvSim，一旦用信息需求描述初始化，就能够为系统的响应提供反馈，并回答潜在的澄清问题。我们在各种最先进的文章检索和神经元重排序模型上的实验表明，有效利用用户反馈可以提高16% 的检索性能。此外，我们观察到随着反馈回合数量的增加，一致的改善(三个回合后 nDCG@3的相对改善率为35%)。这指出了在开发特定的反馈处理模块方面的研究差距，并为 CS 的重大进展开辟了潜力。为了支持本课题的进一步研究，我们发布了超过30,000份基于已建立的 CS 数据集的系统-模拟器交互的文本。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploiting+Simulated+User+Feedback+for+Conversational+Search:+Ranking,+Rewriting,+and+Beyond)|0|
|[Modeling Orders of User Behaviors via Differentiable Sorting: A Multi-task Framework to Predicting User Post-click Conversion](https://doi.org/10.1145/3539618.3592023)|Menghan Wang, Jinming Yang, Yuchen Guo, Yuming Shen, Mengying Zhu, Yanlin Wang||User post-click conversion prediction is of high interest to researchers and developers. Recent studies employ multi-task learning to tackle the selection bias and data sparsity problem, two severe challenges in post-click behavior prediction, by incorporating click data. However, prior works mainly focused on pointwise learning and the orders of labels (i.e., click and post-click) are not well explored, which naturally poses a listwise learning problem. Inspired by recent advances on differentiable sorting, in this paper, we propose a novel multi-task framework that leverages orders of user behaviors to predict user post-click conversion in an end-to-end approach. Specifically, we define an aggregation operator to combine predicted outputs of different tasks to a unified score, then we use the computed scores to model the label relations via differentiable sorting. Extensive experiments on public and industrial datasets show the superiority of our proposed model against competitive baselines.|用户点击后的转换预测是研究人员和开发人员非常感兴趣的。最近的研究采用多任务学习来解决选择偏差和数据稀疏问题，这两个严峻的挑战后点击行为预测，通过整合点击数据。然而，以前的作品主要集中在点式学习和标签的顺序(即，点击和后点击)没有很好的探索，这自然造成了一个列表式学习问题。受可微分排序技术最新进展的启发，本文提出了一种新的多任务框架，该框架利用用户行为的顺序来预测端到端的用户点击后转换。具体来说，我们定义了一个聚合算子，将不同任务的预测输出结合成一个统一的得分，然后使用计算出的得分通过可微排序来建模标签关系。在公共和工业数据集上的大量实验表明了我们提出的模型对竞争基线的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modeling+Orders+of+User+Behaviors+via+Differentiable+Sorting:+A+Multi-task+Framework+to+Predicting+User+Post-click+Conversion)|0|
|[Semantic-enhanced Modality-asymmetric Retrieval for Online E-commerce Search](https://doi.org/10.1145/3539618.3591863)|Zhigong Zhou, Ning Ding, Xiaochuan Fan, Yue Shang, Yiming Qiu, Jingwei Zhuo, Zhiwei Ge, Songlin Wang, Lin Liu, Sulong Xu, Han Zhang|JD.com, California, China; JD.com, Beijing, China|Semantic retrieval, which retrieves semantically matched items given a textual query, has been an essential component to enhance system effectiveness in e-commerce search. In this paper, we study the multimodal retrieval problem, where the visual information (e.g, image) of item is leveraged as supplementary of textual information to enrich item representation and further improve retrieval performance. Though learning from cross-modality data has been studied extensively in tasks such as visual question answering or media summarization, multimodal retrieval remains a non-trivial and unsolved problem especially in the asymmetric scenario where the query is unimodal while the item is multimodal. In this paper, we propose a novel model named SMAR, which stands for Semantic-enhanced Modality-Asymmetric Retrieval, to tackle the problem of modality fusion and alignment in this kind of asymmetric scenario. Extensive experimental results on an industrial dataset show that the proposed model outperforms baseline models significantly in retrieval accuracy. We have open sourced our industrial dataset for the sake of reproducibility and future research works.|语义检索是提高电子商务搜索系统效率的重要组成部分，它检索给定文本查询的语义匹配项。本文研究了利用项目的视觉信息(如图像)作为文本信息的补充来丰富项目表示并进一步提高检索性能的多模态检索问题。尽管在视觉问题回答或媒体摘要等任务中对跨模态数据的学习已经进行了广泛的研究，但是多模态检索仍然是一个非常重要的未解决的问题，尤其是在查询为单模态而项目为多模态的非对称情况下。本文提出了一种新的模型 SMAR，即语义增强的情态-非对称检索模型，以解决这种非对称情景下的情态融合和对齐问题。在工业数据集上的大量实验结果表明，该模型在检索精度上明显优于基线模型。为了重现性和未来的研究工作，我们已经开源了我们的工业数据集。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semantic-enhanced+Modality-asymmetric+Retrieval+for+Online+E-commerce+Search)|0|
|[Intent-aware Ranking Ensemble for Personalized Recommendation](https://doi.org/10.1145/3539618.3591702)|Jiayu Li, Peijie Sun, Zhefan Wang, Weizhi Ma, Yangkun Li, Min Zhang, Zhoutian Feng, Daiyue Xue||Ranking ensemble is a critical component in real recommender systems. When a user visits a platform, the system will prepare several item lists, each of which is generally from a single behavior objective recommendation model. As multiple behavior intents, e.g., both clicking and buying some specific item category, are commonly concurrent in a user visit, it is necessary to integrate multiple single-objective ranking lists into one. However, previous work on rank aggregation mainly focused on fusing homogeneous item lists with the same objective while ignoring ensemble of heterogeneous lists ranked with different objectives with various user intents. In this paper, we treat a user's possible behaviors and the potential interacting item categories as the user's intent. And we aim to study how to fuse candidate item lists generated from different objectives aware of user intents. To address such a task, we propose an Intent-aware ranking Ensemble Learning~(IntEL) model to fuse multiple single-objective item lists with various user intents, in which item-level personalized weights are learned. Furthermore, we theoretically prove the effectiveness of IntEL with point-wise, pair-wise, and list-wise loss functions via error-ambiguity decomposition. Experiments on two large-scale real-world datasets also show significant improvements of IntEL on multiple behavior objectives simultaneously compared to previous ranking ensemble models.|等级集成是实际推荐系统中的一个重要组成部分。当用户访问一个平台时，系统将准备几个项目列表，每个项目列表通常来自一个行为客观推荐模型。由于多种行为意图，例如点击和购买某个特定的商品类别，在用户访问中通常是并发的，因此有必要将多个单目标排名列表集成到一个列表中。然而，以往的排名聚合研究主要集中在同一目标下的同类项目列表的融合，而忽略了不同目标下不同用户意图的异类项目列表的集合。在本文中，我们将用户的可能行为和潜在的交互项目类别视为用户的意图。研究了如何根据用户意图对不同目标生成的候选项目表进行融合。为了解决这个问题，我们提出了一个意图感知排名集成学习模型(IntEL) ，该模型将多个单目标项目列表与不同的用户意图相融合，其中项目级别的个性化权重被学习。此外，我们还通过误差模糊度分解从理论上证明了对点损耗、对损耗和列表损耗函数的有效性。在两个大规模真实世界数据集上的实验也表明，与以前的排序集合模型相比，IntEL 在同时处理多个行为目标上也有显著的改善。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Intent-aware+Ranking+Ensemble+for+Personalized+Recommendation)|0|
|[A Model-Agnostic Popularity Debias Training Framework for Click-Through Rate Prediction in Recommender System](https://doi.org/10.1145/3539618.3591939)|Fan Zhang, Qijie Shen|Alibaba Group, Hangzhou, China; Shopee, Shanghai, China|Recommender system (RS) is widely applied in a multitude of scenarios to aid individuals obtaining the information they require efficiently. At the same time, the prevalence of popularity bias in such systems has become a widely acknowledged issue. To address this challenge, we propose a novel method named Model-Agnostic Popularity Debias Training Framework (MDTF). It consists of two basic modules including 1) General Ranking Model (GRM), which is model-agnostic and can be implemented as any ranking models; and 2) Popularity Debias Module (PDM), which estimates the impact of the competitiveness and popularity of candidate items on the CTR, by utilizing the feedback of cold-start users to re-weigh the loss in GRM. MDTF seamlessly integrates these two modules in an end-to-end multi-task learning framework. Extensive experiments on both real-world offline dataset and online A/B test demonstrate its superiority over state-of-the-art methods.|推荐系统(RS)在多种情况下被广泛应用，以帮助个人有效地获得他们所需要的信息。与此同时，这类系统中普遍存在的受欢迎程度偏差已经成为一个广为人知的问题。为了应对这一挑战，我们提出了一种新的方法称为模型不可知流行性偏差训练框架(MDTF)。它包括两个基本模块: 1)模型无关的通用排名模型(GRM) ，可以作为任何排名模型实现; 2)流行度偏差模型(PDM) ，利用冷启动用户的反馈重新权衡排名模型中的损失，估计候选项的竞争力和流行程度对 CTR 的影响。MDTF 在端到端多任务学习框架中无缝地集成了这两个模块。通过对现实世界离线数据集和在线 A/B 测试的大量实验，证明了该方法优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Model-Agnostic+Popularity+Debias+Training+Framework+for+Click-Through+Rate+Prediction+in+Recommender+System)|0|
|[U-NEED: A Fine-grained Dataset for User Needs-Centric E-commerce Conversational Recommendation](https://doi.org/10.1145/3539618.3591878)|Yuanxing Liu, Weinan Zhang, Baohua Dong, Yan Fan, Hang Wang, Fan Feng, Yifan Chen, Ziyu Zhuang, Hengbin Cui, Yongbin Li, Wanxiang Che||Conversational recommender systems (CRSs) aim to understand the information needs and preferences expressed in a dialogue to recommend suitable items to the user. Most of the existing conversational recommendation datasets are synthesized or simulated with crowdsourcing, which has a large gap with real-world scenarios. To bridge the gap, previous work contributes a dataset E-ConvRec, based on pre-sales dialogues between users and customer service staff in E-commerce scenarios. However, E-ConvRec only supplies coarse-grained annotations and general tasks for making recommendations in pre-sales dialogues. Different from that, we use real user needs as a clue to explore the E-commerce conversational recommendation in complex pre-sales dialogues, namely user needs-centric E-commerce conversational recommendation (UNECR). In this paper, we construct a user needs-centric E-commerce conversational recommendation dataset (U-NEED) from real-world E-commerce scenarios. U-NEED consists of 3 types of resources: (i) 7,698 fine-grained annotated pre-sales dialogues in 5 top categories (ii) 333,879 user behaviors and (iii) 332,148 product knowledge tuples. To facilitate the research of UNECR, we propose 5 critical tasks: (i) pre-sales dialogue understanding (ii) user needs elicitation (iii) user needs-based recommendation (iv) pre-sales dialogue generation and (v) pre-sales dialogue evaluation. We establish baseline methods and evaluation metrics for each task. We report experimental results of 5 tasks on U-NEED. We also report results in 3 typical categories. Experimental results indicate that the challenges of UNECR in various categories are different.|会话推荐系统(CRS)旨在理解对话中表达的信息需求和偏好，从而向用户推荐合适的项目。现有的会话推荐数据集大多是通过众包合成或模拟的，与现实情景有很大的差距。为了弥补这一差距，以前的工作提供了一个数据集 E-ConvRec，该数据集基于电子商务场景中用户和客户服务人员之间的售前对话。然而，E-ConvRec 只提供粗粒度的注释和一般性任务，用于在售前对话中提出建议。与此不同的是，我们以用户真实需求为线索，探索复杂的售前对话中的电子商务会话推荐，即以用户需求为中心的电子商务会话推荐。本文从现实电子商务场景出发，构建了一个以用户需求为中心的电子商务会话推荐数据集(U-Need)。U-Need 由3种类型的资源组成: (i)在5个顶级类别中的7,698个细粒度注释的售前对话(ii)333,879个用户行为和(iii)332,148个产品知识元组。为了促进 UNECR 的研究，我们提出了5个关键任务: (i)售前对话理解(ii)用户需求启发(iii)基于用户需求的推荐(iv)售前对话生成和(v)售前对话评估。我们为每个任务建立基线方法和评估指标。我们报告了5个任务的实验结果。我们还报告了3个典型类别的结果。实验结果表明，联合国难民事务高级专员办事处在不同类别中面临的挑战是不同的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=U-NEED:+A+Fine-grained+Dataset+for+User+Needs-Centric+E-commerce+Conversational+Recommendation)|0|
|[Continuous Input Embedding Size Search For Recommender Systems](https://doi.org/10.1145/3539618.3591653)|Yunke Qu, Tong Chen, Xiangyu Zhao, Lizhen Cui, Kai Zheng, Hongzhi Yin||Latent factor models are the most popular backbones for today's recommender systems owing to their prominent performance. Latent factor models represent users and items as real-valued embedding vectors for pairwise similarity computation, and all embeddings are traditionally restricted to a uniform size that is relatively large (e.g., 256-dimensional). With the exponentially expanding user base and item catalog in contemporary e-commerce, this design is admittedly becoming memory-inefficient. To facilitate lightweight recommendation, reinforcement learning (RL) has recently opened up opportunities for identifying varying embedding sizes for different users/items. However, challenged by search efficiency and learning an optimal RL policy, existing RL-based methods are restricted to highly discrete, predefined embedding size choices. This leads to a largely overlooked potential of introducing finer granularity into embedding sizes to obtain better recommendation effectiveness under a given memory budget. In this paper, we propose continuous input embedding size search (CIESS), a novel RL-based method that operates on a continuous search space with arbitrary embedding sizes to choose from. In CIESS, we further present an innovative random walk-based exploration strategy to allow the RL policy to efficiently explore more candidate embedding sizes and converge to a better decision. CIESS is also model-agnostic and hence generalizable to a variety of latent factor RSs, whilst experiments on two real-world datasets have shown state-of-the-art performance of CIESS under different memory budgets when paired with three popular recommendation models.|潜在因素模型是当今推荐系统最受欢迎的骨干，因为它们具有突出的性能。潜在因素模型将用户和项目表示为实值嵌入向量进行两两相似性计算，所有的嵌入传统上都限制在一个相对较大的统一大小(例如，256维)。随着当代电子商务中用户数量和商品目录的指数级增长，这种设计无可否认地变得内存效率低下。为了方便轻量级推荐，强化学习(RL)最近为不同用户/项目提供了识别不同嵌入大小的机会。然而，由于受到搜索效率和学习最优 RL 策略的挑战，现有的基于 RL 的方法仅限于高度离散的、预定义的嵌入大小选择。这导致在给定的内存预算下，为了获得更好的推荐效果，在嵌入大小中引入更细粒度的可能性被很大程度上忽略了。本文提出了连续输入嵌入大小搜索(CIESS) ，这是一种新的基于 RL 的方法，可以在任意嵌入大小的连续搜索空间上进行选择。在 CIESS，我们进一步提出了一个创新的基于随机漫步的探索策略，使 RL 策略能够有效地探索更多的候选嵌入规模，并收敛到一个更好的决策。CIESS 也是模型无关的，因此可以推广到各种潜在因素 RS，而在两个真实世界数据集上的实验显示，当与三种流行的推荐模型配对时，CIESS 在不同内存预算下的最先进性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Continuous+Input+Embedding+Size+Search+For+Recommender+Systems)|0|
|[A Geometric Framework for Query Performance Prediction in Conversational Search](https://doi.org/10.1145/3539618.3591625)|Guglielmo Faggioli, Nicola Ferro, Cristina Ioana Muntean, Raffaele Perego, Nicola Tonellotto|University of Padova, Padova, Italy; University of Pisa, Pisa, Italy; ISTI-CNR, Pisa, Italy|Thanks to recent advances in IR and NLP, the way users interact with search engines is evolving rapidly, with multi-turn conversations replacing traditional one-shot textual queries. Given its interactive nature, Conversational Search (CS) is one of the scenarios that can benefit the most from Query Performance Prediction (QPP) techniques. QPP for the CS domain is a relatively new field and lacks proper framing. In this study, we address this gap by proposing a framework for the application of QPP in the CS domain and use it to evaluate the performance of predictors. We characterize what it means to predict the performance in the CS scenario, where information needs are not independent queries but a series of closely related utterances. We identify three main ways to use QPP models in the CS domain: as a diagnostic tool, as a way to adjust the system's behaviour during a conversation, or as a way to predict the system's performance on the next utterance. Due to the lack of established evaluation procedures for QPP in the CS domain, we propose a protocol to evaluate QPPs for each of the use cases. Additionally, we introduce a set of spatial-based QPP models designed to work the best in the conversational search domain, where dense neural retrieval models are the most common approaches and query cutoffs are typically small. We show how the proposed QPP approaches improve significantly the predictive performance over the state-of-the-art in different scenarios and collections.|由于最近在 IR 和 NLP 方面的进步，用户与搜索引擎交互的方式正在迅速发展，多回合会话取代了传统的一次性文本查询。鉴于其交互性，会话搜索(Conversational Search，CS)是可以从查询性能预测(Query Performance Prevention，QPP)技术中获益最多的场景之一。CS 领域的 QPP 是一个相对较新的领域，缺乏合适的框架。在这项研究中，我们通过提出一个在 CS 领域应用 QPP 的框架来弥补这一差距，并用它来评估预测因子的性能。我们描述了在 CS 场景中预测性能意味着什么，在 CS 场景中，信息需求不是独立的查询，而是一系列密切相关的语句。我们确定了在 CS 领域使用 QPP 模型的三种主要方法: 作为一种诊断工具，作为一种在谈话中调整系统行为的方法，或者作为一种预测系统在下一次谈话中的表现的方法。由于在 CS 领域缺乏已建立的 QPP 评估程序，我们提出了一个协议来评估每个用例的 QPP。此外，我们还介绍了一套基于空间的 QPP 模型，该模型旨在在会话搜索领域中发挥最佳作用，其中密集神经检索模型是最常用的方法，而查询截断通常很小。我们展示了所提出的 QPP 方法如何在不同的场景和集合中显著提高最先进的预测性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Geometric+Framework+for+Query+Performance+Prediction+in+Conversational+Search)|0|
|[Neighborhood-based Hard Negative Mining for Sequential Recommendation](https://doi.org/10.1145/3539618.3591995)|Lu Fan, Jiashu Pu, Rongsheng Zhang, XiaoMing Wu||Negative sampling plays a crucial role in training successful sequential recommendation models. Instead of merely employing random negative sample selection, numerous strategies have been proposed to mine informative negative samples to enhance training and performance. However, few of these approaches utilize structural information. In this work, we observe that as training progresses, the distributions of node-pair similarities in different groups with varying degrees of neighborhood overlap change significantly, suggesting that item pairs in distinct groups may possess different negative relationships. Motivated by this observation, we propose a Graph-based Negative sampling approach based on Neighborhood Overlap (GNNO) to exploit structural information hidden in user behaviors for negative mining. GNNO first constructs a global weighted item transition graph using training sequences. Subsequently, it mines hard negative samples based on the degree of overlap with the target item on the graph. Furthermore, GNNO employs curriculum learning to control the hardness of negative samples, progressing from easy to difficult. Extensive experiments on three Amazon benchmarks demonstrate GNNO's effectiveness in consistently enhancing the performance of various state-of-the-art models and surpassing existing negative sampling strategies. The code will be released at \url{https://github.com/floatSDSDS/GNNO}.|负抽样对顺序推荐模型的建立起着至关重要的作用。为了提高训练效果，人们提出了许多挖掘负样本信息的策略，而不仅仅是采用随机的负样本选择。然而，这些方法很少利用结构信息。本研究发现，随着训练的进行，邻域重叠程度不同的不同群体中节点对相似性的分布发生了显著的变化，提示不同群体中的项目对可能具有不同的负相关关系。在此基础上，本文提出了一种基于邻域重叠(GNNO)的负向采样方法，利用用户行为中隐藏的结构信息进行负向挖掘。GNNO 首先使用训练序列构造一个全局加权项目转移图。然后，根据图上目标项的重叠程度挖掘硬负样本。此外，GNNO 使用课程学习来控制负面样本的硬度，从容易进展到难。在三个 Amazon 基准上的大量实验证明了 GNNO 在持续提高各种最先进模型的性能和超越现有的负采样策略方面的有效性。代码将在 url { https://github.com/floatsdsds/gnno }发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neighborhood-based+Hard+Negative+Mining+for+Sequential+Recommendation)|0|
|[Query Performance Prediction: From Ad-hoc to Conversational Search](https://doi.org/10.1145/3539618.3591919)|Chuan Meng, Negar Arabzadeh, Mohammad Aliannejadi, Maarten de Rijke||Query performance prediction (QPP) is a core task in information retrieval. The QPP task is to predict the retrieval quality of a search system for a query without relevance judgments. Research has shown the effectiveness and usefulness of QPP for ad-hoc search. Recent years have witnessed considerable progress in conversational search (CS). Effective QPP could help a CS system to decide an appropriate action to be taken at the next turn. Despite its potential, QPP for CS has been little studied. We address this research gap by reproducing and studying the effectiveness of existing QPP methods in the context of CS. While the task of passage retrieval remains the same in the two settings, a user query in CS depends on the conversational history, introducing novel QPP challenges. In particular, we seek to explore to what extent findings from QPP methods for ad-hoc search generalize to three CS settings: (i) estimating the retrieval quality of different query rewriting-based retrieval methods, (ii) estimating the retrieval quality of a conversational dense retrieval method, and (iii) estimating the retrieval quality for top ranks vs. deeper-ranked lists. Our findings can be summarized as follows: (i) supervised QPP methods distinctly outperform unsupervised counterparts only when a large-scale training set is available; (ii) point-wise supervised QPP methods outperform their list-wise counterparts in most cases; and (iii) retrieval score-based unsupervised QPP methods show high effectiveness in assessing the conversational dense retrieval method, ConvDR.|查询性能预测是信息检索的核心任务。QPP 任务是在没有相关性判断的情况下预测查询搜索系统的检索质量。研究表明 QPP 在自组织搜索中的有效性和实用性。近年来，会话搜索取得了长足的进步。有效的质量保证计划可以帮助 CS 系统决定下一轮要采取的适当行动。尽管 QPP 具有很大的潜力，但是对它的研究还很少。我们通过再现和研究现有的 QPP 方法在 CS 背景下的有效性来弥补这一研究差距。虽然在这两种情况下，文章检索的任务是相同的，但用户在 CS 中的查询依赖于会话历史，引入了新的 QPP 挑战。特别是，我们试图探索用于特别搜索的 QPP 方法的结果在多大程度上概括为三种 CS 设置: (i)估计不同基于查询重写的检索方法的检索质量，(ii)估计会话密集检索方法的检索质量，以及(iii)估计顶级与更深级列表的检索质量。我们的研究结果可以总结如下: (i)监督 QPP 方法只有在大规模训练集可用时才明显优于无监督的对应方法; (ii)点式监督 QPP 方法在大多数情况下优于其列表式对应方法; 和(iii)基于检索评分的无监督 QPP 方法在评估会话密集检索方法，ConvDR 方面显示出高效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Query+Performance+Prediction:+From+Ad-hoc+to+Conversational+Search)|0|
|[Integrity and Junkiness Failure Handling for Embedding-based Retrieval: A Case Study in Social Network Search](https://doi.org/10.1145/3539618.3591831)|Wenping Wang, Yunxi Guo, Chiyao Shen, Shuai Ding, Guangdeng Liao, Hao Fu, Pramodh Karanth Prabhakar||Embedding based retrieval has seen its usage in a variety of search applications like e-commerce, social networking search etc. While the approach has demonstrated its efficacy in tasks like semantic matching and contextual search, it is plagued by the problem of uncontrollable relevance. In this paper, we conduct an analysis of embedding-based retrieval launched in early 2021 on our social network search engine, and define two main categories of failures introduced by it, integrity and junkiness. The former refers to issues such as hate speech and offensive content that can severely harm user experience, while the latter includes irrelevant results like fuzzy text matching or language mismatches. Efficient methods during model inference are further proposed to resolve the issue, including indexing treatments and targeted user cohort treatments, etc. Though being simple, we show the methods have good offline NDCG and online A/B tests metrics gain in practice. We analyze the reasons for the improvements, pointing out that our methods are only preliminary attempts to this important but challenging problem. We put forward potential future directions to explore.|嵌入式检索在电子商务、社交网络搜索等多种搜索应用中得到了广泛的应用。虽然该方法在语义匹配和上下文搜索等任务中表现出了很好的效果，但是相关性不可控的问题仍然困扰着该方法。本文对2021年初在我国社交网络搜索引擎上推出的嵌入式检索进行了分析，并定义了嵌入式检索引入的两类主要失败: 完整性和垃圾性。前者指的是可能严重损害用户体验的仇恨言论和冒犯性内容等问题，而后者包括模糊文本匹配或语言不匹配等不相关的结果。进一步提出了有效的模型推理方法，包括索引处理和目标用户队列处理等。实践表明，该方法简单易行，具有良好的离线 NDCG 和在线 A/B 测试指标收益。我们分析了改进的原因，指出我们的方法只是对这个重要但具有挑战性的问题的初步尝试。我们提出了潜在的未来发展方向。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Integrity+and+Junkiness+Failure+Handling+for+Embedding-based+Retrieval:+A+Case+Study+in+Social+Network+Search)|0|
|[Embedding Based Retrieval in Friend Recommendation](https://doi.org/10.1145/3539618.3591848)|Jiahui Shi, Vivek Chaurasiya, Yozen Liu, Shubham Vij, Yan Wu, Satya Kanduri, Neil Shah, Peicheng Yu, Nik Srivastava, Lei Shi, Ganesh Venkataraman, Jun Yu|Snap Inc., Santa Monica, USA|Friend recommendation systems in online social and professional networks such as Snapchat helps users find friends and build connections, leading to better user engagement and retention. Traditional friend recommendation systems take advantage of the principle of locality and use graph traversal to retrieve friend candidates, e.g. Friends-of-Friends (FoF). While this approach has been adopted and shown efficacy in companies with large online networks such as Linkedin and Facebook, it suffers several challenges: (i) discrete graph traversal offers limited reach in cold-start settings, (ii) it is expensive and infeasible in realtime settings beyond 1 or 2 hop requests owing to latency constraints, and (iii) it cannot well-capture the complexity of graph topology or connection strengths, forcing one to resort to other mechanisms to rank and find top-K candidates. In this paper, we proposed a new Embedding Based Retrieval (EBR) system for retrieving friend candidates, which complements the traditional FoF retrieval by retrieving candidates beyond 2-hop, and providing a natural way to rank FoF candidates. Through online A/B test, we observe statistically significant improvements in the number of friendships made with EBR as an additional retrieval source in both low- and high-density network markets. Our contributions in this work include deploying a novel retrieval system to a large-scale friend recommendation system at Snapchat, generating embeddings for billions of users using Graph Neural Networks, and building EBR infrastructure in production to support Snapchat scale.|Snapchat 等在线社交和专业网络中的朋友推荐系统可以帮助用户找到朋友并建立联系，从而提高用户的参与度和保留率。传统的朋友推荐系统利用这个定域性原理，并使用图形遍历来检索朋友候选人，例如朋友的朋友(foF)。虽然这种方法已经被大型在线网络公司(如 Linkedin 和 Facebook)采用并显示出有效性，但它遇到了几个挑战: (i)离散图遍历在冷启动设置中提供有限的覆盖范围，(ii)由于延迟限制，在超过1或2跳请求的实时设置中成本高昂且不可行，以及(iii)它不能很好地捕捉图拓扑结构或连接强度的复杂性，迫使人们求助于其他机制来排名和找到前 K 的候选人。本文提出了一种新的基于嵌入式检索(EBR)的朋友候选人检索系统，该系统通过检索超过2跳的候选人来补充传统的 FoF 检索，并为 FoF 候选人排序提供了一种自然的方法。通过在线 A/B 测试，我们观察到在低密度和高密度网络市场中，使用 EBR 作为额外检索源的友谊数量有统计学意义上的显著改善。我们在这项工作中的贡献包括在 Snapchat 的大规模朋友推荐系统中部署一个新颖的检索系统，使用图形神经网络为数十亿用户生成嵌入，以及在生产中建立支持 Snapchat 规模的 EBR 基础设施。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Embedding+Based+Retrieval+in+Friend+Recommendation)|0|
|[Exploring User and Item Representation, Justification Generation, and Data Augmentation for Conversational Recommender Systems](https://doi.org/10.1145/3539618.3591795)|Sergey Volokhin|Emory University, Atlanta, USA|Conversational Recommender Systems (CRS) aim to provide personalized and contextualized recommendations through natural language conversations with users. The objective of my proposed dissertation is to capitalize on the recent developments in conversational interfaces to advance the field of Recommender Systems in several directions. I aim to address several problems in recommender systems: user and item representation, justification generation, and data sparsity. A critical challenge in CRS is learning effective representations of users and items that capture their preferences and characteristics. First, we focus on user representation, where we use a separate corpus of reviews to learn user representation. We attempt to map conversational users into the space of reviewers using semantic similarity between the conversation and the texts of reviews. Second, we improve item representation by incorporating textual features such as item descriptions into the user-item interaction graph, which captures a great deal of semantic and behavioral information unavailable from the purely topological structure of the interaction graph. Justifications for recommendations enhance the explainability and transparency of CRS; however, existing approaches, such as rule-based and template-based methods, have limitations. In this work, we propose an extractive method using a corpus of reviews to identify relevant information for generating concise and coherent justifications. We address the challenge of data scarcity for CRS by generating synthetic conversations using SOTA generative pre trained transformers (GPT). These synthetic conversations are used to augment the data used for training the CRS. In addition, we also evaluate if the GPTs exhibit emerging abilities of CRS (or a non-conversational RecSys) due to the large amount of data they are trained on, which potentially includes the reviews and opinions of users.|会话推荐系统(CRS)旨在通过与用户的自然语言对话，提供个性化和上下文化的推荐。我提出的论文的目标是利用会话接口的最新发展，在几个方向上推进推荐系统的领域。我的目标是解决推荐系统中的几个问题: 用户和项目表示、合理化生成和数据稀疏。CRS 的一个关键挑战是学习用户和项目的有效表示，以捕捉他们的偏好和特征。首先，我们关注用户表示，在这里我们使用一个单独的评论语料库来学习用户表示。我们试图利用会话和评论文本之间的语义相似性将会话用户映射到评论者的空间中。其次，通过在用户-项目交互图中引入项目描述等文本特征来改进项目表示，从而获取了大量从交互图纯拓扑结构中无法获得的语义和行为信息。提出建议的理由增强了 CRS 的可解释性和透明度; 然而，基于规则和基于模板的方法等现有方法存在局限性。在这项工作中，我们提出了一种提取方法，使用一个语料库的审查，以确定相关信息，产生简明和连贯的理由。我们通过使用 SOTA 生成预训练变压器(GPT)生成合成会话来解决 CRS 数据稀缺的挑战。这些合成会话被用来增强用于训练 CRS 的数据。此外，我们还评估 GPTs 是否表现出 CRS (或非对话性 RecSys)的新兴能力，因为它们接受了大量的数据培训，其中可能包括用户的评论和意见。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+User+and+Item+Representation,+Justification+Generation,+and+Data+Augmentation+for+Conversational+Recommender+Systems)|0|
|[MELT: Mutual Enhancement of Long-Tailed User and Item for Sequential Recommendation](https://doi.org/10.1145/3539618.3591725)|Kibum Kim, Dongmin Hyun, Sukwon Yun, Chanyoung Park||The long-tailed problem is a long-standing challenge in Sequential Recommender Systems (SRS) in which the problem exists in terms of both users and items. While many existing studies address the long-tailed problem in SRS, they only focus on either the user or item perspective. However, we discover that the long-tailed user and item problems exist at the same time, and considering only either one of them leads to sub-optimal performance of the other one. In this paper, we propose a novel framework for SRS, called Mutual Enhancement of Long-Tailed user and item (MELT), that jointly alleviates the long-tailed problem in the perspectives of both users and items. MELT consists of bilateral branches each of which is responsible for long-tailed users and items, respectively, and the branches are trained to mutually enhance each other, which is trained effectively by a curriculum learning-based training. MELT is model-agnostic in that it can be seamlessly integrated with existing SRS models. Extensive experiments on eight datasets demonstrate the benefit of alleviating the long-tailed problems in terms of both users and items even without sacrificing the performance of head users and items, which has not been achieved by existing methods. To the best of our knowledge, MELT is the first work that jointly alleviates the long-tailed user and item problems in SRS.|长尾问题是顺序推荐系统(SRS)中长期存在的一个挑战，其中问题既存在于用户中，也存在于项目中。虽然许多现有的研究解决了 SRS 中的长尾问题，但他们只关注用户或项目的角度。然而，我们发现长尾用户问题和条目问题同时存在，只考虑其中一个会导致另一个的性能次优。本文提出了一种新的 SRS 框架，称为长尾用户和项目的相互增强(MELT) ，它从用户和项目的角度共同解决了长尾问题。MELT 包括两个分支机构，每个分支机构分别负责长尾用户和项目，各分支机构接受培训以相互促进，通过基于课程学习的培训进行有效培训。MELT 是模型无关的，因为它可以与现有的 SRS 模型无缝集成。在8个数据集上的大量实验表明，在不牺牲首用户和项目性能的前提下，可以缓解用户和项目的长尾问题，这是现有方法所不能达到的。据我们所知，MELT 是第一个共同解决 SRS 中长尾用户和项目问题的工作。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MELT:+Mutual+Enhancement+of+Long-Tailed+User+and+Item+for+Sequential+Recommendation)|0|
|[DMBIN: A Dual Multi-behavior Interest Network for Click-Through Rate Prediction via Contrastive Learning](https://doi.org/10.1145/3539618.3591669)|Tianqi He, Kaiyuan Li, Shan Chen, Haitao Wang, Qiang Liu, Xingxing Wang, Dong Wang|Meituan.com, Beijing, China|Click-through rate (CTR) prediction plays a critical role in various online applications, aiming to estimate the user's click probability. User interest modeling from various interactive behaviors(e.g., click, add-to-cart, order) is becoming a mainstream approach to CTR prediction. We argue that the various user behaviors contain two important intrinsic characteristics: 1) The discrepancy in various behaviors reveals different aspects of user's behavior-specific interests. For example, one may click out of need but pay more attention to the rating when purchasing. 2) The consistency of various behaviors contains user's behavior-invariant interest. For example, the user prefers interacted items rather than other items. Therefore, it is necessary to disentangle the discrepancy and consistency signals from the massive behavior information. Unfortunately, previous methods have yet to study this phenomenon well, which limits the recommendation performance. To tackle this challenge, we propose a novel Dual Multi-Behavior Interest Network (DMBIN for short) to disentangle behavior-specific and behavioral-invariant interests from various behaviors for a better recommendation. Specifically, DMBIN formalizethe discrepancy and consistency characteristics among various behaviors. Extensive experiments and empirical analysis on two real-world datasets demonstrate that DMBIN significantly outperforms the state-of-the-art methods. Moreover, DMBIN is also deployed in the online sponsored search advertising system in Meituan and achieves 2.11% and 2.76% improvement on CTR and CPM, respectively.s the dismantlement task as two contrastive learning tasks of multi-behavior interests extracted through the Multi-behavior Interest Module: Multi-behavior Interest Contrast(MIC) task and Multi-behavior Interest Alignment(MIA) task. These two tasks focus on extracting|点进率点击率(ctrl)预测在各种在线应用程序中扮演着重要的角色，旨在估计用户的点击概率。来自各种交互行为的用户兴趣建模(例如，单击、添加到购物车、订单)正在成为 CTR 预测的主流方法。我们认为，不同的用户行为包含两个重要的内在特征: 1)不同行为的差异揭示了用户行为特定兴趣的不同方面。例如，一个可能点击需要，但更注意评级时，购买。2)各种行为的一致性包含用户行为不变的兴趣。例如，用户更喜欢交互式项目而不是其他项目。因此，有必要从海量的行为信息中分离出差异性和一致性信号。不幸的是，以前的方法还没有很好地研究这种现象，这限制了推荐性能。为了应对这一挑战，我们提出了一种新的双重多行为兴趣网络(简称 DMBIN) ，将行为特异性兴趣和行为不变性兴趣从各种行为中分离出来，以获得更好的推荐。具体来说，DMBIN 形式化了各种行为之间的差异性和一致性特征。对两个实际数据集的大量实验和实证分析表明，DMBIN 显著优于最先进的方法。此外，DMBIN 也被应用于在线赞助商搜索广告系统中，在点击率和美团成本方面分别取得了2.11% 和2.76% 的提高。拆分任务是通过多行为兴趣模块提取的两个多行为兴趣对比学习任务: 多行为兴趣对比任务(MIC)和多行为兴趣对齐任务(MIA)。这两个任务的重点是提取|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DMBIN:+A+Dual+Multi-behavior+Interest+Network+for+Click-Through+Rate+Prediction+via+Contrastive+Learning)|0|
|[Ensemble Modeling with Contrastive Knowledge Distillation for Sequential Recommendation](https://doi.org/10.1145/3539618.3591679)|Hanwen Du, Huanhuan Yuan, Pengpeng Zhao, Fuzhen Zhuang, Guanfeng Liu, Lei Zhao, Yanchi Liu, Victor S. Sheng||Sequential recommendation aims to capture users' dynamic interest and predicts the next item of users' preference. Most sequential recommendation methods use a deep neural network as sequence encoder to generate user and item representations. Existing works mainly center upon designing a stronger sequence encoder. However, few attempts have been made with training an ensemble of networks as sequence encoders, which is more powerful than a single network because an ensemble of parallel networks can yield diverse prediction results and hence better accuracy. In this paper, we present Ensemble Modeling with contrastive Knowledge Distillation for sequential recommendation (EMKD). Our framework adopts multiple parallel networks as an ensemble of sequence encoders and recommends items based on the output distributions of all these networks. To facilitate knowledge transfer between parallel networks, we propose a novel contrastive knowledge distillation approach, which performs knowledge transfer from the representation level via Intra-network Contrastive Learning (ICL) and Cross-network Contrastive Learning (CCL), as well as Knowledge Distillation (KD) from the logits level via minimizing the Kullback-Leibler divergence between the output distributions of the teacher network and the student network. To leverage contextual information, we train the primary masked item prediction task alongside the auxiliary attribute prediction task as a multi-task learning scheme. Extensive experiments on public benchmark datasets show that EMKD achieves a significant improvement compared with the state-of-the-art methods. Besides, we demonstrate that our ensemble method is a generalized approach that can also improve the performance of other sequential recommenders. Our code is available at this link: https://github.com/hw-du/EMKD.|序贯推荐旨在捕捉用户的动态兴趣，预测用户的下一项偏好。大多数顺序推荐方法使用深层神经网络作为序列编码器来生成用户和项目表示。现有的工作主要集中在设计一个更强的序列编码器。然而，很少有人尝试将一个网络集合训练成序列编码器，它比单个网络更强大，因为一个并行网络集合可以产生不同的预测结果，从而提高准确性。本文提出了一种基于对比知识提取的序贯推荐系统集成建模方法。我们的框架采用多个并行网络作为序列编码器的集成，并根据这些网络的输出分布推荐项目。为了促进并行网络之间的知识转移，提出了一种新的对比知识提取方法，该方法通过网络内对比学习(ICL)和跨网络对比学习(CCL)从表示层进行知识转移，通过最小化教师网络和学生网络输出分布之间的 Kullback-Leibler 差异，从 logits 层进行知识提取。为了充分利用上下文信息，我们将主要隐藏项目预测任务与辅助属性预测任务一起训练为一个多任务学习方案。在公共基准数据集上进行的大量实验表明，EMKD 方法与最新的方法相比，取得了显著的改进。此外，我们证明了我们的集成方法是一种通用的方法，也可以改善其他顺序推荐器的性能。我们的代码可在以下连结找到:  https://github.com/hw-du/emkd。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ensemble+Modeling+with+Contrastive+Knowledge+Distillation+for+Sequential+Recommendation)|0|
|[HDNR: A Hyperbolic-Based Debiased Approach for Personalized News Recommendation](https://doi.org/10.1145/3539618.3591693)|Shicheng Wang, Shu Guo, Lihong Wang, Tingwen Liu, Hongbo Xu|Institute of Information Engineering, Chinese Academy of Sciences & School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; National Computer Network Emergency Response Technical Team/Coordination Center, Beijing, China|Personalized news recommendation aims to recommend candidate news to the target user, according to the clicked news history. The user-news interaction data exhibits power-law distribution, however, existing works usually learn representations in Euclidean space which makes inconsistent capacities between data space and embedding space, leading to severe representation distortion problem. Besides, the existence of conformity bias, a potential cause of power-law distribution, may introduce biased guidance to learn user representations. In this paper, we propose a novel debiased method based on hyperbolic space, named HDNR, to tackle the above problems. Specifically, first, we employ hyperboloid model with exponential growth capacity to conduct user and news modeling, in order to solve inconsistent space capacities problem and obtain low distortion representations. Second, we design a re-weighting aggregation module to further mitigate conformity bias in data distribution, through considering local importance of the clicked news among contextual history and its global popularity degree simultaneously. Finally, we calculate the relevance score between target user and candidate news representations. We conduct experiments on two real-world news recommendation datasets MIND-Large, MIND-Small and empirical results demonstrate the effectiveness of our approach from multiple perspectives.|个性化新闻推荐的目的是根据点击新闻的历史记录向目标用户推荐候选新闻。用户-新闻交互数据呈幂律分布，但现有的作品通常在欧氏空间中学习表示，使得数据空间与嵌入空间的容量不一致，从而导致严重的表示失真问题。此外，幂律分布的一个潜在原因——从众偏差的存在，可能会引入偏差指导来学习用户表征。在本文中，我们提出了一种新的基于双曲空间的去偏方法，命名为 HDNR，以解决上述问题。具体来说，首先，我们使用具有双曲面模型容量的指数增长来进行用户和新闻建模，以解决空间容量不一致的问题，并获得低失真表示。其次，通过同时考虑点击新闻在上下文历史中的局部重要性及其全球受欢迎程度，设计了一个重新加权聚合模块，进一步减轻数据分布中的一致性偏差。最后，计算目标用户与候选新闻表征之间的相关度得分。我们对两个真实世界的新闻推荐数据集 MIND-Large、 MIND-Small 和实证结果进行了实验，从多个角度证明了我们的方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HDNR:+A+Hyperbolic-Based+Debiased+Approach+for+Personalized+News+Recommendation)|0|
|[LinRec: Linear Attention Mechanism for Long-term Sequential Recommender Systems](https://doi.org/10.1145/3539618.3591717)|Langming Liu, Liu Cai, Chi Zhang, Xiangyu Zhao, Jingtong Gao, Wanyu Wang, Yifu Lv, Wenqi Fan, Yiqi Wang, Ming He, Zitao Liu, Qing Li|Harbin Engineering University, Harbin, China; City University of Hong Kong, Hong kong, Hong Kong; Guangdong Institute of Smart Education, Jinan University, Guangdong, China; National University of Defense Technology, Changsha, China; Ant Group, Beijing, China; The Hong Kong Polytechnic University, Hong Kong, Hong Kong; AI Lab, Lenovo Research, Beijing, China; City University of Hong Kong, Hong Kong, Hong Kong|Transformer models have achieved remarkable success in sequential recommender systems (SRSs). However, computing the attention matrix in traditional dot-product attention mechanisms results in a quadratic complexity with sequence lengths, leading to high computational costs for long-term sequential recommendation. Motivated by the above observation, we propose a novel L2-Normalized Linear Attention for the Transformer-based Sequential Recommender Systems (LinRec), which theoretically improves efficiency while preserving the learning capabilities of the traditional dot-product attention. Specifically, by thoroughly examining the equivalence conditions of efficient attention mechanisms, we show that LinRec possesses linear complexity while preserving the property of attention mechanisms. In addition, we reveal its latent efficiency properties by interpreting the proposed LinRec mechanism through a statistical lens. Extensive experiments are conducted based on two public benchmark datasets, demonstrating that the combination of LinRec and Transformer models achieves comparable or even superior performance than state-of-the-art Transformer-based SRS models while significantly improving time and memory efficiency. The implementation code is available online at https://github.com/Applied-Machine-Learning-Lab/LinRec.>|变压器模型在顺序推荐系统(SRS)中取得了显著的成功。然而，在传统的点积注意机制中，注意矩阵的计算会导致序列长度的二次复杂性，从而导致长期序列推荐的计算成本较高。在此基础上，我们提出了一种新的基于变压器的顺序推荐系统(LinRec)的 L2归一化线性注意算法，该算法在保留传统点积注意学习能力的同时，从理论上提高了效率。具体来说，通过对有效注意机制等价条件的深入研究，我们证明了 LinRec 具有线性复杂性，同时保留了注意机制的性质。此外，我们通过统计透镜解释所提出的 LinRec 机制，揭示了其潜在的效率特性。基于两个公开的基准数据集进行了大量的实验，结果表明，LinRec 模型和 Transformer 模型的组合在显著提高时间和内存效率的同时，实现了与基于最新变压器的 SRS 模型相当甚至更好的性能。实施守则可于网上 https://github.com/applied-machine-learning-lab/linrec 下载。 >|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LinRec:+Linear+Attention+Mechanism+for+Long-term+Sequential+Recommender+Systems)|0|
|[Personalized Retrieval over Millions of Items](https://doi.org/10.1145/3539618.3591749)|Hemanth Vemuri, Sheshansh Agrawal, Shivam Mittal, Deepak Saini, Akshay Soni, Abhinav V. Sambasivan, Wenhao Lu, Yajun Wang, Mehul Parsana, Purushottam Kar, Manik Varma|Google, Mountain View, CA, USA; Microsoft Research, Bengaluru, India; LinkedIn Corporation, Sunnyvale, CA, USA; Microsoft, Mountain View, CA, USA; IIT Kanpur, Kanpur, India; Microsoft, Bengaluru, India; Microsoft, Redmond, WA, USA|Personalized retrieval seeks to retrieve items relevant to a user event (e.g. a page visit or a query) that are adapted to the user's personal preferences. For example, two users who happen to perform the same event such as visiting the same product page or asking the same query should receive potentially distinct recommendations adapted to their individual tastes. Personalization is seldom attempted over catalogs of millions of items since the cost of existing personalization routines scale linearly in the number of candidate items. For example, performing two-sided personalized retrieval (with both event and item embeddings personalized to the user) incurs prohibitive storage and compute costs. Instead, it is common to use non-personalized retrieval to obtain a small shortlist of items over which personalized re-ranking can be done quickly. Despite being scalable, this strategy risks losing items uniquely relevant to a user that fail to get shortlisted during non-personalized retrieval. This paper bridges this gap by developing the XPERT algorithm that identifies a form of two-sided personalization that can be scalably implemented over millions of items and hundreds of millions of users. Key to overcoming the computational challenges of personalized retrieval is a novel concept of morph operators that can be used with arbitrary encoder architectures, completely avoids the steep memory overheads of two-sided personalization, provides millisecond-time inference and offers multi-intent retrieval. On multiple public and proprietary datasets, XPERT offered upto 5% superior recall and AUC than state-of-the-art techniques. Code for XPERT is available at https://github.com/personalizedretrieval/xpert.|个性化检索寻求检索与用户事件相关的项目(例如页面访问或查询) ，这些项目适合用户的个人喜好。例如，碰巧执行相同事件(如访问同一产品页面或询问同一查询)的两个用户应该会收到适合他们各自口味的潜在不同的推荐。由于现有个性化程序的成本在候选项的数量上呈线性规模，因此很少尝试对数百万个项目的目录进行个性化处理。例如，执行双边个性化检索(同时嵌入针对用户的个性化事件和项目)会带来令人望而却步的存储和计算成本。相反，通常使用非个性化检索来获得一个小的项目短名单，可以对其进行快速的个性化重新排名。尽管这种策略具有可伸缩性，但如果用户在非个性化检索过程中未能进入决选名单，则可能丢失与其唯一相关的项目。本文通过开发 XPERT 算法来弥补这一差距，该算法确定了一种双边个性化的形式，这种形式可以在数百万条目和数亿用户中可伸缩地实现。克服个性化检索的计算挑战的关键是一种新颖的变形运算符概念，它可以用于任意编码器架构，完全避免双边个性化的高昂内存开销，提供毫秒级推理和提供多意图检索。在多个公共和专有数据集上，XPERT 比最先进的技术提供高达5% 的优越召回率和 AUC。XPERT 代码可在 https://github.com/personalizedretrieval/XPERT 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalized+Retrieval+over+Millions+of+Items)|0|
|[BKD: A Bridge-based Knowledge Distillation Method for Click-Through Rate Prediction](https://doi.org/10.1145/3539618.3591958)|Yin Deng, Yingxin Chen, Xin Dong, Lingchao Pan, Hai Li, Lei Cheng, Linjian Mo|Ant Group, Hangzhou, China; Ant Group, Shanghai, China|Prediction models for click-through rate (CTR) learn feature interactions underlying user behaviors, which are crucial in recommendation systems. Due to their size and complexity, existing approaches have a limited range of applications. In order to decrease inference delay, knowledge distillation techniques have been used in recommendation systems. Due to the student model's lower capacity, the knowledge distillation process is less effective when there is a significant difference in the complexity of the network architecture between the teacher model and the student model. We present a novel knowledge distillation approach called Bridge-based Knowledge Distillation (BKD), which employs a bridge model to facilitate the student model's learning from the teacher model's latent representations. The bridge model is based on Graph Neural Networks (GNNs), and leverages the edges of GNNs to identify significant feature interaction relationships, while simultaneously reducing redundancy for improved efficiency. To further enhance the efficiency of knowledge distillation, we decoupled the extracted knowledge and transferred each component separately to the student model, aiming to improve the distillation sufficiency of each module. Extensive experimental results show that our proposed BKD approach outperforms state-of-the-art competitors on various tasks.|点进率预测模型(CTR)学习用户行为背后的特征交互，这在推荐系统中是至关重要的。由于其规模和复杂性，现有方法的应用范围有限。为了减少推理时延，在推荐系统中引入了知识提取技术。由于学生模型的容量较小，当教师模型和学生模型的网络结构复杂度存在显著差异时，知识提取过程的有效性较差。本文提出了一种新的知识提取方法——基于桥梁的知识提取方法(BKD) ，该方法采用一种桥梁模型来促进学生模型从教师模型的潜在表征中学习。该桥梁模型基于图形神经网络(GNN) ，利用 GNN 的边缘来识别重要的特征交互关系，同时减少冗余以提高效率。为了进一步提高知识提取的效率，对提取的知识进行解耦，并将每个组件分别转移到学生模型中，以提高每个模块的知识提取充分性。大量的实验结果表明，我们提出的 BKD 方法在各种任务上都优于最先进的竞争对手。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BKD:+A+Bridge-based+Knowledge+Distillation+Method+for+Click-Through+Rate+Prediction)|0|
|[Hierarchical Type Enhanced Negative Sampling for Knowledge Graph Embedding](https://doi.org/10.1145/3539618.3591996)|Zhenzhou Lin, Zishuo Zhao, Jingyou Xie, Ying Shen|Sun Yat-sen University, Shenzhen, China|Knowledge graph embedding aims at modeling knowledge by projecting entities and relations into a low-dimensional semantic space. Most of the works on knowledge graph embedding construct negative samples by negative sampling as knowledge graphs typically only contain positive facts. Although substantial progress has been made by dynamic distribution based sampling methods, selecting plausible and prior information-engaged negative samples still poses many challenges. Inspired by type constraint methods, we propose Hierarchical Type Enhanced Negative Sampling (HTENS) which leverages hierarchical entity type information and entity-relation cooccurrence information to optimize the sampling probability distribution of negative samples. The experiments performed on the link prediction task demonstrate the effectiveness of HTENS. Additionally, HTENS shows its superiority in versatility and can be integrated into scalable systems with enhanced negative sampling.|知识图嵌入的目的是将实体和关系投影到低维语义空间中，对知识进行建模。大多数关于知识图嵌入的工作都是通过负抽样来构造负样本，因为知识图通常只包含正事实。尽管基于动态分布的抽样方法已经取得了实质性的进展，但是选择合理的和事先信息参与的负样本仍然带来许多挑战。受到类型约束方法的启发，我们提出了层次类型增强负面抽样(HTENS) ，它利用层次实体类型信息和实体关系共现信息来优化负面样本的抽样概率分布。在链路预测任务上的实验验证了 HTENS 的有效性。此外，HTENS 显示了其通用性的优势，可以集成到增强负采样的可扩展系统中。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Type+Enhanced+Negative+Sampling+for+Knowledge+Graph+Embedding)|0|
|[LOVF: Layered Organic View Fusion for Click-through Rate Prediction in Online Advertising](https://doi.org/10.1145/3539618.3592014)|Lingwei Kong, Lu Wang, Xiwei Zhao, Junsheng Jin, Zhangang Lin, Jinghe Hu, Jingping Shao|JD.com, Beijing, China|Organic recommendation and advertising recommendation usually coexist on e-commerce platforms. In this paper, we study the problem of utilizing data from organic recommendation to reinforce click-through rate prediction in advertising scenarios from a multi-view learning perspective. We propose a novel method, termed LOVF (Layered Organic View Fusion). LOVF implements a multi-view fusion mechanism - for each advertising instance, LOVF derives deep representations layer-by-layer from the organic recommendation view and these deep representations are then fused into the corresponding vanilla representations of the advertising view. Extensive experiments across a variety of backbones demonstrate LOVF's generality, effectiveness and efficiency on a new real-world production dataset. The dataset encompasses data from both the organic recommendation and advertising scenarios. Notably, LOVF has been successfully deployed in the advertising recommender system of JD.com, which is one of the world's largest e-commerce platforms; online A/B testing shows that LOVF achieves impressive improvement on advertising clicks and revenue. Our code and dataset are available at https://github.com/adsturing/lovf for facilitating further research.|在电子商务平台上，有机推荐和广告推荐通常并存。在本文中，我们从多视角学习的角度来研究利用有机推荐的数据来加强广告场景中的点进率预测的问题。我们提出了一种新的方法，称为 LOVF (分层有机视图融合)。LOVF 实现了一种多视图融合机制——对于每个广告实例，LOVF 从有机推荐视图中逐层获得深度表示，然后将这些深度表示融合到相应的普通广告视图中。横跨各种骨干的大量实验证明了 LOVF 在一个新的真实世界生产数据集上的通用性、有效性和效率。数据集包含来自有机推荐和广告场景的数据。值得注意的是，在世界最大的电子商务平台之一京东的广告推荐系统中，LOVF 已经成功部署，在线 A/B 测试表明，LOVF 在广告点击量和收入方面取得了令人印象深刻的进步。我们的代码和数据集可供 https://github.com/adsturing/lovf 使用，以方便进一步的研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LOVF:+Layered+Organic+View+Fusion+for+Click-through+Rate+Prediction+in+Online+Advertising)|0|
|[Optimizing Reciprocal Rank with Bayesian Average for improved Next Item Recommendation](https://doi.org/10.1145/3539618.3592033)|Xiangkui Lu, Jun Wu, Jianbo Yuan|Beijing Jiaotong University & Engineering Research Center of Integration and Application of Digital Learning Technology,Ministry of Education, Beijing, China; Beijing Jiaotong University, Beijing, China; ByteDance, Bellevue, WA, USA|Next item recommendation is a crucial task of session-based recommendation. However, the gap between the optimization objective (Binary Cross Entropy) and the ranking metric (Mean Reciprocal Rank) has not been well-explored, resulting in sub-optimal recommendations. In this paper, we propose a novel objective function, namely Adjusted-RR, to directly optimize Mean Reciprocal Rank. Specifically, Adjusted-RR adopts Bayesian Average to adjust Reciprocal Rank loss with Normal Rank loss by creating position-aware weights between them. Adjusted-RR is a plug-and-play objective that is compatible with various models. We apply Adjusted-RR on two base models and two datasets, and experimental results show that it makes a significant improvement in the next item recommendation.|下一个项目推荐是基于会话的推荐的关键任务。然而，优化目标(二元交叉熵)和排名指标(平均倒数排名)之间的差距还没有得到很好的探索，导致次优建议。在本文中，我们提出了一个新的目标函数，即调整后的平均倒数排名，直接优化。具体来说，调整后的 RR 采用贝叶斯平均法，通过在两者之间建立位置感知权重来调整相互秩损失和正常秩损失。调整后的 RR 是一个即插即用的目标，它与各种模型兼容。我们在两个基本模型和两个数据集上应用了调整后的 RR，实验结果表明它对下一个项目的推荐有显著的改善。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+Reciprocal+Rank+with+Bayesian+Average+for+improved+Next+Item+Recommendation)|0|
|[uCTRL: Unbiased Contrastive Representation Learning via Alignment and Uniformity for Collaborative Filtering](https://doi.org/10.1145/3539618.3592076)|Jaewoong Lee, Seongmin Park, Mincheol Yoon, Jongwuk Lee||Because implicit user feedback for the collaborative filtering (CF) models is biased toward popular items, CF models tend to yield recommendation lists with popularity bias. Previous studies have utilized inverse propensity weighting (IPW) or causal inference to mitigate this problem. However, they solely employ pointwise or pairwise loss functions and neglect to adopt a contrastive loss function for learning meaningful user and item representations. In this paper, we propose Unbiased ConTrastive Representation Learning (uCTRL), optimizing alignment and uniformity functions derived from the InfoNCE loss function for CF models. Specifically, we formulate an unbiased alignment function used in uCTRL. We also devise a novel IPW estimation method that removes the bias of both users and items. Despite its simplicity, uCTRL equipped with existing CF models consistently outperforms state-of-the-art unbiased recommender models, up to 12.22% for Recall@20 and 16.33% for NDCG@20 gains, on four benchmark datasets.|由于协同过滤(CF)模型的隐式用户反馈偏向于流行项目，CF 模型倾向于产生带有流行偏见的推荐列表。以往的研究采用逆倾向加权(IPW)或因果推断来缓解这一问题。然而，他们仅仅使用逐点损失函数或成对损失函数，而忽略了采用对比损失函数来学习有意义的用户和项目表示。本文针对 CF 模型，提出了无偏对比表示学习(uCTRL)、优化对齐和由 InfoNCE 损失函数导出的均匀性函数。具体来说，我们制定了一个无偏对齐函数在 uCTRL 中使用。我们还设计了一种新的 IPW 估计方法，消除了用户和项目的偏差。尽管简单，配备现有 CF 模型的 uCTRL 始终优于最先进的无偏差推荐模型，在四个基准数据集上，Recall@20最高为12.22% ，NDCG@20最高为16.33% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=uCTRL:+Unbiased+Contrastive+Representation+Learning+via+Alignment+and+Uniformity+for+Collaborative+Filtering)|0|
|[Unsupervised Query Performance Prediction for Neural Models with Pairwise Rank Preferences](https://doi.org/10.1145/3539618.3592082)|Ashutosh Singh, Debasis Ganguly, Suchana Datta, Craig MacDonald|University of Glasgow, Glasgow, United Kingdom; University College Dublin, Dublin, Ireland|A query performance prediction (QPP) method predicts the effectiveness of an IR system for a given query. While unsupervised approaches have been shown to work well for statistical IR models, it is likely that these approaches would yield limited effectiveness for neural ranking models (NRMs) because the retrieval scores of these models lie within a short range unlike their statistical counterparts. In this work, we propose to leverage a pairwise inference-based NRM's (specifically, DuoT5) output to accumulate evidences on the pairwise believes of one document ranked above the other. We hypothesize that the more consistent these pairwise likelihoods are, the higher is the likelihood of the retrieval to be of better quality, thus yielding a higher QPP score. We conduct our experiments on the TREC-DL dataset leveraging pairwise likelihoods from an auxiliary model DuoT5. Our experiments demonstrate that the proposed method called Pairwise Rank Preference-based QPP (QPP-PRP) leads to significantly better results than a number of standard unsupervised QPP baselines on several NRMs.|一种查询性能预测(QPP)方法预测一个给定查询的红外系统的有效性。虽然无监督方法已被证明对于统计 IR 模型工作得很好，但是这些方法可能对神经排序模型(NRM)产生有限的有效性，因为这些模型的检索分数在与其统计对应物不同的短范围内。在这项工作中，我们建议利用基于成对推理的 NRM (特别是 DuoT5)输出来积累证据，证明一个文档的成对信念高于另一个文档。我们假设这些成对可能性越一致，检索的质量越好的可能性就越高，从而产生更高的 QPP 评分。我们利用来自辅助模型 DuoT5的成对可能性在 TREC-DL 数据集上进行实验。我们的实验表明，所提出的方法称为成对秩优先的 QPP (QPP-PRP)导致明显更好的结果比一些标准的无监督 QPP 基线上的几个 NRM。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unsupervised+Query+Performance+Prediction+for+Neural+Models+with+Pairwise+Rank+Preferences)|0|
|[Repetition and Exploration in Sequential Recommendation](https://doi.org/10.1145/3539618.3591914)|Ming Li, Ali Vardasbi, Andrew Yates, Maarten de Rijke|University of Amsterdam, Amsterdam, Netherlands|In several recommendation scenarios, including next basket recommendation, the importance of repetition and exploration has been discovered and studied. Sequential recommenders (SR) aim to infer a user's preferences and suggest the next item for them to interact with based on their historical interaction sequences. There has not been a systematic analysis of sequential recommenders from the perspective of repetition and exploration. As a result, it is unclear how these models, that are typically optimized for accuracy, perform in terms of repetition and exploration, as well as the potential drawbacks of deploying them in real applications. In this paper, we examine whether repetition and exploration are important dimensions in the sequential recommendation scenario. We consider this generalizability question both from a user-centered and an item-centered perspective. Towards the latter, we define item repeat exposure and item explore exposure and examine the recommendation performance of sequential recommendation models in terms of both accuracy and exposure from the perspective of repetition and exploration. We find that (i) there is an imbalance in accuracy and difficulty w.r.t. repetition and exploration in SR scenarios, (ii) using the conventional average overall accuracy with a significance test does not fully represent a model's recommendation accuracy, and (iii) accuracy-oriented sequential recommendation models may suffer from less/zero item explore exposure issue, where items are mostly (or even only) recommended to their repeat users and fail to reach their potential new users. To analyze our findings, we remove repeat samples from the dataset, that often act as easy shortcuts, and focus on a pure exploration SR scenario. We find that (i) removing the repetition shortcut increases the recommendation novelty and helps users who prefer to consume novel items next, (ii) neural-based models fail to learn the basic characteristics of this pure exploration scenario and suffer from an inherent repetitive bias issue, (iii) using shared item embeddings in the prediction layer may skew recommendations to repeat items, and (iv) removing all repeat items to post-processing recommendation results leads to a substantial improvement on top of several SR methods.|在几个推荐场景中，包括下一个篮子推荐，重复和探索的重要性已经被发现和研究。顺序推荐(SR)旨在推断用户的偏好，并根据用户的历史交互顺序建议下一个用户交互的项目。从重复和探索的角度对序贯推荐进行了系统的分析。因此，目前还不清楚这些通常为精确性而优化的模型在重复和探索方面如何执行，以及在实际应用中部署它们的潜在缺点。在本文中，我们研究了重复和探索是否是顺序推荐场景中的重要维度。我们从以用户为中心和以项目为中心的角度来考虑这个可推广性问题。对于后者，我们定义了项目重复暴露和项目探索暴露，并从重复和探索的角度考察了序贯推荐模型在准确性和暴露性方面的推荐绩效。我们发现(i)在 SR 情景下重复和探索的准确性和难度存在不平衡，(ii)使用传统的平均整体准确性和显着性测试并不完全代表模型的推荐准确性，以及(iii)面向准确性的顺序推荐模型可能遭受较少/零项目探索暴露问题，其中项目大部分(甚至只有)被推荐给其重复用户，并且未能达到其潜在的新用户。为了分析我们的发现，我们从数据集中移除了重复的样本，这些样本通常作为简单的捷径，并专注于一个纯粹的探索 SR 场景。我们发现(i)删除重复快捷方式增加了推荐的新颖性，并帮助喜欢接下来消费新颖项目的用户，(ii)基于神经的模型未能学习这种纯探索场景的基本特征，并遭受固有的重复偏差问题，(iii)在预测层中使用共享项嵌入可能使推荐偏向重复项目，以及(iv)删除所有重复项目以后处理推荐结果导致几种 SR 方法之上的实质性改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Repetition+and+Exploration+in+Sequential+Recommendation)|0|
|[JDsearch: A Personalized Product Search Dataset with Real Queries and Full Interactions](https://doi.org/10.1145/3539618.3591900)|Jiongnan Liu, Zhicheng Dou, Guoyu Tang, Sulong Xu||Recently, personalized product search attracts great attention and many models have been proposed. To evaluate the effectiveness of these models, previous studies mainly utilize the simulated Amazon recommendation dataset, which contains automatically generated queries and excludes cold users and tail products. We argue that evaluating with such a dataset may yield unreliable results and conclusions, and deviate from real user satisfaction. To overcome these problems, in this paper, we release a personalized product search dataset comprised of real user queries and diverse user-product interaction types (clicking, adding to cart, following, and purchasing) collected from JD.com, a popular Chinese online shopping platform. More specifically, we sample about 170,000 active users on a specific date, then record all their interacted products and issued queries in one year, without removing any tail users and products. This finally results in roughly 12,000,000 products, 9,400,000 real searches, and 26,000,000 user-product interactions. We study the characteristics of this dataset from various perspectives and evaluate representative personalization models to verify its feasibility. The dataset can be publicly accessed at Github: https://github.com/rucliujn/JDsearch.|近年来，个性化产品搜索引起了人们的广泛关注，提出了许多个性化产品搜索模型。为了评估这些模型的有效性，以往的研究主要利用模拟亚马逊推荐数据集，其中包含自动生成的查询，并排除了冷用户和尾部产品。我们认为，使用这样的数据集进行评估可能产生不可靠的结果和结论，并偏离真正的用户满意度。为了克服这些问题，本文发布了一个个性化产品搜索数据集，该数据集由真实用户查询和不同的用户-产品交互类型(点击、添加到购物车、跟随和购买)组成，收集自中国流行的在线购物平台京东(JD.com)。更具体地说，我们在一个特定的日期抽样约170,000个活跃用户，然后记录他们所有的交互产品，并在一年内发布查询，而不删除任何尾用户和产品。这最终产生了大约12,000,000个产品，9,400,000个实际搜索，以及26,000,000个用户-产品交互。我们从不同的角度研究了这个数据集的特点，并评价了具有代表性的个性化模型，以验证其可行性。该数据集可以在 Github:  https://Github.com/rucliujn/jdsearch 上公开访问。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=JDsearch:+A+Personalized+Product+Search+Dataset+with+Real+Queries+and+Full+Interactions)|0|
|[FedAds: A Benchmark for Privacy-Preserving CVR Estimation with Vertical Federated Learning](https://doi.org/10.1145/3539618.3591909)|Penghui Wei, Hongjian Dou, Shaoguo Liu, Rongjun Tang, Li Liu, Liang Wang, Bo Zheng||Conversion rate (CVR) estimation aims to predict the probability of conversion event after a user has clicked an ad. Typically, online publisher has user browsing interests and click feedbacks, while demand-side advertising platform collects users' post-click behaviors such as dwell time and conversion decisions. To estimate CVR accurately and protect data privacy better, vertical federated learning (vFL) is a natural solution to combine two sides' advantages for training models, without exchanging raw data. Both CVR estimation and applied vFL algorithms have attracted increasing research attentions. However, standardized and systematical evaluations are missing: due to the lack of standardized datasets, existing studies adopt public datasets to simulate a vFL setting via hand-crafted feature partition, which brings challenges to fair comparison. We introduce FedAds, the first benchmark for CVR estimation with vFL, to facilitate standardized and systematical evaluations for vFL algorithms. It contains a large-scale real world dataset collected from Alibaba's advertising platform, as well as systematical evaluations for both effectiveness and privacy aspects of various vFL algorithms. Besides, we also explore to incorporate unaligned data in vFL to improve effectiveness, and develop perturbation operations to protect privacy well. We hope that future research work in vFL and CVR estimation benefits from the FedAds benchmark.|转换率(CVR)估计的目的是预测用户点击广告后发生转换事件的概率。通常，在线出版商有用户浏览兴趣和点击反馈，而需求侧广告平台收集用户的点击后行为，如停留时间和转换决定。为了更准确地估计 CVR，更好地保护数据隐私，垂直联邦学习(vFL)是一种自然而然的解决方案，它结合了双方在训练模型方面的优势，不需要交换原始数据。CVR 估计和应用的 vFL 算法都受到了越来越多的研究关注。然而，由于缺乏标准化的数据集，现有的研究采用公共数据集通过手工特征划分来模拟 vFL 设置，这给公平比较带来了挑战。我们引入 FedAds，第一个用 vFL 进行 CVR 估计的基准，以促进 vFL 算法的标准化和系统化评估。它包含从阿里巴巴广告平台收集的大规模真实世界数据集，以及对各种 vfL 算法的有效性和隐私方面的系统评估。此外，我们还探讨了在 vFL 中加入未对齐数据以提高效率，并开发扰动操作以更好地保护隐私。我们希望未来在 vFL 和 CVR 估计方面的研究工作能够从 FedAds 基准中受益。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedAds:+A+Benchmark+for+Privacy-Preserving+CVR+Estimation+with+Vertical+Federated+Learning)|0|
|[TMML: Text-Guided MuliModal Product Location For Alleviating Retrieval Inconsistency in E-Commerce](https://doi.org/10.1145/3539618.3591836)|Youhua Tang, Xiong Xiong, Siyang Sun, Baoliang Cui, Yun Zheng, Haihong Tang|Alibaba Group, Hangzhou, China|Image retrieval system (IRS) is commonly used in E-Commerce platforms for a wide range of applications such as price comparison and commodity recommendation. However, customers may experience inconsistent retrieval problems. Although the retrieved image contains the query object, the main product of the retrieved image is not associated with the query product. This is caused by the wrong product instance location when building the product image retrieval library. We can easily determine which product is on sale through the hint of the title, so we propose Text-Guided MuliModal Product Location (TMML) to use additional product titles to assist in locating the actual selling product instance. We design a weakly-aligned region-text data collection method to generate region-text pseudo-label by utilizing the IRS and user behavior from the E-commerce platform. To mitigate the impact of data noise, we propose a Mutual-Aware Contrastive Loss. Our results show that the proposed TMML outperforms the state-of-the-art method GLIP [11] by 3.95% in top-1 precision on our multi-objects test set, and 2.53% error located images in AliExpress has been corrected, which greatly alleviates the retrieval inconsistencies in IRS.|图像检索系统(IRS)广泛应用于电子商务平台的价格比较和商品推荐等领域。但是，客户可能会遇到不一致的检索问题。虽然检索到的图像包含查询对象，但是检索到的图像的主产品不与查询产品关联。这是由于构建产品图像检索库时产品实例位置错误造成的。我们可以通过标题的提示轻松地确定正在销售的产品，因此我们建议使用文本导向的 MuliModal Product Location (TMML)来使用附加的产品标题来帮助定位实际的销售产品实例。设计了一种弱对齐区域文本数据采集方法，利用电子商务平台的 IRS 和用户行为生成区域文本伪标签。为了减轻数据噪声的影响，我们提出了一种相互感知的对比度损失。实验结果表明，本文提出的 TMML 算法在多目标测试集上的检索精度比最先进的 GLIP [11]算法提高了3.95% ，并且对 AliExpress 中2.53% 的错误定位图像进行了校正，大大减轻了 IRS 中的检索不一致性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TMML:+Text-Guided+MuliModal+Product+Location+For+Alleviating+Retrieval+Inconsistency+in+E-Commerce)|0|
|[Alleviating Matching Bias in Marketing Recommendations](https://doi.org/10.1145/3539618.3591854)|Junpeng Fang, Qing Cui, Gongduo Zhang, Caizhi Tang, Lihong Gu, Longfei Li, Jinjie Gu, Jun Zhou, Fei Wu|Ant Group, Hangzhou, China; Zhejiang University, Hangzhou, China|In marketing recommendations, the campaign organizers will distribute coupons to users to encourage consumption. In general, a series of strategies are employed to interfere with the coupon distribution process, leading to a growing imbalance between user-coupon interactions, resulting in a bias in the estimation of conversion probabilities. We refer to the estimation bias as the matching bias. In this paper, we explore how to alleviate the matching bias from the causal-effect perspective. We regard the historical distributions of users and coupons over each other as confounders and characterize the matching bias as a confounding effect to reveal and eliminate the spurious correlations between user-coupon representations and conversion probabilities. Then we propose a new training paradigm named De-Matching Bias Recommendation (DMBR) to remove the confounding effects during model training via the backdoor adjustment. We instantiate DMBR on two representative models: DNN and MMOE, and conduct extensive offline and online experiments to demonstrate the effectiveness of our proposed paradigm.|在营销建议中，活动组织者将向用户分发优惠券以鼓励消费。一般来说，一系列的策略被用来干扰优惠券的分配过程，导致用户-优惠券之间的交互日益不平衡，导致在估计转换概率方面的偏差。我们把估计偏差称为匹配偏差。本文从因果关系的角度探讨如何减轻匹配偏差。我们将用户和优惠券的历史分布视为混杂因素，并将匹配偏差描述为混杂效应，以揭示和消除用户-优惠券表示和转换概率之间的虚假相关性。然后提出了一种新的训练范式——去匹配偏差推荐(DMBR) ，通过后门调整消除模型训练过程中的混杂效应。我们在两个代表性的模型上实例化了 DMBR: DNN 和 MMOE，并进行了大量的离线和在线实验来证明我们提出的范例的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Alleviating+Matching+Bias+in+Marketing+Recommendations)|0|
|[Implicit Query Parsing at Amazon Product Search](https://doi.org/10.1145/3539618.3591858)|Chen Luo, Rahul Goutam, Haiyang Zhang, Chao Zhang, Yangqiu Song, Bing Yin|Amazon Search, Palo Alto, CA, USA; HKUST, Hong Kong, Hong Kong; Gatech, Atlanta, GA, USA|Query Parsing aims to extract product attributes, such as color, brand, and product type, from search queries. These attributes play a crucial role in search engines for tasks such as matching, ranking, and recommendation. There are two types of attributes: explicit attributes that are mentioned explicitly in the search query, and implicit attributes that are mentioned implicitly. Existing works on query parsing do not differentiate between explicit query parsing and implicit query parsing, which limits their performance in product search engines. In this work, we demonstrate the critical importance of implicit attributes in real-world product search engines. We then present our solution for implicit query parsing at Amazon Search, which is a unified framework combining recent advancements in knowledge graph technologies and customer behavior analysis. We demonstrate the effectiveness of our proposal through offline experiments on Amazon search log data. We also show how to deploy and use the framework on Amazon search to improve customers' shopping experiences.|查询解析旨在从搜索查询中提取产品属性，如颜色、品牌和产品类型。这些属性在搜索引擎的任务(如匹配、排名和推荐)中起着至关重要的作用。有两种类型的属性: 在搜索查询中显式提到的显式属性和隐式提到的隐式属性。现有的查询解析工作没有区分显式查询解析和隐式查询解析，这限制了它们在产品搜索引擎中的性能。在这项工作中，我们证明了隐式属性在现实世界的产品搜索引擎中的重要性。然后，我们提出了我们的解决方案隐式查询解析在亚马逊搜索，这是一个统一的框架相结合的最新进展，知识图技术和客户行为分析。我们通过亚马逊搜索日志数据的离线实验来证明我们提议的有效性。我们还展示了如何部署和使用 Amazon 搜索框架来改善客户的购物体验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Implicit+Query+Parsing+at+Amazon+Product+Search)|0|
|[Delving into E-Commerce Product Retrieval with Vision-Language Pre-training](https://doi.org/10.1145/3539618.3591859)|Xiaoyang Zheng, Fuyu Lv, Zilong Wang, Qingwen Liu, Xiaoyi Zeng||E-commerce search engines comprise a retrieval phase and a ranking phase, where the first one returns a candidate product set given user queries. Recently, vision-language pre-training, combining textual information with visual clues, has been popular in the application of retrieval tasks. In this paper, we propose a novel V+L pre-training method to solve the retrieval problem in Taobao Search. We design a visual pre-training task based on contrastive learning, outperforming common regression-based visual pre-training tasks. In addition, we adopt two negative sampling schemes, tailored for the large-scale retrieval task. Besides, we introduce the details of the online deployment of our proposed method in real-world situations. Extensive offline/online experiments demonstrate the superior performance of our method on the retrieval task. Our proposed method is employed as one retrieval channel of Taobao Search and serves hundreds of millions of users in real time.|电子商务搜索引擎包括检索阶段和排名阶段，其中第一个搜索引擎返回给定用户查询的候选产品集。近年来，将文本信息与视觉线索相结合的视觉语言预训练方法在检索任务中的应用越来越普遍。本文针对淘宝搜索中的检索问题，提出了一种新的 V + L 预训练方法。我们设计了一个基于对比学习的视觉预训练任务，其表现优于一般的基于回归的视觉预训练任务。此外，我们还采用了两种负抽样方案，以适应大规模的检索任务。此外，我们还详细介绍了我们提出的方法在现实环境中的在线部署。大量的离线/在线实验证明了该方法在检索任务中的优越性能。我们提出的方法被用作淘宝搜索的一个检索通道，为数亿用户提供实时服务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Delving+into+E-Commerce+Product+Retrieval+with+Vision-Language+Pre-training)|0|
|[Contrastive Box Embedding for Collaborative Reasoning](https://doi.org/10.1145/3539618.3591654)|Tingting Liang, Yuanqing Zhang, Qianhui Di, Congying Xia, Youhuizi Li, Yuyu Yin|Hangzhou Dianzi University, Hangzhou, China; Salesforce AI Research, Palo Alto, USA|Most of the existing personalized recommendation methods predict the probability that one user might interact with the next item by matching their representations in the latent space. However, as a cognitive task, it is essential for an impressive recommender system to acquire the cognitive capacity rather than to decide the users' next steps by learning the pattern from the historical interactions through matching-based objectives. Therefore, in this paper, we propose to model the recommendation as a logical reasoning task which is more in line with an intelligent recommender system. Different from the prior works, we embed each query as a box rather than a single point in the vector space, which is able to model sets of users or items enclosed and logical operators (e.g., intersection) over boxes in a more natural manner. Although modeling the logical query with box embedding significantly improves the previous work of reasoning-based recommendation, there still exist two intractable issues including aggregation of box embeddings and training stalemate in critical point of boxes. To tackle these two limitations, we propose a Contrastive Box learning framework for Collaborative Reasoning (CBox4CR). Specifically, CBox4CR combines a smoothed box volume-based contrastive learning objective with the logical reasoning objective to learn the distinctive box representations for the user's preference and the logical query based on the historical interaction sequence. Extensive experiments conducted on four publicly available datasets demonstrate the superiority of our CBox4CR over the state-of-the-art models in recommendation task.|现有的大多数个性化推荐方法通过匹配用户在潜在空间中的表示来预测用户与下一个项目交互的概率。然而，作为一项认知任务，一个令人印象深刻的推荐系统必须获得认知能力，而不是通过基于匹配的目标从历史交互中学习模式来决定用户的下一步行动。因此，在本文中，我们建议将推荐建模为一个更符合智能逻辑推理的推荐系统任务。与之前的工作不同，我们将每个查询嵌入一个框，而不是向量空间中的一个点，这样就能够以更自然的方式在框之上建模用户集或被封闭的项以及逻辑运算符集(例如，交叉)。框嵌入逻辑查询建模虽然显著改善了以往基于推理的推荐工作，但仍然存在框嵌入的聚合和框关键点的训练僵局等难题。为了解决这两个限制，我们提出了一个用于协作推理的对比框学习框架(CBox4CR)。具体来说，cbox4CR 结合了基于光滑盒子体积的对比学习目标和逻辑推理目标，学习用户偏好的独特盒子表示和基于历史交互序列的逻辑查询。在四个公开数据集上进行的大量实验证明了我们的 CBox4CR 在推荐任务中优于最先进的模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contrastive+Box+Embedding+for+Collaborative+Reasoning)|0|
|[Dual Contrastive Transformer for Hierarchical Preference Modeling in Sequential Recommendation](https://doi.org/10.1145/3539618.3591672)|Chengkai Huang, Shoujin Wang, Xianzhi Wang, Lina Yao|CSIRO's Data 61 and UNSW, Sydney, NSW, Australia; The University of New South Wales, Sydney, NSW, Australia; University of Technology Sydney, Sydney, NSW, Australia|Sequential recommender systems (SRSs) aim to predict the subsequent items which may interest users via comprehensively modeling users' complex preference embedded in the sequence of user-item interactions. However, most of existing SRSs often model users' single low-level preference based on item ID information while ignoring the high-level preference revealed by item attribute information, such as item category. Furthermore, they often utilize limited sequence context information to predict the next item while overlooking richer inter-item semantic relations. To this end, in this paper, we proposed a novel hierarchical preference modeling framework to substantially model the complex low- and high-level preference dynamics for accurate sequential recommendation. Specifically, in the framework, a novel dual-transformer module and a novel dual contrastive learning scheme have been designed to discriminatively learn users' low- and high-level preference and to effectively enhance both low- and high-level preference learning respectively. In addition, a novel semantics-enhanced context embedding module has been devised to generate more informative context embedding for further improving the recommendation performance. Extensive experiments on six real-world datasets have demonstrated both the superiority of our proposed method over the state-of-the-art ones and the rationality of our design.|序贯推荐系统通过对用户交互序列中的复杂偏好进行综合建模，对用户可能感兴趣的后续项目进行预测。然而，现有的 SRS 往往基于项目 ID 信息对用户的单一低层偏好进行建模，而忽略了项目属性信息(如项目类别)所揭示的高层偏好。此外，他们往往利用有限的序列上下文信息来预测下一个项目，而忽略了更丰富的项目间语义关系。为此，本文提出了一种新的层次偏好建模框架，对复杂的低层和高层偏好动态进行实质性建模，以实现精确的顺序推荐。在该框架中，设计了一种新的双变压器模块和一种新的双对比学习方案，以区分学习用户的低级和高级偏好，并分别有效地增强低级和高级偏好学习。此外，还设计了一种新的语义增强的上下文嵌入模块，以生成更多的信息，从而进一步提高推荐性能。在六个实际数据集上的大量实验证明了该方法的优越性和设计的合理性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dual+Contrastive+Transformer+for+Hierarchical+Preference+Modeling+in+Sequential+Recommendation)|0|
|[Meta-optimized Contrastive Learning for Sequential Recommendation](https://doi.org/10.1145/3539618.3591727)|Xiuyuan Qin, Huanhuan Yuan, Pengpeng Zhao, Junhua Fang, Fuzhen Zhuang, Guanfeng Liu, Yanchi Liu, Victor S. Sheng||Contrastive Learning (CL) performances as a rising approach to address the challenge of sparse and noisy recommendation data. Although having achieved promising results, most existing CL methods only perform either hand-crafted data or model augmentation for generating contrastive pairs to find a proper augmentation operation for different datasets, which makes the model hard to generalize. Additionally, since insufficient input data may lead the encoder to learn collapsed embeddings, these CL methods expect a relatively large number of training data (e.g., large batch size or memory bank) to contrast. However, not all contrastive pairs are always informative and discriminative enough for the training processing. Therefore, a more general CL-based recommendation model called Meta-optimized Contrastive Learning for sequential Recommendation (MCLRec) is proposed in this work. By applying both data augmentation and learnable model augmentation operations, this work innovates the standard CL framework by contrasting data and model augmented views for adaptively capturing the informative features hidden in stochastic data augmentation. Moreover, MCLRec utilizes a meta-learning manner to guide the updating of the model augmenters, which helps to improve the quality of contrastive pairs without enlarging the amount of input data. Finally, a contrastive regularization term is considered to encourage the augmentation model to generate more informative augmented views and avoid too similar contrastive pairs within the meta updating. The experimental results on commonly used datasets validate the effectiveness of MCLRec.|对比学习(CL)作为一种新兴的方法来解决稀疏和噪声推荐数据的挑战。虽然已经取得了很好的效果，但是现有的 CL 方法只能通过手工数据或者模型增强来生成对比对，以便为不同的数据集找到合适的增强操作，这使得模型很难推广。此外，由于输入数据不足可能导致编码器学习折叠嵌入，这些 CL 方法期望相对大量的训练数据(例如，大批量或内存库)进行对比。然而，并非所有的对比对都具有足够的信息量和判别力来进行训练处理。因此，本文提出了一种更为通用的基于 CL 的推荐模型，称为序贯推荐元优化对比学习(MCLRec)。通过应用数据增强和可学习模型增强操作，对比数据和模型增强视图，创新标准 CL 框架，自适应地捕捉随机数据增强中隐藏的信息特征。此外，MCLRec 利用元学习方式来指导模型增强器的更新，这有助于提高对比对的质量，而不需要扩大输入数据的数量。最后，考虑了一个对比正则化项，以鼓励增强模型生成更多的信息增强视图，避免元更新中出现过于相似的对比对。在常用数据集上的实验结果验证了 MCLRec 算法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Meta-optimized+Contrastive+Learning+for+Sequential+Recommendation)|0|
|[A Personalized Dense Retrieval Framework for Unified Information Access](https://doi.org/10.1145/3539618.3591626)|Hansi Zeng, Surya Kallumadi, Zaid Alibadi, Rodrigo Frassetto Nogueira, Hamed Zamani||Developing a universal model that can efficiently and effectively respond to a wide range of information access requests -- from retrieval to recommendation to question answering -- has been a long-lasting goal in the information retrieval community. This paper argues that the flexibility, efficiency, and effectiveness brought by the recent development in dense retrieval and approximate nearest neighbor search have smoothed the path towards achieving this goal. We develop a generic and extensible dense retrieval framework, called \framework, that can handle a wide range of (personalized) information access requests, such as keyword search, query by example, and complementary item recommendation. Our proposed approach extends the capabilities of dense retrieval models for ad-hoc retrieval tasks by incorporating user-specific preferences through the development of a personalized attentive network. This allows for a more tailored and accurate personalized information access experience. Our experiments on real-world e-commerce data suggest the feasibility of developing universal information access models by demonstrating significant improvements even compared to competitive baselines specifically developed for each of these individual information access tasks. This work opens up a number of fundamental research directions for future exploration.|开发一个通用的模型，可以有效率和有效地响应广泛的信息访问请求——从检索到推荐到问答——一直是信息检索社区的长期目标。本文认为，最近在密集检索和近似最近邻搜索方面的发展所带来的灵活性、效率和有效性为实现这一目标铺平了道路。我们开发了一个通用的和可扩展的密集检索框架，称为框架，可以处理广泛的(个性化的)信息访问请求，如关键字搜索，按例查询，补充项目推荐。我们提出的方法扩展了密集检索模型的能力，通过合并用户特定的偏好，通过开发一个个性化的注意网络的特定检索任务。这样可以提供更加量身定制和准确的个性化信息访问体验。我们对现实世界电子商务数据的实验表明，即使与为这些个人信息获取任务中的每一项专门制定的竞争性基线相比，通用信息获取模型的开发也是可行的。这项工作为今后的探索开辟了一些基础性的研究方向。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Personalized+Dense+Retrieval+Framework+for+Unified+Information+Access)|0|
|[One Blade for One Purpose: Advancing Math Information Retrieval using Hybrid Search](https://doi.org/10.1145/3539618.3591746)|Wei Zhong, ShengChieh Lin, JhengHong Yang, Jimmy Lin|University of Waterloo, Waterloo, Canada|Neural retrievers have been shown to be effective for math-aware search. Their ability to cope with math symbol mismatches, to represent highly contextualized semantics, and to learn effective representations are critical to improving math information retrieval. However, the most effective retriever for math remains impractical as it depends on token-level dense representations for each math token, which leads to prohibitive storage demands, especially considering that math content generally consumes more tokens. In this work, we try to alleviate this efficiency bottleneck while boosting math information retrieval effectiveness via hybrid search. To this end, we propose MABOWDOR, a Math-Aware Bestof-Worlds Domain Optimized Retriever, which has an unsupervised structure search component, a dense retriever, and optionally a sparse retriever on top of a domain-adapted backbone learned by context-enhanced pretraining, each addressing a different need in retrieving heterogeneous data from math documents. Our hybrid search outperforms the previous state-of-the-art math IR system while eliminating efficiency bottlenecks. Our system is available at https://github.com/approach0/pya0.|神经检索器已被证明对数学感知搜索是有效的。他们处理数学符号不匹配的能力，表达高度上下文化语义的能力，以及学习有效表达的能力对于提高数学信息检索至关重要。然而，最有效的数学检索仍然不切实际，因为它依赖于每个数学令牌的令牌级密集表示，这导致了令人望而却步的存储需求，特别是考虑到数学内容通常消耗更多的令牌。在这项工作中，我们试图通过混合搜索提高数学信息检索的效率，同时缓解这一效率瓶颈。为此，我们提出了 MABOWDOR，一种数学意识最强的领域优化检索器，它具有无监督的结构搜索组件，密集检索器，以及可选的在通过上下文增强预训练学习的领域适应主干上的稀疏检索器，每个都解决了从数学文档中检索异构数据的不同需求。我们的混合搜索优于以前的最先进的数学红外系统，同时消除了效率瓶颈。我们的系统 https://github.com/approach0/pya0可用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=One+Blade+for+One+Purpose:+Advancing+Math+Information+Retrieval+using+Hybrid+Search)|0|
|[Poisoning Self-supervised Learning Based Sequential Recommendations](https://doi.org/10.1145/3539618.3591751)|Yanling Wang, Yuchen Liu, Qian Wang, Cong Wang, Chenliang Li|Wuhan University & City University of Hong Kong, Wuhan; Hong Kong, China; Wuhan University, Wuhan, China; City University of Hong Kong, Hong Kong, China|Self-supervised learning (SSL) has been recently applied to sequential recommender systems to provide high-quality user representations. However, while facilitating the learning process recommender systems, SSL is not without security threats: carefully crafted inputs can poison the pre-trained models driven by SSL, thus reducing the effectiveness of the downstream recommendation model. This work shows that poisoning attacks against the pre-training stage threaten sequential recommender systems. Without any background knowledge of the model architecture and parameters, nor any API queries, our strategy proves the feasibility of poisoning attacks on mainstream SSL-based recommender schemes as well as on commonly used datasets. By injecting only a tiny amount of fake users, we get the target item recommended to real users more than thousands of times as before, demonstrating that recommender systems have a new attack surface due to SSL. We further show our attack is challenging for recommendation platforms to detect and defend. Our work highlights the weakness of self-supervised recommender systems and shows the necessity for researchers to be aware of this security threat. Our source code is available at https://github.com/CongGroup/Poisoning-SSL-based-RS.|为了提供高质量的用户表示，自监督学习(SSL)最近被应用到顺序推荐系统中。然而，在促进学习过程推荐系统的同时，SSL 也不是没有安全威胁: 精心设计的输入可能毒害由 SSL 驱动的预先训练的模型，从而降低下游推荐模型的有效性。研究结果表明，针对训练前阶段的中毒攻击会对顺序推荐系统造成威胁。在没有任何模型体系结构和参数的背景知识，也没有任何 API 查询的情况下，我们的策略证明了对主流的基于 SSL 的推荐方案以及常用数据集进行中毒攻击的可行性。通过注入少量的虚假用户，我们可以像以前一样将目标条目推荐给真正的用户数千次，这表明推荐系统由于 SSL 而具有了一个新的攻击面。我们进一步表明，我们的攻击对推荐平台的检测和防御是具有挑战性的。我们的工作突出了自我监督推荐系统的弱点，并表明了研究人员意识到这种安全威胁的必要性。我们的源代码可以在 https://github.com/conggroup/poisoning-ssl-based-rs 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Poisoning+Self-supervised+Learning+Based+Sequential+Recommendations)|0|
|[Graph Masked Autoencoder for Sequential Recommendation](https://doi.org/10.1145/3539618.3591692)|Yaowen Ye, Lianghao Xia, Chao Huang|University of Hong Kong, Hong Kong, Hong Kong|While some powerful neural network architectures (e.g., Transformer, Graph Neural Networks) have achieved improved performance in sequential recommendation with high-order item dependency modeling, they may suffer from poor representation capability in label scarcity scenarios. To address the issue of insufficient labels, Contrastive Learning (CL) has attracted much attention in recent methods to perform data augmentation through embedding contrasting for self-supervision. However, due to the hand-crafted property of their contrastive view generation strategies, existing CL-enhanced models i) can hardly yield consistent performance on diverse sequential recommendation tasks; ii) may not be immune to user behavior data noise. In light of this, we propose a simple yet effective graph masked autoencoder that adaptively and dynamically distills global item transitional information for self-supervised augmentation. It naturally avoids the above issue of heavy reliance on constructing high-quality embedding contrastive views. Instead, an adaptive data reconstruction paradigm is designed to be integrated with the long-range item dependency modeling, for informative augmentation in sequential recommendation. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baseline models and can learn more accurate representations against data noise and sparsity. Our implemented model code is available at https://github.com/HKUDS/GMRec.|虽然一些强大的神经网络结构(例如，Transformer、 Graph 神经网络)已经通过高阶项依赖建模实现了序贯推荐的性能改善，但是在标签稀缺的情况下，它们可能会受到表示能力较差的影响。为了解决标签不足的问题，近年来，对比学习(CL)通过嵌入对比度进行自我监督来实现数据增强的方法受到了广泛关注。然而，由于其对比视图生成策略的手工特性，现有的 CL 增强模型 i)很难在不同的顺序推荐任务上产生一致的性能; ii)可能不能免疫用户行为数据噪声。鉴于此，我们提出了一种简单而有效的图掩码自动编码器，自适应和动态提取全局项目过渡信息的自监督增强。它自然而然地避免了上述严重依赖于构造高质量嵌入对比视图的问题。相反，设计了一种自适应数据重建范式，将其与长程项目依赖性建模相结合，用于顺序推荐中的信息增强。广泛的实验表明，我们的方法明显优于国家的最先进的基线模型，可以学习更准确的表示对数据噪音和稀疏。我们已实现的模型代码可在 https://github.com/hkuds/gmrec 获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Masked+Autoencoder+for+Sequential+Recommendation)|0|
|[A Generic Learning Framework for Sequential Recommendation with Distribution Shifts](https://doi.org/10.1145/3539618.3591624)|Zhengyi Yang, Xiangnan He, Jizhi Zhang, Jiancan Wu, Xin Xin, Jiawei Chen, Xiang Wang|University of Science and Technology of China, Hefei, China; Zhejiang University, Hangzhou, China; Shandong University, Qingdao, China|Leading sequential recommendation (SeqRec) models adopt empirical risk minimization (ERM) as the learning framework, which inherently assumes that the training data (historical interaction sequences) and the testing data (future interactions) are drawn from the same distribution. However, such i.i.d. assumption hardly holds in practice, due to the online serving and dynamic nature of recommender system.For example, with the streaming of new data, the item popularity distribution would change, and the user preference would evolve after consuming some items. Such distribution shifts could undermine the ERM framework, hurting the model's generalization ability for future online serving. In this work, we aim to develop a generic learning framework to enhance the generalization of recommenders in the dynamic environment. Specifically, on top of ERM, we devise a Distributionally Robust Optimization mechanism for SeqRec (DROS). At its core is our carefully-designed distribution adaption paradigm, which considers the dynamics of data distribution and explores possible distribution shifts between training and testing. Through this way, we can endow the backbone recommenders with better generalization ability.It is worth mentioning that DROS is an effective model-agnostic learning framework, which is applicable to general recommendation scenarios.Theoretical analyses show that DROS enables the backbone recommenders to achieve robust performance in future testing data.Empirical studies verify the effectiveness against dynamic distribution shifts of DROS. Codes are anonymously open-sourced at https://github.com/YangZhengyi98/DROS.|先导序贯推荐(SeqRec)模型采用经验风险最小化(ERM)作为学习框架，内在假设训练数据(历史交互序列)和测试数据(未来交互)来自同一分布。然而，由于网上服务及推荐系统的动态性，这种身份证假设在实际上难以成立。例如，随着新数据的流动，项目受欢迎程度的分布将发生变化，并且用户偏好将在使用某些项目之后发生变化。这种分销转变可能会破坏 ERM 框架，损害该模型对未来在线服务的推广能力。在这项工作中，我们的目标是开发一个通用的学习框架，以提高推荐系统在动态环境中的推广能力。具体来说，在 ERM 的基础上，我们为 SeqRec (DROS)设计了一种分布式鲁棒优化机制。其核心是我们精心设计的分布适应范例，它考虑到了数据分布的动态性，并探索了培训和测试之间可能的分布变化。通过这种方式，我们可以赋予骨干推荐系统更好的泛化能力。值得一提的是，DROS 是一种有效的模型无关学习框架，适用于一般推荐场景。理论分析表明，DROS 可以使骨干推荐器在未来的测试数据中获得稳健的性能。实证研究验证了该方法对 DROS 动态分布移位的有效性。代码在 https://github.com/yangzhengyi98/dros 是匿名开源的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Generic+Learning+Framework+for+Sequential+Recommendation+with+Distribution+Shifts)|0|
|[Single-shot Feature Selection for Multi-task Recommendations](https://doi.org/10.1145/3539618.3591767)|Yejing Wang, Zhaocheng Du, Xiangyu Zhao, Bo Chen, Huifeng Guo, Ruiming Tang, Zhenhua Dong|City University of Hong Kong, Hong Kong, Hong Kong; Huawei Noah's Ark Lab, Shen Zhen, China|Multi-task Recommender Systems (MTRSs) has become increasingly prevalent in a variety of real-world applications due to their exceptional training efficiency and recommendation quality. However, conventional MTRSs often input all relevant feature fields without distinguishing their contributions to different tasks, which can lead to confusion and a decline in performance. Existing feature selection methods may neglect task relations or require significant computation during model training in multi-task setting. To this end, this paper proposes a novel Single-shot Feature Selection framework for MTRSs, referred to as MultiSFS, which is capable of selecting feature fields for each task while considering task relations in a single-shot manner. Specifically, MultiSFS first efficiently obtains task-specific feature importance through a single forward-backward pass. Then, a data-task bipartite graph is constructed to learn field-level task relations. Subsequently, MultiSFS merges the feature importance according to task relations and selects feature fields for different tasks. To demonstrate the effectiveness and properties of MultiSFS, we integrate it with representative MTRS models and evaluate on three real-world datasets. The implementation code is available online to ease reproducibility.|多任务推荐系统由于其出色的培训效率和推荐质量，在各种实际应用中越来越普遍。然而，传统的中期审查系统往往输入所有相关特征领域，而不区分它们对不同任务的贡献，这可能导致混乱和业绩下降。现有的特征选择方法在多任务环境下进行模型训练时，可能会忽略任务间的关系，或者需要进行大量的计算。为此，本文提出了一种新的单镜头特征选择框架 MultiSFS，该框架能够在考虑单镜头任务关系的情况下为每个任务选择特征域。具体来说，MultiSFS 首先通过单个向前向后传递有效地获得特定于任务的特征重要性。然后，构造一个数据-任务二分图来学习场级任务关系。随后，MultiSFS 根据任务关系对特征重要度进行合并，并为不同的任务选择特征域。为了验证 MultiSFS 的有效性和特性，我们将其与具有代表性的中期业绩预测模型相结合，并在三个实际数据集上进行了评估。实现代码可在线获得，以减轻重复性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Single-shot+Feature+Selection+for+Multi-task+Recommendations)|0|
|[Fine-Grained Preference-Aware Personalized Federated POI Recommendation with Data Sparsity](https://doi.org/10.1145/3539618.3591688)|Xiao Zhang, Ziming Ye, Jianfeng Lu, Fuzhen Zhuang, Yanwei Zheng, Dongxiao Yu|Wuhan University of Science and Technology, Wuhan, China; Beihang University, Zhongguancun Laboratory, Beijing, China; Shandong University, Qingdao, China|With the raised privacy concerns and rigorous data regulations, federated learning has become a hot collaborative learning paradigm for the recommendation model without sharing the highly sensitive POI data. However, the time-sensitive, heterogeneous, and limited POI records seriously restrict the development of federated POI recommendation. To this end, in this paper, we design the fine-grained preference-aware personalized federated POI recommendation framework, namely PrefFedPOI, under extremely sparse historical trajectories to address the above challenges. In details, PrefFedPOI extracts the fine-grained preference of current time slot by combining historical recent preferences and periodic preferences within each local client. Due to the extreme lack of POI data in some time slots, a data amount aware selective strategy is designed for model parameters uploading. Moreover, a performance enhanced clustering mechanism with reinforcement learning is proposed to capture the preference relatedness among all clients to encourage the positive knowledge sharing. Furthermore, a clustering teacher network is designed for improving efficiency by clustering guidance. Extensive experiments are conducted on two diverse real-world datasets to demonstrate the effectiveness of proposed PrefFedPOI comparing with state-of-the-arts. In particular, personalized PrefFedPOI can achieve 7% accuracy improvement on average among data-sparsity clients.|随着隐私问题的提高和严格的数据监管，联合学习已经成为推荐模型的一个热门合作学习，而不需要共享高度敏感的 POI 数据。然而，时间敏感、异构、有限的 POI 记录严重制约了联邦 POI 推荐的发展。为此，本文在极其稀疏的历史轨迹下，设计了细粒度偏好感知的个性化联邦 POI 推荐框架 PrefFedPOI，以解决上述挑战。具体来说，PrefFedPOI 通过组合每个本地客户机内的历史最近首选项和周期首选项来提取当前时隙的细粒度首选项。针对某些时隙中 POI 数据极度缺乏的情况，设计了一种数据量感知的模型参数选择策略。此外，我们亦建议采用一个有强化学习的表现增强聚类机制，以捕捉所有客户之间的偏好关系，从而鼓励他们积极分享知识。为了提高聚类指导的效率，设计了一个聚类教师网络。在两个不同的真实世界数据集上进行了广泛的实验，以验证所提出的 PrefFedPOI 与最新技术相比的有效性。特别是，在数据稀疏的客户端中，个性化 PrefFedPOI 可以平均提高7% 的准确性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fine-Grained+Preference-Aware+Personalized+Federated+POI+Recommendation+with+Data+Sparsity)|0|
|[Model-Agnostic Decentralized Collaborative Learning for On-Device POI Recommendation](https://doi.org/10.1145/3539618.3591733)|Jing Long, Tong Chen, Quoc Viet Hung Nguyen, Guandong Xu, Kai Zheng, Hongzhi Yin||As an indispensable personalized service in Location-based Social Networks (LBSNs), the next Point-of-Interest (POI) recommendation aims to help people discover attractive and interesting places. Currently, most POI recommenders are based on the conventional centralized paradigm that heavily relies on the cloud to train the recommendation models with large volumes of collected users' sensitive check-in data. Although a few recent works have explored on-device frameworks for resilient and privacy-preserving POI recommendations, they invariably hold the assumption of model homogeneity for parameters/gradients aggregation and collaboration. However, users' mobile devices in the real world have various hardware configurations (e.g., compute resources), leading to heterogeneous on-device models with different architectures and sizes. In light of this, We propose a novel on-device POI recommendation framework, namely Model-Agnostic Collaborative learning for on-device POI recommendation (MAC), allowing users to customize their own model structures (e.g., dimension \& number of hidden layers). To counteract the sparsity of on-device user data, we propose to pre-select neighbors for collaboration based on physical distances, category-level preferences, and social networks. To assimilate knowledge from the above-selected neighbors in an efficient and secure way, we adopt the knowledge distillation framework with mutual information maximization. Instead of sharing sensitive models/gradients, clients in MAC only share their soft decisions on a preloaded reference dataset. To filter out low-quality neighbors, we propose two sampling strategies, performance-triggered sampling and similarity-based sampling, to speed up the training process and obtain optimal recommenders. In addition, we design two novel approaches to generate more effective reference datasets while protecting users' privacy.|作为基于位置的社交网络(LBSNs)中不可或缺的个性化服务，下一个兴趣点(POI)推荐旨在帮助人们发现有吸引力和有趣的地方。目前，大多数 POI 推荐系统都是基于传统的集中式模式，这种模式严重依赖于云来训练推荐模型，并收集了大量用户的敏感签入数据。虽然最近的一些工作已经探索了弹性和保护隐私的 POI 建议在设备上的框架，他们总是持有参数/梯度聚合和协作的模型同质性的假设。然而，在现实世界中，用户的移动设备有各种各样的硬件配置(例如，计算资源) ，导致了具有不同体系结构和大小的异构设备上模型。鉴于此，我们提出了一个新的设备上 POI 推荐框架，即设备上 POI 推荐的模型不可知合作学习(model-Agnotic) ，允许用户定制自己的模型结构(例如，尺寸和隐藏层数)。为了弥补设备上用户数据的稀缺性，我们建议根据物理距离、类别级别偏好和社交网络预先选择合作的邻居。为了有效、安全地吸收上述邻居的知识，我们采用了具有互信息最大化的知识提取框架。MAC 中的客户端不共享敏感的模型/梯度，而只在预加载的参考数据集上共享他们的软决策。为了过滤掉低质量的邻居，我们提出了两种抽样策略: 性能触发抽样和基于相似度的抽样，以加快训练过程，并获得最优的推荐。此外，我们设计了两种新的方法来生成更有效的参考数据集，同时保护用户的隐私。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Model-Agnostic+Decentralized+Collaborative+Learning+for+On-Device+POI+Recommendation)|0|
|[Multi-view Hypergraph Contrastive Policy Learning for Conversational Recommendation](https://doi.org/10.1145/3539618.3591737)|Sen Zhao, Wei Wei, XianLing Mao, Shuai Zhu, Minghui Yang, Zujie Wen, Dangyang Chen, Feida Zhu||Conversational recommendation systems (CRS) aim to interactively acquire user preferences and accordingly recommend items to users. Accurately learning the dynamic user preferences is of crucial importance for CRS. Previous works learn the user preferences with pairwise relations from the interactive conversation and item knowledge, while largely ignoring the fact that factors for a relationship in CRS are multiplex. Specifically, the user likes/dislikes the items that satisfy some attributes (Like/Dislike view). Moreover social influence is another important factor that affects user preference towards the item (Social view), while is largely ignored by previous works in CRS. The user preferences from these three views are inherently different but also correlated as a whole. The user preferences from the same views should be more similar than that from different views. The user preferences from Like View should be similar to Social View while different from Dislike View. To this end, we propose a novel model, namely Multi-view Hypergraph Contrastive Policy Learning (MHCPL). Specifically, MHCPL timely chooses useful social information according to the interactive history and builds a dynamic hypergraph with three types of multiplex relations from different views. The multiplex relations in each view are successively connected according to their generation order.|会话推荐系统(CRS)旨在交互式地获取用户偏好，从而向用户推荐项目。准确地了解动态用户偏好对 CRS 至关重要。以往的研究主要是从交互式会话和项目知识中了解成对关系的用户偏好，而忽略了 CRS 中影响关系的因素是多重的这一事实。具体来说，用户喜欢/不喜欢满足某些属性的项(Like/Dislike 视图)。此外，社会影响力是影响用户对商品偏好的另一个重要因素(社会视图) ，而以往的研究大多忽略了社会影响力。来自这三个视图的用户首选项在本质上是不同的，但作为一个整体也是相关的。来自相同视图的用户首选项应该比来自不同视图的用户首选项更加相似。来自 Like View 的用户首选项应该类似于 Social View，而不同于 Dislike View。为此，我们提出了一个新的模型，即多视图超图对比策略学习(MHCPL)。具体来说，MHCPL 根据交互历史及时选择有用的社会信息，从不同角度构建了三类多元关系的动态超图。每个视图中的多路复用关系根据它们的生成顺序依次连接。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-view+Hypergraph+Contrastive+Policy+Learning+for+Conversational+Recommendation)|0|
|[Learnable Pillar-based Re-ranking for Image-Text Retrieval](https://doi.org/10.1145/3539618.3591712)|Leigang Qu, Meng Liu, Wenjie Wang, Zhedong Zheng, Liqiang Nie, TatSeng Chua||Image-text retrieval aims to bridge the modality gap and retrieve cross-modal content based on semantic similarities. Prior work usually focuses on the pairwise relations (i.e., whether a data sample matches another) but ignores the higher-order neighbor relations (i.e., a matching structure among multiple data samples). Re-ranking, a popular post-processing practice, has revealed the superiority of capturing neighbor relations in single-modality retrieval tasks. However, it is ineffective to directly extend existing re-ranking algorithms to image-text retrieval. In this paper, we analyze the reason from four perspectives, i.e., generalization, flexibility, sparsity, and asymmetry, and propose a novel learnable pillar-based re-ranking paradigm. Concretely, we first select top-ranked intra- and inter-modal neighbors as pillars, and then reconstruct data samples with the neighbor relations between them and the pillars. In this way, each sample can be mapped into a multimodal pillar space only using similarities, ensuring generalization. After that, we design a neighbor-aware graph reasoning module to flexibly exploit the relations and excavate the sparse positive items within a neighborhood. We also present a structure alignment constraint to promote cross-modal collaboration and align the asymmetric modalities. On top of various base backbones, we carry out extensive experiments on two benchmark datasets, i.e., Flickr30K and MS-COCO, demonstrating the effectiveness, superiority, generalization, and transferability of our proposed re-ranking paradigm.|图像-文本检索旨在弥合情态差异，基于语义相似性检索跨情态内容。先前的工作通常侧重于成对关系(例如，一个数据样本是否匹配另一个) ，但忽略了高阶邻居关系(例如，多个数据样本之间的匹配结构)。重排序是一种流行的后处理实践，它揭示了在单模态检索任务中捕获邻居关系的优越性。然而，将现有的重新排序算法直接推广到图像文本检索是无效的。本文从概括性、灵活性、稀疏性和不对称性四个方面分析了排序问题产生的原因，并提出了一种新的可学习的基于支柱的排序范式。具体来说，我们首先选择排名最高的模态内邻居和模态间邻居作为支柱，然后利用它们与支柱之间的邻居关系重构数据样本。通过这种方式，每个样本只需使用相似性即可映射到多模态支柱空间，从而确保泛化。然后设计邻域感知图推理模块，灵活地利用邻域间的关系，挖掘邻域内的稀疏正项。我们还提出了一个结构调整约束，以促进跨模式协作和调整不对称的模式。在各种基础骨干之上，我们对 Flickr30K 和 MS-COCO 这两个基准数据集进行了广泛的实验，证明了我们提出的重新排序范式的有效性、优越性、通用性和可转移性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learnable+Pillar-based+Re-ranking+for+Image-Text+Retrieval)|0|
|[When Search Meets Recommendation: Learning Disentangled Search Representation for Recommendation](https://doi.org/10.1145/3539618.3591786)|Zihua Si, Zhongxiang Sun, Xiao Zhang, Jun Xu, Xiaoxue Zang, Yang Song, Kun Gai, JiRong Wen||Modern online service providers such as online shopping platforms often provide both search and recommendation (S&R) services to meet different user needs. Rarely has there been any effective means of incorporating user behavior data from both S&R services. Most existing approaches either simply treat S&R behaviors separately, or jointly optimize them by aggregating data from both services, ignoring the fact that user intents in S&R can be distinctively different. In our paper, we propose a Search-Enhanced framework for the Sequential Recommendation (SESRec) that leverages users' search interests for recommendation, by disentangling similar and dissimilar representations within S&R behaviors. Specifically, SESRec first aligns query and item embeddings based on users' query-item interactions for the computations of their similarities. Two transformer encoders are used to learn the contextual representations of S&R behaviors independently. Then a contrastive learning task is designed to supervise the disentanglement of similar and dissimilar representations from behavior sequences of S&R. Finally, we extract user interests by the attention mechanism from three perspectives, i.e., the contextual representations, the two separated behaviors containing similar and dissimilar interests. Extensive experiments on both industrial and public datasets demonstrate that SESRec consistently outperforms state-of-the-art models. Empirical studies further validate that SESRec successfully disentangle similar and dissimilar user interests from their S&R behaviors.|现代在线服务提供商，如在线购物平台，经常同时提供搜索和推荐(S & R)服务，以满足不同的用户需求。很少有任何有效的方法来整合来自 S & R 服务的用户行为数据。大多数现有的方法要么单独处理 S & R 行为，要么通过聚合来自两个服务的数据共同优化它们，忽略了 S & R 中的用户意图可能截然不同的事实。在我们的论文中，我们提出了一个搜索增强的序列推荐框架(SESRec) ，它利用用户的搜索兴趣进行推荐，通过在 S & R 行为中分离相似和不相似的表示。具体来说，SESRec 首先根据用户的查询-项交互对查询和项嵌入进行对齐，以计算它们的相似性。使用两个变压器编码器分别学习 S & R 行为的上下文表示。然后设计了一个对比学习任务来监督 S & R 行为序列中相似和不相似表征的分离。最后，通过注意机制从三个方面提取用户的兴趣，即语境表征，两个分离的具有相似兴趣和不同兴趣的行为。在工业和公共数据集上的大量实验表明，SESRec 始终优于最先进的模型。实证研究进一步验证了 SESRec 成功地将相似和不同的用户兴趣从他们的 S & R 行为中分离出来。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=When+Search+Meets+Recommendation:+Learning+Disentangled+Search+Representation+for+Recommendation)|0|
|[Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?](https://doi.org/10.1145/3539618.3591703)|Shuai Wang, Harrisen Scells, Bevan Koopman, Guido Zuccon||Systematic reviews are comprehensive reviews of the literature for a highly focused research question. These reviews are often treated as the highest form of evidence in evidence-based medicine, and are the key strategy to answer research questions in the medical field. To create a high-quality systematic review, complex Boolean queries are often constructed to retrieve studies for the review topic. However, it often takes a long time for systematic review researchers to construct a high quality systematic review Boolean query, and often the resulting queries are far from effective. Poor queries may lead to biased or invalid reviews, because they missed to retrieve key evidence, or to extensive increase in review costs, because they retrieved too many irrelevant studies. Recent advances in Transformer-based generative models have shown great potential to effectively follow instructions from users and generate answers based on the instructions being made. In this paper, we investigate the effectiveness of the latest of such models, ChatGPT, in generating effective Boolean queries for systematic review literature search. Through a number of extensive experiments on standard test collections for the task, we find that ChatGPT is capable of generating queries that lead to high search precision, although trading-off this for recall. Overall, our study demonstrates the potential of ChatGPT in generating effective Boolean queries for systematic review literature search. The ability of ChatGPT to follow complex instructions and generate queries with high precision makes it a valuable tool for researchers conducting systematic reviews, particularly for rapid reviews where time is a constraint and often trading-off higher precision for lower recall is acceptable.|系统评价是对一个高度关注的研究问题的文献的综合评价。这些评论往往被视为循证医学中最高形式的证据，是回答医学领域研究问题的关键策略。为了创建一个高质量的系统综述，复杂的布尔查询通常被构建来检索复习主题的研究。然而，对于系统综述研究人员来说，构建一个高质量的系统综述布尔查询通常需要很长的时间，而且往往得到的结果远远不够有效。差的查询可能导致有偏见或无效的评论，因为他们错过了检索关键证据，或广泛增加审查成本，因为他们检索了太多不相关的研究。最近在变压器为基础的生成模型显示了巨大的潜力，有效地遵循指令从用户和生成答案的基础上，正在作出的指令。在本文中，我们研究了最新的这类模型 ChatGPT 在为系统综述文献检索生成有效的布尔查询方面的有效性。通过对任务的标准测试集进行大量的实验，我们发现 ChatGPT 能够生成导致高搜索精度的查询，尽管在这方面取舍于召回。总的来说，我们的研究展示了 chatgPT 在为系统综述文献检索生成有效的布尔查询方面的潜力。ChatGPT 能够遵循复杂的指令并以高精度生成查询，这使得它成为研究人员进行系统评价的有价值的工具，特别是对于时间受到限制的快速评价，往往以较高的精度换取较低的回忆是可以接受的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+ChatGPT+Write+a+Good+Boolean+Query+for+Systematic+Review+Literature+Search?)|0|
|[Candidate-aware Graph Contrastive Learning for Recommendation](https://doi.org/10.1145/3539618.3591647)|Wei He, Guohao Sun, Jinhu Lu, Xiu Susie Fang|Donghua University, Shanghai, China|Recently, Graph Neural Networks (GNNs) have become a mainstream recommender system method, where it captures high-order collaborative signals between nodes by performing convolution operations on the user-item interaction graph to predict user preferences for different items. However, in real scenarios, the user-item interaction graph is extremely sparse, which means numerous users only interact with a small number of items, resulting in the inability of GNN in learning high-quality node embeddings. To alleviate this problem, the Graph Contrastive Learning (GCL)-based recommender system method is proposed. GCL improves embedding quality by maximizing the similarity of the positive pair and minimizing the similarity of the negative pair. However, most GCL-based methods use heuristic data augmentation methods, i.e., random node/edge drop and attribute masking, to construct contrastive pairs, resulting in the loss of important information. To solve the problems in GCL-based methods, we propose a novel method, Candidate-aware Graph Contrastive Learning for Recommendation, called CGCL. In CGCL, we explore the relationship between the user and the candidate item in the embedding at different layers and use similar semantic embeddings to construct contrastive pairs. By our proposed CGCL, we construct structural neighbor contrastive learning objects, candidate contrastive learning objects, and candidate structural neighbor contrastive learning objects to obtain high-quality node embeddings. To validate the proposed model, we conducted extensive experiments on three publicly available datasets. Compared with various state-of-the-art DNN-, GNN- and GCL-based methods, our proposed CGCL achieved significant improvements in all indicators.|最近，图形神经网络(GNN)已经成为一种主流的推荐系统方法，它通过在用户-项目交互图上执行卷积操作来预测用户对不同项目的偏好，从而捕获节点之间的高阶协作信号。然而，在实际场景中，用户-项目交互图是非常稀疏的，这意味着大量的用户只与少量的项目交互，导致 GNN 无法学习高质量的节点嵌入。为了解决这个问题，我们提出了基于图形对比学习(gCL)的推荐系统学习方法。GCL 通过最大化正对的相似度和最小化负对的相似度来提高嵌入质量。然而，大多数基于 GCL 的方法使用启发式数据增强方法，即随机节点/边降和属性掩蔽来构造对比对，导致重要信息的丢失。为了解决基于 GCL 的方法中存在的问题，我们提出了一种新的方法，候选感知的图形对比推荐学习，称为 CGCL。在 CGCL 中，我们在不同层次的嵌入中探索用户与候选项之间的关系，并利用相似语义嵌入构造对比对。通过我们提出的 CGCL，我们构造了结构化邻域对比学习对象、候选对比学习对象和候选结构化邻域对比学习对象，以获得高质量的节点嵌入。为了验证所提出的模型，我们在三个公开的数据集上进行了广泛的实验。与基于 DNN、 GNN 和 GCL 的各种最新方法相比，我们提出的 CGCL 在所有指标上都取得了显著的改善。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Candidate-aware+Graph+Contrastive+Learning+for+Recommendation)|0|
|[Attention Mixtures for Time-Aware Sequential Recommendation](https://doi.org/10.1145/3539618.3591951)|VietAnh Tran, Guillaume SalhaGalvan, Bruno Sguerra, Romain Hennequin||Transformers emerged as powerful methods for sequential recommendation. However, existing architectures often overlook the complex dependencies between user preferences and the temporal context. In this short paper, we introduce MOJITO, an improved Transformer sequential recommender system that addresses this limitation. MOJITO leverages Gaussian mixtures of attention-based temporal context and item embedding representations for sequential modeling. Such an approach permits to accurately predict which items should be recommended next to users depending on past actions and the temporal context. We demonstrate the relevance of our approach, by empirically outperforming existing Transformers for sequential recommendation on several real-world datasets.|变压器作为强大的顺序推荐方法出现了。然而，现有的体系结构往往忽略了用户首选项和时态上下文之间的复杂依赖关系。在这篇简短的文章中，我们介绍了 MOJITO，一个改进的 formers顺序推荐系统，它解决了这个问题。MOJITO 利用基于注意的时间上下文和项目嵌入表示的高斯混合序列建模。这种方法允许根据过去的行为和时间上下文准确地预测哪些项目应该被推荐给用户。我们证明了我们的方法的相关性，通过经验优于现有的变形金刚顺序推荐几个真实世界的数据集。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Attention+Mixtures+for+Time-Aware+Sequential+Recommendation)|0|
|[Augmenting Passage Representations with Query Generation for Enhanced Cross-Lingual Dense Retrieval](https://doi.org/10.1145/3539618.3591952)|Shengyao Zhuang, Linjun Shou, Guido Zuccon||Effective cross-lingual dense retrieval methods that rely on multilingual pre-trained language models (PLMs) need to be trained to encompass both the relevance matching task and the cross-language alignment task. However, cross-lingual data for training is often scarcely available. In this paper, rather than using more cross-lingual data for training, we propose to use cross-lingual query generation to augment passage representations with queries in languages other than the original passage language. These augmented representations are used at inference time so that the representation can encode more information across the different target languages. Training of a cross-lingual query generator does not require additional training data to that used for the dense retriever. The query generator training is also effective because the pre-training task for the generator (T5 text-to-text training) is very similar to the fine-tuning task (generation of a query). The use of the generator does not increase query latency at inference and can be combined with any cross-lingual dense retrieval method. Results from experiments on a benchmark cross-lingual information retrieval dataset show that our approach can improve the effectiveness of existing cross-lingual dense retrieval methods. Implementation of our methods, along with all generated query files are made publicly available at https://github.com/ielab/xQG4xDR.|有效的跨语言密集检索方法依赖于多语言预训练语言模型(PLM)需要培训，以涵盖相关性匹配任务和跨语言对齐任务。然而，用于培训的跨语言数据往往很少。在本文中，我们不使用更多的跨语言数据进行训练，而是提出使用跨语言查询生成来增强非原始语言中的查询的段落表示。这些增强表示在推理时使用，以便表示可以跨不同的目标语言编码更多的信息。跨语言查询生成器的训练不需要比密集检索器所使用的训练数据更多的训练数据。查询生成器训练也很有效，因为生成器的预训练任务(T5文本到文本训练)与微调任务(生成查询)非常相似。该生成器的使用不会增加推断时的查询延迟，并且可以与任何跨语言密集检索方法结合使用。对基准跨语言信息检索数据集的实验结果表明，我们的方法可以提高现有跨语言密集检索方法的有效性。我们方法的实现，以及所有生成的查询文件都可以在 https://github.com/ielab/xqg4xdr 上公开获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Augmenting+Passage+Representations+with+Query+Generation+for+Enhanced+Cross-Lingual+Dense+Retrieval)|0|
|[CEC: Towards Learning Global Optimized Recommendation through Causality Enhanced Conversion Model](https://doi.org/10.1145/3539618.3591962)|Ran Le, Guoqing Jiang, Xiufeng Shu, Ruidong Han, Qianzhong Li, Yacheng Li, Xiang Li, Wei Lin|Unaffiliated, Beijing, China; Meituan, Beijing, China|Most e-commerce platforms consist of multiple entries (e.g., recommendation, search, shopping cart and etc.) for users to purchase their liked items. Among the research on the recommendation entry, most of them focus on improving the conversion volumes merely in the recommendation entry. However, such way could not ensure an increase in the global conversion volumes of the e-commerce platform. To achieve this goal by optimizing the recommendation entry only, in this paper, we focus on modeling the causality between the recommendation-entry-impression and the conversion by proposing the two-stage Causality Enhanced Conversion (CEC) model. In the first stage, we define the recommendation-entry-impression as treatment, then we estimate the conversion rate conditioned on the inclusion or exclusion of treatment respectively and calculate the corresponding individual treatment effect (ITE). In the second stage, we propose a propensity-normalization (PN) based method to transform the learned ITE to a weight term for instance weighting in the conversion loss. Extensive offline and online experiments on a large-scale food e-commerce scenario demonstrate that the CEC model could focus more on those conversed instances that can improve the global conversion volumes of the platform.|大多数电子商务平台由多个条目组成(例如，推荐、搜索、购物车等) ，用户可以购买他们喜欢的商品。在对推荐条目的研究中，大多数研究仅仅关注于提高推荐条目的转化率。然而，这种方式并不能保证增加电子商务平台的全球转换量。为了通过优化推荐条目来实现这一目标，本文提出了两阶段因果关系增强转换(CEC)模型，着重对推荐条目印象与转换之间的因果关系进行建模。在第一阶段，我们将推荐入口印象定义为治疗，然后分别估计以包含或排除治疗为条件的转化率，并计算相应的个体治疗效果(ITE)。在第二阶段，我们提出了一个基于倾向标准化(PN)的方法，将学习到的 ITE 转换为一个权重项，例如在转换损失中的权重。大规模食品电子商务情景的大量离线和在线实验表明，CEC 模型可以更多地关注那些可以提高平台全球转换量的转换实例。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CEC:+Towards+Learning+Global+Optimized+Recommendation+through+Causality+Enhanced+Conversion+Model)|0|
|[ConQueR: Contextualized Query Reduction using Search Logs](https://doi.org/10.1145/3539618.3591966)|Hyeyoung Kim, Minjin Choi, Sunkyung Lee, Eunseong Choi, YoungIn Song, Jongwuk Lee||Query reformulation is a key mechanism to alleviate the linguistic chasm of query in ad-hoc retrieval. Among various solutions, query reduction effectively removes extraneous terms and specifies concise user intent from long queries. However, it is challenging to capture hidden and diverse user intent. This paper proposes Contextualized Query Reduction (ConQueR) using a pre-trained language model (PLM). Specifically, it reduces verbose queries with two different views: core term extraction and sub-query selection. One extracts core terms from an original query at the term level, and the other determines whether a sub-query is a suitable reduction for the original query at the sequence level. Since they operate at different levels of granularity and complement each other, they are finally aggregated in an ensemble manner. We evaluate the reduction quality of ConQueR on real-world search logs collected from a commercial web search engine. It achieves up to 8.45% gains in exact match scores over the best competing model.|查询重构是自组织检索中缓解查询语言鸿沟的关键机制。在各种解决方案中，查询缩减有效地去除了多余的术语，并从长查询中指定了简洁的用户意图。然而，捕捉隐藏的和多样化的用户意图是一个挑战。提出了一种基于预训练语言模型(PLM)的上下文查询约简(ConQueR)算法。具体来说，它使用两种不同的视图来减少冗长查询: 核心术语提取和子查询选择。一个在术语级别从原始查询中提取核心术语，另一个在序列级别确定子查询对于原始查询是否是合适的简化。由于它们在不同的粒度级别上运行并相互补充，因此最终以集成的方式进行聚合。我们对从商业网络搜索引擎收集的真实世界搜索日志中的 ConQueR 的还原质量进行评估。与最佳竞争模型相比，它在精确匹配得分上获得了高达8.45% 的增益。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ConQueR:+Contextualized+Query+Reduction+using+Search+Logs)|0|
|[Explain Like I am BM25: Interpreting a Dense Model's Ranked-List with a Sparse Approximation](https://doi.org/10.1145/3539618.3591982)|Michael Llordes, Debasis Ganguly, Sumit Bhatia, Chirag Agarwal||Neural retrieval models (NRMs) have been shown to outperform their statistical counterparts owing to their ability to capture semantic meaning via dense document representations. These models, however, suffer from poor interpretability as they do not rely on explicit term matching. As a form of local per-query explanations, we introduce the notion of equivalent queries that are generated by maximizing the similarity between the NRM's results and the result set of a sparse retrieval system with the equivalent query. We then compare this approach with existing methods such as RM3-based query expansion and contrast differences in retrieval effectiveness and in the terms generated by each approach.|神经检索模型(NRM)由于能够通过密集的文档表示来捕获语义信息，因此其性能优于统计检索模型。然而，这些模型的可解释性很差，因为它们不依赖于明确的术语匹配。作为局部查询解释的一种形式，我们引入了等价查询的概念，这些等价查询是通过最大化 NRM 的结果与具有等价查询的稀疏检索系统的结果集之间的相似性而生成的。然后，我们比较了这种方法与现有的方法，如基于 RM3的查询扩展和对比差异的检索效果和术语生成的每种方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explain+Like+I+am+BM25:+Interpreting+a+Dense+Model's+Ranked-List+with+a+Sparse+Approximation)|0|
|[Graph Collaborative Signals Denoising and Augmentation for Recommendation](https://doi.org/10.1145/3539618.3591994)|Ziwei Fan, Ke Xu, Zhang Dong, Hao Peng, Jiawei Zhang, Philip S. Yu||Graph collaborative filtering (GCF) is a popular technique for capturing high-order collaborative signals in recommendation systems. However, GCF's bipartite adjacency matrix, which defines the neighbors being aggregated based on user-item interactions, can be noisy for users/items with abundant interactions and insufficient for users/items with scarce interactions. Additionally, the adjacency matrix ignores user-user and item-item correlations, which can limit the scope of beneficial neighbors being aggregated. In this work, we propose a new graph adjacency matrix that incorporates user-user and item-item correlations, as well as a properly designed user-item interaction matrix that balances the number of interactions across all users. To achieve this, we pre-train a graph-based recommendation method to obtain users/items embeddings, and then enhance the user-item interaction matrix via top-K sampling. We also augment the symmetric user-user and item-item correlation components to the adjacency matrix. Our experiments demonstrate that the enhanced user-item interaction matrix with improved neighbors and lower density leads to significant benefits in graph-based recommendation. Moreover, we show that the inclusion of user-user and item-item correlations can improve recommendations for users with both abundant and insufficient interactions. The code is in \url{https://github.com/zfan20/GraphDA}.|图形协同过滤(gCF)是在推荐系统中捕获高阶协作信号的一种流行技术。然而，绿色气候基金的二分邻接矩阵定义了基于用户-项目交互聚合的邻居，这对于交互丰富的用户/项目来说可能是嘈杂的，对于交互稀少的用户/项目来说则是不足的。此外，邻接矩阵忽略了用户-用户和项目-项目之间的相关性，这可能会限制有益邻居被聚合的范围。在这项工作中，我们提出了一个新的图形邻接矩阵，包括用户-用户和项目-项目的相关性，以及一个适当设计的用户-项目交互矩阵，平衡所有用户之间的交互数量。为了实现这一目标，我们预先训练了一种基于图的推荐方法来获得用户/项目的嵌入，然后通过 top-K 抽样来增强用户-项目的交互矩阵。我们还增加了对称的用户-用户和项目-项目相关组件的邻接矩阵。我们的实验表明，增强的用户项目交互矩阵与改进的邻居和较低的密度导致显着的好处，在图的推荐。此外，我们表明，包含用户-用户和项目-项目的相关性可以改善对交互丰富和不充分的用户的推荐。代码在 url { https://github.com/zfan20/graphda }中。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Collaborative+Signals+Denoising+and+Augmentation+for+Recommendation)|0|
|[Improving Conversational Passage Re-ranking with View Ensemble](https://doi.org/10.1145/3539618.3592002)|JiaHuei Ju, ShengChieh Lin, MingFeng Tsai, ChuanJu Wang||This paper presents ConvRerank, a conversational passage re-ranker that employs a newly developed pseudo-labeling approach. Our proposed view-ensemble method enhances the quality of pseudo-labeled data, thus improving the fine-tuning of ConvRerank. Our experimental evaluation on benchmark datasets shows that combining ConvRerank with a conversational dense retriever in a cascaded manner achieves a good balance between effectiveness and efficiency. Compared to baseline methods, our cascaded pipeline demonstrates lower latency and higher top-ranking effectiveness. Furthermore, the in-depth analysis confirms the potential of our approach to improving the effectiveness of conversational search.|本文提出了一种会话文章重新排序算法，该算法采用了一种新的伪标记方法。我们提出的视图集成方法提高了伪标记数据的质量，从而改善了逆重排的微调。我们对基准数据集的实验评估表明，使用级联的方式将 ConverRank 与会话密集检索器相结合，实现了有效性和效率之间的良好平衡。与基线方法相比，我们的级联流水线显示出更低的延迟和更高的排名有效性。此外，深入的分析证实了我们的方法在提高会话搜索的有效性方面的潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Conversational+Passage+Re-ranking+with+View+Ensemble)|0|
|[Inference at Scale: Significance Testing for Large Search and Recommendation Experiments](https://doi.org/10.1145/3539618.3592004)|Ngozi Ihemelandu, Michael D. Ekstrand||A number of information retrieval studies have been done to assess which statistical techniques are appropriate for comparing systems. However, these studies are focused on TREC-style experiments, which typically have fewer than 100 topics. There is no similar line of work for large search and recommendation experiments; such studies typically have thousands of topics or users and much sparser relevance judgements, so it is not clear if recommendations for analyzing traditional TREC experiments apply to these settings. In this paper, we empirically study the behavior of significance tests with large search and recommendation evaluation data. Our results show that the Wilcoxon and Sign tests show significantly higher Type-1 error rates for large sample sizes than the bootstrap, randomization and t-tests, which were more consistent with the expected error rate. While the statistical tests displayed differences in their power for smaller sample sizes, they showed no difference in their power for large sample sizes. We recommend the sign and Wilcoxon tests should not be used to analyze large scale evaluation results. Our result demonstrate that with Top-N recommendation and large search evaluation data, most tests would have a 100% chance of finding statistically significant results. Therefore, the effect size should be used to determine practical or scientific significance.|已经进行了大量的信息检索研究，以评估哪些统计技术适合于比较系统。然而，这些研究主要集中在 TREC 风格的实验上，这些实验通常只有不到100个主题。大型搜索和推荐实验没有类似的工作; 这类研究通常有成千上万的主题或用户，相关性判断更少，因此不清楚分析传统 TREC 实验的推荐是否适用于这些设置。本文利用大量的搜索和推荐评价数据，对显著性检验的行为进行了实证研究。我们的研究结果表明，Wilcoxon 和 Sign 检验显示，对于大样本量，显著高于自举、随机和 t 检验的1型错误率，这与预期错误率更为一致。虽然统计检验显示，在较小的样本规模的权力差异，他们没有显示出他们的权力在大样本规模。我们建议不要使用符号和 Wilcoxon 试验来分析大规模的评估结果。我们的结果表明，有了 Top-N 推荐和大量的搜索评估数据，大多数测试将有100% 的机会找到统计学上显著的结果。因此，应该用效应大小来确定实际意义或科学意义。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Inference+at+Scale:+Significance+Testing+for+Large+Search+and+Recommendation+Experiments)|0|
|[Improving News Recommendation via Bottlenecked Multi-task Pre-training](https://doi.org/10.1145/3539618.3592003)|Xiongfeng Xiao, Qing Li, Songlin Liu, Kun Zhou|Renmin University of China, Beijing, China; Aegis Information Technology Ltd. Co., Beijing, China; Peking University, Beijing, China|Recent years have witnessed the boom of deep neural networks in online news recommendation service. As news articles mainly consist of textual content, pre-trained language models~(PLMs) (e.g. BERT) have been widely adopted as the backbone to encode them into news embeddings, which would be utilized to generate the user representations or perform the semantic matching. However, existing PLMs are mostly pre-trained on large-scale general corpus, and have not been specially adapted for capturing the rich information within news articles. Therefore, their produced news embeddings may be not informative enough to represent the news content or characterize the relations among news. To solve it, we propose a bottlenecked multi-task pre-training approach, which relies on an information-bottleneck encoder-decoder architecture to compress the useful semantic information into the news embedding. Concretely, we design three pre-training tasks, to enforce the news embedding to recover the news contents of itself, its frequently oc-occurring neighbours, and the news with similar topics. We conduct experiments on the MIND dataset and show that our approach can outperform competitive pre-training methods.|近年来，深度神经网络在在线新闻推荐服务中的应用日益广泛。由于新闻文章主要由文本内容构成，预训练语言模型 ~ (PLM)(例如 BERT)已被广泛用作新闻嵌入的骨干，用于生成用户表示或进行语义匹配。然而，现有的 PLM 大多是在大规模的一般语料库中进行预先培训的，并没有经过特别的调整来捕捉新闻文章中的丰富信息。因此，他们制作的新闻嵌入可能没有足够的信息来代表新闻内容或表征新闻之间的关系。为了解决这一问题，我们提出了一种瓶颈多任务预训练方法，该方法依赖于信息瓶颈编解码器结构，将有用的语义信息压缩为新闻嵌入。具体来说，我们设计了三个预训练任务，通过加强新闻嵌入来恢复新闻本身、其频繁出现的邻居以及相似话题的新闻内容。我们在 MIND 数据集上进行了实验，结果表明我们的方法可以胜过竞争性的预训练方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+News+Recommendation+via+Bottlenecked+Multi-task+Pre-training)|0|
|[LADER: Log-Augmented DEnse Retrieval for Biomedical Literature Search](https://doi.org/10.1145/3539618.3592005)|Qiao Jin, Andrew Shin, Zhiyong Lu||Queries with similar information needs tend to have similar document clicks, especially in biomedical literature search engines where queries are generally short and top documents account for most of the total clicks. Motivated by this, we present a novel architecture for biomedical literature search, namely Log-Augmented DEnse Retrieval (LADER), which is a simple plug-in module that augments a dense retriever with the click logs retrieved from similar training queries. Specifically, LADER finds both similar documents and queries to the given query by a dense retriever. Then, LADER scores relevant (clicked) documents of similar queries weighted by their similarity to the input query. The final document scores by LADER are the average of (1) the document similarity scores from the dense retriever and (2) the aggregated document scores from the click logs of similar queries. Despite its simplicity, LADER achieves new state-of-the-art (SOTA) performance on TripClick, a recently released benchmark for biomedical literature retrieval. On the frequent (HEAD) queries, LADER largely outperforms the best retrieval model by 39% relative NDCG@10 (0.338 v.s. 0.243). LADER also achieves better performance on the less frequent (TORSO) queries with 11% relative NDCG@10 improvement over the previous SOTA (0.303 v.s. 0.272). On the rare (TAIL) queries where similar queries are scarce, LADER still compares favorably to the previous SOTA method (NDCG@10: 0.310 v.s. 0.295). On all queries, LADER can improve the performance of a dense retriever by 24%-37% relative NDCG@10 while not requiring additional training, and further performance improvement is expected from more logs. Our regression analysis has shown that queries that are more frequent, have higher entropy of query similarity and lower entropy of document similarity, tend to benefit more from log augmentation.|具有相似信息需求的查询往往具有相似的文档点击率，特别是在生物医学文献搜索引擎中，查询通常较短，顶级文档占总点击率的大部分。受此启发，我们提出了一种用于生物医学文献检索的新型架构，即 Log-AugmentedDEnse Retrieval (LADER) ，它是一个简单的插件模块，通过从类似的训练查询中检索到的点击日志来增强稠密检索器。具体来说，LADER 通过密集检索器找到与给定查询相似的文档和查询。然后，LADER 根据相似查询与输入查询的相似性对相关(单击)文档进行加权。LADER 的最终文档得分是(1)密集检索器的文档相似性得分和(2)相似查询的点击日志的聚合文档得分的平均值。尽管其简单，LADER 在 TripClick 上实现了新的最先进的(SOTA)性能，TripClick 是最近发布的生物医学文献检索基准。在频繁查询(HEAD)中，LADER 的性能比最佳检索模型高出39% ，相对于 NDCG@10(0.338 vs。0.243)。LADER 在不太频繁的(TORSO)查询上也取得了更好的性能，相对于之前的 SOTA (0.303对0.272) ，相对于 NDCG@10有11% 的改进。在相似查询稀少的罕见(TAIL)查询中，LADER 仍然优于以前的 SOTA 方法(NDCG@10:0.310 vs. 0.295)。在所有查询中，相对于 NDCG@10，LADER 可以将稠密检索器的性能提高24% -37% ，同时不需要额外的训练，并且预计通过更多的日志可以进一步提高性能。我们的回归分析表明，更频繁的查询、更高的查询相似度熵和更低的文档相似度熵往往更有利于日志增强。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LADER:+Log-Augmented+DEnse+Retrieval+for+Biomedical+Literature+Search)|0|
|[Measuring Service-Level Learning Effects in Search Via Query-Randomized Experiments](https://doi.org/10.1145/3539618.3592020)|Paul Musgrave, Cuize Han, Parth Gupta|Amazon Search, Palo Alto, CA, USA|In order to determine the relevance of a given item to a query, most modern search ranking systems make use of features which aggregate prior user behavior for that item and query (e.g. click rate). For practical reasons, when running A/B tests on ranking systems, these features are generally shared between all treatments. For the most common experiment designs, which randomize traffic by user or by session, this creates a pathway by which the behavior of units in one treatment can effect the outcomes for units in other treatments, violating the Stable Unit Treatment Value Assumption (SUTVA) and biasing measured outcomes. Moreover, for experiments targeting improvements to the behavior data available to such features (e.g. online exploration), this pathway is precisely the one we are trying to affect; if such changes occur identically in treatment and control, then they cannot be measured. To address this, we propose the use of experiments which instead randomize traffic based on the search query. To validate our approach, we perform a pair of A/B tests on an explore-exploit framework in the Amazon search page: one under query randomization, and one under user randomization. In line with the theoretical predictions, we find that the query-randomized iteration is able to measure a statistically significant effect (+0.66% Purchases, p=0.001) where the user-randomized iteration does not (-0.02% Purchases, p=0.851).|为了确定给定条目与查询的相关性，大多数现代搜索排名系统使用的特征是聚合该条目和查询之前的用户行为(例如点击率)。出于实际原因，在排名系统上运行 A/B 测试时，这些特性通常在所有处理之间共享。对于最常见的实验设计，其按用户或会话随机化流量，这创建了一个途径，通过这个途径，一个治疗中的单位的行为可以影响其他治疗中的单位的结果，违反稳定单位治疗价值假设(SUTVA)和偏倚测量结果。此外，对于针对改善这些特征的行为数据(例如在线探索)的实验，这条路径正是我们试图影响的路径; 如果这些变化在治疗和控制中发生相同，那么它们就无法被测量。为了解决这个问题，我们提出用实验代替基于搜索查询的流量随机化。为了验证我们的方法，我们在 Amazon 搜索页面中的探索开发框架上执行了两个 A/B 测试: 一个在查询随机化下，另一个在用户随机化下。与理论预测一致，我们发现查询随机迭代能够测量统计学显着效应(+ 0.66% 购买，p = 0.001) ，而用户随机迭代不能(- 0.02% 购买，p = 0.851)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Measuring+Service-Level+Learning+Effects+in+Search+Via+Query-Randomized+Experiments)|0|
|[Personalized Dynamic Recommender System for Investors](https://doi.org/10.1145/3539618.3592035)|Takehiro Takayanagi, ChungChi Chen, Kiyoshi Izumi|The University of Tokyo, Tokyo, Japan; National Institute of Advanced Industrial Science and Technology, Tokyo, Japan|With the development of online platforms, people can share and obtain opinions quickly. It also makes individuals' preferences change dynamically and rapidly because they may change their minds when getting convincing opinions from other users. Unlike representative areas of recommendation research such as e-commerce platforms where items' features are fixed, in investment scenarios financial instruments' features such as stock price, also change dynamically over time. To capture these dynamic features and provide a better-personalized recommendation for amateur investors, this study proposes a Personalized Dynamic Recommender System for Investors, PDRSI. The proposed PDRSI considers two investor's personal features: dynamic preferences and historical interests, and two temporal environmental properties: recent discussions on the social media platform and the latest market information. The experimental results support the usefulness of the proposed PDRSI, and the ablation studies show the effect of each module. For reproduction, we follow Twitter's developer policy to share our dataset for future work.|随着在线平台的发展，人们可以快速地分享和获取意见。它还使个人的喜好动态而迅速地改变，因为当他们从其他用户那里得到令人信服的意见时，他们可能会改变自己的想法。不像有代表性的推荐研究领域，如电子商务平台，项目的特征是固定的，在投资情景下，金融工具的特征，如股票价格，也随着时间的推移而动态变化。为了捕捉这些动态特征，并为业余投资者提供更好的个性化推荐，本研究提出了一个投资者个性化动态推荐系统 PDRSI。提出的 PDRSI 考虑了两个投资者的个人特征: 动态偏好和历史兴趣，以及两个时间环境属性: 最近在社交媒体平台上的讨论和最新的市场信息。实验结果支持了所提出的 PDRSI 的有效性，并且消融研究显示了每个模块的效果。为了复制，我们遵循 Twitter 的开发者政策来共享我们的数据集，以备将来使用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalized+Dynamic+Recommender+System+for+Investors)|0|
|[Personalized Showcases: Generating Multi-Modal Explanations for Recommendations](https://doi.org/10.1145/3539618.3592036)|An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, Julian John McAuley||Existing explanation models generate only text for recommendations but still struggle to produce diverse contents. In this paper, to further enrich explanations, we propose a new task named personalized showcases, in which we provide both textual and visual information to explain our recommendations. Specifically, we first select a personalized image set that is the most relevant to a user's interest toward a recommended item. Then, natural language explanations are generated accordingly given our selected images. For this new task, we collect a large-scale dataset from Google Local (i.e.,~maps) and construct a high-quality subset for generating multi-modal explanations. We propose a personalized multi-modal framework which can generate diverse and visually-aligned explanations via contrastive learning. Experiments show that our framework benefits from different modalities as inputs, and is able to produce more diverse and expressive explanations compared to previous methods on a variety of evaluation metrics.|现有的解释模型只生成建议的文本，但仍然难以产生不同的内容。在本文中，为了进一步丰富解释，我们提出了一个新的任务称为个性化展示，其中我们提供文本和视觉信息来解释我们的建议。具体来说，我们首先选择与用户对推荐项目的兴趣最相关的个性化图像集。然后，根据所选图像生成自然语言解释。对于这个新任务，我们从 Google Local (即 ~ map)收集了一个大规模的数据集，并构建了一个高质量的子集来生成多模态解释。我们提出了一个个性化的多模态框架，它可以通过对比学习产生多样化和视觉一致的解释。实验表明，我们的框架受益于不同的模式作为输入，并能够产生更多的多样性和表达的解释比以前的方法在各种评价指标。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalized+Showcases:+Generating+Multi-Modal+Explanations+for+Recommendations)|0|
|[PersonalTM: Transformer Memory for Personalized Retrieval](https://doi.org/10.1145/3539618.3592037)|Ruixue Lian, Sixing Lu, Clint Solomon, Gustavo Aguilar, Pragaash Ponnusamy, Jialong Han, Chengyuan Ma, Chenlei Guo|Amazon Alexa AI, Seattle, WA, USA; University of Wisconsin-Madison, Madison, WI, USA|The Transformer Memory as a Differentiable Search Index (DSI) has been proposed as a new information retrieval paradigm, which aims to address the limitations of dual-encoder retrieval framework based on the similarity score. The DSI framework outperforms strong baselines by directly generating relevant document identifiers from queries without relying on an explicit index. The memorization power of DSI framework makes it suitable for personalized retrieval tasks. Therefore, we propose a Personal Transformer Memory (PersonalTM) architecture for personalized text retrieval. PersonalTM incorporates user-specific profiles and contextual user click behaviors, and introduces hierarchical loss in the decoding process to align with the hierarchical assignment of document identifier. Additionally, PersonalTM also employs an adapter architecture to improve the scalability for index updates and reduce computation costs, compared to the vanilla DSI. Experiments show that PersonalTM outperforms the DSI baseline, BM25, fine-tuned dual-encoder, and other personalized models in terms of precision at top 1st and 10th positions and Mean Reciprocal Rank (MRR). Specifically, PersonalTM improves p@1 by 58%, 49%, and 12% compared to BM25, Dual-encoder, and DSI, respectively.|作为一种新的信息检索检索范式，变压器内存作为一种可微检索索引(DSI)已被提出，其目的是解决基于相似性得分的双编码器检索框架的局限性。DSI 框架通过直接从查询中生成相关文档标识符而不依赖于显式索引，从而优于强大的基线。DSI 框架的记忆能力使其适用于个性化检索任务。因此，我们提出了一种个性化文本检索的个人变压器内存(PersonalTM)体系结构。PersonalTM 融合了用户特定的配置文件和上下文用户点击行为，并在解码过程中引入了层次性丢失，以便与文档标识符的层次性分配保持一致。此外，PersonalTM 还采用了适配器架构，以提高索引更新的可伸缩性并降低计算成本。实验表明，PersonalTM 在位置和平均倒数排名(MRR)的精度方面优于 DSI 基线、 BM25、微调双编码器和其他个性化模型。具体来说，PersonalTM 比 BM25、双编码器和 DSI 分别提高了58% 、49% 和12% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PersonalTM:+Transformer+Memory+for+Personalized+Retrieval)|0|
|[Prediction then Correction: An Abductive Prediction Correction Method for Sequential Recommendation](https://doi.org/10.1145/3539618.3592040)|Yulong Huang, Yang Zhang, Qifan Wang, Chenxu Wang, Fuli Feng||Sequential recommender models typically generate predictions in a single step during testing, without considering additional prediction correction to enhance performance as humans would. To improve the accuracy of these models, some researchers have attempted to simulate human analogical reasoning to correct predictions for testing data by drawing analogies with the prediction errors of similar training data. However, there are inherent gaps between testing and training data, which can make this approach unreliable. To address this issue, we propose an \textit{Abductive Prediction Correction} (APC) framework for sequential recommendation. Our approach simulates abductive reasoning to correct predictions. Specifically, we design an abductive reasoning task that infers the most probable historical interactions from the future interactions predicted by a recommender, and minimizes the discrepancy between the inferred and true historical interactions to adjust the predictions.We perform the abductive inference and adjustment using a reversed sequential model in the forward and backward propagation manner of neural networks. Our APC framework is applicable to various differentiable sequential recommender models. We implement it on three backbone models and demonstrate its effectiveness. We release the code at https://github.com/zyang1580/APC.|顺序推荐模型通常在测试期间在单一步骤中生成预测，而不像人类那样考虑额外的预测修正以提高性能。为了提高这些模型的准确性，一些研究人员尝试模拟人类的类比推理，通过与类似训练数据的预测错误进行类比来修正测试数据的预测。然而，测试和训练数据之间存在固有的差距，这使得这种方法不可靠。为了解决这个问题，我们提出了一个文本{溯因预测修正}(APC)框架的顺序推荐。我们的方法模拟溯因推理来修正预测。具体来说，我们设计了一个溯因推理任务，从推荐者预测的未来相互作用中推断出最可能的历史相互作用，并最小化推断的和真实的历史相互作用之间的差异来调整预测。我们使用逆序模型在神经网络的前向和后向传播方式中执行溯因推理和调整。我们的 APC 框架适用于各种可微顺序推荐模型。我们在三个骨干模型上实现了它，并验证了它的有效性。我们在 https://github.com/zyang1580/apc 公布密码。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Prediction+then+Correction:+An+Abductive+Prediction+Correction+Method+for+Sequential+Recommendation)|0|
|[Priming and Actions: An Analysis in Conversational Search Systems](https://doi.org/10.1145/3539618.3592041)|Xiao Fu, Aldo Lipani|University College London, London, United Kingdom|In order to accurately simulate users in conversational systems, it is essential to comprehend the factors that influence their behaviour. This is a critical challenge for the Information Retrieval (IR) field, as conventional methods are not well-suited for the interactive and unique sequential structure of conversational contexts. In this study, we employed the concept of Priming effects from the Psychology literature to identify core stimuli for each abstracted effect. We then examined these stimuli on various datasets to investigate their correlations with users' actions. Finally, we trained Logistic Regression (LR) models based on these stimuli to anticipate users' actions. Our findings offer a basis for creating more realistic user models and simulators, as we identified the subset of stimuli with strong relationships with users' actions. Additionally, we built a model that can predict users' actions.|为了准确地模拟会话系统中的用户，有必要了解影响用户行为的因素。这对于信息检索研究领域来说是一个严峻的挑战，因为传统的研究方法并不适用于交互式和独特的会话语境顺序结构。在本研究中，我们运用心理学文献中启动效应的概念来识别每个抽象效应的核心刺激。然后，我们在不同的数据集上检查这些刺激，以调查它们与用户行为的相关性。最后，我们训练了基于这些刺激的 Logit模型模型来预测用户的行为。我们的研究结果为创建更加真实的用户模型和模拟器提供了基础，因为我们确定了与用户行为有密切关系的刺激子集。此外，我们还建立了一个能够预测用户行为的模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Priming+and+Actions:+An+Analysis+in+Conversational+Search+Systems)|0|
|[Quantifying Ranker Coverage of Different Query Subspaces](https://doi.org/10.1145/3539618.3592045)|Negar Arabzadeh, Amin Bigdeli, Radin Hamidi Rad, Ebrahim Bagheri|University of Waterloo, Waterloo, ON, Canada; Toronto Metropolitan University, Toronto, ON, Canada|The information retrieval community has observed significant performance improvements over various tasks due to the introduction of neural architectures. However, such improvements do not necessarily seem to have happened uniformly across a range of queries. As we will empirically show in this paper, the performance of neural rankers follow a long-tail distribution where there are many subsets of queries, which are not effectively satisfied by neural methods. Despite this observation, performance is often reported using standard retrieval metrics, such as MRR or nDCG, which capture average performance over all queries. As such, it is not clear whether reported improvements are due to incremental boost on a small subset of already well-performing queries or addressing queries that have been difficult to address by existing methods. In this paper, we propose the Task Subspace Coverage (TaSC /tAHsk/) metric, which systematically quantifies whether and to what extent improvements in retrieval effectiveness happen on similar or disparate query subspaces for different rankers. Our experiments show that the consideration of our proposed TaSC metric in conjunction with existing ranking metrics provides deeper insight into ranker performance and their contribution to overall advances on a given task.|由于引入了神经结构，信息检索社区已经观察到在各种任务中的表现有了显著的改善。然而，这种改进似乎并不一定在一系列查询中一致发生。正如我们将在本文中实证表明，神经排序器的性能遵循长尾分布，其中有许多子集的查询，这是不能有效地满足神经方法。尽管如此，性能通常使用标准检索指标(如 MRR 或 nDCG)报告，这些指标捕获所有查询的平均性能。因此，目前尚不清楚所报告的改进是否是由于对一小部分已经运行良好的查询进行了增量提升，或者是由于解决了现有方法难以解决的查询。在本文中，我们提出了任务子空间覆盖度量(TaSC/tAHsk/) ，该度量系统地量化是否和在什么程度上提高检索效率发生在相似或不同的查询子空间对不同的排名。我们的实验表明，考虑我们提出的 TaSC 度量结合现有的排名度量提供了更深入的洞察排名器的表现和他们的贡献，对整体进步的给定的任务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Quantifying+Ranker+Coverage+of+Different+Query+Subspaces)|0|
|[Sinkhorn Transformations for Single-Query Postprocessing in Text-Video Retrieval](https://doi.org/10.1145/3539618.3592064)|Konstantin Yakovlev, Gregory Polyakov, Ilseyar Alimova, Alexander Podolskiy, Andrey Bout, Sergey Nikolenko, Irina Piontkovskaya|Huawei Noah's Ark Lab, Moscow, Russian Fed.; Ivannikov Institute for System Programming of the RAS & St. Petersburg Department of the Steklov Institute of Mathematics, Moscow, Russian Fed.|A recent trend in multimodal retrieval is related to postprocessing test set results via the dual-softmax loss (DSL). While this approach can bring significant improvements, it usually presumes that an entire matrix of test samples is available as DSL input. This work introduces a new postprocessing approach based on Sinkhorn transformations that outperforms DSL. Further, we propose a new postprocessing setting that does not require access to multiple test queries. We show that our approach can significantly improve the results of state of the art models such as CLIP4Clip, BLIP, X-CLIP, and DRL, thus achieving a new state-of-the-art on several standard text-video retrieval datasets both with access to the entire test set and in the single-query setting.|多模态检索最近的一个趋势是通过双软件最大丢失(DSL)对测试集结果进行后处理。虽然这种方法可以带来显著的改进，但它通常假定一个完整的测试样本矩阵可以作为 DSL 输入。本文介绍了一种新的基于 Sinkhorn 变换的后处理方法，其性能优于 DSL。此外，我们提出了一个新的后处理设置，不需要访问多个测试查询。结果表明，该方法可以显著提高 CLIP4Clip、 BLIP、 X-CLIP 和 DRL 等先进模型的检索结果，从而在几个标准的文本视频检索数据集上实现了一个新的先进水平，既可以访问整个测试集，又可以访问单个查询设置。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sinkhorn+Transformations+for+Single-Query+Postprocessing+in+Text-Video+Retrieval)|0|
|[SparseEmbed: Learning Sparse Lexical Representations with Contextual Embeddings for Retrieval](https://doi.org/10.1145/3539618.3592065)|Weize Kong, Jeffrey M. Dudek, Cheng Li, Mingyang Zhang, Michael Bendersky|Google Research, Mountain View, CA, USA|In dense retrieval, prior work has largely improved retrieval effectiveness using multi-vector dense representations, exemplified by ColBERT. In sparse retrieval, more recent work, such as SPLADE, demonstrated that one can also learn sparse lexical representations to achieve comparable effectiveness while enjoying better interpretability. In this work, we combine the strengths of both the sparse and dense representations for first-stage retrieval. Specifically, we propose SparseEmbed - a novel retrieval model that learns sparse lexical representations with contextual embeddings. Compared with SPLADE, our model leverages the contextual embeddings to improve model expressiveness. Compared with ColBERT, our sparse representations are trained end-to-end to optimize both efficiency and effectiveness.|在密集检索中，先验工作使用多向量密集表示极大地提高了检索效率，ColBERT 就是一个例子。在稀疏检索中，最近的工作，如 SPLADE，表明人们也可以学习稀疏词汇表示，以实现可比的有效性，同时享受更好的可解释性。在这项工作中，我们结合了稀疏和密集表示的优势在第一阶段检索。具体来说，我们提出了一种新的检索模型 SparseEmbed，它通过上下文嵌入来学习稀疏的词汇表示。与 SPLADE 相比，我们的模型利用上下文嵌入来提高模型的表达能力。与 ColBERT 算法相比，我们对稀疏表示进行了端到端的训练，以优化效率和有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SparseEmbed:+Learning+Sparse+Lexical+Representations+with+Contextual+Embeddings+for+Retrieval)|0|
|[The Role of Relevance in Fair Ranking](https://doi.org/10.1145/3539618.3591933)|Aparna Balagopalan, Abigail Z. Jacobs, Asia J. Biega||Online platforms mediate access to opportunity: relevance-based rankings create and constrain options by allocating exposure to job openings and job candidates in hiring platforms, or sellers in a marketplace. In order to do so responsibly, these socially consequential systems employ various fairness measures and interventions, many of which seek to allocate exposure based on worthiness. Because these constructs are typically not directly observable, platforms must instead resort to using proxy scores such as relevance and infer them from behavioral signals such as searcher clicks. Yet, it remains an open question whether relevance fulfills its role as such a worthiness score in high-stakes fair rankings. In this paper, we combine perspectives and tools from the social sciences, information retrieval, and fairness in machine learning to derive a set of desired criteria that relevance scores should satisfy in order to meaningfully guide fairness interventions. We then empirically show that not all of these criteria are met in a case study of relevance inferred from biased user click data. We assess the impact of these violations on the estimated system fairness and analyze whether existing fairness interventions may mitigate the identified issues. Our analyses and results surface the pressing need for new approaches to relevance collection and generation that are suitable for use in fair ranking.|在线平台调节获得机会的途径: 基于相关性的排名通过分配招聘平台中的职位空缺和求职者，或市场中的卖家的曝光度来创造和限制选择。为了负责任地做到这一点，这些社会后果系统采用了各种公平措施和干预措施，其中许多措施试图根据价值分配曝光率。由于这些结构通常不能直接观察到，因此平台必须转而使用相关性等代理评分，并从搜索者点击等行为信号中推断出这些评分。然而，相关性是否能够在高风险的公平排名中发挥其价值得分的作用，仍然是一个悬而未决的问题。在这篇论文中，我们结合了社会科学、信息检索和机器学习中的公平性的观点和工具，得出了一组期望的标准，相关性得分应该满足这些标准，以便有意义地指导公平性干预。然后，我们经验性地表明，并非所有这些标准都符合从有偏见的用户点击数据推断出的相关性的案例研究。我们评估这些违规行为对估计的系统公平性的影响，并分析现有的公平性干预措施是否可以减轻已确定的问题。我们的分析和结果表明，迫切需要新的相关性收集和生成方法，适合在公平排名中使用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Role+of+Relevance+in+Fair+Ranking)|0|
|[Towards Building Voice-based Conversational Recommender Systems: Datasets, Potential Solutions and Prospects](https://doi.org/10.1145/3539618.3591876)|Xinghua Qu, Hongyang Liu, Zhu Sun, Xiang Yin, Yew Soon Ong, Lu Lu, Zejun Ma||Conversational recommender systems (CRSs) have become crucial emerging research topics in the field of RSs, thanks to their natural advantages of explicitly acquiring user preferences via interactive conversations and revealing the reasons behind recommendations. However, the majority of current CRSs are text-based, which is less user-friendly and may pose challenges for certain users, such as those with visual impairments or limited writing and reading abilities. Therefore, for the first time, this paper investigates the potential of voice-based CRS (VCRSs) to revolutionize the way users interact with RSs in a natural, intuitive, convenient, and accessible fashion. To support such studies, we create two VCRSs benchmark datasets in the e-commerce and movie domains, after realizing the lack of such datasets through an exhaustive literature review. Specifically, we first empirically verify the benefits and necessity of creating such datasets. Thereafter, we convert the user-item interactions to text-based conversations through the ChatGPT-driven prompts for generating diverse and natural templates, and then synthesize the corresponding audios via the text-to-speech model. Meanwhile, a number of strategies are delicately designed to ensure the naturalness and high quality of voice conversations. On this basis, we further explore the potential solutions and point out possible directions to build end-to-end VCRSs by seamlessly extracting and integrating voice-based inputs, thus delivering performance-enhanced, self-explainable, and user-friendly VCRSs. Our study aims to establish the foundation and motivate further pioneering research in the emerging field of VCRSs. This aligns with the principles of explainable AI and AI for social good, viz., utilizing technology's potential to create a fair, sustainable, and just world.|会话推荐系统(CRS)由于其通过交互式对话获取用户偏好并揭示推荐背后的原因的天然优势，已经成为 RSS 领域新兴的重要研究课题。然而，目前大多数 CRS 是基于文本的，不太方便用户，可能对某些用户构成挑战，例如那些有视力障碍或写作和阅读能力有限的用户。因此，本文首次研究了基于语音的 CRS (VCRS)的潜力，它可以彻底改变用户以一种自然、直观、方便和可访问的方式与 RSS 交互的方式。为了支持这样的研究，我们通过详尽的文献回顾，在电子商务和电影领域创建了两个 VCRS 基准数据集。具体来说，我们首先通过经验验证创建这样的数据集的好处和必要性。之后，我们通过 ChatGPT-driven 提示将用户项目的交互转换为基于文本的对话，生成多样化的自然模板，然后通过文本到语音模型合成相应的音频。同时，为了保证语音对话的自然性和高质量，我们精心设计了一系列的语音对话策略。在此基础上，我们进一步探索了潜在的解决方案，并指出了通过无缝提取和整合基于语音的输入来构建端到端录像机的可能方向，从而提供性能增强、自我解释和用户友好的录像机。我们的研究旨在为 VCRS 这一新兴领域的进一步开拓性研究奠定基础。这符合可解释的人工智能和人工智能的社会公益原则，也就是说，利用技术的潜力来创造一个公平、可持续和公正的世界。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Building+Voice-based+Conversational+Recommender+Systems:+Datasets,+Potential+Solutions+and+Prospects)|0|
|[Towards Explainable Conversational Recommender Systems](https://doi.org/10.1145/3539618.3591884)|Shuyu Guo, Shuo Zhang, Weiwei Sun, Pengjie Ren, Zhumin Chen, Zhaochun Ren||Explanations in conventional recommender systems have demonstrated benefits in helping the user understand the rationality of the recommendations and improving the system's efficiency, transparency, and trustworthiness. In the conversational environment, multiple contextualized explanations need to be generated, which poses further challenges for explanations. To better measure explainability in conversational recommender systems (CRS), we propose ten evaluation perspectives based on concepts from conventional recommender systems together with the characteristics of CRS. We assess five existing CRS benchmark datasets using these metrics and observe the necessity of improving the explanation quality of CRS. To achieve this, we conduct manual and automatic approaches to extend these dialogues and construct a new CRS dataset, namely Explainable Recommendation Dialogues (E-ReDial). It includes 756 dialogues with over 2,000 high-quality rewritten explanations. We compare two baseline approaches to perform explanation generation based on E-ReDial. Experimental results suggest that models trained on E-ReDial can significantly improve explainability while introducing knowledge into the models can further improve the performance. GPT-3 in the in-context learning setting can generate more realistic and diverse movie descriptions. In contrast, T5 training on E-ReDial can better generate clear reasons for recommendations based on user preferences. E-ReDial is available at https://github.com/Superbooming/E-ReDial.|传统推荐系统中的解释已经证明了在帮助用户理解推荐的合理性和提高系统的效率、透明度和可信度方面的好处。在会话环境中，需要产生多种语境化的解释，这对解释提出了进一步的挑战。为了更好地衡量会话推荐系统(CRS)的可解释性，结合 CRS 的特点，从传统推荐系统的概念出发，提出了10种评价视角。我们使用这些指标评估了5个现有的 CRS 基准数据集，并观察了提高 CRS 解释质量的必要性。为了实现这一目标，我们采用手动和自动的方法来扩展这些对话，并构建一个新的 CRS 数据集，即可解释的建议对话(E-ReDial)。它包括756个对话，超过2000个高质量的改写解释。我们比较了两种基线方法来执行基于 E-ReDial 的解释生成。实验结果表明，在 E-ReDial 上训练的模型可以显著提高模型的可解释性，而在模型中引入知识可以进一步提高模型的性能。GPT-3在上下文学习环境中可以产生更加真实和多样化的电影描述。相比之下，关于 E-ReDial 的 T5培训可以更好地根据用户偏好为推荐产生明确的理由。电子重拨可在 https://github.com/superbooming/E-ReDial 使用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Explainable+Conversational+Recommender+Systems)|0|
|[MG-ShopDial: A Multi-Goal Conversational Dataset for e-Commerce](https://doi.org/10.1145/3539618.3591883)|Nolwenn Bernard, Krisztian Balog||Conversational systems can be particularly effective in supporting complex information seeking scenarios with evolving information needs. Finding the right products on an e-commerce platform is one such scenario, where a conversational agent would need to be able to provide search capabilities over the item catalog, understand and make recommendations based on the user's preferences, and answer a range of questions related to items and their usage. Yet, existing conversational datasets do not fully support the idea of mixing different conversational goals (i.e., search, recommendation, and question answering) and instead focus on a single goal. To address this, we introduce MG-ShopDial: a dataset of conversations mixing different goals in the domain of e-commerce. Specifically, we make the following contributions. First, we develop a coached human-human data collection protocol where each dialogue participant is given a set of instructions, instead of a specific script or answers to choose from. Second, we implement a data collection tool to facilitate the collection of multi-goal conversations via a web chat interface, using the above protocol. Third, we create the MG-ShopDial collection, which contains 64 high-quality dialogues with a total of 2,196 utterances for e-commerce scenarios of varying complexity. The dataset is additionally annotated with both intents and goals on the utterance level. Finally, we present an analysis of this dataset and identify multi-goal conversational patterns.|对话系统可以特别有效地支持不断变化的信息需求的复杂信息搜索情景。在电子商务平台上寻找合适的产品就是这样一种情况，会话代理需要能够在商品目录上提供搜索功能，理解并根据用户的偏好提出建议，并回答一系列与商品及其使用相关的问题。然而，现有的会话数据集并不完全支持混合不同会话目标(即搜索、推荐和问答)的想法，而是集中在一个单一的目标上。为了解决这个问题，我们介绍了 MG-ShopDial: 一个混合了电子商务领域不同目标的会话数据集。具体来说，我们做出了以下贡献。首先，我们开发一个训练有素的人类-人类数据收集协议，其中每个对话参与者被给予一组指令，而不是一个特定的脚本或答案选择。其次，利用上述协议实现了一个数据收集工具，通过网络聊天接口实现了多目标会话的收集。第三，我们创建 MG-ShopDial 集合，其中包含64个高质量对话，总共有2,196个用于不同复杂度的电子商务场景的语句。此外，数据集还在语句级别上用意图和目标进行了注释。最后，我们对该数据集进行了分析，并识别了多目标会话模式。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MG-ShopDial:+A+Multi-Goal+Conversational+Dataset+for+e-Commerce)|0|
|[The Archive Query Log: Mining Millions of Search Result Pages of Hundreds of Search Engines from 25 Years of Web Archives](https://doi.org/10.1145/3539618.3591890)|Jan Heinrich Reimer, Sebastian Schmidt, Maik Fröbe, Lukas Gienapp, Harrisen Scells, Benno Stein, Matthias Hagen, Martin Potthast||The Archive Query Log (AQL) is a previously unused, comprehensive query log collected at the Internet Archive over the last 25 years. Its first version includes 356 million queries, 166 million search result pages, and 1.7 billion search results across 550 search providers. Although many query logs have been studied in the literature, the search providers that own them generally do not publish their logs to protect user privacy and vital business data. Of the few query logs publicly available, none combines size, scope, and diversity. The AQL is the first to do so, enabling research on new retrieval models and (diachronic) search engine analyses. Provided in a privacy-preserving manner, it promotes open research as well as more transparency and accountability in the search industry.|存档查询日志(Archive Query Log，AQL)是过去25年中在互联网档案馆收集的一个以前没有使用过的综合查询日志。它的第一个版本包括3.56亿个查询，1.66亿个搜索结果页面，以及来自550个搜索提供商的17亿个搜索结果。尽管文献中研究了许多查询日志，但拥有这些日志的搜索提供商通常不会发布日志，以保护用户隐私和重要业务数据。在少数几个公开可用的查询日志中，没有一个包含大小、范围和多样性。AQL 是第一个这样做的机构，它能够研究新的检索模型和(历时的)搜索引擎分析。以一种保护隐私的方式提供，它促进了开放的研究，以及在搜索行业中更多的透明度和问责制。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Archive+Query+Log:+Mining+Millions+of+Search+Result+Pages+of+Hundreds+of+Search+Engines+from+25+Years+of+Web+Archives)|0|
|[DECAF: A Modular and Extensible Conversational Search Framework](https://doi.org/10.1145/3539618.3591913)|Marco Alessio, Guglielmo Faggioli, Nicola Ferro|University of Padova, Padova, Italy|The Conversational Search (CS) paradigm allows for an intuitive interaction between the user and the system through natural language sentences and it is increasingly being adopted in various scenarios. However, its widespread experimentation has led to the birth of a multitude of conversational search systems with custom implementations and variants of information retrieval models. This exacerbates the reproducibility crisis already observed in several research areas, including Information Retrieval (IR). To address this issue, we propose DECAF: a modular and extensible conversational search framework designed for fast prototyping and development of conversational agents. Our framework integrates all the components that characterize a modern conversational search system and allows for the seamless integration of Machine Learning (ML) and Large Language Models (LLMs)-based techniques. Furthermore, thanks to its uniform interface, DECAF allows for experiments characterized by a high degree of reproducibility. DECAF contains several state-of-the-art components including query rewriting, search functions under BoW and dense paradigms, and re-ranking functions. Our framework is tested on two well-known conversational collections: TREC CAsT 2019 and TREC CAsT 2020 and the results can be used by future practitioners as baselines. Our contributions include the identification of a series of state-of-the-art components for the conversational search task and the definition of a modular framework for its implementation.|会话搜索(CS)范式允许用户和系统之间通过自然语言句子进行直观的交互，并且越来越多地被各种场景所采用。然而，它的广泛实验导致了大量会话搜索系统的诞生，这些系统有自定义实现和信息检索模型的变体。这加剧了包括信息检索(IR)在内的一些研究领域已经观察到的可重复性危机。为了解决这个问题，我们提出了 DECAF: 一个模块化和可扩展的会话搜索框架，用于会话代理的快速原型设计和开发。我们的框架集成了现代会话搜索系统的所有组件，并且允许基于机器学习(ML)和大语言模型(LLM)的技术的无缝集成。此外，由于其统一的界面，DECAF 允许实验拥有属性的高度重现性。DECAF 包含几个最先进的组件，包括查询重写、 BW 和稠密范例下的搜索函数以及重新排序函数。我们的框架在两个著名的会话集合上进行了测试: TREC CAsT 2019和 TREC CAsT 2020，结果可以被未来的从业者用作基线。我们的贡献包括为会话搜索任务确定了一系列最先进的组件，并为其实现定义了一个模块化框架。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DECAF:+A+Modular+and+Extensible+Conversational+Search+Framework)|0|
|[LongEval-Retrieval: French-English Dynamic Test Collection for Continuous Web Search Evaluation](https://doi.org/10.1145/3539618.3591921)|Petra Galuscáková, Romain Deveaud, Gabriela González Sáez, Philippe Mulhem, Lorraine Goeuriot, Florina Piroi, Martin Popel||LongEval-Retrieval is a Web document retrieval benchmark that focuses on continuous retrieval evaluation. This test collection is intended to be used to study the temporal persistence of Information Retrieval systems and will be used as the test collection in the Longitudinal Evaluation of Model Performance Track (LongEval) at CLEF 2023. This benchmark simulates an evolving information system environment - such as the one a Web search engine operates in - where the document collection, the query distribution, and relevance all move continuously, while following the Cranfield paradigm for offline evaluation. To do that, we introduce the concept of a dynamic test collection that is composed of successive sub-collections each representing the state of an information system at a given time step. In LongEval-Retrieval, each sub-collection contains a set of queries, documents, and soft relevance assessments built from click models. The data comes from Qwant, a privacy-preserving Web search engine that primarily focuses on the French market. LongEval-Retrieval also provides a 'mirror' collection: it is initially constructed in the French language to benefit from the majority of Qwant's traffic, before being translated to English. This paper presents the creation process of LongEval-Retrieval and provides baseline runs and analysis.|LongEval-Retrieval 是一个关注于持续检索评估的 Web 文献检索基准。这个测试集旨在用于研究信息检索系统的时间持续性，并将作为 CLEF 2023年模型性能跟踪纵向评估(LongEval)的测试集。这个基准测试模拟了一个不断发展的信息系统环境——比如 Web 搜索引擎运行的环境——其中文档收集、查询分发和相关性都在不断变化，同时遵循 Cranfield 离线评估范例。为此，我们引入了动态测试集合的概念，该集合由连续的子集合组成，每个子集合表示给定时间步骤中信息系统的状态。在 LongEval-Retrieval，每个子集包含一组查询、文档和基于点击模型的软相关性评估。这些数据来自 Qwant，一个主要关注法国市场的保护隐私的网络搜索引擎。LongEval-Retrieval 还提供了一个“镜像”集合: 它最初是用法语构建的，以便从 Qwant 的大部分流量中受益，然后才被翻译成英语。本文介绍了 LongEval-Retrieval 的创建过程，并提供了基线运行和分析。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LongEval-Retrieval:+French-English+Dynamic+Test+Collection+for+Continuous+Web+Search+Evaluation)|0|
|[OpenMatch-v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit](https://doi.org/10.1145/3539618.3591813)|Shi Yu, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu|Microsoft Research, Seattle, WA, USA; Tsinghua University, Beijing, China; Northeastern University, Shenyang, China|Pre-trained language models (PLMs) have emerged as the foundation of the most advanced Information Retrieval (IR) models. Powered by PLMs, the latest IR research has proposed novel models, new domain adaptation algorithms as well as enlarged datasets. In this paper, we present a Python-based IR toolkit OpenMatch-v2. As a full upgrade of OpenMatch proposed in 2021, OpenMatch-v2 incorporates the most recent advancements of PLM-based IR research, providing support for new, cross-modality models and enhanced domain adaptation techniques with a streamlined, optimized infrastructure. The code of OpenMatch is publicly available at https://github.com/OpenMatch/OpenMatch.|预先训练的语言模型(PLM)已经成为最先进的信息检索模型(IR)的基础。在 PLM 的推动下，最新的 IR 研究提出了新的模型、新的领域自适应算法以及扩大的数据集。在本文中，我们介绍了一个基于 Python 的 IR 工具包 OpenMatch-v2。作为2021年提出的 OpenMatch 的全面升级，OpenMatch-v2整合了基于 PLM 的 IR 研究的最新进展，为新的跨模式模型和增强的领域适应技术提供支持，同时提供简化、优化的基础设施。OpenMatch 的代码可在 https://github.com/OpenMatch/OpenMatch 公开查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OpenMatch-v2:+An+All-in-one+Multi-Modality+PLM-based+Information+Retrieval+Toolkit)|0|
|[Bootstrapping Query Suggestions in Spotify's Instant Search System](https://doi.org/10.1145/3539618.3591827)|Alva Liu, Humberto Jesús Corona Pampin, Enrico Palumbo|Spotify, Stockholm, Sweden; Spotify, Turin, Italy; Spotify, Amsterdam, Netherlands|Instant search systems present results to the user at every keystroke. This type of search system works best when the query ambiguity is low, the catalog is limited, and users know what they are looking for. However, Spotify's catalog is large and diverse, leading some users to struggle when formulating search intents. Query suggestions can be a powerful tool that helps users to express intents and explore content from the long-tail of the catalog. In this paper, we explain how we introduce query suggestions in Spotify's instant search system--a system that connects hundreds of millions of users with billions of items in our audio catalog. Specifically, we describe how we: (1) generate query suggestions from instant search logs, which largely contains in-complete prefix queries that cannot be directly applied as suggestions; (2) experiment with the generated suggestions in a specific UI feature, Related Searches; and (3) develop new metrics to measure whether the feature helps users to express search intent and formulate exploratory queries.|即时搜索系统在每次按键时向用户显示结果。这种类型的搜索系统在查询模糊度低、目录有限、用户知道他们在寻找什么的情况下工作得最好。然而，Spotify 的目录规模庞大且多样化，导致一些用户在制定搜索意图时举步维艰。查询建议是一个强大的工具，可以帮助用户表达意图并从目录的长尾部分查看内容。在本文中，我们将解释如何在 Spotify 的即时搜索系统中引入查询建议——该系统将数亿用户与我们的音频目录中的数十亿条目联系起来。具体来说，我们描述了我们如何: (1)从即时搜索日志生成查询建议，其中大部分包含不完整的前缀查询，不能直接作为建议应用; (2)实验生成的建议在一个特定的用户界面功能，相关搜索; (3)开发新的指标，以衡量功能是否有助于用户表达搜索意图和制定探索性查询。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bootstrapping+Query+Suggestions+in+Spotify's+Instant+Search+System)|0|
|[Long-Form Information Retrieval for Enterprise Matchmaking](https://doi.org/10.1145/3539618.3591833)|Pengyuan Li, GuangJie Ren, Anna Lisa Gentile, Chad DeLuca, Daniel Tan, Sandeep Gopisetty|IBM Research - Almaden, San Jose, CA, USA|Understanding customer requirements is a key success factor for both business-to-consumer (B2C) and business-to-business (B2B) enterprises. In a B2C context, most requirements are directly related to products and therefore expressed in keyword-based queries. In comparison, B2B requirements contain more information about customer needs and as such the queries are often in a longer form. Such long-form queries pose significant challenges to the information retrieval task in B2B context. In this work, we address the long-form information retrieval challenges by proposing a combination of (i) traditional retrieval methods, to leverage the lexical match from the query, and (ii) state-of-the-art sentence transformers, to capture the rich context in the long queries. We compare our method against traditional TF-IDF and BM25 models on an internal dataset of 12,368 pairs of long-form requirements and products sold. The evaluation shows promising results and provides directions for future work.|理解客户需求是 B2C 和 B2B 企业成功的关键因素。在 B2C 上下文中，大多数需求与产品直接相关，因此以基于关键字的查询表示。相比之下，B2B 需求包含更多关于客户需求的信息，因此查询通常是较长的形式。这种长形式的查询对 B2B 环境下的信息检索任务构成了重大挑战。在这项工作中，我们通过提出一个组合来解决长信息检索的挑战: (i)传统检索方法，利用查询中的词汇匹配; (ii)最先进的句子转换器，在长查询中捕捉丰富的上下文。我们比较了我们的方法与传统的 TF-IDF 和 BM25模型的内部数据集的12,368对长形式的要求和产品销售。评价结果显示了有希望的结果，并为今后的工作提供了方向。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Long-Form+Information+Retrieval+for+Enterprise+Matchmaking)|0|
|[Facebook Content Search: Efficient and Effective Adapting Search on A Large Scale](https://doi.org/10.1145/3539618.3591840)|Xiangyu Niu, YuWei Wu, Xiao Lu, Gautam Nagpal, Philip Pronin, Kecheng Hao, Zhen Liao, Guangdeng Liao|Meta Platforms, Inc, Menlo Park, CA, USA|Facebook content search is a critical channel that enables people to discover the best content to deepen their engagement with friends and family, creators, and communities. Building a highly personalized search engine to serve billions of daily active users to find the best results from a large scale of candidates is a challenging task. The search engine must take multiple dimensions into consideration, including different content types, different query intents, and user social graph, etc. In this paper, we discuss the challenges of Facebook content search in depth, and then describe our novel approach to efficiently handling a massive number of documents with advanced query understanding, retrieval, and machine learning techniques. The proposed system has been fully verified and applied to the production system of Facebook Search, which serves billions of users.|Facebook 内容搜索是一个重要的渠道，它使人们能够发现最好的内容，从而加深他们与朋友、家人、创作者和社区的联系。构建一个高个性化检索的引擎，为数十亿日常活跃用户提供服务，从大量候选人中找到最佳结果，是一项具有挑战性的任务。搜索引擎必须考虑多个维度，包括不同的内容类型，不同的查询意图，用户社会图等。在本文中，我们深入讨论了 Facebook 内容搜索的挑战，然后描述了我们的新方法，有效地处理大量具有先进的查询理解，检索和机器学习技术的文档。该系统已经得到充分验证，并应用于 Facebook 搜索的生产系统，该系统服务于数十亿用户。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Facebook+Content+Search:+Efficient+and+Effective+Adapting+Search+on+A+Large+Scale)|0|
|[Personalized Stock Recommendation with Investors' Attention and Contextual Information](https://doi.org/10.1145/3539618.3591850)|Takehiro Takayanagi, Kiyoshi Izumi, Atsuo Kato, Naoyuki Tsunedomi, Yukina Abe|The University of Tokyo, Tokyo, Japan; Daiwa Institute of Research Ltd., Tokyo, Japan; Daiwa Securities Group Inc., Tokyo, Japan; CONNECT Co.Ltd., Tokyo, Japan|The personalized stock recommendation is a task to recommend suitable stocks for each investor. The personalized recommendations are valuable, especially in investment decision making as the objective of building a portfolio varies by each retail investor. In this paper, we propose a Personalized Stock Recommendation with Investors' Attention and Contextual Information (PSRIC). PSRIC aims to incorporate investors' financial decision-making process into a stock recommendation, and it consists of an investor modeling module and a context module. The investor modeling module models the investor's attention toward various stock information. The context module incorporates stock dynamics and investor profiles. The result shows that the proposed model outperforms the baseline models and verifies the usefulness of both modules in ablation studies.|个性化股票推荐是为每个投资者推荐合适的股票的任务。个性化的建议是有价值的，特别是在投资决策中，因为建立一个投资组合的目标因每个散户投资者而异。本文提出了一种基于投资者注意力和上下文信息的个性化股票推荐方法。PSRIC 旨在将投资者的财务决策过程融入到股票推荐中，它由投资者建模模块和上下文模块组成。投资者建模模块模拟投资者对各种股票信息的关注。上下文模块包括股票动态和投资者概况。结果表明，该模型的性能优于基线模型，验证了两个模块在烧蚀研究中的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalized+Stock+Recommendation+with+Investors'+Attention+and+Contextual+Information)|0|
|[GreenSeq: Automatic Design of Green Networks for Sequential Recommendation Systems](https://doi.org/10.1145/3539618.3591855)|Yankun Ren, Xinxing Yang, Xingyu Lu, Longfei Li, Jun Zhou, Jinjie Gu, Guannan Zhang|Ant Group, Hangzhou, China|Transformer-based models have achieved tremendous success in sequential recommendation (SR), but they suffer from consuming excessive computational resources, particularly in the inference stage. Thus, developing lightweight yet effective SR models has become a frequent demand in industrial applications, which is also in line with the ideals of Green AI and Green IR. In this applied paper, we introduce GreenSeq deployed in Alipay to automatically design Green networks that can provide appropriate recommendations with lower computational consumption in SR. Specifically, GreenSeq uses a novel multi-layer search space that allows for flexible network design and a Greenness-aware loss term for balancing efficiency and effectiveness. Experiments on benchmark datasets and A/B testing show that GreenSeq performs well while using fewer resources. GreenSeq also reduces electricity and carbon emissions in Alipay.|基于变压器的模型在顺序推荐(SR)方面取得了巨大的成功，但是它们耗费了过多的计算资源，特别是在推理阶段。因此，开发轻量化而有效的 SR 模型已成为工业应用中的一个频繁需求，这也符合绿色人工智能和绿色红外的理想。在这篇应用文章中，我们引入支付宝中的 GreenSeq 来自动设计绿色网络，以较低的计算消耗提供适当的建议。具体来说，GreenSeq 使用了一个新颖的多层搜索空间，允许灵活的网络设计和一个绿色感知的损失术语来平衡效率和有效性。对基准数据集和 A/B 测试的实验表明，GreenSeq 在使用较少资源的情况下表现良好。GreenSeq 还降低了支付宝的电力和碳排放。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GreenSeq:+Automatic+Design+of+Green+Networks+for+Sequential+Recommendation+Systems)|0|
|[Conversational Bibliographic Search](https://doi.org/10.1145/3539618.3591789)|Markus Nilles|Trier University, Trier, Germany|In almost every area of research, it is necessary to find experts and publications on a topic. However, finding experts and publications is a difficult task not only for computers, but also for humans. For example, searching for experts, a user often enters a topic into a search engine, which then checks which people have published on that topic. A problem arises when a user does not make their query specific enough which can happen intentionally, e.g. when the user is doing a navigational search, or unintentionally, e.g., when the user lacks knowledge. As a result, the quality of the search results may not be very high and the best results may not be found. Current and widely used search engines for bibliographic metadata, such as dblp[2], ResearchGate, Google Scholar or Semantic Scholar allow only keyword-based searches. Kreutz et al.[1] presented SchenQL, a query language for bibliographic metadata that allows users to formulate their queries more easily and precisely than SQL. However, it requires training to understand the language and is not as easy for non-experts to use e.g. Google Scholar. To address the limitations of insufficient attention to the user's search intent and lack of search support, we aim to develop a conversational retrieval system in the domain of bibliographic metadata. This conversational search system assists users in achieving their search intent through a natural language dialog. It should be possible not only to find experts, but also to search for bibliographic metadata with the help of the system without prior knowledge. In this work, we aim to answer the research question: How beneficial is a conversational information retrieval system for searching bibliographic data? To address the research question, our contribution is threefold. First, we present an architecture for such a conversational information retrieval system for bibliographic metadata. Second, we will implement all the components of this system and evaluate our system by comparing it to existing bibliographic data search engines in terms of effectiveness, efficiency, and user satisfaction. Third, we will create and publish a dataset consisting of user queries that we will use to train our system. The architecture we propose consists of five main components: i) user intent classification, ii) a keyword extractor, iii) a search module, iv) a conversational module and v) the conversation history. i) The task of user intent classification is to determine the goal the user wants to achieve with their search query. The user intent classification consists of a set of corresponding user intent classifiers, each of which is responsible for one intent. If no classifier can match the user's query to their intent, or multiple classifiers conclude that the query matches their intents, the system asks the user to specify the query accordingly. ii) If the intent is correctly determined, a keyword extractor extracts the actual search term from the query. iii) After the intent and the search term have been determined, the actual search takes place in the search module. The user's query could be reformulated into a SchenQL query (Kreutz et al.[1] and sent to the database. iv) In the conversational module, the results are converted into a natural language response and the user is given suggestions for further queries related to the previous ones. v) The conversation history stores the user's queries and the system's responses, both to consider the entire session when determining intent and to improve the system's components. For example, new question formulations could improve the accuracy of the user intent classifiers as well as reveal what new intents a user of such a conversational search system might have that have not yet been implemented. In first experiments, we defined four user intents and already evaluated the user intent classification. The four user intents are: (1) searching for persons/authors/experts on a topic, (2) searching for publications by author name, (3) searching for publications on a topic and (4) searching for similar topics of a topic. Our classifiers achieved an accuracy of 0.998 in correctly determining user intent. In the future, we not only want to evaluate the individual components of a conversational information retrieval system for bibliographic data, but also want to work out the advantages and disadvantages of the conversational information retrieval system for bibliographic data, in comparison to already existing systems that do not support the user in their search process via natural language conversations.|在几乎每一个研究领域，都需要找到关于某一主题的专家和出版物。然而，寻找专家和出版物不仅对计算机，而且对人类来说都是一项艰巨的任务。例如，在搜索专家时，用户通常会在搜索引擎中输入一个主题，然后检查哪些人已经就该主题发表了文章。当用户没有使他们的查询足够具体时，问题就出现了，这可能是有意为之，例如当用户正在进行导航搜索时，或者无意中，例如当用户缺乏知识时。因此，搜索结果的质量可能不是很高，也可能找不到最好的结果。目前广泛使用的书目元数据搜索引擎，如 dblp [2]、 Research Gate、 Google Scholar 或 Semantic Scholar，只允许基于关键字的搜索。Kreutz 等[1]介绍了 SchenQL，一种书目元数据的查询语言，它允许用户比 SQL 更容易、更精确地表达他们的查询。然而，它需要训练才能理解这门语言，非专业人士使用起来就不那么容易了，比如 Google Scholar。为了解决对用户搜索意图关注不足和缺乏搜索支持的局限性，我们的目标是开发一个书目元数据领域的会话检索系统。这个会话搜索系统通过一个自然语言对话框帮助用户实现他们的搜索目的。不仅可以找到专家，而且可以在系统的帮助下在没有事先知识的情况下搜索书目元数据。在这项工作中，我们的目标是回答研究问题: 一个会话信息检索系统在搜索书目数据方面有多大益处？为了解决这个研究问题，我们的贡献是三方面的。首先，我们提出了一个针对书目元数据的会话信息检索系统的架构。其次，我们将实现该系统的所有组件，并通过与现有的书目数据搜索引擎在效率、效率和用户满意度方面的比较来评估我们的系统。第三，我们将创建和发布一个由用户查询组成的数据集，用于训练系统。我们提出的体系结构由五个主要组成部分组成: i)用户意图分类，ii)关键字提取，iii)搜索模块，iv)会话模块和 v)会话历史。I)用户意图分类的任务是确定用户希望通过搜索查询达到的目标。用户意图分类由一组相应的用户意图分类器组成，每个分类器负责一个意图。如果没有分类器能够将用户的查询与他们的意图匹配，或者多个分类器认为查询与他们的意图匹配，系统将要求用户相应地指定查询。Ii)如果意图被正确地确定，关键字提取器将从查询中提取实际的搜索词。(iii)在确定意图和搜索词之后，实际的搜索在搜索模块中进行。用户的查询可以被重新表述为一个 SchenQL 查询(Kreutz et al。[1]并发送到数据库。Iv)在会话模块中，结果被转换成自然语言响应，用户可以得到与前面相关的进一步查询的建议。V)会话历史记录存储用户的查询和系统的响应，以便在确定意图时考虑整个会话，并改进系统的组件。例如，新的问题形式可以提高用户意图分类器的准确性，并揭示这种会话搜索系统的用户可能有哪些尚未实现的新意图。在第一个实验中，我们定义了四个用户意图，并且已经对用户意图分类进行了评估。这四个用户意图是: (1)搜索某个主题的人/作者/专家; (2)按作者名称搜索出版物; (3)搜索某个主题的出版物; (4)搜索某个主题的相似主题。我们的分类器在正确确定用户意图方面达到了0.998的准确性。在未来，我们不仅要评估书目数据会话信息检索系统的各个组成部分，而且还要找出书目数据会话信息检索系统的优缺点，与现有的不支持用户通过自然语言会话进行搜索的系统相比。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Conversational+Bibliographic+Search)|0|
|[Aligning Distillation For Cold-start Item Recommendation](https://doi.org/10.1145/3539618.3591732)|Feiran Huang, Zefan Wang, Xiao Huang, Yufeng Qian, Zhetao Li, Hao Chen|The Hong Kong Polytechnic University, Hong Kong, China; Georgia Institute of Technology, Miami, China; Jinan University, Guangzhou, China|Recommending cold items in recommendation systems is a longstanding challenge due to the inherent differences between warm items, which are recommended based on user behavior, and cold items, which are recommended based on content features. To tackle this, generative models generate synthetic embeddings from content features, while dropout models enhance the robustness of the recommendation system by randomly dropping behavioral embeddings during training. However, these models primarily focus on handling the recommendation of cold items, but do not effectively address the differences between warm and cold recommendations. As a result, generative models may over-recommend either warm or cold items, neglecting the other type, and dropout models may negatively impact warm item recommendations. To address this, we propose the Aligning Distillation (ALDI) framework, which leverages warm items as "teachers" to transfer their behavioral information to cold items, referred to as "students". ALDI aligns the students with the teachers by comparing the differences in their recommendation characters, using tailored rating distribution aligning, ranking aligning, and identification aligning losses to narrow these differences. Furthermore, ALDI incorporates a teacher-qualifying weighting structure to prevent students from learning inaccurate information from unreliable teachers. Experiments on three datasets show that our approach outperforms state-of-the-art baselines in terms of overall, warm, and cold recommendation performance with three different recommendation backbones.|在推荐系统中推荐冷项是一个长期的挑战，因为基于用户行为的推荐热项和基于内容特征的推荐冷项之间存在固有的差异。为了解决这个问题，生成模型根据内容特征生成合成嵌入，而辍学模型通过在训练过程中随机删除行为嵌入来增强推荐系统的鲁棒性。然而，这些模型主要侧重于处理冷项目的推荐，但没有有效地解决温暖和冷的推荐之间的差异。因此，生成模型可能会过分推荐热的或冷的项目，而忽略了其他类型，辍学模型可能会对热的项目的推荐产生负面影响。为了解决这个问题，我们提出了校准蒸馏(ALDI)框架，它利用作为“老师”的热项目将他们的行为信息传递给称为“学生”的冷项目。ALDI 通过比较学生与教师在推荐特征上的差异，使用量身定制的评分分布对齐、排名对齐和识别对齐损失来缩小这些差异，从而将学生与教师对齐。此外，ALDI 采用了教师资格加权结构，以防止学生从不可靠的教师那里学到不准确的信息。在三个数据集上的实验表明，我们的方法在三个不同的推荐骨干方面的总体性能、暖推荐性能和冷推荐性能都优于最先进的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Aligning+Distillation+For+Cold-start+Item+Recommendation)|0|
|[M2EU: Meta Learning for Cold-start Recommendation via Enhancing User Preference Estimation](https://doi.org/10.1145/3539618.3591719)|Zhenchao Wu, Xiao Zhou|Renmin University of China, Beijing, China|The cold-start problem is commonly encountered in recommender systems when delivering recommendations to users or items with limited interaction information and can seriously harm the performance of the system. To cope with this issue, meta-learning-based approaches have come to the rescue in recent years by enabling models to learn user preferences globally in the pre-training stage followed by local fine-tuning for a target user with only a few interactions. However, we argue that the user representation learned in this way may be inadequate to capture user preference well since solely utilizing his/her own interactions may be far from enough in cold-start scenarios. To tackle this problem, we propose a novel meta-learning method named M2EU to enrich the representations of cold-start users by incorporating the information from other similar users who are identified based on the similarity of both inherent attributes and historical interactions. In addition, we design an attention mechanism according to the variances of ratings in the aggregation of similar user embeddings. To further enhance the capability of user preference modeling, we devise different neural layers to generate user or item embeddings at the rating level and utilize the weight-sharing strategy to guarantee adequate parameters learning of neural layers in our meta-learning approach. In meta-training with mini-batching, we adopt an incremental learning scheme to learn a set of generalized parameters for all tasks. Experimental results on the public benchmark datasets demonstrate that M2EU outperforms state-of-the-art methods through extensive quantitative evaluations in various cold-start scenarios.|在向交互信息有限的用户或项目提供建议时，推荐系统常常遇到冷启动问题，这可能严重损害系统的性能。为了解决这个问题，近年来，基于元学习的方法得到了拯救，使模型能够在培训前阶段全面了解用户偏好，然后对目标用户进行局部微调，只有少量的交互。然而，我们认为用这种方式学到的用户表示可能不足以很好地捕获用户偏好，因为在冷启动场景中，仅仅利用他/她自己的交互可能远远不够。为了解决这个问题，我们提出了一种新的元学习方法 M2EU，通过合并其他类似用户的信息来丰富冷启动用户的表示，这些用户是基于内在属性和历史交互的相似性来识别的。另外，根据相似用户嵌入聚合中评分的变化，设计了一种注意机制。为了进一步提高用户偏好建模的能力，我们设计了不同的神经层，在评分级别上生成用户或项目的嵌入，并利用权重分享策略来保证我们的元学习方法中神经层有足够的参数学习。在小批量元培训中，我们采用在线机机器学习方案来学习一组适用于所有任务的通用参数。对公共基准数据集的实验结果表明，M2EU 在各种冷启动情景下通过广泛的定量评估优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=M2EU:+Meta+Learning+for+Cold-start+Recommendation+via+Enhancing+User+Preference+Estimation)|0|
|[Exploration of Unranked Items in Safe Online Learning to Re-Rank](https://doi.org/10.1145/3539618.3591985)|Hiroaki Shiino, Kaito Ariu, Kenshi Abe, Riku Togashi||Bandit algorithms for online learning to rank (OLTR) problems often aim to maximize long-term revenue by utilizing user feedback. From a practical point of view, however, such algorithms have a high risk of hurting user experience due to their aggressive exploration. Thus, there has been a rising demand for safe exploration in recent years. One approach to safe exploration is to gradually enhance the quality of an original ranking that is already guaranteed acceptable quality. In this paper, we propose a safe OLTR algorithm that efficiently exchanges one of the items in the current ranking with an item outside the ranking (i.e., an unranked item) to perform exploration. We select an unranked item optimistically to explore based on Kullback-Leibler upper confidence bounds (KL-UCB) and safely re-rank the items including the selected one. Through experiments, we demonstrate that the proposed algorithm improves long-term regret from baselines without any safety violation.|用于在线学习排序(OLTR)问题的盗贼算法通常旨在利用用户反馈使长期收益最大化。然而，从实际的角度来看，这样的算法由于其积极的探索而有很高的风险损害用户体验。因此，近年来对安全勘探的需求不断增长。安全勘探的一种方法是逐步提高已经保证可接受质量的原始排名的质量。本文提出了一种安全的 OLTR 算法，该算法可以有效地将当前排名中的一个项目与排名之外的一个项目(即未排名的项目)进行交换，从而进行探索。我们选择一个未排序的项目乐观地探索基于 Kullback-Leibler 上置信界(KL-UCB)和安全重新排序的项目，包括选定的一个。实验结果表明，该算法在不违反安全约束的情况下，提高了基线长期后悔率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploration+of+Unranked+Items+in+Safe+Online+Learning+to+Re-Rank)|0|
|[Mining Interest Trends and Adaptively Assigning Sample Weight for Session-based Recommendation](https://doi.org/10.1145/3539618.3592021)|Kai Ouyang, Xianghong Xu, Miaoxin Chen, Zuotong Xie, HaiTao Zheng, Shuangyong Song, Yu Zhao|Tsinghua University, Shenzhen, China; China Telecom Corporation Ltd. & Data&AI Technology Company, Beijing, China|Session-based Recommendation (SR) aims to predict users' next click based on their behavior within a short period, which is crucial for online platforms. However, most existing SR methods somewhat ignore the fact that user preference is not necessarily strongly related to the order of interactions. Moreover, they ignore the differences in importance between different samples, which limits the model-fitting performance. To tackle these issues, we put forward the method, Mining Interest Trends and Adaptively Assigning Sample Weight, abbreviated as MTAW. Specifically, we model users' instant interest based on their present behavior and all their previous behaviors. Meanwhile, we discriminatively integrate instant interests to capture the changing trend of user interest to make more personalized recommendations. Furthermore, we devise a novel loss function that dynamically weights the samples according to their prediction difficulty in the current epoch. Extensive experimental results on two benchmark datasets demonstrate the effectiveness and superiority of our method.|基于会话的推荐(SR)旨在根据用户的行为在短时间内预测用户的下一次点击，这对在线平台至关重要。然而，大多数现有的 SR 方法都忽略了这样一个事实，即用户偏好并不一定与交互顺序密切相关。此外，他们忽略了不同样本之间的重要性差异，这限制了模型拟合的性能。为了解决这些问题，我们提出了挖掘兴趣趋势和自适应分配样本权重的方法，简称 MTAW。具体来说，我们根据用户当前的行为和他们之前的所有行为来建立用户的即时兴趣模型。同时，我们有选择性地整合即时兴趣，捕捉用户兴趣的变化趋势，提出更加个性化的推荐。此外，我们设计了一个新的损失函数，动态权重的样本根据他们的预测困难在当前的纪元。在两个基准数据集上的大量实验结果表明了该方法的有效性和优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mining+Interest+Trends+and+Adaptively+Assigning+Sample+Weight+for+Session-based+Recommendation)|0|
|[Unbiased Pairwise Learning from Implicit Feedback for Recommender Systems without Biased Variance Control](https://doi.org/10.1145/3539618.3592077)|Yi Ren, Hongyan Tang, Jiangpeng Rong, Siwen Zhu||Generally speaking, the model training for recommender systems can be based on two types of data, namely explicit feedback and implicit feedback. Moreover, because of its general availability, we see wide adoption of implicit feedback data, such as click signal. There are mainly two challenges for the application of implicit feedback. First, implicit data just includes positive feedback. Therefore, we are not sure whether the non-interacted items are really negative or positive but not displayed to the corresponding user. Moreover, the relevance of rare items is usually underestimated since much fewer positive feedback of rare items is collected compared with popular ones. To tackle such difficulties, both pointwise and pairwise solutions are proposed before for unbiased relevance learning. As pairwise learning suits well for the ranking tasks, the previously proposed unbiased pairwise learning algorithm already achieves state-of-the-art performance. Nonetheless, the existing unbiased pairwise learning method suffers from high variance. To get satisfactory performance, non-negative estimator is utilized for practical variance control but introduces additional bias. In this work, we propose an unbiased pairwise learning method, named UPL, with much lower variance to learn a truly unbiased recommender model. Extensive offline experiments on real world datasets and online A/B testing demonstrate the superior performance of our proposed method.|一般来说，推荐系统的模型训练可以基于两种类型的数据，即显式反馈和隐式反馈。此外，由于它的普遍可用性，我们看到了隐式反馈数据的广泛采用，如点击信号。内隐反馈的应用主要面临两个挑战。首先，隐式数据只包括正反馈。因此，我们不能确定未交互的项目是否真的是负面的还是正面的，但是没有显示给对应的用户。此外，稀有物品的相关性通常被低估，因为与流行物品相比，收集到的稀有物品的正面反馈要少得多。为了克服这些困难，本文提出了点对式和成对式的无偏相关学习解决方案。由于成对学习很适合排序任务，因此提出的无偏成对学习算法已经取得了很好的性能。然而，现有的无偏成对学习方法存在较大的方差。为了获得令人满意的性能，在实际方差控制中采用了非负估计，但引入了附加偏差。在这项工作中，我们提出了一个无偏的成对学习方法，称为 UPL，与更低的方差学习一个真正无偏的推荐模型。在现实世界数据集上的大量离线实验和在线 A/B 测试证明了我们提出的方法的优越性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unbiased+Pairwise+Learning+from+Implicit+Feedback+for+Recommender+Systems+without+Biased+Variance+Control)|0|
|[Using Entropy for Group Sampling in Pairwise Ranking from implicit feedback](https://doi.org/10.1145/3539618.3592084)|Yujie Chen, Runlong Yu, Qi Liu, Enhong Chen, Zhenya Huang|University of Science and Technology of China,State Key Laboratory of Cognitive Intelligence, Hefei, China|In recent years, pairwise methods, such as Bayesian Personalized Ranking (BPR), have gained significant attention in the field of collaborative filtering for recommendation systems. Group BPR is an extension of BPR that incorporates user groups to relax the strict assumption of independence between two users. However, the reliability of its user groups may be compromised as they only focus on a few behavioral similarities. To address this problem, this paper proposes a new entropy-weighted similarity measure for implicit feedback to quantify the relation between two users and sample like-minded user groups. We first introduce the group preference into several pairwise ranking algorithms and then utilize the entropy-weighted similarity to sample groups to further improve these algorithms. Unlike other approaches that rely solely on common item ratings, our method incorporates global information into the similarity measure, resulting in a more reliable approach to group sampling. We conducted experiments on two real-world datasets and evaluated our method using different metrics. The results show that our method can construct better user groups from sparse data and produce more accurate recommendations. Our approach can be applied to a wide range of recommendation systems, and this can significantly improve the performance of pairwise ranking algorithms, making it an effective tool for pairwise ranking.|近年来，成对方法，如贝叶斯个性化排名(bPR) ，在推荐系统的协同过滤领域得到了重视。组 BPR 是 BPR 的一个扩展，它包含了用户组，从而放松了两个用户之间独立性的严格假设。然而，其用户组的可靠性可能会受到影响，因为它们只关注少数行为上的相似性。针对这一问题，本文提出了一种新的隐式反馈熵加权相似度量方法，用于量化两个用户和样本相似用户群之间的关系。我们首先将群体偏好引入到几种成对排序算法中，然后利用熵权相似度对样本群体进行进一步的改进。与其他仅仅依赖于共同项目评分的方法不同，我们的方法将全局信息整合到相似性度量中，从而产生了更可靠的分组抽样方法。我们在两个真实世界的数据集上进行了实验，并使用不同的度量来评估我们的方法。结果表明，该方法能够从稀疏的数据中构造出更好的用户组，并能够产生更准确的推荐。我们的方法可以应用于广泛的推荐系统，这可以显著提高成对排序算法的性能，使其成为一个有效的工具成对排序。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Using+Entropy+for+Group+Sampling+in+Pairwise+Ranking+from+implicit+feedback)|0|
|[Beyond Single Items: Exploring User Preferences in Item Sets with the Conversational Playlist Curation Dataset](https://doi.org/10.1145/3539618.3591881)|Arun Tejasvi Chaganty, Megan Leszczynski, Shu Zhang, Ravi Ganti, Krisztian Balog, Filip Radlinski||Users in consumption domains, like music, are often able to more efficiently provide preferences over a set of items (e.g. a playlist or radio) than over single items (e.g. songs). Unfortunately, this is an underexplored area of research, with most existing recommendation systems limited to understanding preferences over single items. Curating an item set exponentiates the search space that recommender systems must consider (all subsets of items!): this motivates conversational approaches-where users explicitly state or refine their preferences and systems elicit preferences in natural language-as an efficient way to understand user needs. We call this task conversational item set curation and present a novel data collection methodology that efficiently collects realistic preferences about item sets in a conversational setting by observing both item-level and set-level feedback. We apply this methodology to music recommendation to build the Conversational Playlist Curation Dataset (CPCD), where we show that it leads raters to express preferences that would not be otherwise expressed. Finally, we propose a wide range of conversational retrieval models as baselines for this task and evaluate them on the dataset.|消费领域的用户，比如音乐，往往能够更有效地提供对一组项目(如播放列表或收音机)的偏好，而不是对单个项目(如歌曲)的偏好。不幸的是，这是一个探索不足的研究领域，大多数现有的推荐系统仅限于理解对单个项目的偏好。管理一个项目集可以指数化推荐系统必须考虑的搜索空间(项目的所有子集!): 这激发了会话方法——用户明确地陈述或改进他们的偏好，系统用自然语言引出偏好——作为理解用户需求的有效方法。我们把这个任务称为会话项目集管理，并提出了一种新的数据收集方法，该方法通过观察项目级和集合级反馈，有效地收集了会话环境中关于项目集的现实偏好。我们将这种方法应用于音乐推荐，以建立对话播放列表策划数据集(CPCD) ，在这里我们表明，它导致评分者表达不会以其他方式表达的偏好。最后，我们提出了广泛的会话检索模型作为这项任务的基线，并在数据集上对它们进行评估。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Beyond+Single+Items:+Exploring+User+Preferences+in+Item+Sets+with+the+Conversational+Playlist+Curation+Dataset)|0|
|[On the Impact of Outlier Bias on User Clicks](https://doi.org/10.1145/3539618.3591745)|Fatemeh Sarvi, Ali Vardasbi, Mohammad Aliannejadi, Sebastian Schelter, Maarten de Rijke||User interaction data is an important source of supervision in counterfactual learning to rank (CLTR). Such data suffers from presentation bias. Much work in unbiased learning to rank (ULTR) focuses on position bias, i.e., items at higher ranks are more likely to be examined and clicked. Inter-item dependencies also influence examination probabilities, with outlier items in a ranking as an important example. Outliers are defined as items that observably deviate from the rest and therefore stand out in the ranking. In this paper, we identify and introduce the bias brought about by outlier items: users tend to click more on outlier items and their close neighbors. To this end, we first conduct a controlled experiment to study the effect of outliers on user clicks. Next, to examine whether the findings from our controlled experiment generalize to naturalistic situations, we explore real-world click logs from an e-commerce platform. We show that, in both scenarios, users tend to click significantly more on outlier items than on non-outlier items in the same rankings. We show that this tendency holds for all positions, i.e., for any specific position, an item receives more interactions when presented as an outlier as opposed to a non-outlier item. We conclude from our analysis that the effect of outliers on clicks is a type of bias that should be addressed in ULTR. We therefore propose an outlier-aware click model that accounts for both outlier and position bias, called outlier-aware position-based model ( OPBM). We estimate click propensities based on OPBM ; through extensive experiments performed on both real-world e-commerce data and semi-synthetic data, we verify the effectiveness of our outlier-aware click model. Our results show the superiority of OPBM against baselines in terms of ranking performance and true relevance estimation.|用户交互数据是反事实学习排序(CLTR)的重要监督来源。这些数据存在表达偏差。无偏学习排名(ULTR)的许多工作都集中在位置偏差上，也就是说，排名较高的项目更容易被检查和点击。项目间的依赖性也会影响考试的概率，排名中的异常项目就是一个重要的例子。异常值被定义为与其他项目明显不同的项目，因此在排名中脱颖而出。在本文中，我们识别并介绍了离群项目带来的偏差: 用户往往点击更多的离群项目和他们的近邻。为此，我们首先进行了一个对照实验来研究离群值对用户点击的影响。接下来，为了检验我们对照实验的结果是否概括为自然情境，我们从一个电子商务平台探索了现实世界的点击日志。我们发现，在这两种情况下，用户在同一排名中点击离群数据项的次数明显多于非离群数据项。我们表明，这种趋势适用于所有的位置，也就是说，对于任何特定的位置，一个项目接受更多的交互作用时，表现为一个异常点，而不是一个非异常点的项目。我们从我们的分析得出结论，异常值对点击的影响是一种类型的偏见，应该在 ULTR 中解决。因此，我们提出了一种异常点击模型，这种模型同时考虑了异常点和位置偏差，称为异常点位置模型(OPBM)。我们基于 OPBM 估计点击倾向; 通过在现实电子商务数据和半合成数据上进行的大量实验，我们验证了我们的离群点击模型的有效性。我们的研究结果显示了 OPBM 在排序性能和真实相关性估计方面相对于基线的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+the+Impact+of+Outlier+Bias+on+User+Clicks)|0|
|[Distilling Semantic Concept Embeddings from Contrastively Fine-Tuned Language Models](https://doi.org/10.1145/3539618.3591667)|Na Li, Hanane Kteich, Zied Bouraoui, Steven Schockaert||Learning vectors that capture the meaning of concepts remains a fundamental challenge. Somewhat surprisingly, perhaps, pre-trained language models have thus far only enabled modest improvements to the quality of such concept embeddings. Current strategies for using language models typically represent a concept by averaging the contextualised representations of its mentions in some corpus. This is potentially sub-optimal for at least two reasons. First, contextualised word vectors have an unusual geometry, which hampers downstream tasks. Second, concept embeddings should capture the semantic properties of concepts, whereas contextualised word vectors are also affected by other factors. To address these issues, we propose two contrastive learning strategies, based on the view that whenever two sentences reveal similar properties, the corresponding contextualised vectors should also be similar. One strategy is fully unsupervised, estimating the properties which are expressed in a sentence from the neighbourhood structure of the contextualised word embeddings. The second strategy instead relies on a distant supervision signal from ConceptNet. Our experimental results show that the resulting vectors substantially outperform existing concept embeddings in predicting the semantic properties of concepts, with the ConceptNet-based strategy achieving the best results. These findings are furthermore confirmed in a clustering task and in the downstream task of ontology completion.|捕捉概念含义的学习向量仍然是一个基本的挑战。也许有些令人惊讶的是，事先训练好的语言模型到目前为止只能适度地提高这种概念嵌入的质量。目前使用语言模型的策略通常是通过平均语料库中提及的语境化表示来表示一个概念。这可能是次优的，至少有两个原因。首先，上下文相关的词向量有一个不寻常的几何形状，这阻碍了下游任务。其次，概念嵌入应该捕捉概念的语义属性，而语境化的词向量也受到其他因素的影响。为了解决这些问题，我们提出了两种对比学习策略，基于这样的观点: 当两个句子显示出相似的属性时，相应的上下文化向量也应该是相似的。一种策略是完全无监督的，根据上下文化词嵌入的邻域结构估计句子中表达的属性。第二种策略则依赖于概念网络发出的远程监控信号。我们的实验结果表明，所得到的向量在预测概念的语义属性方面大大优于现有的概念嵌入，基于概念网的策略取得了最好的效果。这些发现在本体完成的聚类任务和下游任务中得到了进一步的证实。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distilling+Semantic+Concept+Embeddings+from+Contrastively+Fine-Tuned+Language+Models)|0|
|[Learning Fine-grained User Interests for Micro-video Recommendation](https://doi.org/10.1145/3539618.3591713)|Yu Shang, Chen Gao, Jiansheng Chen, Depeng Jin, Meng Wang, Yong Li|Tsinghua University, Beijing, China; University of Science and Technology Beijing, Beijing, China; Hefei University of Technology, Hefei, China|Recent years have witnessed the rapid development of online micro-video platforms, in which the recommender system plays an essential role in overcoming the information overloading problem and providing personalized content for users. Although some progress has been achieved in the micro-video recommendation, there are still some limitations in learning the representations of user interests and video features. Specifically, the user modeling in existing works is performed at a coarse-grained level, i.e., video level. However, in micro-video recommendation, the user feedback is at a continuous form---users can skip over a video at each frame---which reveals fine-grained user preferences. In this work, we approach the problem of learning fine-grained user preferences for micro-video recommendation by first collecting two real-world datasets. To address the challenges of preference modeling and weak supervision signal, we propose a solution named FRAME (short for Fine-gRAined preference-modeling for Micro-video rEcommendation). Specifically, we first adopt visual feature extraction and transformation to maintain the fine-grained video embeddings. We then propose graph convolution layers to learn the user preference from complex and fine-grained user-clip relations, and hybrid-supervision objectives for enhancing the supervision signal. The experimental results on two collected real-world datasets demonstrate the effectiveness of our proposed model. We release the datasets and codes in https://github.com/tsinghua-fib-lab/FRAME, which we believe can benefit the community.|近年，网上微型视像平台发展迅速，推荐系统在克服资讯过载问题和为用户提供个人化内容方面发挥重要作用。虽然在微视频推荐方面已经取得了一些进展，但是在学习用户兴趣和视频特征的表示方面仍然存在一些局限性。具体来说，现有作品中的用户建模是在粗粒度级别(即视频级别)上执行的。然而，在微视频推荐中，用户的反馈是一种连续的形式——用户可以在每帧视频中跳过一段——这揭示了细粒度的用户偏好。在本研究中，我们首先采集两个真实世界的资料集，探讨学习微型影片推荐的细粒度使用者喜好的问题。为了解决偏好建模和弱监控信号的问题，提出了一种基于细粒度偏好建模的微视频推荐系统 FRAME 解决方案。具体来说，我们首先采用视觉特征提取和变换来维护视频的细粒度嵌入。然后提出图卷积层，从复杂细粒度的用户-剪辑关系中学习用户偏好，并提出混合监督目标来增强监督信号。在两个实际数据集上的实验结果证明了该模型的有效性。我们以 https://github.com/tsinghua-fib-lab/frame 的形式发布数据集和代码，我们相信这对社区有好处。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Fine-grained+User+Interests+for+Micro-video+Recommendation)|0|
|[Improving Implicit Feedback-Based Recommendation through Multi-Behavior Alignment](https://doi.org/10.1145/3539618.3591697)|Xin Xin, Xiangyuan Liu, Hanbing Wang, Pengjie Ren, Zhumin Chen, Jiahuan Lei, Xinlei Shi, Hengliang Luo, Joemon M. Jose, Maarten de Rijke, Zhaochun Ren||Recommender systems that learn from implicit feedback often use large volumes of a single type of implicit user feedback, such as clicks, to enhance the prediction of sparse target behavior such as purchases. Using multiple types of implicit user feedback for such target behavior prediction purposes is still an open question. Existing studies that attempted to learn from multiple types of user behavior often fail to: (i) learn universal and accurate user preferences from different behavioral data distributions, and (ii) overcome the noise and bias in observed implicit user feedback. To address the above problems, we propose multi-behavior alignment (MBA), a novel recommendation framework that learns from implicit feedback by using multiple types of behavioral data. We conjecture that multiple types of behavior from the same user (e.g., clicks and purchases) should reflect similar preferences of that user. To this end, we regard the underlying universal user preferences as a latent variable. The variable is inferred by maximizing the likelihood of multiple observed behavioral data distributions and, at the same time, minimizing the Kullback-Leibler divergence (KL-divergence) between user models learned from auxiliary behavior (such as clicks or views) and the target behavior separately. MBA infers universal user preferences from multi-behavior data and performs data denoising to enable effective knowledge transfer. We conduct experiments on three datasets, including a dataset collected from an operational e-commerce platform. Empirical results demonstrate the effectiveness of our proposed method in utilizing multiple types of behavioral data to enhance the prediction of the target behavior.|从隐式反馈中学习的推荐系统经常使用大量的单一类型的隐式用户反馈，如点击，以增强对稀疏目标行为(如购买)的预测。使用多种类型的隐式用户反馈来预测目标行为仍然是一个悬而未决的问题。现有的试图从多种类型的用户行为中学习的研究往往失败: (i)从不同的行为数据分布中学习普遍和准确的用户偏好，(ii)克服观察到的隐性用户反馈中的噪音和偏见。为了解决上述问题，我们提出了多行为协调(MBA) ，一个新的推荐框架，学习的内隐反馈使用多种类型的行为数据。我们推测来自同一用户的多种类型的行为(例如点击和购买)应该反映该用户的相似偏好。为此，我们认为潜在的通用用户偏好是一个潜在的变量。通过最大化多个观察到的行为数据分布的可能性来推断变量，同时最小化从辅助行为(如点击或视图)学习的用户模型与目标行为之间的 Kullback-Leibler 散度(KL- 散度)。MBA 从多行为数据中推断出普遍的用户偏好，并对数据进行去噪以实现有效的知识转移。我们在三个数据集上进行实验，包括从一个操作性电子商务平台收集的数据集。实证结果表明，该方法能够有效地利用多种类型的行为数据来提高对目标行为的预测能力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Implicit+Feedback-Based+Recommendation+through+Multi-Behavior+Alignment)|0|
|[Not Just Skipping: Understanding the Effect of Sponsored Content on Users' Decision-Making in Online Health Search](https://doi.org/10.1145/3539618.3591744)|Anat Hashavit, Hongning Wang, Tamar Stern, Sarit Kraus||Advertisements (ads) are an innate part of search engine business models. Advertisers are willing to pay search engines to promote their content to a prominent position in the search result page (SERP). This raises concerns about the search engine manipulation effect (SEME): the opinions of users can be influenced by the way search results are presented. In this work, we investigate the connection between SEME and sponsored content in the health domain. We conduct a series of user studies in which participants need to evaluate the effectiveness of different non-prescription natural remedies for various medical conditions. We present participants SERPs with different intentionally created biases towards certain viewpoints, with or without sponsored content, and ask them to evaluate the effectiveness of the treatment only based on the information presented to them. We investigate two types of sponsored content: 1. Direct marketing ads that directly market the product without expressing an opinion about its effectiveness, and 2. Indirect marketing ads that explicitly advocate the product's effectiveness on the condition in the query. Our results reveal a significant difference between the influence on users from these two ad types. Though direct marketing ads are mostly skipped by users, they can tilt users decision making towards more positive viewpoints. Indirect marketing ads affect both the users' examination behaviour and their perception of the treatment's effectiveness. We further discover that the contrast between the indirect marketing ads and the viewpoint presented in the organic search results plays an important role in users' decision-making. When the contrast is high, users exhibit a strong preference towards a negative viewpoint, and when the contrast is low or none, users exhibit preference towards a more positive viewpoint.|广告是搜索引擎商业模式固有的一部分。广告商愿意支付搜索引擎，以促进其内容在搜索结果页面(SERP)的突出位置。这引起了人们对搜索引擎操作效果(SEME)的关注: 用户的意见可能会受到搜索结果呈现方式的影响。在这项工作中，我们调查了健康领域中 SEME 和赞助内容之间的关系。我们进行了一系列的用户研究，其中参与者需要评估不同的非处方自然疗法对各种医疗条件的有效性。我们向参与者展示了不同的有意制造的偏向于某些观点的 SERP，有或没有赞助内容，并要求他们仅仅根据提供给他们的信息来评估治疗的有效性。我们调查了两种类型的赞助内容: 1。直接营销广告，直接营销的产品，而不表达对其有效性的意见，和2。间接营销广告，明确提倡在查询条件下产品的有效性。我们的研究结果揭示了这两种广告类型对用户的影响存在显著差异。虽然直接营销广告大多被用户跳过，但它们可以使用户的决策倾向于更积极的观点。间接营销广告既影响用户的考试行为，也影响他们对治疗效果的感知。我们进一步发现，间接营销广告与有机搜索结果中的观点之间的对比在用户的决策中起着重要作用。当对比度较高时，用户表现出对负面观点的强烈偏好，当对比度较低或没有对比度时，用户表现出对更积极的观点的偏好。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Not+Just+Skipping:+Understanding+the+Effect+of+Sponsored+Content+on+Users'+Decision-Making+in+Online+Health+Search)|0|
|[A Preference Learning Decoupling Framework for User Cold-Start Recommendation](https://doi.org/10.1145/3539618.3591627)|Chunyang Wang, Yanmin Zhu, Aixin Sun, Zhaobo Wang, Ke Wang|Shanghai Jiao Tong University, Shanghai, China; Nanyang Technological University, Singapore, Singapore|The issue of user cold-start poses a long-standing challenge to recommendation systems, due to the scarce interactions of new users. Recently, meta-learning based studies treat each cold-start user as a user-specific few-shot task and then derive meta-knowledge about fast model adaptation across training users. However, existing solutions mostly do not clearly distinguish the concept of new users and the concept of novel preferences, leading to over-reliance on meta-learning based adaptability to novel patterns. In addition, we also argue that the existing meta-training task construction inherently suffers from the memorization overfitting issue, which inevitably hinders meta-generalization to new users. In response to the aforementioned issues, we propose a preference learning decoupling framework, which is enhanced with meta-augmentation (PDMA), for user cold-start recommendation. To rescue the meta-learning from unnecessary adaptation to common patterns, our framework decouples preference learning for a cold-start user into two complementary aspects: common preference transfer, and novel preference adaptation. To handle the memorization overfitting issue, we further propose to augment meta-training users by injecting attribute-based noises, to achieve mutually-exclusive tasks. Extensive experiments on benchmark datasets demonstrate that our framework achieves superior performance improvements against state-of-the-art methods. We also show that our proposed framework is effective in alleviating memorization overfitting.|由于新用户之间的交互很少，用户冷启动问题对推荐系统提出了长期的挑战。最近，基于元学习的研究将每个冷启动用户视为一个特定于用户的短暂任务，然后得到关于跨培训用户快速模型适应的元知识。然而，现有的解决方案大多没有明确区分新用户的概念和新偏好的概念，导致过度依赖基于元学习的新模式适应性。此外，我们还认为，现有的元训练任务结构本质上存在记忆过拟合问题，这不可避免地阻碍了对新用户的元概括。针对上述问题，本文提出了一种基于元增强(PDMA)的偏好学习解耦框架，用于用户冷启动推荐。为了将元学习从不必要的对共同模式的适应中解救出来，我们的框架将冷启动用户的偏好学习分解为两个互补的方面: 共同偏好转移和新的偏好适应。为了解决记忆过拟合问题，我们进一步提出通过注入基于属性的噪声来增加元训练用户，以实现相互排斥的任务。在基准数据集上的大量实验表明，与最先进的方法相比，我们的框架实现了更好的性能改进。我们还表明，我们提出的框架在缓解记忆过度拟合方面是有效的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Preference+Learning+Decoupling+Framework+for+User+Cold-Start+Recommendation)|0|
|[Online Conversion Rate Prediction via Neural Satellite Networks in Delayed Feedback Advertising](https://doi.org/10.1145/3539618.3591747)|Qiming Liu, Haoming Li, Xiang Ao, Yuyao Guo, Zhihong Dong, Ruobing Zhang, Qiong Chen, Jianfeng Tong, Qing He|Tencent, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China|The delayed feedback is becoming one of the main obstacles in online advertising due to the pervasive deployment of the cost-per-conversion display strategy requesting a real-time conversion rate (CVR) prediction. It makes the observed data contain a large number of fake negatives that temporarily have no feedback but will convert later. Training on such biased data distribution would severely harm the performance of models. Prevailing approaches wait for a set period of time to see if samples convert before training on them, but solutions to guaranteeing data freshness remain under-explored by current research. In this work, we propose Delayed Feed-back modeling via neural Satellite Networks (DFSN for short) for online CVR prediction. It tackles the issue of data freshness to permit adaptive waiting windows. We first assign a long waiting window for our main model to cover most of conversions and greatly reduce fake negatives. Meanwhile, two kinds of satellite models are devised to learn from the latest data, and online transfer learning techniques are utilized to sufficiently exploit their knowledge. With information from satellites, our main model can deal with the issue of data freshness, achieving better performance than previous methods. Extensive experiments on two real-world advertising datasets demonstrate the superiority of our model.|延迟反馈正在成为网络广告的主要障碍之一，这是由于广泛采用了要求实时转换率(CVR)预测的按转换成本计费的显示策略。它使观测数据包含大量的假阴性，暂时没有反馈，但将在以后转换。关于这种有偏见的数据分发的培训将严重损害模型的性能。目前流行的方法要等一段时间才能看到样本在训练之前是否转换，但是目前的研究仍然没有探索出保证数据新鲜的解决方案。本文提出了基于神经网络的延迟反馈模型(简称 DFSN)用于 CVR 在线预测。它解决了数据新鲜性的问题，允许自适应的等待窗口。我们首先为我们的主要模型指定一个长的等待窗口，以覆盖大部分转换，并大大减少假否定。同时，设计了两种卫星模型来学习最新的数据，并利用在线迁移学习技术来充分利用他们的知识。利用卫星信息，我们的主模型可以处理数据的新鲜性问题，比以前的方法获得更好的性能。在两个实际广告数据集上的大量实验证明了该模型的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+Conversion+Rate+Prediction+via+Neural+Satellite+Networks+in+Delayed+Feedback+Advertising)|0|
|[M2GNN: Metapath and Multi-interest Aggregated Graph Neural Network for Tag-based Cross-domain Recommendation](https://doi.org/10.1145/3539618.3591720)|Zepeng Huai, Yuji Yang, Mengdi Zhang, Zhongyi Zhang, Yichun Li, Wei Wu||Cross-domain recommendation (CDR) is an effective way to alleviate the data sparsity problem. Content-based CDR is one of the most promising branches since most kinds of products can be described by a piece of text, especially when cold-start users or items have few interactions. However, two vital issues are still under-explored: (1) From the content modeling perspective, sufficient long-text descriptions are usually scarce in a real recommender system, more often the light-weight textual features, such as a few keywords or tags, are more accessible, which is improperly modeled by existing methods. (2) From the CDR perspective, not all inter-domain interests are helpful to infer intra-domain interests. Caused by domain-specific features, there are part of signals benefiting for recommendation in the source domain but harmful for that in the target domain. Therefore, how to distill useful interests is crucial. To tackle the above two problems, we propose a metapath and multi-interest aggregated graph neural network (M2GNN). Specifically, to model the tag-based contents, we construct a heterogeneous information network to hold the semantic relatedness between users, items, and tags in all domains. The metapath schema is predefined according to domain-specific knowledge, with one metapath for one domain. User representations are learned by GNN with a hierarchical aggregation framework, where the intra-metapath aggregation firstly filters out trivial tags and the inter-metapath aggregation further filters out useless interests. Offline experiments and online A/B tests demonstrate that M2GNN achieves significant improvements over the state-of-the-art methods and current industrial recommender system in Dianping, respectively. Further analysis shows that M2GNN offers an interpretable recommendation.|跨域推荐(CDR)是解决数据稀疏问题的有效方法。基于内容的 CDR 是最有前途的分支之一，因为大多数类型的产品都可以用文本描述，特别是当冷启动用户或项目几乎没有交互时。然而，有两个重要的问题仍然没有得到充分的探索: (1)从内容建模的角度来看，在真实的推荐系统中，充足的长文本描述通常是稀缺的，更常见的是轻量级的文本特征，如一些关键字或标签，是更容易获得的，这是不适当的建模现有方法。(2)从 CDR 的角度来看，并非所有域间利益都有助于推断域内利益。由于特定于领域的特性，有一部分信号在源域中有利于推荐，但在目标域中有害于推荐。因此，如何提取有用的利益至关重要。为了解决上述两个问题，我们提出了一种元路径和多兴趣聚合图神经网络(M2GNN)。具体来说，为了对基于标签的内容进行建模，我们构建了一个异构的信息网络来保持所有领域中用户、项目和标签之间的语义关系。元路径模式是根据特定于领域的知识预定义的，对于一个领域只有一个元路径。GNN 利用层次聚合框架学习用户表示，其中元路径内聚合首先过滤掉平凡的标签，元路径间聚合进一步过滤掉无用的兴趣。离线实验和在线 A/B 测试表明，M2GNN 在最先进的方法和 Dianping 目前的工业推荐系统上分别取得了显著的改进。进一步的分析表明，M2GNN 提供了一个可解释的建议。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=M2GNN:+Metapath+and+Multi-interest+Aggregated+Graph+Neural+Network+for+Tag-based+Cross-domain+Recommendation)|0|
|[Beyond the Overlapping Users: Cross-Domain Recommendation via Adaptive Anchor Link Learning](https://doi.org/10.1145/3539618.3591642)|Yi Zhao, Chaozhuo Li, Jiquan Peng, Xiaohan Fang, Feiran Huang, Senzhang Wang, Xing Xie, Jibing Gong|Yanshan University & The Key Laboratory for Computer Virtual Technology and System Integration of Hebei Province, Qinhuangdao, China; Yanshan University, Qinhuangdao, China; Central South University, Changsha, China; Jinan University, Guangzhou, China; Microsoft Research Asia, Beijing, China|Cross-Domain Recommendation (CDR) is capable of incorporating auxiliary information from multiple domains to advance recommendation performance. Conventional CDR methods primarily rely on overlapping users, whereby knowledge is conveyed between the source and target identities belonging to the same natural person. However, such a heuristic assumption is not universally applicable due to an individual may exhibit distinct or even conflicting preferences in different domains, leading to potential noises. In this paper, we view the anchor links between users of various domains as the learnable parameters to learn the task-relevant cross-domain correlations. A novel optimal transport based model ALCDR is further proposed to precisely infer the anchor links and deeply aggregate collaborative signals from the perspectives of intra-domain and inter-domain. Our proposal is extensively evaluated over real-world datasets, and experimental results demonstrate its superiority.|跨域推荐(CDR)能够结合来自多个域的辅助信息来提高推荐性能。传统的 CDR 方法主要依赖于重叠的用户，即在属于同一自然人的来源和目标身份之间传递知识。然而，这种启发式假设并不普遍适用，因为个体在不同的领域可能表现出不同的甚至相互冲突的偏好，导致潜在的噪音。本文将不同领域的用户之间的锚链视为学习任务相关的跨领域关联的可学习参数。进一步提出了一种新的基于最优传输的 ALCDR 模型，从域内和域间两个角度精确推断锚链和深度聚合协作信号。我们的建议是广泛的评估在现实世界的数据集，实验结果表明其优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Beyond+the+Overlapping+Users:+Cross-Domain+Recommendation+via+Adaptive+Anchor+Link+Learning)|0|
|[Manipulating Federated Recommender Systems: Poisoning with Synthetic Users and Its Countermeasures](https://doi.org/10.1145/3539618.3591722)|Wei Yuan, Quoc Viet Hung Nguyen, Tieke He, Liang Chen, Hongzhi Yin||Federated Recommender Systems (FedRecs) are considered privacy-preserving techniques to collaboratively learn a recommendation model without sharing user data. Since all participants can directly influence the systems by uploading gradients, FedRecs are vulnerable to poisoning attacks of malicious clients. However, most existing poisoning attacks on FedRecs are either based on some prior knowledge or with less effectiveness. To reveal the real vulnerability of FedRecs, in this paper, we present a new poisoning attack method to manipulate target items' ranks and exposure rates effectively in the top-$K$ recommendation without relying on any prior knowledge. Specifically, our attack manipulates target items' exposure rate by a group of synthetic malicious users who upload poisoned gradients considering target items' alternative products. We conduct extensive experiments with two widely used FedRecs (Fed-NCF and Fed-LightGCN) on two real-world recommendation datasets. The experimental results show that our attack can significantly improve the exposure rate of unpopular target items with extremely fewer malicious users and fewer global epochs than state-of-the-art attacks. In addition to disclosing the security hole, we design a novel countermeasure for poisoning attacks on FedRecs. Specifically, we propose a hierarchical gradient clipping with sparsified updating to defend against existing poisoning attacks. The empirical results demonstrate that the proposed defending mechanism improves the robustness of FedRecs.|联邦推荐系统(FedRecs)被认为是一种保护隐私的技术，可以在不共享用户数据的情况下协同学习推荐模型。由于所有参与者都可以通过上传梯度直接影响系统，因此 FedRecs 很容易受到恶意客户端的中毒攻击。然而，大多数现有的对联邦卫生委员会的中毒攻击要么是基于一些先前的知识，要么效果较差。为了揭示 FedRecs 的真实脆弱性，本文提出了一种新的中毒攻击方法，在不依赖任何先验知识的情况下，有效地操纵目标项目的等级和暴露率。具体来说，我们的攻击通过一组合成的恶意用户操纵目标项目的暴露率，这些用户上传有毒梯度，并考虑目标项目的替代产品。我们使用两个广泛使用的 FedRecs (Fed-NCF 和 Fed-LightGCN)在两个真实世界的推荐数据集上进行了广泛的实验。实验结果表明，我们的攻击可以显著提高暴露率的不受欢迎的目标项目与极少的恶意用户和更少的全球纪元比最先进的攻击。除了揭示安全漏洞，我们还设计了一种新的对策来防止对 FedRecs 的中毒攻击。具体来说，我们提出了一个分层梯度剪裁与稀疏更新，以防御现有的中毒攻击。实验结果表明，该防御机制提高了 FedRecs 的鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Manipulating+Federated+Recommender+Systems:+Poisoning+with+Synthetic+Users+and+Its+Countermeasures)|0|
|[Behavior Modeling for Point of Interest Search](https://doi.org/10.1145/3539618.3591955)|Haitian Chen, Qingyao Ai, Zhijing Wu, Zhihong Wang, Yiqun Liu, Min Zhang, Shaoping Ma, Juan Hu, Naiqiang Tan, Hua Chai|Tsinghua University, Beijing, China; Didi Chuxing, Beijing, China; Beijing Institute of Technology, Beijing, China|With the increasing popularity of location-based services, the point-of-interest (POI) search has received considerable attention in recent years. Existing studies on POI search mostly focus on how to construct better retrieval models to retrieve the relevant POI based on query-POI matching. However, user behavior in POI search, i.e., how users examine the search engine result page (SERP), is mostly underexplored. A good understanding of user behavior is well-recognized as a key to develop effective user models and retrieval models to improve the search quality. Therefore, in this paper, we propose to investigate user behavior in POI search with a lab study in which users' eye movements and their implicit feedback on the SERP are collected. Based on the collected data, we analyze (1) query-level user behavior patterns in POI search, i.e., examination and interactions on SERP; (2) session-level user behavior patterns in POI search, i.e., query reformulation, termination of search, etc. Our work sheds light on user behavior in POI search and could potentially benefit future studies on related research topics.|随着基于位置服务的日益普及，感兴趣点(POI)搜索近年来受到了广泛的关注。现有的 POI 检索研究主要集中在如何构造更好的检索模型来检索基于查询-POI 匹配的相关 POI。然而，POI 搜索中的用户行为，即用户如何检查搜索引擎结果页面(SERP) ，大多数都没有得到充分的研究。对用户行为的深入了解是建立有效的用户模型和检索模型以提高检索质量的关键。因此，在本文中，我们提出了一个实验室研究 POI 搜索中的用户行为，其中用户的眼球运动和他们对 SERP 的内隐反馈收集。基于收集到的数据，本文分析了(1) POI 搜索中的查询级用户行为模式，即在 SERP 上的检查和交互; (2) POI 搜索中的会话级用户行为模式，即查询重构、搜索终止等。我们的工作揭示了 POI 搜索中的用户行为，并可能有利于未来相关研究主题的研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Behavior+Modeling+for+Point+of+Interest+Search)|0|
|[Disentangling User Conversations with Voice Assistants for Online Shopping](https://doi.org/10.1145/3539618.3591974)|Nikhita Vedula, Marcus D. Collins, Oleg Rokhlenko|Amazon, Seattle, USA|Conversation disentanglement aims to identify and group utterances from a conversation into separate threads. Existing methods primarily focus on disentangling multi-party conversations with three or more speakers, explicitly or implicitly incorporating speaker-related feature signals to disentangle. Most existing models require a large amount of human annotated data for model training, and often focus on pairwise relations between utterances, not accounting much for the conversational context. In this work, we propose a multi-task learning approach with a contrastive learning objective, DiSC, to disentangle conversations between two speakers -- a user and a virtual speech assistant, for a novel domain of e-commerce. We analyze multiple ways and granularities to define conversation "threads''. DiSC jointly learns the relation between pairs of utterances, as well as between utterances and their respective thread context. We train and evaluate our models on multiple multi-threaded conversation datasets that were automatically created, without any human labeling effort. Experimental results on public datasets as well as real-world shopping conversations from a commercial speech assistant show that DiSC outperforms state-of-the-art baselines by at least 3%, across both automatic and human evaluation metrics. We also demonstrate how DiSC improves downstream dialog response generation in the shopping domain.|会话分离旨在将会话中的话语识别并分组为不同的线程。现有的方法主要集中在与三个或三个以上说话人的多方会话的分离，明确或隐含地结合说话人相关的特征信号来分离。大多数现有的模型需要大量的人工注释数据来进行模型训练，并且往往侧重于话语之间的成对关系，而不考虑会话语境。在这项工作中，我们提出了一个多任务的学习方法与对比学习的目标，DiSC，以分离两个说话人之间的会话-一个用户和虚拟语音助手，为一个新的领域的电子商务。我们分析了定义会话“线程”的多种方式和粒度。DiSC 共同学习话语对之间的关系，以及话语和它们各自的线索上下文之间的关系。我们在多个自动创建的多线程会话数据集上训练和评估我们的模型，而不需要任何人工标记。对公共数据集以及来自商业语音助手的真实世界购物会话的实验结果表明，DiSC 在自动和人工评估指标方面的表现至少比最先进的基线水平高出3% 。我们还演示了 DiSC 如何改进购物领域中的下游对话响应生成。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Disentangling+User+Conversations+with+Voice+Assistants+for+Online+Shopping)|0|
|[MaxSimE: Explaining Transformer-based Semantic Similarity via Contextualized Best Matching Token Pairs](https://doi.org/10.1145/3539618.3592017)|Eduardo Brito, Henri Iser|Fraunhofer Institute for Intelligent Analysis and Information Systems IAIS & Lamarr Institute for Machine Learning and Artificial Intelligence, Sankt Augustin, Germany|Current semantic search approaches rely on black-box language models, such as BERT, which limit their interpretability and transparency. In this work, we propose MaxSimE, an explanation method for language models applied to measure semantic similarity. Our approach is inspired by the explainable-by-design ColBERT architecture and generates explanations by matching contextualized query tokens to the most similar tokens from the retrieved document according to the cosine similarity of their embeddings. Unlike existing post-hoc explanation methods, which may lack fidelity to the model and thus fail to provide trustworthy explanations in critical settings, we demonstrate that MaxSimE can generate faithful explanations under certain conditions and how it improves the interpretability of semantic search results on ranked documents from the LoTTe benchmark, showing its potential for trustworthy information retrieval.|当前的语义搜索方法依赖于黑盒语言模型，比如 BERT，这限制了它们的可解释性和透明性。本文提出了一种用于语义相似度测量的语言模型解释方法 MaxSimE。我们的方法受到设计中可解释的 ColBERT 架构的启发，通过根据嵌入的余弦距离将上下文相关的查询标记与检索到的文档中最相似的标记匹配来生成解释。与现有的事后解释方法不同，这些方法可能对模型缺乏保真度，因此无法在关键环境下提供可信的解释，我们证明 MaxSimE 可以在某些条件下产生可信的解释，以及它如何提高对来自 LotTe 基准的排名文档的语义搜索结果的可解释性，显示其潜在的可信信息检索。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MaxSimE:+Explaining+Transformer-based+Semantic+Similarity+via+Contextualized+Best+Matching+Token+Pairs)|0|
|[Searching for Products in Virtual Reality: Understanding the Impact of Context and Result Presentation on User Experience](https://doi.org/10.1145/3539618.3592057)|Austin R. Ward, Sandeep Avula, Hao Fei Cheng, Sheikh Muhammad Sarwar, Vanessa Murdock, Eugene Agichtein|UNC Chapel Hill, Chapel Hill, NC, USA; Amazon, Seattle, WA, USA|Immersive technologies such as virtual reality (VR) and head-mounted displays (HMD) have seen increased adoption in recent years. In this work, we study two factors that influence users' experience when shopping in VR through voice queries: (1) context alignment of the search environment and (2) the level of detail on the Search Engine Results Page (SERP). To this end, we developed a search system for VR and conducted a within-subject exploratory study (N=18) to understand the impact of the two experimental conditions. Our results suggest that both context alignment and SERP are important factors for information-seeking in VR, which present unique opportunities and challenges. More specifically, based on our findings, we suggest that search systems for VR must be able to: (1) provide cues for information-seeking in both the VR environment and SERP, (2) distribute attention between the VR environment and the search interface, (3) reduce distractions in the VR environment and (4) provide a ''sense of control'' to search in the VR environment.|近年来，虚拟现实(VR)和头戴式显示器(HMD)等沉浸式技术得到了越来越多的采用。在这项工作中，我们研究了两个因素，影响用户的体验，当在虚拟现实购物时，通过语音查询: (1)上下文对齐的搜索环境和(2)搜索引擎结果页面(SERP)的详细程度。为此，我们开发了一个虚拟现实搜索系统，并进行了一个主题内探索性研究(N = 18) ，以了解这两个实验条件的影响。我们的研究结果表明，上下文对齐和 SERP 都是虚拟现实中信息搜索的重要因素，它们提供了独特的机遇和挑战。更具体地说，根据我们的研究结果，我们建议 VR 搜索系统必须能够: (1)为 VR 环境和 SERP 中的信息搜索提供线索; (2)在 VR 环境和搜索界面之间分配注意力; (3)减少 VR 环境中的干扰; (4)提供在 VR 环境中搜索的“控制感”。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Searching+for+Products+in+Virtual+Reality:+Understanding+the+Impact+of+Context+and+Result+Presentation+on+User+Experience)|0|
|[Uncertainty-aware Consistency Learning for Cold-Start Item Recommendation](https://doi.org/10.1145/3539618.3592078)|Taichi Liu, Chen Gao, Zhenyu Wang, Dong Li, Jianye Hao, Depeng Jin, Yong Li||Graph Neural Network (GNN)-based models have become the mainstream approach for recommender systems. Despite the effectiveness, they are still suffering from the cold-start problem, i.e., recommend for few-interaction items. Existing GNN-based recommendation models to address the cold-start problem mainly focus on utilizing auxiliary features of users and items, leaving the user-item interactions under-utilized. However, embeddings distributions of cold and warm items are still largely different, since cold items' embeddings are learned from lower-popularity interactions, while warm items' embeddings are from higher-popularity interactions. Thus, there is a seesaw phenomenon, where the recommendation performance for the cold and warm items cannot be improved simultaneously. To this end, we proposed a Uncertainty-aware Consistency learning framework for Cold-start item recommendation (shorten as UCC) solely based on user-item interactions. Under this framework, we train the teacher model (generator) and student model (recommender) with consistency learning, to ensure the cold items with additionally generated low-uncertainty interactions can have similar distribution with the warm items. Therefore, the proposed framework improves the recommendation of cold and warm items at the same time, without hurting any one of them. Extensive experiments on benchmark datasets demonstrate that our proposed method significantly outperforms state-of-the-art methods on both warm and cold items, with an average performance improvement of 27.6%.|基于图形神经网络(GNN)的模型已经成为推荐系统的主流方法。尽管有效，他们仍然受到冷启动问题的困扰，也就是说，推荐几个互动项目。现有的基于 GNN 的推荐模型主要是利用用户和项目的辅助特性来解决冷启动问题，使用户-项目交互得不到充分利用。但是，冷暖项目的嵌入分布仍然存在很大差异，因为冷暖项目的嵌入是从低人气互动中学习的，而暖项目的嵌入是从高人气互动中学习的。因此，存在一种跷跷板现象，即冷和暖项目的推荐性能不能同时得到改善。为此，我们提出了一个完全基于用户-项目交互的冷启动项目推荐(简称 UCC)的不确定性一致性学习框架。在此框架下，我们训练具有一致性学习的教师模型(生成器)和学生模型(推荐器) ，以确保具有额外生成的低不确定性交互的冷项目能够与热项目有相似的分布。因此，本文提出的框架在不损害任何一个项目的情况下，同时改进了对冷和暖项目的推荐。在基准数据集上进行的大量实验表明，我们提出的方法在热项和冷项上都明显优于最先进的方法，平均性能提高了27.6% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Uncertainty-aware+Consistency+Learning+for+Cold-Start+Item+Recommendation)|0|
|[User-Dependent Learning to Debias for Recommendation](https://doi.org/10.1145/3539618.3592083)|Fangyuan Luo, Jun Wu|Beijing Jiaotong University, Beijing, China|In recommender systems (RSs), inverse propensity score (IPS) has been a key technique to mitigate popularity bias by decreasing the contribution of popular items in modeling user-item interactions. However, conventional IPS treats all users equally, which tends to over-debias the popularity-insensitive (PI) users and under-debias the popularity-sensitive (PS) users. Furthermore, in such a treatment, IPS only performs slightly well on the debiased test while does not work on the normal biased test. To this end, we propose a user-dependent IPS (UDIPS in short) method, which adaptively conducts propensity estimation for each user-item pair based on the user's sensitivity to item popularity. Like IPS, our theoretical analysis validates the unbiasedness of UDIPS. Remarkably, our solution is model-agnostic and can be easily used to upgrade current unbiased recommenders. We implemented it in four state-of-the-art models for unbiased recommendation, and experimental results on two benchmark datasets demonstrate the effectiveness of our method in both unbiased and normal biased test.|在推荐系统(RS)中，逆倾向评分(IPS)是通过减少流行项目对用户项目交互建模的贡献来缓解流行偏差的关键技术。然而，传统的 IPS 对所有用户一视同仁，这往往会过度偏低人气敏感(PI)用户和低人气敏感(PS)用户。此外，在这样的治疗中，IPS 仅仅在去偏测试中表现稍好，而在正常偏测试中不起作用。为此，我们提出了一种基于用户依赖的 IPS (简称 UDIPS)方法，该方法基于用户对项目流行度的敏感度，自适应地对每个用户项目对进行倾向估计。与 IPS 一样，我们的理论分析也验证了 UDIPS 的无偏性。值得注意的是，我们的解决方案是模型无关的，可以很容易地用于升级当前的无偏差推荐器。我们在四个最先进的无偏推荐模型中实现了它，并且在两个基准数据集上的实验结果证明了我们的方法在无偏和正常有偏测试中的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=User-Dependent+Learning+to+Debias+for+Recommendation)|0|
|[MythQA: Query-Based Large-Scale Check-Worthy Claim Detection through Multi-Answer Open-Domain Question Answering](https://doi.org/10.1145/3539618.3591907)|Yang Bai, Anthony Colas, Daisy Zhe Wang||Check-worthy claim detection aims at providing plausible misinformation to downstream fact-checking systems or human experts to check. This is a crucial step toward accelerating the fact-checking process. Many efforts have been put into how to identify check-worthy claims from a small scale of pre-collected claims, but how to efficiently detect check-worthy claims directly from a large-scale information source, such as Twitter, remains underexplored. To fill this gap, we introduce MythQA, a new multi-answer open-domain question answering(QA) task that involves contradictory stance mining for query-based large-scale check-worthy claim detection. The idea behind this is that contradictory claims are a strong indicator of misinformation that merits scrutiny by the appropriate authorities. To study this task, we construct TweetMythQA, an evaluation dataset containing 522 factoid multi-answer questions based on controversial topics. Each question is annotated with multiple answers. Moreover, we collect relevant tweets for each distinct answer, then classify them into three categories: "Supporting", "Refuting", and "Neutral". In total, we annotated 5.3K tweets. Contradictory evidence is collected for all answers in the dataset. Finally, we present a baseline system for MythQA and evaluate existing NLP models for each system component using the TweetMythQA dataset. We provide initial benchmarks and identify key challenges for future models to improve upon. Code and data are available at: https://github.com/TonyBY/Myth-QA|值得核查的索赔检测旨在向下游事实核查系统或人类专家提供似是而非的错误信息以供核查。这是加快事实核查过程的关键一步。如何从一小部分预先收集的索赔中识别有价值的索赔已经做了很多努力，但是如何直接从大规模的信息来源(如 Twitter)中有效地识别有价值的索赔仍然没有得到充分的探索。为了填补这一空白，我们引入了 MythQA，这是一个新的多答案开放域问题回答(QA)任务，它涉及到基于查询的矛盾立场挖掘，用于大规模的值得检查的索赔检测。这背后的想法是，相互矛盾的说法是错误信息的有力指标，值得有关当局进行审查。为了研究这项任务，我们构建了一个评估数据集 TweetMythQA，其中包含522个基于有争议话题的事实性多答案问题。每个问题都有多个答案。此外，我们收集每个不同答案的相关推文，然后将它们分为三类: “支持”，“反驳”和“中立”。总的来说，我们注释了5.3千条 tweet。为数据集中的所有答案收集相互矛盾的证据。最后，我们提出一个 MythQA 的基线系统，并使用 TweetMythQA 数据集评估每个系统组件的现有 NLP 模型。我们提供了初步的基准，并确定了未来模型需要改进的关键挑战。代码和数据可在以下 https://github.com/tonyby/myth-qa 查阅:|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MythQA:+Query-Based+Large-Scale+Check-Worthy+Claim+Detection+through+Multi-Answer+Open-Domain+Question+Answering)|0|
|[Podify: A Podcast Streaming Platform with Automatic Logging of User Behaviour for Academic Research](https://doi.org/10.1145/3539618.3591824)|Francesco Meggetto, Yashar Moshfeghi|University of Strathclyde, Glasgow, United Kingdom|Podcasts are spoken documents that, in recent years, have gained widespread popularity. Despite the growing research interest in this domain, conducting user studies remains challenging due to the lack of datasets that include user behaviour. In particular, there is a need for a podcast streaming platform that reduces the overhead of conducting user studies. To address these issues, in this work, we present Podify. It is the first web-based platform for podcast streaming and consumption specifically designed for research. The platform highly resembles existing streaming systems to provide users with a high level of familiarity on both desktop and mobile. A catalogue of podcast episodes can be easily created via RSS feeds. The platform also offers Elasticsearch-based indexing and search that is highly customisable, allowing research and experimentation in podcast search. Users can manually curate playlists of podcast episodes for consumption. With mechanisms to collect explicit feedback from users (i.e., liking and disliking behaviour), Podify also automatically collects implicit feedback (i.e., all user interactions). Users' behaviour can be easily exported to a readable format for subsequent experimental analysis. A demonstration of the platform is available at https://youtu.be/k9Z5w_KKHr8, with the code and documentation available at https://github.com/NeuraSearch/Podify.|播客是近年来广受欢迎的口头文件。尽管在这个领域的研究兴趣日益增长，进行用户研究仍然具有挑战性，由于缺乏包括用户行为的数据集。特别是，需要一个播客流媒体平台，以减少进行用户研究的开销。为了解决这些问题，在本文中，我们介绍 Podify。它是第一个专门为研究设计的基于网络的播客流媒体和消费平台。该平台非常类似于现有的流媒体系统，为用户提供高水平的熟悉程度的桌面和移动。通过 RSS 源可以很容易地创建播客节目目录。该平台还提供高度可定制的基于 Elasticsearch 的索引和搜索，允许在播客搜索中进行研究和实验。用户可以手动管理播客节目的播放列表。通过收集用户明确反馈(即喜欢和不喜欢行为)的机制，Podify 还自动收集隐式反馈(即所有用户交互)。用户的行为可以很容易地导出到一个可读的格式，以便随后的实验分析。该平台的演示可在 https://youtu.be/k9z5w_kkhr8下载，代码和文档可在 https://github.com/neurasearch/podify 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Podify:+A+Podcast+Streaming+Platform+with+Automatic+Logging+of+User+Behaviour+for+Academic+Research)|0|
|[Multi-lingual Semantic Search for Domain-specific Applications: Adobe Photoshop and Illustrator Help Search](https://doi.org/10.1145/3539618.3591826)|Jayant Kumar, Ashok Gupta, Zhaoyu Lu, Andrei Stefan, Tracy Holloway King|Adobe Inc., San Jose, CA, USA|Search has become an integral part of Adobe products and users rely on it to learn about tool usage, shortcuts, quick links, and ways to add creative effects and to find assets such as backgrounds, templates, and fonts. Within applications such as Photoshop and Illustrator, users express domain-specific search intents via short text queries. In this work, we leverage sentence-BERT models fine-tuned on Adobe's HelpX data to perform multi-lingual semantic search on help and tutorial documents. We used behavioral data (queries, clicks, and impressions) and additional annotated data to train several BERT-based models for scoring query-document pairs for semantic similarity. We benchmarked the keyword-based production system against semantic search. Subsequent AB tests demonstrate that this approach improves engagement for longer queries while reducing null results significantly.|搜索已经成为 Adobe 产品不可或缺的一部分，用户依靠它来了解工具使用、快捷方式、快速链接、添加创意效果的方法以及寻找背景、模板和字体等资源。在 Photoshop 和 Illustrator 等应用程序中，用户通过短文本查询表达特定领域的搜索意图。在这项工作中，我们利用对 Adobe 的 HelpX 数据进行微调的句子 BERT 模型，对帮助和教程文档执行多语言语义搜索。我们使用行为数据(查询、点击和印象)和附加的注释数据来训练几个基于 BERT 的模型，以便对查询-文档对进行语义相似性评分。我们将基于关键字的生产系统与语义搜索进行了对比。随后的 AB 测试表明，这种方法提高了对较长查询的参与度，同时显著减少了空结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-lingual+Semantic+Search+for+Domain-specific+Applications:+Adobe+Photoshop+and+Illustrator+Help+Search)|0|
|[AttriBERT - Session-based Product Attribute Recommendation with BERT](https://doi.org/10.1145/3539618.3594714)|Akshay Jagatap, Nikki Gupta, Sachin Farfade, Prakash Mandayam Comar|Amazon, Bangalore, India|Finding the right product on e-commerce websites with millions of products is a daunting task for a large set of customers. On the search page, product attribute filters a.k.a. "refinements" emerge as a convenient navigational option for customers to narrow down the search results along product attributes of their choice (e.g., Material:Cotton, Color:Black for 'shirt'). However, on mobile devices, refinements are not easily discoverable due to lack of screen space. To improve discoverability, contextually relevant refinements are suggested in-line on search page by refinement recommendation systems. Existing works on refinement recommendations primarily rely on the search context as input, and are trained using aggregated refinement preferences 'explicitly' expressed by customers. These solutions fail to capture 'implicit' preferences expressed during the customer shopping mission through in-session browsing activity. In this paper, we propose a session-based recommendation system (SBRS) which recommends refinements by inferring product attribute preferences of customers based on the sequence of products viewed earlier in the session. For the task of refinement recommendation, we propose a) AttriBERT, a model which extends BERT architecture to learn from the attribute values of products and b) a novel product representation strategy, which represents each product as a dictionary of attribute:value pairs (e.g., RAM Size:64GB). We evaluate our approach on RecSys 2022 Challenge and Amazon e-commerce datasets. Our approach consistently outperforms various state-of-the-art sequence models on the task of session-based refinement recommendation.|在拥有数百万产品的电子商务网站上找到合适的产品对于一大批客户来说是一项艰巨的任务。在搜索页面上，产品属性过滤器，也就是“改进”，成为一个方便的导航选项，用户可以根据自己选择的产品属性缩小搜索结果范围(例如，材质: 棉花，颜色: 黑色表示“衬衫”)。然而，在移动设备上，由于缺乏屏幕空间，细化不容易被发现。为了提高可发现性，通过改进推荐系统，在搜索页面上提出与上下文相关的改进建议。现有的细化推荐工作主要依赖于搜索上下文作为输入，并使用客户“明确”表达的聚合细化偏好进行培训。这些解决方案无法捕获在客户购物任务期间通过会话浏览活动表达的“隐式”偏好。在本文中，我们提出了一个基于会话的推荐系统(SBRS) ，该系统通过推断客户的产品属性偏好，基于会话前面查看的产品序列来推荐改进。针对精化推荐的任务，我们提出了一种新的产品表示策略: (a) AttriBERT 模型，该模型扩展了 BERT 体系结构，以便从产品的属性值中学习; (b)一种新的产品表示策略，将每个产品表示为属性值对的字典(例如，RAM 大小: 64GB)。我们评估我们的方法对 RecSys 2022挑战和亚马逊电子商务数据集。我们的方法在基于会话的细化推荐任务上始终优于各种最先进的序列模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AttriBERT+-+Session-based+Product+Attribute+Recommendation+with+BERT)|0|
|[Adaptive Popularity Debiasing Aggregator for Graph Collaborative Filtering](https://doi.org/10.1145/3539618.3591635)|Huachi Zhou, Hao Chen, Junnan Dong, Daochen Zha, Chuang Zhou, Xiao Huang|Rice University, Houston, USA; The Hong Kong Polytechnic University, Hung Hom, Hong Kong|The graph neural network-based collaborative filtering (CF) models user-item interactions as a bipartite graph and performs iterative aggregation to enhance performance. Unfortunately, the aggregation process may amplify the popularity bias, which impedes user engagement with niche (unpopular) items. While some efforts have studied the popularity bias in CF, they often focus on modifying loss functions, which can not fully address the popularity bias in GNN-based CF models. This is because the debiasing loss can be falsely backpropagated to non-target nodes during the backward pass of the aggregation. In this work, we study whether we can fundamentally neutralize the popularity bias in the aggregation process of GNN-based CF models. This is challenging because 1) estimating the effect of popularity is difficult due to the varied popularity caused by the aggregation from high-order neighbors, and 2) it is hard to train learnable popularity debiasing aggregation functions because of data sparsity. To this end, we theoretically analyze the cause of popularity bias and propose a quantitative metric, named inverse popularity score, to measure the effect of popularity in the representation space. Based on it, a novel graph aggregator named APDA is proposed to learn per-edge weight to neutralize popularity bias in aggregation. We further strengthen the debiasing effect with a weight scaling mechanism and residual connections. We apply APDA to two backbones and conduct extensive experiments on three real-world datasets. The results show that APDA significantly outperforms the state-of-the-art baselines in terms of recommendation performance and popularity debiasing.|基于图形神经网络的协同过滤(CF)将用户-项目的交互作用建模为一个二分图，并执行迭代聚合以提高性能。不幸的是，聚合过程可能会放大流行偏见，这阻碍了用户参与利基(不受欢迎)项目。虽然已经有人研究了 CF 模型中的流行偏差，但是他们往往侧重于修改损失函数，这不能完全解决基于 GNN 的 CF 模型中的流行偏差问题。这是因为在聚合的后向传递过程中，去偏损失可能被错误地反向传播到非目标节点。在这项工作中，我们研究是否可以从根本上中和流行偏见的聚合过程中的 GNN 为基础的 CF 模型。这是具有挑战性的，因为1)估计流行的影响是困难的，因为不同的流行由高阶邻居的聚合造成，2)很难训练可学习的流行去偏聚合函数，因为数据稀疏。为此，我们从理论上分析了人气偏差产生的原因，并提出了一个定量的度量方法——逆人气得分，来衡量人气在表征空间中的作用。在此基础上，提出了一种新的图聚合器 APDA，该算法通过学习每边权值来中和聚合中的流行偏差。我们进一步加强了重量标度机制和残余连接的去偏效应。我们将 APDA 应用于两个主干网，并在三个真实世界的数据集上进行了广泛的实验。结果表明，APDA 在推荐性能和受欢迎程度方面明显优于最先进的基准。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adaptive+Popularity+Debiasing+Aggregator+for+Graph+Collaborative+Filtering)|0|
|[Rectifying Unfairness in Recommendation Feedback Loop](https://doi.org/10.1145/3539618.3591754)|Mengyue Yang, Jun Wang, JeanFrancois Ton|ByteDance Research, London, United Kingdom; University College London, London, United Kingdom|The issue of fairness in recommendation systems has recently become a matter of growing concern for both the academic and industrial sectors due to the potential for bias in machine learning models. One such bias is that of feedback loops, where the collection of data from an unfair online system hinders the accurate evaluation of the relevance scores between users and items. Given that recommendation systems often recommend popular content and vendors, the underlying relevance scores between users and items may not be accurately represented in the training data. Hence, this creates a feedback loop in which the user is not longer recommended based on their true relevance score but instead based on biased training data. To address this problem of feedback loops, we propose a two-stage representation learning framework, B-FAIR, aimed at rectifying the unfairness caused by biased historical data in recommendation systems. The framework disentangles the context data into sensitive and non-sensitive components using a variational autoencoder and then applies a novel Balanced Fairness Objective (BFO) to remove bias in the observational data when training a recommendation model. The efficacy of B-FAIR is demonstrated through experiments on both synthetic and real-world benchmarks, showing improved performance over state-of-the-art algorithms.|由于机器学习模型存在偏差，推荐系统的公平性问题最近成为学术界和工业界越来越关注的问题。其中一种偏见是反馈循环，即从不公平的在线系统收集数据阻碍了对用户和项目之间相关性得分的准确评估。鉴于推荐系统往往推荐流行内容和供应商，用户和项目之间的潜在相关性得分可能无法准确地反映在培训数据中。因此，这创建了一个反馈循环，其中用户不再被推荐基于他们的真实相关性得分，而是基于有偏见的训练数据。为了解决这个问题，我们提出了一个两阶段的表示学习框架，B-FAIR，旨在纠正推荐系统中有偏的历史数据所造成的不公平性。该框架使用变分自动编码器将上下文数据分解为敏感和非敏感组件，然后在训练推荐模型时应用一种新的平衡公平目标(BFO)来消除观测数据中的偏差。通过在合成和真实世界基准上的实验，证明了 B-FAIR 算法的有效性，表明其性能优于最先进的算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rectifying+Unfairness+in+Recommendation+Feedback+Loop)|0|
|[Measuring Item Global Residual Value for Fair Recommendation](https://doi.org/10.1145/3539618.3591724)|Jiayin Wang, Weizhi Ma, Chumeng Jiang, Min Zhang, Yuan Zhang, Biao Li, Peng Jiang||In the era of information explosion, numerous items emerge every day, especially in feed scenarios. Due to the limited system display slots and user browsing attention, various recommendation systems are designed not only to satisfy users' personalized information needs but also to allocate items' exposure. However, recent recommendation studies mainly focus on modeling user preferences to present satisfying results and maximize user interactions, while paying little attention to developing item-side fair exposure mechanisms for rational information delivery. This may lead to serious resource allocation problems on the item side, such as the Snowball Effect. Furthermore, unfair exposure mechanisms may hurt recommendation performance. In this paper, we call for a shift of attention from modeling user preferences to developing fair exposure mechanisms for items. We first conduct empirical analyses of feed scenarios to explore exposure problems between items with distinct uploaded times. This points out that unfair exposure caused by the time factor may be the major cause of the Snowball Effect. Then, we propose to explicitly model item-level customized timeliness distribution, Global Residual Value (GRV), for fair resource allocation. This GRV module is introduced into recommendations with the designed Timeliness-aware Fair Recommendation Framework (TaFR). Extensive experiments on two datasets demonstrate that TaFR achieves consistent improvements with various backbone recommendation models. By modeling item-side customized Global Residual Value, we achieve a fairer distribution of resources and, at the same time, improve recommendation performance.|在信息爆炸的时代，每天都会出现大量的条目，特别是在提要场景中。由于系统显示时隙和用户浏览注意力的限制，各种推荐系统不仅要满足用户的个性化信息需求，还要分配项目的曝光量。然而，最近的推荐研究主要集中在建立用户偏好模型，以呈现令人满意的结果和最大化用户交互，而很少注意开发项目侧公平曝光机制，以合理的信息传递。这可能会在项目方面导致严重的资源分配问题，比如雪球效应。此外，不公平的曝光机制可能会损害推荐性能。在本文中，我们呼吁将注意力从建模用户偏好转移到开发项目的公平曝光机制。我们首先进行饲料情景的实证分析，以探讨不同上传时间的项目之间的暴露问题。由此指出，时间因素造成的不公平曝光可能是雪球效应的主要原因。然后，为了实现资源的公平分配，我们提出了项目级定制时间分配的显式模型——全局剩余价值(GRV)。这个 GRV 模块被引入到建议中，并设计了时间感知的公平推荐框架(TaFR)。在两个数据集上的大量实验表明，TaFR 与各种骨干推荐模型实现了一致的改进。通过对项目端定制的全局剩余价值建模，我们实现了更公平的资源分配，同时提高了推荐性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Measuring+Item+Global+Residual+Value+for+Fair+Recommendation)|0|
|[Knowledge-enhanced Multi-View Graph Neural Networks for Session-based Recommendation](https://doi.org/10.1145/3539618.3591706)|Qian Chen, Zhiqiang Guo, Jianjun Li, Guohui Li|Huazhong University of Science and Technology, Wuhan, China|Session-based recommendation (SBR) has received increasing attention to predict the next item via extracting and integrating both global and local item-item relationships. However, there still exist some deficiencies in current works when capturing these two kinds of relationships. For global item-item relationships, the global graph constructed by most SBR is a pseudo-global graph, which may cause redundant mining of sequence relationships. For local item-item relationships, conventional SBR only mines the sequence patterns while ignoring the feature patterns, which may introduce noise when learning users' interests. To address these problems, we propose a novel Knowledge-enhanced Multi-View Graph Neural Network (KMVG) by constructing three views, namely knowledge view, session view, and pairwise view. Specifically, benefiting from the rich semantic information in the knowledge graph (KG), we build a genuine global graph that is sequence-independent based on KG to mine the global item-item relationships in the knowledge view. Then, a session view is utilized to capture the contextual transitions among items as the sequence patterns of local item-item relationships, and a pairwise view is used to explore the feature commonality within a session as the feature patterns of the local item-item relationships. Extensive experiments on three real-world public datasets demonstrate the superiority of KMVG, showing that it outperforms the state-of-the-art baselines. Further analysis also reveals the effectiveness of KMVG in exploiting the item-item relationships under multiple views.|基于会话的推荐(SBR)通过提取和整合全局和局部项目-项目关系来预测下一个项目已经受到越来越多的关注。然而，当前的作品在捕捉这两种关系时仍然存在一些不足。对于全局项目-项目关系，大多数 SBR 构造的全局图是伪全局图，这可能导致序列关系的冗余挖掘。对于局部项目-项目关系，传统的 SBR 只挖掘序列模式，而忽略了特征模式，在学习用户兴趣时可能会引入噪声。为了解决这些问题，本文通过构造知识视图、会话视图和成对视图三种视图，提出了一种新的知识增强型多视图图神经网络(KMVG)。具体来说，利用知识图中丰富的语义信息，我们建立了一个真正的基于知识图的序列无关的全局图来挖掘知识视图中的全局项目-项目关系。然后，利用会话视图作为本地项目-项目关系的序列模式来捕获项目之间的上下文转换，并利用成对视图作为本地项目-项目关系的特征模式来探索会话中的特征共性。在三个真实世界的公共数据集上的大量实验证明了 KMVG 的优越性，表明它的性能优于最先进的基线。进一步的分析还揭示了 KMVG 在多视图下利用项目-项目关系的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge-enhanced+Multi-View+Graph+Neural+Networks+for+Session-based+Recommendation)|0|
|[Multi-behavior Self-supervised Learning for Recommendation](https://doi.org/10.1145/3539618.3591734)|Jingcao Xu, Chaokun Wang, Cheng Wu, Yang Song, Kai Zheng, Xiaowei Wang, Changping Wang, Guorui Zhou, Kun Gai||Modern recommender systems often deal with a variety of user interactions, e.g., click, forward, purchase, etc., which requires the underlying recommender engines to fully understand and leverage multi-behavior data from users. Despite recent efforts towards making use of heterogeneous data, multi-behavior recommendation still faces great challenges. Firstly, sparse target signals and noisy auxiliary interactions remain an issue. Secondly, existing methods utilizing self-supervised learning (SSL) to tackle the data sparsity neglect the serious optimization imbalance between the SSL task and the target task. Hence, we propose a Multi-Behavior Self-Supervised Learning (MBSSL) framework together with an adaptive optimization method. Specifically, we devise a behavior-aware graph neural network incorporating the self-attention mechanism to capture behavior multiplicity and dependencies. To increase the robustness to data sparsity under the target behavior and noisy interactions from auxiliary behaviors, we propose a novel self-supervised learning paradigm to conduct node self-discrimination at both inter-behavior and intra-behavior levels. In addition, we develop a customized optimization strategy through hybrid manipulation on gradients to adaptively balance the self-supervised learning task and the main supervised recommendation task. Extensive experiments on five real-world datasets demonstrate the consistent improvements obtained by MBSSL over ten state-of-the art (SOTA) baselines. We release our model implementation at: https://github.com/Scofield666/MBSSL.git.|现代推荐系统经常处理各种用户交互，例如点击、转发、购买等，这需要底层推荐引擎完全理解和利用来自用户的多行为数据。尽管最近努力利用异构数据，多行为推荐仍然面临巨大的挑战。首先，稀疏目标信号和噪声辅助交互作用仍然是一个问题。其次，现有的利用自监督学习(SSL)解决数据稀疏问题的方法忽略了 SSL 任务与目标任务之间存在严重的优化不平衡。因此，我们提出了一个多行为自我监督学习(MBSSL)框架和自适应优化方法。具体来说，我们设计了一个行为感知图形神经网络结合自我注意机制，以捕捉行为的多样性和依赖性。为了提高目标行为和辅助行为引起的噪声干扰下对数据稀疏性的鲁棒性，提出了一种新的自监督学习范式，在行为间和行为内两个层次上进行节点自辨识。此外，通过对梯度的混合操作，我们开发了一个定制的优化策略，以自适应地平衡自监督学习任务和主要的监督推荐任务。在五个真实世界数据集上的大量实验证明了 MBSSL 在超过十个最先进的(SOTA)基线上获得的一致性改进。我们在以下 https://github.com/scofield666/mbssl.git 发布我们的模型实现。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-behavior+Self-supervised+Learning+for+Recommendation)|0|
|[LOAM: Improving Long-tail Session-based Recommendation via Niche Walk Augmentation and Tail Session Mixup](https://doi.org/10.1145/3539618.3591718)|Heeyoon Yang, YunSeok Choi, Gahyung Kim, JeeHyong Lee|Sungkyunkwan University, Suwon, Republic of Korea|Session-based recommendation aims to predict the user's next action based on anonymous sessions without using side information. Most of the real-world session datasets are sparse and have long-tail item distribution. Although long-tail item recommendation plays a crucial role in improving user satisfaction, only a few methods have been proposed to take the long-tail session recommendation into consideration. Previous works in handling data sparsity problems are mostly limited to self-supervised learning techniques with heuristic augmentation which can ruin the original characteristic of session datasets, sequential and co-occurrences, and make noisier short sessions by dropping items and cropping sequences. We propose a novel method, LOAM, improving LOng-tail session-based recommendation via niche walk Augmentation and tail session Mixup, that alleviates popularity bias and enhances long-tail recommendation performance. LOAM consists of two modules, Niche Walk Augmentation (NWA) and Tail Session Mixup (TSM). NWA can generate synthetic sessions considering long-tail distribution which are likely to be found in original datasets, unlike previous heuristic methods, and expose a recommender model to various item transitions with global information. This improves the item coverage of recommendations. TSM makes the model more generalized and robust by interpolating sessions at the representation level. It encourages the recommender system to predict niche items with more diversity and relevance. We conduct extensive experiments with four real-world datasets and verify that our methods greatly improve tail performance while balancing overall performance.|基于会话的推荐系统旨在基于匿名会话预测用户的下一步操作，而不使用侧信息。大多数真实世界的会话数据集是稀疏的，并且具有长尾项分布。尽管长尾条目推荐在提高用户满意度方面发挥着关键作用，但只有少数几种方法被提出来考虑长尾会话推荐。以往处理数据稀疏问题的工作大多局限于采用启发式增强的自监督学习技术，这种方法会破坏会话数据集的原有特性，使得会话数据具有连续性和共现性，并且通过丢弃项目和剪切序列使得会话变得更加嘈杂。我们提出了一种新的方法 LOAM，通过小生境步行增强和尾部会话混合来改进基于长尾会话的推荐，减少了流行偏差，提高了长尾会话的推荐性能。LOAM 包括两个模块，小生境步行增强(NWA)和尾会话混合(TSM)。与以往的启发式方法不同，NWA 可以考虑原始数据集中可能出现的长尾分布，生成综合会话，并将推荐模型暴露给具有全局信息的各种项目转换。这提高了建议的项目覆盖率。TSM 通过在表示层次上插入会话，使模型更加通用和健壮。它鼓励推荐系统预测更具多样性和相关性的利基项目。我们对四个实际数据集进行了广泛的实验，验证了我们的方法在平衡整体性能的同时大大提高了尾部性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LOAM:+Improving+Long-tail+Session-based+Recommendation+via+Niche+Walk+Augmentation+and+Tail+Session+Mixup)|0|
|[An Offline Metric for the Debiasedness of Click Models](https://doi.org/10.1145/3539618.3591639)|Romain Deffayet, Philipp Hager, JeanMichel Renders, Maarten de Rijke||A well-known problem when learning from user clicks are inherent biases prevalent in the data, such as position or trust bias. Click models are a common method for extracting information from user clicks, such as document relevance in web search, or to estimate click biases for downstream applications such as counterfactual learning-to-rank, ad placement, or fair ranking. Recent work shows that the current evaluation practices in the community fail to guarantee that a well-performing click model generalizes well to downstream tasks in which the ranking distribution differs from the training distribution, i.e., under covariate shift. In this work, we propose an evaluation metric based on conditional independence testing to detect a lack of robustness to covariate shift in click models. We introduce the concept of debiasedness and a metric for measuring it. We prove that debiasedness is a necessary condition for recovering unbiased and consistent relevance scores and for the invariance of click prediction under covariate shift. In extensive semi-synthetic experiments, we show that our proposed metric helps to predict the downstream performance of click models under covariate shift and is useful in an off-policy model selection setting.|当从用户点击中学习时，一个众所周知的问题是数据中普遍存在的固有偏差，如位置偏差或信任偏差。点击模型是一种从用户点击中提取信息的常用方法，比如网络搜索中的文档相关性，或者评估下游应用的点击偏差，比如反事实学习排名、广告位置或公平排名。最近的研究表明，目前社区中的评估实践不能保证一个良好的点击模型很好地推广到下游任务，其中排名分布不同于训练分布，即在协变量转移。在这项工作中，我们提出了一个基于条件独立测试的评估指标来检测点击模型中缺乏协变量转移的稳健性。我们介绍了偏差的概念和一个衡量它的度量。我们证明了偏差是恢复无偏和一致相关分数的必要条件，以及点击预测在协变量移动下的不变性。在广泛的半综合实验中，我们表明，我们提出的度量有助于预测下游性能的点击模型下的协变量移动，是有用的非策略模型选择设置。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Offline+Metric+for+the+Debiasedness+of+Click+Models)|0|
|[InceptionXML: A Lightweight Framework with Synchronized Negative Sampling for Short Text Extreme Classification](https://doi.org/10.1145/3539618.3591699)|Siddhant Kharbanda, Atmadeep Banerjee, Devaansh Gupta, Akash Palrecha, Rohit Babbar|Aalto University & Microsoft Corporation, Espoo, Finland; Aalto University & University of Bath, Espoo, Finland; Aalto University, Espoo, Finland; Aalto University & BITS Pilani, Espoo, Finland|Automatic annotation of short-text data to a large number of target labels, referred to as Short Text Extreme Classification, has found numerous applications including prediction of related searches and product recommendation. In this paper, we propose a convolutional architecture InceptionXML which is light-weight, yet powerful, and robust to the inherent lack of word-order in short-text queries encountered in search and recommendation. We demonstrate the efficacy of applying convolutions by recasting the operation along the embedding dimension instead of the word dimension as applied in conventional CNNs for text classification. Towards scaling our model to datasets with millions of labels, we also propose SyncXML pipeline which improves upon the shortcomings of the recently proposed dynamic hard-negative mining technique for label shortlisting by synchronizing the label-shortlister and extreme classifier. SyncXML not only reduces the inference time to half but is also an order of magnitude smaller than state-of-the-art Astec in terms of model size. Through a comprehensive empirical comparison, we show that not only can InceptionXML outperform existing approaches on benchmark datasets but also the transformer baselines requiring only 2% FLOPs. The code for InceptionXML is available at https://github.com/xmc-aalto.|将短文本数据自动标注到大量的目标标签，即短文本极端分类，已经发现了许多应用程序，包括相关搜索的预测和产品推荐。在本文中，我们提出了一个卷积体系结构 InceptionXML，它是轻量级的，但强大的，和健壮的固有缺乏词序的短文本查询遇到的搜索和推荐。我们通过重铸嵌入维数的卷积运算来代替传统的词维数用于文本分类，从而证明了卷积运算在文本分类中的有效性。为了将我们的模型扩展到具有数百万个标签的数据集，我们还提出了 SyncXML 流水线，通过同步标签短列表和极端分类器，改进了最近提出的标签短列表动态硬负面挖掘技术的缺点。SyncXML 不仅将推理时间减少了一半，而且在模型大小方面也比最先进的 Astec 数量级小。通过全面的实证比较，我们发现 InceptionXML 不仅在基准数据集上优于现有的方法，而且在只需要2% FLOP 的转换器基线上也优于现有的方法。有关 InceptionXML 的代码可于 https://github.com/xmc-aalto 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=InceptionXML:+A+Lightweight+Framework+with+Synchronized+Negative+Sampling+for+Short+Text+Extreme+Classification)|0|
|[Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding](https://doi.org/10.1145/3539618.3591782)|Susik Yoon, Dongha Lee, Yunyi Zhang, Jiawei Han||Unsupervised discovery of stories with correlated news articles in real-time helps people digest massive news streams without expensive human annotations. A common approach of the existing studies for unsupervised online story discovery is to represent news articles with symbolic- or graph-based embedding and incrementally cluster them into stories. Recent large language models are expected to improve the embedding further, but a straightforward adoption of the models by indiscriminately encoding all information in articles is ineffective to deal with text-rich and evolving news streams. In this work, we propose a novel thematic embedding with an off-the-shelf pretrained sentence encoder to dynamically represent articles and stories by considering their shared temporal themes. To realize the idea for unsupervised online story discovery, a scalable framework USTORY is introduced with two main techniques, theme- and time-aware dynamic embedding and novelty-aware adaptive clustering, fueled by lightweight story summaries. A thorough evaluation with real news data sets demonstrates that USTORY achieves higher story discovery performances than baselines while being robust and scalable to various streaming settings.|不受监督地实时发现与相关新闻文章相关的故事，可以帮助人们消化大量的新闻流，而不需要昂贵的人工注释。现有的无监督在线故事发现研究的一种常用方法是使用基于符号或图形的嵌入方法来表示新闻文章，并逐步地将它们聚类到故事中。最近的大型语言模型预计将进一步改善嵌入，但直接采用模型，不加区分地编码文章中的所有信息，对处理文本丰富和不断变化的新闻流是无效的。在这项工作中，我们提出了一个新的主题嵌入与现成的预先训练的句子编码器，以动态表示文章和故事，考虑其共享的时间主题。为了实现无监督在线故事发现的思想，引入了一个可扩展框架 USTORY，该框架采用了两种主要技术: 主题和时间感知的动态嵌入和新颖感知的自适应聚类，并以轻量级故事摘要为基础。使用真实新闻数据集进行的全面评估表明，USTORY 实现了比基线更高的故事发现性能，同时对各种流设置具有鲁棒性和可扩展性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unsupervised+Story+Discovery+from+Continuous+News+Streams+via+Scalable+Thematic+Embedding)|0|
|[When Newer is Not Better: Does Deep Learning Really Benefit Recommendation From Implicit Feedback?](https://doi.org/10.1145/3539618.3591785)|Yushun Dong, Jundong Li, Tobias Schnabel||In recent years, neural models have been repeatedly touted to exhibit state-of-the-art performance in recommendation. Nevertheless, multiple recent studies have revealed that the reported state-of-the-art results of many neural recommendation models cannot be reliably replicated. A primary reason is that existing evaluations are performed under various inconsistent protocols. Correspondingly, these replicability issues make it difficult to understand how much benefit we can actually gain from these neural models. It then becomes clear that a fair and comprehensive performance comparison between traditional and neural models is needed. Motivated by these issues, we perform a large-scale, systematic study to compare recent neural recommendation models against traditional ones in top-n recommendation from implicit data. We propose a set of evaluation strategies for measuring memorization performance, generalization performance, and subgroup-specific performance of recommendation models. We conduct extensive experiments with 13 popular recommendation models (including two neural models and 11 traditional ones as baselines) on nine commonly used datasets. Our experiments demonstrate that even with extensive hyper-parameter searches, neural models do not dominate traditional models in all aspects, e.g., they fare worse in terms of average HitRate. We further find that there are areas where neural models seem to outperform non-neural models, for example, in recommendation diversity and robustness between different subgroups of users and items. Our work illuminates the relative advantages and disadvantages of neural models in recommendation and is therefore an important step towards building better recommender systems.|近年来，神经模型一再被吹捧为具有最先进性能的推荐模型。然而，最近的多项研究表明，许多神经推荐模型的报告的最新结果不能可靠地复制。一个主要原因是现有的评估是在各种不一致的协议下执行的。相应地，这些可复制性问题使我们很难理解我们实际上能从这些神经模型中获得多少好处。因此，很明显，需要在传统模型和神经模型之间进行公平和全面的性能比较。在这些问题的激励下，我们进行了一项大规模的系统研究，以比较最近的神经推荐模型和传统的顶部 n 推荐模型的隐含数据。我们提出了一套评估策略，用于测量记忆性能、概括性能和推荐模型的子组特定性能。我们在9个常用的数据集上对13个流行的推荐模型(包括2个神经模型和11个传统模型作为基线)进行了广泛的实验。我们的实验表明，即使有广泛的超参数搜索，神经模型也不能在所有方面支配传统模型，例如，它们在平均 HitRate 方面的表现更差。我们进一步发现，在一些领域，神经模型似乎比非神经模型表现得更好，例如，在不同的用户和项目子群之间的推荐多样性和鲁棒性。我们的工作阐明了神经模型在推荐中的相对优缺点，因此是建立更好的推荐系统的重要一步。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=When+Newer+is+Not+Better:+Does+Deep+Learning+Really+Benefit+Recommendation+From+Implicit+Feedback?)|0|
|[Session Search with Pre-trained Graph Classification Model](https://doi.org/10.1145/3539618.3591766)|Shengjie Ma, Chong Chen, Jiaxin Mao, Qi Tian, Xuhui Jiang|Renmin University of China, Beijing, China; Huawei Cloud BU, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Huawei Cloud BU, Shenzhen, China|Session search is a widely adopted technique in search engines that seeks to leverage the complete interaction history of a search session to better understand the information needs of users and provide more relevant ranking results. The vast majority of existing methods model a search session as a sequence of queries and previously clicked documents. However, if we simply represent a search session as a sequence we will lose the topological information in the original search session. It is non-trivial to model the intra-session interactions and complicated structural patterns among the previously issued queries, clicked documents, as well as the terms or entities that appeared in them. To solve this problem, in this paper, we propose a novel Session Search with Graph Classification Model (SSGC), which regards session search as a graph classification task on a heterogeneous graph that represents the search history in each session. To improve the performance of the graph classification, we design a specific pre-training strategy for our proposed GNN-based classification model. Extensive experiments on two public session search datasets demonstrate the effectiveness of our model in the session search task.|会话搜索是搜索引擎中广泛采用的一种技术，旨在利用搜索会话的完整交互历史，更好地理解用户的信息需求，并提供更相关的排名结果。绝大多数现有方法将搜索会话建模为一系列查询和先前单击的文档。但是，如果我们仅仅将一个搜索会话表示为一个序列，我们将丢失原始搜索会话中的拓扑信息。对以前发出的查询、单击的文档以及它们中出现的术语或实体之间的会话内交互和复杂的结构模式建模是非常重要的。为了解决这一问题，本文提出了一种新的基于图分类模型的会话搜索(SSGC)方法，该方法将会话搜索作为异构图上的一个图分类任务，表示每个会话中的搜索历史。为了提高图的分类性能，我们针对所提出的基于 GNN 的分类模型设计了一种特定的预训练策略。在两个公开会话搜索数据集上的大量实验证明了该模型在会话搜索任务中的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Session+Search+with+Pre-trained+Graph+Classification+Model)|0|
|[Leveraging Transferable Knowledge Concept Graph Embedding for Cold-Start Cognitive Diagnosis](https://doi.org/10.1145/3539618.3591774)|Weibo Gao, Hao Wang, Qi Liu, Fei Wang, Xin Lin, Linan Yue, Zheng Zhang, Rui Lv, Shijin Wang|University of Science and Technology of China, Hefei, China; iFLYTEK CO., LTD, Hefei, China|Cognitive diagnosis (CD) aims to reveal the proficiency of students on specific knowledge concepts and traits of test exercises (e.g., difficulty). It plays a critical role in intelligent education systems by supporting personalized learning guidance. However, recent developments in CD mostly concentrate on improving the accuracy of diagnostic results and often overlook the important and practical task: domain-level zero-shot cognitive diagnosis (DZCD). The primary challenge of DZCD is the deficiency of student behavior data in the target domain due to the absence of student-exercise interactions or unavailability of exercising records for training purposes. To tackle the cold-start issue, we propose a two-stage solution named TechCD (Transferable knowledgE Concept grapH embedding framework for Cognitive Diagnosis). The fundamental notion involves utilizing a pedagogical knowledge concept graph (KCG) as a mediator to connect disparate domains, allowing the transmission of student cognitive signals from established domains to the zero-shot cold-start domain. Specifically, a naive yet effective graph convolutional network (GCN) with the bottom-layer discarding operation is initially employed over the KCG to learn transferable student cognitive states and domain-specific exercise traits. Moreover, we give three implementations of the general TechCD framework following the typical cognitive diagnosis solutions. Finally, extensive experiments on real-world datasets not only prove that Tech can effectively perform zero-shot diagnosis, but also give some popular applications such as exercise recommendation.|认知诊断(CD)旨在揭示学生对特定知识概念的熟练程度和测验题的特点(例如难度)。它通过支持个性化学习指导在智能教育系统中发挥着关键作用。然而，近年来 CD 的发展主要集中在提高诊断结果的准确性上，而往往忽视了领域级零点认知诊断(DZCD)这一重要而实际的任务。DZCD 的主要挑战是由于缺乏学生与练习的交互作用，或者缺乏用于训练目的的练习记录，导致目标领域的学生行为数据不足。为了解决冷启动问题，我们提出了一个两阶段的解决方案 TechCD (用于认知诊断的可转移知识概念图嵌入框架)。基本概念包括利用教学知识概念图(KCG)作为中介连接不同的领域，允许学生的认知信号从建立的领域传输到零拍冷启动领域。具体来说，在 KCG 上首先使用一个具有底层丢弃操作的幼稚而有效的图卷积网络(GCN)来学习可转移的学生认知状态和领域特定的练习特征。此外，在典型的认知诊断解决方案的基础上，我们给出了三种通用 TechCD 框架的实现。最后，在实际数据集上进行了大量的实验，不仅证明了 Tech 可以有效地进行零点诊断，而且给出了一些常用的应用，如运动推荐。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leveraging+Transferable+Knowledge+Concept+Graph+Embedding+for+Cold-Start+Cognitive+Diagnosis)|0|
|[Editable User Profiles for Controllable Text Recommendations](https://doi.org/10.1145/3539618.3591677)|Sheshera Mysore, Mahmood Jasim, Andrew McCallum, Hamed Zamani||Methods for making high-quality recommendations often rely on learning latent representations from interaction data. These methods, while performant, do not provide ready mechanisms for users to control the recommendation they receive. Our work tackles this problem by proposing LACE, a novel concept value bottleneck model for controllable text recommendations. LACE represents each user with a succinct set of human-readable concepts through retrieval given user-interacted documents and learns personalized representations of the concepts based on user documents. This concept based user profile is then leveraged to make recommendations. The design of our model affords control over the recommendations through a number of intuitive interactions with a transparent user profile. We first establish the quality of recommendations obtained from LACE in an offline evaluation on three recommendation tasks spanning six datasets in warm-start, cold-start, and zero-shot setups. Next, we validate the controllability of LACE under simulated user interactions. Finally, we implement LACE in an interactive controllable recommender system and conduct a user study to demonstrate that users are able to improve the quality of recommendations they receive through interactions with an editable user profile.|提出高质量建议的方法通常依赖于从交互数据中学习潜在的表示。这些方法虽然具有良好的性能，但是并没有为用户提供现成的机制来控制他们收到的推荐。我们的工作解决了这个问题，提出了 LACE，一个新的概念价值瓶颈模型的可控文本推荐。通过检索给定的用户交互文档，LACE 用一组简洁的人类可读的概念表示每个用户，并学习基于用户文档的概念的个性化表示。然后利用这个基于概念的用户配置文件来提出建议。我们的模型的设计通过与透明用户配置文件的直观交互提供了对推荐的控制。我们首先建立从 LACE 获得的建议的质量，在一个离线评估中，对三个建议任务进行评估，这三个任务跨越六个数据集，分别是暖启动、冷启动和零启动设置。接下来，我们验证了 LACE 在模拟用户交互下的可控性。最后，我们以交互式可控推荐系统实施 LACE，并进行用户研究，以证明用户能够通过与可编辑的用户资料进行交互，提高他们收到的建议的质量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Editable+User+Profiles+for+Controllable+Text+Recommendations)|0|
|[Keyword-Based Diverse Image Retrieval by Semantics-aware Contrastive Learning and Transformer](https://doi.org/10.1145/3539618.3591705)|Minyi Zhao, Jinpeng Wang, Dongliang Liao, Yiru Wang, Huanzhong Duan, Shuigeng Zhou||In addition to relevance, diversity is an important yet less studied performance metric of cross-modal image retrieval systems, which is critical to user experience. Existing solutions for diversity-aware image retrieval either explicitly post-process the raw retrieval results from standard retrieval systems or try to learn multi-vector representations of images to represent their diverse semantics. However, neither of them is good enough to balance relevance and diversity. On the one hand, standard retrieval systems are usually biased to common semantics and seldom exploit diversity-aware regularization in training, which makes it difficult to promote diversity by post-processing. On the other hand, multi-vector representation methods are not guaranteed to learn robust multiple projections. As a result, irrelevant images and images of rare or unique semantics may be projected inappropriately, which degrades the relevance and diversity of the results generated by some typical algorithms like top-k. To cope with these problems, this paper presents a new method called CoLT that tries to generate much more representative and robust representations for accurately classifying images. Specifically, CoLT first extracts semantics-aware image features by enhancing the preliminary representations of an existing one-to-one cross-modal system with semantics-aware contrastive learning. Then, a transformer-based token classifier is developed to subsume all the features into their corresponding categories. Finally, a post-processing algorithm is designed to retrieve images from each category to form the final retrieval result. Extensive experiments on two real-world datasets Div400 and Div150Cred show that CoLT can effectively boost diversity, and outperforms the existing methods as a whole (with a higher F1 score).|除了相关性之外，多样性是跨模态图像检索系统中一个重要但研究较少的性能指标，它对用户体验至关重要。现有的基于多样性的图像检索解决方案要么对标准检索系统的原始检索结果进行显式的后处理，要么尝试学习图像的多向量表示来表示图像的多样性语义。然而，它们都不足以平衡相关性和多样性。一方面，标准检索系统往往偏向于通用语义，在训练中很少采用多样性感知的正则化方法，这使得后处理难以提高多样性。另一方面，多向量表示方法不能保证学习鲁棒的多重投影。因此，不相关的图像和罕见或唯一语义的图像可能会被不适当地投影，从而降低了由 top-k 等典型算法产生的结果的相关性和多样性。为了解决这些问题，本文提出了一种称为 CoLT 的新方法，该方法试图生成更具代表性和鲁棒性的图像表示，从而实现图像的准确分类。具体来说，CoLT 首先提取语义感知的图像特征，通过增强现有的具有语义感知对比学习的一对一交叉模式系统的初步表示。然后，开发了一个基于转换器的令牌分类器，将所有特征归入相应的类别。最后，设计了一个后处理算法来检索每个类别的图像，形成最终的检索结果。在 Div400和 Div150Cred 两个实际数据集上的大量实验表明，CoLT 能够有效地提高多样性，并且整体上优于现有的方法(具有更高的 F1分数)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Keyword-Based+Diverse+Image+Retrieval+by+Semantics-aware+Contrastive+Learning+and+Transformer)|0|
|[Next Basket Recommendation with Intent-aware Hypergraph Adversarial Network](https://doi.org/10.1145/3539618.3591742)|Ran Li, Liang Zhang, Guannan Liu, Junjie Wu|Nanyang Technological University, Singapore, Singapore; Beihang University, Beijing, China; Beihang University & Key Laboratory of Data Intelligence and Management, MIIT, Beiing, China; Beihang University & Key Laboratory of Data Intelligence and Management, MIIT, Bejing, China|Next Basket Recommendation (NBR) that recommends a basket of items to users has become a promising promotion artifice for online businesses. The key challenge of NBR is rooted in the complicated relations of items that are dependent on one another in a same basket with users' diverse purchasing intentions, which goes far beyond the pairwise item relations in traditional recommendation tasks, and yet has not been well addressed by existing NBR methods that mostly model the inter-basket item relations only. To that end, in this paper, we construct a hypergraph from basket-wise purchasing records and probe the inter-basket and intra-basket item relations behind the hyperedges. In particular, we combine the strength of HyperGraph Neural Network with disentangled representation learning to derive the intent-aware representations of hyperedges for characterizing the nuances of user purchasing patterns. Moreover, considering the information loss in traditional item-wise optimization, we propose a novel basket-wise optimization scheme via an adversarial network to generate high-quality negative baskets. Extensive experiments conducted on four different data sets demonstrate the superior performances over the state-of-the-art NBR methods. Notably, our method is shown to strike a good balance in recommending both repeated and explorative items as a basket.|下一个篮子推荐(NBR)向用户推荐一篮子商品已经成为在线商务的一个有前途的促销手段。NBR 的主要挑战来自于同一篮子中相互依赖的商品之间的复杂关系，以及用户不同的购买意图，这种复杂关系远远超出了传统推荐任务中的成对商品关系，但是现有的 NBR 方法并没有很好地解决这个问题，因为大多数 NBR 方法只是模拟篮子间的商品关系。为此，在本文中，我们从篮子购买记录构造了一个超图，并探讨了超边背后的篮子间和篮子内的项目关系。特别是，我们结合超图神经网络的力量和分离表示学习来推导超边界的意图感知表示，以表征用户购买模式的细微差别。此外，考虑到传统项目优化中的信息损失，本文提出了一种新的基于对抗网络的篮子优化方案来生成高质量的负篮子。在四个不同的数据集上进行的大量实验表明，该方法的性能优于目前最先进的 NBR 方法。值得注意的是，我们的方法在推荐重复项目和探索性项目作为一个篮子方面取得了很好的平衡。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Next+Basket+Recommendation+with+Intent-aware+Hypergraph+Adversarial+Network)|0|
|[AutoTransfer: Instance Transfer for Cross-Domain Recommendations](https://doi.org/10.1145/3539618.3591701)|Jingtong Gao, Xiangyu Zhao, Bo Chen, Fan Yan, Huifeng Guo, Ruiming Tang|Huawei Noah's Ark Lab, Shenzhen, China; City University of Hong Kong, Hong Kong, China|Cross-Domain Recommendation (CDR) is a widely used approach for leveraging information from domains with rich data to assist domains with insufficient data. A key challenge of CDR research is the effective and efficient transfer of helpful information from source domain to target domain. Currently, most existing CDR methods focus on extracting implicit information from the source domain to enhance the target domain. However, the hidden structure of the extracted implicit information is highly dependent on the specific CDR model, and is therefore not easily reusable or transferable. Additionally, the extracted implicit information only appears within the intermediate substructure of specific CDRs during training and is thus not easily retained for more use. In light of these challenges, this paper proposes AutoTransfer, with an Instance Transfer Policy Network, to selectively transfers instances from source domain to target domain for improved recommendations. Specifically, AutoTransfer acts as an agent that adaptively selects a subset of informative and transferable instances from the source domain. Notably, the selected subset possesses extraordinary re-utilization property that can be saved for improving model training of various future RS models in target domain. Experimental results on two public CDR benchmark datasets demonstrate that the proposed method outperforms state-of-the-art CDR baselines and classic Single-Domain Recommendation (SDR) approaches. The implementation code is available for easy reproduction.|跨域推荐(CDR)是一种广泛使用的方法，用于利用来自具有丰富数据的域的信息来帮助数据不足的域。CDR 研究的一个关键挑战是有效和高效地将有用的信息从源域传递到目标域。目前，大多数 CDR 方法主要从源域中提取隐式信息来增强目标域。然而，提取的隐含信息的隐藏结构高度依赖于特定的 CDR 模型，因此不容易重用或转移。此外，提取的隐含信息只出现在训练期间特定 CDR 的中间子结构中，因此不容易保留以供更多使用。针对这些挑战，本文提出使用实例传输策略网络的自动传输，选择性地将实例从源域传输到目标域，以获得改进的建议。具体来说，AutoTransfer 充当一个代理，自适应地从源域中选择信息丰富和可转移实例的子集。值得注意的是，所选择的子集具有非凡的再利用特性，可以用来改进目标域中各种未来 RS 模型的模型训练。在两个公共 CDR 基准数据集上的实验结果表明，该方法的性能优于现有的 CDR 基准和经典的单域推荐(SDR)方法。实现代码可以很容易地复制。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AutoTransfer:+Instance+Transfer+for+Cross-Domain+Recommendations)|0|
|[Distillation-Enhanced Graph Masked Autoencoders for Bundle Recommendation](https://doi.org/10.1145/3539618.3591666)|Yuyang Ren, Haonan Zhang, Luoyi Fu, Xinbing Wang, Chenghu Zhou|Shanghai Jiao Tong University, Shanghai, China; ; Institute of Geographical Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, China|Bundle recommendation aims to recommend a bundle of items to users as a whole with user-bundle (U-B) interaction information, and auxiliary user-item (U-I) interaction and bundle-item affiliation information. Recent methods usually use two graph neural networks (GNNs) to model user's bundle preferences separately from the U-B graph (bundle view) and U-I graph (item view). However, by conducting statistical analysis, we find that the auxiliary U-I information is far underexplored due to the following reasons: 1) Loosely combining the predicted results cannot well synthesize the knowledge from both views. 2) The local U-B and U-I collaborative relations might not be consistent, leading to GNN's inaccurate modeling of user's bundle preference from the U-I graph. 3) The U-I interactions are usually modeled equally while the significant ones corresponding to user's bundle preference are less emphasized. Based on these analyses, we propose a Distillation-enhanced Graph Masked AutoEncoder (DGMAE) for bundle recommendation. Our framework extracts the knowledge of first- and higher-order U-B relations from the U-B graph and injects it into a well-designed graph masked autoencoder (student model). The student model is built with two key designs to jointly capture significant local and global U-I relations from the U-I graph. In specific, we design a transformer-enhanced GNN encoder for global relation learning, which increases the model's representational power of depicting user's bundle preferences. Meanwhile, an adaptive edge masking strategy and reconstruction target are designed on the significant U-I edges to guide the student model to identify the potential ones suggesting user's bundle preferences. Extensive experiments on benchmark datasets show the significant improvements of DGMAE over the SOTA methods.|Bundle 推荐的目的是向用户作为一个整体推荐一个包，包含用户包(U-B)交互信息、辅助用户项(U-I)交互信息和包项关联信息。最近的方法通常使用两个图神经网络(GNN)来模型用户的捆绑偏好分别从 U-B 图(捆绑视图)和 U-I 图(项目视图)。然而，通过统计分析发现，由于以下原因，辅助 U-I 信息的开发远远不够: 1)松散地结合预测结果不能很好地综合两种观点的知识。2)本地 U-B 和 U-I 协作关系可能不一致，导致 GNN 从 U-I 图中对用户捆绑偏好建模不准确。3) U-I 交互通常是均匀建模的，而与用户捆绑包偏好相对应的重要交互通常较少被强调。基于这些分析，我们提出了一种用于捆绑推荐的蒸馏增强型图掩码自动编码器(DGMAE)。我们的框架从 U-B 图中提取出一阶和高阶 U-B 关系的知识，并将其注入到一个设计良好的图掩码自动编码器(学生模型)中。学生模型由两个关键设计构建，从 U-I 图中共同捕获重要的本地和全球 U-I 关系。具体来说，我们设计了一个用于全局关系学习的变压器增强型 GNN 编码器，增强了模型描述用户束偏好的表示能力。同时，在显著的 U-I 边缘上设计了自适应边缘掩蔽策略和重构目标，以指导学生模型识别提示用户捆绑偏好的潜在边缘。在基准数据集上的大量实验表明，DGMAE 方法比 SOTA 方法有明显的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distillation-Enhanced+Graph+Masked+Autoencoders+for+Bundle+Recommendation)|0|
|[Popularity Debiasing from Exposure to Interaction in Collaborative Filtering](https://doi.org/10.1145/3539618.3591947)|Yuanhao Liu, Qi Cao, Huawei Shen, Yunfan Wu, Shuchang Tao, Xueqi Cheng||Recommender systems often suffer from popularity bias, where popular items are overly recommended while sacrificing unpopular items. Existing researches generally focus on ensuring the number of recommendations exposure of each item is equal or proportional, using inverse propensity weighting, causal intervention, or adversarial training. However, increasing the exposure of unpopular items may not bring more clicks or interactions, resulting in skewed benefits and failing in achieving real reasonable popularity debiasing. In this paper, we propose a new criterion for popularity debiasing, i.e., in an unbiased recommender system, both popular and unpopular items should receive Interactions Proportional to the number of users who Like it, namely IPL criterion. Under the guidance of the criterion, we then propose a debiasing framework with IPL regularization term which is theoretically shown to achieve a win-win situation of both popularity debiasing and recommendation performance. Experiments conducted on four public datasets demonstrate that when equipping two representative collaborative filtering models with our framework, the popularity bias is effectively alleviated while maintaining the recommendation performance.|推荐系统经常受到受欢迎程度偏差的影响，受欢迎的项目被过度推荐，而牺牲了不受欢迎的项目。现有的研究通常集中在确保每个项目的建议暴露数量相等或成比例，使用反倾向权重，因果干预，或对抗性训练。然而，增加不受欢迎的项目的曝光可能不会带来更多的点击或互动，导致扭曲的利益和未能实现真正合理的流行去偏见。在这篇文章中，我们提出了一个新的减低受欢迎程度的准则，即在一个不偏不倚的推荐系统下，受欢迎和不受欢迎的项目都应该接受与喜欢它的用户数成比例的互动，即 IPL 准则。在该准则的指导下，我们提出了一个带有 IPL 正则项的去偏框架，理论上证明了该框架可以实现人气去偏和推荐性能的双赢。在四个公共数据集上进行的实验表明，当在我们的框架中安装两个具有代表性的协同过滤模型时，在保持推荐性能的同时有效地减少了流行偏差。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Popularity+Debiasing+from+Exposure+to+Interaction+in+Collaborative+Filtering)|0|
|[AutoDPQ: Automated Differentiable Product Quantization for Embedding Compression](https://doi.org/10.1145/3539618.3591953)|Xin Gan, Yuhao Wang, Xiangyu Zhao, Wanyu Wang, Yiqi Wang, Zitao Liu|Guangdong Institute of Smart Education, Jinan University, Guangzhou, China; National University of Defense Technology, Changsha, China; City University of Hong Kong, Hong Kong, China|Deep recommender systems typically involve numerous feature fields for users and items, with a large number of low-frequency features. These low-frequency features would reduce the prediction accuracy with large storage space due to their vast quantity and inadequate training. Some pioneering studies have explored embedding compression techniques to address this issue of the trade-off between storage space and model predictability. However, these methods have difficulty compacting the embedding of low-frequency features in various feature fields due to the high demand for human experience and computing resources during hyper-parameter searching. In this paper, we propose the AutoDPQ framework, which automatically compacts low-frequency feature embeddings for each feature field to an adaptive magnitude. Experimental results indicate that AutoDPQ can significantly reduce the parameter space while improving recommendation accuracy. Moreover, AutoDPQ is compatible with various deep CTR models by improving their performance significantly with high efficiency.|深度推荐系统通常涉及用户和项目的大量特性字段，以及大量低频特性。这些低频特征由于其数量庞大，训练不足，在存储空间较大的情况下会降低预测精度。一些开创性的研究探索了嵌入式压缩技术来解决存储空间和模型可预测性之间的权衡问题。然而，由于在超参数搜索过程中对人的经验和计算资源的要求很高，这些方法很难将低频特征嵌入到各种特征域中。在本文中，我们提出了 AutoDPQ 框架，它自动压缩每个特征字段的低频特征嵌入到一个自适应的大小。实验结果表明，AutoDPQ 在提高推荐精度的同时，可以显著减少参数空间。此外，AutoDPQ 与各种深度 CTR 模型兼容，大大提高了它们的性能和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AutoDPQ:+Automated+Differentiable+Product+Quantization+for+Embedding+Compression)|0|
|[Can Generative LLMs Create Query Variants for Test Collections? An Exploratory Study](https://doi.org/10.1145/3539618.3591960)|Marwah Alaofi, Luke Gallagher, Mark Sanderson, Falk Scholer, Paul Thomas|RMIT University, Melbourne, VIC, Australia; Microsoft, Adelaide, SA, Australia|This paper explores the utility of a Large Language Model (LLM) to automatically generate queries and query variants from a description of an information need. Given a set of information needs described as backstories, we explore how similar the queries generated by the LLM are to those generated by humans. We quantify the similarity using different metrics and examine how the use of each set would contribute to document pooling when building test collections. Our results show potential in using LLMs to generate query variants. While they may not fully capture the wide variety of human-generated variants, they generate similar sets of relevant documents, reaching up to 71.1% overlap at a pool depth of 100.|本文探讨了大语言模型(LLM)的实用性，该模型根据对信息需求的描述自动生成查询和查询变体。给定一组被描述为背景故事的信息需求，我们探索 LLM 生成的查询与人工生成的查询有多相似。我们使用不同的度量标准量化相似性，并检查在构建测试集合时，每个集合的使用如何有助于文档池。我们的结果显示了使用 LLM 生成查询变量的潜力。虽然它们可能不能完全捕获各种各样的人类产生的变体，但它们生成了类似的相关文档集，在池深度为100的情况下，重叠率高达71.1% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+Generative+LLMs+Create+Query+Variants+for+Test+Collections?+An+Exploratory+Study)|0|
|[Causal Disentangled Variational Auto-Encoder for Preference Understanding in Recommendation](https://doi.org/10.1145/3539618.3591961)|Siyu Wang, Xiaocong Chen, Quan Z. Sheng, Yihong Zhang, Lina Yao||Recommendation models are typically trained on observational user interaction data, but the interactions between latent factors in users' decision-making processes lead to complex and entangled data. Disentangling these latent factors to uncover their underlying representation can improve the robustness, interpretability, and controllability of recommendation models. This paper introduces the Causal Disentangled Variational Auto-Encoder (CaD-VAE), a novel approach for learning causal disentangled representations from interaction data in recommender systems. The CaD-VAE method considers the causal relationships between semantically related factors in real-world recommendation scenarios, rather than enforcing independence as in existing disentanglement methods. The approach utilizes structural causal models to generate causal representations that describe the causal relationship between latent factors. The results demonstrate that CaD-VAE outperforms existing methods, offering a promising solution for disentangling complex user behavior data in recommendation systems.|推荐模型通常基于观测用户交互数据进行训练，但用户决策过程中潜在因素之间的相互作用会导致复杂和纠缠的数据。分离这些潜在因素，揭示它们的基底形式，可以提高推荐模型的稳健性、可解释性和可控性。介绍了一种从推荐系统中的交互数据中学习因果解缠表示的新方法——因果解缠变分自动编码器(CaD-VAE)。在现实推荐场景中，CaD-VAE 方法考虑了语义相关因素之间的因果关系，而不是像现有的分离方法那样强制独立性。该方法利用结构性因果模型来产生描述潜在因素之间因果关系的因果表示。结果表明，CaD-VAE 方法的性能优于现有方法，为推荐系统中复杂用户行为数据的分离提供了一种有前途的解决方案。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causal+Disentangled+Variational+Auto-Encoder+for+Preference+Understanding+in+Recommendation)|0|
|[Connecting Unseen Domains: Cross-Domain Invariant Learning in Recommendation](https://doi.org/10.1145/3539618.3591965)|Yang Zhang, Yue Shen, Dong Wang, Jinjie Gu, Guannan Zhang|Ant Group, Hangzhou, China|As web applications continue to expand and diversify their services, user interactions exist in different scenarios. To leverage this wealth of information, cross-domain recommendation (CDR) has gained significant attention in recent years. However, existing CDR approaches mostly focus on information transfer between observed domains, with little attention paid to generalizing to unseen domains. Although recent research on invariant learning can help for the purpose of generalization, relying only on invariant preference may be overly conservative and result in mediocre performance when the unseen domain shifts slightly. In this paper, we present a novel framework that considers both CDR and domain generalization through a united causal invariant view. We assume that user interactions are determined by domain-invariant preference and domain-specific preference. The proposed approach differentiates the invariant preference and the specific preference from observational behaviors in a way of adversarial learning. Additionally, a novel domain routing module is designed to connect unseen domains to observed domains. Extensive experiments on public and industry datasets have proved the effectiveness of the proposed approach under both CDR and domain generalization settings.|随着 Web 应用程序不断扩展和使其服务多样化，用户交互存在于不同的场景中。为了充分利用这些丰富的信息，跨域推荐(CDR)近年来受到了广泛的关注。然而，现有的 CDR 方法大多侧重于观察域之间的信息传递，很少关注对未知域的推广。虽然近年来对不变学习的研究有助于推广，但仅依赖不变偏好可能会过于保守，当不可见领域发生轻微变化时，学习效果平平。在本文中，我们提出了一个新的框架，通过一个统一的因果不变量的观点，同时考虑 CDR 和领域推广。我们假设用户交互是由领域不变偏好和领域特定偏好决定的。该方法以对抗学习的方式区分不变偏好和特定偏好与观察行为。此外，一个新颖的域路由模组被设计来连接看不见的域到观察到的域。在公共和行业数据集上的大量实验证明了该方法在 CDR 和领域泛化设置下的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Connecting+Unseen+Domains:+Cross-Domain+Invariant+Learning+in+Recommendation)|0|
|[Event-Aware Adaptive Clustering Uplift Network for Insurance Creative Ranking](https://doi.org/10.1145/3539618.3591980)|Wanjie Tao, Huihui Liu, Xuqi Li, Qun Dai, Hong Wen, Zulong Chen|Alibaba Group, Hangzhou, China; Nanjing University of Aeronautics and Astronautics, Nanjing, China|In the classical e-commerce platforms, the personalized product-tying recommendation has proven to be of great added value, which improves users' purchase willingness to product-tying by displaying the suitable marketing creative. In this paper, we present a new recommendation problem, i.e., the Pop-up One-time Marketing (POM), where the product-tying marketing creative only pops up one time when the user pays for the main item. POM has become a ubiquitous application in e-commerce platforms, e.g., buy the mobile tying mobile case and buy flight ticket tying insurance. However, many existing recommendation methods are sub-optimal for the creative marketing in the POM scenario due to unconsidering the unique characteristics in the scenario. To tackle this problem, we propose a novel framework named Event-aware Adaptive Clustering Uplift Network (EACU-Net) for the POM scenario, which is to our best knowledge the first attempt along this line. EACU-Net contains three modules: (1) the event-aware graph cascading learning, which employs a heterogeneous graph network to comprehensively learn the embedding for the user attributes, event categories, and creative elements by stage. (2) an adaptive clustering uplift network, which learns the sensitivity of users to creatives under the same context. (3) an event-aware information gain network to learn more information from samples with event affection. Extensive offline and online evaluations on a real-world e-commerce platform demonstrate the superior performance of the proposed model compared with the state-of-the-art method.|在传统的电子商务平台中，个性化产品绑定推荐被证明具有很大的附加价值，它通过展示合适的营销创意来提高用户的产品绑定购买意愿。本文提出了一个新的推荐问题，即弹出式一次性营销(POM) ，即当用户为主要商品付费时，产品绑定营销创意只会弹出一次。POM 已经成为电子商务平台上无处不在的应用程序，例如，购买移动绑定手机套和购买机票绑定保险。然而，许多现有的推荐方法对于 POM 场景中的创造性营销来说是次优的，因为没有考虑到场景中的独特特征。为了解决这个问题，我们提出了一种新的 POM 场景的事件感知自适应聚类提升网络(EACU-Net)框架，这是我们所知道的沿着这条路线的第一次尝试。EACU-Net 包含三个模块: (1)事件感知图级联学习，该学习采用异构图网络，分阶段全面学习用户属性、事件类别和创新元素的嵌入。(2)自适应聚类提升网络，学习用户在相同情境下对创新的敏感性。(事件感知信息获取网络从具有事件影响的样本中获取更多的信息。在一个真实世界的电子商务平台上进行的大量离线和在线评估表明，与最先进的方法相比，所提出的模型具有更好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Event-Aware+Adaptive+Clustering+Uplift+Network+for+Insurance+Creative+Ranking)|0|
|[Exploiting Cluster-Skipping Inverted Index for Semantic Place Retrieval](https://doi.org/10.1145/3539618.3591983)|Enes Recep Cinar, Ismail Sengor Altingovde|Middle East Technical University, Ankara, Turkey|Semantic place retrieval aims to find the top-k place entities, which are both textually relevant and spatially close to a given query, from a knowledge graph. In this work, our contribution toward improving the efficiency of semantic place retrieval is two-fold. First, we show that by applying an ad hoc yet intuitive restriction on the depth of search on the knowledge graph, it is possible to adopt IR-tree indexing scheme [7], which has been introduced for processing spatial keyword queries, for the semantic place retrieval scenario. Secondly, as a novel solution to this problem, we adapt the idea of cluster-skipping inverted index (CS-IIS) [1, 4], which has been originally proposed for retrieval over topically clustered document collections. Our experiments show that CS-IIS is comparable to IR-tree in terms of CPU time, while it yields substantial efficiency gains in terms of I/O time during query processing.|语义位置检索的目的是从知识图中找到文本相关且在空间上接近于给定查询的前 k 位置实体。在这项工作中，我们对提高语义位置检索的效率的贡献是双重的。首先，我们表明，通过对知识图上的搜索深度应用一个即兴但直观的限制，可以采用 IR-tree 索引方案[7] ，这是为处理空间关键字查询而引入的，用于语义位置检索场景。其次，作为这个问题的一个新的解决方案，我们采用了跳跃聚类倒排索引(CS-IIS)[1,4]的思想，这是最初提出的主题聚类文档集合的检索。我们的实验表明，CS-IIS 在 CPU 时间方面与 IR 树相当，而在查询处理过程中，它在 I/O 时间方面产生了显著的效率提高。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploiting+Cluster-Skipping+Inverted+Index+for+Semantic+Place+Retrieval)|0|
|[Friend Ranking in Online Games via Pre-training Edge Transformers](https://doi.org/10.1145/3539618.3591990)|Liang Yao, Jiazhen Peng, Shenggong Ji, Qiang Liu, Hongyun Cai, Feng He, Xu Cheng||Friend recall is an important way to improve Daily Active Users (DAU) in online games. The problem is to generate a proper lost friend ranking list essentially. Traditional friend recall methods focus on rules like friend intimacy or training a classifier for predicting lost players' return probability, but ignore feature information of (active) players and historical friend recall events. In this work, we treat friend recall as a link prediction problem and explore several link prediction methods which can use features of both active and lost players, as well as historical events. Furthermore, we propose a novel Edge Transformer model and pre-train the model via masked auto-encoders. Our method achieves state-of-the-art results in the offline experiments and online A/B Tests of three Tencent games.|朋友回忆是提高网络游戏日常活跃用户(DAU)水平的重要途径。问题是生成一个适当的失去朋友排名名单本质上。传统的朋友回忆方法侧重于朋友间的亲密关系或训练一个分类器来预测失去的玩家的返回概率等规则，而忽略了(活跃的)玩家的特征信息和历史的朋友回忆事件。本文将好友回忆视为一个链接预测问题，探索了几种既能利用主动玩家特征又能利用丢失玩家特征以及历史事件特征的链接预测方法。此外，我们提出了一个新的边缘变压器模型和预训练的掩码自动编码器的模型。我们的方法在三个腾讯游戏的离线实验和在线 A/B 测试中取得了最先进的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Friend+Ranking+in+Online+Games+via+Pre-training+Edge+Transformers)|0|
|[Generative Relevance Feedback with Large Language Models](https://doi.org/10.1145/3539618.3591992)|Iain Mackie, Shubham Chatterjee, Jeffrey Dalton||Current query expansion models use pseudo-relevance feedback to improve first-pass retrieval effectiveness; however, this fails when the initial results are not relevant. Instead of building a language model from retrieved results, we propose Generative Relevance Feedback (GRF) that builds probabilistic feedback models from long-form text generated from Large Language Models. We study the effective methods for generating text by varying the zero-shot generation subtasks: queries, entities, facts, news articles, documents, and essays. We evaluate GRF on document retrieval benchmarks covering a diverse set of queries and document collections, and the results show that GRF methods significantly outperform previous PRF methods. Specifically, we improve MAP between 5-19% and NDCG@10 17-24% compared to RM3 expansion, and achieve the best R@1k effectiveness on all datasets compared to state-of-the-art sparse, dense, and expansion models.|当前的查询扩展模型使用伪相关反馈来提高首次检索的有效性，但是，当初始结果不相关时，这种方法就会失败。我们建议使用生成关联反馈(Generative) ，从大型语言模型生成的长形文本中建立概率反馈模型，而不是根据检索到的结果建立语言模型。我们研究了通过改变零镜头生成子任务(查询、实体、事实、新闻文章、文档和随笔)来生成文本的有效方法。我们根据涵盖不同查询和文档集合的文献检索基准对 GRF 进行评估，结果显示 GRF 方法明显优于以前的 PRF 方法。具体而言，与 RM3扩展相比，我们改善了5-19% 和 NDCG@1017-24% 之间的 MAP，并且与最先进的稀疏、密集和扩展模型相比，在所有数据集上实现了最佳的 R@1k 效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+Relevance+Feedback+with+Large+Language+Models)|0|
|[Improved Vector Quantization For Dense Retrieval with Contrastive Distillation](https://doi.org/10.1145/3539618.3592001)|James O'Neill, Sourav Dutta|Huawei, Dublin, Ireland|Recent work has identified that distillation can be used to create vector quantization based ANN indexes by learning the inverted file index and product quantization. The argued advantage of using a fixed teacher encoder for queries and documents is that the scores produced by the teacher can be used instead of the label judgements that are required when using traditional supervised learning, such as contrastive learning. However, current work only distills the teacher encoder outputs of dot products between quantized query embedddings and product quantized document embeddings. Our work combines the benefits of contrastive learning and distillation by using contrastive distillation whereby the teacher outputs contrastive scores that the student learns from. Our experimental results on MSMARCO passage retrieval and NQ open question answering datasets show that contrastive distillation improves over current state of the art for vector quantized dense retrieval.|最近的研究表明，蒸馏可以通过学习倒排文件索引和产品量化来创建基于向量量化的人工神经网络索引。使用固定的教师编码器进行查询和文档编码的一个有争议的优势是，教师所得的分数可以用来代替使用传统监督式学习(如对比学习)时所需的标签判断。然而，目前的工作只是提取量化查询嵌入和量化产品文档嵌入之间的点积的教师编码器输出。我们的工作结合了对比学习和蒸馏的好处，通过使用对比蒸馏，教师输出对比分数，学生学习。我们在 MSMARCO 段落检索和 NQ 开放式问题回答数据集上的实验结果表明，对比精馏方法在矢量量化密集检索方面优于现有技术。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improved+Vector+Quantization+For+Dense+Retrieval+with+Contrastive+Distillation)|0|
|[LogicRec: Recommendation with Users' Logical Requirements](https://doi.org/10.1145/3539618.3592012)|Zhenwei Tang, Griffin Floto, Armin Toroghi, Shichao Pei, Xiangliang Zhang, Scott Sanner||Users may demand recommendations with highly personalized requirements involving logical operations, e.g., the intersection of two requirements, where such requirements naturally form structured logical queries on knowledge graphs (KGs). To date, existing recommender systems lack the capability to tackle users' complex logical requirements. In this work, we formulate the problem of recommendation with users' logical requirements (LogicRec) and construct benchmark datasets for LogicRec. Furthermore, we propose an initial solution for LogicRec based on logical requirement retrieval and user preference retrieval, where we face two challenges. First, KGs are incomplete in nature. Therefore, there are always missing true facts, which entails that the answers to logical requirements can not be completely found in KGs. In this case, item selection based on the answers to logical queries is not applicable. We thus resort to logical query embedding (LQE) to jointly infer missing facts and retrieve items based on logical requirements. Second, answer sets are under-exploited. Existing LQE methods can only deal with query-answer pairs, where queries in our case are the intersected user preferences and logical requirements. However, the logical requirements and user preferences have different answer sets, offering us richer knowledge about the requirements and preferences by providing requirement-item and preference-item pairs. Thus, we design a multi-task knowledge-sharing mechanism to exploit these answer sets collectively. Extensive experimental results demonstrate the significance of the LogicRec task and the effectiveness of our proposed method.|用户可能需要具有高度个性化需求的推荐，这些需求涉及逻辑操作，例如，两个需求的交叉点，这些需求自然而然地在知识图(KG)上形成结构化的逻辑查询。迄今为止，现有的推荐系统缺乏处理用户复杂逻辑需求的能力。在这项工作中，我们提出了与用户的逻辑需求(LogicRec)的推荐问题，并构造了基准数据集的 LogicRec。在此基础上，提出了基于逻辑需求检索和用户偏好检索的 LogicRec 初步解决方案。首先，幼稚园本质上是不完整的。因此，总是缺少真正的事实，这就意味着逻辑要求的答案不能完全在幼稚园中找到。在这种情况下，基于逻辑查询答案的项选择是不适用的。因此，我们使用逻辑查询嵌入(LQE)来联合推断丢失的事实并根据逻辑需求检索项。其次，答案集未得到充分利用。现有的 LQE 方法只能处理查询-答案对，在我们的例子中，查询是交叉的用户首选项和逻辑需求。然而，逻辑需求和用户偏好有不同的答案集，通过提供需求项和偏好项对，为我们提供了关于需求和偏好的更丰富的知识。因此，我们设计了一个多任务知识共享机制来共同利用这些答案集。大量的实验结果表明了 LogicRec 任务的重要性和提出的方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LogicRec:+Recommendation+with+Users'+Logical+Requirements)|0|
|[Matching Point of Interests and Travel Blog with Multi-view Information Fusion](https://doi.org/10.1145/3539618.3592016)|Shuokai Li, Jingbo Zhou, Jizhou Huang, Hao Chen, Fuzhen Zhuang, Qing He, Dejing Dou|BCG X, Beijing, China; Beihang University, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences & University of Chinese Academy of Sciences, Beijing, China; Baidu Inc., Beijing, China; Baidu Research, Beijing, China|The past few years have witnessed an explosive growth of user-generated POI-centric travel blogs, which can provide a comprehensive understanding of a POI for people. However, evaluating the quality of the POI-centric travel blogs and ranking the blogs is not a simple task without domain knowledge or actual travel experience on the target POI. Nevertheless, our insight is that the user search behavior related to the target POI on the online map service can partly valid the rationality of the POIs appearing in the travel blogs, which helps for travel blogs ranking. To this end, in this paper, we propose a novel end-to-end framework for travel blogs ranking, coined Matching POI and Travel Blogs with Multi-view InFormation (MOTIF). Concretely, we first construct two POI graphs as multi-view information: (1) the search-level POI graph which reflects the user behaviors on the online map service; and (2) the document-level POI graph which shows the POI co-occurrence frequency in travel blogs. Then, to better model the intrinsic correlation of the two graphs, we adopt Mutual Information Maximization to align the search-level and document-level semantic spaces. Moreover, we leverage a pair-wise ranking loss for POI-document relevance scoring. Extensive experiments on two real-world datasets demonstrate the superiority of our method.|在过去的几年里，以用户为中心的旅游博客呈爆炸式增长，它可以为人们提供对 POI 的全面理解。然而，评估以 POI 为中心的旅游博客的质量并对这些博客进行排名并不是一项简单的任务，因为在目标 POI 上没有领域知识或实际的旅游经验。然而，我们的研究发现，在线地图服务中与目标 POI 相关的用户搜索行为可以部分验证出现在旅游博客中的 POI 的合理性，这有助于旅游博客的排名。为此，在本文中，我们提出了一个新颖的端到端的旅游博客排名框架，创造匹配 POI 和旅游博客多视图信息(MOTIF)。具体来说，我们首先构建两个 POI 图作为多视图信息: (1)反映在线地图服务中用户行为的搜索级 POI 图; (2)显示旅游博客中 POI 共现频率的文档级 POI 图。然后，为了更好地建立两个图的内在相关性模型，我们采用互信息最大化来对齐搜索级和文档级语义空间。此外，我们利用一对 POI 文档相关性评分的排名损失。在两个实际数据集上的大量实验证明了该方法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Matching+Point+of+Interests+and+Travel+Blog+with+Multi-view+Information+Fusion)|0|
|[One-Shot Labeling for Automatic Relevance Estimation](https://doi.org/10.1145/3539618.3592032)|Sean MacAvaney, Luca Soldaini||Dealing with unjudged documents ("holes") in relevance assessments is a perennial problem when evaluating search systems with offline experiments. Holes can reduce the apparent effectiveness of retrieval systems during evaluation and introduce biases in models trained with incomplete data. In this work, we explore whether large language models can help us fill such holes to improve offline evaluations. We examine an extreme, albeit common, evaluation setting wherein only a single known relevant document per query is available for evaluation. We then explore various approaches for predicting the relevance of unjudged documents with respect to a query and the known relevant document, including nearest neighbor, supervised, and prompting techniques. We find that although the predictions of these One-Shot Labelers (1SLs) frequently disagree with human assessments, the labels they produce yield a far more reliable ranking of systems than the single labels do alone. Specifically, the strongest approaches can consistently reach system ranking correlations of over 0.85 with the full rankings over a variety of measures. Meanwhile, the approach substantially reduces the false positive rate of t-tests due to holes in relevance assessments (from 15-30% down to under 5%), giving researchers more confidence in results they find to be significant.|在用离线实验评估搜索系统时，处理相关性评估中未经判断的文档(“漏洞”)是一个长期存在的问题。漏洞会降低检索系统在评价过程中的表观有效性，并在不完全数据训练的模型中引入偏差。在这项工作中，我们探讨是否大型语言模型可以帮助我们填补这些漏洞，以改善离线评估。我们检查一个极端的，尽管是常见的，评估设置，其中每个查询只有一个已知的相关文档可用于评估。然后，我们探讨了预测未判断文档与查询和已知相关文档相关性的各种方法，包括最近邻、监督和提示技术。我们发现，尽管这些一次性标签(1SLs)的预测经常与人类的评估不一致，但是他们生产的标签产生的系统排名比单独的标签产生的系统排名可靠得多。具体来说，最强的方法可以始终达到系统排名相关性超过0.85与完整的排名在各种措施。同时，这种方法大大降低了由于相关性评估漏洞(从15-30% 下降到5% 以下)而导致的 t 检验的假阳性率，使研究人员对他们发现的重要结果更有信心。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=One-Shot+Labeling+for+Automatic+Relevance+Estimation)|0|
|[Quantifying and Leveraging User Fatigue for Interventions in Recommender Systems](https://doi.org/10.1145/3539618.3592044)|Hitesh Sagtani, Madan Gopal Jhawar, Akshat Gupta, Rishabh Mehrotra|Sharechat, Bengaluru, India|Predicting churn and designing intervention strategies are crucial for online platforms to maintain user engagement. We hypothesize that predicting churn, i.e. users leaving from the system without further return, is often a delayed act, and it might get too late for the system to intervene. We propose detecting early signs of users losing interest, allowing time for intervention, and introduce a new formulation ofuser fatigue as short-term dissatisfaction, providing early signals to predict long-term churn. We identify behavioral signals predicting fatigue and develop models for fatigue prediction. Furthermore, we leverage the predicted fatigue estimates to develop fatigue-aware ad-load balancing intervention strategy that reduces churn, improving short- and long-term user retention. Results from deployed recommendation system and multiple live A/B tests across over 80 million users generating over 200 million sessions highlight gains for user engagement and platform strategic metrics.|预测流失和设计干预策略对于在线平台维持用户参与度至关重要。我们假设预测流失，也就是说用户没有进一步返回就离开系统，通常是一个延迟行为，系统可能会为时已晚进行干预。我们建议检测用户失去兴趣的早期迹象，为干预留出时间，并引入一种新的用户疲劳公式作为短期不满，为预测长期流失提供早期信号。我们识别预测疲劳的行为信号，并建立疲劳预测模型。此外，我们利用预测的疲劳估计来开发具有疲劳意识的广告负载平衡干预策略，以减少流失，提高短期和长期用户保留率。已部署的推荐系统和超过8,000万用户的多个现场 A/B 测试的结果产生了超过2亿次会议，突出显示了用户参与和平台战略指标的收益。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Quantifying+and+Leveraging+User+Fatigue+for+Interventions+in+Recommender+Systems)|0|
|[Rating Prediction in Conversational Task Assistants with Behavioral and Conversational-Flow Features](https://doi.org/10.1145/3539618.3592048)|Rafael Ferreira, David Semedo, João Magalhães|NOVA University of Lisbon, Lisbon, Portugal|Predicting the success of Conversational Task Assistants (CTA) can be critical to understand user behavior and act accordingly. In this paper, we propose TB-Rater, a Transformer model which combines conversational-flow features with user behavior features for predicting user ratings in a CTA scenario. In particular, we use real human-agent conversations and ratings collected in the Alexa TaskBot challenge, a novel multimodal and multi-turn conversational context. Our results show the advantages of modeling both the conversational-flow and behavioral aspects of the conversation in a single model for offline rating prediction. Additionally, an analysis of the CTA-specific behavioral features brings insights into this setting and can be used to bootstrap future systems.|预测会话任务助理(CTA)的成功对于理解用户行为并据此采取行动至关重要。在本文中，我们提出了 TB-Rater 模型，它结合了会话流特征和用户行为特征来预测 CTA 场景中的用户评分。特别是，我们使用真实的人类代理会话和评分收集在 Alexa TaskBot 挑战，一个新颖的多模式和多回合会话环境。我们的研究结果显示了在一个单一的离线评分预测模型中同时建模会话流和会话的行为方面的优势。此外，对 CTA 特定行为特征的分析可以深入了解这种设置，并可用于引导未来的系统。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rating+Prediction+in+Conversational+Task+Assistants+with+Behavioral+and+Conversational-Flow+Features)|0|
|[Retrieval-Enhanced Generative Model for Large-Scale Knowledge Graph Completion](https://doi.org/10.1145/3539618.3592052)|Donghan Yu, Yiming Yang|Carnegie Mellon University, Pittsburgh, PA, USA|The task of knowledge graph completion (KGC) is of great importance. To achieve scalability when dealing with large-scale knowledge graphs, recent works formulate KGC as a sequence-to-sequence process, where the incomplete triplet (input) and the missing entity (output) are both verbalized as text sequences. However, inference with these methods relies solely on the model parameters for implicit reasoning and neglects the use of KG itself, which limits the performance since the model lacks the capacity to memorize a vast number of triplets. To tackle this issue, we introduce ReSKGC, a Retrieval-enhanced Seq2seq KGC model, which selects semantically relevant triplets from the KG and uses them as evidence to guide output generation with explicit reasoning. Our method has demonstrated state-of-the-art performance on benchmark datasets Wikidata5M and WikiKG90Mv2, which contain about 5M and 90M entities, respectively.|知识图完成任务(KGC)具有重要意义。为了在处理大规模知识图时实现可伸缩性，最近的工作将 KGC 表述为一个序列到序列的过程，其中不完整的三元组(输入)和缺失的实体(输出)都表述为文本序列。然而，这些方法的推理完全依赖于模型参数的隐式推理，而忽略了 KG 本身的使用，这限制了性能，因为模型缺乏记忆大量三联体的能力。为了解决这个问题，我们引入了 ReSKGC，一种检索增强的 Seq2seq KGC 模型，它从 KG 中选择语义相关的三联体，并使用它们作为证据来指导显式推理的输出生成。我们的方法已经在基准数据集 Wikidata5M 和 WikiKG90Mv2上演示了最先进的性能，它们分别包含大约5M 和90M 实体。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Retrieval-Enhanced+Generative+Model+for+Large-Scale+Knowledge+Graph+Completion)|0|
|[Review-based Multi-intention Contrastive Learning for Recommendation](https://doi.org/10.1145/3539618.3592053)|Wei Yang, Tengfei Huo, Zhiqiang Liu, Chi Lu|Institute of Computing Technology, Chinese Academy of Sciences & University of Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences & University of Chinese Academy of Sciences, Beijing, China; Kuaishou Technology, Beijing, China|Real recommendation systems contain various features, which are often high-dimensional, sparse, and difficult to learn effectively. In addition to numerical features, user reviews contain rich semantic information including user preferences, which are used as auxiliary features by researchers. The methods of supplementing data features based on reviews have certain effects. However, most of them simply concatenate review representations and other features together, without considering that the text representation contains a lot of noise information. In addition, the important intentions contained in user reviews are not modeled effectively. In order to solve the above problems, we propose a novel Review-based Multi-intention Contrastive Learning (RMCL) method. In detail, RMCL proposes an intention representation method based on mixed Gaussian distribution hypothesis. Further, RMCL adopts a multi-intention contrastive strategy, which establishes a fine-grained connection between user reviews and item reviews. Extensive experiments on five real-world datasets demonstrate significant improvements of our proposed RMCL model over the state-of-the-art methods.|真正的推荐系统包含各种各样的特性，这些特性通常是高维的、稀疏的，并且难以有效地学习。除了数字特征之外，用户评论还包含丰富的语义信息，包括用户偏好，研究人员将其作为辅助特征。基于评论的数据特征补充方法具有一定的效果。然而，它们中的大多数只是简单地将评论表示和其他特性连接在一起，而没有考虑到文本表示包含大量噪声信息。此外，包含在用户评论中的重要意图没有被有效地建模。为了解决上述问题，我们提出了一种新的基于复习的多意图对比学习(RMCL)方法。具体来说，RMCL 提出了一种基于混合正态分布假设的意图表示方法。此外，RMCL 还采用了多目的对比策略，在用户评论和项目评论之间建立了细粒度的联系。在五个真实世界数据集上的大量实验表明，我们提出的 RMCL 模型比最先进的方法有显著的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Review-based+Multi-intention+Contrastive+Learning+for+Recommendation)|0|
|[Rows or Columns? Minimizing Presentation Bias When Comparing Multiple Recommender Systems](https://doi.org/10.1145/3539618.3592056)|Patrik Dokoupil, Ladislav Peska, Ludovico Boratto|University of Cagliari, Cagliari, Italy; Faculty of Mathematics and Physics, Charles University, Prague, Czech Rep|Going beyond accuracy in the evaluation of a recommender system is an aspect that is receiving more and more attention. Among the many perspectives that can be considered, the impact of presentation bias is of central importance. Under presentation bias, the attention of the users to the items in a recommendation list changes, thus affecting their possibility to be considered and the effectiveness of a model. Page-wise within-subject studies are widely employed in the recommender systems literature to compare algorithms by displaying their results in parallel. However, no study has ever been performed to assess the impact of presentation bias in this context. In this paper, we characterize how presentation bias affects different layout options, which present the results in column- or row-wise fashion. Concretely, we present a user study where six layout variants are proposed to the users in a page-wise within-subject setting, so as to evaluate their perception of the displayed recommendations. Results show that presentation bias impacts users clicking behavior (low-level feedback), but not so much the perceived performance of a recommender system (high-level feedback). Source codes and raw results are available at https://tinyurl.com/PresBiasSIGIR2023.|除了评估推荐系统的准确性之外，这方面也越来越受到重视。在许多可以考虑的观点中，表达偏见的影响是非常重要的。在列报偏差的情况下，用户对推荐清单中项目的注意力发生变化，从而影响其被考虑的可能性和模式的有效性。在推荐系统文献中，主题内的逐页研究被广泛用于通过并行显示算法的结果来比较算法。然而，还没有研究评估在这种情况下呈现偏差的影响。在本文中，我们描述了表示偏差如何影响不同的布局选项，它们以列或行的方式呈现结果。具体来说，我们提出了一个用户研究，其中六个布局变量提出了用户在一个页面明确的主题设置，以评估他们的感知显示的建议。结果显示，呈现偏差影响用户的点击行为(低级别反馈) ，但对推荐系统的感知表现(高级别反馈)影响不大。源代码和原始结果可在 https://tinyurl.com/presbiassigir2023下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rows+or+Columns?+Minimizing+Presentation+Bias+When+Comparing+Multiple+Recommender+Systems)|0|
|[Simpler is Much Faster: Fair and Independent Inner Product Search](https://doi.org/10.1145/3539618.3592061)|Kazuyoshi Aoyama, Daichi Amagata, Sumio Fujita, Takahiro Hara|Osaka University, Chiyodaku, Tokyo, Japan; Osaka University, Suita, Osaka, Japan|The problem of inner product search (IPS) is important in many fields. Although maximum inner product search (MIPS) is often considered, its result is usually skewed and static. Users are hence hard to obtain diverse and/or new items by using the MIPS problem. Motivated by this, we formulate a new problem, namely the fair and independent IPS problem. Given a query, a threshold, and an output size k, this problem randomly samples k items from a set of items such that the inner product of the query and item is not less than the threshold. For each item that satisfies the threshold, this problem is fair, because the probability that such an item is outputted is equal to that for each other item. This fairness can yield diversity and novelty, but this problem faces a computational challenge. Some existing (M)IPS techniques can be employed in this problem, but they require O(n) or o(n) time, where n is the dataset size. To scale well to large datasets, we propose a simple yet efficient algorithm that runs in O(log n + k) expected time. We conduct experiments using real datasets, and the results demonstrate that our algorithm is up to 330 times faster than baselines.|内积搜索问题在许多领域都具有重要意义。尽管经常考虑最大内积搜索(MIPS) ，但其结果通常是倾斜和静态的。因此，用户很难通过使用 MIPS 问题来获得不同的和/或新的项目。在此基础上，我们提出了一个新的问题，即公平独立的知识产权问题。给定一个查询、一个阈值和一个输出大小 k，该问题从一组项中随机抽样 k 项，使得查询和项的内积不小于阈值。对于每个满足阈值的项目，这个问题是公平的，因为这样一个项目输出的概率等于其他项目的概率。这种公平性可以产生多样性和新颖性，但是这个问题面临着计算上的挑战。一些现有的(M) IPS 技术可以用于这个问题，但是它们需要 O (n)或 o (n)时间，其中 n 是数据集大小。为了能够很好地扩展到大数据集，我们提出了一个简单而有效的算法，该算法在 O (log n + k)期望时间内运行。我们使用真实数据集进行实验，结果表明我们的算法比基线快330倍。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Simpler+is+Much+Faster:+Fair+and+Independent+Inner+Product+Search)|0|
|[Simplifying Content-Based Neural News Recommendation: On User Modeling and Training Objectives](https://doi.org/10.1145/3539618.3592062)|Andreea Iana, Goran Glavas, Heiko Paulheim||The advent of personalized news recommendation has given rise to increasingly complex recommender architectures. Most neural news recommenders rely on user click behavior and typically introduce dedicated user encoders that aggregate the content of clicked news into user embeddings (early fusion). These models are predominantly trained with standard point-wise classification objectives. The existing body of work exhibits two main shortcomings: (1) despite general design homogeneity, direct comparisons between models are hindered by varying evaluation datasets and protocols; (2) it leaves alternative model designs and training objectives vastly unexplored. In this work, we present a unified framework for news recommendation, allowing for a systematic and fair comparison of news recommenders across several crucial design dimensions: (i) candidate-awareness in user modeling, (ii) click behavior fusion, and (iii) training objectives. Our findings challenge the status quo in neural news recommendation. We show that replacing sizable user encoders with parameter-efficient dot products between candidate and clicked news embeddings (late fusion) often yields substantial performance gains. Moreover, our results render contrastive training a viable alternative to point-wise classification objectives.|个性化新闻推荐的出现引发了日益复杂的推荐体系结构。大多数神经新闻推荐器依赖于用户的点击行为，通常引入专用的用户编码器，将点击新闻的内容聚合到用户嵌入(早期融合)中。这些模型主要用标准的逐点分类目标进行训练。现有的工作主体表现出两个主要缺点: (1)尽管总体设计同质化，但不同的评估数据集和协议阻碍了模型之间的直接比较; (2)留下了大量未被探索的替代模型设计和培训目标。在这项工作中，我们提出了一个新闻推荐的统一框架，允许在几个关键的设计维度上对新闻推荐进行系统和公平的比较: (i)用户建模中的候选人意识，(ii)点击行为融合，以及(iii)培训目标。我们的发现挑战了神经新闻推荐的现状。我们表明，在候选和点击新闻嵌入(后期融合)之间用参数有效的点积替换大型用户编码器通常会产生显著的性能提高。此外，我们的结果使对比训练成为一个可行的替代点分类目标。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Simplifying+Content-Based+Neural+News+Recommendation:+On+User+Modeling+and+Training+Objectives)|0|
|[SimTDE: Simple Transformer Distillation for Sentence Embeddings](https://doi.org/10.1145/3539618.3592063)|Jian Xie, Xin He, Jiyang Wang, Zimeng Qiu, Ali Kebarighotbi, Farhad Ghassemi|Amazon Alexa, Cambridge, MA, USA; University of Central Florida, Orlando, FL, USA; Amazon Alexa, Bellevue, WA, USA|In this paper we introduce SimTDE, a simple knowledge distillation framework to compress sentence embeddings transformer models with minimal performance loss and significant size and latency reduction. SimTDE effectively distills large and small transformers via a compact token embedding block and a shallow encoding block, connected with a projection layer, relaxing dimension match requirement. SimTDE simplifies distillation loss to focus only on token embedding and sentence embedding. We evaluate on standard semantic textual similarity (STS) tasks and entity resolution (ER) tasks. It achieves 99.94% of the state-of-the-art (SOTA) SimCSE-Bert-Base performance with 3 times size reduction and 96.99% SOTA performance with 12 times size reduction on STS tasks. It also achieves 99.57% of teacher's performance on multi-lingual ER data with a tiny transformer student model of 1.4M parameters and 5.7MB size. Moreover, compared to other distilled transformers SimTDE is 2 times faster at inference given similar size and still 1.17 times faster than a model 33% smaller (e.g. MiniLM). The easy-to-adopt framework, strong accuracy and low latency of SimTDE can widely enable runtime deployment of SOTA sentence embeddings.|本文介绍了一个简单的知识提取框架 SimTDE，它可以以最小的性能损失、最大的规模和最小的延迟来压缩句子嵌入转换器模型。SimTDE 通过一个紧凑的令牌嵌入块和一个与投影层连接的浅层编码块，有效地提取大型和小型变压器，放松了尺寸匹配要求。SimTDE 简化了蒸馏损失，只关注标记嵌入和句子嵌入。我们对标准语义文本相似度(STS)任务和实体解析(ER)任务进行了评估。它实现了99.94% 的最先进的(SOTA) SimCSE-Bert-Base 性能与3倍的大小减少和96.99% 的 SOTA 性能与12倍的大小减少的 STS 任务。采用1.4 M 参数、5.7 MB 大小的微型变压器学生模型，实现了99.57% 的教师多语种 ER 数据处理效果。此外，与其他蒸馏变压器相比，SimTDE 在相似尺寸下的推断速度是其他变压器的2倍，仍然比小33% 的型号(例如 MiniLM)快1.17倍。SimTDE 的框架易于采用、准确性强、延迟低，可以广泛应用于 SOTA 句子嵌入的运行时部署。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SimTDE:+Simple+Transformer+Distillation+for+Sentence+Embeddings)|0|
|[TAML: Time-Aware Meta Learning for Cold-Start Problem in News Recommendation](https://doi.org/10.1145/3539618.3592068)|Jingyuan Li, Yue Zhang, Xuan Lin, Xinxing Yang, Ge Zhou, Longfei Li, Hong Chen, Jun Zhou|Ant Group, Hangzhou, China|Meta-learning has become a widely used method for the user cold-start problem in recommendation systems, as it allows the model to learn from similar learning tasks and transfer the knowledge to new tasks. However, most existing meta-learning methods do not consider the temporal factor of users' preferences, which is crucial for news recommendation scenarios where news streams change dynamically over time. In this paper, we propose Time-Aware Meta-Learning (TAML), a novel framework that focuses on cold-start users in news recommendation systems. TAML factorizes user preferences into time-specifc and time-shift representations that jointly affect users' news preferences. These temporal factors are further incorporated into the meta-learning framework to achieve accurate and timely cold-start recommendations. Extensive experiments are conducted on two real-world datasets, demonstrating the superior performance of TAML over state-of-the-art methods.|在推荐系统中，元学习使得模型能够从相似的学习任务中学习知识并将知识转移到新的任务中，因而成为解决用户冷启动问题的一种广泛应用的方法。然而，大多数现有的元学习方法都没有考虑用户偏好的时间因素，这对于新闻流随时间动态变化的新闻推荐场景来说是至关重要的。在本文中，我们提出了时间感知元学习(TAML) ，一个新的框架，侧重于冷启动用户在新闻推荐系统。TAML 将用户偏好分解为特定时间和时移表示，共同影响用户的新闻偏好。这些时间因素进一步纳入元学习框架，以实现准确和及时的冷启动建议。在两个真实世界的数据集上进行了广泛的实验，证明了 TAML 优于最先进的方法的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TAML:+Time-Aware+Meta+Learning+for+Cold-Start+Problem+in+News+Recommendation)|0|
|[Uncertainty-based Heterogeneous Privileged Knowledge Distillation for Recommendation System](https://doi.org/10.1145/3539618.3592079)|Ang Li, Jian Hu, Ke Ding, Xiaolu Zhang, Jun Zhou, Yong He, Xu Min|Ant Group, Hangzhou, China; Queen Mary University of London, London, United Kingdom; Ant Group, Beijing, China|In industrial recommendation systems, both data sizes and computational resources vary across different scenarios. For scenarios with limited data, data sparsity can lead to a decrease in model performance. Heterogeneous knowledge distillation-based transfer learning can be used to transfer knowledge from models in data-rich domains. However, in recommendation systems, the target domain possesses specific privileged features that significantly contribute to the model. While existing knowledge distillation methods have not taken these features into consideration, leading to suboptimal transfer weights. To overcome this limitation, we propose a novel algorithm called Uncertainty-based Heterogeneous Privileged Knowledge Distillation (UHPKD). Our method aims to quantify the knowledge of both the source and target domains, which represents the uncertainty of the models. This approach allows us to derive transfer weights based on the knowledge gain, which captures the difference in knowledge between the source and target domains. Experiments conducted on both public and industrial datasets demonstrate the superiority of our UHPKD algorithm compared to other state-of-the-art methods.|在工业推荐系统中，数据大小和计算资源在不同的场景中都有所不同。对于数据有限的场景，数据稀疏会导致模型性能下降。基于异构知识提取的转移学习可以用来从数据丰富的领域的模型中转移知识。然而，在推荐系统中，目标域具有特定的特权特性，这些特性对模型有很大的贡献。现有的知识提取方法没有考虑到这些特点，导致传递权重不够优化。为了克服这一局限性，我们提出了一种基于不确定性的异构特权知识提取(UHPKD)算法。我们的方法旨在量化源域和目标域的知识，这代表了模型的不确定性。这种方法可以根据知识增益推导出传递权重，从而获取源域和目标域之间的知识差异。在公共数据集和工业数据集上进行的实验表明，与其他最先进的算法相比，我们的 UHPKD 算法具有优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Uncertainty-based+Heterogeneous+Privileged+Knowledge+Distillation+for+Recommendation+System)|0|
|[WSFE: Wasserstein Sub-graph Feature Encoder for Effective User Segmentation in Collaborative Filtering](https://doi.org/10.1145/3539618.3592089)|Yankai Chen, Yifei Zhang, Menglin Yang, Zixing Song, Chen Ma, Irwin King||Maximizing the user-item engagement based on vectorized embeddings is a standard procedure of recent recommender models. Despite the superior performance for item recommendations, these methods however implicitly deprioritize the modeling of user-wise similarity in the embedding space; consequently, identifying similar users is underperforming, and additional processing schemes are usually required otherwise. To avoid thorough model re-training, we propose WSFE, a model-agnostic and training-free representation encoder, to be flexibly employed on the fly for effective user segmentation. Underpinned by the optimal transport theory, the encoded representations from WSFE present a matched user-wise similarity/distance measurement between the realistic and embedding space. We incorporate WSFE into six state-of-the-art recommender models and conduct extensive experiments on six real-world datasets. The empirical analyses well demonstrate the superiority and generality of WSFE to fuel multiple downstream tasks with diverse underlying targets in recommendation.|最大化基于向量化嵌入的用户项目参与是最近推荐模型的一个标准过程。尽管项目推荐的性能优越，但是这些方法隐含地剥夺了嵌入空间中用户相似性的建模优先级; 因此，识别相似的用户表现不佳，并且通常需要额外的处理方案。为了避免彻底的模型再训练，我们提出了 WSFE，一种模型不可知和无训练的表示编码器，可以灵活地应用于有效的用户分割。在最优传输理论的支持下，来自 WSFE 的编码表示提供了现实空间和嵌入空间之间匹配的用户相似度/距离度量。我们将 WSFE 整合到六个最先进的推荐模型中，并在六个真实世界的数据集上进行广泛的实验。实证分析很好地证明了 WSFE 在推荐目标不同的多下游任务方面的优越性和普遍性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=WSFE:+Wasserstein+Sub-graph+Feature+Encoder+for+Effective+User+Segmentation+in+Collaborative+Filtering)|0|
|[Balanced Topic Aware Sampling for Effective Dense Retriever: A Reproducibility Study](https://doi.org/10.1145/3539618.3591915)|Shuai Wang, Guido Zuccon|The University of Queensland, Brisbane, Australia|Knowledge distillation plays a key role in boosting the effectiveness of rankers based on pre-trained language models (PLMs); this is achieved using an effective but inefficient large model to teach a more efficient student model. In the context of knowledge distillation for a student dense passage retriever, the balanced topic-aware sampling method has been shown to provide state-of-the-art effectiveness. This method intervenes in the creation of the training batches by creating batches that contain positive-negative pairs of passages from the same topic, and balancing the pairwise margins of the positive and negative passages. In this paper, we reproduce the balanced topic-aware sampling method; we do so for both the dataset used for evaluation in the original work (MS MARCO) and for a dataset in a different domain, that of product search (Amazon shopping queries dataset) to study whether the original results generalize to a different context. We show that while we could not replicate the exact results from the original paper, we do confirm the original findings in terms of trends: balanced topic-aware sampling indeed leads to highly effective dense retrievers. These results partially generalize to the other search task we investigate, product search: although we observe the improvements are less significant compared to MS MARCO. In addition to reproducing the original results and studying how the method generalizes to a different dataset, we also investigate a key aspect that influences the effectiveness of the method: the use of a hard margin threshold for negative sampling. This aspect was not studied in the original paper. With respect to hard margins, we find that while setting different hard margin values significantly influences the effectiveness of the student model, this impact is dataset-dependent -- and indeed, it does depend on the score distributions exhibited by retrieval models on the dataset at hand. Our reproducibility code is available at https://github.com/ielab/TAS-B-Reproduction.|知识提取在提高基于预训练语言模型(PLM)的排名的有效性方面起着关键作用，这是通过使用一个有效但低效的大模型来教授一个更有效的学生模型来实现的。在对学生密集通道检索器进行知识提取的背景下，平衡主题感知抽样方法已被证明可以提供最先进的有效性。这种方法通过创建包含来自同一主题的正负段落对的批处理，以及平衡正负段落的成对边距来干预训练批处理的创建。在本文中，我们重现了平衡主题感知抽样方法; 我们这样做的数据集用于评估在原始工作(MS MARCO)和在不同领域的数据集，产品搜索(亚马逊购物查询数据集) ，以研究是否原始结果概括到不同的背景。我们表明，虽然我们不能复制从原始论文的确切结果，我们确实证实了原始发现的趋势: 平衡的主题感知抽样确实导致高效的密集检索。这些结果部分推广到我们调查的其他搜索任务，产品搜索: 虽然我们观察到的改进不太显着相比微软 MARCO。除了重现原始结果和研究该方法如何推广到不同的数据集，我们还研究了影响该方法有效性的一个关键方面: 对负采样使用硬边界阈值。这方面的研究没有在原来的文章。关于硬边际值，我们发现当设置不同的硬边际值显著影响学生模型的有效性时，这种影响是依赖于数据集的——事实上，它确实依赖于手边数据集的检索模型显示的分数分布。我们的重复性代码可以在 https://github.com/ielab/tas-b-reproduction 上找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Balanced+Topic+Aware+Sampling+for+Effective+Dense+Retriever:+A+Reproducibility+Study)|0|
|[T2Ranking: A Large-scale Chinese Benchmark for Passage Ranking](https://doi.org/10.1145/3539618.3591874)|Xiaohui Xie, Qian Dong, Bingning Wang, Feiyang Lv, Ting Yao, Weinan Gan, Zhijing Wu, Xiangsheng Li, Haitao Li, Yiqun Liu, Jin Ma||Passage ranking involves two stages: passage retrieval and passage re-ranking, which are important and challenging topics for both academics and industries in the area of Information Retrieval (IR). However, the commonly-used datasets for passage ranking usually focus on the English language. For non-English scenarios, such as Chinese, the existing datasets are limited in terms of data scale, fine-grained relevance annotation and false negative issues. To address this problem, we introduce T2Ranking, a large-scale Chinese benchmark for passage ranking. T2Ranking comprises more than 300K queries and over 2M unique passages from real-world search engines. Expert annotators are recruited to provide 4-level graded relevance scores (fine-grained) for query-passage pairs instead of binary relevance judgments (coarse-grained). To ease the false negative issues, more passages with higher diversities are considered when performing relevance annotations, especially in the test set, to ensure a more accurate evaluation. Apart from the textual query and passage data, other auxiliary resources are also provided, such as query types and XML files of documents which passages are generated from, to facilitate further studies. To evaluate the dataset, commonly used ranking models are implemented and tested on T2Ranking as baselines. The experimental results show that T2Ranking is challenging and there is still scope for improvement. The full data and all codes are available at https://github.com/THUIR/T2Ranking/|短文排序包括短文检索和短文重排两个阶段，这两个阶段对于信息检索领域的学术界和工业界来说都是非常重要和具有挑战性的课题。然而，常用于文章排名的数据集通常集中在英语语言上。对于非英语情景，如中文，现有的数据集在数据规模、细粒度相关性标注和虚假否定问题等方面存在局限性。为了解决这个问题，我们引入了 T2Ranking，一个大规模的中文通过排名基准。T2Rank 包含超过30万个查询和超过200万个来自现实世界搜索引擎的独特段落。招募专家注释者为查询-通过对提供4级分级相关分数(细粒度) ，而不是二进制相关判断(粗粒度)。为了减少错误的否定性问题，在进行相关注释时，特别是在测试集中，需要考虑更多多样性较高的段落，以确保更准确的评价。除了文字查询和段落资料外，还提供其他辅助资源，例如查询类型和产生段落的文件的 XML 档案，以方便进一步研究。为了评估数据集，常用的排名模型被实现，并在 T2Ranking 上作为基线进行测试。实验结果表明，T2Ranking 是具有挑战性的，仍然有改进的空间。完整的数据和所有的代码都可以在 https://github.com/thuir/t2ranking/找到|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=T2Ranking:+A+Large-scale+Chinese+Benchmark+for+Passage+Ranking)|0|
|[Towards a More User-Friendly and Easy-to-Use Benchmark Library for Recommender Systems](https://doi.org/10.1145/3539618.3591889)|Lanling Xu, Zhen Tian, Gaowei Zhang, Junjie Zhang, Lei Wang, Bowen Zheng, Yifan Li, Jiakai Tang, Zeyu Zhang, Yupeng Hou, Xingyu Pan, Wayne Xin Zhao, Xu Chen, JiRong Wen|Renmin University of China, Beijing, China|In recent years, the reproducibility of recommendation models has become a severe concern in recommender systems. In light of this challenge, we have previously released a unified, comprehensive and efficient recommendation library called RecBole, attracting much attention from the research community. With the increasing number of users, we have received a number of suggestions and update requests. This motivates us to make further improvements on our library, so as to meet the user requirements and contribute to the research community. In this paper, we present a significant update of RecBole, making it more user-friendly and easy-to-use as a comprehensive benchmark library for recommendation. More specifically, the highlights of this update are summarized as: (1) we include more benchmark models and datasets, improve the benchmark framework in terms of data processing, training and evaluation, and release reproducible configurations to benchmark the recommendation models; (2) we upgrade the user friendliness of our library by providing more detailed documentation and well-organized frequently asked questions, and (3) we propose several development guidelines for the open-source library developers. These extensions make it much easier to reproduce the benchmark results and stay up-to-date with the recent advances on recommender systems. Our update is released at the link: https://github.com/RUCAIBox/RecBole.|近年来，推荐模型的可重复性已经成为推荐系统中的一个重要问题。鉴于这一挑战，我们以前发布了一个统一、全面和高效的推荐库，名为 RecBole，吸引了研究界的广泛关注。随着用户数量的增加，我们收到了一些建议和更新请求。这促使我们进一步改善我们的图书馆，以满足用户的要求，并为研究社区作出贡献。在本文中，我们提出了一个重大的更新 RecBole，使其更加友好的用户和易于使用作为一个综合的基准库推荐。更具体地说，这次更新的亮点总结如下: (1)我们包括了更多的基准模型和数据集，改进了数据处理、培训和评估方面的基准框架，并发布了可重复的配置来作为推荐模型的基准; (2)我们通过提供更详细的文档和组织良好的常见问题来提升我们图书馆的用户友好性; (3)我们为开源图书馆开发者提出了几条开发指导方针。这些扩展使得复制基准测试结果和保持最新的推荐系统的最新进展变得更加容易。我们的更新发布在链接:  https://github.com/rucaibox/recbole。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+a+More+User-Friendly+and+Easy-to-Use+Benchmark+Library+for+Recommender+Systems)|0|
|[RiverText: A Python Library for Training and Evaluating Incremental Word Embeddings from Text Data Streams](https://doi.org/10.1145/3539618.3591908)|Gabriel IturraBocaz, Felipe BravoMarquez|University of Chile, Santiago, Chile|Word embeddings have become essential components in various information retrieval and natural language processing tasks, such as ranking, document classification, and question answering. However, despite their widespread use, traditional word embedding models present a limitation in their static nature, which hampers their ability to adapt to the constantly evolving language patterns that emerge in sources such as social media and the web (e.g., new hashtags or brand names). To overcome this problem, incremental word embedding algorithms are introduced, capable of dynamically updating word representations in response to new language patterns and processing continuous data streams. This paper presents RiverText, a Python library for training and evaluating incremental word embeddings from text data streams. Our tool is a resource for the information retrieval and natural language processing communities that work with word embeddings in streaming scenarios, such as analyzing social media. The library implements different incremental word embedding techniques, such as Skip-gram, Continuous Bag of Words, and Word Context Matrix, in a standardized framework. In addition, it uses PyTorch as its backend for neural network training. We have implemented a module that adapts existing intrinsic static word embedding evaluation tasks for word similarity and word categorization to a streaming setting. Finally, we compare the implemented methods with different hyperparameter settings and discuss the results. Our open-source library is available at https://github.com/dccuchile/rivertext.|单词嵌入已经成为各种信息检索和自然语言处理任务的重要组成部分，例如排序、文档分类和问答。然而，尽管它们被广泛使用，传统的单词嵌入模型在其静态本质上存在局限性，这阻碍了它们适应不断变化的语言模式的能力，这些模式出现在社交媒体和网络等资源中(例如，新的标签或品牌名称)。为了克服这一问题，引入了增量式词嵌入算法，该算法能够根据新的语言模式动态更新词表示并处理连续的数据流。本文介绍了 RiverText，一个用于训练和评估文本数据流中增量单词嵌入的 Python 库。我们的工具是一个为信息检索和自然语言处理社区提供的资源，这些社区使用流媒体场景中的单词嵌入，比如分析社交媒体。该库在一个标准化的框架内实现了不同的增量式单词嵌入技术，如跳跃图、连续单词袋和单词上下文矩阵。此外，它使用 PyTorch 作为神经网络训练的后端。我们实现了一个模块，适应现有的内在静态词语嵌入评价任务的词语相似性和词语分类的流设置。最后，我们比较了不同超参数设置下的实现方法并讨论了结果。我们的开源图书馆可以在 https://github.com/dccuchile/rivertext 上使用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RiverText:+A+Python+Library+for+Training+and+Evaluating+Incremental+Word+Embeddings+from+Text+Data+Streams)|0|
|[HeteroCS: A Heterogeneous Community Search System With Semantic Explanation](https://doi.org/10.1145/3539618.3591812)|Weibin Cai, Fanwei Zhu, Zemin Liu, Minghui Wu|Hangzhou City University, Singapore, China; Hangzhou City University, Hangzhou, China|Community search, which looks for query-dependent communities in a graph, is an important task in graph analysis. Existing community search studies address the problem by finding a densely-connected subgraph containing the query. However, many real-world networks are heterogeneous with rich semantics. Queries in heterogeneous networks generally involve in multiple communities with different semantic connections, while returning a single community with mixed semantics has limited applications. In this paper, we revisit the community search problem on heterogeneous networks and introduce a novel paradigm of heterogeneous community search and ranking. We propose to automatically discover the query semantics to enable the search of different semantic communities and develop a comprehensive community evaluation model to support the ranking of results. We build HeteroCS, a heterogeneous community search system with semantic explanation, upon our semantic community model, and deploy it on two real-world graphs. We present a demonstration case to illustrate the novelty and effectiveness of the system.|社区搜索是图分析中的一项重要任务，它在图中寻找与查询相关的社区。现有的社区搜索研究通过查找包含查询的密集连接子图来解决这个问题。然而，许多现实世界的网络具有丰富的语义异构性。异构网络中的查询通常涉及具有不同语义连接的多个社区，而返回具有混合语义的单个社区的应用程序有限。本文重新讨论了异构网络上的社区搜索问题，提出了一种新的异构社区搜索和排序方法。我们提出自动发现查询语义，以便搜索不同的语义社区，并开发一个综合的社区评价模型，以支持结果的排序。我们在语义社区模型的基础上构建了一个具有语义解释的异构社区搜索系统，并将其部署在两个真实世界的图中。通过一个实例说明了该系统的新颖性和有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HeteroCS:+A+Heterogeneous+Community+Search+System+With+Semantic+Explanation)|0|
|[ranxhub: An Online Repository for Information Retrieval Runs](https://doi.org/10.1145/3539618.3591823)|Elias Bassani|University of Milano-Bicocca, Milan, Italy|ranxhub is an online repository for sharing artifacts deriving from the evaluation of Information Retrieval systems. Specifically, we provide a platform for sharing pre-computed runs: the ranked lists of documents retrieved for a specific set of queries by a retrieval model. We also extend ranx, a Python library for the evaluation and comparison of Information Retrieval runs, adding functionalities to integrate the usage of ranxhub seamlessly, allowing the user to compare the results of multiple systems in just a few lines of code. In this paper, we first outline the many advantages and implications that an online repository for sharing runs can bring to the table. Then, we introduce ranxhub and its integration with ranx, showing its very simple usage. Finally, we discuss some use cases for which ranxhub can be highly valuable for the research community.|Ranxhub 是一个在线资源库，用于共享信息检索系统评估产生的工件。具体来说，我们提供了一个共享预计算运行的平台: 通过检索模型为特定查询集检索的文档排序列表。我们还扩展了 ranx，一个用于评估和比较信息检索运行的 Python 库，增加了无缝集成 ranxhub 使用的功能，允许用户在几行代码中比较多个系统的结果。在本文中，我们首先概述了用于共享运行的在线存储库可以带来的许多优势和影响。然后介绍了 ranxhub 及其与 ranx 的集成，说明了它的简单用法。最后，我们讨论了一些使用实例，其中 ranxhub 对于研究社区非常有价值。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ranxhub:+An+Online+Repository+for+Information+Retrieval+Runs)|0|
|[Exploratory Visualization Tool for the Continuous Evaluation of Information Retrieval Systems](https://doi.org/10.1145/3539618.3591825)|Gabriela González Sáez, Petra Galuscáková, Romain Deveaud, Lorraine Goeuriot, Philippe Mulhem|Université Grenoble Alpes, Grenoble, France; Qwant, Paris, France|This paper introduces a novel visualization tool that facilitates the exploratory analysis of continuous evaluation for information retrieval systems. We base our analysis on score standardization and meta-analysis techniques applied to Information Retrieval evaluation. We present three functionalities: evaluation overview, delta evaluation, and meta-analysis applied to three perspectives: evaluation rounds, queries, and systems. To illustrate the use of the tool, we provide an example using the TREC-COVID test collection.|本文介绍一种新颖的可视化工具，可以方便地对信息检索系统的连续评估进行探索性分析。我们的分析基于分数标准化和应用于信息检索评估的荟萃分析技术。我们提出了三种功能: 评估概述、增量评估和应用于三个视角的元分析: 评估轮、查询和系统。为了说明该工具的使用，我们提供了一个使用 TREC-COVID 测试集合的示例。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploratory+Visualization+Tool+for+the+Continuous+Evaluation+of+Information+Retrieval+Systems)|0|
|[COUPA: An Industrial Recommender System for Online to Offline Service Platforms](https://doi.org/10.1145/3539618.3591828)|Sicong Xie, Binbin Hu, Fengze Li, Ziqi Liu, Zhiqiang Zhang, Wenliang Zhong, Jun Zhou||Aiming at helping users locally discovery retail services (e.g., entertainment and dinning), Online to Offline (O2O) service platforms have become popular in recent years, which greatly challenge current recommender systems. With the real data in Alipay, a feeds-like scenario for O2O services, we find that recurrence based temporal patterns and position biases commonly exist in our scenarios, which seriously threaten the recommendation effectiveness. To this end, we propose COUPA, an industrial system targeting for characterizing user preference with following two considerations: (1) Time aware preference: we employ the continuous time aware point process equipped with an attention mechanism to fully capture temporal patterns for recommendation. (2) Position aware preference: a position selector component equipped with a position personalization module is elaborately designed to mitigate position bias in a personalized manner. Finally, we carefully implement and deploy COUPA on Alipay with a cooperation of edge, streaming and batch computing, as well as a two-stage online serving mode, to support several popular recommendation scenarios. We conduct extensive experiments to demonstrate that COUPA consistently achieves superior performance and has potential to provide intuitive evidences for recommendation|为了帮助用户在本地发现零售服务(如娱乐和餐饮) ，Online To Offline线上到线下服务平台近年来越来越流行，这极大地挑战了现有的推荐系统。利用支付宝中的实际数据，我们发现在 O2O 服务中，基于循环的时间模式和位置偏差在我们的场景中普遍存在，这严重威胁了推荐的有效性。为此，我们提出了 COUPA，这是一个针对用户偏好特征的工业系统，具有以下两个考虑因素: (1)时间感知偏好: 我们采用连续的时间感知点过程，配备注意机制，以充分捕获推荐的时间模式。(2)位置感知偏好: 精心设计的配有位置个性化模块的位置选择元件，以个性化的方式减轻位置偏差。最后，我们在支付宝上小心地实现和部署了 COUPA，并结合了边缘计算、流计算和批处理计算，以及两阶段的在线服务模式，以支持多种流行的推荐场景。我们进行了广泛的实验，以证明 COUPA 始终如一地实现卓越的性能，并有潜力为推荐提供直观的证据|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=COUPA:+An+Industrial+Recommender+System+for+Online+to+Offline+Service+Platforms)|0|
|[A Practical Online Allocation Framework at Industry-scale in Constrained Recommendation](https://doi.org/10.1145/3539618.3591835)|Daohong Jian, Yang Bao, Jun Zhou, Hua Wu|AntGroup, Beijing, China|Online allocation is a critical challenge in constrained recommendation systems, where the distribution of goods, ads, vouchers, and other content to users with limited resources needs to be managed effectively. While the existing literature has made significant progress in improving recommendation algorithms for various scenarios, less attention has been given to developing and deploying industry-scale online allocation system in an efficient manner. To address this issue, this paper introduces an integrated and efficient learning framework in constrained recommendation scenarios at Alipay. The framework has been tested through experiments, demonstrating its superiority over other state-of-the-art methods.|在有限的推荐系统中，在线分配是一个关键的挑战，需要有效地管理向资源有限的用户分发商品、广告、凭证和其他内容。虽然现有文献在改进各种情景下的推荐算法方面取得了重大进展，但对于高效开发和部署行业规模的在线分配系统的关注较少。为了解决这个问题，本文在支付宝的受限推荐场景中引入了一个集成的、高效的学习框架。该框架已通过实验进行了测试，证明了其优于其他最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Practical+Online+Allocation+Framework+at+Industry-scale+in+Constrained+Recommendation)|0|
|[A Transformer-Based Substitute Recommendation Model Incorporating Weakly Supervised Customer Behavior Data](https://doi.org/10.1145/3539618.3591847)|Wenting Ye, Hongfei Yang, Shuai Zhao, Haoyang Fang, Xingjian Shi, Naveen Neppalli||The substitute-based recommendation is widely used in E-commerce to provide better alternatives to customers. However, existing research typically uses the customer behavior signals like co-view and view-but-purchase-another to capture the substitute relationship. Despite its intuitive soundness, we find that such an approach might ignore the functionality and characteristics of products. In this paper, we adapt substitute recommendation into language matching problem by taking product title description as model input to consider product functionality. We design a new transformation method to de-noise the signals derived from production data. In addition, we consider multilingual support from the engineering point of view. Our proposed end-to-end transformer-based model achieves both successes from offline and online experiments. The proposed model has been deployed in a large-scale E-commerce website for 11 marketplaces in 6 languages. Our proposed model is demonstrated to increase revenue by 19% based on an online A/B experiment.|基于替代品的推荐广泛应用于电子商务中，为客户提供更好的替代品。然而，现有的研究通常使用顾客行为信号，如共同查看和查看-但购买-另一个来捕捉替代关系。尽管这种方法直观可靠，但我们发现它可能会忽略产品的功能和特性。本文以产品名称描述作为模型输入，考虑产品功能，将替代推荐应用到语言匹配问题中。我们设计了一种新的变换方法来去除从生产数据中得到的信号的噪声。此外，我们从工程的角度考虑多语言支持。我们提出的基于端到端变压器的模型在离线和在线实验中都取得了成功。拟议的模式已经在一个大型电子商务网站上用6种语言为11个市场部署。基于在线 A/B 实验，我们提出的模型被证明可以增加19% 的收入。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Transformer-Based+Substitute+Recommendation+Model+Incorporating+Weakly+Supervised+Customer+Behavior+Data)|0|
|[DCBT: A Simple But Effective Way for Unified Warm and Cold Recommendation](https://doi.org/10.1145/3539618.3591856)|Jieyu Yang, Liang Zhang, Yong He, Ke Ding, Zhaoxin Huan, Xiaolu Zhang, Linjian Mo|Ant Group, Hangzhou, China; Ant Group, Shanghai, China|The cold-start problem of conversion rate prediction is a common challenge in online advertising systems. To alleviate this problem, a large number of methods either use content information or uncertainty methods, or use meta-learning based methods to improve the ranking performance of cold-start items. However, they can work for cold-start scenarios but fail to adaptively unify warm and cold recommendations into one model, requiring additional human efforts or knowledge to adapt to different scenarios. Meanwhile, none of them pay attention to the discrepancy between model predictions and true likelihoods of cold items, while over- or under-estimation is harmful to the ROI (Return on Investment) of advertising placements. In this paper, in order to address the above issues, we propose a framework called Distribution-Constrained Batch Transformer (DCBT). Specifically, the framework introduces a Transformer module into the batch dimension to automatically choose proper information from warm samples to enhance the representation of cold samples and preserve the property of warm samples. In addition, to avoid the distribution of cold samples being affected by the warm samples, the framework adds MMD loss to constrain the sample distribution before and after feeding into the Transformer module. Extensive offline experiments on two real-world datasets show that our proposed method attains state-of-the-art performance in AUC and PCOC (Predicted CVR over CVR) for cold items and warm items. An online A/B test demonstrates that the DCBT model obtained a 20.08% improvement in CVR and a 13.21% increase in GMV (Gross Merchandise Volume).|转化率预测的冷启动问题是在线广告系统中普遍存在的问题。为了缓解这一问题，大量的方法或者使用内容信息或不确定性方法，或者使用基于元学习的方法来改善冷启动项目的排序性能。然而，它们可以适用于冷启动的情况，但不能适应性地将温暖和冷的建议统一到一个模型中，需要额外的人力或知识来适应不同的情况。同时，他们都没有注意到模型预测与真实可能性之间的差异，而过高或过低的估计对广告投放的投资回报率(ROI)是有害的。为了解决上述问题，本文提出了一种分布约束间歇变压器(DCBT)框架。具体来说，该框架引入了变压器模块，自动选择适当的信息从热样本，以增强表示冷样本和保持性能的热样本。此外，为了避免冷样品的分布受到温样品的影响，该框架增加了 MMD 损耗，以约束送入变压器模块前后的样品分布。在两个实际数据集上的大量离线实验表明，我们提出的方法在冷项目和温项目的 AUC 和 PCOC (预测 CVR)方面达到了最先进的性能。在线 A/B 测试表明，DCBT 模型获得了20.08% 的改善 CVR 和13.21% 的增加 GMV (总商品量)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DCBT:+A+Simple+But+Effective+Way+for+Unified+Warm+and+Cold+Recommendation)|0|
|[OFAR: A Multimodal Evidence Retrieval Framework for Illegal Live-streaming Identification](https://doi.org/10.1145/3539618.3591864)|Dengtian Lin, Yang Ma, Yuhong Li, Xuemeng Song, Jianlong Wu, Liqiang Nie||Illegal live-streaming identification, which aims to help live-streaming platforms immediately recognize the illegal behaviors in the live-streaming, such as selling precious and endangered animals, plays a crucial role in purifying the network environment. Traditionally, the live-streaming platform needs to employ some professionals to manually identify the potential illegal live-streaming. Specifically, the professional needs to search for related evidence from a large-scale knowledge database for evaluating whether a given live-streaming clip contains illegal behavior, which is time-consuming and laborious. To address this issue, in this work, we propose a multimodal evidence retrieval system, named OFAR, to facilitate the illegal live-streaming identification. OFAR consists of three modules: Query Encoder, Document Encoder, and MaxSim-based Contrastive Late Intersection. Both query encoder and document encoder are implemented with the advanced OFA encoder, which is pretrained on a large-scale multimodal dataset. In the last module, we introduce contrastive learning on the basis of the MaxiSim-based late intersection, to enhance the model's ability of query-document matching. The proposed framework achieves significant improvement on our industrial dataset TaoLive, demonstrating the advances of our scheme.|非法流媒体识别是帮助流媒体平台及时识别流媒体中的非法行为，如出售珍稀濒危动物等，对净化网络环境起着至关重要的作用。传统上，直播平台需要雇佣一些专业人员来手动识别潜在的非法直播。具体来说，专业人员需要从大规模的知识库中搜索相关证据，以评估给定的直播剪辑是否包含违法行为，这是一项既费时又费力的工作。为了解决这个问题，本文提出了一个多模态证据检索系统 OFAR，以方便非法流媒体证据的识别。OFAR 由三个模块组成: 查询编码器、文档编码器和基于 MaxSim 的对比晚交。查询编码器和文档编码器都是用先进的 OFA 编码器实现的，该编码器是在大规模多模态数据集上预先训练好的。在最后一个模块中，我们引入了基于 MaxiSim 的后期交集对比学习，以提高模型的查询-文档匹配能力。所提出的框架对我们的工业数据集 TaoLive 进行了重大改进，展示了我们方案的进步。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OFAR:+A+Multimodal+Evidence+Retrieval+Framework+for+Illegal+Live-streaming+Identification)|0|
|[Complex Item Set Recommendation](https://doi.org/10.1145/3539618.3594248)|Mozhdeh Ariannezhad, Ming Li, Sami Jullien, Maarten de Rijke|University of Amsterdam, Amsterdam, Netherlands|In this tutorial, we aim to shed light on the task of recommending a set of multiple items at once. In this scenario, historical interaction data between users and items could also be in the form of a sequence of interactions with sets of items. Complex sets of items being recommended together occur in different and diverse domains, such as grocery shopping with so-called baskets and fashion set recommendation with a focus on outfits rather than individual clothing items. We describe the current landscape of research and expose our participants to real-world examples of item set recommendation. We further provide our audience with hands-on experience via a notebook session. Finally, we describe open challenges and call for further research in the area, which we hope will inspire both early stage and more experienced researchers.|在本教程中，我们的目标是阐明一次推荐多个项目的任务。在这个场景中，用户和项之间的历史交互数据也可以是与项集的交互序列的形式。复杂的项目集被推荐一起出现在不同的和多样化的领域，例如杂货店购物与所谓的篮子和时尚集推荐的重点是服装而不是个别的衣服项目。我们描述了当前的研究状况，并让我们的参与者接触到项目集推荐的现实世界的例子。我们进一步提供我们的观众通过笔记本会议的实践经验。最后，我们描述了开放的挑战，并呼吁在该领域进一步的研究，我们希望这将激励早期阶段和更有经验的研究人员。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Complex+Item+Set+Recommendation)|0|
|[Recent Advances in the Foundations and Applications of Unbiased Learning to Rank](https://doi.org/10.1145/3539618.3594247)|Shashank Gupta, Philipp Hager, Jin Huang, Ali Vardasbi, Harrie Oosterhuis||Since its inception, the field of unbiased learning to rank (ULTR) has remained very active and has seen several impactful advancements in recent years. This tutorial provides both an introduction to the core concepts of the field and an overview of recent advancements in its foundations along with several applications of its methods. The tutorial is divided into four parts: Firstly, we give an overview of the different forms of bias that can be addressed with ULTR methods. Secondly, we present a comprehensive discussion of the latest estimation techniques in the ULTR field. Thirdly, we survey published results of ULTR in real-world applications. Fourthly, we discuss the connection between ULTR and fairness in ranking. We end by briefly reflecting on the future of ULTR research and its applications. This tutorial is intended to benefit both researchers and industry practitioners who are interested in developing new ULTR solutions or utilizing them in real-world applications.|自成立以来，无偏学习排名(ULTR)领域一直非常活跃，近年来取得了一些有影响力的进展。本教程介绍了该领域的核心概念，概述了该领域基础方面的最新进展及其方法的若干应用。本教程分为四个部分: 首先，我们概述了不同形式的偏倚，可以用 ULTR 方法处理。其次，我们对 ULTR 领域中的最新估计技术进行了全面的讨论。第三，我们调查了已发表的 ULTR 在实际应用中的结果。第四，我们讨论了 ULTR 与排名公平性之间的关系。最后，我们简要地回顾了 ULTR 研究及其应用的未来。本教程旨在使那些对开发新的 ULTR 解决方案或在实际应用中使用它们感兴趣的研究人员和行业从业人员受益。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Recent+Advances+in+the+Foundations+and+Applications+of+Unbiased+Learning+to+Rank)|0|
|[Large-Scale Data Processing for Information Retrieval Applications](https://doi.org/10.1145/3539618.3591797)|Pooya Khandel|University of Amsterdam, Amsterdam, Netherlands|Developing Information Retrieval (IR) applications such as search engines and recommendation systems require training of models that are growing in complexity and size with immense collections of data that contain multiple dimensions (documents/items text, user profiles, and interactions). Much of the research in IR concentrates on improving the performance of ranking models; however, given the high training time and high computational resources required to improve the performance by designing new models, it is crucial to address efficiency aspects of the design and deployment of IR applications at large-scale. In my thesis, I aim to improve the training efficiency of IR applications and speed up the development phase of new models, by applying dataset distillation approaches to reduce the dataset size while preserving the ranking quality and employing efficient High-Performance Computing (HPC) solutions to increase the processing speed.|开发像搜索引擎和推荐系统这样的信息检索(IR)应用需要对模型进行培训，这些模型的复杂性和规模都在不断增长，其中包含大量的数据，这些数据包含多个维度(文档/项目文本、用户配置文件和交互)。信息检索领域的大部分研究集中在提高排序模型的性能上; 然而，考虑到通过设计新模型来提高性能所需的高训练时间和高计算资源，在大规模设计和部署信息检索应用时，解决效率方面的问题是至关重要的。在本文中，我的目标是提高红外应用程序的训练效率，加快新模型的开发阶段，通过应用数据集精馏方法来减少数据集的大小，同时保持排序质量，并使用高效的高性能计算(HPC)解决方案来提高处理速度。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large-Scale+Data+Processing+for+Information+Retrieval+Applications)|0|
|[Generative Information Retrieval](https://doi.org/10.1145/3539618.3591871)|Marc Najork|Instacart, San Francisco, CA, USA; Walmart Labs, Sunnyvale, CA, USA|ABSTRACTIn the relatively short history of machine learning, the subtle balance between engineering and theoretical progress has been proved critical at various stages. The most recent wave of AI has brought to the IR community powerful techniques, particularly for pattern recognition. While many benefits from the burst of ideas as numerous tasks become algorithmically feasible, the balance is tilting toward the application side. The existing theoretical tools in IR can no longer explain, guide, and justify the newly-established methodologies. With no choices, we have to bet our design on black-box mechanisms that we only empirically understand. The consequences can be suffering: in stark contrast to how the IR industry has envisioned modern AI making life easier, many are experiencing increased confusion and costs in data manipulation, model selection, monitoring, censoring, and decision making. This reality is not surprising: without handy theoretical tools, we often lack principled knowledge of the pattern recognition model's expressivity, optimization property, generalization guarantee, and our decision-making process has to rely on over-simplified assumptions and human judgments from time to time. Facing all the challenges, we started researching advanced theoretical tools emerging from various domains that can potentially resolve modern IR problems. We encountered many impactful ideas and made several independent publications emphasizing different pieces. Time is now to bring the community a systematic tutorial on how we successfully adapt those tools and make significant progress in understanding, designing, and eventually productionize impactful IR systems. We emphasize systematicity because IR is a comprehensive discipline that touches upon particular aspects of learning, causal inference analysis, interactive (online) decision-making, etc. It thus requires systematic calibrations to render the actual usefulness of the imported theoretical tools to serve IR problems, as they usually exhibit unique structures and definitions. Therefore, we plan this tutorial to systematically demonstrate our learning and successful experience of using advanced theoretical tools for understanding and designing IR systems.|在机器学习相对较短的历史中，工程与理论进步之间的微妙平衡在不同的阶段被证明是至关重要的。最近的人工智能浪潮给红外社区带来了强大的技术，特别是模式识别。虽然随着大量任务在算法上变得可行，思想的迸发带来了许多好处，但是平衡正在向应用程序方面倾斜。现有的 IR 理论工具已经不能解释、指导和证明新建立的方法论。由于别无选择，我们不得不把我们的设计押在我们只能凭经验理解的黑盒机制上。其结果可能是痛苦的: 与红外行业设想的现代人工智能如何使生活变得更容易形成鲜明对比的是，许多人在数据操作、模型选择、监控、审查和决策方面正经历着越来越多的混乱和成本。这一现实并不令人惊讶: 没有方便的理论工具，我们往往缺乏模式识别模型的表达能力、优化特性、泛化保证的原则性知识，我们的决策过程不得不依赖于过于简化的假设和人为判断。面对这些挑战，我们开始研究来自不同领域的先进理论工具，这些工具可以解决现代国际关系问题。我们遇到了许多有影响力的想法，并作出了几个独立的出版物，强调不同的作品。现在是时候给社区带来一个系统的教程，告诉他们我们如何成功地调整这些工具，并在理解、设计和最终生产有影响力的 IR 系统方面取得重大进展。我们强调系统性，因为国际关系是一个综合性的学科，涉及到学习的特定方面，因果推理分析，互动(在线)决策，等等。因此，需要进行系统的校准，以提供进口的理论工具的实际用途，以服务红外问题，因为他们通常表现出独特的结构和定义。因此，我们计划本教程系统地展示我们使用先进的理论工具来理解和设计 IR 系统的学习和成功经验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+Information+Retrieval)|0|
|[Tasks, Copilots, and the Future of Search](https://doi.org/10.1145/3539618.3593069)|Ryen W. White|Microsoft Research, Redmond, WA, USA|Tasks are central to information retrieval (IR) and drive interactions with search systems [2, 4, 10]. Understanding and modeling tasks helps these systems better support user needs [8, 9, 11]. This keynote focuses on search tasks, the emergence of generative artificial intelligence (AI), and the implications of recent work at their intersection for the future of search. Recent estimates suggest that half of Web search queries go unanswered, many of them connected to complex search tasks that are ill-defined or multi-step and span several queries[6]. AI copilots, e.g., ChatGPT and Bing Chat, are emerging to address complex search tasks and many other challenges. These copilots are built on large foundation models such as GPT-4 and are being extended with skills and plugins. Copilots broaden the surface of tasks achievable via search, moving toward creation not just finding (e.g., interview preparation, email composition), and can make searchers more efficient and more successful. Users currently engage with AI copilots via natural language queries and dialog and the copilots generate answers with source attribution [7]. However, in delegating responsibility for answer generation, searchers also lose some control over aspects of the search process, such as directly manipulating queries and examining lists of search results [1]. The efficiency gains from auto-generating a single, synthesized answer may also reduce opportunities for user learning and serendipity. A wholesale move to copilots for all search tasks is neither practical nor necessary: model inference is expensive, conversational interfaces are unfamiliar to many users in a search context, and traditional search already excels for many types of task. Instead, experiences that unite search and chat are becoming more common, enabling users to adjust the modality and other aspects (e.g., answer tone) based on the task. The rise of AI copilots creates many opportunities for IR, including aligning generated answers with user intent, tasks, and applications via human feedback [3]; understanding copilot usage, including functional fixedness [5]; using context and data to tailor responses to people and situations (e.g., grounding, personalization); new search experiences (e.g., unifying search and chat); reliability and safety (e.g., accuracy, bias); understanding impacts on user learning and agency; and evaluation (e.g., model-based feedback, searcher simulations [12] repeatability). Research in these and related areas will enable search systems to more effectively utilize new copilot technologies together with traditional search to help searchers better tackle a wider variety of tasks.|任务是信息检索(IR)的核心，并驱动与搜索系统的交互[2,4,10]。理解和建模任务有助于这些系统更好地支持用户需求[8,9,11]。本演讲的重点是搜索任务，生成性人工智能(AI)的出现，以及最近的工作在其交叉点对搜索的未来的影响。最近的估计表明，一半的网络搜索查询没有得到回答，其中许多与复杂的搜索任务相关，这些任务定义不明确或多步骤，跨越多个查询[6]。人工智能副驾驶员，如 ChatGPT 和 Bing Chat，正在出现，以解决复杂的搜索任务和许多其他挑战。这些副驾驶员是建立在大型基础模型，如 GPT-4，并正在扩展技能和插件。副驾驶员拓宽了通过搜索可以完成的任务的表面，不仅仅是创造性的搜索(例如，面试准备，电子邮件写作) ，而且可以使搜索者更有效率和更成功。用户目前通过自然语言查询和对话与人工智能副驾驶员交流，副驾驶员通过源代码归属生成答案[7]。然而，在分配生成答案的责任时，搜索者也失去了对搜索过程某些方面的控制，比如直接操作查询和检查搜索结果列表[1]。自动生成一个单一的综合答案所带来的效率也可能减少用户学习和意外发现的机会。对于所有搜索任务而言，大规模转向副驾驶员既不实际，也不必要: 模型推理成本高昂，对于搜索上下文中的许多用户而言，会话界面并不熟悉，而且传统搜索已经在许多类型的任务中表现出色。相反，将搜索和聊天结合在一起的体验正变得越来越普遍，使用户能够根据任务调整模式和其他方面(例如，回答语气)。人工智能副驾驶员的崛起为信息检索创造了许多机会，包括通过人工反馈将生成的答案与用户意图、任务和应用程序对齐[3] ; 理解副驾驶员的使用，包括功能固着[5] ; 使用上下文和数据来调整对人和情况的反应(例如，禁足，个性化) ; 新的搜索体验(例如，统一搜索和聊天) ; 可靠性和安全性(例如，准确性，偏见) ; 理解对用户学习和代理的影响; 以及评估(例如，基于模型的反馈，搜索器模拟[。这些领域和相关领域的研究将使搜索系统能够更有效地利用新的副驾驶技术和传统搜索，以帮助搜索人员更好地处理更广泛的各种任务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tasks,+Copilots,+and+the+Future+of+Search)|0|
|[Learning to Re-rank with Constrained Meta-Optimal Transport](https://doi.org/10.1145/3539618.3591714)|Andrés Hoyos Idrobo||Many re-ranking strategies in search systems rely on stochastic ranking policies, encoded as Doubly-Stochastic (DS) matrices, that satisfy desired ranking constraints in expectation, e.g., Fairness of Exposure (FOE). These strategies are generally two-stage pipelines: \emph{i)} an offline re-ranking policy construction step and \emph{ii)} an online sampling of rankings step. Building a re-ranking policy requires repeatedly solving a constrained optimization problem, one for each issued query. Thus, it is necessary to recompute the optimization procedure for any new/unseen query. Regarding sampling, the Birkhoff-von-Neumann decomposition (BvND) is the favored approach to draw rankings from any DS-based policy. However, the BvND is too costly to compute online. Hence, the BvND as a sampling solution is memory-consuming as it can grow as $\gO(N\, n^2)$ for $N$ queries and $n$ documents. This paper offers a novel, fast, lightweight way to predict fair stochastic re-ranking policies: Constrained Meta-Optimal Transport (CoMOT). This method fits a neural network shared across queries like a learning-to-rank system. We also introduce Gumbel-Matching Sampling (GumMS), an online sampling approach from DS-based policies. Our proposed pipeline, CoMOT + GumMS, only needs to store the parameters of a single model, and it generalizes to unseen queries. We empirically evaluated our pipeline on the TREC 2019 and 2020 datasets under FOE constraints. Our experiments show that CoMOT rapidly predicts fair re-ranking policies on held-out data, with a speed-up proportional to the average number of documents per query. It also displays fairness and ranking performance similar to the original optimization-based policy. Furthermore, we empirically validate the effectiveness of GumMS to approximate DS-based policies in expectation.|搜索系统中的许多重排序策略都依赖于随机排序策略，这些策略被编码为双随机(DS)矩阵，满足期望的排序约束，例如，公平曝光(FOE)。这些策略通常是两个阶段的管道: emph { i }离线重新排序策略构建步骤和 emph { ii }在线排序步骤抽样。构建一个重新排序策略需要重复解决一个受限制的最佳化问题，每个发出的查询一个。因此，有必要重新计算任何新的/未见查询的优化过程。关于抽样，Birkhoff-von-Neumann 分解(BvND)是从任何基于 DS 的政策中提取排名的最受欢迎的方法。然而，在线计算 BvND 的成本太高。因此，作为抽样解决方案的 BvND 占用内存，因为对于 $N $查询和 $n $document，它可以增长为 $gO (N，n ^ 2) $。本文提出了一种新颖、快速、轻量级的预测公平随机重排策略的方法: 约束元最优运输(CoMOT)。该方法适用于跨查询共享的神经网络，如学习排序系统。我们还介绍了 Gumbel 匹配抽样(GumMS) ，一种基于 DS 策略的在线抽样方法。我们提出的流水线 CoMOT + GumMS 只需要存储单个模型的参数，并且它可以推广到不可见的查询。我们在 FOE 约束下对 TREC 2019和2020数据集的管道进行了实证评估。我们的实验表明，CoMOT 能够快速地预测对被拒绝的数据进行公平的重新排序的策略，其速度与每个查询的平均文档数成正比。它还显示公平性和排名性能类似于原来的优化为基础的政策。此外，我们还实验验证了 GumMS 在预期情况下逼近基于 DS 策略的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Re-rank+with+Constrained+Meta-Optimal+Transport)|0|
|[Constructing Tree-based Index for Efficient and Effective Dense Retrieval](https://doi.org/10.1145/3539618.3591651)|Haitao Li, Qingyao Ai, Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Zheng Liu, Zhao Cao||Recent studies have shown that Dense Retrieval (DR) techniques can significantly improve the performance of first-stage retrieval in IR systems. Despite its empirical effectiveness, the application of DR is still limited. In contrast to statistic retrieval models that rely on highly efficient inverted index solutions, DR models build dense embeddings that are difficult to be pre-processed with most existing search indexing systems. To avoid the expensive cost of brute-force search, the Approximate Nearest Neighbor (ANN) algorithm and corresponding indexes are widely applied to speed up the inference process of DR models. Unfortunately, while ANN can improve the efficiency of DR models, it usually comes with a significant price on retrieval performance. To solve this issue, we propose JTR, which stands for Joint optimization of TRee-based index and query encoding. Specifically, we design a new unified contrastive learning loss to train tree-based index and query encoder in an end-to-end manner. The tree-based negative sampling strategy is applied to make the tree have the maximum heap property, which supports the effectiveness of beam search well. Moreover, we treat the cluster assignment as an optimization problem to update the tree-based index that allows overlapped clustering. We evaluate JTR on numerous popular retrieval benchmarks. Experimental results show that JTR achieves better retrieval performance while retaining high system efficiency compared with widely-adopted baselines. It provides a potential solution to balance efficiency and effectiveness in neural retrieval system designs.|近年来的研究表明，密集检索(DR)技术可以显著提高红外系统第一阶段检索的性能。尽管 DR 在实证研究中取得了一定的成效，但其应用仍然有限。与依赖于高效率倒排索引解决方案的统计检索模型相比，DR 模型构建了密集的嵌入，这些嵌入很难在大多数现有的搜索索引系统中进行预处理。为了避免昂贵的暴力搜索法成本，近似最近邻(ANN)算法和相应的索引被广泛应用于加快 DR 模型的推理过程。遗憾的是，尽管人工神经网络可以提高 DR 模型的效率，但它通常会给检索性能带来巨大的代价。为了解决这个问题，我们提出了 JTR，它代表了基于树的索引和查询编码的联合优化。具体来说，我们设计了一种新的统一对比学习丢失算法，用于以端到端的方式训练基于树的索引和查询编码器。采用基于树的负采样策略，使树具有最大的堆性质，很好地支持了波束搜索的有效性。此外，我们把集群分配当作一个最佳化问题，以更新允许重叠集群的基于树的索引。我们在许多流行的检索基准上评估 JTR。实验结果表明，与广泛采用的基线相比，JTR 在保持较高系统效率的同时，获得了较好的检索性能。它提供了一个潜在的解决方案，以平衡效率和有效的神经检索系统设计。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Constructing+Tree-based+Index+for+Efficient+and+Effective+Dense+Retrieval)|0|
|[Multivariate Representation Learning for Information Retrieval](https://doi.org/10.1145/3539618.3591740)|Hamed Zamani, Michael Bendersky||Dense retrieval models use bi-encoder network architectures for learning query and document representations. These representations are often in the form of a vector representation and their similarities are often computed using the dot product function. In this paper, we propose a new representation learning framework for dense retrieval. Instead of learning a vector for each query and document, our framework learns a multivariate distribution and uses negative multivariate KL divergence to compute the similarity between distributions. For simplicity and efficiency reasons, we assume that the distributions are multivariate normals and then train large language models to produce mean and variance vectors for these distributions. We provide a theoretical foundation for the proposed framework and show that it can be seamlessly integrated into the existing approximate nearest neighbor algorithms to perform retrieval efficiently. We conduct an extensive suite of experiments on a wide range of datasets, and demonstrate significant improvements compared to competitive dense retrieval models.|密集检索模型使用双编码器网络结构来学习查询和文档表示。这些表示通常采用向量表示的形式，它们的相似性通常使用点乘函数计算。本文提出了一种新的密集检索表示学习框架。我们的框架不是为每个查询和文档学习一个向量，而是学习一个联合分布，并使用负的多元 KL 散度来计算分布之间的相似性。出于简单和有效的原因，我们假设这些分布是多元正态分布，然后训练大型语言模型来产生这些分布的均值和方差向量。我们为该框架提供了理论基础，并表明该框架可以无缝集成到现有的近似最近邻算法中，从而有效地进行检索。我们在广泛的数据集上进行了大量的实验，并证明了与竞争性的密集检索模型相比有显著的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multivariate+Representation+Learning+for+Information+Retrieval)|0|
|[Prompt Learning for News Recommendation](https://doi.org/10.1145/3539618.3591752)|Zizhuo Zhang, Bang Wang||Some recent \textit{news recommendation} (NR) methods introduce a Pre-trained Language Model (PLM) to encode news representation by following the vanilla pre-train and fine-tune paradigm with carefully-designed recommendation-specific neural networks and objective functions. Due to the inconsistent task objective with that of PLM, we argue that their modeling paradigm has not well exploited the abundant semantic information and linguistic knowledge embedded in the pre-training process. Recently, the pre-train, prompt, and predict paradigm, called \textit{prompt learning}, has achieved many successes in natural language processing domain. In this paper, we make the first trial of this new paradigm to develop a \textit{Prompt Learning for News Recommendation} (Prompt4NR) framework, which transforms the task of predicting whether a user would click a candidate news as a cloze-style mask-prediction task. Specifically, we design a series of prompt templates, including discrete, continuous, and hybrid templates, and construct their corresponding answer spaces to examine the proposed Prompt4NR framework. Furthermore, we use the prompt ensembling to integrate predictions from multiple prompt templates. Extensive experiments on the MIND dataset validate the effectiveness of our Prompt4NR with a set of new benchmark results.|最近的一些文本{新闻推荐}(NR)方法引入了一个预训练语言模型(Pre-training Language Model，PLM) ，通过使用精心设计的特定于推荐的神经网络和目标函数，遵循普通的预训练和微调范式，对新闻表示进行编码。由于 PLM 的任务目标不一致，我们认为他们的建模范式没有很好地利用预培训过程中包含的丰富的语义信息和语言知识。近年来，自然语言处理领域的预训练、提示和预测范式(文本{提示学习})取得了许多成功。本文首次尝试开发了一个文本提示学习新闻推荐(Prompt4NR)框架，该框架将预测用户是否点击候选新闻的任务转化为完形填空式的面具预测任务。具体来说，我们设计了一系列的提示模板，包括离散的、连续的和混合的模板，并构造它们相应的答案空间来检查提出的 Prompt4NR 框架。此外，我们使用提示合并来集成来自多个提示模板的预测。MIND 数据集上的大量实验验证了 Prompt4NR 的有效性，并提供了一组新的基准测试结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Prompt+Learning+for+News+Recommendation)|0|
|[Alleviating Matthew Effect of Offline Reinforcement Learning in Interactive Recommendation](https://doi.org/10.1145/3539618.3591636)|Chongming Gao, Kexin Huang, Jiawei Chen, Yuan Zhang, Biao Li, Peng Jiang, Shiqi Wang, Zhong Zhang, Xiangnan He||Offline reinforcement learning (RL), a technology that offline learns a policy from logged data without the need to interact with online environments, has become a favorable choice in decision-making processes like interactive recommendation. Offline RL faces the value overestimation problem. To address it, existing methods employ conservatism, e.g., by constraining the learned policy to be close to behavior policies or punishing the rarely visited state-action pairs. However, when applying such offline RL to recommendation, it will cause a severe Matthew effect, i.e., the rich get richer and the poor get poorer, by promoting popular items or categories while suppressing the less popular ones. It is a notorious issue that needs to be addressed in practical recommender systems. In this paper, we aim to alleviate the Matthew effect in offline RL-based recommendation. Through theoretical analyses, we find that the conservatism of existing methods fails in pursuing users' long-term satisfaction. It inspires us to add a penalty term to relax the pessimism on states with high entropy of the logging policy and indirectly penalizes actions leading to less diverse states. This leads to the main technical contribution of the work: Debiased model-based Offline RL (DORL) method. Experiments show that DORL not only captures user interests well but also alleviates the Matthew effect. The implementation is available via https://github.com/chongminggao/DORL-codes.|离线强化学习(off-line)是一种无需与在线环境交互就可以从已记录的数据中学习策略的技术，已经成为诸如交互式推荐等决策过程中的一个有利选择。脱机 RL 面临价值高估问题。为了解决这个问题，现有的方法采用了保守主义，例如，通过限制学习政策接近行为政策或惩罚很少访问的国家行动对。然而，当把这种线下 RL 应用于推荐时，它会产生一种严重的马太效应，也就是说，富人变得更富，穷人变得更穷，通过推销受欢迎的项目或类别，同时压制不太受欢迎的项目或类别。这是一个臭名昭著的问题，需要解决的实际推荐系统。本文旨在减轻基于 RL 的离线推荐中的马太效应。通过理论分析，我们发现现有方法的保守性不足以追求用户的长期满意度。它启发我们增加一个惩罚项，以放松对伐木政策的高熵状态的悲观情绪，并间接惩罚导致较少多样性状态的行动。这导致了这项工作的主要技术贡献: 基于消偏模型的离线 RL (DORL)方法。实验表明，DORL 不仅能够很好地捕获用户兴趣，而且能够减轻马太效应。有关实施方案可透过 https://github.com/chongminggao/dorl-codes 提供。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Alleviating+Matthew+Effect+of+Offline+Reinforcement+Learning+in+Interactive+Recommendation)|0|
|[Distributionally Robust Sequential Recommnedation](https://doi.org/10.1145/3539618.3591668)|Rui Zhou, Xian Wu, Zhaopeng Qiu, Yefeng Zheng, Xu Chen|Renmin University of China, Beijing, China; Tencent Jarvis Lab, Shenzhen, China|Modeling user sequential behaviors have been demonstrated to be effective in promoting the recommendation performance. While previous work has achieved remarkable successes, they mostly assume that the training and testing distributions are consistent, which may contradict with the diverse and complex user preferences, and limit the recommendation performance in real-world scenarios. To alleviate this problem, in this paper, we propose a robust sequential recommender framework to overcome the potential distribution shift between the training and testing sets. In specific, we firstly simulate different training distributions via sample reweighting. Then, we minimize the largest loss induced by these distributions to optimize the 'worst-case' loss for improving the model robustness. Considering that there can be too many sample weights, which may introduce too much flexibility and be hard to optimize, we cluster the training samples based on both hard and soft strategies, and assign each cluster with a unified weight. At last, we analyze our framework by presenting the generalization error bound of the above minimax objective, which help us to better understand the proposed framework from the theoretical perspective. We conduct extensive experiments based on three real-world datasets to demonstrate the effectiveness of our proposed framework. To reproduce our experiments and promote this research direction, we have released our project at https://anonymousrsr.github.io/RSR/.|建立用户序列行为模型可以有效地提高推荐性能。虽然以前的工作已经取得了显著的成功，他们大多假设培训和测试分布是一致的，这可能与多样化和复杂的用户偏好相矛盾，并限制了推荐性能在现实世界的场景。为了解决这一问题，本文提出了一种鲁棒的顺序推荐框架来克服训练集和测试集之间潜在的分布偏移。具体地说，我们首先通过样本重权重模拟不同的训练分布。然后，我们最小化由这些分布引起的最大损失，优化“最坏情况”的损失，以提高模型的鲁棒性。针对训练样本权重过多、灵活性大、难以优化等问题，采用软硬策略相结合的方法对训练样本进行聚类，并对每个聚类进行统一权重分配。最后，我们通过提出上述极大极小目标的泛化误差界限来分析我们的框架，这有助于我们从理论的角度更好地理解提出的框架。我们进行了广泛的实验基于三个真实世界的数据集，以证明我们提出的框架的有效性。为了重现我们的实验并推动这一研究方向，我们已经在 https://anonymousrsr.github.io/rsr/上发布了我们的项目。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distributionally+Robust+Sequential+Recommnedation)|0|
|[Knowledge-refined Denoising Network for Robust Recommendation](https://doi.org/10.1145/3539618.3591707)|Xinjun Zhu, Yuntao Du, Yuren Mao, Lu Chen, Yujia Hu, Yunjun Gao||Knowledge graph (KG), which contains rich side information, becomes an essential part to boost the recommendation performance and improve its explainability. However, existing knowledge-aware recommendation methods directly perform information propagation on KG and user-item bipartite graph, ignoring the impacts of \textit{task-irrelevant knowledge propagation} and \textit{vulnerability to interaction noise}, which limits their performance. To solve these issues, we propose a robust knowledge-aware recommendation framework, called \textit{Knowledge-refined Denoising Network} (KRDN), to prune the task-irrelevant knowledge associations and noisy implicit feedback simultaneously. KRDN consists of an adaptive knowledge refining strategy and a contrastive denoising mechanism, which are able to automatically distill high-quality KG triplets for aggregation and prune noisy implicit feedback respectively. Besides, we also design the self-adapted loss function and the gradient estimator for model optimization. The experimental results on three benchmark datasets demonstrate the effectiveness and robustness of KRDN over the state-of-the-art knowledge-aware methods like KGIN, MCCLK, and KGCL, and also outperform robust recommendation models like SGL and SimGCL.|知识图(KG)包含了丰富的边信息，是提高推荐性能、增强推荐可解释性的重要组成部分。然而，现有的知识感知推荐方法直接在 KG 和用户项二分图上进行信息传播，忽略了文本{任务无关知识传播}和文本{交互噪声脆弱性}的影响，从而限制了它们的性能。为了解决这些问题，我们提出了一个鲁棒的知识感知推荐框架，称为 texttit { Knowledge-finedDenoisingNetwork }(KRDN) ，它可以同时修剪与任务无关的知识关联和有噪隐式反馈。KRDN 由自适应知识精炼策略和对比去噪机制两部分组成，它们分别能够自动提取高质量的 KG 三元组用于聚集和删除含噪隐式反馈。此外，我们还设计了自适应损失函数和模型优化的梯度估计器。在三个基准数据集上的实验结果显示了 KRDN 相对于最先进的知识感知方法(如 KGIN、 MCCLK 和 KGCL)的有效性和稳健性，也优于稳健的推荐模型(如 SGL 和 SimGCL)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge-refined+Denoising+Network+for+Robust+Recommendation)|0|
|[Mixed-Curvature Manifolds Interaction Learning for Knowledge Graph-aware Recommendation](https://doi.org/10.1145/3539618.3591730)|Jihu Wang, Yuliang Shi, Han Yu, Xinjun Wang, Zhongmin Yan, Fanyu Kong|Shandong University, Jinan, China; Nanyang Technological University, Singapore, Singapore; Shandong University & Dareway Software Co., Ltd, Jinan, China|As auxiliary collaborative signals, the entity connectivity and relation semanticity beneath knowledge graph (KG) triples can alleviate the data sparsity and cold-start issues of recommendation tasks. Thus many works consider obtaining user and item representations via information aggregation on graph-structured data within Euclidean space. However, the scale-free graphs (e.g., KGs) inherently exhibit non-Euclidean geometric topologies, such as tree-like and circle-like structures. The existing recommendation models built in a single type of embedding space do not have enough capacity to embrace various geometric patterns, consequently, resulting in suboptimal performance. To address this limitation, we propose a KG-aware recommendation model with mixed-curvature manifolds interaction learning, namely CurvRec. On the one hand, it aims to preserve various global geometric structures in KG with mixed-curvature manifold spaces as the backbone. On the other hand, we integrate Ricci curvature into graph convolutional networks (GCNs) to capture local geometric structural properties when aggregating neighbor nodes. Besides, to exploit the expressive spatial features in KG, we incorporate interaction learning to ensure the geometric message passing between curved manifolds. Specifically, we adopt curvature-aware geodesic distance metrics to maximize the mutual information between Euclidean space and non-Euclidean spaces. Through extensive experiments, we demonstrate that the proposed CurvRec outperforms state-of-the-art baselines.|知识图(KG)三元组下的实体连通性和关系语义作为辅助协同信号，可以缓解推荐任务的数据稀疏性和冷启动问题。因此，许多工作考虑通过在欧几里得空间中对图形结构数据进行信息聚合来获得用户和项目表示。然而，无标度图(例如 KGs)本质上表现出非欧几里德几何拓扑，例如树状结构和圆形结构。现有的建立在单一类型嵌入空间中的推荐模型没有足够的容量来包含各种几何模式，从而导致性能不理想。针对这一局限性，提出了一种基于 KG 的混合曲率流形交互学习推荐模型，即 CurvRec。一方面，以混合曲率流形空间为骨架，保留了 KG 中的各种整体几何结构;。另一方面，我们将 Ricci 曲率集成到图卷积网络(GCNs)中，以便在聚集邻居节点时捕获局部的几何结构性质。此外，为了利用 KG 中的表达空间特征，我们引入了交互学习来保证曲面流形之间的几何信息传递。具体来说，我们采用曲率感知的测地距离度量来最大化欧氏空间和非欧氏空间之间的互信息。通过大量的实验，我们证明了所提出的 CurvRec 优于最先进的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mixed-Curvature+Manifolds+Interaction+Learning+for+Knowledge+Graph-aware+Recommendation)|0|
|[EEDN: Enhanced Encoder-Decoder Network with Local and Global Context Learning for POI Recommendation](https://doi.org/10.1145/3539618.3591678)|Xinfeng Wang, Fumiyo Fukumoto, Jin Cui, Yoshimi Suzuki, Jiyi Li, Dongjin Yu|University of Yamanashi, Kofu, Japan; Hangzhou Dianzi University, Hangzhou, China|The point-of-interest (POI) recommendation predicts users' destinations, which might be of interest to users and has attracted considerable attention as one of the major applications in location-based social networks (LBSNs). Recent work on graph-based neural networks (GNN) or matrix factorization-based (MF) approaches has resulted in better representations of users and POIs to forecast users' latent preferences. However, they still suffer from the implicit feedback and cold-start problems of check-in data, as they cannot capture both local and global graph-based relations among users (or POIs) simultaneously, and the cold-start neighbors are not handled properly during graph convolution in GNN. In this paper, we propose an enhanced encoder-decoder network (EEDN) to exploit rich latent features between users, POIs, and interactions between users and POIs for POI recommendation. The encoder of EEDN utilizes a hybrid hypergraph convolution to enhance the aggregation ability of each graph convolution step and learns to derive more robust cold-start-aware user representations. In contrast, the decoder mines local and global interactions by both graph- and sequential-based patterns for modeling implicit feedback, especially to alleviate exposure bias. Extensive experiments in three public real-world datasets demonstrate that EEDN outperforms state-of-the-art methods. Our source codes and data are released at https://github.com/WangXFng/EEDN|兴趣点(POI)推荐可以预测用户的目的地，这可能是用户感兴趣的，并且作为基于位置的社交网络(LBSNs)的主要应用之一已经引起了相当大的关注。基于图的神经网络(GNN)或基于矩阵分解(MF)的方法已经导致了用户和 POI 更好的表示，以预测用户的潜在偏好。然而，由于它们不能同时捕获用户之间基于局部和全局图的关系(或 POI) ，并且在 GNN 中的图卷积过程中没有正确处理冷启动邻居，因此仍然存在签入数据的隐式反馈和冷启动问题。本文提出了一种增强型编解码网络(EEDN) ，利用用户间、用户与 POI 之间的交互以及用户与 POI 之间丰富的潜在特征进行 POI 推荐。EEDN 的编码器利用混合超图卷积来增强每个图卷积步骤的聚合能力，并学习推导出更健壮的冷启动感知用户表示。相比之下，解码器通过基于图和序列的模式挖掘局部和全局的交互作用来建模隐式反馈，特别是为了减轻暴露偏差。在三个公开的真实世界数据集中的大量实验表明，EEDN 的性能优于最先进的方法。我们的源代码和数据 https://github.com/wangxfng/eedn 公布|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EEDN:+Enhanced+Encoder-Decoder+Network+with+Local+and+Global+Context+Learning+for+POI+Recommendation)|0|
|[Adaptive Graph Representation Learning for Next POI Recommendation](https://doi.org/10.1145/3539618.3591634)|Zhaobo Wang, Yanmin Zhu, Chunyang Wang, Wenze Ma, Bo Li, Jiadi Yu|Shanghai Jiao Tong University, Shanghai, China; Hong Kong University of Science and Technology, Hong Kong, Hong Kong|Next Point-of-Interest (POI) recommendation is an essential part of the flourishing location-based applications, where the demands of users are not only conditioned by their recent check-in behaviors but also by the critical influence stemming from geographical dependencies among POIs. Existing methods leverage Graph Neural Networks with the aid of pre-defined POI graphs to capture such indispensable correlations for modeling user preferences, assuming that the appropriate geographical dependencies among POIs could be pre-determined. However, the pre-defined graph structures are always far from the optimal graph topology due to noise and adaptability issues, which may decrease the expressivity of learned POI representations as well as the credibility of modeling user preferences. In this paper, we propose a novel Adaptive Graph Representation-enhanced Attention Network (AGRAN) for next POI recommendation, which explores the utilization of graph structure learning to replace the pre-defined static graphs for learning more expressive representations of POIs. In particular, we develop an adaptive POI graph matrix and learn it via similarity learning with POI embeddings, automatically capturing the underlying geographical dependencies for representation learning. Afterward, we incorporate the learned representations of POIs and personalized spatial-temporal information with an extension to the self-attention mechanism for capturing dynamic user preferences. Extensive experiments conducted on two real-world datasets validate the superior performance of our proposed method over state-of-the-art baselines.|下一个兴趣点(POI)推荐是蓬勃发展的基于位置的应用程序的重要组成部分，其中用户的需求不仅受到他们最近的签入行为的制约，而且受到来自 POI 之间的地理依赖性的关键影响。现有的方法利用图形神经网络与预定义的 POI 图的帮助，以捕获这种不可或缺的相关性建模用户偏好，假设适当的地理依赖关系之间的 POI 可以预先确定。然而，由于噪声和适应性问题，预定义的图结构往往远离最优的图拓扑结构，这可能会降低所学习的 POI 表示的表达能力以及建模用户偏好的可信度。本文提出了一种新的自适应图表示增强注意力网络(AGRAN) ，探讨了利用图结构学习代替预定义的静态图学习更具表现力的 POI 表示。特别地，我们开发了一个自适应的 POI 图形矩阵，并通过 POI 嵌入的相似性学习来学习它，自动捕捉潜在的地理依赖性来进行表示学习。然后，将 POI 的学习表示和个性化的时空信息结合起来，扩展了自我注意机制来获取动态用户偏好。在两个真实世界数据集上进行的大量实验验证了我们提出的方法优于最先进的基线的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adaptive+Graph+Representation+Learning+for+Next+POI+Recommendation)|0|
|[Spatio-Temporal Hypergraph Learning for Next POI Recommendation](https://doi.org/10.1145/3539618.3591770)|Xiaodong Yan, Tengwei Song, Yifeng Jiao, Jianshan He, Jiaotuan Wang, Ruopeng Li, Wei Chu|Ant Group, Hangzhou, China; Ant Group, Beijing, China; Ant Group & Beihang University, Beijing, China|Next Point-of-Interest (POI) recommendation task focuses on predicting the immediate next position a user would visit, thus providing appealing location advice. In light of this, graph neural networks (GNNs) based models have recently been emerging as breakthroughs for this task due to their ability to learn global user preferences and alleviate cold-start challenges. Nevertheless, most existing methods merely focus on the relations between POIs, neglecting the higher-order information including user trajectories and the collaborative relations among trajectories. In this paper, we propose the Spatio-Temporal HyperGraph Convolutional Network (STHGCN). This model leverages a hypergraph to capture the trajectory-grain information and learn from user's historical trajectories (intra-user) as well as collaborative trajectories from other users (inter-user). Furthermore, a novel hypergraph transformer is introduced to effectively combine the hypergraph structure encoding with spatio-temporal information. Extensive experiments on real-world datasets demonstrate that our model outperforms the existing state-of-the-art methods and further analysis confirms the effectiveness in alleviating cold-start issues and achieving improved performance for both short and long trajectories.|下一个兴趣点(POI)推荐任务的重点是预测用户即将访问的下一个位置，从而提供有吸引力的位置建议。有鉴于此，基于图形神经网络(GNN)的模型由于能够学习全球用户偏好和缓解冷启动挑战，最近已经成为这项任务的突破点。然而，现有的方法大多只关注 POI 之间的关系，而忽略了包括用户轨迹在内的高阶信息以及轨迹之间的协同关系。本文提出了时空超图卷积网络(STHGCN)。该模型利用一个超图来捕获轨迹信息，并从用户的历史轨迹(内部用户)以及其他用户(内部用户)的协作轨迹中学习。此外，引入了一种新的超图变换器，将超图结构编码与时空信息有效地结合起来。对真实世界数据集的大量实验表明，我们的模型优于现有的最先进的方法，进一步的分析证实了在缓解冷启动问题和实现短期和长期轨迹性能改善方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Spatio-Temporal+Hypergraph+Learning+for+Next+POI+Recommendation)|0|
|[Mining Stable Preferences: Adaptive Modality Decorrelation for Multimedia Recommendation](https://doi.org/10.1145/3539618.3591729)|Jinghao Zhang, Qiang Liu, Shu Wu, Liang Wang||Multimedia content is of predominance in the modern Web era. In real scenarios, multiple modalities reveal different aspects of item attributes and usually possess different importance to user purchase decisions. However, it is difficult for models to figure out users' true preference towards different modalities since there exists strong statistical correlation between modalities. Even worse, the strong statistical correlation might mislead models to learn the spurious preference towards inconsequential modalities. As a result, when data (modal features) distribution shifts, the learned spurious preference might not guarantee to be as effective on the inference set as on the training set. We propose a novel MOdality DEcorrelating STable learning framework, MODEST for brevity, to learn users' stable preference. Inspired by sample re-weighting techniques, the proposed method aims to estimate a weight for each item, such that the features from different modalities in the weighted distribution are decorrelated. We adopt Hilbert Schmidt Independence Criterion (HSIC) as independence testing measure which is a kernel-based method capable of evaluating the correlation degree between two multi-dimensional and non-linear variables. Our method could be served as a play-and-plug module for existing multimedia recommendation backbones. Extensive experiments on four public datasets and four state-of-the-art multimedia recommendation backbones unequivocally show that our proposed method can improve the performances by a large margin.|在现代网络时代，多媒体内容占有主导地位。在实际场景中，多种模式揭示了商品属性的不同方面，对用户的购买决策具有不同的重要性。然而，由于模式之间存在很强的统计相关性，模型很难计算出用户对不同模式的真实偏好。更糟糕的是，这种强烈的统计相关性可能会误导模型，使其学会对无关紧要的模式的虚假偏好。因此，当数据(模态特征)分布发生变化时，学习到的虚假偏好可能不能保证在推理集上和在训练集上同样有效。我们提出了一个新的模态解相关稳定学习框架，MODEST 为简洁，学习用户的稳定偏好。该方法受样本重新加权技术的启发，旨在估计每个项目的权重，使不同方式的特征在加权分布中不相关。我们采用 Hilbert Schmidt 独立准则(HSIC)作为独立性测度，这是一种基于核的方法，能够评估两个多维和非线性变量之间的相关程度。我们的方法可以作为现有多媒体推荐主干的播放和即插即用模块。对四个公共数据集和四个最先进的多媒体推荐骨干网的大量实验表明，我们提出的方法可以大幅度提高性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mining+Stable+Preferences:+Adaptive+Modality+Decorrelation+for+Multimedia+Recommendation)|0|
|[MEME: Multi-Encoder Multi-Expert Framework with Data Augmentation for Video Retrieval](https://doi.org/10.1145/3539618.3591726)|SeongMin Kang, YoonSik Cho|Chung-Ang University, Seoul, Republic of Korea|Text-to-video(T2V) retrieval aims to find relevant videos from text queries. The recently introduced Contrastive Language Image Pretraining (CLIP), a pretrained language-vision model trained on large-scale image and caption pairs, has been extensively studied in the literature for this task. Existing studies on T2V task have aimed to transfer the CLIP knowledge and focus on enhancing retrieval performance through fine-grained representation learning. While fine-grained contrast has achieved some remarkable results, less attention has been paid to coarse-grained contrasts. To this end, we propose a method called Graph Patch Spreading (GPS) to aggregate patches across frames at the coarse-grained level. We apply GPS to our proposed framework called Multi-Encoder Multi-Expert (MEME) framework. Our proposed scheme is general enough to be applied to any existing CLIP-based video-text retrieval models. We demonstrate the effectiveness of our method on existing models over the benchmark datasets MSR-VTT, MSVD, and LSMDC datasets. Our code can be found at https://github.com/kang7734/MEME__.|文本到视频(T2V)检索旨在从文本查询中查找相关视频。最近引入的对比语言图像预训练(CLIP)是一种基于大规模图像和字幕对的预训练语言视觉模型。现有关于 T2V 任务的研究主要集中在传递 CLIP 知识和通过细粒度表征学习提高检索性能方面。虽然细粒度对比度取得了一些显著的成果，但对粗粒度对比度的研究较少。为此，我们提出了一种称为图形补丁扩展(GPS)的方法，在粗粒度层次上聚集跨帧的补丁。我们将 GPS 应用到我们提出的多编码器多专家(MEME)框架中。我们提出的方案是通用的，足以应用于任何现有的基于 CLIP 的视频文本检索模型。在 MSR-VTT、 MSVD 和 LSMDC 基准数据集上验证了该方法在现有模型上的有效性。我们的代码可以在 https://github.com/kang7734/meme__ 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MEME:+Multi-Encoder+Multi-Expert+Framework+with+Data+Augmentation+for+Video+Retrieval)|0|
|[Multi-Scenario Ranking with Adaptive Feature Learning](https://doi.org/10.1145/3539618.3591736)|Yu Tian, Bofang Li, Si Chen, Xubin Li, Hongbo Deng, Jian Xu, Bo Zheng, Qian Wang, Chenliang Li||Recently, Multi-Scenario Learning (MSL) is widely used in recommendation and retrieval systems in the industry because it facilitates transfer learning from different scenarios, mitigating data sparsity and reducing maintenance cost. These efforts produce different MSL paradigms by searching more optimal network structure, such as Auxiliary Network, Expert Network, and Multi-Tower Network. It is intuitive that different scenarios could hold their specific characteristics, activating the user's intents quite differently. In other words, different kinds of auxiliary features would bear varying importance under different scenarios. With more discriminative feature representations refined in a scenario-aware manner, better ranking performance could be easily obtained without expensive search for the optimal network structure. Unfortunately, this simple idea is mainly overlooked but much desired in real-world systems.Further analysis also validates the rationality of adaptive feature learning under a multi-scenario scheme. Moreover, our A/B test results on the Alibaba search advertising platform also demonstrate that Maria is superior in production environments.|近年来，多场景学习(Multi-Scenario Learning，MSL)技术被广泛应用于业界的推荐和检索系统中，因为它可以方便地从不同场景中进行转移学习，减少数据稀疏性，降低维护成本。这些努力通过寻找更多的最优网络结构，如辅助网络，专家网络和多塔网络，产生不同的 MSL 范例。直观地说，不同的场景可以保持其特定的特征，激活用户的意图完全不同。换句话说，不同的辅助特征在不同的场景下具有不同的重要性。随着更多的区分性特征表示在场景感知方式精化，可以很容易地获得更好的排序性能，而不需要对最优网络结构进行昂贵的搜索。不幸的是，这个简单的想法在现实系统中基本上被忽视了，但是却是非常需要的。进一步的分析也验证了多场景方案下自适应特征学习的合理性。此外，我们在阿里巴巴搜索广告平台的 A/B 测试结果也表明，玛丽亚在生产环境方面更胜一筹。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Scenario+Ranking+with+Adaptive+Feature+Learning)|0|
|[Curse of "Low" Dimensionality in Recommender Systems](https://doi.org/10.1145/3539618.3591659)|Naoto Ohsaka, Riku Togashi||Beyond accuracy, there are a variety of aspects to the quality of recommender systems, such as diversity, fairness, and robustness. We argue that many of the prevalent problems in recommender systems are partly due to low-dimensionality of user and item embeddings, particularly when dot-product models, such as matrix factorization, are used. In this study, we showcase empirical evidence suggesting the necessity of sufficient dimensionality for user/item embeddings to achieve diverse, fair, and robust recommendation. We then present theoretical analyses of the expressive power of dot-product models. Our theoretical results demonstrate that the number of possible rankings expressible under dot-product models is exponentially bounded by the dimension of item factors. We empirically found that the low-dimensionality contributes to a popularity bias, widening the gap between the rank positions of popular and long-tail items; we also give a theoretical justification for this phenomenon.|除了准确性之外，推荐系统的质量还有很多方面，比如多样性、公平性和鲁棒性。我们认为，推荐系统中许多普遍存在的问题，部分是由于用户和项目嵌入的维度较低，特别是当使用像矩阵分解这样的点产品模型时。在这项研究中，我们展示了一些经验证明，它们表明了用户/项目嵌入需要足够的维度来实现多样化、公平和强大的推荐。然后，我们提出了理论分析的表达能力的点积模式。我们的理论结果表明，在点乘模型下可表达的排名的数量是指数约束的项目因素的维度。实证研究发现，低维度导致了流行偏差，扩大了流行项目和长尾项目的排名差距，并对这一现象进行了理论解释。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Curse+of+"Low"+Dimensionality+in+Recommender+Systems)|0|
|[Subgraph Search over Neural-Symbolic Graphs](https://doi.org/10.1145/3539618.3591773)|Ye Yuan, Delong Ma, Anbiao Wu, Jianbin Qin|Northeastern University, China, Shenyang, China; Shenzhen University, Shenzhen, China; Beijing Institute of Technology, Beijing, China|In this paper, we propose neural-symbolic graph databases (NSGDs) that extends traditional graph data with content and structural embeddings in every node. The content embeddings can represent unstructured data (e.g., images, videos, and texts), while structural embeddings can be used to deal with incomplete graphs. We can advocate machine learning models (e.g., deep learning) to transform unstructured data and graph nodes to these embeddings. NSGDs can support a wide range of applications (e.g., online recommendation and natural language question answering) in social-media networks, multi-modal knowledge graphs and etc. As a typical search over graphs, we study subgraph search over a large NSGD, called neural-symbolic subgraph matching (NSMatch) that includes a novel ranking search function. Specifically, we develop a general algorithmic framework to process NSMatch efficiently. Using real-life multi-modal graphs, we experimentally verify the effectiveness, scalability and efficiency of NSMatch.|在本文中，我们提出了神经符号图数据库(NSGD) ，它扩展了传统图数据的内容和结构嵌入在每个节点。内容嵌入可以表示非结构化数据(如图像、视频和文本) ，而结构嵌入可以用来处理不完整的图形。我们可以提倡机器学习模型(例如深度学习) ，将非结构化数据和图形节点转换为这些嵌入。NSGD 可以支持社交媒体网络、多模式知识图表等的广泛应用(例如，在线推荐和自然语言问答)。作为一个典型的图搜索，我们研究了一个大型 NSGD 上的子图搜索，称为神经符号子图匹配(NSMatch) ，它包含一个新的排序搜索函数。具体来说，我们开发了一个通用的算法框架来有效地处理 NSMatch。利用现实生活中的多模态图，实验验证了 NSMatch 的有效性、可扩展性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Subgraph+Search+over+Neural-Symbolic+Graphs)|0|
|[Explainable Conversational Question Answering over Heterogeneous Sources via Iterative Graph Neural Networks](https://doi.org/10.1145/3539618.3591682)|Philipp Christmann, Rishiraj Saha Roy, Gerhard Weikum||In conversational question answering, users express their information needs through a series of utterances with incomplete context. Typical ConvQA methods rely on a single source (a knowledge base (KB), or a text corpus, or a set of tables), thus being unable to benefit from increased answer coverage and redundancy of multiple sources. Our method EXPLAIGNN overcomes these limitations by integrating information from a mixture of sources with user-comprehensible explanations for answers. It constructs a heterogeneous graph from entities and evidence snippets retrieved from a KB, a text corpus, web tables, and infoboxes. This large graph is then iteratively reduced via graph neural networks that incorporate question-level attention, until the best answers and their explanations are distilled. Experiments show that EXPLAIGNN improves performance over state-of-the-art baselines. A user study demonstrates that derived answers are understandable by end users.|在会话问答中，用户通过一系列语境不完整的话语来表达自己的信息需求。典型的 ConvQA 方法依赖于单个源(知识库(KB)、文本语料库或一组表) ，因此无法从增加的答案覆盖率和多个源的冗余中获益。我们的方法 EXPLAIGNN 克服了这些限制，整合了来自各种来源的信息和用户可理解的答案解释。它通过从知识库、文本语料库、 Web 表格和信息框中检索到的实体和证据片段构造一个异构图。然后通过包含问题级注意力的图形神经网络迭代地缩减这个大图，直到最佳答案及其解释被提炼出来。实验表明，EXPLAIGNN 提高了性能超过最先进的基线。用户研究表明，最终用户可以理解派生的答案。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explainable+Conversational+Question+Answering+over+Heterogeneous+Sources+via+Iterative+Graph+Neural+Networks)|0|
|[Data-Aware Proxy Hashing for Cross-modal Retrieval](https://doi.org/10.1145/3539618.3591660)|RongCheng Tu, XianLing Mao, Wenjin Ji, Wei Wei, Heyan Huang|Beijing Institute of Technology, Beijing, China; Beijing Institute of Technology, Wuhan, China|Recently, numerous proxy hash code based methods, which sufficiently exploit the label information of data to supervise the training of hashing models, have been proposed. Although these methods have made impressive progress, their generating processes of proxy hash codes are based only on the class information of the dataset or labels of data but do not take the data themselves into account. Therefore, these methods will probably generate some inappropriate proxy hash codes, thus damaging the retrieval performance of the hash models. To solve the aforementioned problem, we propose a novel Data-Aware Proxy Hashing for cross-modal retrieval, called DAPH. Specifically, our proposed method first train a data-aware proxy network that takes the data points, label vectors of data, and the class vectors of the dataset as inputs to generate class-based data-aware proxy hash codes, label-fused image-aware proxy hash codes and label-fused text-aware proxy hash codes. Then, we propose a novel hash loss that exploits the three types of data-aware proxy hash codes to supervise the training of modality-specific hashing networks. After training, DAPH is able to generate discriminate hash codes with the semantic information preserved adequately. Extensive experiments on three benchmark datasets show that the proposed DAPH outperforms the state-of-the-art baselines in cross-modal retrieval tasks.|近年来，人们提出了许多基于代理哈希码的方法，充分利用数据的标签信息来监督哈希模型的训练。虽然这些方法已经取得了令人印象深刻的进展，但它们的代理哈希码的生成过程只是基于数据集的类信息或数据标签，而没有考虑数据本身。因此，这些方法可能会产生一些不适当的代理哈希码，从而损害哈希模型的检索性能。为了解决上述问题，我们提出了一种新的跨模态检索的数据感知代理哈希算法，称为 DAPH。具体来说，我们提出的方法首先训练一个数据感知代理网络，该网络以数据点、数据标签向量和数据集的类向量作为输入，生成基于类的数据感知代理哈希码、标签融合图像感知代理哈希码和标签融合文本感知代理哈希码。然后，提出了一种新的哈希丢失算法，该算法利用三种数据感知代理哈希码来监控特定模态哈希网络的训练。经过训练后，DAPH 能够在充分保留语义信息的情况下生成有区别的哈希码。在三个基准数据集上的大量实验表明，所提出的 DAPH 算法在跨模态检索任务中的性能优于最先进的基准算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Data-Aware+Proxy+Hashing+for+Cross-modal+Retrieval)|0|
|[Hear Me Out: A Study on the Use of the Voice Modality for Crowdsourced Relevance Assessments](https://doi.org/10.1145/3539618.3591694)|Nirmal Roy, Agathe Balayn, David Maxwell, Claudia Hauff||The creation of relevance assessments by human assessors (often nowadays crowdworkers) is a vital step when building IR test collections. Prior works have investigated assessor quality & behaviour, though into the impact of a document's presentation modality on assessor efficiency and effectiveness. Given the rise of voice-based interfaces, we investigate whether it is feasible for assessors to judge the relevance of text documents via a voice-based interface. We ran a user study (n = 49) on a crowdsourcing platform where participants judged the relevance of short and long documents sampled from the TREC Deep Learning corpus-presented to them either in the text or voice modality. We found that: (i) participants are equally accurate in their judgements across both the text and voice modality; (ii) with increased document length it takes participants significantly longer (for documents of length > 120 words it takes almost twice as much time) to make relevance judgements in the voice condition; and (iii) the ability of assessors to ignore stimuli that are not relevant (i.e., inhibition) impacts the assessment quality in the voice modality-assessors with higher inhibition are significantly more accurate than those with lower inhibition. Our results indicate that we can reliably leverage the voice modality as a means to effectively collect relevance labels from crowdworkers.|由人工评估员(通常是现在的众包工作者)创建相关性评估是构建 IR 测试集的关键步骤。以前的工作已经调查了评估员的质量和行为，尽管文件的呈现方式对评估员的效率和有效性的影响。鉴于基于语音的接口的兴起，我们研究了评估者通过基于语音的接口来判断文本文档的相关性是否可行。我们在一个众包平台上运行了一个用户研究(n = 49) ，参与者判断从 TREC 深度学习语料库中取样的短文档和长文档的相关性——以文本或语音形式呈现给他们。我们发现: (i)参与者在文本和语音模式中的判断同样准确; (ii)随着文件长度的增加，参与者在语音条件下做出相关判断的时间显著延长(对于长度 > 120个单词的文件，需要几乎两倍的时间) ; 以及(iii)评估者忽略不相关刺激(即抑制)的能力影响语音模式中的评估质量-抑制程度较高的评估者比抑制程度较低的评估者明显更准确。我们的研究结果表明，我们可以可靠地利用语音模式作为一种手段，有效地收集相关标签从众工作者。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hear+Me+Out:+A+Study+on+the+Use+of+the+Voice+Modality+for+Crowdsourced+Relevance+Assessments)|0|
|[Asymmetric Hashing for Fast Ranking via Neural Network Measures](https://doi.org/10.1145/3539618.3591640)|Khoa D. Doan, Shulong Tan, Weijie Zhao, Ping Li|Coupang, Inc., Mountain View, CA, USA; Linkedin Ads, Seattle, WA, USA; Rochester Institute of Technology, Rochester, NY, USA; VinUniversity, Hanoi, Vietnam|Fast item ranking is an important task in recommender systems. In previous works, graph-based Approximate Nearest Neighbor (ANN) approaches have demonstrated good performance on item ranking tasks with generic searching/matching measures (including complex measures such as neural network measures). However, since these ANN approaches must go through the neural measures several times during ranking, the computation is not practical if the neural measure is a large network. On the other hand, fast item ranking using existing hashing-based approaches, such as Locality Sensitive Hashing (LSH), only works with a limited set of measures. Previous learning-to-hash approaches are also not suitable to solve the fast item ranking problem since they can take a significant amount of time and computation to train the hash functions. Hashing approaches, however, are attractive because they provide a principle and efficient way to retrieve candidate items. In this paper, we propose a simple and effective learning-to-hash approach for the fast item ranking problem that can be used for any type of measure, including neural network measures. Specifically, we solve this problem with an asymmetric hashing framework based on discrete inner product fitting. We learn a pair of related hash functions that map heterogeneous objects (e.g., users and items) into a common discrete space where the inner product of their binary codes reveals their true similarity defined via the original searching measure. The fast ranking problem is reduced to an ANN search via this asymmetric hashing scheme. Then, we propose a sampling strategy to efficiently select relevant and contrastive samples to train the hashing model. We empirically validate the proposed method against the existing state-of-the-art fast item ranking methods in several combinations of non-linear searching functions and prominent datasets.|项目快速排序是推荐系统中的一项重要任务。在以往的工作中，基于图的近似最近邻(ANN)方法已经证明了良好的性能项目排序任务与一般的搜索/匹配措施(包括复杂的措施，如神经网络措施)。然而，由于这些神经网络方法在排序过程中必须经过多次神经测度，如果神经测度是一个大型网络，计算是不切实际的。另一方面，使用现有的基于哈希的方法(如区域敏感哈希(LSH))进行快速项目排序，只能在有限的度量集合下进行。以往的哈希学习方法也不适合解决快速项目排序问题，因为它们需要大量的时间和计算来训练哈希函数。然而，散列方法很有吸引力，因为它们提供了检索候选项的原则和有效方法。在本文中，我们提出了一个简单而有效的学习-哈希方法，用于快速项目排序问题，可以用于任何类型的测度，包括神经网络测度。具体地说，我们采用基于离散内积拟合的非对称散列框架来解决这个问题。我们学习了一对相关的散列函数，它们将异构对象(例如，用户和项)映射到一个公共的离散空间中，在这个空间中，它们的二进制码的内积揭示了它们通过原始搜索度量定义的真实相似性。通过这种非对称散列方法，将快速排序问题简化为人工神经网络搜索问题。然后，我们提出了一种抽样策略，有效地选择相关和对比的样本来训练散列模型。针对现有的快速项目排序方法，在多种非线性搜索函数和突出数据集的组合下，对该方法进行了实验验证。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Asymmetric+Hashing+for+Fast+Ranking+via+Neural+Network+Measures)|0|
|[Wisdom of Crowds and Fine-Grained Learning for Serendipity Recommendations](https://doi.org/10.1145/3539618.3591787)|Zhe Fu, Xi Niu, Li Yu|Renmin University of China, Beijing, China; University of North Carolina at Charlotte, Charlotte, USA|Serendipity is a notion that means an unexpected but valuable discovery. Due to its elusive and subjective nature, serendipity is difficult to study even with today's advances in machine learning and deep learning techniques. Both ground truth data collecting and model developing are the open research questions. This paper addresses both the data and the model challenges for identifying serendipity in recommender systems. For the ground truth data collecting, it proposes a new and scalable approach by using both user generated reviews and a crowd sourcing method. The result is a large-scale ground truth data on serendipity. For model developing, it designed a self-enhanced module to learn the fine-grained facets of serendipity in order to mitigate the inherent data sparsity problem in any serendipity ground truth dataset. The self-enhanced module is general enough to be applied with many base deep learning models for serendipity. A series of experiments have been conducted. As the result, a base deep learning model trained on our collected ground truth data, as well as with the help of the self-enhanced module, outperforms the state-of-the-art baseline models in predicting serendipity.|意外的发现意味着一个意想不到但有价值的发现。由于其难以捉摸和主观的性质，即使在今天的机器学习和深度学习技术的进步，意外发现是难以研究。地面真实数据采集和模型开发都是开放性的研究课题。本文讨论了在推荐系统中识别意外发现的数据和模型挑战。对于地面真相数据的收集，它提出了一种新的和可扩展的方法，通过使用用户生成的评论和众包的方法。其结果是一个关于意外发现的大规模地面真相数据。在模型开发方面，设计了一个自增强模块来学习机遇的细粒度方面，以减轻任何机遇地面真实数据集中固有的数据稀疏问题。该自增强模块具有较高的通用性，可以应用于许多基础深度学习模型中。进行了一系列的实验。因此，基于我们收集的地面真相数据的基础深度学习模型，以及在自我增强模块的帮助下，在预测偶然性方面优于最先进的基线模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Wisdom+of+Crowds+and+Fine-Grained+Learning+for+Serendipity+Recommendations)|0|
|[An Effective Framework for Enhancing Query Answering in a Heterogeneous Data Lake](https://doi.org/10.1145/3539618.3591637)|Qin Yuan, Ye Yuan, Zhenyu Wen, He Wang, Shiyuan Tang|Beijing Institute of Technology, Hangzhou, China; Beijing Institute of Technology, Beijing, China|There has been a growing interest in cross-source searching to gain rich knowledge in recent years. A data lake collects massive raw and heterogeneous data with different data schemas and query interfaces. Many real-life applications require query answering over the heterogeneous data lake, such as e-commerce, bioinformatics and healthcare. In this paper, we propose LakeAns that semantically integrates heterogeneous data schemas of the lake to enhance the semantics of query answers. To this end, we propose a novel framework to efficiently and effectively perform the cross-source searching. The framework exploits a reinforcement learning method to semantically integrate the data schemas and further create a global relational schema for the heterogeneous data. It then performs a query answering algorithm based on the global schema to find answers across multiple data sources. We conduct extensive experimental evaluations using real-life data to verify that our approach outperforms existing solutions in terms of effectiveness and efficiency.|近年来，人们对跨源搜索以获取丰富的知识越来越感兴趣。数据湖通过不同的数据模式和查询接口收集大量的原始和异构数据。许多实际应用程序需要在异构数据湖上进行查询回答，例如电子商务、生物信息学和医疗保健。在本文中，我们提出了一种基于 LakeAns 的方法，该方法在语义上集成了湖中的异构数据模式，从而增强了查询答案的语义。为此，我们提出了一个新的框架，以高效和有效地执行跨源搜索。该框架采用了一种强化学习方法，从语义上集成数据模式，并进一步为异构数据创建一个全局关系模式。然后，它根据全局模式执行查询应答算法，以跨多个数据源查找答案。我们使用实际数据进行广泛的实验评估，以验证我们的方法在有效性和效率方面优于现有的解决方案。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Effective+Framework+for+Enhancing+Query+Answering+in+a+Heterogeneous+Data+Lake)|0|
|[BeamQA: Multi-hop Knowledge Graph Question Answering with Sequence-to-Sequence Prediction and Beam Search](https://doi.org/10.1145/3539618.3591698)|Farah Atif, Ola El Khatib, Djellel Eddine Difallah|NYU Abu Dhabi, Abu Dhabi, UAE|Knowledge Graph Question Answering (KGQA) is a task that aims to answer natural language queries by extracting facts from a knowledge graph. Current state-of-the-art techniques for KGQA rely on text-based information from graph entity and relations labels, as well as external textual corpora. By reasoning over multiple edges in the graph, these can accurately rank and return the most relevant entities. However, one of the limitations of these methods is that they cannot handle the inherent incompleteness of real-world knowledge graphs and may lead to inaccurate answers due to missing edges. To address this issue, recent advances in graph representation learning have led to the development of systems that can use link prediction techniques to handle missing edges probabilistically, allowing the system to reason with incomplete information. However, existing KGQA frameworks that use such techniques often depend on learning a transformation from the query representation to the graph embedding space, which requires access to a large training dataset. We present BeamQA, an approach that overcomes these limitations by combining a sequence-to-sequence prediction model with beam search execution in the embedding space. Our model uses a pre-trained large language model and synthetic question generation. Our experiments demonstrate the effectiveness of BeamQA when compared to other KGQA methods on two knowledge graph question-answering datasets.|知识图问题回答(KGQA)是一个通过从知识图中提取事实来回答自然语言查询的任务。KGQA 目前最先进的技术依赖于来自图形实体和关系标签以及外部文本语料库的基于文本的信息。通过对图中的多条边进行推理，这些边可以准确地对最相关的实体进行排序并返回。然而，这些方法的局限性之一是它们不能处理真实世界知识图固有的不完备性，并可能由于缺少边而导致不准确的答案。为了解决这个问题，图表示学习的最新进展导致了系统的发展，可以使用链接预测技术来处理缺失的边概率，使系统推理不完整的信息。然而，使用这些技术的现有 KGQA 框架通常依赖于学习从查询表示到图嵌入空间的转换，这需要访问大型训练数据集。我们提出了一种 BeamQA 方法，该方法将序列到序列预测模型与嵌入空间中的波束搜索执行相结合，克服了这些局限性。我们的模型使用预先训练的大型语言模型和综合问题生成。实验结果表明，BeamQA 方法与其他 KGQA 方法相比，在两个知识图问答数据集上具有较好的效果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BeamQA:+Multi-hop+Knowledge+Graph+Question+Answering+with+Sequence-to-Sequence+Prediction+and+Beam+Search)|0|
|[Time-interval Aware Share Recommendation via Bi-directional Continuous Time Dynamic Graphs](https://doi.org/10.1145/3539618.3591775)|Ziwei Zhao, Xi Zhu, Tong Xu, Aakas Lizhiyu, Yu Yu, Xueying Li, Zikai Yin, Enhong Chen|University of Science and Technology of China, Hefei, China; Alibaba Group, Hangzhou, China|Dynamic share recommendation, which aims at recommending a friend who would like to share a particular item at a certain timestamp, has emerged as a novel task for social-oriented e-commerce platforms. Different from traditional graph-based recommendation tasks, with integrating the interconnected social interactions and fine-grained temporal information from historical share records, this novel task may encounter one unique challenge, i.e., how to deal with the dynamic social connections and asymmetric share interactions. Even worse, users may keep inactive during some periods, which results in difficulties in updating personalized profiles. To address the above challenges, in this paper, we propose a dynamic graph share recommendation model called DynShare. Specifically, we first divide each user embedding into two parts, namely the invitation embedding and vote embedding to show the tendencies of sending and receiving items, respectively. Then, temporal graph attention networks (TGATs) based on bi-directional continuous time dynamic graphs (CTDGs) are leveraged to encode temporal neighbor information from different directions. Afterward, to estimate how different users perceive the time intervals after the last interaction, we further design a time-interval aware personalized projection operator on the foundation of temporal point processes (TPPs) to project user embedding for the next-time share prediction. Extensive experiments on a real-world e-commerce share dataset have demonstrated that our proposed DynShare can achieve better results compared with state-of-the-art baseline methods. And our code is available on the project website: https://github.com/meteor-gif/DynShare.|动态分享推荐，目的是推荐一个朋友谁愿意分享一个特定的项目在一定的时间戳，已经成为一个新的任务的社会导向的电子商务平台。与传统的基于图的推荐任务不同，该任务融合了互联的社会交互和历史共享记录中的细粒度时间信息，可能会遇到一个独特的挑战，即如何处理动态的社会关系和非对称的共享交互。更糟糕的是，用户可能会在某些时期保持不活跃，这导致难以更新个性化配置文件。为了解决上述问题，本文提出了一种动态图形共享推荐模型 DynShare。具体来说，我们首先将每个用户嵌入分为邀请嵌入和投票嵌入两部分，分别表示发送和接收条目的趋势。然后，利用基于双向连续时间动态图(CTDGs)的时间图注意网络(TGAT)对不同方向的时间邻居信息进行编码。然后，为了估计不同用户在最后一次交互后对时间间隔的感知程度，我们进一步设计了一个基于时间点过程(TPP)的时间间隔感知个性化投影算子，以投影用户嵌入来预测下一次共享。在一个真实的电子商务共享数据集上的大量实验表明，我们提出的 DynShare 可以取得比最先进的基线方法更好的结果。我们的代码可以在项目网站上找到:  https://github.com/meteor-gif/dynshare。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Time-interval+Aware+Share+Recommendation+via+Bi-directional+Continuous+Time+Dynamic+Graphs)|0|
|[Diffusion Recommender Model](https://doi.org/10.1145/3539618.3591663)|Wenjie Wang, Yiyan Xu, Fuli Feng, Xinyu Lin, Xiangnan He, TatSeng Chua||Generative models such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) are widely utilized to model the generative process of user interactions. However, these generative models suffer from intrinsic limitations such as the instability of GANs and the restricted representation ability of VAEs. Such limitations hinder the accurate modeling of the complex user interaction generation procedure, such as noisy interactions caused by various interference factors. In light of the impressive advantages of Diffusion Models (DMs) over traditional generative models in image synthesis, we propose a novel Diffusion Recommender Model (named DiffRec) to learn the generative process in a denoising manner. To retain personalized information in user interactions, DiffRec reduces the added noises and avoids corrupting users' interactions into pure noises like in image synthesis. In addition, we extend traditional DMs to tackle the unique challenges in practical recommender systems: high resource costs for large-scale item prediction and temporal shifts of user preference. To this end, we propose two extensions of DiffRec: L-DiffRec clusters items for dimension compression and conducts the diffusion processes in the latent space; and T-DiffRec reweights user interactions based on the interaction timestamps to encode temporal information. We conduct extensive experiments on three datasets under multiple settings (e.g. clean training, noisy training, and temporal training). The empirical results and in-depth analysis validate the superiority of DiffRec with two extensions over competitive baselines.|生成模型如生成对抗网络(GANs)和变分自动编码器(VAE)被广泛地用于模拟用户交互的生成过程。然而，这些生成模型受到 GAN 的不稳定性和 VAE 表示能力的限制等内在的局限性。这些局限性阻碍了复杂用户交互生成过程的精确建模，例如由各种干扰因素引起的噪声交互。鉴于扩散模型在图像合成中相对于传统生成模型的显著优势，我们提出了一种新的扩散推荐模型。为了在用户交互中保留个性化信息，区分反射降低了附加的噪声，避免了在图像合成中将用户交互变成纯粹的噪声。此外，我们还扩展了传统的数据模型，以解决实际推荐系统中面临的独特挑战: 大规模项目预测的高资源成本和用户偏好的时间变化。为此，我们提出了区分反射的两个扩展: L-DiffRec 聚类项用于维数压缩并在潜空间中进行扩散过程; T-DiffRec 基于交互时间戳重新加权用户交互以编码时间信息。我们在三个数据集上进行了多重设置下的广泛实验(例如干净训练、噪声训练和时间训练)。实证结果和深入的分析验证了区分共享的优越性与两个扩展的竞争基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Diffusion+Recommender+Model)|0|
|[Relation-Aware Multi-Positive Contrastive Knowledge Graph Completion with Embedding Dimension Scaling](https://doi.org/10.1145/3539618.3591756)|Bin Shang, Yinliang Zhao, Di Wang, Jun Liu|Xi'dian University, Xi'an, China; Xi'an Jiaotong University, Xi'an, China|Recently, a large amount of work has emerged for knowledge graph completion (KGC), which aims to reason over known facts and to infer the missing links. Meanwhile, contrastive learning has been applied to the KGC tasks, which can improve the representation quality of entities and relations. However, existing KGC approaches tend to improve their performance with high-dimensional embeddings and complex models, which make them suffer from large storage space and high training costs. Furthermore, contrastive loss with single positive sample learns little structural and semantic information in knowledge graphs due to the complex relation types. To address these challenges, we propose a novel knowledge graph completion model named ConKGC with the embedding dimension scaling and a relation-aware multi-positive contrastive loss. In order to achieve both space consumption reduction and model performance improvement, a new scoring function is proposed to map the raw low-dimensional embeddings of entities and relations to high-dimensional embedding space, and predict low-dimensional tail entities with latent semantic information of high-dimensional embeddings. In addition, ConKGC designs a multiple weak positive samples based contrastive loss under different relation types to maintain two important training targets, Alignment and Uniformity. This loss function and few parameters of the model ensure that ConKGC performs best and has fast convergence speed. Extensive experiments on three standard datasets confirm the effectiveness of our innovations, and the performance of ConKGC is significantly improved compared to the state-of-the-art methods.|近年来，知识图完成(KGC)领域出现了大量的工作，其目的是对已知事实进行推理，并推断出缺失的环节。同时，将对比学习应用到 KGC 任务中，提高了实体和关系的表示质量。然而，现有的 KGC 方法往往通过高维嵌入和复杂模型来提高性能，这使得它们存储空间大，培训成本高。此外，由于复杂的关系类型，单个正样本的对比损失对知识图的结构和语义信息影响很小。为了解决这些问题，我们提出了一种新的知识图完成模型 ConKGC，该模型具有嵌入维度缩放和关系感知的多正对比度损失。为了同时降低空间消耗和提高模型性能，提出了一种新的评分函数，将实体的原始低维嵌入和关系映射到高维嵌入空间，并预测具有高维嵌入潜在语义信息的低维尾实体。此外，ConKGC 在不同的关系类型下设计了多个基于对比度损失的弱阳性样本，以维持校准和均匀性这两个重要训练目标。该损失函数和模型参数较少，保证了 ConKGC 算法的最佳性能和较快的收敛速度。在三个标准数据集上的大量实验证实了我们创新的有效性，并且与最先进的方法相比，ConKGC 的性能得到了显著的提高。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Relation-Aware+Multi-Positive+Contrastive+Knowledge+Graph+Completion+with+Embedding+Dimension+Scaling)|0|
|[Weighted Knowledge Graph Embedding](https://doi.org/10.1145/3539618.3591784)|Zhao Zhang, Zhanpeng Guan, Fuwei Zhang, Fuzhen Zhuang, Zhulin An, Fei Wang, Yongjun Xu|Institute of Artificial Intelligence, Beihang University & Zhongguancun Laboratory, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences & University of Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China|Knowledge graph embedding (KGE) aims to project both entities and relations in a knowledge graph (KG) into low-dimensional vectors. Indeed, existing KGs suffer from the data imbalance issue, i.e., entities and relations conform to a long-tail distribution, only a small portion of entities and relations occur frequently, while the vast majority of entities and relations only have a few training samples. Existing KGE methods assign equal weights to each entity and relation during the training process. Under this setting, long-tail entities and relations are not fully trained during training, leading to unreliable representations. In this paper, we propose WeightE, which attends differentially to different entities and relations. Specifically, WeightE is able to endow lower weights to frequent entities and relations, and higher weights to infrequent ones. In such manner, WeightE is capable of increasing the weights of long-tail entities and relations, and learning better representations for them. In particular, WeightE tailors bilevel optimization for the KGE task, where the inner level aims to learn reliable entity and relation embeddings, and the outer level attempts to assign appropriate weights for each entity and relation. Moreover, it is worth noting that our technique of applying weights to different entities and relations is general and flexible, which can be applied to a number of existing KGE models. Finally, we extensively validate the superiority of WeightE against various state-of-the-art baselines.|知识图嵌入(KGE)是将知识图中的实体和关系投影到低维向量中。实际上，现有的幼儿园存在数据不平衡问题，即实体和关系符合长尾分布，只有一小部分实体和关系频繁出现，而绝大多数实体和关系只有少数训练样本。现有的 KGE 方法在训练过程中对每个实体和关系赋予相同的权重。在这种情况下，长尾实体和关系在训练期间没有得到充分的训练，导致不可靠的表示。在本文中，我们提出了权重 E，它区别对待不同的实体和关系。具体来说，WeightE 能够赋予频繁实体和关系较低的权重，赋予不频繁实体和关系较高的权重。通过这种方式，WeightE 能够增加长尾实体和关系的权重，并为它们学习更好的表示。特别是，WeightE 为 KGE 任务定制了两层优化，其中内层的目标是学习可靠的实体和关系嵌入，外层的目标是为每个实体和关系分配适当的权重。此外，值得注意的是，我们的技术应用权重不同的实体和关系是通用的和灵活的，这可以适用于现有的 KGE 模型。最后，我们广泛地验证了 WeightE 对于各种最先进的基线的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Weighted+Knowledge+Graph+Embedding)|0|
|[Contrastive State Augmentations for Reinforcement Learning-Based Recommender Systems](https://doi.org/10.1145/3539618.3591656)|Zhaochun Ren, Na Huang, Yidan Wang, Pengjie Ren, Jun Ma, Jiahuan Lei, Xinlei Shi, Hengliang Luo, Joemon M. Jose, Xin Xin||Learning reinforcement learning (RL)-based recommenders from historical user-item interaction sequences is vital to generate high-reward recommendations and improve long-term cumulative benefits. However, existing RL recommendation methods encounter difficulties (i) to estimate the value functions for states which are not contained in the offline training data, and (ii) to learn effective state representations from user implicit feedback due to the lack of contrastive signals. In this work, we propose contrastive state augmentations (CSA) for the training of RL-based recommender systems. To tackle the first issue, we propose four state augmentation strategies to enlarge the state space of the offline data. The proposed method improves the generalization capability of the recommender by making the RL agent visit the local state regions and ensuring the learned value functions are similar between the original and augmented states. For the second issue, we propose introducing contrastive signals between augmented states and the state randomly sampled from other sessions to improve the state representation learning further. To verify the effectiveness of the proposed CSA, we conduct extensive experiments on two publicly accessible datasets and one dataset collected from a real-life e-commerce platform. We also conduct experiments on a simulated environment as the online evaluation setting. Experimental results demonstrate that CSA can effectively improve recommendation performance.|从历史用户项目交互序列中学习基于强化学习的推荐对于产生高回报的推荐和提高长期累积效益至关重要。然而，现有的 RL 推荐方法遇到了困难(i)估计不包含在离线训练数据中的状态的值函数，以及(ii)由于缺乏对比信号而从用户隐式反馈中学习有效的状态表示。在这项工作中，我们提出了对比状态增强(CSA)的训练基于 RL 的推荐系统。针对第一个问题，我们提出了四种状态增强策略来扩大离线数据的状态空间。该方法通过使 RL 代理访问局部状态区域，保证学习值函数在原状态和增广状态之间相似，提高了推荐器的泛化能力。对于第二个问题，我们提出在增广状态和从其他会话中随机采样的状态之间引入对比信号，以进一步改进状态表示学习。为了验证所提出的 CSA 的有效性，我们对从现实生活中的电子商务平台收集的两个公开可访问的数据集和一个数据集进行了广泛的实验。我们还进行了模拟环境的实验，作为在线评价设置。实验结果表明，CSA 能有效提高推荐性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contrastive+State+Augmentations+for+Reinforcement+Learning-Based+Recommender+Systems)|0|
|[Multi-order Matched Neighborhood Consistent Graph Alignment in a Union Vector Space](https://doi.org/10.1145/3539618.3591735)|Wei Tang, Haifeng Sun, Jingyu Wang, Qi Qi, Jing Wang, Hao Yang, Shimin Tao|Beijing University of Posts and Telecommunications, Beijing, China; Huawei, Beijing, China|In this paper, we study the unsupervised plain graph alignment problem, which aims to find node correspondences across two graphs without any side information. The majority of previous works addressed UPGA based on structural information, which will inevitably lead to subgraph isomorphism issues. That is, unaligned nodes could take similar local structural information. To mitigate this issue, we present the Multi-order Matched Neighborhood Consistent (MMNC) which tries to match nodes by aligning the learned node embeddings with only a small number of pseudo alignment seeds. In particular, we extend matched neighborhood consistency (MNC) to vector space and further develop embedding-based MNC (EMNC). By minimizing the EMNC-based loss function, we can utilize the limited pseudo alignment seeds to approximate the orthogonal transformation matrix between two groups of node embeddings with high efficiency and accuracy. Through extensive experiments on public benchmarks, we show that the proposed methods achieve a good balance between alignment accuracy and speed over multiple datasets compared with existing methods.|本文研究了无监督平面图对齐问题，目的是在没有任何边信息的情况下寻找两个图之间的节点对应。以往的大多数研究都是基于结构信息的 UPGA，这必然会导致子图同构问题。也就是说，未对齐的节点可以采用类似的本地结构信息。为了解决这个问题，我们提出了多阶匹配邻域一致性(MMNC)算法，该算法通过只使用少量伪对齐种子对齐学习节点嵌入来匹配节点。特别地，我们将匹配邻域一致性(MNC)扩展到向量空间，并进一步发展了基于嵌入的 MNC (EMNC)。通过最小化基于 EMNC 的损耗函数，我们可以利用有限的伪对齐种子来逼近两组节点嵌入之间的正交变换矩阵，从而提高效率和准确性。通过对公共基准的大量实验表明，与现有方法相比，本文提出的方法在多个数据集的对准精度和速度之间取得了良好的平衡。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-order+Matched+Neighborhood+Consistent+Graph+Alignment+in+a+Union+Vector+Space)|0|
|[Personalized Federated Relation Classification over Heterogeneous Texts](https://doi.org/10.1145/3539618.3591748)|Ning Pang, Xiang Zhao, Weixin Zeng, Ji Wang, Weidong Xiao|National University of Defense Technology, Changsha, China|Relation classification detects the semantic relation between two annotated entities from a piece of text, which is a useful tool for structurization of knowledge. Recently, federated learning has been introduced to train relation classification models in decentralized settings. Current methods strive for a strong server model by decoupling the model training at server from direct access to texts at clients while taking advantage of them. Nevertheless, they overlook the fact that clients have heterogeneous texts (i.e., texts with diversely skewed distribution of relations), which renders existing methods less practical. In this paper, we propose to investigate personalized federated relation classification, in which strong client models adapted to their own data are desired. To further meet the challenges brought by heterogeneous texts, we present a novel framework, namely pf-RC, with several optimized designs. It features a knowledge aggregation method that exploits a relation-wise weighting mechanism, and a feature augmentation method that leverages prototypes to adaptively enhance the representations of instances of long-tail relations. We experimentally validate the superiority of pf-RC against competing baselines in various settings, and the results suggest that the tailored techniques mitigate the challenges.|关系分类从一段文本中检测两个注释实体之间的语义关系，是知识结构化的有效工具。最近，联邦学习被引入到分散环境中训练关系分类模型。目前的方法力求建立一个强大的服务器模型，它将服务器上的模型训练与客户端直接访问文本分离开来，同时利用这些优势。尽管如此，他们忽视了这样一个事实: 客户的文本是异质的(即，文本具有不同的偏态分布的关系) ，这使得现有的方法不太实用。在本文中，我们提出研究个性化的联邦关系分类，其中需要强大的客户端模型适应自己的数据。为了进一步迎接异构文本带来的挑战，我们提出了一个新的框架，即 pf-RC，并进行了一些优化设计。提出了一种利用关系加权机制的知识聚合方法和一种利用原型自适应增强长尾关系实例表示的特征增强方法。我们通过实验验证了 pf-RC 在不同环境下对抗竞争基线的优越性，结果表明量身定制的技术可以缓解这一挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalized+Federated+Relation+Classification+over+Heterogeneous+Texts)|0|
|[SAILER: Structure-aware Pre-trained Language Model for Legal Case Retrieval](https://doi.org/10.1145/3539618.3591761)|Haitao Li, Qingyao Ai, Jia Chen, Qian Dong, Yueyue Wu, Yiqun Liu, Chong Chen, Qi Tian||Legal case retrieval, which aims to find relevant cases for a query case, plays a core role in the intelligent legal system. Despite the success that pre-training has achieved in ad-hoc retrieval tasks, effective pre-training strategies for legal case retrieval remain to be explored. Compared with general documents, legal case documents are typically long text sequences with intrinsic logical structures. However, most existing language models have difficulty understanding the long-distance dependencies between different structures. Moreover, in contrast to the general retrieval, the relevance in the legal domain is sensitive to key legal elements. Even subtle differences in key legal elements can significantly affect the judgement of relevance. However, existing pre-trained language models designed for general purposes have not been equipped to handle legal elements. To address these issues, in this paper, we propose SAILER, a new Structure-Aware pre-traIned language model for LEgal case Retrieval. It is highlighted in the following three aspects: (1) SAILER fully utilizes the structural information contained in legal case documents and pays more attention to key legal elements, similar to how legal experts browse legal case documents. (2) SAILER employs an asymmetric encoder-decoder architecture to integrate several different pre-training objectives. In this way, rich semantic information across tasks is encoded into dense vectors. (3) SAILER has powerful discriminative ability, even without any legal annotation data. It can distinguish legal cases with different charges accurately. Extensive experiments over publicly available legal benchmarks demonstrate that our approach can significantly outperform previous state-of-the-art methods in legal case retrieval.|法律案例检索是智能法律系统的核心，其目的是为查询案例寻找相关案例。尽管在特别检索任务方面的预先培训取得了成功，但仍有待探索法律案件检索的有效预先培训战略。与一般文献相比，法律案例文献通常是具有内在逻辑结构的长文本序列。然而，大多数现有的语言模型难以理解不同结构之间的长距离依赖关系。此外，与一般检索相比，法律领域的相关性对关键的法律要素是敏感的。即使在关键法律要素方面存在细微差别，也会对相关性的判断产生重大影响。然而，为一般目的而设计的现有预先训练的语言模式尚未具备处理法律要素的能力。为了解决这些问题，本文提出了一种新的法律案例检索的结构感知预训练语言模型 SAILER。这主要体现在以下三个方面: (1) SAILER 充分利用了法律案例文档中的结构信息，更加注重关键的法律要素，类似于法律专家浏览法律案例文档的方式。(2) SAILER 采用非对称编解码体系结构集成多个不同的训练前目标。通过这种方式，跨任务的丰富语义信息被编码成密集的向量。(3)即使没有任何法律注释数据，赛勒仍具有强大的辨别能力。它可以准确地区分不同指控的法律案件。对公开可用的法律基准进行的大量实验表明，我们的方法在法律案件检索方面可以显著优于以前的最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SAILER:+Structure-aware+Pre-trained+Language+Model+for+Legal+Case+Retrieval)|0|
|[Triple Structural Information Modelling for Accurate, Explainable and Interactive Recommendation](https://doi.org/10.1145/3539618.3591779)|Jiahao Liu, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, Li Shang, Ning Gu||In dynamic interaction graphs, user-item interactions usually follow heterogeneous patterns, represented by different structural information, such as user-item co-occurrence, sequential information of user interactions and the transition probabilities of item pairs. However, the existing methods cannot simultaneously leverage all three structural information, resulting in suboptimal performance. To this end, we propose TriSIM4Rec, a triple structural information modeling method for accurate, explainable and interactive recommendation on dynamic interaction graphs. Specifically, TriSIM4Rec consists of 1) a dynamic ideal low-pass graph filter to dynamically mine co-occurrence information in user-item interactions, which is implemented by incremental singular value decomposition (SVD); 2) a parameter-free attention module to capture sequential information of user interactions effectively and efficiently; and 3) an item transition matrix to store the transition probabilities of item pairs. Then, we fuse the predictions from the triple structural information sources to obtain the final recommendation results. By analyzing the relationship between the SVD-based and the recently emerging graph signal processing (GSP)-based collaborative filtering methods, we find that the essence of SVD is an ideal low-pass graph filter, so that the interest vector space in TriSIM4Rec can be extended to achieve explainable and interactive recommendation, making it possible for users to actively break through the information cocoons. Experiments on six public datasets demonstrated the effectiveness of TriSIM4Rec in accuracy, explainability and interactivity.|在动态交互图中，用户-项目交互通常遵循异构模式，表现为不同的结构信息，如用户-项目共现、用户交互的序列信息和项目对的转移概率。然而，现有的方法不能同时利用所有三个结构信息，导致次优性能。为此，我们提出了一种三重结构信息建模方法 TriSIM4Rec，用于对动态交互图进行精确、可解释和交互式的推荐。具体来说，TriSIM4Rec 包括: 1)一个动态的理想低通图形过滤器，用于动态挖掘用户-项目交互中的共现信息，该过滤器通过增量奇异值分解(SVD)实现; 2)一个无参数注意模块，用于有效和高效地捕获用户交互的连续信息; 3)一个项目，用于存储项目对的过渡概率转移矩阵。然后，我们融合来自三重结构信息源的预测，得到最终的推荐结果。通过分析基于奇异值分解和最近出现的基于图形信号处理(gSP)的协同过滤方法之间的关系，我们发现奇异值分解的本质是一个理想的低通图形滤波器，因此可以扩展 TriSIM4Rec 中的兴趣向量空间，以实现可解释和交互式的推荐，使用户能够积极地突破信息茧。在六个公共数据集上的实验证明了 TriSIM4Rec 在准确性、可解释性和交互性方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Triple+Structural+Information+Modelling+for+Accurate,+Explainable+and+Interactive+Recommendation)|0|
|[Blurring-Sharpening Process Models for Collaborative Filtering](https://doi.org/10.1145/3539618.3591645)|Jeongwhan Choi, Seoyoung Hong, Noseong Park, SungBae Cho||Collaborative filtering is one of the most fundamental topics for recommender systems. Various methods have been proposed for collaborative filtering, ranging from matrix factorization to graph convolutional methods. Being inspired by recent successes of graph filtering-based methods and score-based generative models (SGMs), we present a novel concept of blurring-sharpening process model (BSPM). SGMs and BSPMs share the same processing philosophy that new information can be discovered (e.g., new images are generated in the case of SGMs) while original information is first perturbed and then recovered to its original form. However, SGMs and our BSPMs deal with different types of information, and their optimal perturbation and recovery processes have fundamental discrepancies. Therefore, our BSPMs have different forms from SGMs. In addition, our concept not only theoretically subsumes many existing collaborative filtering models but also outperforms them in terms of Recall and NDCG in the three benchmark datasets, Gowalla, Yelp2018, and Amazon-book. In addition, the processing time of our method is comparable to other fast baselines. Our proposed concept has much potential in the future to be enhanced by designing better blurring (i.e., perturbation) and sharpening (i.e., recovery) processes than what we use in this paper.|协同过滤是推荐系统最基本的课题之一。人们提出了各种各样的协同过滤方法，从矩阵分解卷积法到图卷积法。受基于图过滤和基于分数的生成模型(SGMs)的启发，我们提出了模糊锐化过程模型(BSPM)的新概念。SGMs 和 BSPM 有着相同的处理哲学，即新信息可以被发现(例如，在 SGMs 中，新图像产生了) ，而原始信息首先受到干扰，然后恢复到其原始形式。然而，SGM 和我们的 BSPM 处理不同类型的信息，他们的最优扰动和恢复过程有根本的差异。因此，我们的 BSPM 有不同于 SGM 的形式。此外，我们的概念不仅在理论上包含了许多现有的协同过滤模型，而且在三个基准数据集(Gowalla、 Yelp2018和亚马逊-book)的 Recall 和 NDCG 方面也优于它们。此外，该方法的处理时间与其他快速基线相当。我们提出的概念在未来有很大的潜力，通过设计更好的模糊(即，扰动)和锐化(即，恢复)过程比我们在本文中使用的增强。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Blurring-Sharpening+Process+Models+for+Collaborative+Filtering)|0|
|[AdaMCL: Adaptive Fusion Multi-View Contrastive Learning for Collaborative Filtering](https://doi.org/10.1145/3539618.3591632)|Guanghui Zhu, Wang Lu, Chunfeng Yuan, Yihua Huang|Nanjing University, Nanjing, China|Graph collaborative filtering has achieved great success in capturing users' preferences over items. Despite effectiveness, graph neural network (GNN)-based methods suffer from data sparsity in real scenarios. Recently, contrastive learning (CL) has been used to address the problem of data sparsity. However, most CL-based methods only leverage the original user-item interaction graph to construct the CL task, lacking the explicit exploitation of the higher-order information (i.e., user-user and item-item relationships). Even for the CL-based method that uses the higher-order information, the reception field of the higher-order information is fixed and regardless of the difference between nodes. In this paper, we propose a novel adaptive multi-view fusion contrastive learning framework, named AdaMCL, for graph collaborative filtering. To exploit the higher-order information more accurately, we propose an adaptive fusion strategy to fuse the embeddings learned from the user-item and user-user graphs. Moreover, we propose a multi-view fusion contrastive learning paradigm to construct effective CL tasks. Besides, to alleviate the noisy information caused by aggregating higher-order neighbors, we propose a layer-level CL task. Extensive experimental results reveal that AdaMCL is effective and outperforms existing collaborative filtering models significantly.|图形协同过滤在捕捉用户对项目的偏好方面取得了巨大成功。尽管有效，基于图神经网络(GNN)的方法在实际场景中仍然存在数据稀疏问题。最近，对比学习(CL)已经被用来解决数据稀疏的问题。然而，大多数基于 CL 的方法只利用原始的用户-项目交互图来构造 CL 任务，缺乏对高阶信息(即用户-用户和项目-项目关系)的显式利用。即使对于使用高阶信息的基于 CL 的方法，高阶信息的接收字段也是固定的，而不管节点之间的差异。在本文中，我们提出了一个新的自适应多视图融合对比学习框架，命名为 AdaMCL，用于图形协同过滤。为了更准确地利用高阶信息，提出了一种自适应融合策略，融合从用户项图和用户-用户图中学到的嵌入信息。此外，我们提出了一个多视角融合对比学习范式来构建有效的 CL 任务。此外，为了减轻高阶邻居聚集所带来的噪声，我们提出了一个层级 CL 任务。大量的实验结果显示 AdaMCL 是有效的，并且比现有的协同过滤模型有显著的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AdaMCL:+Adaptive+Fusion+Multi-View+Contrastive+Learning+for+Collaborative+Filtering)|0|
|[Collaborative Residual Metric Learning](https://doi.org/10.1145/3539618.3591649)|Tianjun Wei, Jianghong Ma, Tommy W. S. Chow||In collaborative filtering, distance metric learning has been applied to matrix factorization techniques with promising results. However, matrix factorization lacks the ability of capturing collaborative information, which has been remarked by recent works and improved by interpreting user interactions as signals. This paper aims to find out how metric learning connect to these signal-based models. By adopting a generalized distance metric, we discovered that in signal-based models, it is easier to estimate the residual of distances, which refers to the difference between the distances from a user to a target item and another item, rather than estimating the distances themselves. Further analysis also uncovers a link between the normalization strength of interaction signals and the novelty of recommendation, which has been overlooked by existing studies. Based on the above findings, we propose a novel model to learn a generalized distance user-item distance metric to capture user preference in interaction signals by modeling the residuals of distance. The proposed CoRML model is then further improved in training efficiency by a newly introduced approximated ranking weight. Extensive experiments conducted on 4 public datasets demonstrate the superior performance of CoRML compared to the state-of-the-art baselines in collaborative filtering, along with high efficiency and the ability of providing novelty-promoted recommendations, shedding new light on the study of metric learning-based recommender systems.|在协同过滤中，距离度量学习已应用于矩阵分解技术，并取得了良好的效果。然而，矩阵分解缺乏获取协作信息的能力，这一点已被最近的研究所证实，并通过将用户交互解释为信号得到了改善。本文旨在找出度量学习如何连接到这些基于信号的模型。通过采用广义距离度量，我们发现在基于信号的模型中，比起估计距离本身，更容易估计距离的残差，残差是指从用户到目标项目和另一个项目的距离之间的差异。进一步的分析还揭示了交互信号的归一化强度与推荐的新颖性之间的联系，这一点一直被现有的研究所忽视。基于上述发现，我们提出了一种新的模型来学习广义距离用户-项目距离度量，通过建立距离残差模型来捕捉交互信号中的用户偏好。提出的 CoRML 模型通过引入新的近似排序权进一步提高了训练效率。在4个公共数据集上进行的大量实验表明，与最先进的协同过滤基线相比，CoRML 具有更好的性能，同时具有高效率和提供新颖推荐的能力，为基于度量学习的推荐系统的研究提供了新的视角。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Collaborative+Residual+Metric+Learning)|0|
|[Generative-Contrastive Graph Learning for Recommendation](https://doi.org/10.1145/3539618.3591691)|Yonghui Yang, Zhengwei Wu, Le Wu, Kun Zhang, Richang Hong, Zhiqiang Zhang, Jun Zhou, Meng Wang||By treating users' interactions as a user-item graph, graph learning models have been widely deployed in Collaborative Filtering(CF) based recommendation. Recently, researchers have introduced Graph Contrastive Learning(GCL) techniques into CF to alleviate the sparse supervision issue, which first constructs contrastive views by data augmentations and then provides self-supervised signals by maximizing the mutual information between contrastive views. Despite the effectiveness, we argue that current GCL-based recommendation models are still limited as current data augmentation techniques, either structure augmentation or feature augmentation. First, structure augmentation randomly dropout nodes or edges, which is easy to destroy the intrinsic nature of the user-item graph. Second, feature augmentation imposes the same scale noise augmentation on each node, which neglects the unique characteristics of nodes on the graph. To tackle the above limitations, we propose a novel Variational Graph Generative-Contrastive Learning(VGCL) framework for recommendation. Specifically, we leverage variational graph reconstruction to estimate a Gaussian distribution of each node, then generate multiple contrastive views through multiple samplings from the estimated distributions, which builds a bridge between generative and contrastive learning. Besides, the estimated variances are tailored to each node, which regulates the scale of contrastive loss for each node on optimization. Considering the similarity of the estimated distributions, we propose a cluster-aware twofold contrastive learning, a node-level to encourage consistency of a node's contrastive views and a cluster-level to encourage consistency of nodes in a cluster. Finally, extensive experimental results on three public datasets clearly demonstrate the effectiveness of the proposed model.|通过将用户交互作为一个用户项目图，图形学习模型已经广泛应用于基于协同过滤(CF)的推荐中。近年来，研究人员将图形对比学习(Graph Contrative Learning，GCL)技术引入到 CF 中，解决了对比视图稀疏监督问题。尽管有效，我们认为目前基于 GCL 的推荐模型仍然是有限的，作为当前的数据增强技术，无论是结构增强或特征增强。首先，结构扩展随机丢弃节点或边，这很容易破坏用户项图的内在本质。其次，特征增强对每个节点进行相同尺度的噪声增强，忽略了图上节点的独特性。针对上述局限性，本文提出了一种新的变分图生成对比学习(VGCL)框架。具体来说，我们利用变分图重建来估计每个节点的正态分布，然后从估计的分布中通过多个样本生成多个对比视图，这在生成学习和对比学习之间架起了一座桥梁。此外，估计的方差是针对每个节点量身定制的，它调节了每个节点在优化时的对比损失规模。考虑到估计分布的相似性，我们提出了一种基于聚类的双重对比学习方法，一个节点级别用于增强节点对比视图的一致性，一个聚类级别用于增强聚类中节点的一致性。最后，在三个公共数据集上的大量实验结果清楚地表明了该模型的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative-Contrastive+Graph+Learning+for+Recommendation)|0|
|[Hydrus: Improving Personalized Quality of Experience in Short-form Video Services](https://doi.org/10.1145/3539618.3591696)|Zhiyu Yuan, Kai Ren, Gang Wang, Xin Miao|Tsinghua University, Beijing, China; Tsinghua University & Kuaishou, Beijing, China; Kuaishou, Beijing, China|Traditional approaches to improving users' quality of experience (QoE) focus on minimizing the latency on the server side. Through an analysis of 15 million users, however, we find that for short-form video apps, user experience depends on both response latency and recommendation accuracy. This observation brings a dilemma to service providers since improving recommendation accuracy requires adopting complex strategies that demand heavy computation, which substantially increases response latency. Our motivation is that users' sensitivity to response latency and recommendation accuracy varies greatly. In other words, some users would accept a 20ms increase in latency to enjoy higher-quality videos, while others prioritize minimizing lag above all else. Inspired by this, we present Hydrus, a novel resource allocation system that delivers the best possible personalized QoE by making tradeoffs between response latency and recommendation accuracy. Specifically, we formulate the resource allocation problem as a utility maximization problem, and Hydrus is guaranteed to solve the problem within a few milliseconds. We demonstrate the effectiveness of Hydrus through offline simulation and online experiments in Kuaishou, a massively popular video app with hundreds of millions of users worldwide. The results show that Hydrus can increase QoE by 35.6% with the same latency or reduce the latency by 10.1% with the same QoE. Furthermore, Hydrus can achieve 54.5% higher throughput without a decrease in QoE. In online A/B testing, Hydrus significantly improves click-through rate (CTR) and watch time; it can also reduce system resource costs without sacrificing QoE.|提高用户体验质量(QoE)的传统方法侧重于最小化服务器端的延迟。然而，通过对1500万用户的分析，我们发现，对于短格式视频应用程序，用户体验既取决于响应时间，也取决于推荐的准确性。这种现象给服务提供商带来了一个难题，因为提高推荐的准确性需要采用复杂的策略，这些策略需要大量的计算，这大大增加了响应延迟。我们的动机是，用户对响应延迟和推荐准确性的敏感性差异很大。换句话说，一些用户会接受延迟增加20毫秒来享受更高质量的视频，而其他人则优先考虑最小化延迟。受此启发，我们提出了 Hydrus，一种新的资源分配系统，通过在响应延迟和推荐准确性之间进行权衡，提供尽可能最好的个性化 QoE。具体来说，我们把资源分配问题当作一个效用最大化，Hydrus 保证能在几毫秒内解决这个问题。我们通过在 Kuaishou 的离线模拟和在线实验来展示 Hydrus 的有效性，这是一个在全世界拥有数亿用户的非常流行的视频应用程序。结果表明，在相同的延迟时间下，Hydrus 可以提高35.6% 的 QoE，而在相同的延迟时间下，Hydrus 可以降低10.1% 的 QoE。此外，Hydrus 可以在不降低 QoE 的情况下提高54.5% 的吞吐量。在线 A/B 测试中，Hydrus 显著提高了点进率和观看时间，它还可以在不牺牲 QoE 的情况下降低系统资源成本。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hydrus:+Improving+Personalized+Quality+of+Experience+in+Short-form+Video+Services)|0|
|[Topic-enhanced Graph Neural Networks for Extraction-based Explainable Recommendation](https://doi.org/10.1145/3539618.3591776)|Jie Shuai, Le Wu, Kun Zhang, Peijie Sun, Richang Hong, Meng Wang|Tsinghua University, Beijing, China; Hefei University of Technology, Hefei, China|Review information has been demonstrated beneficial for the explainable recommendation. It can be treated as training corpora for generation-based methods or knowledge bases for extraction-based models. However, for generation-based methods, the sparsity of user-generated reviews and the high complexity of generative language models lead to a lack of personalization and adaptability. For extraction-based methods, focusing only on relevant attributes makes them invalid in situations where explicit attribute words are absent, limiting the potential of extraction-based models. To this end, in this paper, we focus on the explicit and implicit analysis of review information simultaneously and propose novel a Topic-enhanced Graph Neural Networks (TGNN) to fully explore review information for better explainable recommendations. To be specific, we first use a pre-trained topic model to analyze reviews at the topic level, and design a sentence-enhanced topic graph to model user preference explicitly, where topics are intermediate nodes between users and items. Corresponding sentences serve as edge features. Thus, the requirement of explicit attribute words can be mitigated. Meanwhile, we leverage a review-enhanced rating graph to model user preference implicitly, where reviews are also considered as edge features for fine-grained user-item interaction modeling. Next, user and item representations from two graphs are used for final rating prediction and explanation extraction. Extensive experiments on three real-world datasets demonstrate the superiority of our proposed TGNN with both recommendation accuracy and explanation quality.|评审信息已被证明对解释性建议有益。它可以作为基于生成的方法的训练语料库，也可以作为基于抽取的模型的知识库。然而，对于基于生成的方法来说，用户生成评论的稀少性和生成语言模型的高度复杂性导致了个性化和适应性的缺乏。对于基于抽取的方法，只关注相关属性会使它们在没有显式属性词的情况下失效，从而限制了基于抽取的模型的潜力。为此，本文集中研究了复习信息的显性和隐性分析，并提出了一种新颖的主题增强图形神经网络(TGNN)来充分挖掘复习信息，以获得更好的解释性建议。具体来说，我们首先使用一个预先训练好的主题模型来分析主题层面的评论，然后设计一个句子增强的主题图来显式地模拟用户偏好，其中主题是用户和项目之间的中间节点。相应的句子作为边缘特征。因此，可以减少显式属性词的需求。同时，我们利用评论增强的评分图来隐式地建模用户偏好，其中评论也被认为是细粒度用户项交互建模的边缘特征。接下来，使用来自两个图的用户和项目表示来进行最终评分预测和解释提取。在三个实际数据集上的大量实验表明，我们提出的 TGNN 在推荐精度和解释质量方面都具有优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Topic-enhanced+Graph+Neural+Networks+for+Extraction-based+Explainable+Recommendation)|0|
|[Strategy-aware Bundle Recommender System](https://doi.org/10.1145/3539618.3591771)|Yinwei Wei, Xiaohao Liu, Yunshan Ma, Xiang Wang, Liqiang Nie, TatSeng Chua|University of Science and Technology of China, Hefei, China; Harbin Institute of Technology (Shenzhen), Shenzhen, China; National University of Singapore, Singapore, Singapore; University of Chinese Academy of Sciences, Beijing, China|A bundle is a group of items that provides improved services to users and increased profits for sellers. However, locating the desired bundles that match the users' tastes still challenges us, due to the sparsity issue. Despite the remarkable performance of existing approaches, we argue that they seldom consider the bundling strategy (i.e., how the items within a bundle are associated with each other) in the bundle recommendation, resulting in the suboptimal user and bundle representations for their interaction prediction. Therefore, we propose to model the strategy-aware user and bundle representations for the bundle recommendation. Towards this end, we develop a new model for bundle recommendation, termed Bundle Graph Transformer (BundleGT), which consists of the token embedding layer, hierarchical graph transformer (HGT) layer, and prediction layer. Specifically, in the token embedding layer, we take the items within bundles as tokens and represent them with items' id embedding learned from user-item interactions. Having the input tokens, the HGT layer can simultaneously model the strategy-aware bundle and user representations. Therein, we encode the prior knowledge of bundling strategy from the well-designed bundles and incorporate it with tokens' embeddings to model the bundling strategy and learn the strategy-aware bundle representations. Meanwhile, upon the correlation between bundles consumed by the same user, we further learn the user preference on bundling strategy. Jointly considering it with the user preference on the item content, we can learn the strategy-aware user representation for user-bundle interaction prediction. Conducting extensive experiments on Youshu, ifashion, and Netease datasets, we demonstrate that our proposed model outperforms the state-of-the-art baselines (e.g., BundelNet [7] Net, BGCN [3] BGCN, and CrossCBR [22]), justifying the effectiveness of our proposed model. Moreover, in HGT layer, our devised light self-attention block improves not only the accuracy performance but efficiency of BundleGT. Our code is publicly available at: https://github.com/Xiaohao-Liu/BundleGT.|捆绑销售是指向用户提供更好的服务并为卖家增加利润的一组商品。然而，由于稀缺性问题，找到符合用户口味的所需捆绑包仍然是一个挑战。尽管现有的方法表现出色，我们认为他们很少考虑捆绑推荐中的捆绑策略(即，一个捆绑包中的项目是如何相互关联的) ，导致用户和捆绑包的交互预测表示次优。因此，我们建议为捆绑推荐建立策略感知用户和捆绑表示的模型。为此，我们提出了一种新的捆绑式推荐模型——捆绑图转换器(Bundle Graph former，BundleGT) ，该模型由令牌嵌入层、层次图转换器(HGT)层和预测层组成。具体来说，在令牌嵌入层中，我们将捆绑包中的项目作为令牌，并使用从用户-项目交互中学到的项目 id 嵌入来表示它们。有了输入令牌，HGT 层可以同时对战略感知捆绑包和用户表示进行建模。其中，我们从设计良好的捆绑包中编码捆绑策略的先验知识，并将其与令牌的嵌入相结合，对捆绑策略进行建模，学习策略感知的捆绑表示。同时，根据同一用户使用的捆绑包之间的相关性，进一步了解用户对捆绑策略的偏好。结合用户对项目内容的偏好，我们可以学习用于用户绑定交互预测的策略感知用户表示。在有书，时尚和网易数据集上进行广泛的实验，我们证明我们提出的模型优于最先进的基线(例如 BundelNet [7] Net，BGCN [3] BGCN 和 CrossCBR [22]) ，证明了我们提出的模型的有效性。此外，在 HGT 层，我们设计的光自注意块不仅提高了 BundleGT 的精度性能，而且提高了效率。我们的代码可以在以下 https://github.com/xiaohao-liu/bundlegt 公开获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Strategy-aware+Bundle+Recommender+System)|0|
|[Soft Prompt Decoding for Multilingual Dense Retrieval](https://doi.org/10.1145/3539618.3591769)|Zhiqi Huang, Hansi Zeng, Hamed Zamani, James Allan||In this work, we explore a Multilingual Information Retrieval (MLIR) task, where the collection includes documents in multiple languages. We demonstrate that applying state-of-the-art approaches developed for cross-lingual information retrieval to MLIR tasks leads to sub-optimal performance. This is due to the heterogeneous and imbalanced nature of multilingual collections -- some languages are better represented in the collection and some benefit from large-scale training data. To address this issue, we present KD-SPD, a novel soft prompt decoding approach for MLIR that implicitly "translates" the representation of documents in different languages into the same embedding space. To address the challenges of data scarcity and imbalance, we introduce a knowledge distillation strategy. The teacher model is trained on rich English retrieval data, and by leveraging bi-text data, our distillation framework transfers its retrieval knowledge to the multilingual document encoder. Therefore, our approach does not require any multilingual retrieval training data. Extensive experiments on three MLIR datasets with a total of 15 languages demonstrate that KD-SPD significantly outperforms competitive baselines in all cases. We conduct extensive analyses to show that our method has less language bias and better zero-shot transfer ability towards new languages.|在这项工作中，我们探索了一个多语言信息检索(mLIR)任务，其中收集包括多种语言的文档。我们证明，将为跨语言信息检索开发的最先进的方法应用于 mLIR 任务，会导致性能欠佳。这是由于多语种集合的异质性和不平衡性——有些语言在集合中得到更好的表示，有些则得益于大规模的培训数据。为了解决这个问题，我们提出了一种新的针对 MLIR 的软提示解码方法 KD-SPD，它将不同语言的文档表示隐式地“翻译”到相同的嵌入空间中。为了解决数据稀缺和不平衡的挑战，我们引入了一种知识提取策略。教师模型是在丰富的英语检索数据上进行训练的，并且通过利用双文本数据，我们的精馏框架将其检索知识传递给多语言文档编码器。因此，我们的方法不需要任何多语言检索训练数据。在三个共有15种语言的 MLIR 数据集上的广泛实验表明，KD-SPD 在所有情况下都显著优于竞争性基线。我们进行了广泛的分析表明，我们的方法具有较少的语言偏见和更好的零-镜头转移能力的新语言。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Soft+Prompt+Decoding+for+Multilingual+Dense+Retrieval)|0|
|[Rethinking Benchmarks for Cross-modal Image-text Retrieval](https://doi.org/10.1145/3539618.3591758)|Weijing Chen, Linli Yao, Qin Jin||Image-text retrieval, as a fundamental and important branch of information retrieval, has attracted extensive research attentions. The main challenge of this task is cross-modal semantic understanding and matching. Some recent works focus more on fine-grained cross-modal semantic matching. With the prevalence of large scale multimodal pretraining models, several state-of-the-art models (e.g. X-VLM) have achieved near-perfect performance on widely-used image-text retrieval benchmarks, i.e. MSCOCO-Test-5K and Flickr30K-Test-1K. In this paper, we review the two common benchmarks and observe that they are insufficient to assess the true capability of models on fine-grained cross-modal semantic matching. The reason is that a large amount of images and texts in the benchmarks are coarse-grained. Based on the observation, we renovate the coarse-grained images and texts in the old benchmarks and establish the improved benchmarks called MSCOCO-FG and Flickr30K-FG. Specifically, on the image side, we enlarge the original image pool by adopting more similar images. On the text side, we propose a novel semi-automatic renovation approach to refine coarse-grained sentences into finer-grained ones with little human effort. Furthermore, we evaluate representative image-text retrieval models on our new benchmarks to demonstrate the effectiveness of our method. We also analyze the capability of models on fine-grained semantic comprehension through extensive experiments. The results show that even the state-of-the-art models have much room for improvement in fine-grained semantic understanding, especially in distinguishing attributes of close objects in images. Our code and improved benchmark datasets are publicly available at: https://github.com/cwj1412/MSCOCO-Flikcr30K_FG, which we hope will inspire further in-depth research on cross-modal retrieval.|图像-文本检索作为信息检索学的一个重要分支，已经引起了广泛的研究关注。这项任务的主要挑战是跨模态语义理解和匹配。最近的一些工作更多地关注于细粒度的跨情态语义匹配。随着大规模多模式预训练模型的普及，一些最先进的模型(例如 X-VLM)在广泛使用的图像-文本检索基准(例如 MSCOCO-Test-5K 和 Flickr30K-Test-1K)上取得了近乎完美的性能。在本文中，我们回顾了这两个常用的基准测试，发现它们不足以评估模型在细粒度跨模态语义匹配方面的真正能力。原因是基准测试中的大量图像和文本是粗粒度的。在此基础上，对原有基准测试中的粗粒度图像和文本进行了改进，建立了改进后的基准测试 MSCOCO-FG 和 Flickr30K-FG。具体地说，在图像方面，我们通过采用更多的相似图像来扩大原始图像池。在文本方面，我们提出了一种新的半自动更新方法，以较少的人工精力将粗粒度的句子细化为细粒度的句子。在此基础上，我们对具有代表性的图像-文本检索模型进行了评估，以验证该方法的有效性。通过大量实验，分析了模型对细粒度语义理解的能力。结果表明，即使是最先进的模型，在细粒度的语义理解方面，尤其是在图像中近似对象的属性识别方面，仍有很大的改进空间。我们的代码和改进的基准数据集可以在以下 https://github.com/cwj1412/mscoco-flikcr30k_fg 公开获得，我们希望这将激发对跨模式检索的进一步深入研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rethinking+Benchmarks+for+Cross-modal+Image-text+Retrieval)|0|
|[From Region to Patch: Attribute-Aware Foreground-Background Contrastive Learning for Fine-Grained Fashion Retrieval](https://doi.org/10.1145/3539618.3591690)|Jianfeng Dong, Xiaoman Peng, Zhe Ma, Daizong Liu, Xiaoye Qu, Xun Yang, Jixiang Zhu, Baolong Liu||Attribute-specific fashion retrieval (ASFR) is a challenging information retrieval task, which has attracted increasing attention in recent years. Different from traditional fashion retrieval which mainly focuses on optimizing holistic similarity, the ASFR task concentrates on attribute-specific similarity, resulting in more fine-grained and interpretable retrieval results. As the attribute-specific similarity typically corresponds to the specific subtle regions of images, we propose a Region-to-Patch Framework (RPF) that consists of a region-aware branch and a patch-aware branch to extract fine-grained attribute-related visual features for precise retrieval in a coarse-to-fine manner. In particular, the region-aware branch is first to be utilized to locate the potential regions related to the semantic of the given attribute. Then, considering that the located region is coarse and still contains the background visual contents, the patch-aware branch is proposed to capture patch-wise attribute-related details from the previous amplified region. Such a hybrid architecture strikes a proper balance between region localization and feature extraction. Besides, different from previous works that solely focus on discriminating the attribute-relevant foreground visual features, we argue that the attribute-irrelevant background features are also crucial for distinguishing the detailed visual contexts in a contrastive manner. Therefore, a novel E-InfoNCE loss based on the foreground and background representations is further proposed to improve the discrimination of attribute-specific representation. Extensive experiments on three datasets demonstrate the effectiveness of our proposed framework, and also show a decent generalization of our RPF on out-of-domain fashion images. Our source code is available at https://github.com/HuiGuanLab/RPF.|特定属性的时尚检索(asFR)是一项具有挑战性的信息检索检索任务，近年来受到越来越多的关注。与以优化整体相似度为核心的传统时尚检索不同，ASFR 任务集中于特定属性的相似度，使得检索结果更加细粒度和可解释性。由于特定属性的相似性通常对应于图像的特定细微区域，因此我们提出了一种由区域感知分支和补丁感知分支组成的区域到补丁框架(Regional-to-Patch Framework，RPF)来提取细粒度的属性相关视觉特征，以便以粗到细的方式进行精确检索。特别地，区域感知分支首先被用来定位与给定属性的语义相关的潜在区域。然后，考虑到所定位的区域比较粗糙，并且仍然包含背景视觉内容，提出了基于补丁感知的分支来从前面的放大区域中获取与补丁相关的属性细节。这种混合结构在区域定位和特征提取之间取得了适当的平衡。此外，与以往单纯侧重于识别与属性相关的前景视觉特征的作品不同，我们认为与属性无关的背景特征对于以对比的方式识别详细的视觉背景也是至关重要的。因此，本文进一步提出了一种基于前景和背景表征的新型 E-InfoNCE 损失算法，以改善特定属性表征的识别能力。在三个数据集上的大量实验证明了我们提出的框架的有效性，同时也显示了我们的 RPF 在域外时尚图像上的良好推广。我们的源代码可以在 https://github.com/huiguanlab/rpf 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Region+to+Patch:+Attribute-Aware+Foreground-Background+Contrastive+Learning+for+Fine-Grained+Fashion+Retrieval)|0|
|[Multi-view Multi-aspect Neural Networks for Next-basket Recommendation](https://doi.org/10.1145/3539618.3591738)|Zhiying Deng, Jianjun Li, Zhiqiang Guo, Wei Liu, Li Zou, Guohui Li|Huazhong University of Science and Technology, Wuhan, China|Next-basket recommendation (NBR) is a type of recommendation that aims to recommend a set of items to users according to their historical basket sequences. Existing NBR methods suffer from two limitations: (1) overlooking low-level item correlations, which results in coarse-grained item representation; and (2) failing to consider spurious interests in repeated behaviors, leading to suboptimal user interest learning. To address these limitations, we propose a novel solution named Multi-view Multi-aspect Neural Recommendation (MMNR) for NBR, which first normalizes the interactions from both the user-side and item-side, respectively, aiming to remove the spurious interests, and utilizes them as weights for items from different views to construct differentiated representations for each interaction item, enabling comprehensive user interest learning. Then, to capture low-level item correlations, MMNR models different aspects of items to obtain disentangled representations of items, thereby fully capturing multiple user interests. Extensive experiments on real-world datasets demonstrate the effectiveness of MMNR, showing that it consistently outperforms several state-of-the-art NBR methods.|下一个篮子推荐(NBR)是一种推荐类型，旨在根据用户的历史篮子序列向用户推荐一组项目。现有的 NBR 方法存在两个局限性: (1)忽视低层次的项目相关性，导致粗粒度的项目表示; (2)未能考虑重复行为中的虚假兴趣，导致次优的用户兴趣学习。针对这些局限性，本文提出了一种基于 NBR 的多视角多方面神经网络推荐方法(Multi-view Multi-spectNearn汪汪推荐方法) ，该方法首先对用户方和项目方的交互进行规范化处理，消除虚假兴趣，并利用这些兴趣作为不同视角项目的权重，为每个交互项目构建差异化表示，从而实现用户兴趣的全面学习。然后，为了捕获低层次的项目相关性，MMNR 对项目的不同方面进行建模，以获得项目的分离表示，从而充分捕获多个用户兴趣。在真实世界数据集上的大量实验证明了 MMNR 的有效性，表明它始终优于几种最先进的 NBR 方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-view+Multi-aspect+Neural+Networks+for+Next-basket+Recommendation)|0|
|[EulerNet: Adaptive Feature Interaction Learning via Euler's Formula for CTR Prediction](https://doi.org/10.1145/3539618.3591681)|Zhen Tian, Ting Bai, Wayne Xin Zhao, JiRong Wen, Zhao Cao||Learning effective high-order feature interactions is very crucial in the CTR prediction task. However, it is very time-consuming to calculate high-order feature interactions with massive features in online e-commerce platforms. Most existing methods manually design a maximal order and further filter out the useless interactions from them. Although they reduce the high computational costs caused by the exponential growth of high-order feature combinations, they still suffer from the degradation of model capability due to the suboptimal learning of the restricted feature orders. The solution to maintain the model capability and meanwhile keep it efficient is a technical challenge, which has not been adequately addressed. To address this issue, we propose an adaptive feature interaction learning model, named as EulerNet, in which the feature interactions are learned in a complex vector space by conducting space mapping according to Euler's formula. EulerNet converts the exponential powers of feature interactions into simple linear combinations of the modulus and phase of the complex features, making it possible to adaptively learn the high-order feature interactions in an efficient way. Furthermore, EulerNet incorporates the implicit and explicit feature interactions into a unified architecture, which achieves the mutual enhancement and largely boosts the model capabilities. Such a network can be fully learned from data, with no need of pre-designed form or order for feature interactions. Extensive experiments conducted on three public datasets have demonstrated the effectiveness and efficiency of our approach. Our code is available at: https://github.com/RUCAIBox/EulerNet.|在 CTR 预测任务中，学习有效的高阶特征交互是非常关键的。然而，在线电子商务平台中计算具有大量特征的高阶特征交互是非常耗时的。大多数现有的方法手工设计一个最大顺序，并进一步从中筛选出无用的交互。虽然它们降低了高阶特征组合的指数增长所造成的高计算成本，但是由于受限特征阶次的次优学习，它们仍然受到模型能力退化的影响。保持模型能力并同时保持其有效性的解决方案是一个技术挑战，尚未得到充分解决。针对这一问题，提出了一种自适应特征交互学习模型 EulerNet。EulerNet 将特征交互的指数幂转化为复杂特征模量和相位的简单线性组合，使得高阶特征交互的自适应学习成为可能。此外，EulerNet 将隐式和显式的特征交互融合到一个统一的体系结构中，实现了相互增强，大大提高了模型的性能。这样的网络可以完全从数据中学习，不需要预先设计的形式或特征交互的顺序。在三个公共数据集上进行的大量实验已经证明了我们方法的有效性和效率。我们的代码可以在以下 https://github.com/rucaibox/eulernet 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EulerNet:+Adaptive+Feature+Interaction+Learning+via+Euler's+Formula+for+CTR+Prediction)|0|
|[FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation](https://doi.org/10.1145/3539618.3591687)|Sebastian Hofstätter, Jiecao Chen, Karthik Raman, Hamed Zamani|University of Massachusetts, Amherst; Technische Universität Wien; Google|Retrieval-augmented generation models offer many benefits over standalone language models: besides a textual answer to a given query they provide provenance items retrieved from an updateable knowledge base. However, they are also more complex systems and need to handle long inputs. In this work, we introduce FiD-Light to strongly increase the efficiency of the state-of-the-art retrieval-augmented FiD model, while maintaining the same level of effectiveness. Our FiD-Light model constrains the information flow from the encoder (which encodes passages separately) to the decoder (using concatenated encoded representations). Furthermore, we adapt FiD-Light with re-ranking capabilities through textual source pointers, to improve the top-ranked provenance precision. Our experiments on a diverse set of seven knowledge intensive tasks (KILT) show FiD-Light consistently improves the Pareto frontier between query latency and effectiveness. FiD-Light with source pointing sets substantial new state-of-the-art results on six KILT tasks for combined text generation and provenance retrieval evaluation, while maintaining reasonable efficiency.|与独立语言模型相比，检索增强生成模型提供了许多好处: 除了对给定查询的文本回答之外，它们还提供了从可更新的知识库中检索到的出处项。然而，它们也是更复杂的系统，需要处理长输入。在这项工作中，我们引入了 FiD-Light，在保持相同的效率水平的同时，强有力地提高了最先进的检索增强 FiD 模型的效率。我们的 FiD-Light 模型约束了从编码器(分别编码通道)到解码器(使用级联编码表示)的信息流。此外，我们通过文本源指针改编具有重新排序能力的 FiD-Light，以提高顶级来源精度。我们在七个不同的知识密集型任务(KILT)上的实验表明，FiD-Light 一致地改善了查询延迟和有效性之间的帕累托边界。带源点的 FiD-Light 在六个 KILT 任务上设置了大量的最新结果，用于组合文本生成和出处检索评估，同时保持合理的效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FiD-Light:+Efficient+and+Effective+Retrieval-Augmented+Text+Generation)|0|
|[PLATE: A Prompt-Enhanced Paradigm for Multi-Scenario Recommendations](https://doi.org/10.1145/3539618.3591750)|Yuhao Wang, Xiangyu Zhao, Bo Chen, Qidong Liu, Huifeng Guo, Huanshuo Liu, Yichao Wang, Rui Zhang, Ruiming Tang|ruizhang.info, Shenzhen, China; City University of Hong Kong, Hong Kong, China; Huawei Noah's Ark Lab, Shenzhen, China; Huawei Noah's Ark Lab, Shanghai, China; Sun Yat-sen University, Guangzhou, China|With the explosive growth of commercial applications of recommender systems, multi-scenario recommendation (MSR) has attracted considerable attention, which utilizes data from multiple domains to improve their recommendation performance simultaneously. However, training a unified deep recommender system (DRS) may not explicitly comprehend the commonality and difference among domains, whereas training an individual model for each domain neglects the global information and incurs high computation costs. Likewise, fine-tuning on each domain is inefficient, and recent advances that apply the prompt tuning technique to improve fine-tuning efficiency rely solely on large-sized transformers. In this work, we propose a novel prompt-enhanced paradigm for multi-scenario recommendation. Specifically, a unified DRS backbone model is first pre-trained using data from all the domains in order to capture the commonality across domains. Then, we conduct prompt tuning with two novel prompt modules, capturing the distinctions among various domains and users. Our experiments on Douban, Amazon, and Ali-CCP datasets demonstrate the effectiveness of the proposed paradigm with two noticeable strengths: (i) its great compatibility with various DRS backbone models, and (ii) its high computation and storage efficiency with only 6% trainable parameters in prompt tuning phase. The implementation code is available for easy reproduction.|随着推荐系统商业应用的迅猛发展，多场景推荐(MSR)技术引起了人们的广泛关注，它利用来自多个领域的数据来同时提高推荐性能。然而，训练一个统一的深度推荐系统(DRS)可能无法明确理解领域之间的共性和差异，而为每个领域训练一个单独的模型会忽略全局信息，并导致高计算成本。同样，每个领域的微调效率都不高，最近应用快速调优技术来提高微调效率的进展完全依赖于大型变压器。在这项工作中，我们提出了一个新的提示增强的多情景推荐范式。具体来说，一个统一的 DRS 骨干模型是首先使用来自所有领域的数据进行预训练，以便捕获跨领域的共性。然后，我们用两个新颖的提示模块进行提示调优，捕捉不同领域和用户之间的差异。我们在 Douban、亚马逊和阿里-CCP 数据集上的实验证明了这种模式的有效性，它有两个显著的优势: (i)它与各种 DRS 骨干模型的极大兼容性，(ii)它的高计算和存储效率，在及时调整阶段只有6% 的可训练参数。实现代码可以很容易地复制。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PLATE:+A+Prompt-Enhanced+Paradigm+for+Multi-Scenario+Recommendations)|0|
|[LightGT: A Light Graph Transformer for Multimedia Recommendation](https://doi.org/10.1145/3539618.3591716)|Yinwei Wei, Wenqi Liu, Fan Liu, Xiang Wang, Liqiang Nie, TatSeng Chua|University of Science and Technology of China, Hefei, China; Harbin Institute of Technology (Shenzhen), Shenzhen, China; National University of Singapore, Singapore, Singapore; Shandong University, Qingdao, China|Multimedia recommendation methods aim to discover the user preference on the multi-modal information to enhance the collaborative filtering (CF) based recommender system. Nevertheless, they seldom consider the impact of feature extraction on the user preference modeling and prediction of the user-item interaction, as the extracted features contain excessive information irrelevant to the recommendation. To capture the informative features from the extracted ones, we resort to Transformer model to establish the correlation between the items historically interacted by the same user. Considering its challenges in effectiveness and efficiency, we propose a novel Transformer-based recommendation model, termed as Light Graph Transformer model (LightGT). Therein, we develop a modal-specific embedding and a layer-wise position encoder for the effective similarity measurement, and present a light self-attention block to improve the efficiency of self-attention scoring. Based on these designs, we can effectively and efficiently learn the user preference from the off-the-shelf items' features to predict the user-item interactions. Conducting extensive experiments on Movielens, Tiktok and Kwai datasets, we demonstrate that LigthGT significantly outperforms the state-of-the-art baselines with less time. Our code is publicly available at: https://github.com/Liuwq-bit/LightGT.|多媒体推荐方法旨在发现用户对多模式信息的偏好，以增强基于协同过滤(CF)的推荐系统。然而，他们很少考虑特征提取对用户偏好建模和用户项目交互预测的影响，因为提取的特征包含了与推荐无关的过多信息。为了从提取的特征中获取信息性特征，我们使用 Transformer 模型来建立历史上由同一用户交互的项之间的相关性。考虑到其在有效性和效率方面的挑战，我们提出了一种新的基于变压器的推荐模型，称为光图形变压器模型(LightGT)。在此基础上，提出了一种基于模态特异性的嵌入方法和分层位置编码器，以提高自我注意评分的效率。基于这些设计，我们可以有效地从现成商品的特征中学习用户偏好，从而预测用户与商品的交互。在 Movielens，Tiktok 和葵数据集上进行了广泛的实验，我们证明 LigthGT 在更短的时间内显著优于最先进的基线。我们的代码可以在以下 https://github.com/liuwq-bit/lightgt 公开获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LightGT:+A+Light+Graph+Transformer+for+Multimedia+Recommendation)|0|
|[Law Article-Enhanced Legal Case Matching: A Causal Learning Approach](https://doi.org/10.1145/3539618.3591709)|Zhongxiang Sun, Jun Xu, Xiao Zhang, Zhenhua Dong, JiRong Wen||Legal case matching, which automatically constructs a model to estimate the similarities between the source and target cases, has played an essential role in intelligent legal systems. Semantic text matching models have been applied to the task where the source and target legal cases are considered as long-form text documents. These general-purpose matching models make the predictions solely based on the texts in the legal cases, overlooking the essential role of the law articles in legal case matching. In the real world, the matching results (e.g., relevance labels) are dramatically affected by the law articles because the contents and the judgments of a legal case are radically formed on the basis of law. From the causal sense, a matching decision is affected by the mediation effect from the cited law articles by the legal cases, and the direct effect of the key circumstances (e.g., detailed fact descriptions) in the legal cases. In light of the observation, this paper proposes a model-agnostic causal learning framework called Law-Match, under which the legal case matching models are learned by respecting the corresponding law articles. Given a pair of legal cases and the related law articles, Law-Match considers the embeddings of the law articles as instrumental variables (IVs), and the embeddings of legal cases as treatments. Using IV regression, the treatments can be decomposed into law-related and law-unrelated parts, respectively reflecting the mediation and direct effects. These two parts are then combined with different weights to collectively support the final matching prediction. We show that the framework is model-agnostic, and a number of legal case matching models can be applied as the underlying models. Comprehensive experiments show that Law-Match can outperform state-of-the-art baselines on three public datasets.|法律案件匹配是一种自动构建判断源案件与目标案件相似性的模型，在智能法律系统中发挥着重要作用。本文将语义文本匹配模型应用于源、目标法律案件作为长文本文档的任务中。这些通用匹配模型仅根据法律案件的案文进行预测，忽视了法律条款在法律案件匹配中的重要作用。在现实世界中，由于案件的内容和判决是在法律的基础上从根本上形成的，所以匹配结果(如关联标签)受到法律条文的巨大影响。从因果关系的角度来看，法律案件引用的法律条文所产生的调解效果，以及法律案件中关键情节(如详细的事实描述)的直接效果，都会影响匹配决策。基于这种观察，本文提出了一个模型无关的因果学习框架——法律匹配，在此框架下，通过尊重相应的法律条款来学习法律案例匹配模型。在给定一对法律案例和相关法律条文的情况下，Law-Match 将法律条文的嵌入视为工具变量，将法律案例的嵌入视为处理方法。利用四元回归，可以将处理分解为与法律相关的部分和与法律无关的部分，分别反映调解效果和直接效果。然后将这两部分与不同的权重相结合，共同支持最终的匹配预测。我们表明，该框架是模型无关的，一些法律案件匹配模型可以作为基础模型应用。综合实验表明，Law-Match 在三个公共数据集上的表现优于最先进的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Law+Article-Enhanced+Legal+Case+Matching:+A+Causal+Learning+Approach)|0|
|[Multimodal Counterfactual Learning Network for Multimedia-based Recommendation](https://doi.org/10.1145/3539618.3591739)|Shuaiyang Li, Dan Guo, Kang Liu, Richang Hong, Feng Xue|Hefei University of Technology & Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, Hefei, China; Hefei University of Technology, Hefei, China|Multimedia-based recommendation (MMRec) utilizes multimodal content (images, textual descriptions, etc.) as auxiliary information on historical interactions to determine user preferences. Most MMRec approaches predict user interests by exploiting a large amount of multimodal contents of user-interacted items, ignoring the potential effect of multimodal content of user-uninteracted items. As a matter of fact, there is a small portion of user preference-irrelevant features in the multimodal content of user-interacted items, which may be a kind of spurious correlation with user preferences, thereby degrading the recommendation performance. In this work, we argue that the multimodal content of user-uninteracted items can be further exploited to identify and eliminate the user preference-irrelevant portion inside user-interacted multimodal content, for example by counterfactual inference of causal theory. Going beyond multimodal user preference modeling only using interacted items, we propose a novel model called Multimodal Counterfactual Learning Network (MCLN), in which user-uninteracted items' multimodal content is additionally exploited to further purify the representation of user preference-relevant multimodal content that better matches the user's interests, yielding state-of-the-art performance. Extensive experiments are conducted to validate the effectiveness and rationality of MCLN. We release the complete codes of MCLN at https://github.com/hfutmars/MCLN.|基于多媒体的推荐(MMRec)利用多模态内容(图像、文本描述等)作为历史交互的辅助信息，以确定用户偏好。大多数 MMRec 方法通过利用用户交互项目的大量多通道内容来预测用户的兴趣，而忽略了用户未交互项目的多通道内容的潜在影响。事实上，在用户交互项目的多通道内容中，有一小部分与用户偏好无关的功能，这可能是一种与用户偏好有关的伪相关，从而降低了推荐性能。本文认为，可以进一步利用用户未交互项目的多通道内容来识别和消除用户交互多通道内容中与用户偏好无关的部分，如通过因果理论的反事实推断。超越了仅使用交互项目的多模态用户偏好建模，我们提出了一种新的模型，称为多模态反事实学习网络(MultimodalCounterfact Learning Network，MCLN) ，其中用户未交互项目的多模态内容被进一步利用，以进一步纯化与用户偏好相关的多模态内容的表示，更好地匹配用户的兴趣，产生最先进的性能。通过大量实验验证了 MCLN 算法的有效性和合理性。我们会在 https://github.com/hfutmars/MCLN 公布 MCLN 的完整代码。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multimodal+Counterfactual+Learning+Network+for+Multimedia-based+Recommendation)|0|
|[Dynamic Graph Evolution Learning for Recommendation](https://doi.org/10.1145/3539618.3591674)|Haoran Tang, Shiqing Wu, Guandong Xu, Qing Li|University of Technology Sydney, Sydney, Australia; The Hong Kong Polytechnic University, Hong Kong, China|Graph neural network (GNN) based algorithms have achieved superior performance in recommendation tasks due to their advanced capability of exploiting high-order connectivity between users and items. However, most existing GNN-based recommendation models ignore the dynamic evolution of nodes, where users will continuously interact with items over time, resulting in rapid changes in the environment (e.g., neighbor and structure). Moreover, the heuristic normalization of embeddings in dynamic recommendation is de-coupled with the model learning process, making the whole system suboptimal. In this paper, we propose a novel framework for generating satisfying recommendations in dynamic environments, called Dynamic Graph Evolution Learning (DGEL). First, we design three efficient real-time update learning methods for nodes from the perspectives of inherent interaction potential, time-decay neighbor augmentation, and symbiotic local structure learning. Second, we construct the re-scaling enhancement networks for dynamic embeddings to adaptively and automatically bridge the normalization process with model learning. Third, we leverage the interaction matching task and the future prediction task together for joint training to further improve performance. Extensive experiments on three real-world datasets demonstrate the effectiveness and improvements of our proposed DGEL. The code is available at https://github.com/henrictang/DGEL.|基于图神经网络(GNN)的推荐算法由于具有利用用户与项目之间高阶连通性的先进能力，在推荐任务中取得了较好的性能。然而，大多数现有的基于 GNN 的推荐模型忽略了节点的动态演化，即用户将随着时间的推移不断地与项目交互，从而导致环境(如邻居和结构)的快速变化。此外，将动态推荐中嵌入的启发式规范化与模型学习过程解耦，使整个系统处于次优状态。在本文中，我们提出了一个在动态环境中生成满意建议的新框架，称为动态图进化学习(DGEL)。首先，我们从固有交互势、时间衰减邻域增强和共生局部结构学习的角度设计了三种有效的节点实时更新学习方法。其次，构造了动态嵌入的可重缩放增强网络，自适应地、自动地连接规范化过程和模型学习。第三，我们将交互匹配任务和未来预测任务结合起来进行联合训练，以进一步提高性能。在三个实际数据集上的大量实验证明了我们提出的 DGEL 的有效性和改进。密码可在 https://github.com/henrictang/dgel 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamic+Graph+Evolution+Learning+for+Recommendation)|0|
|[Causal Decision Transformer for Recommender Systems via Offline Reinforcement Learning](https://doi.org/10.1145/3539618.3591648)|Siyu Wang, Xiaocong Chen, Dietmar Jannach, Lina Yao||Reinforcement learning-based recommender systems have recently gained popularity. However, the design of the reward function, on which the agent relies to optimize its recommendation policy, is often not straightforward. Exploring the causality underlying users' behavior can take the place of the reward function in guiding the agent to capture the dynamic interests of users. Moreover, due to the typical limitations of simulation environments (e.g., data inefficiency), most of the work cannot be broadly applied in large-scale situations. Although some works attempt to convert the offline dataset into a simulator, data inefficiency makes the learning process even slower. Because of the nature of reinforcement learning (i.e., learning by interaction), it cannot collect enough data to train during a single interaction. Furthermore, traditional reinforcement learning algorithms do not have a solid capability like supervised learning methods to learn from offline datasets directly. In this paper, we propose a new model named the causal decision transformer for recommender systems (CDT4Rec). CDT4Rec is an offline reinforcement learning system that can learn from a dataset rather than from online interaction. Moreover, CDT4Rec employs the transformer architecture, which is capable of processing large offline datasets and capturing both short-term and long-term dependencies within the data to estimate the causal relationship between action, state, and reward. To demonstrate the feasibility and superiority of our model, we have conducted experiments on six real-world offline datasets and one online simulator.|基于强化学习的推荐系统最近越来越受欢迎。然而，代理人依赖于奖励函数来优化其推荐策略，这种奖励函数的设计往往并不简单。探索用户行为背后的因果关系可以代替奖励功能，引导代理捕获用户的动态兴趣。此外，由于模拟环境的典型局限性(例如，数据效率低下) ，大多数工作不能广泛应用于大规模情况。尽管有些工作试图将离线数据集转换为模拟器，但数据效率低下使得学习过程更加缓慢。由于强化学习的本质(即通过交互来学习) ，它无法收集足够的数据来训练一次交互。此外，传统的强化学习算法没有像监督式学习算法那样可以直接从离线数据集中学习的强大能力。本文提出了一种新的推荐系统因果决策转换模型(CDT4Rec)。CDT4Rec 是一个离线强化学习系统，可以从数据集而不是在线交互中学习。此外，CDT4Rec 使用转换器结构，其能够处理大型离线数据集并捕获数据中的短期和长期依赖性，以估计行动，状态和报酬之间的因果关系。为了验证该模型的可行性和优越性，我们在六个真实世界的离线数据集和一个在线模拟器上进行了实验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causal+Decision+Transformer+for+Recommender+Systems+via+Offline+Reinforcement+Learning)|0|
|[It's Enough: Relaxing Diagonal Constraints in Linear Autoencoders for Recommendation](https://doi.org/10.1145/3539618.3591704)|Jaewan Moon, Hyeyoung Kim, Jongwuk Lee||Linear autoencoder models learn an item-to-item weight matrix via convex optimization with L2 regularization and zero-diagonal constraints. Despite their simplicity, they have shown remarkable performance compared to sophisticated non-linear models. This paper aims to theoretically understand the properties of two terms in linear autoencoders. Through the lens of singular value decomposition (SVD) and principal component analysis (PCA), it is revealed that L2 regularization enhances the impact of high-ranked PCs. Meanwhile, zero-diagonal constraints reduce the impact of low-ranked PCs, leading to performance degradation for unpopular items. Inspired by this analysis, we propose simple-yet-effective linear autoencoder models using diagonal inequality constraints, called Relaxed Linear AutoEncoder (RLAE) and Relaxed Denoising Linear AutoEncoder (RDLAE). We prove that they generalize linear autoencoders by adjusting the degree of diagonal constraints. Experimental results demonstrate that our models are comparable or superior to state-of-the-art linear and non-linear models on six benchmark datasets; they significantly improve the accuracy of long-tail items. These results also support our theoretical insights on regularization and diagonal constraints in linear autoencoders.|线性自动编码器模型通过 L2正则化和零对角约束的凸优化学习一个项目对项目的权重矩阵。尽管它们很简单，但与复杂的非线性模型相比，它们表现出了显著的性能。本文旨在从理论上理解线性自动编码器中两项的性质。通过奇异值分解(SVD)和主成分分析(PCA)透镜，我们发现二语正规化增强了高等级个人电脑的影响。与此同时，零对角线约束减少了低排名个人电脑的影响，导致不受欢迎项目的性能下降。受此分析的启发，我们提出了使用对角不等式约束的简单而有效的线性自动编码器模型，称为松弛线性自动编码器(RLAE)和松弛去噪线性自动编码器(RDLAE)。我们证明了它们通过调整对角线约束的程度来推广线性自动编码器。实验结果表明，在六个基准数据集上，我们的模型与最先进的线性和非线性模型具有可比性或优越性，它们显著提高了长尾项目的准确性。这些结果也支持我们对正则化和对角线约束的线性自动编码器的理论见解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=It's+Enough:+Relaxing+Diagonal+Constraints+in+Linear+Autoencoders+for+Recommendation)|0|
|[Graph Transformer for Recommendation](https://doi.org/10.1145/3539618.3591723)|Chaoliu Li, Lianghao Xia, Xubin Ren, Yaowen Ye, Yong Xu, Chao Huang||This paper presents a novel approach to representation learning in recommender systems by integrating generative self-supervised learning with graph transformer architecture. We highlight the importance of high-quality data augmentation with relevant self-supervised pretext tasks for improving performance. Towards this end, we propose a new approach that automates the self-supervision augmentation process through a rationale-aware generative SSL that distills informative user-item interaction patterns. The proposed recommender with Graph TransFormer (GFormer) that offers parameterized collaborative rationale discovery for selective augmentation while preserving global-aware user-item relationships. In GFormer, we allow the rationale-aware SSL to inspire graph collaborative filtering with task-adaptive invariant rationalization in graph transformer. The experimental results reveal that our GFormer has the capability to consistently improve the performance over baselines on different datasets. Several in-depth experiments further investigate the invariant rationale-aware augmentation from various aspects. The source code for this work is publicly available at: https://github.com/HKUDS/GFormer.|提出了一种将生成式自监督学习与图形变换结构相结合的推荐系统表示学习方法。我们强调高质量的数据增强与相关的自我监督的借口任务的重要性，以提高性能。为此，我们提出了一种新的方法，该方法通过一个基于理论的生成 SSL 来自动化自我监督增强过程，该 SSL 提取信息丰富的用户项交互模式。提出的推荐与图形转换器(GForm) ，提供参数化的协作理论发现的选择性增强，同时保持全局感知的用户项目关系。在 gform 中，我们允许基于逻辑的 SSL 激发图形协同过滤，在图形转换器中使用任务自适应的不变合理化。实验结果表明，该算法能够持续改善不同数据集的基线性能。几个深入的实验从各个方面进一步研究了不变的理论意识增强。这项工作的源代码可以在以下 https://github.com/hkuds/gformer 公开获得:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Transformer+for+Recommendation)|0|
|[Topic-oriented Adversarial Attacks against Black-box Neural Ranking Models](https://doi.org/10.1145/3539618.3591777)|YuAn Liu, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Wei Chen, Yixing Fan, Xueqi Cheng||Neural ranking models (NRMs) have attracted considerable attention in information retrieval. Unfortunately, NRMs may inherit the adversarial vulnerabilities of general neural networks, which might be leveraged by black-hat search engine optimization practitioners. Recently, adversarial attacks against NRMs have been explored in the paired attack setting, generating an adversarial perturbation to a target document for a specific query. In this paper, we focus on a more general type of perturbation and introduce the topic-oriented adversarial ranking attack task against NRMs, which aims to find an imperceptible perturbation that can promote a target document in ranking for a group of queries with the same topic. We define both static and dynamic settings for the task and focus on decision-based black-box attacks. We propose a novel framework to improve topic-oriented attack performance based on a surrogate ranking model. The attack problem is formalized as a Markov decision process (MDP) and addressed using reinforcement learning. Specifically, a topic-oriented reward function guides the policy to find a successful adversarial example that can be promoted in rankings to as many queries as possible in a group. Experimental results demonstrate that the proposed framework can significantly outperform existing attack strategies, and we conclude by re-iterating that there exist potential risks for applying NRMs in the real world.|神经排序模型(nRM)在信息检索领域引起了广泛的关注。不幸的是，非线性回归模型可能继承了一般神经网络的对抗性弱点，这些弱点可能被黑帽搜索引擎优化从业者所利用。最近，针对 NRM 的对抗性攻击已经在配对攻击设置中得到了探索，为特定查询对目标文档产生了对抗性扰动。在本文中，我们着重研究了一种更一般的扰动类型，并介绍了面向主题的对抗性自然模型排序攻击任务，其目的是找到一种不可察觉的扰动，可以促进目标文档在同一主题的一组查询中的排序。我们为任务定义了静态和动态设置，重点关注基于决策的黑盒攻击。提出了一种基于代理排序模型的面向主题攻击性能改进框架。攻击问题被形式化为一个马可夫决策过程(mDP) ，并使用强化学习来解决。具体来说，面向主题的奖励函数引导策略找到一个成功的对抗性例子，这个例子可以在一组中尽可能多的查询中进行排名推广。实验结果表明，该框架的性能明显优于现有的攻击策略，并通过重申在现实世界中应用 NRM 存在潜在的风险得出结论。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Topic-oriented+Adversarial+Attacks+against+Black-box+Neural+Ranking+Models)|0|
|[RCENR: A Reinforced and Contrastive Heterogeneous Network Reasoning Model for Explainable News Recommendation](https://doi.org/10.1145/3539618.3591753)|Hao Jiang, Chuanzhen Li, Juanjuan Cai, Jingling Wang|Communication University of China, Beijing, China|Existing news recommendation methods suffer from sparse and weak interaction data, leading to reduced effectiveness and explainability. Knowledge reasoning, which explores inferential trajectories in the knowledge graph, can alleviate data sparsity and provide explicitly recommended explanations. However, brute-force pre-processing approaches used in conventional methods are not suitable for fast-changing news recommendation. Therefore, we propose an explainable news recommendation model: the Reinforced and Contrastive Heterogeneous Network Reasoning Model for Explainable News Recommendation (RCENR), consisting of NHN-R2 and MR&CO frameworks. The NHN-R2 framework generates user/news subgraphs to enhance recommendation and extend the dimensions and diversity of reasoning. The MR&CO framework incorporates contrastive learning with a reinforcement-based strategy for self-supervised and efficient model training. Experiments on the MIND dataset show that RCENR is able to improve recommendation accuracy and provide diverse and credible explanations.|现有的新闻推荐方法存在交互数据稀少、交互性差等问题，导致推荐的有效性和可解释性降低。知识推理探索知识图中的推理轨迹，可以缓解数据稀疏性，并提供明确推荐的解释。然而，传统方法中使用的强力预处理方法并不适用于快速变化的新闻推荐。因此，我们提出了一个可解释的新闻推荐模型: 可解释新闻推荐的强化和对比异质网路推理模型(RCENR) ，由 NHN-R2和 MR & CO 框架组成。NHN-R2框架生成用户/新闻子图以增强推荐并扩展推理的维度和多样性。MR & CO 框架结合了对比学习和基于强化的自我监督和有效的模型训练策略。在 MIND 数据集上的实验表明，RCENR 能够提高推荐的准确性，并提供多样化和可信的解释。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RCENR:+A+Reinforced+and+Contrastive+Heterogeneous+Network+Reasoning+Model+for+Explainable+News+Recommendation)|0|
|[Seq-HGNN: Learning Sequential Node Representation on Heterogeneous Graph](https://doi.org/10.1145/3539618.3591765)|Chenguang Du, Kaichun Yao, Hengshu Zhu, Deqing Wang, Fuzhen Zhuang, Hui Xiong||Recent years have witnessed the rapid development of heterogeneous graph neural networks (HGNNs) in information retrieval (IR) applications. Many existing HGNNs design a variety of tailor-made graph convolutions to capture structural and semantic information in heterogeneous graphs. However, existing HGNNs usually represent each node as a single vector in the multi-layer graph convolution calculation, which makes the high-level graph convolution layer fail to distinguish information from different relations and different orders, resulting in the information loss in the message passing. %insufficient mining of information. To this end, we propose a novel heterogeneous graph neural network with sequential node representation, namely Seq-HGNN. To avoid the information loss caused by the single vector node representation, we first design a sequential node representation learning mechanism to represent each node as a sequence of meta-path representations during the node message passing. Then we propose a heterogeneous representation fusion module, empowering Seq-HGNN to identify important meta-paths and aggregate their representations into a compact one. We conduct extensive experiments on four widely used datasets from Heterogeneous Graph Benchmark (HGB) and Open Graph Benchmark (OGB). Experimental results show that our proposed method outperforms state-of-the-art baselines in both accuracy and efficiency. The source code is available at https://github.com/nobrowning/SEQ_HGNN.|近年来，异质图形神经网络(HGNN)在信息检索应用方面发展迅速。许多现有的 HGNN 设计了各种定制的图形卷积来捕获异构图形中的结构和语义信息。然而，现有的 HGNN 在多层图卷积计算中通常将每个节点表示为一个向量，使得高层图卷积层无法区分不同关系和不同顺序的信息，导致信息传递中的信息丢失。信息挖掘不足百分比。为此，我们提出了一种新的具有序列节点表示的异构图神经网络，即 Seq-HGNN。为了避免单向量节点表示造成的信息丢失，我们首先设计了一种序列节点表示学习机制，在节点消息传递过程中将每个节点表示为一系列元路径表示。然后提出了一个异构表示融合模块，赋予 Seq-HGNN 识别重要元路径的能力，并将它们的表示聚合成一个紧凑的元路径。我们对异构图基准(HGB)和开放图基准(OGB)的四个广泛使用的数据集进行了广泛的实验。实验结果表明，我们提出的方法在准确性和效率方面都优于现有的基线方法。源代码可在 https://github.com/nobrowning/seq_hgnn 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Seq-HGNN:+Learning+Sequential+Node+Representation+on+Heterogeneous+Graph)|0|
|[A Lightweight Constrained Generation Alternative for Query-focused Summarization](https://doi.org/10.1145/3539618.3591936)|Zhichao Xu, Daniel Cohen||Query-focused summarization (QFS) aims to provide a summary of a document that satisfies information need of a given query and is useful in various IR applications, such as abstractive snippet generation. Current QFS approaches typically involve injecting additional information, e.g. query-answer relevance or fine-grained token-level interaction between a query and document, into a finetuned large language model. However, these approaches often require extra parameters \& training, and generalize poorly to new dataset distributions. To mitigate this, we propose leveraging a recently developed constrained generation model Neurological Decoding (NLD) as an alternative to current QFS regimes which rely on additional sub-architectures and training. We first construct lexical constraints by identifying important tokens from the document using a lightweight gradient attribution model, then subsequently force the generated summary to satisfy these constraints by directly manipulating the final vocabulary likelihood. This lightweight approach requires no additional parameters or finetuning as it utilizes both an off-the-shelf neural retrieval model to construct the constraints and a standard generative language model to produce the QFS. We demonstrate the efficacy of this approach on two public QFS collections achieving near parity with the state-of-the-art model with substantially reduced complexity.|以查询为中心的摘要(Query-focus Summary，QFS)旨在提供满足给定查询的信息需求的文档摘要，并且在各种 IR 应用程序(如抽象代码片段生成)中非常有用。当前的 QFS 方法通常涉及到向一个微调的大型语言模型中注入额外的信息，例如查询-回答相关性或查询与文档之间的细粒度令牌级交互。然而，这些方法通常需要额外的参数和训练，并且很难推广到新的数据集分布。为了缓解这种情况，我们建议利用最近开发的约束生成模型神经解码(NLD)作为替代目前依赖于额外的子架构和培训的 QFS 制度。我们首先利用一个轻量级的梯度属性模型从文档中识别出重要的标记来构造词汇约束，然后通过直接操作最终的词汇可能性来强制生成的摘要满足这些约束。这种轻量级方法不需要额外的参数或微调，因为它既利用现成的神经检索模型来构造约束，又利用标准的生成语言模型来生成 QFS。我们证明了这种方法的功效，两个公共 QFS 收集实现接近平等的国家的最先进的模型，大大降低了复杂性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Lightweight+Constrained+Generation+Alternative+for+Query-focused+Summarization)|0|
|[A Static Pruning Study on Sparse Neural Retrievers](https://doi.org/10.1145/3539618.3591941)|Carlos Lassance, Simon Lupart, Hervé Déjean, Stéphane Clinchant, Nicola Tonellotto||Sparse neural retrievers, such as DeepImpact, uniCOIL and SPLADE, have been introduced recently as an efficient and effective way to perform retrieval with inverted indexes. They aim to learn term importance and, in some cases, document expansions, to provide a more effective document ranking compared to traditional bag-of-words retrieval models such as BM25. However, these sparse neural retrievers have been shown to increase the computational costs and latency of query processing compared to their classical counterparts. To mitigate this, we apply a well-known family of techniques for boosting the efficiency of query processing over inverted indexes: static pruning. We experiment with three static pruning strategies, namely document-centric, term-centric and agnostic pruning, and we assess, over diverse datasets, that these techniques still work with sparse neural retrievers. In particular, static pruning achieves $2\times$ speedup with negligible effectiveness loss ($\leq 2\%$ drop) and, depending on the use case, even $4\times$ speedup with minimal impact on the effectiveness ($\leq 8\%$ drop). Moreover, we show that neural rerankers are robust to candidates from statically pruned indexes.|稀疏神经检索器，如 DeepImpact、 uniCOIL 和 SPLADE，作为一种有效的反向索引检索方法，近年来得到了广泛的应用。他们的目标是学习词汇的重要性，并在某些情况下，文档扩展，以提供更有效的文档排序相比，传统的单词袋检索模型，如 BM25。然而，这些稀疏的神经检索已被证明增加了计算成本和查询处理的延迟相比，他们的经典对应物。为了缓解这种情况，我们应用了一系列众所周知的技术来提高反向索引上的查询处理效率: 静态剪枝。我们试验了三种静态修剪策略，即以文档为中心的、以术语为中心的和不可知的修剪，并且我们在不同的数据集上评估，这些技术仍然适用于稀疏的神经检索器。特别是，静态修剪可以达到 $2倍的加速效果，而且效果损失可以忽略不计($leq 2% $drop) ，根据用例的不同，甚至可以达到 $4倍的加速效果，而且对效果的影响最小($leq 8% $drop)。此外，我们证明了神经重新排序器对静态剪枝索引的候选者是鲁棒的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Static+Pruning+Study+on+Sparse+Neural+Retrievers)|0|
|[Adapting Learned Sparse Retrieval for Long Documents](https://doi.org/10.1145/3539618.3591943)|Thong Nguyen, Sean MacAvaney, Andrew Yates||Learned sparse retrieval (LSR) is a family of neural retrieval methods that transform queries and documents into sparse weight vectors aligned with a vocabulary. While LSR approaches like Splade work well for short passages, it is unclear how well they handle longer documents. We investigate existing aggregation approaches for adapting LSR to longer documents and find that proximal scoring is crucial for LSR to handle long documents. To leverage this property, we proposed two adaptations of the Sequential Dependence Model (SDM) to LSR: ExactSDM and SoftSDM. ExactSDM assumes only exact query term dependence, while SoftSDM uses potential functions that model the dependence of query terms and their expansion terms (i.e., terms identified using a transformer's masked language modeling head). Experiments on the MSMARCO Document and TREC Robust04 datasets demonstrate that both ExactSDM and SoftSDM outperform existing LSR aggregation approaches for different document length constraints. Surprisingly, SoftSDM does not provide any performance benefits over ExactSDM. This suggests that soft proximity matching is not necessary for modeling term dependence in LSR. Overall, this study provides insights into handling long documents with LSR, proposing adaptations that improve its performance.|学习稀疏检索(LSR)是一类将查询和文档转换为与词汇表对齐的稀疏权重向量的神经检索方法。虽然像 Splade 这样的 LSR 方法在处理短文时效果很好，但是它们在处理较长文档时效果如何还不清楚。我们研究了现有的将 LSR 适应于较长文档的聚合方法，发现近似评分对于 LSR 处理较长文档至关重要。为了利用这个特性，我们提出了序列依赖模型(SDM)对 LSR 的两种适应: ExactSDM 和 SoftSDM。ExactSDM 只假设精确的查询术语依赖性，而 SoftSDM 使用潜在函数对查询术语及其扩展术语的依赖性进行建模(即，使用转换器的掩蔽语言建模头标识的术语)。在 MSMARCO Document 和 TREC Robust04数据集上的实验表明，ExactSDM 和 SoftSDM 在不同文档长度约束下的性能都优于现有的 LSR 聚合方法。令人惊讶的是，与 ExactSDM 相比，SoftSDM 并没有提供任何性能优势。这表明软接近匹配对于 LSR 中的项依赖建模是不必要的。总的来说，这项研究为使用 LSR 处理长文档提供了见解，并提出了改进其性能的建议。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adapting+Learned+Sparse+Retrieval+for+Long+Documents)|0|
|[Affective Relevance: Inferring Emotional Responses via fNIRS Neuroimaging](https://doi.org/10.1145/3539618.3591946)|Tuukka Ruotsalo, Kalle Mäkelä, Michiel M. A. Spapé, Luis A. Leiva|University of Copenhagen and University of Helsinki, Helsinki, Finland; University of Helsinki, Helsinki, Finland; University of Luxembourg, Luxembourg, Luxembourg|Information retrieval (IR) relies on a general notion of relevance, which is used as the principal foundation for ranking and evaluation methods. However, IR does not account for more a nuanced affective experience. Here, we consider the emotional response decoded directly from the human brain as an alternative dimension of relevance. We report an experiment covering seven different scenarios in which we measure and predict how users emotionally respond to visual image contents by using functional near-infrared spectroscopy (fNIRS) neuroimaging on two commonly used affective dimensions: valence (negativity and positivity) and arousal (boredness and excitedness). Our results show that affective states can be successfully decoded using fNIRS, and utilized to complement the present notion of relevance in IR studies. For example, we achieved 0.39 Balanced accuracy and 0.61 AUC in 4-class classification of affective states (vs. 0.25 Balanced accuracy and 0.5 AUC of a random classifier). Likewise, we achieved 0.684 Precision@20 when retrieving high-arousal images. Our work opens new avenues for incorporating emotional states in IR evaluation, affective feedback, and information filtering.|信息检索(IR)依赖于一个普遍的相关性概念，这个概念被用作排名和评估方法的主要基础。然而，IR 并不能解释更细微的情感体验。在这里，我们认为情绪反应解码直接从人类大脑作为一个替代维度的相关性。我们报告了一个实验，在这个实验中，我们测量和预测了使用者对视觉图像内容的情绪反应，通过使用功能近红外光谱技术(fNIRS)神经成像技术，在两个常用的情感维度上进行测量和预测: 效价(消极和积极)和唤醒(无聊和兴奋)。我们的研究结果表明，情感状态可以成功地解码使用 fNIRS，并利用补充现有的概念的相关性在国际关系研究。例如，在情感状态的4类分类中，我们实现了0.39平衡准确性和0.61 AUC (相对于随机分类器的0.25平衡准确性和0.5 AUC)。同样，我们在检索高唤醒图像时也达到了0.684 Precision@20。我们的工作开辟了将情绪状态纳入 IR 评估、情感反馈和信息过滤的新途径。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Affective+Relevance:+Inferring+Emotional+Responses+via+fNIRS+Neuroimaging)|0|
|[Attacking Pre-trained Recommendation](https://doi.org/10.1145/3539618.3591949)|Yiqing Wu, Ruobing Xie, Zhao Zhang, Yongchun Zhu, Fuzhen Zhuang, Jie Zhou, Yongjun Xu, Qing He|WeChat, Tencent & Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; WeChat, Tencent, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Institute of Artificial Intelligence, Beihang University, Beijing, China|Recently, a series of pioneer studies have shown the potency of pre-trained models in sequential recommendation, illuminating the path of building an omniscient unified pre-trained recommendation model for different downstream recommendation tasks. Despite these advancements, the vulnerabilities of classical recommender systems also exist in pre-trained recommendation in a new form, while the security of pre-trained recommendation model is still unexplored, which may threaten its widely practical applications. In this study, we propose a novel framework for backdoor attacking in pre-trained recommendation. We demonstrate the provider of the pre-trained model can easily insert a backdoor in pre-training, thereby increasing the exposure rates of target items to target user groups. Specifically, we design two novel and effective backdoor attacks: basic replacement and prompt-enhanced, under various recommendation pre-training usage scenarios. Experimental results on real-world datasets show that our proposed attack strategies significantly improve the exposure rates of target items to target users by hundreds of times in comparison to the clean model.|最近，一系列的先驱研究已经显示了预训练模型在顺序推荐中的效力，阐明了为不同的下游推荐任务建立一个全知的统一预训练推荐模型的途径。尽管取得了这些进展，但传统推荐系统的脆弱性还存在于新形式的预训练推荐中，而预训练推荐模型的安全性仍未得到探索，这可能威胁到其广泛的实际应用。在这项研究中，我们提出了一个新的框架，用于后门攻击的预训练推荐。我们证明预训练模型的提供者可以很容易地在预训练中插入一个后门，从而增加目标项目对目标用户群的暴露率。具体来说，我们设计了两种新颖而有效的后门攻击: 基本替换和提示增强，在不同的推荐预训练使用场景下。在实际数据集上的实验结果表明，与清洁模型相比，我们提出的攻击策略显著提高了目标项对目标用户的暴露率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Attacking+Pre-trained+Recommendation)|0|
|[Always Strengthen Your Strengths: A Drift-Aware Incremental Learning Framework for CTR Prediction](https://doi.org/10.1145/3539618.3591948)|Congcong Liu, Fei Teng, Xiwei Zhao, Zhangang Lin, Jinghe Hu, Jingping Shao|JD.COM, Beijing, China|Click-through rate (CTR) prediction is of great importance in recommendation systems and online advertising platforms. When served in industrial scenarios, the user-generated data observed by the CTR model typically arrives as a stream. Streaming data has the characteristic that the underlying distribution drifts over time and may recur. This can lead to catastrophic forgetting if the model simply adapts to new data distribution all the time. Also, it's inefficient to relearn distribution that has been occurred. Due to memory constraints and diversity of data distributions in large-scale industrial applications, conventional strategies for catastrophic forgetting such as replay, parameter isolation, and knowledge distillation are difficult to be deployed. In this work, we design a novel drift-aware incremental learning framework based on ensemble learning to address catastrophic forgetting in CTR prediction. With explicit error-based drift detection on streaming data, the framework further strengthens well-adapted ensembles and freezes ensembles that do not match the input distribution avoiding catastrophic interference. Both evaluations on offline experiments and A/B test shows that our method outperforms all baselines considered.|在推荐系统和在线广告平台中，点进率预测非常重要。在工业场景中服务时，CTR 模型观察到的用户生成的数据通常以流的形式到达。流数据的特征是底层分布随时间漂移并可能重复出现。如果模型只是一直适应新的数据分布，这可能导致灾难性遗忘。此外，重新学习已经发生的分发是低效的。在大规模工业应用中，由于存储约束和数据分布的多样性，传统的灾难遗忘策略如重播、参数隔离和知识提取等难以实现。在这项工作中，我们设计了一个新的基于在线机机器学习的漂移感知集成学习框架，以解决 ctrr 预测中的灾难性遗忘问题。通过对流式数据进行明确的基于错误的漂移检测，该框架进一步强化了适应性良好的集合，并冻结了与输入分布不匹配的集合，以避免出现灾难性干扰。对离线实验和 A/B 测试的评估都表明，我们的方法优于所考虑的所有基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Always+Strengthen+Your+Strengths:+A+Drift-Aware+Incremental+Learning+Framework+for+CTR+Prediction)|0|
|[Attention-guided Multi-step Fusion: A Hierarchical Fusion Network for Multimodal Recommendation](https://doi.org/10.1145/3539618.3591950)|Yan Zhou, Jie Guo, Hao Sun, Bin Song, Fei Richard Yu|Shenzhen University, Shenzhen, China; Xidian University, Xi'an, China|The main idea of multimodal recommendation is the rational utilization of the item's multimodal information to improve the recommendation performance. Previous works directly integrate item multimodal features with item ID embeddings, ignoring the inherent semantic relations contained in the multimodal features. In this paper, we propose a novel and effective aTtention-guided Multi-step FUsion Network for multimodal recommendation, named TMFUN. Specifically, our model first constructs modality feature graph and item feature graph to model the latent item-item semantic structures. Then, we use the attention module to identify inherent connections between user-item interaction data and multimodal data, evaluate the impact of multimodal data on different interactions, and achieve early-step fusion of item features. Furthermore, our model optimizes item representation through the attention-guided multi-step fusion strategy and contrastive learning to improve recommendation performance. The extensive experiments on three real-world datasets show that our model has superior performance compared to the state-of-the-art models.|多通道推荐的主要思想是合理利用项目的多通道信息来提高推荐性能。以往的研究直接将项目多模态特征与项目 ID 嵌入相结合，忽略了项目多模态特征所包含的内在语义关系。在本文中，我们提出了一种新颖而有效的注意力引导多步融合网络 TMFUN。具体来说，我们的模型首先构造情态特征图和项目特征图，对潜在的项目-项目语义结构进行建模。然后，利用注意模块识别用户项目交互数据与多模态数据之间的内在联系，评估多模态数据对不同交互的影响，实现项目特征的早期融合。此外，我们的模型通过注意引导的多步融合策略和对比学习优化项目表示，以提高推荐性能。在三个真实世界数据集上的大量实验表明，我们的模型比最先进的模型具有更好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Attention-guided+Multi-step+Fusion:+A+Hierarchical+Fusion+Network+for+Multimodal+Recommendation)|0|
|[Benchmarking Middle-Trained Language Models for Neural Search](https://doi.org/10.1145/3539618.3591956)|Hervé Déjean, Stéphane Clinchant, Carlos Lassance, Simon Lupart, Thibault Formal|Naver Labs Europe, Meylan, France|Middle training methods aim to bridge the gap between the Masked Language Model (MLM) pre-training and the final finetuning for retrieval. Recent models such as CoCondenser, RetroMAE, and LexMAE argue that the MLM task is not sufficient enough to pre-train a transformer network for retrieval and hence propose various tasks to do so. Intrigued by those novel methods, we noticed that all these models used different finetuning protocols, making it hard to assess the benefits of middle training. We propose in this paper a benchmark of CoCondenser, RetroMAE, and LexMAE, under the same finetuning conditions. We compare both dense and sparse approaches under various finetuning protocols and middle training on different collections (MS MARCO, Wikipedia or Tripclick). We use additional middle training baselines, such as a standard MLM finetuning on the retrieval collection, optionally augmented by a CLS predicting the passage term frequency. For the sparse approach, our study reveals that there is almost no statistical difference between those methods: the more effective the finetuning procedure is, the less difference there is between those models. For the dense approach, RetroMAE using MS MARCO as middle-training collection shows excellent results in almost all the settings. Finally, we show that middle training on the retrieval collection, thus adapting the language model to it, is a critical factor. Overall, a better experimental setup should be adopted to evaluate middle training methods. Code available at https://github.com/naver/splade/tree/benchmarch-SIGIR23|中间训练方法旨在弥补蒙版语言模型(MLM)预训练和检索的最终微调之间的差距。最近的一些模型，如 CoCondenser，RotMAE 和 LexMAE 认为传销任务不足以预先训练一个变压器网络进行检索，因此提出了各种各样的任务来这样做。被这些新奇的方法所吸引，我们注意到所有这些模型使用不同的微调协议，使得评估中间训练的好处变得困难。在本文中，我们提出了一个基准的协同凝聚器，反向 MAE 和 LexMAE，在相同的微调条件下。我们比较了在各种微调协议和不同集合(MS MARCO，Wikipedia 或 Tripclick)的中间培训下的密集和稀疏方法。我们使用额外的中间训练基线，例如在检索集合上的标准 MLM 微调，可选地通过预测通过项频率的 CLS 增强。对于稀疏方法，我们的研究表明，这些方法之间几乎没有统计上的差异: 微调过程越有效，这些模型之间的差异就越小。对于密集的方法，使用 MS MARCO 作为中间训练收集在几乎所有的设置中都显示出优异的结果。最后，我们表明，中间训练的检索集，从而使语言模型适应它，是一个关键因素。总的来说，应该采用更好的实验设置来评价中间训练方法。Https://github.com/naver/splade/tree/benchmarch-sigir23提供密码|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Benchmarking+Middle-Trained+Language+Models+for+Neural+Search)|0|
|[Computational Versus Perceived Popularity Miscalibration in Recommender Systems](https://doi.org/10.1145/3539618.3591964)|Oleg Lesota, Gustavo Escobedo, Yashar Deldjoo, Bruce Ferwerda, Simone Kopeinik, Elisabeth Lex, Navid Rekabsaz, Markus Schedl|Jönköping University, Jönköping, Sweden; Know-Center GmbH, Graz, Austria; Graz University of Technology, Graz, Austria; Johannes Kepler University Linz, Linz, Austria; Johannes Kepler University Linz and Linz Institute of Technology, Linz, Austria; Polytechnic University of Bari, Bari, Italy|Popularity bias in recommendation lists refers to over-representation of popular content and is a challenge for many recommendation algorithms. Previous research has suggested several offline metrics to quantify popularity bias, which commonly relate the popularity of items in users' recommendation lists to the popularity of items in their interaction history. Discrepancies between these two factors are referred to as popularity miscalibration. While popularity metrics provide a straightforward and well-defined means to measure popularity bias, it is unknown whether they actually reflect users' perception of popularity bias. To address this research gap, we conduct a crowd-sourced user study on Prolific, involving 56 participants, to (1) investigate whether the level of perceived popularity miscalibration differs between common recommendation algorithms, (2) assess the correlation between perceived popularity miscalibration and its corresponding quantification according to a common offline metric. We conduct our study in a well-defined and important domain, namely music recommendation using the standardized LFM-2b dataset, and quantify popularity miscalibration of five recommendation algorithms by utilizing Jensen-Shannon distance (JSD). Challenging the findings of previous studies, we observe that users generally do perceive significant differences in terms of popularity bias between algorithms if this bias is framed as popularity miscalibration. In addition, JSD correlates moderately with users' perception of popularity, but not with their perception of unpopularity.|推荐列表中的流行度偏差指的是流行内容的过度表现，是许多推荐算法面临的一个挑战。先前的研究已经提出了一些离线指标来量化流行偏差，这些指标通常将用户推荐列表中项目的流行程度与其交互历史中项目的流行程度联系起来。这两个因素之间的差异被称为流行度误差。虽然流行指标提供了一个直接和明确的方法来衡量流行偏见，但它们是否真正反映了用户对流行偏见的感知尚不清楚。为了解决这一研究差距，我们对 Prolific 进行了一项涉及56名参与者的众包用户研究，以(1)调查常见推荐算法之间的感知流行度误差水平是否不同，(2)根据常见的离线度量评估感知流行度误差与其相应的量化之间的相关性。我们在一个定义明确的重要领域进行研究，即使用标准化的 LFM-2b 数据集进行音乐推荐，并通过使用 Jensen-Shannon 距离(JSD)量化五种推荐算法的流行性误差。挑战以往的研究结果，我们观察到，如果这种偏差被定义为流行性误差，用户通常会感觉到算法之间的流行性偏差方面的显著差异。此外，JSD 与用户的受欢迎程度有一定的相关性，但与他们的不受欢迎程度无关。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Computational+Versus+Perceived+Popularity+Miscalibration+in+Recommender+Systems)|0|
|[Context-Aware Modeling via Simulated Exposure Page for CTR Prediction](https://doi.org/10.1145/3539618.3591967)|Xiang Li, Shuwei Chen, Jian Dong, Jin Zhang, Yongkang Wang, Xingxing Wang, Dong Wang|Meituan, Beijing, China|Click-through rate (CTR) prediction plays a crucial role in industrial recommendation and advertising systems, which generate and expose multiple items for each user request. Although the user's click action on an item will be affected by the other exposed items (called contextual items), current CTR prediction methods do not exploit this context because CTR prediction is performed before the contextual items are generated. This paper introduces a solution Contextual Items Simulation and Modeling (CISM) to tackle this limitation. Specifically, we propose a near-line Context Simulation Center to simulate exposure page without affecting online service latency, and an online Context Modeling Transformer to learn user-wise context from the simulated results w.r.t. the candidate item. In addition, knowledge distillation is introduced to further improve CTR prediction. Extensive experiments on both public and industrial datasets demonstrate the effectiveness of CISM. Currently, CISM has been deployed in the online display advertising system of Meituan Waimai, serving the main traffic.|在工业推荐和广告系统中，点进率(ctrl)预测起着至关重要的作用，它为每个用户请求生成和公开多个条目。虽然用户对一个项目的点击操作会受到其他公开项目(称为上下文项目)的影响，但是当前的 CTR 预测方法不会利用这个上下文，因为 CTR 预测是在生成上下文项目之前执行的。本文介绍了一种解决上下文项目模拟与建模(CISM)的方法来解决这一局限性。具体来说，我们提出了一个近线上下文模拟中心来模拟暴露页面而不影响在线服务延迟，以及一个在线上下文建模转换器来从模拟结果中学习用户明智的上下文。此外，引入知识提取技术，进一步改进了 CTR 预测。在公共和工业数据集上的大量实验证明了 CISM 的有效性。目前，CISM 已经部署在美团外卖的在线显示广告系统中，服务于主要流量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Context-Aware+Modeling+via+Simulated+Exposure+Page+for+CTR+Prediction)|0|
|[Contrastive Learning for Conversion Rate Prediction](https://doi.org/10.1145/3539618.3591968)|Wentao Ouyang, Rui Dong, Xiuwu Zhang, Chaofeng Guo, Jinmei Luo, Xiangzheng Liu, Yanlong Du|Alibaba Group, Beijing, China|Conversion rate (CVR) prediction plays an important role in advertising systems. Recently, supervised deep neural network-based models have shown promising performance in CVR prediction. However, they are data hungry and require an enormous amount of training data. In online advertising systems, although there are millions to billions of ads, users tend to click only a small set of them and to convert on an even smaller set. This data sparsity issue restricts the power of these deep models. In this paper, we propose the Contrastive Learning for CVR prediction (CL4CVR) framework. It associates the supervised CVR prediction task with a contrastive learning task, which can learn better data representations exploiting abundant unlabeled data and improve the CVR prediction performance. To tailor the contrastive learning task to the CVR prediction problem, we propose embedding masking (EM), rather than feature masking, to create two views of augmented samples. We also propose a false negative elimination (FNE) component to eliminate samples with the same feature as the anchor sample, to account for the natural property in user behavior data. We further propose a supervised positive inclusion (SPI) component to include additional positive samples for each anchor sample, in order to make full use of sparse but precious user conversion events. Experimental results on two real-world conversion datasets demonstrate the superior performance of CL4CVR. The source code is available at https://github.com/DongRuiHust/CL4CVR.|转化率(CVR)预测在广告系统中起着重要作用。近年来，基于监督深层神经网络的 CVR 预测模型在 CVR 预测中表现出了良好的性能。然而，它们需要大量的数据，并且需要大量的训练数据。在在线广告系统中，尽管有数百万到数十亿的广告，用户往往只点击其中的一小部分，然后转换成更小的一部分。这种数据稀疏问题限制了这些深度模型的能力。本文提出了 CVR 预测的对比学习(CL4CVR)框架。该方法将有监督的 CVR 预测任务与对比学习任务相结合，利用大量未标记数据学习更好的数据表示，提高 CVR 预测性能。为了使对比学习任务适用于 CVR 预测问题，我们提出了嵌入掩蔽(EM)方法，而不是特征掩蔽方法，来创建增广样本的两个视图。我们还提出了一种假阴性消除(FNE)组件来消除具有与锚样本相同特征的样本，以解释用户行为数据的自然属性。我们进一步提出了一个监督正包含(SPI)组件，为每个锚样本包含额外的正样本，以充分利用稀疏但宝贵的用户转换事件。在两个实际转换数据集上的实验结果表明，CL4CVR 具有较好的性能。源代码可在 https://github.com/dongruihust/cl4cvr 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contrastive+Learning+for+Conversion+Rate+Prediction)|0|
|[Curriculum Modeling the Dependence among Targets with Multi-task Learning for Financial Marketing](https://doi.org/10.1145/3539618.3591969)|Yunpeng Weng, Xing Tang, Liang Chen, Xiuqiang He|FiT, Tencent, Shenzhen, China|Multi-task learning for various real-world applications usually involves tasks with logical sequential dependence. For example, in online marketing, the cascade behavior pattern of $impression \rightarrow click \rightarrow conversion$ is usually modeled as multiple tasks in a multi-task manner, where the sequential dependence between tasks is simply connected with an explicitly defined function or implicitly transferred information in current works. These methods alleviate the data sparsity problem for long-path sequential tasks as the positive feedback becomes sparser along with the task sequence. However, the error accumulation and negative transfer will be a severe problem for downstream tasks. Especially, at the beginning stage of training, the optimization for parameters of former tasks is not converged yet, and thus the information transferred to downstream tasks is negative. In this paper, we propose a prior information merged model (\textbf{PIMM}), which explicitly models the logical dependence among tasks with a novel prior information merged (\textbf{PIM}) module for multiple sequential dependence task learning in a curriculum manner. Specifically, the PIM randomly selects the true label information or the prior task prediction with a soft sampling strategy to transfer to the downstream task during the training. Following an easy-to-difficult curriculum paradigm, we dynamically adjust the sampling probability to ensure that the downstream task will get the effective information along with the training. The offline experimental results on both public and product datasets verify that PIMM outperforms state-of-the-art baselines. Moreover, we deploy the PIMM in a large-scale FinTech platform, and the online experiments also demonstrate the effectiveness of PIMM.|实际应用中的多任务学习通常涉及逻辑顺序依赖的任务。例如，在网络营销中，右键点击右键转换的级联行为模式通常被建模为多任务的多任务方式，其中任务之间的顺序依赖只是与一个明确定义的功能或当前作品中隐式传递的信息相关联。这些方法缓解了长路顺序任务的数据稀疏性问题，因为正反馈随着任务顺序的减少而变得稀疏。然而，对于下游任务来说，错误积累和负迁移将是一个严重的问题。特别是在训练的初始阶段，前期任务的参数优化还没有收敛，因此传递给后期任务的信息是负的。本文提出了一种先验信息合并模型(textbf { PIMM }) ，该模型使用一种新的先验信息合并模型(textbf { PIM })对任务间的逻辑依赖关系进行显式建模，并以课程的形式进行多个顺序依赖任务的学习。具体来说，PIM 通过软抽样策略随机选择真实的标签信息或先前的任务预测，在训练过程中传递给下游任务。采用易难课程范式，动态调整抽样概率，确保下游任务在训练过程中获得有效信息。在公共数据集和产品数据集上的离线实验结果验证了 PIMM 的性能优于最先进的基线。在此基础上，将 PIMM 应用于大型金融技术平台，并通过在线实验验证了 PIMM 的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Curriculum+Modeling+the+Dependence+among+Targets+with+Multi-task+Learning+for+Financial+Marketing)|0|
|[Denoise to Protect: A Method to Robustify Visual Recommenders from Adversaries](https://doi.org/10.1145/3539618.3591971)|Felice Antonio Merra, Vito Walter Anelli, Tommaso Di Noia, Daniele Malitesta, Alberto Carlo Maria Mancino|Amazon, Berlin, Germany; Politecnico di Bari, Bari, Italy|While the integration of product images enhances the recommendation performance of visual-based recommender systems (VRSs), this can make the model vulnerable to adversaries that can produce noised images capable to alter the recommendation behavior. Recently, stronger and stronger adversarial attacks have emerged to raise awareness of these risks; however, effective defense methods are still an urgent open challenge. In this work, we propose "Adversarial Image Denoiser" (AiD), a novel defense method that cleans up the item images by malicious perturbations. In particular, we design a training strategy whose denoising objective is to minimize both the visual differences between clean and adversarial images and preserve the ranking performance in authentic settings. We perform experiments to evaluate the efficacy of AiD using three state-of-the-art adversarial attacks mounted against standard VRSs. Code and datasets at https://github.com/sisinflab/Denoise-to-protect-VRS.|虽然产品图像的集成增强了基于可视化的推荐系统(VRS)的推荐性能，但是这会使模型容易受到能够产生噪声图像的对手的攻击，而这些噪声图像能够改变推荐行为。近年来，越来越强大的对抗性攻击已经出现，以提高人们对这些风险的认识，但是，有效的防御方法仍然是一个迫切的公开挑战。在这项工作中，我们提出了“对抗性图像去噪器”(AiD) ，一种新的防御方法，清除项目图像的恶意扰动。特别地，我们设计了一个训练策略，其去噪目标是最小化干净图像和对手图像之间的视觉差异，并保持在真实环境中的排名性能。我们进行实验来评估艾滋病的功效使用三个国家的最先进的敌对攻击安装对标准的 VRS。Https://github.com/sisinflab/denoise-to-protect-vrs 代码和数据集。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Denoise+to+Protect:+A+Method+to+Robustify+Visual+Recommenders+from+Adversaries)|0|
|[Edge-cloud Collaborative Learning with Federated and Centralized Features](https://doi.org/10.1145/3539618.3591976)|Zexi Li, Qunwei Li, Yi Zhou, Wenliang Zhong, Guannan Zhang, Chao Wu|Ant Group, Hangzhou, China; Zhejiang University, Hangzhou, China; The University of Utah, Salt Lake City, UT, USA|Federated learning (FL) is a popular way of edge computing that doesn't compromise users' privacy. Current FL paradigms assume that data only resides on the edge, while cloud servers only perform model averaging. However, in real-life situations such as recommender systems, the cloud server has the ability to store historical and interactive features. In this paper, our proposed Edge-Cloud Collaborative Knowledge Transfer Framework (ECCT) bridges the gap between the edge and cloud, enabling bi-directional knowledge transfer between both, sharing feature embeddings and prediction logits. ECCT consolidates various benefits, including enhancing personalization, enabling model heterogeneity, tolerating training asynchronization, and relieving communication burdens. Extensive experiments on public and industrial datasets demonstrate ECCT's effectiveness and potential for use in academia and industry.|联邦学习(FL)是一种流行的边缘计算方法，它不会损害用户的隐私。当前的 FL 范例假设数据只停留在边缘，而云服务器只执行模型平均。但是，在推荐系统等实际情况下，云服务器具有存储历史和交互特性的能力。本文提出的边缘-云协同知识转移框架(ECCT)弥补了边缘和云之间的差距，实现了边缘和云之间的双向知识转移，共享特征嵌入和预测逻辑。ECCT 巩固了各种好处，包括增强个性化、支持模型异构性、容忍培训异步以及减轻通信负担。在公共和工业数据集上的大量实验证明了 ECCT 的有效性和在学术界和工业界使用的潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Edge-cloud+Collaborative+Learning+with+Federated+and+Centralized+Features)|0|
|[Evaluating Cross-modal Generative Models Using Retrieval Task](https://doi.org/10.1145/3539618.3591979)|Shivangi Bithel, Srikanta Bedathur|IIT Delhi, New Delhi, India|Generative models have taken the world by storm -- image generative models such as Stable Diffusion and DALL-E generate photo-realistic images, whereas image captioning models such as BLIP, GIT, ClipCap, and ViT-GPT2 generate descriptive and informative captions. While it may be true that these models produce remarkable results, their systematic evaluation is missing, making it hard to advance the research further. Currently, heuristic metrics such as the Inception Score and the Fréchet Inception Distance are the most prevalent metrics for the image generation task, while BLEU, CIDEr, SPICE, METEOR, BERTScore, and CLIPScore are common for the image captioning task. Unfortunately, these are poorly interpretable and are not based on the solid user-behavior model that the Information Retrieval community has worked towards. In this paper, we present a novel cross-modal retrieval framework to evaluate the effectiveness of cross-modal (image-to-text and text-to-image) generative models using reference text and images. We propose the use of scoring models based on user-behavior, such as Normalized Discounted Cumulative Gain (nDCG'@K ) and Rank-Biased Precision (RBP'@K) adjusted for incomplete judgments. Experiments using ECCV Caption and Flickr8k-EXPERTS benchmark datasets demonstrate the effectiveness of various image captioning and image generation models for the proposed retrieval task. Results also indicate that the nDCG'@K and RBP'@K scores are consistent with heuristics-driven metrics, excluding CLIPScore, in model selection.|生成模型已经席卷世界——图像生成模型，如稳定扩散和 DALL-E 生成逼真的图像，而图像字幕模型，如 BLIP、 GIT、 ClipCap 和 ViT-GPT2生成描述性和信息性的字幕。尽管这些模型确实产生了显著的结果，但它们缺乏系统的评估，使得进一步的研究难以推进。目前，启发式指标，如先启评分和弗雷谢先启距离是最普遍的图像生成任务的指标，而 BLEU，CIDEr，SPICE，METEOR，BERTScore 和 CLIPScore 是常见的图像字幕任务。不幸的是，这些都是很难解释的，而且不是基于信息检索社区已经努力建立的可靠的用户行为模型。在本文中，我们提出了一个新的跨模态检索框架来评估跨模态(图像到文本和文本到图像)生成模型的有效性使用参考文本和图像。我们建议使用基于用户行为的评分模型，如对不完全判断进行调整的标准化贴现累积增益(nDCG’@K)和秩偏差精度(RBP’@K)。使用 ECCV 字幕和 Flickr8k-EXPERTS 基准数据集的实验证明了各种图像字幕和图像生成模型对提出的检索任务的有效性。结果还表明，在模型选择中，nDCG’@K 和 RBP’@K 得分与启发式驱动的指标(不包括 CLIPScore)一致。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Evaluating+Cross-modal+Generative+Models+Using+Retrieval+Task)|0|
|[SLIM: Sparsified Late Interaction for Multi-Vector Retrieval with Inverted Indexes](https://doi.org/10.1145/3539618.3591977)|Minghan Li, ShengChieh Lin, Xueguang Ma, Jimmy Lin|University of Waterloo, Waterloo, ON, Canada|This paper introduces a method called Sparsified Late Interaction for Multi-vector retrieval with inverted indexes (SLIM). Although multi-vector models have demonstrated their effectiveness in various information retrieval tasks, most of their pipelines require custom optimization to be efficient in both time and space. Among them, ColBERT is probably the most established method which is based on the late interaction of contextualized token embeddings of pre-trained language models. Unlike ColBERT where all its token embeddings are low-dimensional and dense, SLIM projects each token embedding into a high-dimensional, sparse lexical space before performing late interaction. In practice, we further propose to approximate SLIM using the lower- and upper-bound of the late interaction to reduce latency and storage. In this way, the sparse outputs can be easily incorporated into an inverted search index and are fully compatible with off-the-shelf search tools such as Pyserini and Elasticsearch. SLIM has competitive accuracy on information retrieval benchmarks such as MS MARCO Passages and BEIR compared to ColBERT while being much smaller and faster on CPUs. Source code and data will be available at https://github.com/castorini/pyserini/blob/master/docs/experiments-slim.md.|本文介绍了一种具有倒排索引的多向量检索方法(SLIM)。尽管多向量模型已经证明了它们在各种信息检索任务中的有效性，但是它们的大多数流水线都需要定制的优化，以便在时间和空间上都有效率。其中，ColBERT 可能是最成熟的方法，它是基于预训练语言模型的上下文化标记嵌入的后期交互。与 ColBERT 中的所有令牌嵌入都是低维和密集的不同，SLIM 在执行后期交互之前将每个令牌嵌入到一个高维、稀疏的词法空间中。在实践中，我们进一步提出使用后期交互的上下界来近似 SLIM，以减少延迟和存储。通过这种方式，稀疏输出可以很容易地合并到反向搜索索引中，并且与 Pyserini 和 Elasticsearch 等现成的搜索工具完全兼容。与 ColBERT 相比，SLIM 在微软 MARCO Passages 和 BEIR 等信息检索基准测试上具有竞争性的准确性，而在 CPU 上则更小、更快。源代码和数据将在 https://github.com/castorini/pyserini/blob/master/docs/experiments-slim.md 提供。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SLIM:+Sparsified+Late+Interaction+for+Multi-Vector+Retrieval+with+Inverted+Indexes)|0|
|[Forget Me Now: Fast and Exact Unlearning in Neighborhood-based Recommendation](https://doi.org/10.1145/3539618.3591989)|Sebastian Schelter, Mozhdeh Ariannezhad, Maarten de Rijke|AIRLab, University of Amsterdam, Amsterdam, Netherlands; University of Amsterdam, Amsterdam, Netherlands|Modern search and recommendation systems are optimized using logged interaction data. There is increasing societal pressure to enable users of such systems to have some of their data deleted from those systems. This paper focuses on "unlearning" such user data from neighborhood-based recommendation models on sparse, high-dimensional datasets. We present caboose, a custom top-k index for such models, which enables fast and exact deletion of user interactions. We experimentally find that caboose provides competitive index building times, makes sub-second unlearning possible (even for a large index built from one million users and 256 million interactions), and, when integrated into three state-of-the-art next-basket recommendation models, allows users to effectively adjust their predictions to remove sensitive items.|现代搜索和推荐系统使用日志交互数据进行优化。越来越多的社会压力要求这些系统的用户从这些系统中删除一些数据。本文主要研究在稀疏、高维数据集上从基于邻域的推荐模型中“去除”这类用户数据。我们提出的守护，这种模型的自定义 top-k 索引，它使快速和准确的删除用户交互。我们通过实验发现，caboose 提供了具有竞争力的索引构建时间，使得亚秒级的忘却成为可能(即使是对于一个由一百万用户和2.56亿互动构建的大型索引) ，并且，当整合到三个最先进的下一篮子推荐模型中时，允许用户有效地调整他们的预测以删除敏感项目。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Forget+Me+Now:+Fast+and+Exact+Unlearning+in+Neighborhood-based+Recommendation)|0|
|[Gradient Coordination for Quantifying and Maximizing Knowledge Transference in Multi-Task Learning](https://doi.org/10.1145/3539618.3591993)|Xuanhua Yang, Jianxin Zhao, Shaoguo Liu, Liang Wang, Bo Zheng|Alibaba Group, Beijing, China|Multi-task learning (MTL) has been widely applied in online advertising and recommender systems. To address the negative transfer issue, recent studies have proposed optimization methods that thoroughly focus on the gradient alignment of directions or magnitudes. However, since prior study has proven that both general and specific knowledge exist in the limited shared capacity, overemphasizing on gradient alignment may crowd out task-specific knowledge, and vice versa. In this paper, we propose a transference-driven approach CoGrad that adaptively maximizes knowledge transference via Coordinated Gradient modification. We explicitly quantify the transference as loss reduction from one task to another, and then derive an auxiliary gradient from optimizing it. We perform the optimization by incorporating this gradient into original task gradients, making the model automatically maximize inter-task transfer and minimize individual losses. Thus, CoGrad can harmonize between general and specific knowledge to boost overall performance. Besides, we introduce an efficient approximation of the Hessian matrix, making CoGrad computationally efficient and simple to implement. Both offline and online experiments verify that CoGrad significantly outperforms previous methods.|多任务学习(MTL)在网络广告和推荐系统中得到了广泛的应用。为了解决负迁移问题，最近的研究提出了一些优化方法，这些方法完全集中在方向或大小的梯度对齐上。然而，由于先前的研究已经证明，一般知识和具体知识都存在于有限的共享能力中，过分强调梯度调整可能会排挤任务特定的知识，反之亦然。本文提出了一种基于转移驱动的 CoGrad 方法，该方法通过协调梯度修正自适应地最大化知识转移。我们明确地量化转移作为损失减少从一个任务到另一个，然后推导出一个辅助梯度优化它。我们通过将梯度合并到原始任务梯度中来执行优化，使模型自动最大化任务间的转移并最小化个体损失。因此，CoGrad 可以协调一般知识和具体知识，以提高整体性能。此外，我们还引入了 Hessian 矩阵的一种有效逼近，使 CoGrad 的计算变得简单有效。线下和线上实验都证实了 CoGrad 显著优于以前的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Gradient+Coordination+for+Quantifying+and+Maximizing+Knowledge+Transference+in+Multi-Task+Learning)|0|
|[Model-free Reinforcement Learning with Stochastic Reward Stabilization for Recommender Systems](https://doi.org/10.1145/3539618.3592022)|Tianchi Cai, Shenliao Bao, Jiyan Jiang, Shiji Zhou, Wenpeng Zhang, Lihong Gu, Jinjie Gu, Guannan Zhang|Ant Group, Hangzhou, China; Ant Group, Beijing, China|Model-free RL-based recommender systems have recently received increasing research attention due to their capability to handle partial feedback and long-term rewards. However, most existing research has ignored a critical feature in recommender systems: one user's feedback on the same item at different times is random. The stochastic rewards property essentially differs from that in classic RL scenarios with deterministic rewards, which makes RL-based recommender systems much more challenging. In this paper, we first demonstrate in a simulator environment where using direct stochastic feedback results in a significant drop in performance. Then to handle the stochastic feedback more efficiently, we design two stochastic reward stabilization frameworks that replace the direct stochastic feedback with that learned by a supervised model. Both frameworks are model-agnostic, i.e., they can effectively utilize various supervised models. We demonstrate the superiority of the proposed frameworks over different RL-based recommendation baselines with extensive experiments on a recommendation simulator as well as an industrial-level recommender system.|基于无模型 RL 的推荐系统由于能够处理部分反馈和长期奖励，近年来受到越来越多的研究关注。然而，大多数现有的研究忽略了推荐系统的一个关键特征: 一个用户在不同时间对同一条目的反馈是随机的。随机奖励属性与传统的确定性奖励的 RL 场景不同，这使得基于 RL 的推荐系统更具挑战性。在本文中，我们首先在一个模拟器环境中演示，使用直接随机反馈会导致性能的显著下降。然后，为了更有效地处理随机反馈，我们设计了两个随机报酬稳定化框架，用监督模型学习的随机反馈代替直接的随机反馈。这两种框架都是模型无关的，也就是说，它们可以有效地利用各种监督模型。我们在推荐模拟器和工业级推荐系统上进行了广泛的实验，证明了所提出的框架相对于不同的基于 RL 的推荐基线的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Model-free+Reinforcement+Learning+with+Stochastic+Reward+Stabilization+for+Recommender+Systems)|0|
|[Multi-Grained Topological Pre-Training of Language Models in Sponsored Search](https://doi.org/10.1145/3539618.3592024)|Zhoujin Tian, Chaozhuo Li, Zhiqiang Zuo, Zengxuan Wen, Xinyue Hu, Xiao Han, Haizhen Huang, Senzhang Wang, Weiwei Deng, Xing Xie, Qi Zhang|Microsoft, Beijing, China; Central South University, Changsha, China; Microsoft Research Asia, Beijing, China|Relevance models measure the semantic closeness between queries and the candidate ads, widely recognized as the nucleus of sponsored search systems. Conventional relevance models solely rely on the textual data within the queries and ads, whose performance is hindered by the scarce semantic information in these short texts. Recently, user behavior graphs have been incorporated to provide complementary information beyond pure textual semantics.Despite the promising performance, behavior-enhanced models suffer from exhausting resource costs due to the extra computations introduced by explicit topological aggregations. In this paper, we propose a novel Multi-Grained Topological Pre-Training paradigm, MGTLM, to teach language models to understand multi-grained topological information in behavior graphs, which contributes to eliminating explicit graph aggregations and avoiding information loss. Extensive experimental results over online and offline settings demonstrate the superiority of our proposal.|关联模型测量查询和候选广告之间的语义接近度，被广泛认为是赞助商搜索系统的核心。传统的关联模型仅仅依赖于查询和广告中的文本数据，而这些数据的表现受到这些短文本中缺乏语义信息的限制。最近，用户行为图被用来提供超越纯文本语义的互补信息。尽管性能良好，但由于显式拓扑聚合引入了额外的计算，行为增强模型会耗尽资源成本。本文提出了一种新的多粒度拓扑预训练模型 MGTLM，用于教语言模型理解行为图中的多粒度拓扑信息，有助于消除显性图集合，避免信息丢失。在在线和离线环境下的大量实验结果证明了我们方案的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Grained+Topological+Pre-Training+of+Language+Models+in+Sponsored+Search)|0|
|[Multi-grained Representation Learning for Cross-modal Retrieval](https://doi.org/10.1145/3539618.3592025)|Shengwei Zhao, Linhai Xu, Yuying Liu, Shaoyi Du|Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Xian, China|The purpose of audio-text retrieval is to learn a cross-modal similarity function between audio and text, enabling a given audio/text to find similar text/audio from a candidate set. Recent audio-text retrieval models aggregate multi-modal features into a single-grained representation. However, single-grained representation is difficult to solve the situation that an audio is described by multiple texts of different granularity levels, because the association pattern between audio and text is complex. Therefore, we propose an adaptive aggregation strategy to automatically find the optimal pool function to aggregate the features into a comprehensive representation, so as to learn valuable multi-grained representation. And multi-grained comparative learning is carried out in order to focus on the complex correlation between audio and text in different granularity. Meanwhile, text-guided token interaction is used to reduce the impact of redundant audio clips. We evaluated our proposed method on two audio-text retrieval benchmark datasets of Audiocaps and Clotho, achieving the state-of-the-art results in text-to-audio and audio-to-text retrieval. Our findings emphasize the importance of learning multi-modal multi-grained representation.|音频文本检索的目的是学习音频和文本之间的跨模式相似度函数，使给定的音频/文本能够从候选集中找到相似的文本/音频。最近的音频文本检索模型将多模态特征聚合为单粒度表示。然而，由于音频和文本之间的关联模式非常复杂，单粒度表示很难解决由不同粒度级别的多个文本描述音频的问题。为此，提出了一种自适应聚合策略，自动寻找最优池函数，将特征聚合成一个综合表示，从而学习有价值的多粒度表示。针对不同粒度的音频和文本之间复杂的相关性，进行了多粒度比较学习。同时，采用文本引导的令牌交互方式来减少冗余音频片段的影响。我们在 Audiocaps 和 Clotho 两个音频文本检索基准数据集上对所提出的方法进行了评估，在文本到音频和音频到文本的检索中取得了最先进的结果。我们的研究结果强调了学习多模态多粒度表示的重要性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-grained+Representation+Learning+for+Cross-modal+Retrieval)|0|
|[On the Effects of Regional Spelling Conventions in Retrieval Models](https://doi.org/10.1145/3539618.3592030)|Andreas Chari, Sean MacAvaney, Iadh Ounis|University of Glasgow, Glasgow, United Kingdom|One advantage of neural ranking models is that they are meant to generalise well in situations of synonymity i.e. where two words have similar or identical meanings. In this paper, we investigate and quantify how well various ranking models perform in a clear-cut case of synonymity: when words are simply expressed in different surface forms due to regional differences in spelling conventions (e.g., color vs colour). We first explore the prevalence of American and British English spelling conventions in datasets used for the pre-training, training and evaluation of neural retrieval methods, and find that American spelling conventions are far more prevalent. Despite these biases in the training data, we find that retrieval models often generalise well in this case of synonymity. We explore the effect of document spelling normalisation in retrieval and observe that all models are affected by normalising the document's spelling. While they all experience a drop in performance when normalised to a different spelling convention than that of the query, we observe varied behaviour when the document is normalised to share the query spelling convention: lexical models show improvements, dense retrievers remain unaffected, and re-rankers exhibit contradictory behaviour.|神经排序模型的一个优点是，它们可以很好地概括同义词的情况，即两个词有相似或相同的意思。在本文中，我们调查和量化各种排名模型在一个明确的同义性情况下表现如何: 当单词只是表示在不同的表面形式，由于拼写惯例的区域差异(例如，颜色与颜色)。我们首先探讨了美国和英国英语拼写惯例在用于神经检索方法的预训练、培训和评估的数据集中的普遍性，发现美国的拼写惯例更为普遍。尽管在训练数据中存在这些偏差，我们发现在这种同义性的情况下，检索模型往往能够很好地推广。我们探讨了文档拼写规范化在检索中的作用，并观察到所有模型都受到文档拼写规范化的影响。虽然他们都经历了性能下降时，规范化的不同拼写约定比查询，我们观察到不同的行为时，文档规范化，共享查询拼写约定: 词汇模型显示改进，密集检索仍然没有受到影响，重新排序表现出矛盾的行为。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+the+Effects+of+Regional+Spelling+Conventions+in+Retrieval+Models)|0|
|[Patterns of Gender-Specializing Query Reformulation](https://doi.org/10.1145/3539618.3592034)|Amifa Raj, Bhaskar Mitra, Nick Craswell, Michael D. Ekstrand|Boise State University, Boise, USA; Microsoft Research, Montreal, USA; Microsoft, Redmond, USA|Users of search systems often reformulate their queries by adding query terms to reflect their evolving information need or to more precisely express their information need when the system fails to surface relevant content. Analyzing these query reformulations can inform us about both system and user behavior. In this work, we study a special category of query reformulations that involve specifying demographic group attributes, such as gender, as part of the reformulated query (e.g., "olympic 2021 soccer results" to "olympic 2021 women's soccer results"). There are many ways a query, the search results, and a demographic attribute such as gender may relate, leading us to hypothesize different causes for these reformulation patterns, such as under-representation on the original result page or based on the linguistic theory of markedness. This paper reports on an observational study of gender-specializing query reformulations -- their contexts and effects -- as a lens on the relationship between system results and gender, based on large-scale search log data from Bing. We find that these reformulations sometimes correct for and other times reinforce gender representation on the original result page, but typically yield better access to the ultimately-selected results. The prevalence of these reformulations -- and which gender they skew towards -- differ by topical context. However, we do not find evidence that either group under-representation or markedness alone adequately explains these reformulations. We hope that future research will use such reformulations as a probe for deeper investigation into gender (and other demographic) representation on the search result page.|搜索系统的用户往往通过添加查询词语来重新表述其查询，以反映其不断变化的信息需求，或在系统未能显示相关内容时更准确地表达其信息需求。分析这些查询重构可以让我们了解系统和用户行为。在这项工作中，我们研究了一个特殊类别的查询重构，涉及指定人口组属性，如性别，作为重构查询的一部分(例如，“2021年奥运会足球结果”到“2021年奥运会女子足球结果”)。查询、搜索结果和性别等人口统计学特征可能有多种关联，这导致我们假设这些重构模式的不同原因，例如原始结果页面上的表示不足或基于标记性语言学理论。这篇论文报道了一系列针对性别的查询观察性研究——它们的背景和效果——作为系统结果和性别之间关系的一个透镜，基于 Bing 的大规模搜索日志数据。我们发现，这些重新编排有时正确，其他时候加强原始结果页面上的性别代表性，但通常产生更好的访问最终选定的结果。这些重新拟订的普遍程度——以及它们倾向于哪一性别——因主题背景的不同而有所不同。然而，我们没有发现证据表明，无论是集团代表性不足或标记单独充分解释这些重新编排。我们希望未来的研究将使用这样的重新编排作为一种探索，对搜索结果页面上的性别(和其他人口统计学)表征进行更深入的调查。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Patterns+of+Gender-Specializing+Query+Reformulation)|0|
|[PiTL: Cross-modal Retrieval with Weakly-supervised Vision-language Pre-training via Prompting](https://doi.org/10.1145/3539618.3592038)|Zixin Guo, TzuJui Julius Wang, Selen Pehlivan, Abduljalil Radman, Jorma Laaksonen|Aalto University, Espoo, Finland|Vision-language (VL) Pre-training (VLP) has shown to well generalize VL models over a wide range of VL downstream tasks, especially for cross-modal retrieval. However, it hinges on a huge amount of image-text pairs, which requires tedious and costly curation. On the contrary, weakly-supervised VLP (W-VLP) explores means with object tags generated by a pre-trained object detector (OD) from images. Yet, they still require paired information, i.e. images and object-level annotations, as supervision to train an OD. To further reduce the amount of supervision, we propose Prompts-in-The-Loop (PiTL) that prompts knowledge from large language models (LLMs) to describe images. Concretely, given a category label of an image, e.g. refinery, the knowledge, e.g. a refinery could be seen with large storage tanks, pipework, and ..., extracted by LLMs is used as the language counterpart. The knowledge supplements, e.g. the common relations among entities most likely appearing in a scene. We create IN14K, a new VL dataset of 9M images and 1M descriptions of 14K categories from ImageNet21K with PiTL. Empirically, the VL models pre-trained with PiTL-generated pairs are strongly favored over other W-VLP works on image-to-text (I2T) and text-to-image (T2I) retrieval tasks, with less supervision. The results reveal the effectiveness of PiTL-generated pairs for VLP.|视觉语言(VL)预训练(VLP)已被证明可以在 VL 下游任务的广泛范围内很好地推广 VL 模型，特别是在跨模式检索中。然而，它依赖于大量的图像-文本对，这需要乏味和昂贵的管理。相反，弱监督 VLP (W-VLP)利用预先训练的目标检测器(OD)从图像中生成目标标记来探索目标检测方法。然而，他们仍然需要成对的信息，即图像和对象级注释，作为培训 OD 的监督。为了进一步减少监督的数量，我们提出了循环提示(Prompts-in-The-Loop，PiTL) ，它提示来自大型语言模型(LLM)的知识来描述图像。具体来说，给定一个图像的类别标签，例如精炼厂，知识，例如精炼厂可以看到大型存储罐，管道系统，和... ，由 LLM 提取被用作语言对应物。知识的补充，例如最可能出现在场景中的实体之间的共同关系。我们使用 PiTL 从 ImageNet21K 创建了 IN14K，这是一个包含9M 图像和1M 描述14K 类别的新的 VL 数据集。经验表明，在图像到文本(I2T)和文本到图像(T2I)的检索任务中，预先使用 PiTL 生成对训练的 VL 模型比其他 W-VLP 模型更受青睐，而且监督更少。结果表明，PiTL 生成的对对于 VLP 是有效的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PiTL:+Cross-modal+Retrieval+with+Weakly-supervised+Vision-language+Pre-training+via+Prompting)|0|
|[Query-specific Variable Depth Pooling via Query Performance Prediction](https://doi.org/10.1145/3539618.3592046)|Debasis Ganguly, Emine Yilmaz|University of Glasgow, Glasgow, United Kingdom; University College London, London, United Kingdom|Due to the massive size of test collections, a standard practice in IR evaluation is to construct a 'pool' of candidate relevant documents comprised of the top-k documents retrieved by a wide range of different retrieval systems - a process called depth-k pooling. A standard practice is to set the depth (k) to a constant value for each query constituting the benchmark set. However, in this paper we argue that the annotation effort can be substantially reduced if the depth of the pool is made a variable quantity for each query, the rationale being that the number of documents relevant to the information need can widely vary across queries. Our hypothesis is that a lower depth for the former class of queries and a higher depth for the latter can potentially reduce the annotation effort without a significant change in retrieval effectiveness evaluation. We make use of standard query performance prediction (QPP) techniques to estimate the number of potentially relevant documents for each query, which is then used to determine the depth of the pool. Our experiments conducted on standard test collections demonstrate that this proposed method of employing query-specific variable depths is able to adequately reflect the relative effectiveness of IR systems with a substantially smaller annotation effort.|由于测试集的规模庞大，IR 评估的一个标准实践是构建一个由各种不同检索系统检索到的 top-k 文档组成的候选相关文档“池”——这个过程被称为深度 k 池。标准实践是将构成基准集的每个查询的深度(k)设置为常量值。然而，在本文中，我们认为，如果对于每个查询，池的深度都是可变的，那么注释工作可以大大减少，其基本原理是，与信息需求相关的文档数量可以在不同的查询之间有很大的差异。我们的假设是，对于前一类查询，较低的深度和对于后一类查询，较高的深度可以潜在地减少注释工作，而不会显著改变检索有效性评估。我们使用标准查询性能预测(QPP)技术来估计每个查询可能相关的文档的数量，然后使用这些文档来确定池的深度。我们在标准测试集合上进行的实验表明，这种使用特定于查询的变量深度的方法能够充分反映 IR 系统的相对有效性，而且注释工作要少得多。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Query-specific+Variable+Depth+Pooling+via+Query+Performance+Prediction)|0|
|[RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses](https://doi.org/10.1145/3539618.3592047)|Honglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui, Ji Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, Michael Bendersky|Google Research, London, United Kingdom; Google Research, Mountain View, USA; Google Research, New York, USA; Google Research, Amsterdam, Netherlands|Recently, substantial progress has been made in text ranking based on pretrained language models such as BERT. However, there are limited studies on how to leverage more powerful sequence-to-sequence models such as T5. Existing attempts usually formulate text ranking as classification and rely on postprocessing to obtain a ranked list. In this paper, we propose RankT5 and study two T5-based ranking model structures, an encoder-decoder and an encoder-only one, so that they not only can directly output ranking scores for each query-document pair, but also can be fine-tuned with "pairwise" or "listwise" ranking losses to optimize ranking performances. Our experiments show that the proposed models with ranking losses can achieve substantial ranking performance gains on different public text ranking data sets. Moreover, when fine-tuned with listwise ranking losses, the ranking model appears to have better zero-shot ranking performance on out-of-domain data sets compared to the model fine-tuned with classification losses.|近年来，基于 BERT 等预训练语言模型的文本排序研究取得了长足的进展。然而，关于如何利用更强大的序列到序列模型(如 T5)的研究很有限。现有的尝试通常将文本排序公式化为分类，并依赖于后处理来获得排序列表。本文提出了 RankT5，并研究了两种基于 T5的排序模型结构: 编码-解码器结构和编码器-纯编码器结构，使它们不仅可以直接输出每个查询-文档对的排序得分，而且可以通过“成对”或“列表”排序损失进行微调，以优化排序性能。我们的实验表明，在不同的公共文本排序数据集上，所提出的带有排序损失的模型可以获得较大的排序性能增益。此外，当用列表排序损失进行微调时，与用分类损失进行微调的模型相比，排序模型在域外数据集上似乎具有更好的零点排序性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RankT5:+Fine-Tuning+T5+for+Text+Ranking+with+Ranking+Losses)|0|
|[Representation Sparsification with Hybrid Thresholding for Fast SPLADE-based Document Retrieval](https://doi.org/10.1145/3539618.3592051)|Yifan Qiao, Yingrui Yang, Shanxiu He, Tao Yang|University of California, Santa Barbara, Santa Barbara, CA, USA|Learned sparse document representations using a transformer-based neural model has been found to be attractive in both relevance effectiveness and time efficiency. This paper describes a representation sparsification scheme based on hard and soft thresholding with an inverted index approximation for faster SPLADE-based document retrieval. It provides analytical and experimental results on the impact of this learnable hybrid thresholding scheme.|使用基于变换器的神经模型学习稀疏文档表示已经被发现在相关性效率和时间效率方面具有吸引力。本文描述了一种基于硬阈值和软阈值的表示稀疏化方案，该方案采用倒排索引近似来实现更快的基于 SPLADE 的文献检索。文中给出了分析和实验结果的影响，这种学习的混合阈值方案。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Representation+Sparsification+with+Hybrid+Thresholding+for+Fast+SPLADE-based+Document+Retrieval)|0|
|[Robust Causal Inference for Recommender System to Overcome Noisy Confounders](https://doi.org/10.1145/3539618.3592055)|Zhiheng Zhang, Quanyu Dai, Xu Chen, Zhenhua Dong, Ruiming Tang|Gaoling School of Artificial Intelligence, Renmin University of China, Shenzhen, China; Huawei Noah's Ark Lab, Shenzhen, China; Institute for Interdisciplinary Information Sciences (IIIS), Tsinghua University, Beijing, China|Recently, there has been growing interest in integrating causal inference into recommender systems to answer the hypothetical question: "what would be the potential feedback when a user is recommended a product?" Various unbiased estimators, including Inverse Propensity Score (IPS) and Doubly Robust (DR), have been proposed to address this question. However, these estimators often assume that confounders are precisely observable, which is not always the case in real-world scenarios. To address this challenge, we propose a novel method called Adversarial Training-based IPS (AT-IPS), which uses adversarial training to handle noisy confounders. The proposed method defines a feasible region for the confounders, obtains the worst-case noise (adversarial noise) within the region, and jointly trains the propensity model and the prediction model against such noise to improve their robustness. We provide a theoretical analysis of the accuracy-robustness tradeoff of AT-IPS and demonstrate its superior performance compared to other popular estimators on both real-world and semi-synthetic datasets.|最近，人们越来越有兴趣将因果推理集成到推荐系统中，以回答这样一个假设性问题: “当用户被推荐一个产品时，潜在的反馈是什么?”针对这一问题，人们提出了各种无偏估计，包括反倾向评分(IPS)和双稳健估计(DR)。然而，这些评估者通常假设混杂因素是可以精确观察到的，而在现实世界的场景中并不总是如此。为了应对这一挑战，我们提出了一种新的方法称为基于对抗性训练的 IPS (AT-IPS) ，它使用对抗性训练来处理噪声混杂因素。该方法为混杂因子定义了一个可行区域，得到了该区域内的最坏情况噪声(对抗噪声) ，并对倾向模型和预测模型进行了联合训练，以提高其鲁棒性。本文从理论上分析了 AT-IPS 算法的精度-鲁棒性折衷问题，并在实际数据集和半合成数据集上证明了该算法与其他常用估计算法相比具有更好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Causal+Inference+for+Recommender+System+to+Overcome+Noisy+Confounders)|0|
|[Sharpness-Aware Graph Collaborative Filtering](https://doi.org/10.1145/3539618.3592059)|Huiyuan Chen, ChinChia Michael Yeh, Yujie Fan, Yan Zheng, Junpeng Wang, Vivian Lai, Mahashweta Das, Hao Yang|Visa Research, Palo Alto, USA|Graph Neural Networks (GNNs) have achieved impressive performance in collaborative filtering. However, GNNs tend to yield inferior performance when the distributions of training and test data are not aligned well. Also, training GNNs requires optimizing non-convex neural networks with an abundance of local and global minima, which may differ widely in their performance at test time. Thus, it is essential to choose the minima carefully. Here we propose an effective training schema, called {gSAM}, under the principle that the \textit{flatter} minima has a better generalization ability than the \textit{sharper} ones. To achieve this goal, gSAM regularizes the flatness of the weight loss landscape by forming a bi-level optimization: the outer problem conducts the standard model training while the inner problem helps the model jump out of the sharp minima. Experimental results show the superiority of our gSAM.|图形神经网络(GNN)在协同过滤方面取得了令人印象深刻的成就。然而，当训练和测试数据的分布不一致时，GNN 往往会产生较差的性能。此外，训练 GNN 需要优化具有丰富的局部和全局最小值的非凸神经网络，这可能在测试时间的性能差异很大。因此，仔细选择最小值是非常必要的。在这里，我们提出了一个有效的训练模式，称为{ gSAM } ，根据文本{平坦}极小比文本{锐利}极小具有更好的泛化能力的原则。为了实现这一目标，gSAM 通过形成一个双层优化来调整减肥景观的平面性: 外部问题进行标准模型训练，而内部问题帮助模型跳出尖锐的极小值。实验结果表明了我们的 gSAM 的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sharpness-Aware+Graph+Collaborative+Filtering)|0|
|[ExaRanker: Synthetic Explanations Improve Neural Rankers](https://doi.org/10.1145/3539618.3592067)|Fernando Ferraretto, Thiago Laitz, Roberto de Alencar Lotufo, Rodrigo Frassetto Nogueira|UNICAMP, Campinas, UNK, Brazil|Recent work has shown that incorporating explanations into the output generated by large language models (LLMs) can significantly enhance performance on a broad spectrum of reasoning tasks. Our study extends these findings by demonstrating the benefits of explanations for neural rankers. By utilizing LLMs such as GPT-3.5 to enrich retrieval datasets with explanations, we trained a sequence-to-sequence ranking model, dubbed ExaRanker, to generate relevance labels and explanations for query-document pairs. The ExaRanker model, finetuned on a limited number of examples and synthetic explanations, exhibits performance comparable to models finetuned on three times more examples, but without explanations. Moreover, incorporating explanations imposes no additional computational overhead into the reranking step and allows for on-demand explanation generation. The codebase and datasets used in this study will be available at https://github.com/unicamp-dl/ExaRanker|最近的研究表明，在大型语言模型(LLM)产生的输出中加入解释能够显著提高大范围推理任务的性能。我们的研究扩展了这些发现，证明了神经排序解释的好处。通过使用诸如 GPT-3.5这样的 LLM 来丰富检索数据集的解释，我们训练了一个序列到序列的排序模型，称为 ExaRanker，以生成查询文档对的相关标签和解释。ExaRanker 模型在有限数量的例子和综合解释的基础上进行了微调，其性能与在三倍以上的例子上进行微调的模型相当，但没有解释。此外，合并解释不会给重新排序步骤带来额外的计算开销，并允许按需生成解释。这项研究所使用的代码库和数据集将在 https://github.com/unicamp-dl/exaranker 提供|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ExaRanker:+Synthetic+Explanations+Improve+Neural+Rankers)|0|
|[Text-to-Motion Retrieval: Towards Joint Understanding of Human Motion Data and Natural Language](https://doi.org/10.1145/3539618.3592069)|Nicola Messina, Jan Sedmidubský, Fabrizio Falchi, Tomás Rebok|Masaryk University, Brno, Czech Rep; ISTI-CNR, Pisa, Italy|Due to recent advances in pose-estimation methods, human motion can be extracted from a common video in the form of 3D skeleton sequences. Despite wonderful application opportunities, effective and efficient content-based access to large volumes of such spatio-temporal skeleton data still remains a challenging problem. In this paper, we propose a novel content-based text-to-motion retrieval task, which aims at retrieving relevant motions based on a specified natural-language textual description. To define baselines for this uncharted task, we employ the BERT and CLIP language representations to encode the text modality and successful spatio-temporal models to encode the motion modality. We additionally introduce our transformer-based approach, called Motion Transformer (MoT), which employs divided space-time attention to effectively aggregate the different skeleton joints in space and time. Inspired by the recent progress in text-to-image/video matching, we experiment with two widely-adopted metric-learning loss functions. Finally, we set up a common evaluation protocol by defining qualitative metrics for assessing the quality of the retrieved motions, targeting the two recently-introduced KIT Motion-Language and HumanML3D datasets. The code for reproducing our results is available at https://github.com/mesnico/text-to-motion-retrieval.|由于近年来姿态估计方法的发展，人体运动可以从常见的视频中以三维骨架序列的形式提取出来。尽管有很好的应用机会，有效和高效的基于内容的访问大量这样的时空骨架数据仍然是一个具有挑战性的问题。本文提出了一种新的基于内容的文本-运动检索任务，该任务旨在基于特定的自然语言文本描述来检索相关运动。为了定义这个未知任务的基线，我们使用 BERT 和 CLIP 语言表示来编码文本模态，使用成功的时空模型来编码运动模态。此外，我们还介绍了我们的基于变压器的方法，称为运动变压器(MoT) ，它使用分割的时空注意力来有效地聚合不同的骨骼关节在时间和空间上。受到文本-图像/视频匹配技术的启发，我们实验了两种广泛采用的度量学习损失函数。最后，针对最近引入的两个 KIT Motion-Language 和 HumanML3D 数据集，通过定义定性指标来评估检索到的运动质量，建立了一个通用的评估协议。复制我们结果的代码可在 https://github.com/mesnico/text-to-motion-retrieval 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Text-to-Motion+Retrieval:+Towards+Joint+Understanding+of+Human+Motion+Data+and+Natural+Language)|0|
|[TripSafe: Retrieving Safety-related Abnormal Trips in Real-time with Trajectory Data](https://doi.org/10.1145/3539618.3592074)|Yueyang Su, Di Yao, Xiaolei Zhou, Yuxuan Zhang, Yunxia Fan, Lu Bai, Jingping Bi|DiDi Global Inc., beijing, China; Institute of Computing Technology Chinese Academy of Sciences, beijing, China|Nowadays safety has become one of the most critical factors for ride-hailing service. Ride-hailing platforms have conducted meticulous background checks for drivers to minimize the risk of abnormal trips, e.g. violence and sexual assault. However, current methods are labor-consuming and highly rely on the personal information of drivers, which may harm the fairness of the order dispatching system. In this paper, we utilize the trip trajectories as inputs and propose a dual variational auto-encoder(VAE) framework, namely TripSafe, to estimate the probability of abnormal safety incidents. Specifically, TripSafe models the moving behavior and route information, as two independent components and employs VAEs to pre-train generative models for normal trips. Then, a fusion network is adopted to fine-tune the whole model with a few labeled samples. In practice, TripSafe monitors the data update and calculate the anomaly score of partial-observed trips in real-time. Experiments on real ridehailing data show that TripSafe is superior to the state-of-the-art baselines with about 14.2%~28.9% improvements on F1 score.|如今，安全已经成为叫车服务最关键的因素之一。叫车平台对司机进行严格的背景调查，以尽量减少不正常行程的风险，例如暴力和性侵犯。然而，目前的方法耗费大量人力资源，对驾驶员个人信息的依赖程度较高，可能会损害订单调度系统的公平性。本文利用出行轨迹作为输入，提出了一种双变分自动编码器(VAE)框架，即 TripSafe，来估计异常安全事件的发生概率。具体来说，TripSafe 将移动行为和路径信息建模为两个独立的组件，并使用 VAE 预训练正常旅行的生成模型。然后，采用融合网络对整个模型进行微调，并加入少量标记样本。在实践中，TripSafe 监视数据更新并实时计算部分观测行程的异常评分。对真实叫车数据的实验表明，TripSafe 优于最先进的基线，F1得分提高了约14.2% ~ 28.9% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TripSafe:+Retrieving+Safety-related+Abnormal+Trips+in+Real-time+with+Trajectory+Data)|0|
|[Unsupervised Dense Retrieval Training with Web Anchors](https://doi.org/10.1145/3539618.3592080)|Yiqing Xie, Xiao Liu, Chenyan Xiong|Carnegie Mellon University, Pittsburgh, PA, USA; Microsoft, Redmond, WA, USA|In this work, we present an unsupervised retrieval method with contrastive learning on web anchors. The anchor text describes the content that is referenced from the linked page. This shows similarities to search queries that aim to retrieve pertinent information from relevant documents. Based on their commonalities, we train an unsupervised dense retriever, Anchor-DR, with a contrastive learning task that matches the anchor text and the linked document. To filter out uninformative anchors (such as ``homepage'' or other functional anchors), we present a novel filtering technique to only select anchors that contain similar types of information as search queries. Experiments show that Anchor-DR outperforms state-of-the-art methods on unsupervised dense retrieval by a large margin (e.g., by 5.3% NDCG@10 on MSMARCO). The gain of our method is especially significant for search and question answering tasks. Our analysis further reveals that the pattern of anchor-document pairs is similar to that of search query-document pairs. Code available at https://github.com/Veronicium/AnchorDR.|本文提出了一种基于对比学习的无监督检索方法。锚文本描述从链接页面引用的内容。这显示了与旨在从相关文档中检索相关信息的搜索查询的相似性。基于它们的共性，我们训练了一个无监督的密集检索器 Anchor-DR，它具有匹配锚文本和链接文档的对比学习任务。为了过滤掉没有信息的锚(如“主页”或其他功能性锚) ，我们提出了一种新的过滤技术，只选择包含与搜索查询类似类型信息的锚。实验表明，Anchor-DR 在无监督密集检索方面的性能大大优于最先进的方法(例如 MSMARCO 上的5.3% NDCG@10)。对于搜索和问答任务，该方法的增益尤为显著。我们的分析进一步揭示了锚-文档对的模式与搜索查询-文档对的模式相似。Https://github.com/veronicium/anchordr 提供密码。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unsupervised+Dense+Retrieval+Training+with+Web+Anchors)|0|
|[When the Music Stops: Tip-of-the-Tongue Retrieval for Music](https://doi.org/10.1145/3539618.3592086)|Samarth Bhargav, Anne Schuth, Claudia Hauff||We present a study of Tip-of-the-tongue (ToT) retrieval for music, where a searcher is trying to find an existing music entity, but is unable to succeed as they cannot accurately recall important identifying information. ToT information needs are characterized by complexity, verbosity, uncertainty, and possible false memories. We make four contributions. (1) We collect a dataset - $ToT_{Music}$ - of 2,278 information needs and ground truth answers. (2) We introduce a schema for these information needs and show that they often involve multiple modalities encompassing several Music IR subtasks such as lyric search, audio-based search, audio fingerprinting, and text search. (3) We underscore the difficulty of this task by benchmarking a standard text retrieval approach on this dataset. (4) We investigate the efficacy of query reformulations generated by a large language model (LLM), and show that they are not as effective as simply employing the entire information need as a query - leaving several open questions for future research.|我们提出了一个音乐舌尖(ToT)检索的研究，其中一个搜索者试图找到一个现有的音乐实体，但不能成功，因为他们不能准确地回忆重要的识别信息。时间信息的需求是拥有属性的复杂性、冗长性、不确定性和可能的错误记忆。我们做了四笔捐款。(1)我们收集了2278个信息需求和基本事实答案的数据集。(2)针对这些信息需求，我们引入了一个模式，表明它们通常涉及多种形式，包括多个 Music IR 子任务，如歌词搜索、基于音频的搜索、音频指纹识别和文本搜索。(3)我们强调这项任务的困难，通过基准测试的标准文本检索方法对这个数据集。(4)我们研究了大语言模型(LLM)生成的查询重构的有效性，结果表明它们不如简单地将整个信息需求作为一个查询来得有效——为未来的研究留下了一些未解决的问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=When+the+Music+Stops:+Tip-of-the-Tongue+Retrieval+for+Music)|0|
|[Reproducibility, Replicability, and Insights into Dense Multi-Representation Retrieval Models: from ColBERT to Col](https://doi.org/10.1145/3539618.3591916)|Xiao Wang, Craig Macdonald, Nicola Tonellotto, Iadh Ounis|University of Glasgow, Glasgow, United Kingdom; University of Pisa, Pisa, Italy|Dense multi-representation retrieval models, exemplified as ColBERT, estimate the relevance between a query and a document based on the similarity of their contextualised token-level embeddings. Indeed, by using contextualised token embeddings, dense retrieval, conducted as either exact or semantic matches, can result in increased effectiveness for both in-domain and out-of-domain retrieval tasks, indicating that it is an important model to study. However, the exact role that these semantic matches play is not yet well investigated. For instance, although tokenisation is one of the crucial design choices for various pretrained language models, its impact on the matching behaviour has not been examined in detail. In this work, we inspect the reproducibility and replicability of the contextualised late interaction mechanism by extending ColBERT to Col⋆ which implements the late interaction mechanism across various pretrained models and different types of tokenisers. As different tokenisation methods can directly impact the matching behaviour within the late interaction mechanism, we study the nature of matches occurring in different Col⋆ models, and further quantify the contribution of lexical and semantic matching on retrieval effectiveness. Overall, our experiments successfully reproduce the performance of ColBERT on various query sets, and replicate the late interaction mechanism upon different pretrained models with different tokenisers. Moreover, our experimental results yield new insights, such as: (i) semantic matching behaviour varies across different tokenisers; (ii) more specifically, high-frequency tokens tend to perform semantic matching than other token families; (iii) late interaction mechanism benefits more from lexical matching than semantic matching; (iv) special tokens, such as [CLS], play a very important role in late interaction.|以 ColBERT 为例的稠密多表示检索模型，基于上下文化标记级嵌入的相似性来估计查询与文档之间的相关性。事实上，通过使用上下文化的令牌嵌入，以精确匹配或语义匹配的方式进行的密集检索可以提高域内和域外检索任务的有效性，这表明它是一个重要的研究模型。然而，这些语义匹配所起的确切作用还没有得到很好的研究。例如，虽然标记化是各种预先训练的语言模型的关键设计选择之一，但它对匹配行为的影响尚未得到详细研究。在这项工作中，我们检查了上下文化的后期交互机制的可重复性和可复制性，通过将 ColBERT 扩展到 Col something，实现了跨各种预先训练的模型和不同类型的标记器的后期交互机制。由于不同的标记化方法可以直接影响后期交互机制中的匹配行为，本文研究了不同 Col 模型中匹配的性质，并进一步量化了词汇和语义匹配对检索效率的贡献。总的来说，我们的实验成功地再现了 ColBERT 在不同查询集上的性能，并且在不同的预训练模型上复制了后期交互机制。此外，我们的实验结果产生了新的见解，如: (i)语义匹配行为不同的标记; (ii)更具体地说，高频标记倾向于执行语义匹配比其他标记家族; (iii)后期交互机制受益于词汇匹配比语义匹配更多; (iv)特殊标记，如[ CLS ] ，在后期交互中发挥非常重要的作用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reproducibility,+Replicability,+and+Insights+into+Dense+Multi-Representation+Retrieval+Models:+from+ColBERT+to+Col)|0|
|[On Stance Detection in Image Retrieval for Argumentation](https://doi.org/10.1145/3539618.3591917)|Miriam Louise Carnot, Lorenz Heinemann, Jan Braker, Tobias Schreieder, Johannes Kiesel, Maik Fröbe, Martin Potthast, Benno Stein|Martin-Luther-Universität Halle Wittenberg, Halle, Germany; Bauhaus-Universität Weimar, Weimar, Germany; Leipzig University and ScaDS.AI, Leipzig, Germany; Leipzig University, Leipzig, Germany|Given a text query on a controversial topic, the task of Image Retrieval for Argumentation is to rank images according to how well they can be used to support a discussion on the topic. An important subtask therein is to determine the stance of the retrieved images, i.e., whether an image supports the pro or con side of the topic. In this paper, we conduct a comprehensive reproducibility study of the state of the art as represented by the CLEF'22 Touché lab and an in-house extension of it. Based on the submitted approaches, we developed a unified and modular retrieval process and reimplemented the submitted approaches according to this process. Through this unified reproduction (which also includes models not previously considered), we achieve an effectiveness improvement in argumentative image detection of up to 0.832 precision@10. However, despite this reproduction success, our study also revealed a previously unknown negative result: for stance detection, none of the reproduced or new approaches can convincingly beat a random baseline. To understand the apparent challenges inherent to image stance detection, we conduct a thorough error analysis and provide insight into potential new ways to approach this task.|给定一个关于有争议话题的文本查询，图像检索用于论证的任务是根据图像在多大程度上可以用于支持关于这个话题的讨论来对它们进行排序。其中一个重要的子任务是确定检索图像的立场，即图像是否支持主题的正面或反面。在本文中，我们进行了一个全面的再现性研究的状态所代表的 CLEF’22 Touché 实验室和它的内部扩展。在提交方法的基础上，我们开发了一个统一的模块化检索流程，并根据该流程重新实现了提交的方法。通过这种统一的再现(其中还包括以前未考虑的模型) ，我们实现了高达0.832精度@10的议论图像检测的有效性改进。然而，尽管复制成功，我们的研究也揭示了一个以前未知的负面结果: 对于姿势检测，没有一种复制或新的方法能够令人信服地超过随机基线。为了理解图像姿态检测固有的明显挑战，我们进行了一个彻底的错误分析，并提供洞察潜在的新方法来处理这一任务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Stance+Detection+in+Image+Retrieval+for+Argumentation)|0|
|[Take a Fresh Look at Recommender Systems from an Evaluation Standpoint](https://doi.org/10.1145/3539618.3591931)|Aixin Sun|Nanyang Technological University, Singapore, Singapore|Recommendation has become a prominent area of research in the field of Information Retrieval (IR). Evaluation is also a traditional research topic in this community. Motivated by a few counter-intuitive observations reported in recent studies, this perspectives paper takes a fresh look at recommender systems from an evaluation standpoint. Rather than examining metrics like recall, hit rate, or NDCG, or perspectives like novelty and diversity, the key focus here is on how these metrics are calculated when evaluating a recommender algorithm. Specifically, the commonly used train/test data splits and their consequences are re-examined. We begin by examining common data splitting methods, such as random split or leave-one-out, and discuss why the popularity baseline is poorly defined under such splits. We then move on to explore the two implications of neglecting a global timeline during evaluation: data leakage and oversimplification of user preference modeling. Afterwards, we present new perspectives on recommender systems, including techniques for evaluating algorithm performance that more accurately reflect real-world scenarios, and possible approaches to consider decision contexts in user preference modeling.|推荐已经成为信息检索领域的一个重要研究领域。评价也是这个社区的一个传统研究课题。受近期研究中一些反直觉观察的启发，本文从评估的角度重新审视推荐系统。与其检查诸如召回率、命中率或 NDCG 之类的指标，或者新颖性和多样性之类的观点，这里的关键焦点是在评估推荐算法时如何计算这些指标。特别是，常用的列车/测试数据分割及其后果被重新检查。我们首先研究常见的数据分割方法，比如随机分割或者漏掉一个，然后讨论为什么在这样的分割下流行基线定义得很差。然后，我们继续探讨在评估过程中忽略全局时间线的两个含义: 数据泄漏和用户偏好建模的过度简化。随后，我们提出了推荐系统的新观点，包括评估算法性能的技术，更准确地反映真实世界的场景，以及可能的方法，以考虑决策上下文在用户偏好建模。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Take+a+Fresh+Look+at+Recommender+Systems+from+an+Evaluation+Standpoint)|0|
|[Where to Go Next for Recommender Systems? ID- vs. Modality-based Recommender Models Revisited](https://doi.org/10.1145/3539618.3591932)|Zheng Yuan, Fajie Yuan, Yu Song, Youhua Li, Junchen Fu, Fei Yang, Yunzhu Pan, Yongxin Ni|Westlake University, Hangzhou, China; Zhejiang Lab, Hangzhou, China|Recommendation models that utilize unique identities (IDs) to represent distinct users and items have been state-of-the-art (SOTA) and dominated the recommender systems (RS) literature for over a decade. Meanwhile, the pre-trained modality encoders, such as BERT and ViT, have become increasingly powerful in modeling the raw modality features of an item, such as text and images. Given this, a natural question arises: can a purely modality-based recommendation model (MoRec) outperforms or matches a pure ID-based model (IDRec) by replacing the itemID embedding with a SOTA modality encoder? In fact, this question was answered ten years ago when IDRec beats MoRec by a strong margin in both recommendation accuracy and efficiency. We aim to revisit this `old' question and systematically study MoRec from several aspects. Specifically, we study several sub-questions: (i) which recommendation paradigm, MoRec or IDRec, performs better in practical scenarios, especially in the general setting and warm item scenarios where IDRec has a strong advantage? does this hold for items with different modality features? (ii) can the latest technical advances from other communities (i.e., natural language processing and computer vision) translate into accuracy improvement for MoRec? (iii) how to effectively utilize item modality representation, can we use it directly or do we have to adjust it with new data? (iv) are there some key challenges for MoRec to be solved in practical applications? To answer them, we conduct rigorous experiments for item recommendations with two popular modalities, i.e., text and vision. We provide the first empirical evidence that MoRec is already comparable to its IDRec counterpart with an expensive end-to-end training method, even for warm item recommendation. Our results potentially imply that the dominance of IDRec in the RS field may be greatly challenged in the future.|推荐模型利用独特的身份(ID)来代表不同的用户和项目已经是最先进的(SOTA) ，并主导推荐系统(RS)文献超过十年。同时，经过训练的情态编码器，如 BERT 和 ViT，在对文本和图像等项目的原始情态特征进行建模方面已经变得越来越强大。鉴于此，一个自然的问题出现了: 通过用 SOTA 模式编码器替换嵌入的 itemID，纯基于模式的推荐模型(MoRec)是否优于或匹配纯基于 ID 的模型(IDRec) ？事实上，这个问题在十年前就得到了回答，当时 IDRec 在推荐的准确性和效率上都大大超过了 MoRec。我们的目标是重新审视这个“老”问题，并从几个方面系统地研究 MoRec。具体来说，我们研究了几个子问题: (i)哪个推荐范例，MoRec 或 IDRec，在实际场景中表现更好，特别是在一般设置和暖项目场景中，IDRec 有很强的优势？对于具有不同形态特征的物品是否适用？(ii)来自其他社区的最新技术进步(即自然语言处理和计算机视觉)能否转化为 MoRec 的准确性改进？(iii)如何有效地利用项目形态表征，是直接使用还是用新数据进行调整？(iv) MoRec 在实际应用中是否存在一些需要解决的关键挑战？为了回答这些问题，我们用两种流行的模式，即文本和视觉，对项目推荐进行了严格的实验。我们提供的第一个经验证明是，MoRec 已经可以与 IDrec 相媲美，它采用了一种昂贵的端到端培训方法，即使是暖项推荐也是如此。我们的研究结果可能意味着 IDRec 在遥感领域的主导地位在未来可能会受到很大的挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Where+to+Go+Next+for+Recommender+Systems?+ID-+vs.+Modality-based+Recommender+Models+Revisited)|0|
|[How Important is Periodic Model update in Recommender System?](https://doi.org/10.1145/3539618.3591934)|Hyunsung Lee, Sungwook Yoo, Dongjun Lee, Jaekwang Kim|Kakao Corporation, Seongnam, Republic of Korea; Sungkyunkwan University, Suwon, Republic of Korea|In real-world recommender model deployments, the models are typically retrained and deployed repeatedly. It is the rule-of-thumb to periodically retrain recommender models to capture up-to-date user behavior and item trends. However, the harm caused by delayed model updates has not been investigated extensively yet. in this perspective paper, we formulate the delayed model update problem and quantitatively demonstrate the delayed model update actually harms the model performance by increasing the number of cold users and cold items increase and decreasing overall model performances. These effects vary across different domains having different characteristics. Upon these findings, we further argue that although the delayed model update has negative effects on online recommender model deployment, yet it has not gathered enough attention from research communities. We argue our verification of the relationship between the model update cycle and model performance calls for further research such as faster model training, and more efficient data pipelines to keep the model more up-to-date with the latest user behaviors and item trends.|在现实世界的推荐模型部署中，模型通常被重复训练和部署。经验法则是定期重新训练推荐模型，以捕获最新的用户行为和项目趋势。然而，延迟模型更新所造成的危害还没有得到广泛的研究。本文提出了模型延迟更新问题，定量地证明了模型延迟更新通过增加冷用户数量、增加冷条目、降低模型整体性能而对模型性能造成损害。这些效应在具有不同特征的不同领域中有所不同。基于这些发现，我们进一步认为，虽然延迟模型更新对在线推荐模型的部署有负面影响，但是它还没有引起研究界足够的重视。我们认为模型更新周期和模型性能之间的关系的验证需要进一步的研究，如更快的模型训练，更有效的数据管道，以保持模型更新与最新的用户行为和项目趋势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+Important+is+Periodic+Model+update+in+Recommender+System?)|0|
|[Metric-agnostic Ranking Optimization](https://doi.org/10.1145/3539618.3591935)|Qingyao Ai, Xuanhui Wang, Michael Bendersky|Tsinghua University, Beijing, China; Google Research, Mountain View, CA, USA|Ranking is at the core of Information Retrieval. Classic ranking optimization studies often treat ranking as a sorting problem with the assumption that the best performance of ranking would be achieved if we rank items according to their individual utility. Accordingly, considerable ranking metrics have been developed and learning-to-rank algorithms that have been designed to optimize these simple performance metrics have been widely used in modern IR systems. As applications evolve, however, people's need for information retrieval have shifted from simply retrieving relevant documents to more advanced information services that satisfy their complex working and entertainment needs. Thus, more complicated and user-centric objectives such as user satisfaction and engagement have been adopted to evaluate modern IR systems today. Those objectives, unfortunately, are difficult to be optimized under existing learning-to-rank frameworks as they are subject to great variance and complicated structures that cannot be explicitly explained or formulated with math equations like those simple performance metrics. This leads to the following research question -- how to optimize result ranking for complex ranking metrics without knowing their internal structures? To address this question, we conduct formal analysis on the limitation of existing ranking optimization techniques and describe three research tasks in \textit{Metric-agnostic Ranking Optimization}. Through the discussion of potential solutions to these tasks, we hope to encourage more people to look into the problem of ranking optimization in complex search and recommendation scenarios.|排名是信息检索的核心。经典的排序优化研究往往把排序当作一个排序问题，假设如果我们根据项目的个别效用进行排序，就可以获得最佳的排序效果。因此，相当多的排序指标已经开发和学习到排序算法的设计，以优化这些简单的性能指标已被广泛应用于现代红外系统。然而，随着应用程序的发展，人们对信息检索的需求已经从简单地检索相关文档转向更先进的信息服务，以满足他们复杂的工作和娱乐需求。因此，更复杂和以用户为中心的目标，如用户满意度和参与度已被采用来评估现代 IR 系统。不幸的是，这些目标很难在现有的学习排名框架下进行优化，因为它们会受到巨大的变化和复杂的结构的影响，而这些变化和结构无法用简单的性能指标这样的数学方程来明确解释或表述。这就引出了下面的研究问题——如何在不知道其内部结构的情况下优化复杂排序指标的结果排序？为了解决这个问题，我们对现有排序优化技术的局限性进行了形式化分析，并描述了《度量无关排序优化》中的三个研究任务。通过讨论这些任务的潜在解决方案，我们希望鼓励更多的人研究复杂搜索和推荐场景中的排序优化问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Metric-agnostic+Ranking+Optimization)|0|
|[Recipe-MPR: A Test Collection for Evaluating Multi-aspect Preference-based Natural Language Retrieval](https://doi.org/10.1145/3539618.3591880)|Haochen Zhang, Anton Korikov, Parsa Farinneya, Mohammad Mahdi Abdollah Pour, Manasa Bharadwaj, Ali Pesaranghader, Xi Yu Huang, Yi Xin Lok, Zhaoqi Wang, Nathan Jones, Scott Sanner|University of Toronto, Toronto, ON, Canada; University of Toronto & Vector Institute of Artificial Intelligence, Toronto, ON, Canada; LG Electronics, Toronto AI Lab, Toronto, ON, Canada|The rise of interactive recommendation assistants has led to a novel domain of natural language (NL) recommendation that would benefit from improved multi-aspect reasoning to retrieve relevant items based on NL statements of preference. Such preference statements often involve multiple aspects, e.g., "I would like meat lasagna but I'm watching my weight". Unfortunately, progress in this domain is slowed by the lack of annotated data. To address this gap, we curate a novel dataset which captures logical reasoning over multi-aspect, NL preference-based queries and a set of multiple-choice, multi-aspect item descriptions. We focus on the recipe domain in which multi-aspect preferences are often encountered due to the complexity of the human diet. The goal of publishing our dataset is to provide a benchmark for joint progress in three key areas: 1) structured, multi-aspect NL reasoning with a variety of properties (e.g., level of specificity, presence of negation, and the need for commonsense, analogical, and/or temporal inference), 2) the ability of recommender systems to respond to NL preference utterances, and 3) explainable NL recommendation facilitated by aspect extraction and reasoning. We perform experiments using a variety of methods (sparse and dense retrieval, zero- and few-shot reasoning with large language models) in two settings: a monolithic setting which uses the full query and an aspect-based setting which isolates individual query aspects and aggregates the results. GPT-3 results in much stronger performance than other methods with 73% zero-shot accuracy and 83% few-shot accuracy in the monolithic setting. Aspect-based GPT-3, which facilitates structured explanations, also shows promise with 68% zero-shot accuracy. These results establish baselines for future research into explainable recommendations via multi-aspect preference-based NL reasoning.|交互式推荐助手的兴起导致了自然语言(NL)推荐的一个新领域，它将受益于改进的多方面推理来检索基于 NL 偏好语句的相关项目。这样的偏好陈述往往涉及多个方面，例如，“我想吃肉千层面，但我在控制体重”。不幸的是，由于缺乏注释数据，这个领域的进展缓慢。为了解决这个问题，我们建立了一个新的数据集，它能够捕获多方面的逻辑推理，基于 NL 偏好的查询，以及一组多选择、多方面的项目描述。我们关注的食谱领域，其中多方面的偏好往往遇到由于人类饮食的复杂性。发布我们的数据集的目标是为三个关键领域的联合进展提供基准: 1)具有各种属性的结构化，多方面的 NL 推理(例如，特异性水平，否定的存在，以及对常识，类比和/或时间推理的需要) ，2)推荐系统响应 NL 偏好话语的能力，和3)通过方面提取和推理促进的可解释的 NL 推荐。我们使用各种方法(稀疏和密集检索，零和大型语言模型的少镜头推理)在两种设置下进行实验: 一种是使用完整查询的整体设置，另一种是基于方面的设置，隔离各个查询方面并聚合结果。GPT-3比其他方法具有更强的性能，在单片机设置中，零拍准确率为73% ，少拍准确率为83% 。基于方面的 GPT-3促进了结构化解释，也显示出68% 的零拍准确率。这些结果为今后通过基于多方面偏好的 NL 推理研究可解释的建议奠定了基础。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Recipe-MPR:+A+Test+Collection+for+Evaluating+Multi-aspect+Preference-based+Natural+Language+Retrieval)|0|
|[The Information Retrieval Experiment Platform](https://doi.org/10.1145/3539618.3591888)|Maik Fröbe, Jan Heinrich Reimer, Sean MacAvaney, Niklas Deckers, Simon Reich, Janek Bevendorff, Benno Stein, Matthias Hagen, Martin Potthast|Friedrich-Schiller-Universität Jena, Jena, Germany; Leipzig University and ScaDS.AI, Leipzig, Germany; Leipzig University, Leipzig, Germany; University of Glasgow, Glasgow, United Kingdom; Bauhaus-Universität Weimar, Weimar, Germany|We integrate ir_datasets, ir_measures, and PyTerrier with TIRA in the Information Retrieval Experiment Platform (TIREx) to promote more standardized, reproducible, scalable, and even blinded retrieval experiments. Standardization is achieved when a retrieval approach implements PyTerrier's interfaces and the input and output of an experiment are compatible with ir_datasets and ir_measures. However, none of this is a must for reproducibility and scalability, as TIRA can run any dockerized software locally or remotely in a cloud-native execution environment. Version control and caching ensure efficient (re)execution. TIRA allows for blind evaluation when an experiment runs on a remote server or cloud not under the control of the experimenter. The test data and ground truth are then hidden from public access, and the retrieval software has to process them in a sandbox that prevents data leaks. We currently host an instance of TIREx with 15 corpora (1.9 billion documents) on which 32 shared retrieval tasks are based. Using Docker images of 50 standard retrieval approaches, we automatically evaluated all approaches on all tasks (50 $\cdot$ 32 = 1,600~runs) in less than a week on a midsize cluster (1,620 CPU cores and 24 GPUs). This instance of TIREx is open for submissions and will be integrated with the IR Anthology, as well as released open source.|我们在信息检索实验平台(TIREx)中整合了 ir _ data set，ir _ 径和 PyTerrier 与 TIRA，以促进更标准化、可重复、可扩展甚至盲法检索实验。当检索方法实现了 PyTerrier 的接口，并且实验的输入和输出与 ir _ data 集和 ir _ 径兼容时，就实现了标准化。然而，所有这些对于可重复性和可伸缩性来说都不是必须的，因为 TIRA 可以在本地或远程运行任何经过停靠的软件，在一个云本地执行环境中。版本控制和缓存确保有效(重新)执行。当实验在远程服务器或不受实验者控制的云上运行时，TIRA 允许盲目评估。然后，测试数据和地面真相被隐藏起来，不为公众所知，检索软件必须在防止数据泄露的沙箱中处理它们。我们目前托管了一个包含15个语料库(19亿个文档)的 TIREx 实例，其中有32个共享检索任务。使用50种标准检索方法的 Docker 图像，我们在不到一周的时间内在一个中型集群(1620个 CPU 核和24个 GPU)上对所有任务(50 $cdot $32 = 1600 ~ run)的所有方法进行自动评估。这个 TIREx 实例对提交者开放，并将与 IR 选集集成，以及发布的开放源码。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Information+Retrieval+Experiment+Platform)|0|
|[RecStudio: Towards a Highly-Modularized Recommender System](https://doi.org/10.1145/3539618.3591894)|Defu Lian, Xu Huang, Xiaolong Chen, Jin Chen, Xingmei Wang, Yankai Wang, Haoran Jin, Rui Fan, Zheng Liu, Le Wu, Enhong Chen|University of Science and Technology of China, Hefei, China; University of Electronics Science and Technology of China, Chengdu, China; Huawei, Beijing, China; Hefei University of Technology, Hefei, China|A dozen recommendation libraries have recently been developed to accommodate popular recommendation algorithms for reproducibility. However, they are almost simply a collection of algorithms, overlooking the modularization of recommendation algorithms and their usage in practical scenarios. Algorithmic modularization has the following advantages: 1) helps to understand the effectiveness of each algorithm; 2) easily assembles new algorithms with well-performed modules by either drag-and-drop programming or automatic machine learning; 3) enables reinforcement between algorithms since one algorithm may act as a module of another algorithm. To this end, we develop a highly-modularized recommender system -- RecStudio, in which any recommendation algorithm is categorized into either a ranker or a retriever. In the RecStudio library, we implement 90 recommendation algorithms with the pure Pytorch, covering both common algorithms in other libraries and complex algorithms involving multiple recommendation models. RecStudio is featured from several perspectives, such as index-supported efficient recommendation and evaluation, GPU-accelerated negative sampling, hyperparameter learning on the validation, and cooperation between the retriever and ranker. RecStudio is also equipped with a web service, where the recommendation pipeline can be quickly established and visually evaluated on selected datasets, and the evaluation results are automatically archived and visualized in a leaderboard. The project and documents are released at http://recstudio.org.cn.|最近已经开发了十几个推荐库来适应流行的推荐算法以实现可重复性。然而，它们几乎只是算法的集合，忽略了推荐算法的模块化及其在实际场景中的应用。算法模块化具有以下优点: 1)有助于理解每个算法的有效性; 2)通过拖放编程或自动机器学习，可以轻松地将表现良好的模块与新算法组合在一起; 3)可以在算法之间进行强化，因为一个算法可以作为另一个算法的模块。为此，我们开发了一个高度模块化的推荐系统—— RecStudio，其中任何推荐算法都可以分为排名算法和检索算法。在 RecStudio 库中，我们使用纯 Python 实现了90个推荐算法，涵盖了其他库中的常见算法和涉及多个推荐模型的复杂算法。RecStudio 的特点主要体现在以下几个方面: 索引支持的高效推荐和评价、 GPU 加速的负抽样、验证过程中的超参数学习以及检索者与排名者之间的协作。RecStudio 还配备了一个网络服务，可以快速建立推荐渠道，并对选定的数据集进行可视化评估，评估结果将自动存档，并在排行榜上可视化。项目和文件将在 http://recstudio.org.cn 公布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RecStudio:+Towards+a+Highly-Modularized+Recommender+System)|0|
|[MR2: A Benchmark for Multimodal Retrieval-Augmented Rumor Detection in Social Media](https://doi.org/10.1145/3539618.3591896)|Xuming Hu, Zhijiang Guo, Junzhe Chen, Lijie Wen, Philip S. Yu|Tsinghua University, Beijing, China; University of Cambridge, Cambridge, United Kingdom; University of Illinois at Chicago, Chicago, IL, USA|As social media platforms are evolving from text-based forums into multi-modal environments, the nature of misinformation in social media is also transforming accordingly. Misinformation spreaders have recently targeted contextual connections between the modalities e.g., text and image. However, existing datasets for rumor detection mainly focus on a single modality i.e., text. To bridge this gap, we construct MR2, a multimodal multilingual retrieval-augmented dataset for rumor detection. The dataset covers rumors with images and texts, and provides evidence from both modalities that are retrieved from the Internet. Further, we develop established baselines and conduct a detailed analysis of the systems evaluated on the dataset. Extensive experiments show that MR2 will provide a challenging testbed for developing rumor detection systems designed to retrieve and reason over social media posts. Source code and data are available at: https://github.com/THU-BPM/MR2.|随着社交媒体平台从基于文本的论坛演变为多模式的环境，社交媒体中错误信息的性质也在相应地发生变化。错误信息传播者最近将目标对准了模式之间的上下文联系，例如文本和图像。然而，现有的谣言检测数据集主要集中在一个单一的模式，即文本。为了弥合这一差距，我们构建了 MR2，一个用于谣言检测的多模态多语言检索增强数据集。该数据集用图像和文本覆盖谣言，并提供从互联网检索到的两种形式的证据。此外，我们建立了基线，并对数据集上评估的系统进行了详细的分析。大量的实验表明，MR2将为开发旨在检索和推理社交媒体帖子的谣言检测系统提供一个具有挑战性的试验平台。源代码和数据可在以下 https://github.com/thu-bpm/mr2获得:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MR2:+A+Benchmark+for+Multimodal+Retrieval-Augmented+Rumor+Detection+in+Social+Media)|0|
|[RL4RS: A Real-World Dataset for Reinforcement Learning based Recommender System](https://doi.org/10.1145/3539618.3591899)|Kai Wang, Zhene Zou, Minghao Zhao, Qilin Deng, Yue Shang, Yile Liang, Runze Wu, Xudong Shen, Tangjie Lyu, Changjie Fan|NetEase Fuxi AI Lab, Hangzhou, China|Reinforcement learning based recommender systems (RL-based RS) aim at learning a good policy from a batch of collected data, by casting recommendations to multi-step decision-making tasks. However, current RL-based RS research commonly has a large reality gap. In this paper, we introduce the first open-source real-world dataset, RL4RS, hoping to replace the artificial datasets and semi-simulated RS datasets previous studies used due to the resource limitation of the RL-based RS domain. Unlike academic RL research, RL-based RS suffers from the difficulties of being well-validated before deployment. We attempt to propose a new systematic evaluation framework, including evaluation of environment simulation, evaluation on environments, counterfactual policy evaluation, and evaluation on environments built from test set. In summary, the RL4RS (Reinforcement Learning for Recommender Systems), a new resource with special concerns on the reality gaps, contains two real-world datasets, data understanding tools, tuned simulation environments, related advanced RL baselines, batch RL baselines, and counterfactual policy evaluation algorithms. The RL4RS suite can be found at https://github.com/fuxiAIlab/RL4RS. In addition to the RL-based recommender systems, we expect the resource to contribute to research in applied reinforcement learning.|基于强化学习的推荐系统(RL-based RS)旨在从一批收集到的数据中学习一个好的政策，将推荐应用到多步决策任务中。然而，目前基于 RL 的遥感研究普遍存在较大的现实差距。本文介绍了第一个开源的真实世界数据集 RL4RS，希望能够取代以往由于基于 RL 的 RS 域资源有限而使用的人工数据集和半模拟 RS 数据集。与学术上的 RL 研究不同，基于 RL 的 RS 在部署前很难得到很好的验证。本文尝试提出一个新的系统评价框架，包括环境模拟评价、环境评价、反事实政策评价和基于测试集的环境评价。总之，RL4RS (推荐系统的强化学习) ，一个特别关注现实差距的新资源，包含两个真实世界的数据集，数据理解工具，调优的仿真环境，相关的高级 RL 基线，批量 RL 基线，和反事实政策评估算法。Rl4RS 套房可在 https://github.com/fuxiailab/RL4RS 找到。除了基于 RL 的推荐系统，我们希望这些资源能够为应用强化学习的研究做出贡献。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RL4RS:+A+Real-World+Dataset+for+Reinforcement+Learning+based+Recommender+System)|0|
|[iQPP: A Benchmark for Image Query Performance Prediction](https://doi.org/10.1145/3539618.3591901)|Eduard Poesina, Radu Tudor Ionescu, Josiane Mothe|INSPE, IRIT UMR5505 CNRS, Université Toulouse Jean-Jaurès, Toulouse, France; University of Bucharest, Bucharest, Romania|To date, query performance prediction (QPP) in the context of content-based image retrieval remains a largely unexplored task, especially in the query-by-example scenario, where the query is an image. To boost the exploration of the QPP task in image retrieval, we propose the first benchmark for image query performance prediction (iQPP). First, we establish a set of four data sets (PASCAL VOC 2012, Caltech-101, ROxford5k and RParis6k) and estimate the ground-truth difficulty of each query as the average precision or the precision@k, using two state-of-the-art image retrieval models. Next, we propose and evaluate novel pre-retrieval and post-retrieval query performance predictors, comparing them with existing or adapted (from text to image) predictors. The empirical results show that most predictors do not generalize across evaluation scenarios. Our comprehensive experiments indicate that iQPP is a challenging benchmark, revealing an important research gap that needs to be addressed in future work. We release our code and data as open source at https://github.com/Eduard6421/iQPP, to foster future research.|迄今为止，在基于内容的图像检索上下文中的查询性能预测(QPP)仍然是一个很大程度上尚未探索的任务，特别是在逐例查询场景中，其中查询是一个图像。为了促进 QPP 任务在图像检索中的应用，我们提出了第一个图像查询性能预测基准(iQPP)。首先，我们建立了一组四个数据集(PASCAL VOC 2012，Caltech-101，ROxford5k 和 RParis6k) ，并使用两种最先进的图像检索模型将每个查询的地面真实度难度估计为平均精度或精度@k。接下来，我们提出并评估新的检索前和检索后的查询性能预测因子，并将它们与现有的或改编的(从文本到图像)预测因子进行比较。经验结果表明，大多数预测因素并不能一概而论。我们的全面实验表明，iQPP 是一个具有挑战性的基准，揭示了一个重要的研究差距，需要在未来的工作中解决。我们将代码和数据作为开源 https://github.com/eduard6421/iqpp 发布，以促进未来的研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=iQPP:+A+Benchmark+for+Image+Query+Performance+Prediction)|0|
|[SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval](https://doi.org/10.1145/3539618.3591902)|Nandan Thakur, Kexin Wang, Iryna Gurevych, Jimmy Lin|University of Waterloo, Waterloo, ON, Canada; Technical University of Darmstadt, Darmstadt, Germany|Traditionally, sparse retrieval systems relied on lexical representations to retrieve documents, such as BM25, dominated information retrieval tasks. With the onset of pre-trained transformer models such as BERT, neural sparse retrieval has led to a new paradigm within retrieval. Despite the success, there has been limited software supporting different sparse retrievers running in a unified, common environment. This hinders practitioners from fairly comparing different sparse models and obtaining realistic evaluation results. Another missing piece is, that a majority of prior work evaluates sparse retrieval models on in-domain retrieval, i.e. on a single dataset: MS MARCO. However, a key requirement in practical retrieval systems requires models that can generalize well to unseen out-of-domain, i.e. zero-shot retrieval tasks. In this work, we provide SPRINT, a unified Python toolkit based on Pyserini and Lucene, supporting a common interface for evaluating neural sparse retrieval. The toolkit currently includes five built-in models: uniCOIL, DeepImpact, SPARTA, TILDEv2 and SPLADEv2. Users can also easily add customized models by defining their term weighting method. Using our toolkit, we establish strong and reproducible zero-shot sparse retrieval baselines across the well-acknowledged benchmark, BEIR. Our results demonstrate that SPLADEv2 achieves the best average score of 0.470 nDCG@10 on BEIR amongst all neural sparse retrievers. In this work, we further uncover the reasons behind its performance gain. We show that SPLADEv2 produces sparse representations with a majority of tokens outside of the original query and document which is often crucial for its performance gains, i.e. a limitation among its other sparse counterparts. We provide our SPRINT toolkit, models, and data used in our experiments publicly here at https://github.com/thakur-nandan/sprint.|传统上，稀疏检索系统依赖于词汇表示来检索文档，比如 BM25，这是主要的信息检索任务。随着像 BERT 这样的预训练变压器模型的出现，神经元稀疏检索在检索中引入了一种新的范式。尽管取得了成功，但支持不同稀疏检索器在统一、通用环境中运行的软件有限。这妨碍了从业人员公平地比较不同的稀疏模型和获得现实的评估结果。另一个缺失的部分是，大多数先前的工作评估稀疏的检索模型在域内检索，即在一个单一的数据集: MS MARCO。然而，实际检索系统中的一个关键要求是模型能够很好地推广到看不见的域外，即零镜头检索任务。在这项工作中，我们提供了 SPRINT，一个基于 Pyserini 和 Lucene 的统一的 Python 工具包，支持一个评估神经稀疏检索的通用接口。该工具包目前包括五个内置模型: uniCOIL、 DeepImpact、 SPARTA、 TILDEv2和 SPLADEv2。用户还可以通过定义术语加权方法轻松添加自定义模型。使用我们的工具包，我们建立强大的和可重现的零拍稀疏检索基线跨公认的基准，BEIR。结果表明，在所有神经稀疏检索器中，SPLADEv2在 BEIR 上平均得分最高，为0.470 nDCG@10。在这项工作中，我们进一步揭示背后的原因，其性能增益。我们展示了 SPLADEv2在原始查询和文档之外使用大部分令牌产生稀疏表示，这对其性能提高通常是至关重要的，即其他稀疏对应方之间的一个限制。我们提供了我们的 SPRINT 工具包，模型和数据用于我们的实验公开在这里的 https://github.com/thakur-nandan/SPRINT。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SPRINT:+A+Unified+Toolkit+for+Evaluating+and+Demystifying+Zero-shot+Neural+Sparse+Retrieval)|0|
|[AToMiC: An Image/Text Retrieval Test Collection to Support Multimedia Content Creation](https://doi.org/10.1145/3539618.3591903)|JhengHong Yang, Carlos Lassance, Rafael Sampaio de Rezende, Krishna Srinivasan, Miriam Redi, Stéphane Clinchant, Jimmy Lin|Google Research, San Francisco, CA, USA; Wikimedia Foundation, London, United Kingdom; Naver Labs Europe, Grenoble, France; University of Waterloo, Waterloo, Canada|This paper presents the AToMiC (Authoring Tools for Multimedia Content) dataset, designed to advance research in image/text cross-modal retrieval. While vision-language pretrained transformers have led to significant improvements in retrieval effectiveness, existing research has relied on image-caption datasets that feature only simplistic image-text relationships and underspecified user models of retrieval tasks. To address the gap between these oversimplified settings and real-world applications for multimedia content creation, we introduce a new approach for building retrieval test collections. We leverage hierarchical structures and diverse domains of texts, styles, and types of images, as well as large-scale image-document associations embedded in Wikipedia. We formulate two tasks based on a realistic user model and validate our dataset through retrieval experiments using baseline models. AToMiC offers a testbed for scalable, diverse, and reproducible multimedia retrieval research. Finally, the dataset provides the basis for a dedicated track at the 2023 Text Retrieval Conference (TREC), and is publicly available at https://github.com/TREC-AToMiC/AToMiC.|本文介绍了 AToMiC (多媒体内容创作工具)数据集，旨在促进图像/文本跨模式检索的研究。虽然视觉语言预先训练的转换器已经导致检索效率的显著提高，但现有的研究依赖于图像标题数据集，这些数据集只具有简单的图像-文本关系和检索任务的用户模型不明确的特点。为了解决这些过于简化的设置与多媒体内容创建的现实应用程序之间的差距，我们引入了一种构建检索测试集的新方法。我们利用层次结构和文本、风格和图像类型的不同领域，以及嵌入在 Wikipedia 中的大规模图像-文档关联。我们基于一个真实的用户模型制定两个任务，并通过基线模型的检索实验验证我们的数据集。AToMiC 为可扩展、多样化和可重复的多媒体检索研究提供了一个试验平台。最后，该数据集为2023年文本检索会议(TREC)的专门跟踪提供了基础，并且可以在 https://github.com/TREC-atomic/atomic 上公开获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AToMiC:+An+Image/Text+Retrieval+Test+Collection+to+Support+Multimedia+Content+Creation)|0|
|[MobileRec: A Large Scale Dataset for Mobile Apps Recommendation](https://doi.org/10.1145/3539618.3591906)|Muhammad Hasan Maqbool, Umar Farooq, Adib Mosharrof, A. B. Siddique, Hassan Foroosh||Recommender systems have become ubiquitous in our digital lives, from recommending products on e-commerce websites to suggesting movies and music on streaming platforms. Existing recommendation datasets, such as Amazon Product Reviews and MovieLens, greatly facilitated the research and development of recommender systems in their respective domains. While the number of mobile users and applications (aka apps) has increased exponentially over the past decade, research in mobile app recommender systems has been significantly constrained, primarily due to the lack of high-quality benchmark datasets, as opposed to recommendations for products, movies, and news. To facilitate research for app recommendation systems, we introduce a large-scale dataset, called MobileRec. We constructed MobileRec from users' activity on the Google play store. MobileRec contains 19.3 million user interactions (i.e., user reviews on apps) with over 10K unique apps across 48 categories. MobileRec records the sequential activity of a total of 0.7 million distinct users. Each of these users has interacted with no fewer than five distinct apps, which stands in contrast to previous datasets on mobile apps that recorded only a single interaction per user. Furthermore, MobileRec presents users' ratings as well as sentiments on installed apps, and each app contains rich metadata such as app name, category, description, and overall rating, among others. We demonstrate that MobileRec can serve as an excellent testbed for app recommendation through a comparative study of several state-of-the-art recommendation approaches. The quantitative results can act as a baseline for other researchers to compare their results against. The MobileRec dataset is available at https://huggingface.co/datasets/recmeapp/mobilerec.|在我们的数字生活中，推荐系统已经变得无处不在，从在电子商务网站上推荐产品到在流媒体平台上推荐电影和音乐。现有的推荐数据集，如 Amazon Product Reviews 和 MovieLens，极大地促进了各自领域推荐系统的研究和开发。虽然移动用户和应用程序(又名应用程序)的数量在过去十年中呈指数级增长，但是对移动应用程序推荐系统的研究受到了严重的限制，主要是由于缺乏高质量的基准数据集，而不是对产品、电影和新闻的推荐。为了促进应用程序推荐系统的研究，我们引入了一个大规模的数据集，称为 MobileRec。我们根据用户在 Google 游戏商店的活动构建了 MobileRec。MobileRec 包含1930万用户交互(例如，应用程序上的用户评论) ，有超过10000个独特的应用程序，分为48个类别。MobileRec 记录了总共70万不同用户的连续活动。这些用户中的每一个都与不少于5个不同的应用程序进行了交互，这与之前的移动应用程序数据集形成了鲜明对比，之前的数据集只记录了每个用户的一次交互。此外，MobileRec 还提供了用户的评分以及对已安装应用程序的看法，每个应用程序都包含丰富的元数据，如应用程序名称、类别、描述和总体评分等。我们通过对几种最先进的推荐方法的比较研究，证明 MobileRec 可以作为一个优秀的应用程序推荐试验台。定量的结果可以作为其他研究人员比较他们的结果的基准。MobileRec 数据集可在 https://huggingface.co/datasets/recmeapp/MobileRec 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MobileRec:+A+Large+Scale+Dataset+for+Mobile+Apps+Recommendation)|0|
|[LibVQ: A Toolkit for Optimizing Vector Quantization and Efficient Neural Retrieval](https://doi.org/10.1145/3539618.3591799)|Chaofan Li, Zheng Liu, Shitao Xiao, Yingxia Shao, Defu Lian, Zhao Cao|Huawei, Beijing, China|Vector quantization is one of the critical techniques which enables dense retrieval for realtime applications. The recent study shows that vanilla vector quantization methods, like those implemented by FAISS [8], are lossy and prone to limited retrieval performances when large acceleration ratios are needed [14, 16, 18]. Besides, there have also been multiple algorithms which make the retriever and VQ better collaborated to alleviate such a loss. On top of these progresses, we develop LibVQ, which optimizes vector quantization for efficient dense retrieval. Our toolkit is highlighted for three advantages. 1. Effectiveness. The retrieval quality can be substantially improved over the vanilla implementations of VQ. 2. Simplicity. The optimization can be conducted in a lowcode fashion, and the optimization results can be easily loaded to ANN indexes to support downstream applications. 3. Universality. The optimization is agnostic to the embedding's learning process, and may accommodate different input conditions and ANN back-ends with little modification of the workflow. LibVQ may also support rich applications beyond dense retrieval, e.g., embedding compression, topic modeling, and de-duplication. In this demo, we provide comprehensive hand-on examples and evaluations for LibVQ. The toolkit is publicly released at: https://github.com/staoxiao/LibVQ/tree/demo.|向量量化检索是实现实时应用密集检索的关键技术之一。最近的研究表明，香草向量量化方法，就像 FAISS [8]实现的那样，是有损耗的，当需要大的加速比时，容易受到有限的检索性能的限制[14,16,18]。此外，还有多种算法可以使检索器和 VQ 更好地协作，以减轻这种损失。在这些进展的基础上，我们开发了 libvQ，它可以优化向量量化，实现高效的密集检索。我们的工具包突出显示了三个优点。1.有效性。与普通的 VQ 实现相比，可以大大提高检索质量。2.简单。优化可以以低代码的方式进行，优化结果可以很容易地加载到 ANN 索引以支持下游应用程序。3.普遍性。该优化算法对嵌入式的学习过程是不可知的，可以适应不同的输入条件和神经网络后端，对工作流的修改很少。除了密集检索之外，LibVQ 还可以支持丰富的应用程序，例如嵌入式压缩、主题建模和去复制。在这个演示中，我们为 LibVQ 提供了全面的实际例子和评估。该工具包在以下 https://github.com/staoxiao/libvq/tree/demo 公开发布:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LibVQ:+A+Toolkit+for+Optimizing+Vector+Quantization+and+Efficient+Neural+Retrieval)|0|
|[VoMBaT: A Tool for Visualising Evaluation Measure Behaviour in High-Recall Search Tasks](https://doi.org/10.1145/3539618.3591802)|Wojciech Kusa, Aldo Lipani, Petr Knoth, Allan Hanbury|The Open University, Milton Keynes, United Kingdom; TU Wien, Vienna, Austria; University College London, London, United Kingdom|The objective of High-Recall Information Retrieval (HRIR) is to retrieve as many relevant documents as possible for a given search topic. One approach to HRIR is Technology-Assisted Review (TAR), which uses information retrieval and machine learning techniques to aid the review of large document collections. TAR systems are commonly used in legal eDiscovery and systematic literature reviews. Successful TAR systems are able to find the majority of relevant documents using the least number of assessments. Commonly used retrospective evaluation assumes that the system achieves a specific, fixed recall level first, and then measures the precision or work saved (e.g., precision at r% recall). This approach can cause problems related to understanding the behaviour of evaluation measures in a fixed recall setting. It is also problematic when estimating time and money savings during technology-assisted reviews. This paper presents a new visual analytics tool to explore the dynamics of evaluation measures depending on recall level. We implemented 18 evaluation measures based on the confusion matrix terms, both from general IR tasks and specific to TAR. The tool allows for a comparison of the behaviour of these measures in a fixed recall evaluation setting. It can also simulate savings in time and money and a count of manual vs automatic assessments for different datasets depending on the model quality. The tool is open-source, and the demo is available under the following URL: https://vombat.streamlit.app.|高召回信息检索(HRIR)的目的是为一个给定的搜索主题检索尽可能多的相关文档。HRIR 的一种方法是技术辅助审查(technology-Assistant Review，TAR) ，它使用信息检索和机器学习技术来帮助审查大型文档集合。TAR 系统通常用于法律电子发现和系统文献综述。成功的第三次评估系统能够使用最少的评估数量找到大多数相关文件。常用的回顾性评估假设系统首先达到一个特定的、固定的召回水平，然后测量精度或节省的工作(例如，在 r% 召回时的精度)。这种方法可能会导致与理解固定召回环境下评估措施的行为有关的问题。在技术辅助审查期间估计节省的时间和金钱也是有问题的。本文提出了一种新的可视化分析工具，以探索评估措施的动态性取决于召回水平。我们实施了18项基于混淆矩阵的评估措施，包括一般 IR 任务和特定于 TAR 的评估措施。该工具允许在一个固定的召回评估环境中比较这些措施的行为。它还可以模拟节省时间和金钱，以及根据模型质量对不同数据集进行人工评估和自动评估的次数。该工具是开源的，演示可以通过以下网址获得:  https://vombat.streamlit.app。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=VoMBaT:+A+Tool+for+Visualising+Evaluation+Measure+Behaviour+in+High-Recall+Search+Tasks)|0|
|[Searching the ACL Anthology with Math Formulas and Text](https://doi.org/10.1145/3539618.3591803)|Bryan Amador, Matt Langsenkamp, Abhisek Dey, Ayush Kumar Shah, Richard Zanibbi|Rochester Institute of Technology, Rochester, NY, USA|Mathematical notation is a key analytical resource for science and technology. Unfortunately, current math-aware search engines require LATEX or template palettes to construct formulas, which can be challenging for non-experts. Also, their indexed collections are primarily web pages where formulas are represented explicitly in machine-readable formats (e.g., LATEX, Presentation MathML). The new MathDeck system searches PDF documents in a portion of the ACL Anthology using both formulas and text, and shows matched words and formulas along with other extracted formulas in-context. In PDF, formulas are not demarcated: a new indexing module extracts formulas using PDF vector graphics information and computer vision techniques. For non-expert users and visual editing, a central design feature of MathDeck's interface is formula 'chips' usable in formula creation, search, reuse, and annotation with titles and descriptions in cards. For experts, LATEX is supported in the text query box and the visual formula editor. MathDeck is open-source, and our demo is available online.|数学符号是科学和技术的重要分析资源。不幸的是，目前的数学感知搜索引擎需要 LATEX 或模板调色板来构造公式，这对非专业人士可能是一个挑战。此外，他们的索引集合主要是网页，其中公式以机器可读的格式显式表示(例如，LATEX，PresentationMathML)。新的 MathDeck 系统使用公式和文本在 ACL 选集的一部分中搜索 PDF 文档，并在上下文中显示匹配的单词和公式以及其他提取的公式。在 PDF 中，公式没有标定: 一个新的索引模块利用 PDF 矢量图形信息和计算机视觉技术提取公式。对于非专业用户和可视化编辑，MathDeck 界面的一个核心设计特征是可用于公式创建、搜索、重用和标题和卡片描述注释的公式“芯片”。对于专家来说，在文本查询框和可视化公式编辑器中支持 LATEX。MathDeck 是开源的，我们的演示可以在线获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Searching+the+ACL+Anthology+with+Math+Formulas+and+Text)|0|
|[Tevatron: An Efficient and Flexible Toolkit for Neural Retrieval](https://doi.org/10.1145/3539618.3591805)|Luyu Gao, Xueguang Ma, Jimmy Lin, Jamie Callan|University of Waterloo, Waterloo, ON, Canada; Carnegie Mellon University, Pittsburgh, PA, USA|Recent rapid advances in deep pre-trained language models and the introduction of large datasets have powered research in embedding-based neural retrieval. While many excellent research papers have emerged, most of them come with their own implementations, which are typically optimized for some particular research goals instead of efficiency or code organization. In this paper, we introduce Tevatron, a neural retrieval toolkit that is optimized for efficiency, flexibility, and code simplicity. Tevatron enables model training and evaluation for a variety of ranking components such as dense retrievers, sparse retrievers, and rerankers. It also provides a standardized pipeline that includes text processing, model training, corpus/query encoding, and search. In addition, Tevatron incorporates well-studied methods for improving retriever effectiveness such as hard negative mining and knowledge distillation. We provide an overview of Tevatron in this paper, demonstrating its effectiveness and efficiency on multiple IR and QA datasets. We highlight Tevatron's flexible design, which enables easy generalization across datasets, model architectures, and accelerator platforms (GPUs and TPUs). Overall, we believe that Tevatron can serve as a solid software foundation for research on neural retrieval systems, including their design, modeling, and optimization.|深度预训练语言模型的快速发展和大数据集的引入为基于嵌入的神经检索研究提供了动力。虽然已经出现了许多优秀的研究论文，但大多数论文都有自己的实现，它们通常是针对某些特定的研究目标而优化的，而不是针对效率或代码组织。在本文中，我们介绍了 Tevatron，一个针对效率、灵活性和代码简单性进行优化的神经检索工具包。Tevatron 支持对各种排序组件进行模型训练和评估，例如稠密检索器、稀疏检索器和重新排序器。它还提供了一个标准化的流水线，包括文本处理、模型训练、语料库/查询编码和搜索。此外，Tevatron 还采用了经过充分研究的方法来提高回收效率，如硬负面挖掘和知识提取。我们在本文中提供了 Tevatron 的概述，展示了它在多个 IR 和 QA 数据集上的有效性和效率。我们着重介绍 Tevatron 的灵活设计，它使得跨数据集、模型架构和加速器平台(GPU 和 TPU)的通用化变得容易。总的来说，我们相信 Tevatron 可以作为研究神经检索系统的坚实的软件基础，包括它们的设计、建模和优化。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tevatron:+An+Efficient+and+Flexible+Toolkit+for+Neural+Retrieval)|0|
|[SciHarvester: Searching Scientific Documents for Numerical Values](https://doi.org/10.1145/3539618.3591808)|Maciej Rybinski, Stephen Wan, Sarvnaz Karimi, Cécile Paris, Brian Jin, Neil I. Huth, Peter J. Thorburn, Dean P. Holzworth|CSIRO, Brisbane, VIC, Australia; CSIRO, Sydney, NSW, Australia; CSIRO, Toowoomba, Australia|A challenge for search technologies is to support scientific literature surveys that present overviews of the reported numerical values documented for specific physical properties. We present SciHarvester, a system tailored to address this problem for agronomic science. It provides an interface to search PubAg documents, allowing complex queries involving restrictions on numerical values. SciHarvester identifies relevant documents and generates overview of reported parameter values. The system allows interrogation of the results to explain the system's performance. Our evaluations demonstrate the promise of incorporating information extraction techniques with the use of neural scoring mechanisms.|搜索技术面临的一个挑战是支持科学文献调查，这些调查概述了为具体物理特性记录的报告数值。我们介绍 SciHarvester，一个专门为农学科学解决这个问题的系统。它提供了一个用于搜索 PubAg 文档的界面，允许包含数值限制的复杂查询。SciHarvester 识别相关文档并生成报告参数值的概述。系统允许询问结果来解释系统的性能。我们的评估显示了将信息抽取技术与神经评分机制相结合的前景。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SciHarvester:+Searching+Scientific+Documents+for+Numerical+Values)|0|
|[A Retrieval System for Images and Videos based on Aesthetic Assessment of Visuals](https://doi.org/10.1145/3539618.3591817)|Daniel Vera Nieto, Saikishore Kalloori, Fabio Zund, Clara FernandezLabrador, Marc Willhaus, Severin Klingler, Markus H. Gross|ETH Zürich, Zurich, Switzerland|Attractive images or videos are the visual backbones of journalism and social media to gain the user's attention. From trailers to teaser images to image galleries, appealing visuals have only grown in importance over the years. However, selecting eye-catching shots from a video or the perfect image from large image collections is a challenging and time-consuming task. We present our tool that can assess image and video content from an aesthetic standpoint. We discovered that it is possible to perform such an assessment by combining expert knowledge with data-driven information. We combine the relevant aesthetic features and machine learning algorithms into an aesthetics retrieval system, which enables users to sort uploaded visuals based on an aesthetic score and interact with additional photographic, cinematic, and person-specific features.|吸引人的图片或视频是新闻业和社交媒体吸引用户注意力的视觉支柱。从预告片到预告图片再到图片画廊，吸引人的视觉效果在这些年里变得越来越重要。然而，从视频中选择引人注目的镜头或从大型图像集合中选择完美的图像是一项具有挑战性和耗时的任务。我们提出了我们的工具，可以评估图像和视频内容从美学的立场。我们发现，通过将专家知识与数据驱动的信息相结合来进行这种评估是可能的。我们将相关的美学特征和机器学习算法结合到一个美学检索系统中，使用户能够根据美学评分对上传的图像进行分类，并与其他照片、电影和特定人物的特征进行交互。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Retrieval+System+for+Images+and+Videos+based+on+Aesthetic+Assessment+of+Visuals)|0|
|[XpmIR: A Modular Library for Learning to Rank and Neural IR Experiments](https://doi.org/10.1145/3539618.3591818)|Yuxuan Zong, Benjamin Piwowarski|Sorbonne Université, CNRS, ISIR, Paris, France; CNRS, Sorbonne Université, ISIR, Paris, France|During past years, several frameworks for (Neural) Information Retrieval have been proposed. However, while they allow reproducing already published results, it is still very hard to re-use some parts of the learning pipelines, such as for instance the pre-training, sampling strategy, or a loss in newly developed models. It is also difficult to use new training techniques with old models, which makes it more difficult to assess the usefulness of ideas on various neural IR models. This slows the adoption of new techniques, and in turn, the development of the IR field. In this paper, we present XpmIR, a Python library defining a reusable set of experimental components. The library already contains state-of-the-art models and indexation techniques and is integrated with the HuggingFace hub.|在过去的几年中，人们提出了几种神经信息检索的框架。然而，尽管它们允许复制已经发表的结果，但仍然很难重复使用学习管道的某些部分，例如预训练、抽样策略或新开发的模型中的损失。新的训练技术也很难与旧的模型一起使用，这使得评估各种神经 IR 模型思想的有用性变得更加困难。这就减缓了新技术的采用，进而也延缓了红外领域的发展。在本文中，我们介绍了 XpmIR，它是一个 Python 库，定义了一组可重用的实验组件。该库已经包含了最先进的模型和索引技术，并与 HuggingFace 集成在一起。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=XpmIR:+A+Modular+Library+for+Learning+to+Rank+and+Neural+IR+Experiments)|0|
|[Searching for Reliable Facts over a Medical Knowledge Base](https://doi.org/10.1145/3539618.3591822)|Fabio Giachelle, Stefano Marchesin, Gianmaria Silvello, Omar Alonso|University of Padua, Padua, Italy; Amazon, Santa Clara, CA, USA|This work presents CoreKB, a Web platform for searching reliable facts over gene expression-cancer associations Knowledge Base (KB). It provides search capabilities over an RDF graph using natural language queries, structured facets, and autocomplete. CoreKB is designed to be intuitive and easy to use for healthcare professionals, medical researchers, and clinicians. The system offers the user a comprehensive overview of the scientific evidence supporting a medical fact. It provides a quantitative comparison between the possible gene-cancer associations a particular fact can reflect.|这项工作介绍了 CoreKB，一个在基因表达-癌症关联知识库(KB)上搜索可靠事实的网络平台。它通过使用自然语言查询、结构化方面和自动完成在 RDF 图上提供搜索功能。CoreKB 的设计是直观和易于使用的医疗保健专业人员，医学研究人员和临床医生。该系统为用户提供了支持医学事实的科学证据的全面概述。它提供了一个可能的基因之间的定量比较癌症的关联，一个特定的事实可以反映。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Searching+for+Reliable+Facts+over+a+Medical+Knowledge+Base)|0|
|[Interactive Recommendation System for Meituan Waimai](https://doi.org/10.1145/3539618.3591830)|Chen Ji, Yacheng Li, Rui Li, Fei Jiang, Xiang Li, Wei Lin, Chenglong Zhang, Wei Wang, Shuyang Wang|Independent, Beijing, China; Meituan, Beijing, China|As the largest local retail & instant delivery platform in China, Meituan Waimai has deployed a personalized recommender system on server and recommend nearby stores to users through APP homepage. To capture real-time intention of users and flexibly adjust the recommendation results on the homepage, we further add an interactive recommender system. The existing interactive recommender systems in the industry mainly capture intention of users based on their feedback on a specific UI of questions. However, we find that it will undermine use fluency and increase use complexity by rashly inserting a new question UI when users browse the homepage. Therefore, we develop an Embedded Interactive Recommender System (EIRS) that directly infers users' intention according to their click behaviors on the homepage and dynamically inserts a new recommendation result into the homepage1. To demonstrate the effectiveness of EIRS, we conduct systematic online A/B Tests, where click-through & conversion rate of the inserted EIRS result is 132% higher than that of the initial result on the homepage, and the overall gross merchandise volume is effectively enhanced by 0.43%.|作为中国最大的本地零售和即时送货平台，美团外卖已经在服务器上部署了个性化的推荐系统，并通过 APP 主页向用户推荐附近的商店。为了捕捉用户的实时意图，并灵活调整网页上的推荐结果，我们进一步加入了互动推荐系统。业界现有的交互式推荐系统主要根据用户对特定用户界面问题的反馈来捕捉用户的意图。然而，我们发现，当用户浏览主页时，它会草率地插入一个新的问题用户界面，从而破坏使用的流畅性并增加使用的复杂性。因此，我们开发了一个嵌入式交互式推荐系统(eIRS) ，它可以根据用户在主页上的点击行为直接推断用户的意图，并动态地将一个新的推荐结果插入到主页1中。为了证明 EIRS 的有效性，我们进行了有系统的网上 A/B 测试，其中插入的 EIRS 结果的点击率和转换率比首页上的初步结果高出132% ，而整体商品销售量有效地提高了0.43% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interactive+Recommendation+System+for+Meituan+Waimai)|0|
|[Practice and Challenges in Building a Business-oriented Search Engine Quality Metric](https://doi.org/10.1145/3539618.3591841)|Nuo Chen, Donghyun Park, Hyungae Park, Kijun Choi, Tetsuya Sakai, Jinyoung Kim|Naver Corp., Seoul, Republic of Korea; Waseda University & Naver Corp., Tokyo, Japan; Waseda University, Japan, Japan; Naver Corp., Belmont, CA, USA|One of the most challenging aspects of operating a large-scale web search engine is to accurately evaluate and monitor the search engine's result quality regardless of search types. From a business perspective, in the face of such challenges, it is important to establish a universal search quality metric that can be easily understood by the entire organisation. In this paper, we introduce a model-based quality metric using Explainable Boosting Machine as the classifier and online user behaviour signals as features to predict search quality. The proposed metric takes into account a variety of search types and has good interpretability. To examine the performance of the metric, we constructed a large dataset of user behaviour on search engine results pages (SERPs) with SERP quality ratings from professional annotators. We compared the performance of the model in our metric to those of other black-box machine learning models on the dataset. We also share a few experiences within our company for the org-wide adoption of this metric relevant to metric design.|运行大规模网络搜索引擎最具挑战性的方面之一是准确评估和监控搜索引擎的结果质量，而不管搜索类型如何。从商业的角度来看，面对这些挑战，建立一个通用的搜索质量度量标准是非常重要的，它可以很容易地被整个组织所理解。本文提出了一种基于模型的质量度量方法，该方法以可解释增强机作为分类器，以在线用户行为信号作为特征来预测搜索质量。所提出的度量标准考虑到各种搜索类型，具有良好的可解释性。为了检验这个指标的性能，我们构建了一个大型的搜索引擎结果页面(SERP)用户行为数据集，其中包括来自专业注释者的 SERP 质量评级。我们将我们度量中的模型的性能与数据集上其他黑盒机器学习模型的性能进行了比较。我们还在公司内部分享了一些与度量设计相关的度量在组织范围内采用的经验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Practice+and+Challenges+in+Building+a+Business-oriented+Search+Engine+Quality+Metric)|0|
|[Building a Graph-Based Patent Search Engine](https://doi.org/10.1145/3539618.3591842)|Sebastian Björkqvist, Juho Kallio|IPRally Technologies Oy, Helsinki, Finland|Performing prior art searches is an essential step in both patent drafting and invalidation. The task is challenging due to the large number of existing patent documents and the domain knowledge required to analyze the documents. We present a graph-based patent search engine that tries to mimic the work done by a professional patent examiner. Each patent document is converted to a graph that describes the parts of the invention and the relations between the parts. The search engine is powered by a graph neural network that learns to find prior art by using novelty citation data from patent office search reports where citations are compiled by human patent examiners. We show that a graph-based approach is an efficient way to perform searches on technical documents and demonstrate it in the context of patent searching.|进行现有技术检索是专利起草和宣告无效的一个重要步骤。这项任务是具有挑战性的，因为大量的现有专利文件和领域知识需要分析的文件。我们提出了一个基于图表的专利搜索引擎，试图模仿专业专利审查员所做的工作。每个专利文件被转换成描述发明的各部分以及各部分之间的关系的图形。搜索引擎由一个图形神经网络驱动，该网络通过使用专利局搜索报告中的新颖引文数据来学习发现先前的技术，这些引文是由人类专利审查员汇编的。我们证明了基于图的方法是一种有效的方法来执行对技术文档的检索，并在专利检索的背景下进行论证。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Building+a+Graph-Based+Patent+Search+Engine)|0|
|[Graph Enhanced BERT for Query Understanding](https://doi.org/10.1145/3539618.3591845)|Juanhui Li, Wei Zeng, Suqi Cheng, Yao Ma, Jiliang Tang, Shuaiqiang Wang, Dawei Yin|; Michigan State University, East Lansing, USA; Baidu Inc., Beijing, China|Query understanding plays a key role in exploring users' search intents and facilitating users to locate their most desired information. However, it is inherently challenging since it needs to capture semantic information from short and ambiguous queries and often requires massive task-specific labeled data. In recent years, pre-trained language models (PLMs) have advanced various natural language processing tasks because they can extract general semantic information from large-scale corpora. Therefore, there are unprecedented opportunities to adopt PLMs for query understanding. However, there is a gap between the goal of query understanding and existing pre-training strategies -- the goal of query understanding is to boost search performance while existing strategies rarely consider this goal. Thus, directly applying them to query understanding is sub-optimal. On the other hand, search logs contain user clicks between queries and urls that provide rich users' search behavioral information on queries beyond their content. Therefore, in this paper, we aim to fill this gap by exploring search logs. In particular, to incorporate search logs into pre-training, we first construct a query graph where nodes are queries and two queries are connected if they lead to clicks on the same urls. Then we propose a novel graph-enhanced pre-training framework, GE-BERT, which can leverage both query content and the query graph. In other words, GE-BERT can capture both the semantic information and the users' search behavioral information of queries. Extensive experiments on various query understanding tasks have demonstrated the effectiveness of the proposed framework.|查询理解在探索用户的搜索意图和帮助用户定位最需要的信息方面起着关键作用。然而，这本身就具有挑战性，因为它需要从短而模糊的查询中捕获语义信息，并且经常需要大量特定于任务的标记数据。近年来，预训练语言模型(pre-training language model，PLM)已经推进了各种自然语言处理任务，因为它们可以从大规模语料库中提取一般语义信息。因此，采用 PLM 来理解查询有着前所未有的机会。然而，在查询理解的目标和现有的预训练策略之间存在差距——查询理解的目标是提高搜索性能，而现有的策略很少考虑这个目标。因此，直接将它们应用于查询理解是次优的。另一方面，搜索日志包含用户在查询和 URL 之间的点击，这些点击提供了丰富用户在其内容之外的查询上的搜索行为信息。因此，在本文中，我们的目标是通过探索搜索日志来填补这一空白。特别是，为了将搜索日志合并到预训练中，我们首先构造一个查询图，其中节点是查询，如果两个查询导致点击相同的网址，则两个查询是连接的。然后提出了一种新的图增强预训练框架 GE-BERT，该框架可以同时利用查询内容和查询图。换句话说，GE-BERT 可以同时捕捉语义信息和用户的搜索行为信息。在各种查询理解任务上的大量实验证明了该框架的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Enhanced+BERT+for+Query+Understanding)|0|
|[Neural Methods for Cross-Language Information Retrieval](https://doi.org/10.1145/3539618.3594244)|Eugene Yang, Dawn J. Lawrie, James Mayfield, Suraj Nair, Douglas W. Oard|Johns Hopkins University, Baltimore, MD, USA; University of Maryland, College Park, MD, USA|This half day tutorial introduces the participant to the basic concepts underlying neural Cross-Language Information Retrieval (CLIR). It discusses the most common algorithmic approaches to CLIR, focusing on modern neural methods; the history of CLIR; where to find and how to use CLIR training collections, test collections and baseline systems; how CLIR training and test collections are constructed; and open research questions in CLIR.|这个半天的教程向参与者介绍神经跨语检索的基本概念。它讨论了 CLIR 最常见的算法方法，重点是现代神经学方法; CLIR 的历史; 在哪里找到和如何使用 CLIR 训练集、测试集和基线系统; CLIR 训练集和测试集是如何构建的; 以及 CLIR 中的开放研究问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Methods+for+Cross-Language+Information+Retrieval)|0|
|[Causal Recommendation: Progresses and Future Directions](https://doi.org/10.1145/3539618.3594245)|Wenjie Wang, Yang Zhang, Haoxuan Li, Peng Wu, Fuli Feng, Xiangnan He|University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Singapore, China; University of Science and Technology of China, Beijing, China|Data-driven recommender systems have demonstrated great success in various Web applications owing to the extraordinary ability of machine learning models to recognize patterns (ie correlation) from users' behaviors. However, they still suffer from several issues such as biases and unfairness due to spurious correlations. Considering the causal mechanism behind data can avoid the influences of such spurious correlations. In this light, embracing causal recommender modeling is an exciting and promising direction. In this tutorial, we aim to introduce the key concepts in causality and provide a systemic review of existing work on causal recommendation. We will introduce existing methods from two different causal frameworks --- the potential outcome (PO) framework and the structural causal model (SCM). We will give examples and discussions regarding how to utilize different causal tools under these two frameworks to model and solve problems in recommendation. Moreover, we will summarize and compare the paradigms of PO-based and SCM-based recommendation. Besides, we identify some open challenges and potential future directions for this area. We hope this tutorial could stimulate more ideas on this topic and facilitate the development of causality-aware recommender systems.|由于机器学习模型能够从用户行为中识别模式(即相关性) ，数据驱动的推荐系统在各种 Web 应用程序中取得了巨大的成功。然而，由于虚假的相关性，他们仍然受到一些问题的困扰，如偏见和不公平。考虑数据背后的因果机制可以避免这种虚假相关性的影响。从这个角度来看，采用因果推荐建模是一个令人兴奋和有前途的方向。在本教程中，我们的目的是介绍因果关系的关键概念，并提供一个系统的回顾，现有的工作在因果推荐。我们将介绍来自两个不同因果框架的现有方法——潜在结果(PO)框架和结构因果模型(SCM)。我们将举例讨论如何在这两个框架下利用不同的因果工具来建模和解决推荐中的问题。此外，我们还将总结和比较基于 PO 和基于 SCM 的推荐模式。此外，我们确定了这一领域的一些开放性挑战和潜在的未来方向。我们希望本教程可以激发更多关于这个主题的想法，并促进因果关系推荐系统的开发。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causal+Recommendation:+Progresses+and+Future+Directions)|0|
|[Neuro-Symbolic Representations for Information Retrieval](https://doi.org/10.1145/3539618.3594246)|Laura Dietz, Hannah Bast, Shubham Chatterjee, Jeffrey Dalton, JianYun Nie, Rodrigo Frassetto Nogueira|University of Freiburg, Freiburg, Germany; University of Glasgow, Glasgow, United Kingdom; State University of Campinas, Campinas, Brazil; University of Montreal, Montreal, PQ, Canada; University of New Hampshire, Durham, NH, USA|This tutorial will provide an overview of recent advances on neuro-symbolic approaches for information retrieval. A decade ago, knowledge graphs and semantic annotations technology led to active research on how to best leverage symbolic knowledge. At the same time, neural methods have demonstrated to be versatile and highly effective. From a neural network perspective, the same representation approach can service document ranking or knowledge graph reasoning. End-to-end training allows to optimize complex methods for downstream tasks. We are at the point where both the symbolic and the neural research advances are coalescing into neuro-symbolic approaches. The underlying research questions are how to best combine symbolic and neural approaches, what kind of symbolic/neural approaches are most suitable for which use case, and how to best integrate both ideas to advance the state of the art in information retrieval. Materials are available online: https://github.com/laura-dietz/neurosymbolic-representations-for-IR|这个教程将提供一个关于信息检索神经符号方法的最新进展的概述。十年前，知识图表和语义注释技术导致了对如何最好地利用符号知识的积极研究。与此同时，神经学方法已被证明是通用的和高度有效的。从神经网络的角度来看，同样的表示方法可以服务文档排序或知识图推理。端到端培训允许为下游任务优化复杂的方法。我们正处于这样一个时刻，符号学和神经学的研究进展正在结合成神经符号学的方法。潜在的研究问题是如何最好地结合符号和神经方法，什么样的符号/神经方法最适合哪种用例，以及如何最好地整合这两种思想以提高信息检索的艺术水平。资料可在网上查阅:  https://github.com/laura-dietz/neurosymbolic-representations-for-ir|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neuro-Symbolic+Representations+for+Information+Retrieval)|0|
|[Explainable Information Retrieval](https://doi.org/10.1145/3539618.3594249)|Avishek Anand, Procheta Sen, Sourav Saha, Manisha Verma, Mandar Mitra|Delft University of Technology, Delft, Netherlands; Indian Statistical Institute, Kolkata, India; Amazon New York, New York, USA; University of Liverpool, Liverpool, United Kingdom|This tutorial presents explainable information retrieval (ExIR), an emerging area focused on fostering responsible and trustworthy deployment of machine learning systems in the context of information retrieval. As the field has rapidly evolved in the past 4-5 years, numerous approaches have been proposed that focus on different access modes, stakeholders, and model development stages. This tutorial aims to introduce IR-centric notions, classification, and evaluation styles in ExIR, while focusing on IR-specific tasks such as ranking, text classification, and learning-to-rank systems. We will delve into method families and their adaptations to IR, extensively covering post-hoc methods, axiomatic and probing approaches, and recent advances in interpretability-by-design approaches. We will also discuss ExIR applications for different stakeholders, such as researchers, practitioners, and end-users, in contexts like web search, patent and legal search, and high-stakes decision-making tasks. To facilitate practical understanding, we will provide a hands-on session on applying ExIR methods, reducing the entry barrier for students, researchers, and practitioners alike.|本教程介绍了可解释信息检索(exIR) ，这是一个新兴的领域，专注于在信息检索环境中培养机器学习系统的负责任和可靠的部署。随着该领域在过去4-5年中的迅速发展，已经提出了许多方法，重点关注不同的访问模式、涉众和模型开发阶段。本教程旨在介绍 ExIR 中以 IR 为中心的概念、分类和评估风格，同时关注特定于 IR 的任务，如排名、文本分类和学习到排名系统。我们将深入研究方法族及其对 IR 的适应性，广泛地涵盖事后方法、公理化方法和探索性方法，以及设计可解释性方法的最新进展。我们还将讨论不同利益相关者(如研究人员、从业人员和最终用户)在网络搜索、专利和法律搜索以及高风险决策任务等上下文中的 ExIR 应用程序。为了便于实际理解，我们将提供一个应用 ExIR 方法的实践会议，减少学生，研究人员和从业人员的进入障碍。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explainable+Information+Retrieval)|0|
|[Proactive Conversational Agents in the Post-ChatGPT World](https://doi.org/10.1145/3539618.3594250)|Lizi Liao, Grace Hui Yang, Chirag Shah|Singapore Management University, Singapore, Singapore; Georgetown University, Washington DC, USA; University of Washington, Seattle, USA|ChatGPT and similar large language model (LLM) based conversational agents have brought shock waves to the research world. Although astonished by their human-like performance, we find they share a significant weakness with many other existing conversational agents in that they all take a passive approach in responding to user queries. This limits their capacity to understand the users and the task better and to offer recommendations based on a broader context than a given conversation. Proactiveness is still missing in these agents, including their ability to initiate a conversation, shift topics, or offer recommendations that take into account a more extensive context. To address this limitation, this tutorial reviews methods for equipping conversational agents with proactive interaction abilities. The full-day tutorial is divided into four parts, including multiple interactive exercises. We will begin the tutorial with an interactive exercise and cover the design of existing conversational systems architecture and challenges. The content includes coverage of LLM-based recent advancements such as ChatGPT and Bard, along with reinforcement learning with human feedback (RLHF) technique. Then we will introduce the concept of proactive conversation agents and preset recent advancements in proactiveness of conversational agents, including actively driving conversations by asking questions, topic shifting, and methods that support strategic planning of conversation. Next, we will discuss important issues in conversational responses' quality control, including safety, appropriateness, language detoxication, hallucination, and alignment. Lastly, we will launch another interactive exercise and discussion with the audience to arrive at concluding remarks, prospecting open challenges and new directions. By exploring new techniques for enhancing conversational agents' proactive behavior to improve user engagement, this tutorial aims to help researchers and practitioners develop more effective conversational agents that can better understand and respond to user needs proactively and safely.|基于 ChatGPT 和类似大语言模型(LLM)的会话代理给研究领域带来了冲击波。虽然它们的人性化表现令人惊讶，但我们发现它们与许多其他现有的会话代理有一个明显的弱点，即它们都采用被动的方式来响应用户查询。这限制了他们更好地理解用户和任务的能力，也限制了他们基于比特定对话更广泛的背景提供建议的能力。这些代理仍然缺乏主动性，包括他们发起对话、转移话题或提供考虑到更广泛背景的建议的能力。为了解决这个限制，本教程回顾了使会话代理具有主动交互能力的方法。全天教程分为四个部分，包括多个互动练习。我们将以一个交互式练习开始本教程，并讨论现有会话系统体系结构和挑战的设计。内容包括基于 LLM 的最新进展，如 ChatGPT 和巴德，以及人类反馈(rlHF)技术的强化学习。然后，我们将介绍主动会话代理的概念，并预设会话代理主动性的最新进展，包括通过提问积极推动会话，话题转移，以及支持会话策略规划的方法。接下来，我们将讨论会话反应质量控制中的重要问题，包括安全性、恰当性、语言排毒、幻觉和对齐。最后，我们会展开另一项互动活动，与听众进行讨论，以达致总结意见、探讨公开挑战和新方向。通过探索增强会话代理主动行为的新技术，以提高用户参与度，本教程旨在帮助研究人员和从业人员开发更有效的会话代理，可以更好地理解和回应用户的需求，主动和安全。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Proactive+Conversational+Agents+in+the+Post-ChatGPT+World)|0|
|[FLIRT: Federated Learning for Information Retrieval](https://doi.org/10.1145/3539618.3591926)|Fabio Pinelli, Gabriele Tolomei, Giovanni Trappolini|Sapienza University of Rome, Rome, Italy; IMT Lucca, Lucca, Italy|A wide range of core information retrieval (IR) tasks, such as searching, ranking, and filtering, to name a few, have seen tremendous improvements thanks to machine learning (ML) and artificial intelligence (AI). The traditional centralized approach to training AI/ML models is still predominant: large volumes of data generated by end users must be transferred from their origins and shared with remote locations for processing. However, this centralized paradigm suffers from significant privacy issues and does not take full advantage of the computing power of client devices like modern smartphones. A possible answer to this need is provided by federated learning (FL), which enables collaborative training of predictive models among a set of cooperating edge devices without disclosing any private local data. Unfortunately, FL is still far from being fully exploited in the IR ecosystem. In this workshop proposal, we have the ambition to start filling this gap. More specifically, the first workshop on ''Federated Learning for Information ReTrieval'' (FLIRT) is willing to provide an open forum for researchers and practitioners where they can exchange ideas, identify key challenges, and define the roadmap toward a successful application of FL in the broad IR area.|由于机器学习(ML)和人工智能(AI)的出现，搜索、排名和过滤等核心信息检索(IR)任务都得到了巨大的改善。传统的集中式人工智能/机器学习模型培训方法仍然占主导地位: 最终用户生成的大量数据必须从他们的来源转移，并与远程位置共享进行处理。然而，这种集中式模式存在严重的隐私问题，并且没有充分利用现代智能手机等客户端设备的计算能力。联邦学习(FL)为这一需求提供了一个可能的答案，它能够在一组协作的边缘设备之间协作训练预测模型，而不需要公开任何私有的本地数据。不幸的是，FL 在红外生态系统中还远远没有得到充分利用。在本次研讨会的提案中，我们有雄心开始填补这一空白。更具体地说，第一个关于“信息检索联合学习”的研讨会(FLIRT)愿意为研究人员和实践者提供一个开放的论坛，在这里他们可以交流想法，确定关键的挑战，并确定在广泛的信息共享领域成功应用联合学习的路线图。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FLIRT:+Federated+Learning+for+Information+Retrieval)|0|
|[Quantifying and Advancing Information Retrieval System Explainability](https://doi.org/10.1145/3539618.3591792)|Catherine Chen|Brown University, Providence, RI, USA|As information retrieval (IR) systems, such as search engines and conversational agents, become ubiquitous in various domains, the need for transparent and explainable systems grows to ensure accountability, fairness, and unbiased results. Despite many recent advances toward explainable AI and IR techniques, there is no consensus on what it means for a system to be explainable. Although a growing body of literature suggests that explainability is comprised of multiple subfactors [2, 5, 6], virtually all existing approaches treat it as a singular notion. Additionally, while neural retrieval models (NRMs) have become popular for their ability to achieve high performance[3, 4, 7, 8], research on the explainability of NRMs has been largely unexplored until recent years. Numerous questions remain unanswered regarding the most effective means of comprehending how these intricate models arrive at their decisions and the extent to which these methods will function efficiently for both developers and end-users. This research aims to develop effective methods to evaluate and advance explainable retrieval systems toward the broader research field goal of creating techniques to make potential biases more identifiable. Specifically, I aim to investigate the following: RQ1: How do we quantitatively measure explainability? RQ2: How can we develop a set of inherently explainable NRMs using feature attributions that are robust across different retrieval domain contexts? RQ3: How can we leverage knowledge about influential training instances to better understand NRMs and promote more efficient search practices? In future work, I plan to address RQ2 and RQ3 by investigating two avenues of attribution methods, feature-based and instance-based, to develop a suite of explainable NRMs. While much work has been done on investigating the interpretability of deep neural network architectures in the general ML field, particularly in vision and language domains, creating inherently explainable neural architectures remains largely unexplored in IR. Thus, I intend to draw on previous work in the broader fields of NLP and ML to develop methods that offer deeper insights into the inner workings of NRMs and how ranking decisions are made. By developing explainable IR systems, we can facilitate users' comprehension of the intricate, non-linear mechanisms that link their search queries to highly ranked content. If applied correctly, this research has the potential to benefit society in a broad range of applications, such as disinformation detection and clinical decision support. Given their critical importance in modern society, these areas demand robust solutions to combat the escalating dissemination of false information. By enhancing the transparency and accountability of these systems, explainable systems can play a crucial role in curbing this trend.|随着信息检索(IR)系统，如搜索引擎和会话代理，在各个领域变得无处不在，对透明和可解释的系统的需求增长，以确保问责制，公平性和无偏见的结果。尽管在可解释的人工智能和红外技术方面取得了许多最新进展，但对于系统可解释意味着什么还没有达成共识。虽然越来越多的文献表明，可解释性是由多个子因素组成的[2,5,6] ，几乎所有现有的方法都把它作为一个单一的概念。此外，虽然神经检索模型(NRM)因其获得高性能的能力而受到欢迎[3,4,7,8] ，但是对 NRM 的可解释性的研究直到最近几年才被大量探索。关于如何最有效地理解这些错综复杂的模型是如何作出决定的，以及这些方法在多大程度上能够有效地为开发人员和最终用户发挥作用，还有许多问题没有得到解答。本研究旨在发展有效的方法来评估和推进可解释的检索系统，以达到更广泛的研究领域目标，即创造技术，使潜在的偏差更容易识别。具体来说，我的目标是调查以下问题: RQ1: 我们如何定量衡量可解释性？RQ2: 我们如何使用在不同检索领域上下文中具有鲁棒性的特征属性来开发一组固有的可解释的 NRM？RQ3: 我们如何利用关于有影响力的培训实例的知识来更好地理解 NRM 并促进更有效的搜索实践？在未来的工作中，我计划通过研究基于特征和基于实例的两种归因方法来解决 RQ2和 RQ3问题，以开发一套可解释的 NRM。尽管在一般机器学习领域，特别是在视觉和语言领域，已经做了很多工作来研究深层神经网络结构的可解释性，但是在 IR 领域，创建内在可解释的神经网络结构仍然很大程度上没有被探索。因此，我打算利用以前在 NLP 和机器学习这两个更广泛领域的工作，来开发能够提供对 NRM 内部工作原理以及排序决策是如何做出的更深层次见解的方法。通过开发可解释的 IR 系统，我们可以帮助用户理解复杂的、非线性的机制，这些机制将用户的搜索查询与排名较高的内容联系起来。如果应用得当，这项研究有可能在广泛的应用领域造福社会，如虚假信息检测和临床决策支持。鉴于这些领域在现代社会中至关重要，需要有力的解决办法来打击不断升级的虚假信息传播。通过提高这些系统的透明度和问责制，可解释的系统可以在遏制这一趋势方面发挥关键作用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Quantifying+and+Advancing+Information+Retrieval+System+Explainability)|0|
|[Multimodal Named Entity Recognition and Relation Extraction with Retrieval-Augmented Strategy](https://doi.org/10.1145/3539618.3591790)|Xuming Hu|Tsinghua University, Beijing, China|Multimodal Named Entity Recognition (MNER) and Multimodal Relation Extraction (MRE) are tasks in information retrieval that aim to recognize entities and extract relations among them using information from multiple modalities, such as text and images. Although current methods have attempted a variety of modality fusion approaches to enhance the information in text, a large amount of readily available internet retrieval data has not been considered. Therefore, we attempt to retrieve real-world text related to images, objects, and entire sentences from the internet and use this retrieved text as input for cross-modal fusion to improve the performance of entity and relation extraction tasks in the text.|多模态命名实体识别(mNER)和多模态关系提取(MRE)是信息检索中的任务，目的是利用文本和图像等多种模态的信息来识别实体并提取它们之间的关系。尽管目前的方法已经尝试了各种模态融合方法来增强文本中的信息，但是大量现成的网络检索数据还没有得到充分考虑。因此，我们尝试从互联网上检索与图像、物体和整个句子相关的真实文本，并将检索到的文本作为跨模态融合的输入，以提高文本中实体和关系抽取任务的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multimodal+Named+Entity+Recognition+and+Relation+Extraction+with+Retrieval-Augmented+Strategy)|0|
|[Defining and Measuring Cost, Effort, and Load in Information Retrieval](https://doi.org/10.1145/3539618.3591794)|Molly McGregor|University of Strathclyde, Glasgow, United Kingdom|The demands imposed on the user during the Information Retrieval (IR) process are well-recognised, with numerous studies highlighting the influence and importance of cost, effort, and load in explaining user search behaviour and experience. Despite this understanding, there exists no universal definitions of these constructs or standardised methods of measuring them within the field of IR. As a result, an open research problem has emerged in relation to how these constructs have been defined, interpreted, and subsequently measured. To address this research problem, the aim of this research is two-fold. Firstly, to establish working definitions and a conceptual framework for defining cost, effort, and load; and secondly, to examine and evaluate existing measures of effort and load within an IR context.|在信息检索(IR)过程中对用户的要求是公认的，许多研究强调了成本、努力和负载在解释用户搜索行为和体验方面的影响和重要性。尽管有这样的理解，但是在 IR 领域中，对于这些结构还没有普遍的定义，也没有测量它们的标准化方法。因此，一个开放的研究问题已经出现了关系到这些构造是如何定义，解释，并随后测量。为了解决这一研究问题，本研究的目的是双重的。首先，建立工作定义和定义成本、工作量和负荷的概念框架; 其次，在信息检索的背景下检查和评估现有的工作量和负荷的衡量标准。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Defining+and+Measuring+Cost,+Effort,+and+Load+in+Information+Retrieval)|0|
|[Resilient Retrieval Models for Large Collection](https://doi.org/10.1145/3539618.3591793)|Dipannita Podder|Indian Institute of Technology, Kharagpur, Kharagpur, India|Modern search engines employ multi-stage ranking pipeline to balance retrieval efficiency and effectiveness for large collections. These pipelines retrieve an initial set of candidate documents from the large repository by some cost-effective retrieval model (such as BM25, LM), then re-rank these candidate documents by neural retrieval models. These pipelines perform well if the first-stage ranker achieves high recall [2]. To achieve this, the first-stage ranker should address the problems in milliseconds. One of the major problems of the search engine is the presence of extraneous terms in the query. Since the query document term matching is the fundamental block of any retrieval model, the retrieval effectiveness drops when the documents are getting matched with these extraneous query terms. The existing models [4, 5] address this issue by estimating weights of the terms either by using supervised approaches or by utilizing the information of a set of initial top-ranked documents and incorporating it into the final ranking function. Although the later category of methods is unsupervised, they are inefficient as ranking the large collection to get the initial top-ranked documents is computationally expensive. Besides, in the real-world collection, some terms may appear multiple times in the documents for several reasons, such as a term may appear for different contexts, the author bursts this term, or it is an outlier. Thus, the existing retrieval models overestimate the relevance score of the irrelevant documents if they contain some query term with extremely high frequency. Paik et al. [3] propose a probabilistic model based on truncated distributions that reduce the contribution of such high-frequency occurrences of the terms in relevance score. But, the truncation point selection does not leverage term-specific distribution information. It treats all the relevant documents as a bag for a set of queries which is not a good way to capture the distribution of terms. Furthermore, this model does not capture the term burstiness; it only reduces the effect of the outliers. Cummins et al. [1] propose a language model based on Dirichlet compound multinomial distribution that can capture the term burstiness. But this model is explicitly specific to the language model. Considering the above research gaps, we focus on the following research questions in this doctoral work. Research Question 1: How can we identify the central query terms from the verbose query without relying on an initial ranked list or relevance judgment and modify the ranking function so that it can focus on the derived central query terms? To address RQ1, we generate the contextual vector of the entire query and individual query terms using the pre-trained BERT (Bidi-rectional Encoder Representations from Transformers) model and subsequently analyze their correlation to estimate the term centrality score so that the ranking function may focus on the central terms while term matching. Research Question 2: How can we identify the outlier terms of the large collection and penalize them in the ranking function? For RQ2, we model the distribution of maximum normalized term frequency values of relevant documents for the terms of a set of queries. Then we estimate the probability that the normalized frequency of a new term is coming from the right extreme of that distribution and uses this probability to penalize them in the ranking function. Research Question 3: How can we detect the bursty terms and incorporate them in the ranking function? To address RQ3, we propose a model that estimates the burstiness score of a term from its information content in a document and use this score to penalize the bursty term in the ranking function. To estimate the information content of a term, we capture the contextual information of each occurrence of a term by utilizing the pre-trained BERT model and estimate the contextual divergence of the occurrence of a term from its previous occurrences.|现代搜索引擎采用多级排序流水线，以平衡检索效率和有效性的大型集合。这些管道通过一些具有成本效益的检索模型(如 BM25，LM)从大型知识库中检索出一组初始的候选文档，然后通过神经检索模型对这些候选文档进行重新排序。如果第一阶段的排名达到高召回率，那么这些流水线将表现良好[2]。为了实现这一点，第一阶段的排名器应该以毫秒为单位解决问题。搜索引擎的主要问题之一是查询中存在无关的术语。由于查询文档术语匹配是任何检索模型的基本块，当文档与这些无关的查询术语匹配时，检索效率就会下降。现有的模型[4,5]通过使用监督方法或利用一组初始顶级文档的信息估计术语的权重，并将其并入最终的排名函数来解决这个问题。尽管后一类方法是无监督的，但它们效率低下，因为对大型集合进行排序以获得最初的排名最高的文档需要耗费大量计算机资源。此外，在现实生活中，一些术语可能会因为多种原因在文献中多次出现，比如一个术语可能出现在不同的上下文中，作者突然使用了这个术语，或者这个术语是一个异常值。因此，现有的检索模型高估了含有极高频率查询词的不相关文档的相关性得分。Paik 等[3]提出了一个基于截断分布的概率模型，该模型可以减少术语高频出现对相关分数的贡献。但是，截断点选择不利用特定于术语的分布信息。它将所有相关文档视为一组查询的包，这不是捕获术语分布的好方法。此外，这个模型没有捕捉到突发性这个术语; 它只是减少了异常值的影响。Cummins 等[1]提出了一种基于 Dirichlet 复合多项式分布的语言模型，该模型能够捕捉词汇突发性。但是这个模型显式地特定于语言模型。考虑到以上研究的差距，本文主要针对以下几个方面的研究问题展开研究。研究问题1: 如何在不依赖初始排序列表或相关性判断的情况下，从冗长的查询中识别出中心查询词，并修改排序函数，使其能够聚焦于派生的中心查询词？为了解决 RQ1问题，我们使用预先训练的 BERT (Bidi-rectionalEncoder 表示来自 Transformers)模型生成整个查询和单个查询术语的上下文向量，随后分析它们的相关性以估计术语中心性评分，以便排名函数可以在术语匹配时关注中心术语。研究问题2: 如何识别大型集合中的异常项，并在排序函数中对其进行处罚？对于 RQ2，我们为一组查询的术语建模相关文档的最大规范化术语频率值的分布。然后我们估计一个新术语的标准化频率来自这个分布的右端的概率，并用这个概率在排名函数中惩罚它们。研究问题3: 我们如何检测突发条件，并将其纳入排名函数？为了解决 RQ3问题，我们提出了一个模型，从文档中的信息内容估计一个词的突发性分数，并使用这个分数来惩罚排名函数中的突发性词。为了估计一个术语的信息内容，我们通过使用预先训练的 BERT 模型来捕获每个术语出现的上下文信息，并估计一个术语出现的上下文背景与它以前出现的上下文背景的差异。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Resilient+Retrieval+Models+for+Large+Collection)|0|
|[Neural Architectures for Searching Subgraph Structures](https://doi.org/10.1145/3539618.3591791)|Radin Hamidi Rad|Toronto Metropolitan University, Toronto, ON, Canada|With the development of new neural network architectures for graph learning in recent years, the use of graphs to store, represent and process data become more trendy. Nowadays, graphs as a rich structured form of data representation are used in many real-world projects. In these projects, objects are often defined in terms of their connections to other things. There are many practical applications in areas such as antibacterial discovery, physics simulations, fake news detection, traffic prediction and recommendation systems. While there are a growing number of neural graph representation learning techniques that allow one to learn effective graph representations [1-3, 5], they may not necessarily be appropriate for the task of searching over graphs for several reasons: (1) nodes within a graph consist of a set of attributes, which are the subject of the search process. However, the number of unique attributes on the graph compared to the number of attributes on each node is extremely sparse; therefore, this makes it very difficult to learn effective graph representations for such sparse information; (2) graph neural networks are capable of generating rich embedding representations for nodes and entities in graph however, depending on the downstream task, the embedding vectors may perform not well as expected. Therefore, researchers need to apply solutions such as custom loss functions or pre-training tasks to adapt mentioned architectures for their specific task. Search in the graph as a downstream task follows the same trend and therefore a tailored graph neural network representation is needed to particularly address the need for a rich embedding representation for the sole purpose of subgraph search. My work focuses on searching for subgraph structures over both complete and incomplete heterogeneous graphs. The significance of my research direction lies in the fact that exact subgraph search is an NP-hard problem and as such existing methods are either accurate but impractically slow, or efficient yet suffering from low effectiveness. With a focus on learning robust neural representations for complete and incomplete graphs, my research focuses on developing search methods that are both effective and efficient. Specifically, my research addresses the following research questions: RQ1) Would it be possible to design and develop graph representation learning methods for heterogeneous graphs that can generate effective embedding vectors from a heterogeneous graph and support effective and efficient subgraph search? RQ2) Whether it would be possible to address the issue of graphs with varying degrees of missing values. Incomplete graphs suffer from missing attributes and/or missing edges. I explore the design of robust graph representation learning models capable of effectively searching in light of missing information. RQ3) Can efficient and effective derivations of my subgraph search methods be used to address practical applications in real-world domains such as team formation, and keyword search over knowledge graphs? Research conducted as a part of my Ph.D. has so far focused on identifying and retrieving subgraphs over complete heterogeneous graphs. I have used the Team Formation problem as a case study in order to evaluate my work [4]. As the next step, I am planning to focus on expanding my research to include incomplete graphs and investigate methodologies to craft optimized graph representations for the specific task of searching on graphs.|近年来，随着用于图形学习的新型神经网络结构的发展，利用图形来存储、表示和处理数据成为一种趋势。目前，图作为一种丰富的结构化数据表示形式，已经广泛应用于实际工程中。在这些项目中，对象通常是根据它们与其他事物的连接来定义的。在抗菌发现、物理模拟、假新闻检测、流量预测和推荐系统等领域有许多实际应用。虽然有越来越多的神经图表示学习技术，允许一个人学习有效的图表示[1-3,5] ，但它们可能不一定适合于在图中搜索的任务，原因有几个: (1)图中的节点由一组属性组成，这是搜索过程的主题。然而，与每个节点上的属性数量相比，图上的唯一属性数量是非常稀疏的，因此，这使得学习这种稀疏信息的有效图表示变得非常困难; (2)图神经网络能够为图中的节点和实体生成丰富的嵌入表示，然而，根据下游任务，嵌入向量的表现可能不如预期。因此，研究人员需要应用解决方案，如自定义丢失功能或预训练任务，以适应提到的架构，他们的具体任务。图中的搜索作为一个下游任务遵循同样的趋势，因此需要一个量身定制的图神经网络表示来特别满足子图搜索对丰富嵌入表示的需求。我的工作重点是在完全和不完全异构图上寻找子图结构。本文研究方向的意义在于精确子图搜索是一个 NP 难问题，因此现有的方法要么是精确但不切实际的慢，要么是有效但效率低的。针对完全图和不完全图的鲁棒神经表示学习，我的研究侧重于开发既有效又高效的搜索方法。具体来说，我的研究解决了以下研究问题: RQ1)是否有可能设计和开发异构图的图表示学习方法，可以从异构图中生成有效的嵌入向量，并支持有效和高效的子图搜索？RQ2)是否有可能解决具有不同程度缺失值的图的问题。不完全图缺少属性和/或缺少边。探讨了一种能够在缺失信息情况下进行有效搜索的鲁棒图表示学习模型的设计方法。RQ3)我的子图搜索方法的高效和有效的推导能否用于解决实际领域中的实际应用，如团队组建和知识图上的关键字搜索？到目前为止，作为我博士学位的一部分进行的研究主要集中在识别和检索完整异构图的子图。为了评估我的工作，我使用了团队形成问题作为案例研究[4]。作为下一步，我计划把重点放在扩大我的研究，包括不完整的图和调查方法，以工艺优化图表示的具体任务，搜索图表。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Architectures+for+Searching+Subgraph+Structures)|0|
|[Dense Passage Retrieval: Architectures and Augmentation Methods](https://doi.org/10.1145/3539618.3591796)|Thilina Rajapakse|University of Amsterdam, Amsterdam, Netherlands|The dual-encoder model is a dense retrieval architecture, consisting of two encoder models, that has surpassed traditional sparse retrieval methods for open-domain retrieval [1]. But, room exists for improvement, particularly when dense retrievers are exposed to unseen passages or queries. Considering out-of-domain queries, i.e., queries originating from domains other than the one the model was trained on, the loss in accuracy may be significant. A main factor for this is the mismatch in the information available to the context encoder and the query encoder during training. Common retireval training datasets contain an overwhelming majority of passages with one query from a passage. I hypothesize that this could lead the dual-encoder model, particularly the passage encoder, to overfit to a single potential query from a given passage to the detriment of out-of-domain performance. Based on this, I seek to answer the following research question: (RQ1.1) Does training a DPR model on data containing multiple queries per passage improve the generalizability of the model? To answer RQ1.1, I build generated datasets that have multiple queries for most passages, and compare dense passage retriever models trained on these datasets against models trained on (mostly) single query per passage datasets. I show that training on passages with multiple queries leads to models that generalize better to out-of-distribution and out-of-domain test datasets [2]. Language can be considered another domain in the context of a dense retrieval. Training a dense retrieval model is especially challenging in languages other than English due to the scarcity of training data. I propose a novel training technique, clustered training, aimed at improving the retrieval quality of dense retrievers, especially in out-of-distribution and zero-shot settings. I address the following research questions: (RQ2.1)Does clustered training improve the effectiveness of multilingual DPR models on in-distribution data? (RQ2.2) Does clustered training improve the effectiveness of multilingual DPR models on out-of-distribution data from languages that it is trained on? (RQ2.2 Does clustered training improve the effectiveness of multilingual DPR models on out-of-distribution data from languages that it is trained on? (RQ2.3) Does clustered training help multilingual DPR models to generalize to new languages (zero-shot)? I show that clustered training improves the out-of-distribution and zero-shot performance of a DPR model without a clear loss in in-distribution performance using the Mr. TyDi [3] dataset. Finally, I propose a modified dual-encoder architecture that can perform both retrieval and reranking with the same model in a single forward pass. While dual encoder models can surpass traditional sparse retrieval methods, they lag behind two stage retrieval pipelines in retrieval quality. I propose a modification to the dual encoder model where a second representation is used to rerank the passages retrieved using the first representation. Here, a second stage model is not required and both representations are generated in a single forward pass from the dual encoder. I aim to answer the following research questions in this work: (RQ3.1), Can the same model be trained to effectively generate two representations intended for two uses? RQ3.2 Can the retrieval quality of the model be improved by simultaneously performing retrieval and reranking? (RQ3.3 What is the tradeoff between retrieval quality vs. latency and compute resource efficiency for the proposed method vs. a two stage retriever? I expect that my proposed architecture would improve the dual encoder retrieval quality without sacrificing throughput or needing more computational resources.|双编码器模型是一个密集的检索体系结构，由两个编码器模型组成，超越了传统的开放域检索稀疏检索方法[1]。但是，还有改进的空间，特别是当密集的检索器暴露于看不见的通道或查询时。考虑域外查询，即来自模型所在域以外的其他域的查询，准确性的损失可能是显著的。造成这种情况的一个主要因素是在培训期间上下文编码器和查询编码器可用的信息不匹配。常见的退休训练数据集包含绝大多数段落，其中只有一个来自段落的查询。我假设，这可能导致双编码器模型，特别是通道编码器，过分适合从给定通道的单个潜在查询，从而损害域外性能。在此基础上，我试图回答以下研究问题: (RQ1.1)在每篇文章包含多个查询的数据上训练 DPR 模型是否提高了模型的通用性？为了回答 RQ1.1的问题，我构建了对大多数段落有多个查询的生成数据集，并将在这些数据集上训练的密集段落检索器模型与(大多数)每个段落数据集上的单个查询训练的模型进行了比较。我展示了对具有多个查询的段落进行训练可以得到更好地推广到分布以外和域外测试数据集的模型[2]。在密集检索的上下文中，语言可以被认为是另一个领域。由于训练数据的稀缺性，在英语以外的语言中训练一个密集的检索模型尤其具有挑战性。本文提出了一种新的训练技术——聚类训练，旨在提高密集型检索器的检索质量，特别是在分布不均匀和零射击的情况下。我提出了以下研究问题: (RQ2.1)集群训练是否提高了多语言 DPR 模型在分布式数据上的有效性？(RQ2.2)集群培训是否提高了多语言 DPR 模型对来自其所受培训语言的分布外数据的有效性？(RQ2.2集群培训是否提高了多语言 DPR 模型对来自所受培训语言的分布外数据的有效性？(RQ2.3)集群训练是否有助于多语言 DPR 模型推广到新语言(零射击) ？我展示了使用 TyDi 先生[3]数据集的聚类训练改善了 DPR 模型的分布外性能和零射性能，而没有明显的分布内性能损失。最后，我提出了一个改进的双编码器体系结构，可以执行检索和重新排序与同一模型在一个前进通道。双编码器模型虽然可以超越传统的稀疏检索方法，但在检索质量上却落后于两个阶段的检索流程。我提议对双重编码器模型进行修改，其中使用第二种表示重新排列使用第一种表示检索的段落。在这里，不需要第二阶段模型，两种表示都是在双重编码器的一次前向传递中生成的。我的目标是在这项工作中回答以下的研究问题: (RQ3.1) ，能否训练同一个模型有效地生成两个表示意图两个用途？RQ3.2通过同时执行检索和重新排序可以提高模型的检索质量吗？(RQ3.3检索质量与延迟和计算资源效率之间的权衡是什么，提出的方法与一个两阶段检索？我希望我提出的架构能够在不牺牲吞吐量或者不需要更多计算资源的情况下提高双编码器的检索质量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dense+Passage+Retrieval:+Architectures+and+Augmentation+Methods)|0|
|[Towards Trustworthy Recommender System: A Faithful and Responsible Recommendation Perspective](https://doi.org/10.1145/3539618.3591798)|Yang Zhang|University of Science and Technology of China, Hefei, China|Recommender systems (RecSys) become increasingly prevalent in modern society, offering personalized information filtering to alleviate information overload and significantly impacting various human online activities. Machine learning-based recommendation methods have been extensively developed in recent years to achieve more accurate recommendations, with some of these approaches having been extensively deployed in industrial applications, such as the Deep Interest Network (DIN). Despite their widespread use, researchers and practitioners have highlighted various trustworthiness issues inherent in these systems, including bias and promoting polarization issues. In order to better serve users and comply with regulations pertaining to recommendation algorithms established by different countries, it is essential to consider the trustworthiness issues of recommender systems. This research focuses on trustworthiness in recommendation from two perspectives of user-centered principles: faithfulness and responsibility. On the one hand, collected recommendation data may not faithfully reflect user preferences, especially those of the service stage, due to bias[2, 3] and temporal effects,[4,5]etc. Achieving faithful recommendations with such data is crucial to ensure user satisfaction, i.e., making recommendations faithfully reflect user preferences during the testing. On the other hand, recommender systems could not only cater to user preferences [1] but also unconsciously and unintentionally affect (or even manipulate) user preferences. In the recommendation process, controlling the influence of recommender systems, such as avoiding potential opinion polarization, to provide responsible recommendations is also an important aspect of building trustworthy recommender systems. Consequently, there raise four research questions on the two aspects: RQ1: How can we model genuine user preferences when training data fails to faithfully reflect the user's current preferences? RQ2: How can we ensure that recommender models faithfully match the user's future preferences? RQ3: How can we quantify and evaluate the impact of a recommender system on user preferences? RQ4: How can we control the impact of a recommender system on user preferences to avoid negative side effects? Our objective is to achieve faithful and responsible recommendations for users while addressing these research questions. We attribute unfaithful recommendation to the discrepancies between the training data and the service objectives, which we formulate as different data shift problems (RQ1 and RQ2). We provide systematic analyses for these data shift problems from causal perspectives and develop several causality-inspired solutions to enhance recommendation faithfulness. In pursuit of responsible recommendations, we investigate the effect of recommender systems on users from a causal perspective. We develop a causal effect evaluation and adjustment framework to quantify and control the influence of recommender systems on user preferences (RQ3 and RQ4).|推荐系统在现代社会越来越普遍，它提供个性化的信息过滤以减轻信息超载，并对各种人类在线活动产生重大影响。基于机器学习的推荐方法近年来得到了广泛的发展，以实现更准确的推荐，其中一些方法已经广泛应用于工业应用，如深度兴趣网络(DIN)。尽管它们被广泛使用，研究人员和从业人员还是强调了这些系统固有的各种可信问题，包括偏见和促进两极分化的问题。为了更好地为用户服务，遵守不同国家制定的关于推荐算法的条例，必须考虑推荐系统的可信度问题。本研究从以用户为中心的忠实性和责任性两个角度对推荐的可信性进行了研究。一方面，由于偏差[2,3]和时间效应[4,5]等原因，收集的推荐数据可能不能忠实地反映用户的偏好，尤其是服务阶段的偏好。使用这些数据实现忠实的推荐对于确保用户满意度至关重要，也就是说，在测试期间使推荐忠实地反映用户的偏好。另一方面，推荐系统不仅可以迎合用户的偏好，而且还可以无意识地影响(甚至操纵)用户的偏好。在推荐过程中，控制推荐系统的影响，如避免潜在的意见分化，提供负责任的推荐，也是建立可信推荐系统的一个重要方面。因此，在这两个方面提出了四个研究问题: RQ1: 当训练数据不能如实反映用户当前的偏好时，我们如何建立真实的用户偏好模型？RQ2: 我们如何确保推荐模型忠实地匹配用户未来的偏好？RQ3: 我们如何量化和评估推荐系统对用户偏好的影响？RQ4: 我们如何控制推荐系统对用户偏好的影响以避免负面影响？我们的目标是在解决这些研究问题的同时，为用户提供忠实和负责任的建议。我们将不忠实的推荐归因于培训数据和服务目标之间的差异，我们将这些差异表述为不同的数据转移问题(RQ1和 RQ2)。我们从因果关系的角度对这些数据转移问题进行了系统的分析，并提出了一些基于因果关系的解决方案，以提高推荐的忠实度。为了追求负责任的推荐，我们从因果关系的角度研究推荐系统对用户的影响。我们开发了一个因果效应评估和调整框架来量化和控制推荐系统对用户偏好的影响(RQ3和 RQ4)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Trustworthy+Recommender+System:+A+Faithful+and+Responsible+Recommendation+Perspective)|0|
|[Adversarial Meta Prompt Tuning for Open Compound Domain Adaptive Intent Detection](https://doi.org/10.1145/3539618.3591945)|Feiteng Fang, Min Yang, Chengming Li, Ruifeng Xu|University of Science and Technology of China, Hefei, China; Harbin Institute of Technology (Shenzhen), Shenzhen, China; SIAT, Chinese Academy of Sciences, Shenzhen, China; Shenzhen MSU-BIT University, Shenzhen, China|Intent detection plays an essential role in dialogue systems. This paper takes the lead to study open compound domain adaptation (OCDA) for intent detection, which brings the advantage of improved generalization to unseen domains. OCDA for intent detection is indeed a more realistic domain adaptation setting, which learns an intent classifier from labeled source domains and adapts it to unlabeled compound target domains containing different intent classes with the source domains. At inference time, we test the intent classifier in open domains that contain previously unseen intent classes. To this end, we propose an Adversarial Meta Prompt Tuning method (called AMPT) for open compound domain adaptive intent detection. Concretely, we propose a meta prompt tuning method, which utilizes language prompts to elicit rich knowledge from large-scale pre-trained language models (PLMs) and automatically finds better prompt initialization that facilitates fast adaptation via meta learning. Furthermore, we leverage a domain adversarial training technique to acquire domain-invariant representations of diverse domains. By taking advantage of the collaborative effect of meta learning, prompt tuning, and adversarial training, we can learn an intent classifier that can effectively generalize to unseen open domains. Experimental results on two benchmark datasets (i.e., HWU64 and CLINC) show that our model can learn substantially better-generalized representations for unseen domains compared with strong competitors.|意图检测在对话系统中起着至关重要的作用。本文率先研究了开放式复合域自适应(OCDA)的意图检测技术，该技术具有对不可见域进行改进泛化的优点。用于意图检测的 OCDA 确实是一种更加现实的域适应设置，它从标记的源域中学习意图分类器，并将其适应于包含不同意图类别的未标记的源域复合目标域。在推理时，我们在包含以前看不到的意图类的开放域中测试意图分类器。为此，我们提出了一种用于开放复合域自适应意图检测的对抗元提示调整方法(AMPT)。具体地说，我们提出了一种元提示调优方法，该方法利用语言提示从大规模预训练语言模型(PLM)中获取丰富的知识，并自动发现更好的提示初始化，通过元学习促进快速适应。此外，我们利用领域对抗性训练技术来获得不同领域的领域不变表示。通过利用元学习、及时调优和对抗性训练的协作效应，我们可以学习一个意图分类器，它可以有效地推广到看不见的开放领域。在两个基准数据集(即 HWU64和 CLINC)上的实验结果表明，与强竞争对手相比，我们的模型能够更好地学习未知领域的广义表示。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adversarial+Meta+Prompt+Tuning+for+Open+Compound+Domain+Adaptive+Intent+Detection)|0|
|[Tahaqqaq: A Real-Time System for Assisting Twitter Users in Arabic Claim Verification](https://doi.org/10.1145/3539618.3591815)|Zien Sheikh Ali, Watheq Mansour, Fatima Haouari, Maram Hasanain, Tamer Elsayed, Abdulaziz AlAli|Qatar University, Doha, Qatar|Over the past years, notable progress has been made towards fighting misinformation spread over social media, encouraging the development of many fact-checking systems. However, systems that operate over Arabic content are scarce. In this work, we bridge this gap by proposing Tahaqqaq (Verify), an Arabic real-time system that helps users verify claims over Twitter with several functionalities, such as identifying check-worthy claims, estimating credibility of users in terms of spreading fake news, and finding authoritative accounts. Tahaqqaq has a friendly online Web interface that supports various real-time user scenarios. In the same breath, we enable public access to Tahaqqaq services through a handy RESTful API. Finally, in terms of performance, multiple components of Tahaqqaq outperform the state-of-the-art models on Arabic datasets.|过去几年来，在打击社交媒体上传播的错误信息方面取得了显著进展，促进了许多事实核查系统的发展。然而，在阿拉伯文内容上运行的系统很少。在这项工作中，我们通过提出 Tahaqqaq (验证)来弥补这一差距，这是一个阿拉伯语实时系统，可以帮助用户通过 Twitter 验证一些功能，比如识别值得检查的声明，评估用户在传播假新闻方面的可信度，以及查找权威账户。Tahaqqaq 有一个友好的在线 Web 界面，支持各种实时用户场景。同时，我们通过一个方便的 RESTful API 使公众能够访问 Tahaqqaq 服务。最后，在性能方面，Tahaqqaq 的多个组件优于阿拉伯数据集上的最先进模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tahaqqaq:+A+Real-Time+System+for+Assisting+Twitter+Users+in+Arabic+Claim+Verification)|0|
|[Building K-Anonymous User Cohorts with Consecutive Consistent Weighted Sampling (CCWS)](https://doi.org/10.1145/3539618.3591857)|Xinyi Zheng, Weijie Zhao, Xiaoyun Li, Ping Li|LinkedIn, Mountain View, USA|To retrieve personalized campaigns and creatives while protecting user privacy, digital advertising is shifting from member-based identity to cohort-based identity. Under such identity regime, an accurate and efficient cohort building algorithm is desired to group users with similar characteristics. In this paper, we propose a scalable $K$-anonymous cohort building algorithm called {\em consecutive consistent weighted sampling} (CCWS). The proposed method combines the spirit of the ($p$-powered) consistent weighted sampling and hierarchical clustering, so that the $K$-anonymity is ensured by enforcing a lower bound on the size of cohorts. Evaluations on a LinkedIn dataset consisting of $>70$M users and ads campaigns demonstrate that CCWS achieves substantial improvements over several hashing-based methods including sign random projections (SignRP), minwise hashing (MinHash), as well as the vanilla CWS.|为了在保护用户隐私的同时检索个性化活动和创意，数字广告正在从基于成员的身份转向基于队列的身份。在这种身份体制下，需要一种准确有效的队列生成算法来对具有相似特征的用户进行分组。本文提出了一种可扩展的 $K $匿名队列构建算法，称为{ em 连续一致加权抽样}(CCWS)。该方法结合了一致性加权抽样和层次聚类的思想，通过对队列大小的下界限制来保证 $K $匿名性。对一个由7000万美元以上的用户和广告活动组成的 LinkedIn 数据集的评估表明，CCWS 比几种基于散列的方法(包括符号随机投影(SignRP) ，minwise 散列(MinHash)以及普通的 CWS)取得了实质性的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Building+K-Anonymous+User+Cohorts+with+Consecutive+Consistent+Weighted+Sampling+(CCWS))|0|
|[On the "Rough Use" of Machine Learning Techniques](https://doi.org/10.1145/3539618.3591872)|ChihJen Lin|National Taiwan University & MBZUAI, Abu Dhabi & Taipei, Taiwan Roc|Machine learning is everywhere, but unfortunately, we are not experts of every method. Sometimes we "inappropriately'' use machine learning techniques. Examples include reporting training instead of test performance and comparing two methods without suitable hyper-parameter searches. However, the reality is that there are more sophisticated or more subtle examples, which we broadly call the "rough use'' of machine learning techniques. The setting may be roughly fine, but seriously speaking, is inappropriate. We briefly discuss two intriguing examples. - In the topic of graph representation learning, to evaluate the quality of the obtained representations, the multi-label problem of node classification is often considered. An unrealistic setting was used in almost the entire area by assuming that the number of labels of each test instance is known in the prediction stage. In practice, such ground truth information is rarely available. Details of this interesting story are in Lin et al. (2021). - In training deep neural networks, the optimization process often relies on the validation performance for termination or selecting the best epoch. Thus in many public repositories, training, validation, and test sets are explicitly provided. Many think this setting is standard in applying any machine learning technique. However, except that the test set should be completely independent, users can do whatever the best setting on all the available labeled data (i.e., training and validation sets combined). Through real stories, we show that many did not clearly see the relation between training, validation, and test sets. The rough use of machine learning methods is common and sometimes unavoidable. The reason is that nothing is called a perfect use of a machine learning method. Further, it is not easy to assess the seriousness of the situation. We argue that having high-quality and easy-to-use software is an important way to improve the practical use of machine learning techniques.|机器学习无处不在，但不幸的是，我们并不是每种方法的专家。有时我们“不恰当地”使用机器学习技术。例子包括报告训练而不是测试性能和比较两种方法没有合适的超参数搜索。然而，现实情况是，有更复杂或更微妙的例子，我们广泛地称为“粗略使用”的机器学习技术。设置可能大致没问题，但严肃地说，是不合适的。我们简要讨论两个有趣的例子。在图表示学习中，为了评价所获得表示的质量，经常考虑节点分类的多标号问题。通过假设每个测试实例的标签数量在预测阶段已知，在几乎整个区域中使用了不现实的设置。在实践中，这样的地面真相信息很少是可用的。这个有趣故事的细节在 Lin et al。(2021)。- 在训练深层神经网络时，优化过程往往取决于终止或选择最佳时期的验证性能。因此，在许多公共存储库中，显式地提供了培训、验证和测试集。许多人认为这种设置是应用任何机器学习技术的标准。然而，除了测试集应该是完全独立的以外，用户可以对所有可用的标记数据进行任何最佳设置(例如，训练集和验证集的组合)。通过真实的故事，我们发现许多人没有清楚地看到训练、验证和测试集之间的关系。粗略使用机器学习方法是常见的，有时也是不可避免的。原因在于，没有什么能称为机器学习方法的完美运用。此外，要评估形势的严重性并不容易。我们认为拥有高质量和易于使用的软件是提高机器学习技术实际应用的一个重要途径。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+the+"Rough+Use"+of+Machine+Learning+Techniques)|0|
|[Adapting Generative Pretrained Language Model for Open-domain Multimodal Sentence Summarization](https://doi.org/10.1145/3539618.3591633)|Dengtian Lin, Liqiang Jing, Xuemeng Song, Meng Liu, Teng Sun, Liqiang Nie|Harbin Institute of Technology (Shenzhen), Shenzhen, China; Shandong Jianzhu University, Jinan, China; Shandong University, Qingdao, China|Multimodal sentence summarization, aiming to generate a brief summary of the source sentence and image, is a new yet challenging task. Although existing methods have achieved compelling success, they still suffer from two key limitations: 1) lacking the adaptation of generative pre-trained language models for open-domain MMSS, and 2) lacking the explicit critical information modeling. To address these limitations, we propose a BART-MMSS framework, where BART is adopted as the backbone. To be specific, we propose a prompt-guided image encoding module to extract the source image feature. It leverages several soft to-be-learned prompts for image patch embedding, which facilitates the visual content injection to BART for open-domain MMSS tasks. Thereafter, we devise an explicit source critical token learning module to directly capture the critical tokens of the source sentence with the reference of the source image, where we incorporate explicit supervision to improve performance. Extensive experiments on a public dataset fully validate the superiority of our proposed method. In addition, the predicted tokens by the vision-guided key-token highlighting module can be easily understood by humans and hence improve the interpretability of our model.|多模态句子摘要是一项新的具有挑战性的任务，其目的是对源语句和意象进行简要的概括。虽然现有的方法已经取得了令人瞩目的成功，但它们仍然存在两个关键的局限性: 1)缺乏适应开放领域 MMSS 的生成性预训练语言模型，2)缺乏明确的关键信息建模。为了解决这些限制，我们提出了一个 BART-MMSS 框架，其中采用 BART 作为骨干。具体来说，我们提出了一个提示指导的图像编码模块来提取源图像的特征。它利用了几个软的要学习的提示为图像补丁嵌入，这促进了可视化内容注入到 BART 的开放域 MMSS 任务。然后，我们设计了一个显式的源关键标记学习模块，通过引用源图像直接捕获源语句的关键标记，在这个模块中我们加入了显式监督以提高性能。在一个公共数据集上的大量实验充分验证了该方法的优越性。此外，视觉引导的密钥标记高亮模块预测的标记可以很容易地被人类理解，从而提高了我们的模型的可解释性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adapting+Generative+Pretrained+Language+Model+for+Open-domain+Multimodal+Sentence+Summarization)|0|
|[SciMine: An Efficient Systematic Prioritization Model Based on Richer Semantic Information](https://doi.org/10.1145/3539618.3591764)|Fang Guo, Yun Luo, Linyi Yang, Yue Zhang|Westlake University, Hangzhou, China|Systematic review is a crucial method that has been widely used. by scholars from different research domains. However, screening for relevant scientific literature from paper candidates remains an extremely time-consuming process so the task of screening prioritization has been established to reduce the human workload. Various methods under the human-in-the-loop fashion are proposed to solve this task by using lexical features. These methods, even though achieving better performance than more sophisticated feature-based models such as BERT, omit rich and essential semantic information, therefore suffered from feature bias. In this study, we propose a novel framework SciMine to accelerate this screening process by capturing semantic feature representations from both background and the corpus. In particular, based on contextual representation learned from the pre-trained language models, our approach utilizes an autoencoder-based classifier and a feature-dependent classification module to extract general document-level and phrase-level information. Then a ranking ensemble strategy is used to combine these two complementary pieces of information. Experiments on five real-world datasets demonstrate that SciMine achieves state-of-the-art performance and comprehensive analysis further shows the efficacy of SciMine to solve feature bias.|系统综述是一种被广泛使用的关键方法。来自不同研究领域的学者。然而，从论文候选人中筛选相关科学文献仍然是一个极其耗时的过程，因此确定了筛选优先次序的任务，以减少人的工作量。针对这一问题，本文提出了多种基于人在回路的方法，利用词汇特征来解决这一问题。这些方法即使比基于特征的更复杂的模型(如 BERT)获得了更好的性能，但是省略了丰富和必要的语义信息，因此受到了特征偏差的影响。在这项研究中，我们提出了一个新的框架 SciMine，通过从背景和语料库中获取语义特征来加速这个筛选过程。特别是基于预训练语言模型的上下文表示，该方法利用基于自动编码器的分类器和特征相关的分类模块来提取一般文档级和短语级信息。然后采用排序集成策略将这两个互补的信息片段进行组合。在五个实际数据集上的实验表明，SciMine 的性能达到了最高水平，综合分析进一步证明了 SciMine 解决特征偏差问题的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SciMine:+An+Efficient+Systematic+Prioritization+Model+Based+on+Richer+Semantic+Information)|0|
|[Towards Multi-Interest Pre-training with Sparse Capsule Network](https://doi.org/10.1145/3539618.3591778)|Zuoli Tang, Lin Wang, Lixin Zou, Xiaolu Zhang, Jun Zhou, Chenliang Li|Ant Group, Hangzhou, China; Wuhan University, Wuhan, China|The pre-training paradigm, i.e., learning universal knowledge across a wide spectrum of domains, has increasingly become a new de-facto practice in many fields, especially for transferring to new domains. The recent progress includes universal pre-training solutions for recommendation. However, we argue that the common treatment utilizing the masked language modeling or simple data augmentation via contrastive learning is not sufficient for pre-training a recommender system, since a user's intent could be more complex than predicting the next word or item. It is more intuitive to go a step further by devising the multi-interest driven pre-training framework for universal user understanding. Nevertheless, incorporating multi-interest modeling in recommender system pre-training is non-trivial due to the dynamic, contextual, and temporary nature of the user interests, particularly when the users are from different domains. The limited effort on this line has greatly rendered it as an open question. In this paper, we propose a novel Multi-Interest Pre-training with Sparse Capsule framework (named Miracle). Miracle performs a universal multi-interest modeling with a sparse capsule network and an interest-aware pre-training task. Specifically, we utilize a text-aware item embedding module, including an MoE adaptor and a deeply-contextual encoding component, to model contextual and transferable item representations. Then, we propose a sparse interest activation mechanism coupled with a position-aware capsule network for adaptive interest extraction. Furthermore, an interest-level contrastive pre-training task is introduced to guide the sparse capsule network to learn universal interests precisely. We conduct extensive experiments on eleven real-world datasets and eight baselines. The results show that our method significantly outperforms a series of SOTA on these benchmark datasets. The code is available at https://github.com/WHUIR/Miracle.|培训前范式，即学习广泛领域的普遍知识，已日益成为许多领域，特别是转移到新的领域的一种新的事实上的做法。最近的进展包括通用的培训前推荐解决方案。然而，我们认为使用掩蔽语言建模或通过对比学习进行简单数据增强的常规处理不足以预先训练推荐系统，因为用户的意图可能比预测下一个单词或项目更复杂。更直观的做法是设计多兴趣驱动的通用用户理解预训练框架。尽管如此，由于用户兴趣的动态性、上下文关联性和临时性，特别是当用户来自不同的领域时，将多推荐系统模型结合到预先培训中是非常重要的。在这方面的有限努力使它成为一个悬而未决的问题。本文提出了一种新的基于稀疏胶囊框架的多兴趣预训练算法(称为奇迹算法)。Miracle 使用稀疏胶囊网络和感兴趣的预训练任务执行通用的多兴趣建模。具体来说，我们利用文本感知项嵌入模块(包括 MoE 适配器和深度上下文编码组件)来建模上下文和可转移的项表示。然后，我们提出了一个稀疏兴趣激活机制耦合位置感知胶囊网络的自适应兴趣提取。引入兴趣级对比预训练任务，引导稀疏胶囊网络精确学习普遍兴趣。我们在十一个真实世界的数据集和8个基线上进行广泛的实验。结果表明，我们的方法在这些基准数据集上的性能明显优于一系列 SOTA。密码可在 https://github.com/whuir/miracle 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Multi-Interest+Pre-training+with+Sparse+Capsule+Network)|0|
|[A Scalable Framework for Automatic Playlist Continuation on Music Streaming Services](https://doi.org/10.1145/3539618.3591628)|Walid Bendada, Guillaume SalhaGalvan, Thomas Bouabça, Tristan Cazenave|Deezer Research, Paris, France; Deezer Research & Université Paris Dauphine - PSL, Paris, France; Université Paris Dauphine & PSL, CNRS, Paris, France|Music streaming services often aim to recommend songs for users to extend the playlists they have created on these services. However, extending playlists while preserving their musical characteristics and matching user preferences remains a challenging task, commonly referred to as Automatic Playlist Continuation (APC). Besides, while these services often need to select the best songs to recommend in real-time and among large catalogs with millions of candidates, recent research on APC mainly focused on models with few scalability guarantees and evaluated on relatively small datasets. In this paper, we introduce a general framework to build scalable yet effective APC models for large-scale applications. Based on a represent-then-aggregate strategy, it ensures scalability by design while remaining flexible enough to incorporate a wide range of representation learning and sequence modeling techniques, e.g., based on Transformers. We demonstrate the relevance of this framework through in-depth experimental validation on Spotify's Million Playlist Dataset (MPD), the largest public dataset for APC. We also describe how, in 2022, we successfully leveraged this framework to improve APC in production on Deezer. We report results from a large-scale online A/B test on this service, emphasizing the practical impact of our approach in such a real-world application.|音乐流媒体服务通常旨在向用户推荐歌曲，以扩展他们在这些服务上创建的播放列表。然而，扩展播放列表，同时保留其音乐特征和匹配用户喜好仍然是一个具有挑战性的任务，通常被称为自动播放列表延续(APC)。此外，虽然这些服务往往需要在实时推荐和有数百万候选人的大型目录中选择最好的歌曲，但最近对 APC 的研究主要集中在几乎没有可扩展性保证的模型上，并在相对较小的数据集上进行评估。在本文中，我们介绍了一个通用的框架，以建立可扩展但有效的 APC 模型的大规模应用。基于先表示后聚合的策略，它通过设计来确保可扩展性，同时保持足够的灵活性，以结合广泛的表示学习和序列建模技术，例如，基于 Transformers。我们通过对 Spotify 的百万播放列表数据集(MPD)—— APC 最大的公共数据集——进行深入的实验验证，证明了该框架的相关性。我们还描述了，在2022年，我们如何成功地利用这个框架来改进 Deezer 上的 APC 生产。我们在这个服务上报告了大规模在线 A/B 测试的结果，强调了我们的方法在这样一个实际应用中的实际影响。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Scalable+Framework+for+Automatic+Playlist+Continuation+on+Music+Streaming+Services)|0|
|[BotMoE: Twitter Bot Detection with Community-Aware Mixtures of Modal-Specific Experts](https://doi.org/10.1145/3539618.3591646)|Yuhan Liu, Zhaoxuan Tan, Heng Wang, Shangbin Feng, Qinghua Zheng, Minnan Luo|University of Washington, Seattle, WA, USA; Xi'an Jiaotong University, Xi'an, China|Twitter bot detection has become a crucial task in efforts to combat online misinformation, mitigate election interference, and curb malicious propaganda. However, advanced Twitter bots often attempt to mimic the characteristics of genuine users through feature manipulation and disguise themselves to fit in diverse user communities, posing challenges for existing Twitter bot detection models. To this end, we propose BotMoE, a Twitter bot detection framework that jointly utilizes multiple user information modalities (metadata, textual content, network structure) to improve the detection of deceptive bots. Furthermore, BotMoE incorporates a community-aware Mixture-of-Experts (MoE) layer to improve domain generalization and adapt to different Twitter communities. Specifically, BotMoE constructs modal-specific encoders for metadata features, textual content, and graphical structure, which jointly model Twitter users from three modal-specific perspectives. We then employ a community-aware MoE layer to automatically assign users to different communities and leverage the corresponding expert networks. Finally, user representations from metadata, text, and graph perspectives are fused with an expert fusion layer, combining all three modalities while measuring the consistency of user information. Extensive experiments demonstrate that BotMoE significantly advances the state-of-the-art on three Twitter bot detection benchmarks. Studies also confirm that BotMoE captures advanced and evasive bots, alleviates the reliance on training data, and better generalizes to new and previously unseen user communities.|在打击网络虚假信息、减轻选举干扰和遏制恶意宣传的努力中，Twitter 机器人检测已成为一项关键任务。然而，先进的 Twitter 机器人往往试图通过特征操作来模仿真实用户的特征，并伪装自己以适应不同的用户群体，这对现有的 Twitter 机器人检测模型构成了挑战。为此，我们提出了 BotMoE，这是一个 Twitter 机器人检测框架，它联合利用多种用户信息模式(元数据、文本内容、网络结构)来改善对欺骗性机器人的检测。此外，BotMoE 还包含了一个社区感知的专家混合(Miture-of-Expert，MoE)层，以提高域泛化能力并适应不同的 Twitter 社区。具体来说，BotMoE 为元数据特性、文本内容和图形结构构建了特定于模式的编码器，这些编码器从三个特定于模式的视角共同为 Twitter 用户建模。然后，我们使用一个社区感知的 MoE 层来自动将用户分配到不同的社区，并利用相应的专家网络。最后，将来自元数据、文本和图形视角的用户表示与专家融合层融合，在测量用户信息一致性的同时结合所有三种模式。大量的实验表明，BotMoE 在三个 Twitter 机器人检测基准上显著提高了最先进的水平。研究还证实，BotMoE 捕获先进和规避机器人，减轻对训练数据的依赖，并更好地推广到新的和以前未见的用户社区。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BotMoE:+Twitter+Bot+Detection+with+Community-Aware+Mixtures+of+Modal-Specific+Experts)|0|
|[Decoupled Hyperbolic Graph Attention Network for Cross-domain Named Entity Recognition](https://doi.org/10.1145/3539618.3591662)|Jingyun Xu, Yi Cai|South China University of Technology, Guangzhou, China|To address the scarcity of massive labeled data, cross-domain named entity recognition (cross-domain NER) attracts increasing attention. Recent studies focus on decomposing NER into two separate tasks (i.e., entity span detection and entity type classification) to reduce the complexity of the cross-domain transfer. Despite the promising results, there still exists room for improvement. In particular, the rich domain-shared syntactic and semantic information, which are respectively important for entity span detection and entity type classification, are still underutilized. In light of these two challenges, we propose applying graph attention networks (GATs) to encode the above two kinds of information. Moreover, considering that GATs mainly operate in the Euclidean space, which may fail to capture the latent hierarchical relations among words for learning high-quality word representations, we further propose to embed words into Hyperbolic spaces. Finally, a decouple hyperbolic graph attention network (DH-GAT) is introduced for cross-domain NER. Empirical results on 10 domain pairs show that DH-GAT achieves state-of-the-art performance on several standard metrics, and further analyses are presented to better understand each component's effectiveness.|为了解决海量标记数据稀缺的问题，跨域命名实体识别(cross-domain NER)越来越受到人们的关注。最近的研究集中于将 NER 分解为两个独立的任务(即实体跨度检测和实体类型分类) ，以降低跨域传输的复杂性。尽管取得了令人鼓舞的成果，但仍有改进的余地。特别是对于实体跨度检测和实体类型分类来说分别重要的丰富的域共享语法和语义信息仍然没有得到充分利用。针对这两个挑战，我们提出应用图注意网络(GAT)对上述两种信息进行编码。此外，考虑到 GAT 主要在欧几里德空间中运作，可能无法捕捉到词之间潜在的层次关系来学习高质量的词表示，我们进一步建议将词嵌入双曲空间。最后，针对跨域 NER，提出了一种解耦双曲图注意网络(DH-GAT)。对10个域对的实证结果表明，DH-GAT 在几个标准指标上达到了最佳性能，并进一步分析以更好地了解每个组件的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Decoupled+Hyperbolic+Graph+Attention+Network+for+Cross-domain+Named+Entity+Recognition)|0|
|[StreamE: Learning to Update Representations for Temporal Knowledge Graphs in Streaming Scenarios](https://doi.org/10.1145/3539618.3591772)|Jiasheng Zhang, Jie Shao, Bin Cui|University of Electronic Science and Technology of China, Chengdu, China; Shenzhen Institute for Advanced Study, University of Electronic Science and Technology of China, Shenzhen, China; Peking University, Beijing, China|Learning representations for temporal knowledge graphs (TKGs) is a fundamental task. Most existing methods regard TKG as a sequence of static snapshots and recurrently learn representations by retracing the previous snapshots. However, new knowledge can be continuously accrued to TKGs as streams. These methods either cannot handle new entities or fail to update representations in real time, making them unfeasible to adapt to the streaming scenarios. In this paper, we propose a lightweight framework called StreamE towards the efficient generation of TKG representations in streaming scenarios. To reduce the parameter size, entity representations in StreamE are decoupled from the model training to serve as the memory module to store the historical information of entities. To achieve efficient update and generation, the process of generating representations is decoupled as two functions in StreamE. An update function is learned to incrementally update entity representations based on the newly-arrived knowledge and a read function is learned to predict the future semantics of entity representations. The update function avoids the recurrent modeling paradigm and thus gains high efficiency while the read function considers multiple semantic change properties. We further propose a joint training strategy with two temporal regularizations to effectively optimize the framework. Experimental results show that StreamE can achieve better performance than baseline methods with 100x faster in inference, 25x faster in training, and only 1/5 parameter size, which demonstrates its superiority. Code is available at https://github.com/zjs123/StreamE.|时态知识图(TKG)的学习表示是一个基本的任务。大多数现有的方法将 TKG 视为一系列静态快照，并通过回溯以前的快照来反复学习表示法。然而，新的知识可以不断累积到 TKG 作为流。这些方法要么不能处理新的实体，要么不能实时更新表示，使它们无法适应流场景。在本文中，我们提出了一个称为 StreamE 的轻量级框架，用于在流场景中有效地生成 TKG 表示。为了减少参数的大小，StreamE 中的实体表示与模型训练解耦，作为存储实体历史信息的内存模块。为了实现有效的更新和生成，生成表示的过程被解耦为 StreamE 中的两个函数。学习更新函数以根据新到达的知识增量更新实体表示，学习读取函数以预测实体表示的未来语义。更新函数避免了重复建模范式，因此在读取函数考虑多种语义变化属性的情况下，获得了高效率。我们进一步提出了一个具有两个时间规则的联合训练策略，以有效地优化框架。实验结果表明，StreamE 方法的推理速度比基线方法快100倍，训练速度比基线方法快25倍，参数只有基线方法的1/5，表明了该方法的优越性。密码可于 https://github.com/zjs123/streame 索取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=StreamE:+Learning+to+Update+Representations+for+Temporal+Knowledge+Graphs+in+Streaming+Scenarios)|0|
|[Understand the Dynamic World: An End-to-End Knowledge Informed Framework for Open Domain Entity State Tracking](https://doi.org/10.1145/3539618.3591781)|Mingchen Li, Lifu Huang|Georgia State University, ATLANTA, GA, USA; Virginia Tech, Blacksburg, VA, USA|Open domain entity state tracking aims to predict reasonable state changes of entities (i.e., [attribute] of [entity] was [before_state] and [after_state] afterwards) given the action descriptions. It's important to many reasoning tasks to support human everyday activities. However, it's challenging as the model needs to predict an arbitrary number of entity state changes caused by the action while most of the entities are implicitly relevant to the actions and their attributes as well as states are from open vocabularies. To tackle these challenges, we propose a novel end-to-end Knowledge Informed framework for open domain Entity State Tracking, namely KIEST, which explicitly retrieves the relevant entities and attributes from external knowledge graph (i.e., ConceptNet) and incorporates them to autoregressively generate all the entity state changes with a novel dynamic knowledge grained encoder-decoder framework. To enforce the logical coherence among the predicted entities, attributes, and states, we design a new constraint decoding strategy and employ a coherence reward to improve the decoding process. Experimental results show that our proposed KIEST framework significantly outperforms the strong baselines on the public benchmark dataset OpenPI.|开放域实体状态跟踪旨在根据行为描述预测实体的合理状态变化(即，[实体]的[属性]是[前 _ 状态]和[后 _ 状态])。对于许多推理任务来说，支持人类的日常活动是非常重要的。然而，这是具有挑战性的，因为模型需要预测由操作引起的任意数量的实体状态变化，而大多数实体都与操作隐式相关，并且它们的属性以及状态都来自开放词汇表。为了应对这些挑战，我们提出了一个新的开放领域实体状态跟踪端到端知识信息框架，即 KIEST，它显式地从外部知识图(即概念网)中检索相关实体和属性，并将它们合并在一个新的动态知识粒度编解码框架中自动回归生成所有实体状态变化。为了增强预测实体、属性和状态之间的逻辑一致性，我们设计了一种新的约束译码策略，并采用一致性奖励来改善译码过程。实验结果表明，我们提出的 KIEST 框架明显优于公共基准数据集 OpenPI 的强基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understand+the+Dynamic+World:+An+End-to-End+Knowledge+Informed+Framework+for+Open+Domain+Entity+State+Tracking)|0|
|[BLADE: Combining Vocabulary Pruning and Intermediate Pretraining for Scaleable Neural CLIR](https://doi.org/10.1145/3539618.3591644)|Suraj Nair, Eugene Yang, Dawn J. Lawrie, James Mayfield, Douglas W. Oard|University of Maryland, College Park, MD, USA; Johns Hopkins University, Baltimore, MD, USA|Learning sparse representations using pretrained language models enhances the monolingual ranking effectiveness. Such representations are sparse vectors in the vocabulary of a language model projected from document terms. Extending such approaches to Cross-Language Information Retrieval (CLIR) using multilingual pretrained language models poses two challenges. First, the larger vocabularies of multilingual models affect both training and inference efficiency. Second, the representations of terms from different languages with similar meanings might not be sufficiently similar. To address these issues, we propose a learned sparse representation model, BLADE, combining vocabulary pruning with intermediate pre-training based on cross-language supervision. Our experiments reveal BLADE significantly reduces indexing time compared to its monolingual counterpart, SPLADE, on machine-translated documents, and it generates rankings with strengths complementary to those of other efficient CLIR methods.|使用预训练语言模型学习稀疏表示增强了单语言排序的有效性。这种表示是从文档术语投影出来的语言模型的词汇表中的稀疏向量。使用多语言预先训练的语言模型将这些方法扩展到跨语检索(CLIR)会带来两个挑战。首先，多语言模型的词汇量越大，训练效率和推理效率越高。其次，来自不同语言的具有相似意义的术语的表示可能不够相似。为了解决这些问题，我们提出了一种学习型稀疏表示模型 BLADE，该模型将词汇修剪和基于跨语言监控的中级预训结合起来。我们的实验显示，与单语言版本的 SPLADE 相比，BLADE 在机器翻译文档上大大减少了索引时间，而且它产生的排名与其他有效的 CLIR 方法相互补充。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BLADE:+Combining+Vocabulary+Pruning+and+Intermediate+Pretraining+for+Scaleable+Neural+CLIR)|0|
|[Cross-Market Product-Related Question Answering](https://doi.org/10.1145/3539618.3591658)|Negin Ghasemi, Mohammad Aliannejadi, Hamed R. Bonab, Evangelos Kanoulas, Arjen P. de Vries, James Allan, Djoerd Hiemstra|Radboud University, Nijmegen, Netherlands; University of Massachusetts Amherst, Amherst, MA, USA; University of Amsterdam, Amsterdam, Netherlands; Amazon Inc., Seattle, WA, USA|Online shops such as Amazon, eBay, and Etsy continue to expand their presence in multiple countries, creating new resource-scarce marketplaces with thousands of items. We consider a marketplace to be resource-scarce when only limited user-generated data is available about the products (e.g., ratings, reviews, and product-related questions). In such a marketplace, an information retrieval system is less likely to help users find answers to their questions about the products. As a result, questions posted online may go unanswered for extended periods. This study investigates the impact of using available data in a resource-rich marketplace to answer new questions in a resource-scarce marketplace, a new problem we call cross-market question answering. To study this problem's potential impact, we collect and annotate a new dataset, XMarket-QA, from Amazon's UK (resource-scarce) and US (resource-rich) local marketplaces. We conduct a data analysis to understand the scope of the cross-market question-answering task. This analysis shows a temporal gap of almost one year between the first question answered in the UK marketplace and the US marketplace. Also, it shows that the first question about a product is posted in the UK marketplace only when 28 questions, on average, have already been answered about the same product in the US marketplace. Human annotations demonstrate that, on average, 65% of the questions in the UK marketplace can be answered within the US marketplace, supporting the concept of cross-market question answering. Inspired by these findings, we develop a new method, CMJim, which utilizes product similarities across marketplaces in the training phase for retrieving answers from the resource-rich marketplace that can be used to answer a question in the resource-scarce marketplace. Our evaluations show CMJim's significant improvement compared to competitive baselines.|亚马逊、 eBay 和 Etsy 等在线商店继续在多个国家扩张业务，创造了拥有数千种商品的新的资源稀缺市场。我们认为市场是资源稀缺的，当只有有限的用户生成的数据可用的产品(例如，评级，评论，和产品相关的问题)。在这样一个市场中，信息检索系统不太可能帮助用户找到有关产品的问题的答案。因此，网上发布的问题可能会长时间得不到回答。这项研究调查了在资源丰富的市场中使用可用数据来回答资源稀缺的市场中的新问题的影响，这个新问题我们称之为跨市场问题回答。为了研究这个问题的潜在影响，我们从亚马逊的英国(资源稀缺)和美国(资源丰富)本地市场收集并注释了一个新的数据集 XMarket-QA。我们进行数据分析，以了解跨市场问答任务的范围。这一分析表明，在英国市场回答的第一个问题与美国市场回答的第一个问题之间存在近一年的时间差。此外，它还显示，只有当平均28个关于同一产品的问题在美国市场上已经得到回答时，关于该产品的第一个问题才会在英国市场上发布。人工注释表明，平均65% 的问题在英国市场可以回答在美国市场，支持跨市场问题回答的概念。受这些发现的启发，我们开发了一种新的方法，CMJim，它利用培训阶段不同市场的产品相似性，从资源丰富的市场检索答案，这些答案可以用来回答资源稀缺的市场中的一个问题。我们的评估显示，与竞争基线相比，CMJim 的进步显著。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cross-Market+Product-Related+Question+Answering)|0|
|[ErrorCLR: Semantic Error Classification, Localization and Repair for Introductory Programming Assignments](https://doi.org/10.1145/3539618.3591680)|Siqi Han, Yu Wang, Xuesong Lu|East China Normal University, Shanghai, China|Programming education at scale increasingly relies on automated feedback to help students learn to program. An important form of feedback is to point out semantic errors in student programs and provide hints for program repair. Such automated feedback depends essentially on solving the tasks of classification, localization and repair of semantic errors. Although there are datasets for the tasks, we observe that they do not have the annotations supporting all three tasks. As such, existing approaches for semantic error feedback treat error classification, localization and repair as independent tasks, resulting in sub-optimal performance on each task. Moreover, existing datasets either contain few programming assignments or have few programs for each assignment. Therefore, existing approaches often leverage rule-based methods and evaluate them with a small number of programming assignments. To tackle the problems, we first describe the creation of a new dataset COJ2022 that contains 5,914 C programs with semantic errors submitted to 498 different assignments in an introductory programming course, where each program is annotated with the error types and locations and is coupled with the repaired program submitted by the same student. We show the advantages of COJ2022 over existing datasets on various aspects. Second, we treat semantic error classification, localization and repair as dependent tasks, and propose a novel two-stage method ErrorCLR to solve them. Specifically, in the first stage we train a model based on graph matching networks to jointly classify and localize potential semantic errors in student programs, and in the second stage we mask error spans in buggy programs using information of error types and locations and train a CodeT5 model to predict correct spans. The predicted spans replace the error spans to form repaired programs. Experimental results show that ErrorCLR remarkably outperforms the comparative methods for all three tasks on COJ2022 and other public datasets. We also conduct a case study to visualize and interpret what is learned by the graph matching network in ErrorCLR. We have released the source code and COJ2022 at https://github.com/DaSESmartEdu/ErrorCLR.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ErrorCLR:+Semantic+Error+Classification,+Localization+and+Repair+for+Introductory+Programming+Assignments)|0|
|[Dual Semantic Knowledge Composed Multimodal Dialog Systems](https://doi.org/10.1145/3539618.3591673)|Xiaolin Chen, Xuemeng Song, Yinwei Wei, Liqiang Nie, TatSeng Chua|School of Computing, National University of Singapore, Singapore, Singapore; School of Computer Science and Technology, Harbin Institute of Technology (Shenzhen), Shenzhen, Singapore; School of Computer Science and Technology, Shandong University, Qingdao, China; School of Software, Joint SDU-NTU Centre for Artificial Intelligence Research, Shandong University, Jinan, China|Textual response generation is an essential task for multimodal task-oriented dialog systems.Although existing studies have achieved fruitful progress, they still suffer from two critical limitations: 1) focusing on the attribute knowledge but ignoring the relation knowledge that can reveal the correlations between different entities and hence promote the response generation}, and 2) only conducting the cross-entropy loss based output-level supervision but lacking the representation-level regularization. To address these limitations, we devise a novel multimodal task-oriented dialog system (named MDS-S2). Specifically, MDS-S2 first simultaneously acquires the context related attribute and relation knowledge from the knowledge base, whereby the non-intuitive relation knowledge is extracted by the n-hop graph walk. Thereafter, considering that the attribute knowledge and relation knowledge can benefit the responding to different levels of questions, we design a multi-level knowledge composition module in MDS-S2 to obtain the latent composed response representation. Moreover, we devise a set of latent query variables to distill the semantic information from the composed response representation and the ground truth response representation, respectively, and thus conduct the representation-level semantic regularization. Extensive experiments on a public dataset have verified the superiority of our proposed MDS-S2. We have released the codes and parameters to facilitate the research community.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dual+Semantic+Knowledge+Composed+Multimodal+Dialog+Systems)|0|
|[Mixup-based Unified Framework to Overcome Gender Bias Resurgence](https://doi.org/10.1145/3539618.3591938)|Liu Yu, Yuzhou Mao, Jin Wu, Fan Zhou|University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China & Kashi Institute of Electronics and Information Industry, Chengdu, China|Unwanted social biases are usually encoded in pretrained language models (PLMs). Recent efforts are devoted to mitigating intrinsic bias encoded in PLMs. However, the separate fine-tuning on applications is detrimental to intrinsic debiasing. A bias resurgence issue arises when fine-tuning the debiased PLMs on downstream tasks. To eliminate undesired stereotyped associations in PLMs during fine-tuning, we present a mixup-based framework Mix-Debias from a new unified perspective, which directly combines debiasing PLMs with fine-tuning applications. The key to Mix-Debias is applying mixup-based linear interpolation on counterfactually augmented downstream datasets, with expanded pairs from external corpora. Besides, we devised an alignment regularizer to ensure original augmented pairs and gender-balanced counterparts are spatially closer. Experimental results show that Mix-Debias can reduce biases in PLMs while maintaining a promising performance in applications.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mixup-based+Unified+Framework+to+Overcome+Gender+Bias+Resurgence)|0|
|[Calibration Learning for Few-shot Novel Product Description](https://doi.org/10.1145/3539618.3591959)|Zheng Liu, Mingjing Wu, Bo Peng, Yichao Liu, Qi Peng, Chong Zou|Nanyang Technological University, Singapore, Singapore; China Institute of Atomic Energy, Beijing, China; University College London, London, United Kingdom; Newcastle University, Newcastle upon Tyne, United Kingdom|In the field of E-commerce, the rapid introduction of new products poses challenges for product description generation. Traditional approaches rely on large labelled datasets, which are often unavailable for novel products with limited data. To address this issue, we propose a calibration learning approach for few-shot novel product description. Our method leverages a small amount of labelled data for calibration and utilizes the novel product's semantic representation as prompts to generate accurate and informative descriptions. We evaluate our approach on three large-scale e-commerce datasets of novel products and demonstrate its effectiveness in significantly improving the quality of generated product descriptions compared to existing methods, especially when only limited data is available. We also conduct the analysis to understand the impact of different modules on the performance.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Calibration+Learning+for+Few-shot+Novel+Product+Description)|0|
|[Decomposing Logits Distillation for Incremental Named Entity Recognition](https://doi.org/10.1145/3539618.3591970)|Duzhen Zhang, Yahan Yu, Feilong Chen, Xiuyi Chen|Huawei Inc., Beijing, China; Baidu Inc., Beijing, China|Incremental Named Entity Recognition (INER) aims to continually train a model with new data, recognizing emerging entity types without forgetting previously learned ones. Prior INER methods have shown that Logits Distillation (LD), which involves preserving predicted logits via knowledge distillation, effectively alleviates this challenging issue. In this paper, we discover that a predicted logit can be decomposed into two terms that measure the likelihood of an input token belonging to a specific entity type or not. However, the traditional LD only preserves the sum of these two terms without considering the change in each component. To explicitly constrain each term, we propose a novel Decomposing Logits Distillation (DLD) method, enhancing the model's ability to retain old knowledge and mitigate catastrophic forgetting. Moreover, DLD is model-agnostic and easy to implement. Extensive experiments show that DLD consistently improves the performance of state-of-the-art INER methods across ten INER settings in three datasets.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Decomposing+Logits+Distillation+for+Incremental+Named+Entity+Recognition)|0|
|[Examining the Impact of Uncontrolled Variables on Physiological Signals in User Studies for Information Processing Activities](https://doi.org/10.1145/3539618.3591981)|Kaixin Ji, Damiano Spina, Danula Hettiachchi, Flora Dilys Salim, Falk Scholer|RMIT University, Melbourne, VIC, Australia; The University of New South Wales, Sydney, NSW, Australia|Physiological signals can potentially be applied as objective measures to understand the behavior and engagement of users interacting with information access systems. However, the signals are highly sensitive, and many controls are required in laboratory user studies. To investigate the extent to which controlled or uncontrolled (i.e., confounding) variables such as task sequence or duration influence the observed signals, we conducted a pilot study where each participant completed four types of information-processing activities (READ, LISTEN, SPEAK, and WRITE). Meanwhile, we collected data on blood volume pulse, electrodermal activity, and pupil responses. We then used machine learning approaches as a mechanism to examine the influence of controlled and uncontrolled variables that commonly arise in user studies. Task duration was found to have a substantial effect on the model performance, suggesting it represents individual differences rather than giving insight into the target variables. This work contributes to our understanding of such variables in using physiological signals in information retrieval user studies.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Examining+the+Impact+of+Uncontrolled+Variables+on+Physiological+Signals+in+User+Studies+for+Information+Processing+Activities)|0|
|[Fairness for both Readers and Authors: Evaluating Summaries of User Generated Content](https://doi.org/10.1145/3539618.3591986)|Garima Chhikara, Kripabandhu Ghosh, Saptarshi Ghosh, Abhijnan Chakraborty|Indian Institute of Science Education and Research Kolkata, Mohanpur, India; Indian Institute of Technology, Delhi Technological University, New Delhi, India; Indian Institute of Technology Kharagpur, Kharagpur, India; Indian Institute of Technology Delhi, New Delhi, India|Summarization of textual content has many applications, ranging from summarizing long documents to recent efforts towards summarizing user generated text (e.g., tweets, Facebook or Reddit posts). Traditionally, the focus of summarization has been to generate summaries which can best satisfy the readers. In this work, we look at summarization of user-generated content as a two-sided problem where satisfaction of both readers and authors is crucial. Through three surveys, we show that for user-generated content, traditional evaluation approach of measuring similarity between reference summaries and algorithmic summaries cannot capture author satisfaction. We propose an author satisfaction-based evaluation metric CROSSEM which, we show empirically, can potentially complement the current evaluation paradigm. We further propose the idea of inequality in satisfaction, to account for individual fairness amongst readers and authors. To our knowledge, this is the first attempt towards developing a fair summary evaluation framework for user generated content, and is likely to spawn lot of future research in this space.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fairness+for+both+Readers+and+Authors:+Evaluating+Summaries+of+User+Generated+Content)|0|
|[Limitations of Open-Domain Question Answering Benchmarks for Document-level Reasoning](https://doi.org/10.1145/3539618.3592011)|Ehsan Kamalloo, Charles L. A. Clarke, Davood Rafiei|University of Waterloo, Waterloo, ON, Canada; University of Alberta, Edmonton, AB, Canada|Many recent QA models retrieve answers from passages, rather than whole documents, due to the limitations of deep learning models with limited context size. However, this approach ignores important document-level cues that can be crucial in answering questions. This paper reviews three open-domain QA benchmarks from a document-level perspective and finds that they are biased towards passage-level information. Out of 17,000 assessed questions, 82 were identified as requiring document-level reasoning and could not be answered by passage-based models. Document-level retrieval (BM25) outperformed both dense and sparse passage-level retrieval on these questions, highlighting the need for more evaluation of models' ability to understand documents, an often-overlooked challenge in open-domain QA.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Limitations+of+Open-Domain+Question+Answering+Benchmarks+for+Document-level+Reasoning)|0|
|[MDDL: A Framework for Reinforcement Learning-based Position Allocation in Multi-Channel Feed](https://doi.org/10.1145/3539618.3592018)|Xiaowen Shi, Ze Wang, Yuanying Cai, Xiaoxu Wu, Fan Yang, Guogang Liao, Yongkang Wang, Xingxing Wang, Dong Wang|Tsinghua Universing, Beijing, China; Meituan, Beijing, China|Nowadays, the mainstream approach in position allocation system is to utilize a reinforcement learning model to allocate appropriate locations for items in various channels and then mix them into the feed. There are two types of data employed to train reinforcement learning (RL) model for position allocation, named strategy data and random data. Strategy data is collected from the current online model, it suffers from an imbalanced distribution of state-action pairs, resulting in severe overestimation problems during training. On the other hand, random data offers a more uniform distribution of state-action pairs, but is challenging to obtain in industrial scenarios as it could negatively impact platform revenue and user experience due to random exploration. As the two types of data have different distributions, designing an effective strategy to leverage both types of data to enhance the efficacy of the RL model training has become a highly challenging problem. In this study, we propose a framework named Multi-Distribution Data Learning (MDDL) to address the challenge of effectively utilizing both strategy and random data for training RL models on mixed multi-distribution data. Specifically, MDDL incorporates a novel imitation learning signal to mitigate overestimation problems in strategy data and maximizes the RL signal for random data to facilitate effective learning. In our experiments, we evaluated the proposed MDDL framework in a real-world position allocation system and demonstrated its superior performance compared to the previous baseline. MDDL has been fully deployed on the Meituan food delivery platform and currently serves over 300 million users.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MDDL:+A+Framework+for+Reinforcement+Learning-based+Position+Allocation+in+Multi-Channel+Feed)|0|
|[On Answer Position Bias in Transformers for Question Answering](https://doi.org/10.1145/3539618.3592029)|Rafael Glater, Rodrygo L. T. Santos|Universidade Federal de Minas Gerais, Belo Horizonte, Brazil|Extractive Transformer-based models for question answering (QA) are trained to predict the start and end position of the answer in a candidate paragraph. However, the true answer position can bias these models when its distribution in the training data is highly skewed. That is, models trained only with the answer at the beginning of the paragraph will perform poorly on test instances with the answer at the end. Many studies have focused on countering answer position bias but have yet to deepen our understanding of how such bias manifests in the main components of the Transformer. In this paper, we analyze the self-attention and embedding generation components of five Transformer-based models with different architectures and position embedding strategies. Our analysis shows that models tend to map position bias in their attention matrices, generating embeddings that correlate the answer and its biased position, ultimately compromising model generalization.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Answer+Position+Bias+in+Transformers+for+Question+Answering)|0|
|[Prompt Learning to Mitigate Catastrophic Forgetting in Cross-lingual Transfer for Open-domain Dialogue Generation](https://doi.org/10.1145/3539618.3592043)|Lei Liu, Jimmy Xiangji Huang|York University, Toronto, ON, Canada|Dialogue systems for non-English languages have long been under-explored. In this paper, we take the first step to investigate few-shot cross-lingual transfer learning (FS-XLT) and multitask learning (MTL) in the context of open-domain dialogue generation for non-English languages with limited data. We observed catastrophic forgetting in both FS-XLT and MTL for all 6 languages in our preliminary experiments. To mitigate the issue, we propose a simple yet effective prompt learning approach that can preserve the multilinguality of multilingual pre-trained language model (mPLM) in FS-XLT and MTL by bridging the gap between pre-training and fine-tuning with Fixed-prompt LM Tuning and our hand-crafted prompts. Experimental results on all 6 languages in terms of both automatic and human evaluations demonstrate the effectiveness of our approach. Our code is available at https://github.com/JeremyLeiLiu/XLinguDial.|长期以来，对非英语语言的对话系统探索不足。本文首先研究了数据有限的非英语语言开放领域对话生成过程中的短镜头跨语言迁移学习(few-shot cross-language Transfer learning，FS-XLT)和多任务学习(multitask learning，MTL)。在我们的初步实验中，我们观察到所有6种语言在 FS-XLT 和 MTL 中的灾难性遗忘。为了缓解这一问题，我们提出了一种简单而有效的快速学习方法，可以通过固定提示 LM 调优和我们手工制作的提示来弥合预训练和微调之间的差距，从而保持 FS-XLT 和 MTL 中多语言预训练语言模型(mPLM)的多语言性。在所有6种语言的自动和人工评估方面的实验结果证明了我们的方法的有效性。我们的代码可以在 https://github.com/jeremyleiliu/xlingudial 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Prompt+Learning+to+Mitigate+Catastrophic+Forgetting+in+Cross-lingual+Transfer+for+Open-domain+Dialogue+Generation)|0|
|[Reducing Spurious Correlations for Relation Extraction by Feature Decomposition and Semantic Augmentation](https://doi.org/10.1145/3539618.3592050)|Tianshu Yu, Min Yang, Chengming Li, Ruifeng Xu|Harbin Institute of Technology (Shenzhen), Shenzhen, China; SIAT, Chinese Academy of Sciences, Shenzhen, China; Shenzhen MSU-BIT University, Shenzhen, China|Deep neural models have become mainstream in relation extraction (RE), yielding state-of-the-art performance. However, most existing neural models are prone to spurious correlations between input features and prediction labels, making the models suffer from low robustness and generalization.In this paper, we propose a spurious correlation reduction method for RE via feature decomposition and semantic augmentation (denoted as FDSA). First, we decompose the original sentence representation into class-related features and context-related features. To obtain better context-related features, we devise a contrastive learning method to pull together the context-related features of the anchor sentence and its augmented sentences, and push away the context-related features of different anchor sentences. In addition, we propose gradient-based semantic augmentation on context-related features in order to improve the robustness of the RE model. Experiments on four datasets show that our model outperforms the strong competitors.|深层神经模型已经成为关系抽取(RE)的主流，具有最先进的性能。然而，现有的神经模型往往存在输入特征与预测标签之间的虚假关联，使得模型的鲁棒性和 generalization.in 性较差。本文提出了一种基于特征分解和语义增强的伪相关约简方法(简称 FDSA)。首先，我们将原始句子表示分解为与类相关的特征和与上下文相关的特征。为了获得更好的上下文相关特征，我们设计了一种对比学习方法，将锚语句及其增强句的上下文相关特征整合起来，去除不同锚语句的上下文相关特征。此外，为了提高 RE 模型的鲁棒性，提出了基于梯度的上下文相关特征语义增强方法。在四个数据集上的实验表明，我们的模型优于强有力的竞争对手。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reducing+Spurious+Correlations+for+Relation+Extraction+by+Feature+Decomposition+and+Semantic+Augmentation)|0|
|[EmoUS: Simulating User Emotions in Task-Oriented Dialogues](https://doi.org/10.1145/3539618.3592092)|HsienChin Lin, Shutong Feng, Christian Geishauser, Nurul Lubis, Carel van Niekerk, Michael Heck, Benjamin Matthias Ruppik, Renato Vukovic, Milica Gasic|Heinrich-Heine-Universität Düsseldorf, Düsseldorf, Germany|Existing user simulators (USs) for task-oriented dialogue systems only model user behaviour on semantic and natural language levels without considering the user persona and emotions. Optimising dialogue systems with generic user policies, which cannot model diverse user behaviour driven by different emotional states, may result in a high drop-off rate when deployed in the real world. Thus, we present EmoUS, a user simulator that learns to simulate user emotions alongside user behaviour. EmoUS generates user emotions, semantic actions, and natural language responses based on the user goal, the dialogue history, and the user persona. By analysing what kind of system behaviour elicits what kind of user emotions, we show that EmoUS can be used as a probe to evaluate a variety of dialogue systems and in particular their effect on the user's emotional state. Developing such methods is important in the age of large language model chat-bots and rising ethical concerns.|现有的面向任务对话系统的用户模拟器(USs)只是在语义和自然语言层面上对用户行为进行建模，而没有考虑用户的角色和情感。使用通用用户策略优化对话系统，不能模拟由不同情绪状态驱动的不同用户行为，在现实世界中部署时可能导致较高的下降率。因此，我们提出了 emoUS，一个用户模拟器，学习模拟用户的情绪以及用户的行为。基于用户目标、对话历史和用户角色，EmoUS 产生用户情感、语义动作和自然语言反应。通过分析什么样的系统行为引发了什么样的用户情绪，我们表明，情绪美可以作为一个探测器来评估各种对话系统，特别是他们对用户的情绪状态的影响。在大型语言模型聊天机器人时代，开发这样的方法非常重要，同时也引起了越来越多的道德关注。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EmoUS:+Simulating+User+Emotions+in+Task-Oriented+Dialogues)|0|
|[BizGraphQA: A Dataset for Image-based Inference over Graph-structured Diagrams from Business Domains](https://doi.org/10.1145/3539618.3591875)|Petr Babkin, William Watson, Zhiqiang Ma, Lucas Cecchi, Natraj Raman, Armineh Nourbakhsh, Sameena Shah|J.P. Morgan AI Research, London, United Kingdom; J.P. Morgan AI Research, Palo Alto, CA, USA; J.P. Morgan AI Research, New York, NY, USA|Graph-structured diagrams, such as enterprise ownership charts or management hierarchies, are a challenging medium for deep learning models as they not only require the capacity to model language and spatial relations but also the topology of links between entities and the varying semantics of what those links represent. Devising Question Answering models that automatically process and understand such diagrams have vast applications to many enterprise domains, and can move the state-of-the-art on multimodal document understanding to a new frontier. Curating real-world datasets to train these models can be difficult, due to scarcity and confidentiality of the documents where such diagrams are included. Recently released synthetic datasets are often prone to repetitive structures that can be memorized or tackled using heuristics. In this paper, we present a collection of 10,000 synthetic graphs that faithfully reflect properties of real graphs in four business domains, and are realistically rendered within a PDF document with varying styles and layouts. In addition, we have generated over 130,000 question instances that target complex graphical relationships specific to each domain. We hope this challenge will encourage the development of models capable of robust reasoning about graph structured images, which are ubiquitous in numerous sectors in business and across scientific disciplines.|图形结构图，如企业所有权图或管理层次结构图，是深度学习模型的一个具有挑战性的媒介，因为它们不仅需要建模语言和空间关系的能力，而且还需要实体之间链接的拓扑结构以及这些链接所代表的不同语义。设计自动处理和理解这些图表的问题回答模型在许多企业领域有着广泛的应用，并且可以将多模式文档理解的最新技术推向一个新的前沿。管理真实世界的数据集来训练这些模型可能是困难的，因为包含这些图表的文档是稀缺的和机密的。最近发布的合成数据集往往倾向于重复结构，可以通过启发式方法记忆或处理。在本文中，我们提出了一个10,000个合成图的集合，它忠实地反映了四个业务领域中真实图的属性，并且在一个具有不同样式和布局的 PDF 文档中真实地呈现。此外，我们已经生成了超过130,000个问题实例，这些实例针对每个领域特定的复杂图形关系。我们希望这一挑战将鼓励开发能够对图形结构图像进行强有力推理的模型，这些图形结构图像在商业和科学分支的许多领域中无处不在。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BizGraphQA:+A+Dataset+for+Image-based+Inference+over+Graph-structured+Diagrams+from+Business+Domains)|0|
|[Introducing MBIB - The First Media Bias Identification Benchmark Task and Dataset Collection](https://doi.org/10.1145/3539618.3591882)|Martin Wessel, Tomás Horych, Terry Ruas, Akiko Aizawa, Bela Gipp, Timo Spinde|Czech Technical University, Prague, Czech Rep; National Institute of Informatics, Tokyo, Japan; University of Göttingen, Göttingen, Germany; University of Konstanz, Konstanz, Germany|Although media bias detection is a complex multi-task problem, there is, to date, no unified benchmark grouping these evaluation tasks. We introduce the Media Bias Identification Benchmark (MBIB), a comprehensive benchmark that groups different types of media bias (e.g., linguistic, cognitive, political) under a common framework to test how prospective detection techniques generalize. After reviewing 115 datasets, we select nine tasks and carefully propose 22 associated datasets for evaluating media bias detection techniques. We evaluate MBIB using state-of-the-art Transformer techniques (e.g., T5, BART). Our results suggest that while hate speech, racial bias, and gender bias are easier to detect, models struggle to handle certain bias types, e.g., cognitive and political bias. However, our results show that no single technique can outperform all the others significantly. We also find an uneven distribution of research interest and resource allocation to the individual tasks in media bias. A unified benchmark encourages the development of more robust systems and shifts the current paradigm in media bias detection evaluation towards solutions that tackle not one but multiple media bias types simultaneously.|尽管媒介偏差检测是一个复杂的多任务问题，但迄今为止，还没有统一的基准对这些评估任务进行分组。我们介绍了媒介偏见识别基准(MBIB) ，这是一个综合性的基准，它将不同类型的媒介偏见(例如，语言的，认知的，政治的)归类在一个共同的框架下，以测试如何前瞻性检测技术一般化。在回顾了115个数据集之后，我们选择了9个任务并仔细提出了22个相关的数据集用于评估媒介偏倚检测技术。我们评估 MBIB 使用最先进的变压器技术(例如，T5，BART)。我们的研究结果表明，尽管仇恨言论、种族偏见和性别偏见更容易被发现，但模特们很难处理某些偏见类型，例如认知和政治偏见。然而，我们的研究结果表明，没有任何一种技术可以显著地优于所有其他技术。我们还发现，在媒介偏见下，研究兴趣和资源分配对个体任务的分配是不均衡的。统一的基准鼓励开发更健全的系统，并将目前媒体偏差检测评价的范式转向同时处理不是一种而是多种媒体偏差类型的解决方案。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Introducing+MBIB+-+The+First+Media+Bias+Identification+Benchmark+Task+and+Dataset+Collection)|0|
|[MetroScope: An Advanced System for Real-Time Detection and Analysis of Metro-Related Threats and Events via Twitter](https://doi.org/10.1145/3539618.3591807)|Jianfeng He, SyuanYing Wu, Abdulaziz Alhamadani, ChihFang Chen, WenFang Lu, ChangTien Lu, David Solnick, Yanlin Li|Washington Metropolitan Area Transit Authority, Washington, DC, USA; Virginia Tech, Falls Church, VA, USA|Metro systems are vital to our daily lives, but they face safety or reliability challenges, such as criminal activities or infrastructure disruptions, respectively. Real-time threat detection and analysis are crucial to ensure their safety and reliability. Although many existing systems use Twitter to detect metro-related threats or events in real-time, they have limitations in event analysis and system maintenance. Specifically, they cannot analyze event development, or prioritize events from numerous tweets. Besides, their users are required to continuously monitor system notifications, use inefficient content retrieval methods, and perform detailed system maintenance. We addressed those issues by developing the MetroScope system, a real-time threat/event detection system applied to Washington D.C. metro system. MetroScope can automatically analyze event development, prioritize events based on urgency, send emergency notifications via emails, provide efficient content retrieval, and self-maintain the system. Our MetroScope system is now available at http://orion.nvc.cs.vt.edu:5000/, with a video (https://www.youtube.com/watch?v=vKIK9M60-J8) introducing its features and instructions. MetroScope is a significant advancement in enhancing the safety and reliability of metro systems.|地铁系统对我们的日常生活至关重要，但它们分别面临安全或可靠性方面的挑战，例如犯罪活动或基础设施中断。实时的威胁检测和分析是保证其安全性和可靠性的关键。尽管许多现有系统使用 Twitter 实时检测与地铁相关的威胁或事件，但它们在事件分析和系统维护方面存在局限性。具体来说，它们不能分析事件开发，也不能对来自大量 tweet 的事件进行优先排序。此外，它们还要求用户不断监视系统通知，使用效率低下的内容检索方法，并执行详细的系统维护。我们通过开发 MetroScope 系统解决了这些问题，该系统是一个应用于华盛顿特区地铁系统的实时威胁/事件检测系统。MetroScope 可以自动分析事件发展，根据紧急情况对事件进行优先排序，通过电子邮件发送紧急通知，提供有效的内容检索，并自我维护系统。我们的 MetroScope 系统现在可以在 http://orion.nvc.cs.vt.edu:5000/上使用，并附有介绍其特性和使用说明的视频( https://www.youtube.com/watch?v=vkik9m60-j8)。MetroScope 在提高地铁系统的安全性和可靠性方面取得了重大进展。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MetroScope:+An+Advanced+System+for+Real-Time+Detection+and+Analysis+of+Metro-Related+Threats+and+Events+via+Twitter)|0|
|[FairUP: A Framework for Fairness Analysis of Graph Neural Network-Based User Profiling Models](https://doi.org/10.1145/3539618.3591814)|Mohamed Abdelrazek, Erasmo Purificato, Ludovico Boratto, Ernesto William De Luca|Otto von Guericke University Magdeburg & Leibniz Institute for Educational Media | Georg Eckert Institute, Magdeburg, Germany; University of Cagliari, Cagliari, Italy; Otto von Guericke University Magdeburg, Magdeburg, Germany|Modern user profiling approaches capture different forms of interactions with the data, from user-item to user-user relationships. Graph Neural Networks (GNNs) have become a natural way to model these behaviours and build efficient and effective user profiles. However, each GNN-based user profiling approach has its own way of processing information, thus creating heterogeneity that does not favour the benchmarking of these techniques. To overcome this issue, we present FairUP, a framework that standardises the input needed to run three state-of-the-art GNN-based models for user profiling tasks. Moreover, given the importance that algorithmic fairness is getting in the evaluation of machine learning systems, FairUP includes two additional components to (1) analyse pre-processing and post-processing fairness and (2) mitigate the potential presence of unfairness in the original datasets through three pre-processing debiasing techniques. The framework, while extensible in multiple directions, in its first version, allows the user to conduct experiments on four real-world datasets. The source code is available at https://link.erasmopurif.com/FairUP-source-code, and the web application is available at https://link.erasmopurif.com/FairUP.|现代用户分析方法捕获与数据的不同交互形式，从用户项到用户-用户关系。图形神经网络(GNN)已经成为一种自然的方式来建模这些行为，并建立有效和有效的用户配置文件。然而，每种基于 GNN 的用户分析方法都有自己处理信息的方式，因此产生了不利于这些技术基准测试的异构性。为了克服这个问题，我们提出了 FairUP，这是一个标准化的框架，用于运行三个最先进的基于 GNN 的模型来完成用户分析任务。此外，考虑到算法公平性在机器学习系统评估中的重要性，FairUP 包括两个额外的组成部分: (1)分析预处理和后处理公平性; (2)通过三种预处理去偏技术缓解原始数据集中潜在的不公平性。该框架在第一个版本中虽然可以向多个方向扩展，但允许用户在四个真实世界的数据集上进行实验。源代码可在 https://link.erasmopurif.com/fairup-source-code 下载，web 应用程序可在 https://link.erasmopurif.com/fairup 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FairUP:+A+Framework+for+Fairness+Analysis+of+Graph+Neural+Network-Based+User+Profiling+Models)|0|
|[SONAR: Web-based Tool for Multimodal Exploration of Non-Fungible Token Inspiration Networks](https://doi.org/10.1145/3539618.3591821)|Lucio La Cava, Davide Costa, Andrea Tagarelli|DIMES, University of Calabria, Rende, Italy|In this work, we present SONAR, a web-based tool for multimodal exploration of Non-Fungible Token (NFT) inspiration networks. SONAR is conceived to support both creators and traders in the emerging Web3 by providing an interactive visualization of the inspiration-driven connections between NFTs, at both individual level and collection level. SONAR can hence be useful to identify new investment opportunities as well as anomalous inspirations. To demonstrate SONAR's capabilities, we present an application to the largest and most representative dataset concerning the NFT landscape to date, showing how our proposed tool can scale and ensure high-level user experience up to millions of edges.|在这项工作中，我们提出了声纳，一个基于网络的工具，多模态探索非可替换令牌(NFT)的灵感网络。SONAR 被设计用来支持新兴的 Web3中的创建者和交易者，通过在个人层面和集合层面上提供一个交互式可视化的 NFT 之间的灵感驱动的连接。因此，声纳可以用来识别新的投资机会以及异常的灵感。为了展示 SONAR 的能力，我们展示了一个迄今为止最大和最具代表性的 NFT 数据集，展示了我们提议的工具如何扩展和确保高水平的用户体验达到数以百万计的边缘。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SONAR:+Web-based+Tool+for+Multimodal+Exploration+of+Non-Fungible+Token+Inspiration+Networks)|0|
|[MDI: A Debiasing Method Combining Unbiased and Biased Data](https://doi.org/10.1145/3539618.3591838)|Han Zhao, Qing Cui, Xinyu Li, Rongzhou Bao, Longfei Li, Jun Zhou, Zhehao Liu, Jinghua Feng|Ant Group, Hangzhou, China; Peking University, Beijing, China|In recent years, many methods have been proposed to alleviate the biases in recommender systems by combining biased data and unbiased data. Among these methods, data imputation method is effective, but previous works only employ a straightforward model to generate imputed data, which can not fully characterize the data. In this paper, we propose a novel data imputation approach that combines an unbiased model and a debiasing model with adaptively learnt weights. We conduct extensive experiments on two public recommendation datasets and one production dataset to demonstrate the effectiveness and robustness of the proposed method.|近年来，通过有偏数据和无偏数据相结合的方法来减少推荐系统中的偏差已经被提出了许多方法。在这些方法中，数据插补方法是有效的，但以往的工作只采用一个简单的模型来生成插补数据，不能充分表征数据。在本文中，我们提出了一种新的数据插补方法，结合无偏模型和去偏模型与自适应学习权重。为了验证该方法的有效性和鲁棒性，我们对两个公共推荐数据集和一个生产数据集进行了广泛的实验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MDI:+A+Debiasing+Method+Combining+Unbiased+and+Biased+Data)|0|
|[A Data-centric Solution to Improve Online Performance of Customer Service Bots](https://doi.org/10.1145/3539618.3591843)|Sen Hu, Changlin Yang, Junjie Wang, Siye Liu, Teng Xu, Wangshu Zhang, Jing Zheng|Ant Group, Beijing, China|The online performance of customer service bots is often less than satisfactory because of the gap between limited training data and real-world user questions. As a straightforward way to improve online performance, model iteration and re-deployment are time consuming and labor-intensive, and therefore difficult to sustain. To fix badcases and improve online performance of chatbots in a timely and continuous manner, we propose a data-centric solution consisting of three main modules: badcase detection, bad case correction, and answer extraction. By making full use of online model signals, implicit user feedback and artificial customer service log, the proposed solution can fix online badcases automatically. Our solution has been deployed and bringing consistently positive impacts for hundreds of customer service bots used by Alipay app.|客户服务机器人的在线性能往往不能令人满意，因为有限的训练数据和现实世界的用户问题之间的差距。作为提高在线性能的直接方法，模型迭代和重新部署耗费时间和人力，因此难以维持。为了及时、持续地修复恶例并提高聊天机器人的在线性能，提出了一种以数据为中心的解决方案，该方案由恶例检测、恶例修正和答案提取三个主要模块组成。该解决方案充分利用在线模型信号、隐式用户反馈和人工客户服务日志，实现了在线坏包的自动修复。我们的解决方案已经部署，并为支付宝应用程序使用的数百个客户服务机器人带来了持续的积极影响。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Data-centric+Solution+to+Improve+Online+Performance+of+Customer+Service+Bots)|0|
|[KATIE: A System for Key Attributes Identification in Product Knowledge Graph Construction](https://doi.org/10.1145/3539618.3591846)|Btissam Er Rahmadi, Arturo Oncevay, Yuanyi Ji, Jeff Z. Pan|The University of Edinburgh, Edinburgh, United Kingdom; Huawei Technologies R&D UK & The University of Edinburgh, Edinburgh, United Kingdom; Huawei Technologies R&D UK, Edinburgh, United Kingdom|We present part of Huawei's efforts in building a Product Knowledge Graph (PKG). We want to identify which product attributes (i.e. properties) are relevant and important in terms of shopping decisions to product categories (i.e. classes). This is particularly challenging when the attributes and their values are mined from online product catalogues, i.e. HTML pages. These web pages contain semi-structured data, which do not follow a concerted format and use diverse vocabulary to designate the same features. We propose a system for key attribute identification (KATIE) based on fine-tuning pre-trained models (e.g., DistilBERT) to predict the applicability and importance of an attribute to a category. We also propose an attribute synonyms identification module that allows us to discover synonymous attributes by considering not only their labels' similarities but also the similarity of their values sets. We have evaluated our approach to Huawei categories taxonomy and a set of internally mined attributes from web pages. KATIE guarantees promising performance results compared to the most recent baselines.|我们介绍华为在建立产品知识图表方面的部分工作。我们希望确定哪些产品属性(即属性)与产品类别(即类别)的购物决策相关且重要。当从在线产品目录(即 HTML 页面)中挖掘属性及其值时，这尤其具有挑战性。这些网页包含半结构化的数据，这些数据没有采用统一的格式，而是使用不同的词汇来表示相同的特征。提出了一种基于微调预训练模型(例如 DistilBERT)的关键属性识别系统(KATIE) ，用于预测属性对类别的适用性和重要性。我们还提出了一个属性同义词识别模块，该模块不仅考虑了同义属性标签的相似性，而且还考虑了同义属性值集的相似性，从而发现同义属性。我们已经评估了我们对华为分类法的方法，以及从网页中挖掘出来的一组内部属性。与最近的基线相比，KATIE 保证了有希望的性能结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=KATIE:+A+System+for+Key+Attributes+Identification+in+Product+Knowledge+Graph+Construction)|0|
|[Synerise Monad: A Foundation Model for Behavioral Event Data](https://doi.org/10.1145/3539618.3591851)|Barbara Rychalska, Szymon Lukasik, Jacek Dabrowski|Synerise & AGH University of Science and Technology, Krakow, Poland; Synerise, Warsaw, Poland; Synerise & Warsaw University of Technology, Warsaw, Poland|The complexity of industry-grade event-based datalakes grows dynamically each passing hour. Companies actively gather behavioral information on their customers, recording multiple types of events, such as clicks, likes, page views, card transactions, add-to-basket, or purchase events. In response to this, the Synerise Monad platform has been proposed. The primary focus of Monad is to produce Universal Behavioral Representations (UBRs) - large vectors encapsulating the behavioral patterns of each user. UBRs do not lose knowledge about individual events, in contrast to aggregated features or averaged embeddings. They are based on award-winning algorithms developed at Synerise - Cleora and EMDE - and allow to process real-life datasets composed of billions of events in record time. In this paper, we introduce a new aspect of Monad: private foundation models for behavioral data, trained on top of UBRs. The foundation models are trained in purely self-supervised manner and allow to exploit general knowledge about human behavior, which proves especially useful when multiple downstream models must be trained and time constraints are tight, or when labeled data is scarce. Experimental results show that the Monad foundation models can cut training time in half and require 3x less data to reach optimal results, often achieving state-of-the-art results.|基于事件的工业级数据湖的复杂性随着时间的推移而动态增长。公司积极地收集客户的行为信息，记录多种类型的事件，比如点击、点赞、页面浏览、卡片交易、添加到购物篮或购物事件。为了应对这种情况，Synerise Monad 平台已经被提出。Monad 的主要焦点是产生通用行为表征(UBRs)——封装每个用户行为模式的大型向量。与聚合特性或平均嵌入相比，UBRs 不会丢失关于单个事件的知识。它们基于 Synerise-Cleora 和 EMDE 开发的获奖算法，可以在创纪录的时间内处理由数十亿事件组成的实际数据集。在本文中，我们介绍了 Monad 的一个新的方面: 在 UBRs 之上训练的行为数据的私有基础模型。基础模型是纯自我监督的训练方式，并允许利用人类行为的一般知识，这证明了特别有用的时候，多个下游模型必须训练和时间约束是紧张的，或标记数据是稀缺的。实验结果表明，Monad 基础模型可以将训练时间减少一半，所需数据减少3倍，达到最佳结果，往往达到最先进的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Synerise+Monad:+A+Foundation+Model+for+Behavioral+Event+Data)|0|
|[Contextual Multilingual Spellchecker for User Queries](https://doi.org/10.1145/3539618.3591861)|Sanat Sharma, Josep VallsVargas, Tracy Holloway King, François Guerin, Chirag Arora|Adobe Inc., San Jose, CA, USA|Spellchecking is one of the most fundamental and widely used search features. Correcting incorrectly spelled user queries not only enhances the user experience but is expected by the user. However, most widely available spellchecking solutions are either lower accuracy than state-of-the-art solutions or too slow to be used for search use cases where latency is a key requirement. Furthermore, most innovative recent architectures focus on English and are not trained in a multilingual fashion and are trained for spell correction in longer text, which is a different paradigm from spell correction for user queries, where context is sparse (most queries are 1-2 words long). Finally, since most enterprises have unique vocabularies such as product names, off-the-shelf spelling solutions fall short of users' needs. In this work, we build a multilingual spellchecker that is extremely fast and scalable and that adapts its vocabulary and hence speller output based on a specific product's needs. Furthermore, our speller out-performs general purpose spellers by a wide margin on in-domain datasets. Our multilingual speller is used in search in Adobe products, powering autocomplete in various applications.|拼写检查是最基本和最广泛使用的搜索特性之一。纠正拼写错误的用户查询不仅增强了用户体验，而且是用户所期望的。然而，大多数广泛使用的拼写检查解决方案要么比最先进的解决方案准确率低，要么太慢，无法用于延迟是关键要求的搜索用例。此外，最近大多数创新的体系结构侧重于英语，没有以多语言方式进行培训，而是针对较长文本的拼写校正进行培训，这与用户查询的拼写校正是不同的范例，用户查询的上下文很少(大多数查询只有1-2个单词)。最后，由于大多数企业都有独特的词汇，如产品名称，现成的拼写解决方案不能满足用户的需求。在这项工作中，我们构建了一个多语言拼写检查器，它非常快速，可伸缩，并根据特定产品的需要调整其词汇表和拼写输出。此外，我们的拼写器性能优于通用拼写器在很大程度上的领域内数据集。我们的多语言拼写器用于 Adobe 产品的搜索，支持各种应用程序的自动完成。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contextual+Multilingual+Spellchecker+for+User+Queries)|0|
|[Exploring 360-Degree View of Customers for Lookalike Modeling](https://doi.org/10.1145/3539618.3591862)|Md. Mostafizur Rahman, Daisuke Kikuta, Satyen Abrol, Yu Hirate, Toyotaro Suzumura, Pablo Loyola, Takuma Ebisu, Manoj Kondapaka|The University of Tokyo, Tokyo, Japan; Rakuten Institute of Technology, Bengaluru, India; Rakuten Institute of Technology, Tokyo, Japan|Lookalike models are based on the assumption that user similarity plays an important role towards product selling and enhancing the existing advertising campaigns from a very large user base. Challenges associated to these models reside on the heterogeneity of the user base and its sparsity. In this work, we propose a novel framework that unifies the customers different behaviors or features such as demographics, buying behaviors on different platforms, customer loyalty behaviors and build a lookalike model to improve customer targeting for Rakuten Group, Inc. Extensive experiments on real e-commerce and travel datasets demonstrate the effectiveness of our proposed lookalike model for user targeting task.|相似模型是基于这样一个假设，即用户相似性对产品销售起着重要作用，并从一个非常大的用户基础上增强现有的广告活动。与这些模型相关的挑战在于用户基础的异质性及其稀疏性。在本文中，我们提出了一个新的框架，统一客户不同的行为或特征，如人口统计学，在不同的平台上的购买行为，客户忠诚行为和建立一个相似的模型，以改善客户的乐天集团，公司的目标顾客。在实际电子商务和旅游数据集上的大量实验证明了我们提出的相似模型对用户定位任务的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+360-Degree+View+of+Customers+for+Lookalike+Modeling)|0|
|[Evaluating Task-oriented Dialogue Systems with Users](https://doi.org/10.1145/3539618.3591788)|Clemencia Siro|University of Amsterdam, Amsterdam, Netherlands|Evaluation is one of the major concerns when developing information retrieval systems. Especially in the field of conversational AI, this topic has been heavily studied in the setting of both non-task and task-oriented conversational agents (dialogue systems).[1] Recently, several automatic metrics e.g., BLEU and ROUGE, proposed for the evaluation of dialogue systems, have shown poor correlation with human judgment and are thus ineffective for the evaluation of dialogue systems. As a consequence, a significant amount of research relies on human evaluation to estimate the effectiveness of dialogue systems[1, 4}. An emerging approach for evaluating task-oriented dialogue systems (TDS) is to estimate a user's overall satisfaction with the system from explicit and implicit user interaction signals [2, 3]. Though useful and effective, overall user satisfaction does not necessarily give insights into what aspects or dimensions a TDS is performing well on. Understanding why a user is satisfied or dissatisfied helps the TDS recover from an error and optimize towards an individual aspect to avoid total dissatisfaction during an interaction session. Understanding a user's satisfaction with TDS is crucial, mainly for two reasons. First, it allows system designers to understand different user perceptions regarding satisfaction, which in turn leads to better user personalization. Secondly, it can be used to avoid total dialogue failure by the system by deploying adaptive conversational approaches, such as failure recovery or switching topics. And, thus, fine-grained evaluation of TDS gives the system an opportunity to learn an individual user's interaction preferences leading to a fulfilled user goal. Therefore in this research, we take the first initiative toward understanding user satisfaction with TDS. We mainly focus on the fine-grained evaluation of conversational systems in a task-oriented setting.|在开发信息检索系统时，评估是主要关注的问题之一。特别是在会话人工智能领域，该课题在非任务和面向任务的会话代理(对话系统)的设置中得到了广泛的研究。[1]近年来，一些用于对话系统评估的自动指标，如 BLEU 和 ROUGE，与人类判断的相关性较差，因此对于对话系统的评估是无效的。因此，大量的研究依赖于人类评估来估计对话系统的有效性[1,4]。评估任务导向对话系统(TDS)的一种新兴方法是通过显性和隐性用户交互信号估计用户对系统的总体满意度[2,3]。虽然有用且有效，但总体用户满意度并不一定能够洞察 TDS 在哪些方面或维度上表现良好。理解用户满意或不满意的原因有助于 TDS 从错误中恢复，并针对个别方面进行优化，以避免在交互会话期间出现完全不满意的情况。理解用户对 TDS 的满意度是至关重要的，主要有两个原因。首先，它允许系统设计人员理解不同的用户对满意度的看法，这反过来又会导致更好的用户个性化。其次，它可以通过部署自适应的会话方法，如故障恢复或话题切换，来避免系统的全面对话失败。因此，TDS 的细粒度评估使系统有机会了解个人用户的交互偏好，从而实现用户目标。因此，在本研究中，我们率先了解使用者对 TDS 的满意度。本文主要研究任务导向环境下会话系统的细粒度评价问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Evaluating+Task-oriented+Dialogue+Systems+with+Users)|0|
|[Bridging Quantitative and Qualitative Digital Experience Testing](https://doi.org/10.1145/3539618.3591873)|Ranjitha Kumar|University of Illinois at Urbana-Champaign & UserTesting, Inc., Champaign & San Francisco, CA, USA|Digital user experiences are a mainstay of modern communication and commerce; multi-billion dollar industries have arisen around optimizing digital design. Usage analytics and A/B testing solutions allow growth hackers to quantitatively compute conversion over key user journeys, while user experience (UX) testing platforms enable UX researchers to qualitatively analyze usability and brand perception. Although these workflows are in pursuit of the same objective - producing better UX - the gulf between quantitative and qualitative testing is wide: they involve different stakeholders, and rely on disparate methodologies, budget, data streams, and software tools. This gap belies the opportunity to create a single platform that optimizes digital experiences holistically: using quantitative methods to uncover what and how much and qualitative analysis to understand why. Such a platform could monitor conversion funnels, identify ano­malous behaviors, intercept live users exhibiting those behaviors, and solicit explicit feedback in situ. This feedback could take many forms: survey responses, screen recordings of participants performing tasks, think-aloud audio, and more. By combining data from multiple users and correlating across feedback types, the platform could surface not just insights that a particular conversion funnel had been affected, but hypotheses about what had caused the change in user behavior. The platform could then rank these insights by how often the observed behavior occurred in the wild, using large-scale analytics to contextualize the results from small-scale UX tests. To this end, a decade of research has focused on interaction mining: a set of techniques for capturing interaction and design data from digital artifacts, and aggregating these multimodal data streams into structured representations bridging quantitative and qualitative experience testing[1-4]. During user sessions, interaction mining systems record user interactions (e.g., clicks, scrolls, text input), screen captures, and render-time data structures (e.g., website DOMs, native app view hierarchies). Once captured, these data streams are aligned and combined into user traces, sequences of user interactions semanticized by the design data of their UI targets [5]. The structure of these traces affords new workflows for composing quantitative and qualitative methods, building toward a unified platform for optimizing digital experiences.|数字用户体验是现代通信和商业的支柱; 数十亿美元的产业已经围绕优化数字设计而兴起。使用分析和 A/B 测试解决方案允许增长黑客定量计算关键用户旅程的转化率，而用户体验(UX)测试平台允许用户体验研究人员定性分析可用性和品牌认知。尽管这些工作流追求同一个目标——产生更好的用户体验——但定量测试和定性测试之间的鸿沟是巨大的: 它们涉及不同的利益相关者，并且依赖于不同的方法、预算、数据流和软件工具。这个差距掩盖了创建一个单一平台，从整体上优化数字体验的机会: 使用定量方法揭示什么和多少，定性分析理解为什么。这样的平台可以监控转换漏斗，识别异常行为，拦截表现出异常行为的现场用户，并在现场获得明确的反馈。这种反馈可以采取多种形式: 调查反馈、参与者执行任务的屏幕录音、有声思考音频等等。通过组合来自多个用户的数据和跨反馈类型的相关性，该平台不仅可以提供某个特定转换漏斗受到影响的见解，还可以提供有关导致用户行为变化的原因的假设。然后，该平台可以根据观察到的行为在野外发生的频率对这些见解进行排名，使用大规模分析来将小规模用户体验测试的结果联系起来。为此，十年的研究集中在交互挖掘: 一套从数字工件中获取交互和设计数据的技术，并将这些多模态数据流聚合成结构化表示，连接定量和定性经验测试[1-4]。在用户会话期间，交互挖掘系统记录用户交互(例如，点击、滚动、文本输入)、屏幕截图和呈现时间数据结构(例如，网站 DOM、本地应用程序视图层次结构)。一旦捕获，这些数据流就被对齐并组合成用户跟踪，用户交互的序列被其 UI 目标的设计数据语义化[5]。这些轨迹的结构为定量和定性方法的组合提供了新的工作流程，为优化数字体验建立了统一的平台。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bridging+Quantitative+and+Qualitative+Digital+Experience+Testing)|0|
|[MGeo: Multi-Modal Geographic Language Model Pre-Training](https://doi.org/10.1145/3539618.3591728)|Ruixue Ding, Boli Chen, Pengjun Xie, Fei Huang, Xin Li, Qiang Zhang, Yao Xu|Damo Academy, Alibaba Group, Hangzhou, China; Gaode Map, Alibaba Group, Beijing, China|Query and point of interest (POI) matching is a core task in location-based services~(LBS), e.g., navigation maps. It connects users' intent with real-world geographic information. Lately, pre-trained language models (PLMs) have made notable advancements in many natural language processing (NLP) tasks. To overcome the limitation that generic PLMs lack geographic knowledge for query-POI matching, related literature attempts to employ continued pre-training based on domain-specific corpus. However, a query generally describes the geographic context (GC) about its destination and contains mentions of multiple geographic objects like nearby roads and regions of interest (ROIs). These diverse geographic objects and their correlations are pivotal to retrieving the most relevant POI. Text-based single-modal PLMs can barely make use of the important GC and are therefore limited. In this work, we propose a novel method for query-POI matching, namely Multi-modal Geographic language model (MGeo), which comprises a geographic encoder and a multi-modal interaction module. Representing GC as a new modality, MGeo is able to fully extract multi-modal correlations to perform accurate query-POI matching. Moreover, there exists no publicly available query-POI matching benchmark. Intending to facilitate further research, we build a new open-source large-scale benchmark for this topic, i.e., Geographic TExtual Similarity (GeoTES). The POIs come from an open-source geographic information system (GIS) and the queries are manually generated by annotators to prevent privacy issues. Compared with several strong baselines, the extensive experiment results and detailed ablation analyses demonstrate that our proposed multi-modal geographic pre-training method can significantly improve the query-POI matching capability of PLMs with or without users' locations. Our code and benchmark are publicly available at https://github.com/PhantomGrapes/MGeo.|查询和感兴趣点(POI)匹配是基于位置服务 ~ (LBS)的核心任务，例如导航地图。它将用户的意图与现实世界的地理信息联系起来。最近，预训练语言模型(PLM)在许多自然语言处理(NLP)任务中取得了显著的进展。为了克服通用 PLM 在查询-POI 匹配方面缺乏地理知识的局限性，相关文献试图采用基于领域特定语料库的持续预训练。但是，查询通常描述其目的地的地理上下文(GC) ，并包含多个地理对象，如附近的道路和感兴趣的区域(ROI)。这些不同的地理对象及其相关性是检索最相关的 POI 的关键。基于文本的单模式 PLM 几乎不能利用重要的 GC，因此是有限的。本文提出了一种新的查询-POI 匹配方法，即多模态地理语言模型(MGeo) ，该模型由一个地理编码器和一个多模态交互模块组成。代表 GC 作为一种新的模式，MGeo 能够充分提取多模态相关性，以执行准确的查询-POI 匹配。此外，不存在公开可用的查询-POI 匹配基准。为了促进进一步的研究，我们为这个主题建立了一个新的开源大规模基准，即地理文本相似度(GeoTES)。POI 来自一个开源的地理信息系统(GIS) ，查询由注释器手动生成，以防止隐私问题。与几个强基线相比，大量的实验结果和详细的烧蚀分析表明，我们提出的多模态地理预训练方法可以显著提高有或无用户位置的 PLM 的查询-POI 匹配能力。我们的代码和基准已经在 https://github.com/phantomgrapes/mgeo 上公开发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MGeo:+Multi-Modal+Geographic+Language+Model+Pre-Training)|0|
|[Large Language Models are Versatile Decomposers: Decomposing Evidence and Questions for Table-based Reasoning](https://doi.org/10.1145/3539618.3591708)|Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, Yongbin Li|University of Science and Technology of China, Hefei, China; SIAT, Chinese Academy of Sciences, Shenzhen, China; Alibaba Group, Beijing, China|Table-based reasoning has shown remarkable progress in combining deep models with discrete reasoning, which requires reasoning over both free-form natural language (NL) questions and structured tabular data. However, previous table-based reasoning solutions usually suffer from significant performance degradation on huge evidence (tables). In addition, most existing methods struggle to reason over complex questions since the required information is scattered in different places. To alleviate the above challenges, we exploit large language models (LLMs) as decomposers for effective table-based reasoning, which (i) decompose huge evidence (a huge table) into sub-evidence (a small table) to mitigate the interference of useless information for table reasoning; and (ii) decompose complex questions into simpler sub-questions for text reasoning. Specifically, we first use the LLMs to break down the evidence (tables) involved in the current question, retaining the relevant evidence and excluding the remaining irrelevant evidence from the huge table. In addition, we propose a "parsing-execution-filling" strategy to alleviate the hallucination dilemma of the chain of thought by decoupling logic and numerical computation in each step. Extensive experiments show that our method can effectively leverage decomposed evidence and questions and outperforms the strong baselines on TabFact, WikiTableQuestion, and FetaQA datasets. Notably, our model outperforms human performance for the first time on the TabFact dataset.|基于表格的推理在深度模型与离散推理相结合方面取得了显著的进展，这需要对自由形式的自然语言(NL)问题和结构化表格数据进行推理。然而，以前的基于表的推理解决方案通常会在大量证据(表)上出现严重的性能下降。此外，由于所需的信息分散在不同的地方，大多数现有方法都难以对复杂问题进行推理。为了缓解上述挑战，我们利用大语言模型(LLM)作为有效的基于表的推理的分解器，它(i)分解巨大的证据(一个巨大的表)为子证据(一个小的表) ，以减轻无用的信息对表推理的干扰; (ii)分解复杂的问题为文本推理的简单的子问题。具体来说，我们首先使用 LLM 来分解当前问题中涉及的证据(表格) ，保留相关的证据，并从庞大的表格中排除其余不相关的证据。此外，我们还提出了一种“解析-执行-填充”策略，通过在每个步骤中解耦逻辑和数值计算来缓解思维链的幻觉困境。大量的实验表明，我们的方法可以有效地利用分解的证据和问题，并优于 TabFact、 WikiTablequestions 和 FetaQA 数据集的强基线。值得注意的是，我们的模型第一次在 TabFact 数据集上优于人类性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+are+Versatile+Decomposers:+Decomposing+Evidence+and+Questions+for+Table-based+Reasoning)|0|
|[DisCover: Disentangled Music Representation Learning for Cover Song Identification](https://doi.org/10.1145/3539618.3591664)|Jiahao Xun, Shengyu Zhang, Yanting Yang, Jieming Zhu, Liqun Deng, Zhou Zhao, Zhenhua Dong, Ruiqi Li, Lichao Zhang, Fei Wu|Huawei Noah's Ark Lab, Shenzhen, China; Zhejiang University, Hangzhou, China|In the field of music information retrieval (MIR), cover song identification (CSI) is a challenging task that aims to identify cover versions of a query song from a massive collection. Existing works still suffer from high intra-song variances and inter-song correlations, due to the entangled nature of version-specific and version-invariant factors in their modeling. In this work, we set the goal of disentangling version-specific and version-invariant factors, which could make it easier for the model to learn invariant music representations for unseen query songs. We analyze the CSI task in a disentanglement view with the causal graph technique, and identify the intra-version and inter-version effects biasing the invariant learning. To block these effects, we propose the disentangled music representation learning framework (DisCover) for CSI. DisCover consists of two critical components: (1) Knowledge-guided Disentanglement Module (KDM) and (2) Gradient-based Adversarial Disentanglement Module (GADM), which block intra-version and inter-version biased effects, respectively. KDM minimizes the mutual information between the learned representations and version-variant factors that are identified with prior domain knowledge. GADM identifies version-variant factors by simulating the representation transitions between intra-song versions, and exploits adversarial distillation for effect blocking. Extensive comparisons with best-performing methods and in-depth analysis demonstrate the effectiveness of DisCover and the and necessity of disentanglement for CSI.|在音乐信息检索(MIR)领域，翻唱歌曲识别(CSI)是一项具有挑战性的任务，其目标是从大量的收藏中识别出一首查询歌曲的翻唱版本。现有作品由于版本特异性因素和版本不变性因素的纠缠性，仍然存在着高度的内部歌曲变异和内部歌曲相关性。本文设定了版本不变因子和版本不变因子的分离目标，使得模型更容易学习未知查询歌曲的不变音乐表示。本文运用因果图技术对 CSI 任务进行了解缠分析，识别出内版本效应和内版本效应对不变学习的影响。为了阻止这些效应，我们提出了一个用于 CSI 的分离音乐表征学习框架(DisCover)。DisCover 由两个关键组成部分组成: (1)知识引导的解缠模块(KDM)和(2)基于梯度的对抗性解缠模块(GADM) ，它们分别阻止内部版本效应和内部版本效应。KDM 最小化了学习表示和版本变异因子之间的相互信息，这些因子与先验领域知识一致。GADM 通过模拟歌曲内部版本之间的表示转换来识别版本变异因素，并利用对抗精馏来阻断效果。通过与最佳方法的广泛比较和深入分析，证明了 DisCover 的有效性以及 CSI 解纠缠的必要性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DisCover:+Disentangled+Music+Representation+Learning+for+Cover+Song+Identification)|0|
|[Augmenting Low-Resource Text Classification with Graph-Grounded Pre-training and Prompting](https://doi.org/10.1145/3539618.3591641)|Zhihao Wen, Yuan Fang|Singapore Management University, Singapore, Singapore|Text classification is a fundamental problem in information retrieval with many real-world applications, such as predicting the topics of online articles and the categories of e-commerce product descriptions. However, low-resource text classification, with few or no labeled samples, poses a serious concern for supervised learning. Meanwhile, many text data are inherently grounded on a network structure, such as a hyperlink/citation network for online articles, and a user-item purchase network for e-commerce products. These graph structures capture rich semantic relationships, which can potentially augment low-resource text classification. In this paper, we propose a novel model called Graph-Grounded Pre-training and Prompting (G2P2) to address low-resource text classification in a two-pronged approach. During pre-training, we propose three graph interaction-based contrastive strategies to jointly pre-train a graph-text model; during downstream classification, we explore prompting for the jointly pre-trained model to achieve low-resource classification. Extensive experiments on four real-world datasets demonstrate the strength of G2P2 in zero- and few-shot low-resource text classification tasks.|文本分类是许多实际应用程序的基本信息检索，例如预测在线文章的主题和电子商务产品说明的类别。然而，低资源的文本分类，很少或没有标记的样本，提出了一个严重的问题，监督式学习。与此同时，许多文本数据内在地基于网络结构，如在线文章的超链接/引用网络，以及电子商务产品的用户项目购买网络。这些图结构捕获了丰富的语义关系，可以潜在地增强低资源文本分类。本文提出了一种新的基于图的预训练和提示(G2P2)模型，采用双管齐下的方法解决低资源文本分类问题。在预训练阶段，我们提出了三种基于图形交互的对比策略来联合预训练一个图文模型; 在下游分类阶段，我们探索了联合预训练模型来实现低资源分类的提示。在四个真实世界数据集上的大量实验证明了 G2P2在零镜头和少镜头低资源文本分类任务中的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Augmenting+Low-Resource+Text+Classification+with+Graph-Grounded+Pre-training+and+Prompting)|0|
|[Smooth Operators for Effective Systematic Review Queries](https://doi.org/10.1145/3539618.3591768)|Harrisen Scells, Ferdinand Schlatt, Martin Potthast|Leipzig University & ScaDS.AI, Leipzig, Germany; Friedrich-Schiller-Universität, Jena, Germany; Leipzig University, Leipzig, Germany|Effective queries are crucial to minimising the time and cost of medical systematic reviews, as all retrieved documents must be judged for relevance. Boolean queries, developed by expert librarians, are the standard for systematic reviews. They guarantee reproducible and verifiable retrieval and more control than free-text queries. However, the result sets of Boolean queries are unranked and difficult to control due to the strict Boolean operators. We address these problems in a single unified retrieval model by formulating a class of smooth operators that are compatible with and extend existing Boolean operators. Our smooth operators overcome several shortcomings of previous extensions of the Boolean retrieval model. In particular, our operators are independent of the underlying ranking function, so that exact-match and large language model rankers can be combined in the same query. We found that replacing Boolean operators with equivalent or similar smooth operators often improves the effectiveness of queries. Their properties make tuning a query to precision or recall intuitive and allow greater control over how documents are retrieved. This additional control leads to more effective queries and reduces the cost of systematic reviews.|有效的查询对于最大限度地减少医疗系统评价的时间和成本至关重要，因为所有检索到的文档都必须判断其相关性。由专家图书馆员开发的布尔查询是系统评审的标准。它们保证了可重复和可验证的检索，并且比自由文本查询更具控制性。然而，由于严格的布尔运算符，布尔查询的结果集是无序的并且难以控制。我们通过构造一类与现有布尔算子兼容并扩展的平滑算子，在一个统一的检索模型中解决了这些问题。我们的平滑算子克服了以前布尔检索模型扩展的一些缺点。特别地，我们的运算符独立于底层的排序函数，因此精确匹配和大型语言模型排序器可以组合在同一个查询中。我们发现，用等效或类似的平滑运算符替换布尔运算符通常会提高查询的有效性。它们的属性使得将查询调优到精确或召回非常直观，并允许对如何检索文档进行更大的控制。这种额外的控制导致更有效的查询，并降低了系统评审的成本。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Smooth+Operators+for+Effective+Systematic+Review+Queries)|0|
|[Do-GOOD: Towards Distribution Shift Evaluation for Pre-Trained Visual Document Understanding Models](https://doi.org/10.1145/3539618.3591670)|Jiabang He, Yi Hu, Lei Wang, Xing Xu, Ning Liu, Hui Liu, Heng Tao Shen|University of Electronic Science and Technology of China, Chengdu, China; Beijing Rongda Technology Co., Ltd., Beijing, China; Beijing Forestry University, Beijing, China; University of Electronic Science and Technology of China & Peng Cheng Laboratory, Chengdu & Shenzhen, China; Singapore Management University, Singapore , Singapore|Numerous pre-training techniques for visual document understanding (VDU) have recently shown substantial improvements in performance across a wide range of document tasks. However, these pre-trained VDU models cannot guarantee continued success when the distribution of test data differs from the distribution of training data. In this paper, to investigate how robust existing pre-trained VDU models are to various distribution shifts, we first develop an out-of-distribution (OOD) benchmark termed Do-GOOD for the fine-Grained analysis on Document image-related tasks specifically. The Do-GOOD benchmark defines the underlying mechanisms that result in different distribution shifts and contains 9 OOD datasets covering 3 VDU related tasks, e.g., document information extraction, classification and question answering. We then evaluate the robustness and perform a fine-grained analysis of 5 latest VDU pre-trained models and 2 typical OOD generalization algorithms on these OOD datasets. Results from the experiments demonstrate that there is a significant performance gap between the in-distribution (ID) and OOD settings for document images, and that fine-grained analysis of distribution shifts can reveal the brittle nature of existing pre-trained VDU models and OOD generalization algorithms. The code and datasets for our Do-GOOD benchmark can be found at https://github.com/MAEHCM/Do-GOOD.|许多可视化文档理解(VDU)的预训练技术最近显示了在广泛的文档任务中性能的实质性改进。然而，当测试数据的分布与训练数据的分布不同时，这些预先训练的 VDU 模型不能保证连续的成功。为了研究现有的预先训练的 VDU 模型对不同分布转移的鲁棒性，我们首先开发了一个名为 Do-Good 的分布外(OOD)基准，专门用于文档图像相关任务的细粒度分析。Do-GOOD 基准定义了导致不同分配转移的基本机制，包含9个面向对象数据集，涵盖3个与视觉数据单相关的任务，例如文档信息抽取、分类和问题回答。然后，我们评估了稳健性，并对这些 OOD 数据集上的5个最新的 VDU 预训练模型和2个典型的 OOD 泛化算法进行了细粒度分析。实验结果表明，文档图像的内分布(ID)和面向对象设置(OOD)之间存在显著的性能差距，对分布偏移的细粒度分析可以揭示现有预先训练的 VDU 模型和面向对象设置泛化算法的脆弱性。我们的 Do-GOOD 基准的代码和数据集可以在 https://github.com/maehcm/Do-GOOD 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Do-GOOD:+Towards+Distribution+Shift+Evaluation+for+Pre-Trained+Visual+Document+Understanding+Models)|0|
|[Continual Learning on Dynamic Graphs via Parameter Isolation](https://doi.org/10.1145/3539618.3591652)|Peiyan Zhang, Yuchen Yan, Chaozhuo Li, Senzhang Wang, Xing Xie, Guojie Song, Sunghun Kim|Hong Kong University of Science and Technology, Hong Kong, Hong Kong; School of Computer Science and Engineering, Central South University, Changsha, China; Hong Kong University of Science and Technology, Beijing, China; Microsoft Research Asia, Beijing, China; School of Intelligence Science and Technology, Peking University, Beijing, China|Many real-world graph learning tasks require handling dynamic graphs where new nodes and edges emerge. Dynamic graph learning methods commonly suffer from the catastrophic forgetting problem, where knowledge learned for previous graphs is overwritten by updates for new graphs. To alleviate the problem, continual graph learning methods are proposed. However, existing continual graph learning methods aim to learn new patterns and maintain old ones with the same set of parameters of fixed size, and thus face a fundamental tradeoff between both goals. In this paper, we propose Parameter Isolation GNN (PI-GNN) for continual learning on dynamic graphs that circumvents the tradeoff via parameter isolation and expansion. Our motivation lies in that different parameters contribute to learning different graph patterns. Based on the idea, we expand model parameters to continually learn emerging graph patterns. Meanwhile, to effectively preserve knowledge for unaffected patterns, we find parameters that correspond to them via optimization and freeze them to prevent them from being rewritten. Experiments on eight real-world datasets corroborate the effectiveness of PI-GNN compared to state-of-the-art baselines.|许多真实世界的图形学习任务需要处理出现新节点和边的动态图形。动态图学习方法通常受到灾难性遗忘问题的困扰，在这个问题中，先前图的知识被新图的更新所覆盖。为了解决这一问题，提出了连续图学习方法。然而，现有的连续图学习方法的目的是学习新的模式和保持固定大小的相同的参数集的旧模式，因此面临着两个目标之间的基本权衡。本文提出了参数隔离 GNN (PI-GNN) ，用于动态图的连续学习，避免了通过参数隔离和扩展进行折衷的问题。我们的动机在于不同的参数有助于学习不同的图形模式。基于这一思想，我们扩展了模型参数来不断学习新兴的图形模式。同时，为了有效地保存未受影响的模式的知识，我们通过优化找到与之对应的参数，并冻结它们以防止它们被重写。在八个真实世界数据集上的实验证实了 PI-GNN 与最先进的基线相比的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Continual+Learning+on+Dynamic+Graphs+via+Parameter+Isolation)|0|
|[An Effective, Efficient, and Scalable Confidence-based Instance Selection Framework for Transformer-Based Text Classification](https://doi.org/10.1145/3539618.3591638)|Washington Cunha, Celso França, Guilherme Fonseca, Leonardo Rocha, Marcos André Gonçalves|Federal University of São João del Rei, São João del Rei, Brazil; Federal University of Minas Gerais, Belo Horizonte, Brazil|Transformer-based deep learning is currently the state-of-the-art in many NLP and IR tasks. However, fine-tuning such Transformers for specific tasks, especially in scenarios of ever-expanding volumes of data with constant re-training requirements and budget constraints, is costly (computationally and financially) and energy-consuming. In this paper, we focus on Instance Selection (IS) - a set of methods focused on selecting the most representative documents for training, aimed at maintaining (or improving) classification effectiveness while reducing total time for training (or fine-tuning). We propose E2SC-IS -- Effective, Efficient, and Scalable Confidence-Based IS -- a two-step framework with a particular focus on Transformers and large datasets. E2SC-IS estimates the probability of each instance being removed from the training set based on scalable, fast, and calibrated weak classifiers. E2SC-IS also exploits iterative heuristics to estimate a near-optimal reduction rate. Our solution can reduce the training sets by 29% on average while maintaining the effectiveness in all datasets, with speedup gains up to 70%, scaling for very large datasets (something that the baselines cannot do).|基于变压器的深度学习是目前许多自然语言处理和红外任务中的最新技术。然而，针对特定任务对这些变形金刚进行微调，特别是在不断扩大的数据量、不断的再培训要求和预算限制的情况下，成本(计算和财务)和能源消耗都很高。在本文中，我们重点讨论实例选择(IS)-一组方法集中在选择最有代表性的文档进行培训，旨在保持(或提高)分类的有效性，同时减少总的培训时间(或微调)。我们提出了 E2SC-IS ——高效、高效和可扩展的基于置信度的 IS ——一个两步框架，特别关注变压器和大型数据集。基于可伸缩、快速和校准的弱分类器，E2SC-IS 估计每个实例从训练集中移除的概率。E2SC-IS 还利用迭代启发式算法来估计接近最优的减速率。我们的解决方案可以平均减少29% 的训练集，同时保持所有数据集的有效性，加速增益高达70% ，适用于非常大的数据集(这是基线无法做到的)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Effective,+Efficient,+and+Scalable+Confidence-based+Instance+Selection+Framework+for+Transformer-Based+Text+Classification)|0|
|[EDIndex: Enabling Fast Data Queries in Edge Storage Systems](https://doi.org/10.1145/3539618.3591676)|Qiang He, Siyu Tan, Feifei Chen, Xiaolong Xu, Lianyong Qi, Xinhong Hei, Hai Jin, Yun Yang|Xi'an University of Technology, Xi'an, China; Swinburne University of Technology, Hawthorn, VIC, Australia; Deakin University, Burwood, VIC, Australia; China University of Petroleum, Qingdao, China; Huazhong University of Science and Technology, Wuhan, China; Southeast University, Nanjing, China; Huazhong University of Science and Technology & Swinburne University of Technology, Wuhan, China; Nanjing University of Information Science and Technology, Nanjing, China|In an edge storage system, popular data can be stored on edge servers to enable low-latency data retrieval for nearby users. Suffering from constrained storage capacities, edge servers must process users' data requests collaboratively. For sourcing data, it is essential to find out which edge servers in the system have the requested data. In this paper, we make the first attempt to study this edge data query (EDQ) problem and present EDIndex, a distributed Edge Data Indexing system to enable fast data queries at the edge. First, we introduce a new index structure named Counting Bloom Filter (CBF) tree for facilitating edge data queries. Then, to improve query performance, we enhance EDIndex with a novel index structure named hierarchical Counting Bloom Filter (HCBF) tree. In EDIndex, each edge server maintains an HCBF tree that indexes the data stored on nearby edge servers to facilitate data sourcing between edge servers at the edge. The results of extensive experiments conducted on an edge storage system comprised of 90 edge servers demonstrate that EDIndex 1) takes up to 8.8x less time to answer edge data queries compared with state-of-the-art edge indexing systems; and 2) can be implemented in practice with a high query accuracy at low initialization and maintenance overheads.|在边缘存储系统中，流行的数据可以存储在边缘服务器上，以便对附近的用户进行低延迟的数据检索。由于存储容量有限，边缘服务器必须协同处理用户的数据请求。为了获取数据，必须找出系统中哪些边缘服务器具有所请求的数据。本文首次尝试研究边缘数据查询(EDQ)问题，提出了一种分布式边缘数据索引系统 EDIndex，该系统可以实现边缘数据的快速查询。首先，我们引入了一种新的索引结构，称为 Counting Bloom Filter (CBF)树，以方便边缘数据查询。然后，为了提高查询性能，我们提出了一种新的索引结构——分层计数布鲁姆过滤(HCBF)树来增强 EDIndex。在 EDIndex 中，每个边缘服务器维护一个 HCBF 树，该树对存储在附近边缘服务器上的数据进行索引，以促进边缘服务器之间的数据源。在由90台边缘服务器组成的边缘存储系统上进行的大量实验结果表明，EDIndex 1)与最先进的边缘索引系统相比，回答边缘数据查询所需的时间少了8.8倍; 以及2)可以在实践中以低初始化和维护开销的高查询精度实现。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EDIndex:+Enabling+Fast+Data+Queries+in+Edge+Storage+Systems)|0|
|[Extending Label Aggregation Models with a Gaussian Process to Denoise Crowdsourcing Labels](https://doi.org/10.1145/3539618.3591685)|Dan Li, Maarten de Rijke|University of Amsterdam & Elsevier, Amsterdam, Netherlands; University of Amsterdam, Amsterdam, Netherlands|Label aggregation (LA) is the task of inferring a high-quality label for an example from multiple noisy labels generated by either human annotators or model predictions. Existing work on LA assumes a label generation process and designs a probabilistic graphical model (PGM) to learn latent true labels from observed crowd labels. However, the performance of PGM-based LA models is easily affected by the noise of the crowd labels. As a consequence, the performance of LA models differs on different datasets and no single LA model outperforms the rest on all datasets. We extend PGM-based LA models by integrating a GP prior on the true labels. The advantage of LA models extended with a GP prior is that they can take as input crowd labels, example features, and existing pre-trained label prediction models to infer the true labels, while the original LA can only leverage crowd labels. Experimental results on both synthetic and real datasets show that any LA models extended with a GP prior and a suitable mean function achieves better performance than the underlying LA models, demonstrating the effectiveness of using a GP prior.|标签聚合(LA)是从人类注释者或模型预测产生的多个噪声标签中推断出一个高质量标签的任务。现有的 LA 工作假设标签生成过程，并设计了一个概率图形模型(PGM)来从观察到的人群标签中学习潜在的真实标签。然而，基于 PGM 的 LA 模型的性能很容易受到人群标签噪声的影响。因此，LA 模型在不同数据集上的性能是不同的，没有一个 LA 模型在所有数据集上的性能优于其他模型。我们扩展了基于 PGM 的 LA 模型通过集成一个 GP 先验的真实标签。利用 GP 先验扩展的 LA 模型的优势在于，它可以将人群标签、示例特征和已有的预训练标签预测模型作为输入，从而推断出真实的标签，而原始的 LA 模型只能利用人群标签。在合成和实际数据集上的实验结果表明，任何具有 GP 先验和适当均值函数的 LA 模型都比基本 LA 模型具有更好的性能，证明了使用 GP 先验的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Extending+Label+Aggregation+Models+with+a+Gaussian+Process+to+Denoise+Crowdsourcing+Labels)|0|
|[Dataset Preparation for Arbitrary Object Detection: An Automatic Approach based on Web Information in English](https://doi.org/10.1145/3539618.3591661)|Shucheng Li, Boyu Chang, Bo Yang, Hao Wu, Sheng Zhong, Fengyuan Xu|National Key Lab for Novel Software Technology, Nanjing University, Nanjing, China|Automatic dataset preparation can help users avoid labor-intensive and costly manual data annotations. The difficulty in preparing a high-quality dataset for object detection involves three key aspects: relevance, naturality, and balance, which are not addressed by existing works. In this paper, we leverage information from the web, and propose a fully-automatic dataset preparation mechanism without any human annotation, which can automatically prepare a high-quality training dataset for the detection task with English text terms describing target objects. It contains three key designs, i.e., keyword expansion, data de-noising, and data balancing. Our experiments demonstrate that the object detectors trained with auto-prepared data are comparable to those trained with benchmark datasets and outperform other baselines. We also demonstrate the effectiveness of our approach in several more challenging real-world object categories that are not included in the benchmark datasets.|自动数据集准备可以帮助用户避免劳动密集型和昂贵的手工数据注释。为目标检测准备一个高质量的数据集的难度涉及三个关键方面: 相关性、自然性和平衡性，这些都是现有作品没有涉及到的。本文利用网络信息，提出了一种无需人工注释的全自动数据集准备机制，该机制能够自动准备高质量的训练数据集，用于目标物体的检测任务。它包含三个关键设计，即关键字扩展、数据去噪和数据平衡。实验结果表明，用自动准备数据训练的目标检测器与用基准数据训练的目标检测器具有可比性，其检测性能优于其他基准检测器。我们还展示了我们的方法在几个更具挑战性的实际对象类别中的有效性，这些对象类别不包括在基准数据集中。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dataset+Preparation+for+Arbitrary+Object+Detection:+An+Automatic+Approach+based+on+Web+Information+in+English)|0|
|[Leader-Generator Net: Dividing Skill and Implicitness for Conquering FairytaleQA](https://doi.org/10.1145/3539618.3591710)|Wei Peng, Wanshui Li, Yue Hu|University College London, London, United Kingdom; Institute of Information Engineering, Chinese Academy of Sciences & University of Chinese Academy of Sciences, Beijing, China|Machine reading comprehension requires systems to understand the given passage and answer questions. Previous methods mainly focus on the interaction between the question and passage. However, they ignore the deep exploration of cognitive elements behind questions, such as fine-grained reading skills (this paper focuses on narrative comprehension skills) and implicitness or explicitness of the question (whether the answer can be found in the passage). Grounded in prior literature on reading comprehension, the understanding of a question is a complex process where human beings need to understand the semantics of the question, use different reading skills for different questions, and then judge the implicitness of the question. To this end, a simple but effective Leader-Generator Network is proposed to explicitly separate and extract fine-grained reading skills and the implicitness or explicitness of the question. Specifically, the proposed skill leader accurately captures the semantic representation of fine-grained reading skills with contrastive learning. And the implicitness-aware pointer-generator adaptively extracts or generates the answer based on the implicitness or explicitness of the question. Furthermore, to validate the generalizability of the methodology, we annotate a new dataset named NarrativeQA 1.1. Experiments on the FairytaleQA and NarrativeQA 1.1 show that the proposed model achieves the state-of-the-art performance (about 5% gain on Rouge-L) on the question answering task. Our annotated data and code are available at https://github.com/pengwei-iie/Leader-Generator-Net.|机器阅读理解要求系统理解给定的文章并回答问题。以往的研究方法主要集中在问句和短文之间的交互作用上。然而，他们忽视了对问题背后认知因素的深入探索，如细粒度阅读技巧(本文侧重于叙事理解技巧)和问题的隐含性或明确性(是否能在文章中找到答案)。根据以往关于阅读理解的文献，理解问题是一个复杂的过程，人们需要理解问题的语义，对不同的问题使用不同的阅读技巧，然后判断问题的隐含性。为此，本文提出了一种简单而有效的领导者-发生器网络，以明确地区分和提取细粒度阅读技能和问题的隐含性或明显性。具体来说，提出的技能领导者准确地捕获了语义表示的细粒度阅读技能与对比学习。而隐式感知指针生成器则根据问题的隐式性或显式性自适应地提取或生成答案。此外，为了验证该方法的普遍性，我们注释了一个名为 NarratveQA 1.1的新数据集。在 FairytaleQA 和 NarratveQA 1.1上的实验表明，该模型在问答任务上达到了最先进的性能(在 Rouge-L 上大约增加了5%)。我们的注释数据和代码可以在 https://github.com/pengwei-iie/leader-generator-net 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leader-Generator+Net:+Dividing+Skill+and+Implicitness+for+Conquering+FairytaleQA)|0|
|[BiTimeBERT: Extending Pre-Trained Language Representations with Bi-Temporal Information](https://doi.org/10.1145/3539618.3591686)|Jiexin Wang, Adam Jatowt, Masatoshi Yoshikawa, Yi Cai|Kyoto University, Kyoto, Japan; University of Innsbruck, Innsbruck, Austria; South China University of Technology, Guangzhou, China|Time is an important aspect of documents and is used in a range of NLP and IR tasks. In this work, we investigate methods for incorporating temporal information during pre-training to further improve the performance on time-related tasks. Compared with common pre-trained language models like BERT which utilize synchronic document collections (e.g., BookCorpus and Wikipedia) as the training corpora, we use long-span temporal news article collection for building word representations. We introduce BiTimeBERT, a novel language representation model trained on a temporal collection of news articles via two new pre-training tasks, which harnesses two distinct temporal signals to construct time-aware language representations. The experimental results show that BiTimeBERT consistently outperforms BERT and other existing pre-trained models with substantial gains on different downstream NLP tasks and applications for which time is of importance (e.g., the accuracy improvement over BERT is 155\% on the event time estimation task).|时间是文档的一个重要方面，用于一系列自然语言处理(NLP)和信息检索(IR)任务。在这项工作中，我们研究的方法，合并时间信息在预训练，以进一步提高绩效的时间相关的任务。与使用同步文档集合(如 BookCorpus 和 Wikipedia)作为训练语料库的常见的预训练语言模型 BERT 相比，我们使用大跨度的时态新闻文章集合来构建单词表示。我们介绍了一种新的语言表示模型 BiTimeBERT，该模型通过两个新的预训练任务训练新闻文章的时间集合，利用两个不同的时间信号来构造具有时间意识的语言表示。实验结果表明，BiTimeBERT 在不同的下游 NLP 任务和重要时间应用(例如，在事件时间估计任务上，误码率比 BERT 提高了155%)上，始终优于 BERT 和其他已有的预训练模型，获得了实质性的提高。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BiTimeBERT:+Extending+Pre-Trained+Language+Representations+with+Bi-Temporal+Information)|0|
|[Incorporating Structured Sentences with Time-enhanced BERT for Fully-inductive Temporal Relation Prediction](https://doi.org/10.1145/3539618.3591700)|Zhongwu Chen, Chengjin Xu, Fenglong Su, Zhen Huang, Yong Dou|International Digital Economy Academy, Shenzhen, China; National University of Defense Technology, Changsha, China|Temporal relation prediction in incomplete temporal knowledge graphs (TKGs) is a popular temporal knowledge graph completion (TKGC) problem in both transductive and inductive settings. Traditional embedding-based TKGC models (TKGE) rely on structured connections and can only handle a fixed set of entities, i.e., the transductive setting. In the inductive setting where test TKGs contain emerging entities, the latest methods are based on symbolic rules or pre-trained language models (PLMs). However, they suffer from being inflexible and not time-specific, respectively. In this work, we extend the fully-inductive setting, where entities in the training and test sets are totally disjoint, into TKGs and take a further step towards a more flexible and time-sensitive temporal relation prediction approach SST-BERT, incorporating Structured Sentences with Time-enhanced BERT. Our model can obtain the entity history and implicitly learn rules in the semantic space by encoding structured sentences, solving the problem of inflexibility. We propose to use a time masking MLM task to pre-train BERT in a corpus rich in temporal tokens specially generated for TKGs, enhancing the time sensitivity of SST-BERT. To compute the probability of occurrence of a target quadruple, we aggregate all its structured sentences from both temporal and semantic perspectives into a score. Experiments on the transductive datasets and newly generated fully-inductive benchmarks show that SST-BERT successfully improves over state-of-the-art baselines.|不完备时态知识图(TKGs)中的时态关系预测问题是一个普遍存在的时态知识图补全问题。传统的基于嵌入的 TKGC 模型(TKGE)依赖于结构化连接，只能处理一组固定的实体，即传导性设置。在归纳环境中，测试 TKG 包含新兴的实体，最新的方法是基于符号规则或预训练语言模型(PLM)。然而，它们分别受到缺乏灵活性和不具体时间的影响。在这项工作中，我们将训练集和测试集中的实体完全分离的完全归纳环境扩展到 TKG，并进一步朝着更加灵活和时间敏感的时间关系预测方法 SST-BERT 迈进了一步，结合结构化句子和时间增强的 BERT。该模型通过对结构化句子进行编码，在语义空间中获取实体历史，并隐式地学习规则，解决了不灵活性问题。我们建议使用时间掩蔽 MLM 任务来预训练 BERT 在一个丰富的时间令牌为 TKG 专门生成的语料库中，提高 SST-BERT 的时间敏感性。为了计算目标四元组出现的概率，我们从时间和语义两个角度将其所有结构化句子聚合成一个分数。对传导数据集和新生成的全归纳基准测试的实验表明，SST-BERT 算法成功地提高了基准测试的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Incorporating+Structured+Sentences+with+Time-enhanced+BERT+for+Fully-inductive+Temporal+Relation+Prediction)|0|
|[Normalizing Flow-based Neural Process for Few-Shot Knowledge Graph Completion](https://doi.org/10.1145/3539618.3591743)|Linhao Luo, YuanFang Li, Gholamreza Haffari, Shirui Pan|Griffith University, Gold Coast, SQ, Australia; Monash University, Melbourne, VIC, Australia|Knowledge graphs (KGs), as a structured form of knowledge representation, have been widely applied in the real world. Recently, few-shot knowledge graph completion (FKGC), which aims to predict missing facts for unseen relations with few-shot associated facts, has attracted increasing attention from practitioners and researchers. However, existing FKGC methods are based on metric learning or meta-learning, which often suffer from the out-of-distribution and overfitting problems. Meanwhile, they are incompetent at estimating uncertainties in predictions, which is critically important as model predictions could be very unreliable in few-shot settings. Furthermore, most of them cannot handle complex relations and ignore path information in KGs, which largely limits their performance. In this paper, we propose a normalizing flow-based neural process for few-shot knowledge graph completion (NP-FKGC). Specifically, we unify normalizing flows and neural processes to model a complex distribution of KG completion functions. This offers a novel way to predict facts for few-shot relations while estimating the uncertainty. Then, we propose a stochastic ManifoldE decoder to incorporate the neural process and handle complex relations in few-shot settings. To further improve performance, we introduce an attentive relation path-based graph neural network to capture path information in KGs. Extensive experiments on three public datasets demonstrate that our method significantly outperforms the existing FKGC methods and achieves state-of-the-art performance. Code is available at https://github.com/RManLuo/NP-FKGC.git.|知识图作为一种结构化的知识表示形式，在现实世界中得到了广泛的应用。近年来，用少镜头关联事实预测未知关系中缺失事实的少镜头知识图完成(FKGC)越来越受到从业人员和研究人员的关注。然而，现有的 FKGC 方法都是基于度量学习或元学习的，这些方法往往存在分布不均衡和过拟合的问题。与此同时，他们无法估计预测中的不确定性，这是至关重要的，因为模型预测可能是非常不可靠的几个镜头的设置。此外，大多数幼儿园学生不能处理复杂的关系，忽视路径信息，这在很大程度上限制了幼儿园学生的学习成绩。本文提出了一种基于规范化流的少镜头知识图补全神经过程(NP-FKGC)。具体地说，我们将归一化流和神经过程相结合，建立了 KG 完备函数的复杂分布模型。这为在估计不确定性的同时预测少镜头关系的事实提供了一种新颖的方法。然后，我们提出了一个随机流形 E 解码器，结合神经过程和处理复杂关系的少镜头设置。为了进一步提高性能，我们引入了一种基于注意关系路径的图神经网络来捕获幼儿园的路径信息。在三个公共数据集上的大量实验表明，该方法明显优于现有的 FKGC 方法，达到了最先进的性能。密码可于 https://github.com/rmanluo/np-fkgc.git 索取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Normalizing+Flow-based+Neural+Process+for+Few-Shot+Knowledge+Graph+Completion)|0|
|[Schema-aware Reference as Prompt Improves Data-Efficient Knowledge Graph Construction](https://doi.org/10.1145/3539618.3591763)|Yunzhi Yao, Shengyu Mao, Ningyu Zhang, Xiang Chen, Shumin Deng, Xi Chen, Huajun Chen|Zhejiang University, Hangzhou, China; National University of Singapore, Singapore, Singapore; Tencent, Shenzhen, China|With the development of pre-trained language models, many prompt-based approaches to data-efficient knowledge graph construction have been proposed and achieved impressive performance. However, existing prompt-based learning methods for knowledge graph construction are still susceptible to several potential limitations: (i) semantic gap between natural language and output structured knowledge with pre-defined schema, which means model cannot fully exploit semantic knowledge with the constrained templates; (ii) representation learning with locally individual instances limits the performance given the insufficient features, which are unable to unleash the potential analogical capability of pre-trained language models. Motivated by these observations, we propose a retrieval-augmented approach, which retrieves schema-aware Reference As Prompt (RAP), for data-efficient knowledge graph construction. It can dynamically leverage schema and knowledge inherited from human-annotated and weak-supervised data as a prompt for each sample, which is model-agnostic and can be plugged into widespread existing approaches. Experimental results demonstrate that previous methods integrated with RAP can achieve impressive performance gains in low-resource settings on five datasets of relational triple extraction and event extraction for knowledge graph construction. Code is available in https://github.com/zjunlp/RAP.|随着预训练语言模型的发展，许多基于提示的数据高效知识图构造方法被提出，并取得了令人印象深刻的性能。然而，现有的基于提示的知识图构建学习方法仍然容易受到一些潜在的限制: (i)自然语言和具有预定义模式的输出结构化知识之间的语义差距，这意味着模型不能充分利用具有约束模板的语义知识; (ii)具有局部个体实例的表示学习限制了性能，因为功能不足，无法释放预训练语言模型的潜在类比能力。基于这些观察结果，我们提出了一种检索增强方法，检索模式感知的参考文献作为提示(RAP) ，用于数据高效的知识图构造。它可以动态地利用从人工注释和弱监督数据继承的模式和知识作为每个样本的提示，这是模型无关的，并且可以插入到广泛存在的方法中。实验结果表明，先前的 RAP 方法在低资源环境下，可以在五个数据集的关系三重提取和事件提取中取得显著的性能提高。代码可在 https://github.com/zjunlp/rap 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Schema-aware+Reference+as+Prompt+Improves+Data-Efficient+Knowledge+Graph+Construction)|0|
|[ML-LJP: Multi-Law Aware Legal Judgment Prediction](https://doi.org/10.1145/3539618.3591731)|Yifei Liu, Yiquan Wu, Yating Zhang, Changlong Sun, Weiming Lu, Fei Wu, Kun Kuang|Alibaba Group, Hangzhou, China; Alibaba Group & Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China|Legal judgment prediction (LJP) is a significant task in legal intelligence, which aims to assist the judges and determine the judgment result based on the case's fact description. The judgment result consists of law articles, charge, and prison term. The law articles serve as the basis for the charge and the prison term, which can be divided into two types, named as charge-related law article and term-related law article, respectively. Recently, many methods have been proposed and made tremendous progress in LJP. However, the existing methods only focus on the prediction of the charge-related law articles, ignoring the term-related law articles (e.g., laws about lenient treatment), which limits the performance in the prison term prediction. In this paper, following the actual legal process, we expand the law article prediction as a multi-label classification task that includes both the charge-related law articles and term-related law articles and propose a novel multi-law aware LJP (ML-LJP) method to improve the performance of LJP. Given the case's fact description, firstly, the label (e.g., law article and charge) definitions in the Code of Law are used to transform the representation of the fact into several label-specific representations and make the prediction of the law articles and the charge. To distinguish the similar content of different label definitions, contrastive learning is conducted in the training. Then, a graph attention network (GAT) is applied to learn the interactions among the multiple law articles for the prediction of the prison term. Since numbers (e.g., amount of theft and weight of drugs) are important for LJP but often ignored by conventional encoders, we design a corresponding number representation method to locate and better represent these effective numbers. Extensive experiments on real-world dataset show that our method achieves the best results compared to the state-of-the-art models, especially in the task of prison term prediction where ML-LJP achieves a 10.07% relative improvement over the best baseline.|法律判决预测是法律情报工作中的一项重要任务，其目的在于帮助法官根据案件的事实描述确定判决结果。判决结果由法律条文、罪名、刑期构成。法律条文作为定罪依据和刑期，可分为定罪相关法律条文和刑期相关法律条文两类。近年来，许多方法被提出并取得了巨大的进展。然而，现有的预测方法只注重罪刑法定条款的预测，而忽视了罪刑法定条款(如宽严相济法)的预测，限制了刑期预测的效果。本文根据实际的法律过程，将法律条款预测扩展为一个包含收费相关法律条款和术语相关法律条款的多标签分类任务，提出了一种新的多法律感知 LJP (ML-LJP)方法，以提高 LJP 的性能。在给定案件事实描述的基础上，首先利用《法典》中的标签(如法律条文和罪名)定义，将事实的表述转化为若干特定标签的表述，并对法律条文和罪名进行预测。为了区分不同标签定义的相似内容，在训练中进行了对比学习。然后，应用图注意网络(GAT)学习多个法律条文之间的相互作用，以预测刑期。由于数字(例如，盗窃量和药品重量)对 LJP 很重要，但常常被常规编码器忽略，因此我们设计了一种相应的数字表示方法来定位和更好地表示这些有效的数字。在实际数据集上的大量实验表明，与最先进的模型相比，我们的方法获得了最好的结果，特别是在监狱刑期预测任务中，ML-LJP 比最佳基线达到了10.07% 的相对改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ML-LJP:+Multi-Law+Aware+Legal+Judgment+Prediction)|0|
|[Creating a Silver Standard for Patent Simplification](https://doi.org/10.1145/3539618.3591657)|Silvia Casola, Alberto Lavelli, Horacio Saggion|University of Padua & Fondazione Bruno Kessler, Padua, Italy; Fondazione Bruno Kessler, Trento, Italy; Universitat Pompeu Fabra, Barcelona, Spain|Patents are legal documents that aim at protecting inventions on the one hand and at making technical knowledge circulate on the other. Their complex style -- a mix of legal, technical, and extremely vague language -- makes their content hard to access for humans and machines and poses substantial challenges to the information retrieval community. This paper proposes an approach to automatically simplify patent text through rephrasing. Since no in-domain parallel simplification data exist, we propose a method to automatically generate a large-scale silver standard for patent sentences. To obtain candidates, we use a general-domain paraphrasing system; however, the process is error-prone and difficult to control. Thus, we pair it with proper filters and construct a cleaner corpus that can successfully be used to train a simplification system. Human evaluation of the synthetic silver corpus shows that it is considered grammatical, adequate, and contains simple sentences.|专利是一种法律文件，一方面是为了保护发明，另一方面是为了使技术知识流通。它们复杂的风格——混合了法律、技术和极其模糊的语言——使得人类和机器很难访问它们的内容，并给信息检索社区带来了巨大的挑战。本文提出了一种通过重新措辞来自动简化专利文本的方法。由于不存在域内并行简化数据，本文提出了一种自动生成大规模专利语句银标准的方法。为了获得候选者，我们使用一个通用的领域解释系统，但是，这个过程是容易出错和难以控制。因此，我们将它与适当的过滤器配对，构造一个更清晰的语料库，可以成功地用于训练一个简化系统。人们对合成银语料库的评价表明，它被认为是合乎语法的，充分的，并包含简单的句子。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Creating+a+Silver+Standard+for+Patent+Simplification)|0|
|[Cone: Unsupervised Contrastive Opinion Extraction](https://doi.org/10.1145/3539618.3591650)|Runcong Zhao, Lin Gui, Yulan He|King's College London, London, United Kingdom|Contrastive opinion extraction aims to extract a structured summary or key points organised as positive and negative viewpoints towards a common aspect or topic. Most recent works for unsupervised key point extraction is largely built on sentence clustering or opinion summarisation based on the popularity of opinions expressed in text. However, these methods tend to generate aspect clusters with incoherent sentences, conflicting viewpoints, redundant aspects. To address these problems, we propose a novel unsupervised Contrastive OpinioN Extraction model, called Cone, which learns disentangled latent aspect and sentiment representations based on pseudo aspect and sentiment labels by combining contrastive learning with iterative aspect/sentiment clustering refinement. Apart from being able to extract contrastive opinions, it is also able to quantify the relative popularity of aspects and their associated sentiment distributions. The model has been evaluated on both a hotel review dataset and a Twitter dataset about COVID vaccines. The results show that despite using no label supervision or aspect-denoted seed words, Cone outperforms a number of competitive baselines on contrastive opinion extraction. The results of Cone can be used to offer a better recommendation of products and services online.|对比意见提取旨在提取一个结构化的总结或关键点组织为积极和消极的观点对一个共同的方面或主题。最近大多数无监督的关键点提取工作主要是建立在句子聚类或意见摘要的基础上表达的意见在文本中的流行。然而，这些方法往往会产生不连贯的句子，相互冲突的观点，冗余的方面聚类。为了解决这些问题，我们提出了一种新的无监督的对比观点提取模型——锥形模型，该模型将对比学习与迭代方面/情感聚类细化相结合，学习基于伪方面和情感标签的分离潜在方面和情感表示。除了能够提取对比意见，它还能够量化方面的相对受欢迎程度及其相关的情绪分布。该模型已经通过酒店评论数据集和关于冠状病毒疾病疫苗的 Twitter 数据集进行了评估。结果表明，尽管没有使用标签监督或方面表示的种子词，锥表现优于一些竞争基线的意见提取对比。Cone 的结果可以用来在线提供更好的产品和服务推荐。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cone:+Unsupervised+Contrastive+Opinion+Extraction)|0|
|[Representation and Labeling Gap Bridging for Cross-lingual Named Entity Recognition](https://doi.org/10.1145/3539618.3591757)|Xinghua Zhang, Bowen Yu, Jiangxia Cao, Quangang Li, Xuebin Wang, Tingwen Liu, Hongbo Xu|Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences & School of Cyber Security, UCAS, Beijing, China; DAMO Academy, Alibaba Group, Beijing, China|Cross-lingual Named Entity Recognition (NER) aims to address the challenge of data scarcity in low-resource languages by leveraging knowledge from high-resource languages. Most current work relies on general multilingual language models to represent text, and then uses classic combined tagging (e.g., B-ORG) to annotate entities; However, this approach neglects the lack of cross-lingual alignment of entity representations in language models, and also ignores the fact that entity spans and types have varying levels of labeling difficulty in terms of transferability. To address these challenges, we propose a novel framework, referred to as DLBri, which addresses the issues of representation and labeling simultaneously. Specifically, the proposed framework utilizes progressive contrastive learning with source-to-target oriented sentence pairs to pre-finetune the language model, resulting in improved cross-lingual entity-aware representations. Additionally, a decomposition-then-combination procedure is proposed, which separately transfers entity span and type, and then combines their information, to reduce the difficulty of cross-lingual entity labeling. Extensive experiments on 13 diverse language pairs confirm the effectiveness of DLBri.|跨语言命名实体识别(NER)旨在通过利用高资源语言的知识来解决低资源语言中的数据稀缺性问题。目前大多数工作依赖于一般的多语言模型来表示文本，然后使用经典的组合标注(例如，B-ORG)来注释实体; 然而，这种方法忽视了实体表示在语言模型中缺乏跨语言对齐，也忽略了实体跨度和类型在可迁移性方面有不同程度的标注困难这一事实。为了应对这些挑战，我们提出了一个新的框架，称为 DLBri，它同时解决了表示和标签的问题。具体来说，该框架利用面向源-目标句子对的逐步对比学习来预调整语言模型，从而改进跨语言实体感知表示。此外，本文还提出了一种分解-组合过程，该过程分别传递实体的跨度和类型，然后组合它们的信息，以减少跨语言实体标注的难度。对13个不同语言对的大量实验证实了 DLBri 的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Representation+and+Labeling+Gap+Bridging+for+Cross-lingual+Named+Entity+Recognition)|0|
|[Unsupervised Readability Assessment via Learning from Weak Readability Signals](https://doi.org/10.1145/3539618.3591695)|Yuliang Liu, Zhiwei Jiang, Yafeng Yin, Cong Wang, Sheng Chen, Zhaoling Chen, Qing Gu|Nanjing University, Nanjing, China|Unsupervised readability assessment aims to evaluate the reading difficulty of text without any manually-labeled data for model training. This is a challenging task because the absence of labeled data makes it difficult for the model to understand what readability is. In this paper, we propose a novel framework to Learn a neural model from Weak Readability Signals (LWRS). Instead of relying on labeled data, LWRS utilizes a set of heuristic signals that specialize in describing text readability from different aspects to guide the model in outputting readability scores for ranking. Specifically, to effectively use multiple heuristic weak signals for model training, we build a multi-signal learning model that ranks the unlabeled texts from multiple readability-related aspects based on intra- and inter-signal learning. We also adopt the pairwise ranking paradigm to reduce the cascade coupling among partial-order pairs. Furthermore, we propose identifying the most representative signal based on the batch-level consensus distribution of all signals. This strategy helps identify the predicted signal that is most correlated with readability in the absence of ground-truth labels. We conduct experiments on three public readability assessment datasets. The experimental results demonstrate that our LWRS outperforms each heuristic signal and their combinations significantly, and can even perform comparably with some supervised methods. Additionally, our LWRS trained on one dataset can be effectively transferred to other datasets, including those in other languages, which indicates its good generalization and potential for wide application.|非监督可读性评估的目的是评估文本的阅读难度，没有任何手工标记的数据进行模型训练。这是一个具有挑战性的任务，因为缺少标记数据使得模型难以理解什么是可读性。本文提出了一种从弱可读性信号(LWRS)中学习神经模型的新框架。LWRS 不依赖于标记数据，而是利用一组专门从不同方面描述文本可读性的启发式信号来指导模型输出可读性分数以进行排名。为了有效地利用多个启发式弱信号进行模型训练，我们建立了一个基于信号内和信号间学习的多信号学习模型，从多个可读性相关方面对未标记文本进行排序。我们还采用成对排序范式来减少偏序对之间的级联耦合。此外，我们提出了基于所有信号的批级一致分布来识别最有代表性的信号。这种策略有助于识别预测的信号，是最相关的可读性在没有地面真相标签。我们在三个公共可读性评估数据集上进行了实验。实验结果表明，我们的 LWRS 方法的性能明显优于各种启发式信号及其组合，甚至可以与一些监督方法相媲美。此外，我们在一个数据集上训练的 LWRS 可以有效地转移到其他数据集，包括其他语言的数据集，这表明它具有良好的通用性和广泛的应用潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unsupervised+Readability+Assessment+via+Learning+from+Weak+Readability+Signals)|0|
|[What If: Generating Code to Answer Simulation Questions in Chemistry Texts](https://doi.org/10.1145/3539618.3591783)|Gal Peretz, Mousa Arraf, Kira Radinsky|Technion - Israel Institute of Technology, Haifa, Israel|Many texts, especially in chemistry and biology, describe complex processes. We focus on texts that describe a chemical reaction process and questions that ask about the process's outcome under different environmental conditions. To answer questions about such processes, one needs to understand the interactions between the different entities involved in the process and simulate their state transitions during the process execution under other conditions. We hypothesize that generating code and executing it to simulate the process will allow answering such questions. We, therefore, define a domain-specific language (DSL) to represent processes. We contribute to the community a unique dataset curated by chemists and annotated by computer scientists. The dataset is composed of process texts, simulation questions, and their corresponding computer codes represented by the DSL. We propose a neural program synthesis approach based on reinforcement learning with a novel state-transition semantic reward. The novel reward is based on the run-time semantic similarity between the predicted code and the reference code. This allows simulating complex process transitions and thus answering simulation questions. Our approach yields a significant boost in accuracy for simulation questions: we achieved 88% accuracy as opposed to 83% accuracy of the state-of-the-art neural program synthesis approaches and 54% accuracy of state-of-the-art end-to-end text-based approaches.|许多文本，特别是在化学和生物学，描述了复杂的过程。我们重点关注描述化学反应过程的文本，以及在不同环境条件下询问过程结果的问题。要回答有关这些流程的问题，需要理解流程中涉及的不同实体之间的交互，并在其他条件下模拟流程执行期间的状态转换。我们假设生成代码并执行它来模拟过程将允许回答这些问题。因此，我们定义一个领域特定语言(DSL)来表示过程。我们为社区贡献了一个独特的数据集，由化学家管理，并由计算机科学家注释。该数据集由过程文本、仿真问题及其相应的由 DSL 表示的计算机代码组成。我们提出了一种基于强化学习的神经程序综合方法，它具有一种新的状态转换语义奖励。新的奖励基于预测代码和引用代码之间的运行时语义相似度。这允许模拟复杂的过程转换，从而回答模拟问题。我们的方法在模拟问题的准确性方面产生了显著的提高: 我们实现了88% 的准确性，而最先进的神经程序综合方法的准确性为83% ，最先进的端到端文本方法的准确性为54% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=What+If:+Generating+Code+to+Answer+Simulation+Questions+in+Chemistry+Texts)|0|
|[A Topic-aware Summarization Framework with Different Modal Side Information](https://doi.org/10.1145/3539618.3591630)|Xiuying Chen, Mingzhe Li, Shen Gao, Xin Cheng, Qiang Yang, Qishen Zhang, Xin Gao, Xiangliang Zhang|KAUST, Jeddah, Saudi Arabia; University of Notre Dame & KAUST, Notre Dame, IL, USA; Ant Group, Beijing, China; Peking University, Beijing, China; Shandong university, Qingdao, China|Automatic summarization plays an important role in the exponential document growth on the Web. On content websites such as CNN.com and WikiHow.com, there often exist various kinds of side information along with the main document for attention attraction and easier understanding, such as videos, images, and queries. Such information can be used for better summarization, as they often explicitly or implicitly mention the essence of the article. However, most of the existing side-aware summarization methods are designed to incorporate either single-modal or multi-modal side information, and cannot effectively adapt to each other. In this paper, we propose a general summarization framework, which can flexibly incorporate various modalities of side information. The main challenges in designing a flexible summarization model with side information include: (1) the side information can be in textual or visual format, and the model needs to align and unify it with the document into the same semantic space, (2) the side inputs can contain information from various aspects, and the model should recognize the aspects useful for summarization. To address these two challenges, we first propose a unified topic encoder, which jointly discovers latent topics from the document and various kinds of side information. The learned topics flexibly bridge and guide the information flow between multiple inputs in a graph encoder through a topic-aware interaction. We secondly propose a triplet contrastive learning mechanism to align the single-modal or multi-modal information into a unified semantic space, where the summary quality is enhanced by better understanding the document and side information. Results show that our model significantly surpasses strong baselines on three public single-modal or multi-modal benchmark summarization datasets.|自动汇总在网络文档的指数增长中扮演着重要的角色。在像 CNN.com 和 WikiHow.com 这样的内容网站上，为了吸引注意力和更容易理解，经常会有各种各样的附属信息和主要文档，比如视频、图片和查询。这些信息可以用于更好的总结，因为它们通常显式或隐式地提到文章的实质。然而，现有的侧面感知摘要方法大多是针对单模态侧面信息或多模态侧面信息而设计的，不能有效地相互适应。本文提出了一个通用的汇总框架，它可以灵活地整合各种形式的侧信息。设计具有侧信息的灵活摘要模型面临的主要挑战包括: (1)侧信息可以是文本格式或可视化格式，模型需要将侧信息与文档对齐统一到相同的语义空间中; (2)侧输入可以包含各个方面的信息，模型应该识别对摘要有用的方面。为了解决这两个问题，我们首先提出了一种统一的主题编码器，它可以联合发现文档中的潜在主题和各种侧信息。所学习的主题通过主题感知交互灵活地连接和引导图形编码器中多个输入之间的信息流。其次，我们提出了一种三元对比学习机制，将单模态或多模态信息整合到一个统一的语义空间中，通过更好地理解文档和侧面信息来提高摘要的质量。结果表明，我们的模型显著超过了三个公共的单模式或多模式基准汇总数据集的强基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Topic-aware+Summarization+Framework+with+Different+Modal+Side+Information)|0|
|[RHB-Net: A Relation-aware Historical Bridging Network for Text2SQL Auto-Completion](https://doi.org/10.1145/3539618.3591759)|Bolong Zheng, Lei Bi, Ruijie Xi, Lu Chen, Yunjun Gao, Xiaofang Zhou, Christian S. Jensen|Aalborg University, Aalborg, Denmark; Huazhong University of Science and Technology, Wuhan, China; Zhejiang University, Hangzhou, China; Hong Kong University of Science and Technology, Hong Kong, China|Test2SQL, a natural language interface to database querying, has seen considerable improvement, in part due to advances in deep learning. However, despite recent improvement, existing Text2SQL proposals allow only input in the form of complete questions. This leaves behind users who struggle to formulate complete questions, e.g., because they lack database expertise or are unfamiliar with the underlying database schema. To address this shortcoming, we study the novel problem of Text2SQL Auto-Completion (TSAC) that extends Text2SQL to also take partial or incomplete questions as input. Specifically, the TSAC problem is to predict the complete, executable SQL query. To solve the problem, we propose a novel Relation-aware Historical Bridging Network (RHB-Net) that consists of a relation-aware union encoder and an extraction-generation sensitive decoder. RHB-Net models relations between questions and database schemas and predicts the ambiguous intents expressed in partial queries. We also propose two optimization strategies: historical query bridging that fuses historical database queries, and a dynamic context construction that prevents repeated generation of the same SQL elements. Extensive experiments with real-world data offer evidence that RHB-Net is capable of outperforming baseline algorithms.|Test2SQL 是一种用于数据库查询的自然语言接口，已经取得了相当大的进步，部分原因是由于深度学习的进步。然而，尽管最近有所改进，现有的 Text2SQL 建议只允许以完整问题的形式输入。这使得用户很难形成完整的问题，例如，因为他们缺乏数据库专业知识或者不熟悉底层的数据库模式。为了解决这个问题，我们研究了 Text2SQL 自动完成(TSAC)的新问题，该问题扩展了 Text2SQL，同时将部分或不完整的问题作为输入。具体来说，TSAC 问题是预测完整的、可执行的 SQL 查询。为了解决这个问题，我们提出了一种新的关系感知历史桥接网络(RHB-Net) ，它由一个关系感知联合编码器和一个抽取生成敏感解码器组成。RHB-Net 模拟问题与数据库模式之间的关系，预测部分查询中表达的歧义意图。我们还提出了两种优化策略: 融合历史数据库查询的历史查询桥接和防止重复生成相同 SQL 元素的动态上下文构造。广泛的实验与真实世界的数据提供的证据表明，RHB-Net 是能够优于基线算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RHB-Net:+A+Relation-aware+Historical+Bridging+Network+for+Text2SQL+Auto-Completion)|0|
|[MAMO: Fine-Grained Vision-Language Representations Learning with Masked Multimodal Modeling](https://doi.org/10.1145/3539618.3591721)|Zijia Zhao, Longteng Guo, Xingjian He, Shuai Shao, Zehuan Yuan, Jing Liu|Institute of Automation, Chinese Academy of Sciences & University of Chinese Academy of Sciences, Beijing, China; Bytedance Inc., Beijing, China|Multimodal representation learning has shown promising improvements on various vision-language tasks (e.g., image-text retrieval, visual question answering, etc) and has significantly advanced the development of multimedia information systems. Most existing methods excel at building global-level alignment between vision and language while lacking effective fine-grained image-text interaction. In this paper, we propose a jointly masked multimodal modeling method to learn fine-grained multimodal representations. Our method performs joint masking on image-text input and integrates both implicit and explicit targets for the masked signals to recover. The implicit target provides a unified and debiased objective for vision and language, where the model predicts latent multimodal representations of the unmasked input. The explicit target further enriches the multimodal representations by recovering high-level and semantically meaningful information: momentum visual features of image patches and concepts of word tokens. Through such a masked modeling process, our model not only learns fine-grained multimodal interaction, but also avoids the semantic gap between high-level representations and low-or mid-level prediction targets (e.g., image pixels, discrete vision tokens), thus producing semantically rich multimodal representations that perform well on both zero-shot and fine-tuned settings. Our pre-trained model (named MAMO) achieves state-of-the-art performance on various downstream vision-language tasks, including image-text retrieval, visual question answering, visual reasoning, and weakly-supervised visual grounding.|多模态表示学习在各种视觉语言任务(如图像-文本检索、视觉问题回答等)中表现出良好的改进效果，极大地推动了多媒体信息系统的发展。现有的大多数方法都擅长于在视觉和语言之间建立全局一致性，但缺乏有效的细粒度图像-文本交互。本文提出了一种联合掩蔽多模态建模方法来学习细粒度多模态表示。该方法对图像-文本输入进行联合掩蔽，并将隐式和显式目标相结合进行掩蔽信号的恢复。隐式目标为视觉和语言提供了一个统一的、无偏的目标，该模型预测未掩盖的输入的潜在多模态表示。显式目标通过恢复高层次的、语义上有意义的信息，即图像块的动量视觉特征和词标记的概念，进一步丰富了多模式表示。通过这样一个隐藏的建模过程，我们的模型不仅学习细粒度的多模态交互，而且还避免了高级表示和低级或中级预测目标(例如，图像像素，离散视觉标记)之间的语义差距，从而产生语义丰富的多模态表示，在零拍摄和微调设置上表现良好。我们的预训练模型(命名为 MAMO)在各种下游视觉语言任务中实现了最先进的性能，包括图像文本检索、视觉问题回答、视觉推理和弱监督视觉接地。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MAMO:+Fine-Grained+Vision-Language+Representations+Learning+with+Masked+Multimodal+Modeling)|0|
|[Learn from Relational Correlations and Periodic Events for Temporal Knowledge Graph Reasoning](https://doi.org/10.1145/3539618.3591711)|Ke Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenxuan Tu, Siwei Wang, Sihang Zhou, Xinwang Liu|National University of Defense Technology, Changsha, China|Reasoning on temporal knowledge graphs (TKGR), aiming to infer missing events along the timeline, has been widely studied to alleviate incompleteness issues in TKG, which is composed of a series of KG snapshots at different timestamps. Two types of information, i.e., intra-snapshot structural information and inter-snapshot temporal interactions, mainly contribute to the learned representations for reasoning in previous models. However, these models fail to leverage (1) semantic correlations between relationships for the former information and (2) the periodic temporal patterns along the timeline for the latter one. Thus, such insufficient mining manners hinder expressive ability, leading to sub-optimal performances. To address these limitations, we propose a novel reasoning model, termed RPC, which sufficiently mines the information underlying the Relational correlations and Periodic patterns via two novel Correspondence units, i.e., relational correspondence unit (RCU) and periodic correspondence unit (PCU). Concretely, relational graph convolutional network (RGCN) and RCU are used to encode the intra-snapshot graph structural information for entities and relations, respectively. Besides, the gated recurrent units (GRU) and PCU are designed for sequential and periodic inter-snapshot temporal interactions, separately. Moreover, the model-agnostic time vectors are generated by time2vector encoders to guide the time-dependent decoder for fact scoring. Extensive experiments on six benchmark datasets show that RPC outperforms the state-of-the-art TKGR models, and also demonstrate the effectiveness of two novel strategies in our model.|时间知识图(TKGR)的推理旨在推断时间轴上的缺失事件，已被广泛研究，以减轻 TKG 中的不完备性问题，TKG 由一系列不同时间戳的 KG 快照组成。快照内部结构信息和快照间的时间相互作用是以往模型推理学习表征的主要方式。然而，这些模型未能利用(1)前者信息的关系之间的语义相关性和(2)后者信息的时间轴上的周期性时间模式。因此，这种不充分的采矿方式阻碍了表达能力，导致次优性能。为了解决这些局限性，我们提出了一种新的推理模型，称为 RPC，它通过两个新的通信单元，即关系通信单元(RCU)和周期通信单元(PCU) ，充分挖掘关系相关性和周期模式的信息。具体地，分别采用关系图卷积网络(RGCN)和 RCU 对实体和关系的快照内图结构信息进行编码。此外，本文还分别针对快照间的时间相互作用设计了门控回归单元(GRU)和 PCU。此外，时间向量编码器产生模型无关的时间向量，以指导时间相关的解码器进行事实评分。在六个基准数据集上的大量实验表明，RPC 算法的性能优于最先进的 TKGR 模型，并且证明了两种新策略在我们的模型中的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learn+from+Relational+Correlations+and+Periodic+Events+for+Temporal+Knowledge+Graph+Reasoning)|0|
|[Dynamic Mixed Membership Stochastic Block Model for Weighted Labeled Networks](https://doi.org/10.1145/3539618.3591675)|Gaël PouxMédard, Julien Velcin, Sabine Loudcher|Université de Lyon, Lyon 2, UR 3083, Lyon, France|Most real-world networks evolve over time. Existing literature proposes models for dynamic networks that are either unlabeled or assumed to have a single membership structure. On the other hand, a new family of Mixed Membership Stochastic Block Models (MMSBM) allows to model static labeled networks under the assumption of mixed-membership clustering. In this work, we propose to extend this later class of models to infer dynamic labeled networks under a mixed membership assumption. Our approach takes the form of a temporal prior on the model's parameters. It relies on the single assumption that dynamics are not abrupt. We show that our method significantly differs from existing approaches, and allows to model more complex systems --dynamic labeled networks. We demonstrate the robustness of our method with several experiments on both synthetic and real-world datasets. A key interest of our approach is that it needs very few training data to yield good results. The performance gain under challenging conditions broadens the variety of possible applications of automated learning tools --as in social sciences, which comprise many fields where small datasets are a major obstacle to the introduction of machine learning methods.|大多数现实世界的网络都是随着时间而演变的。现有的文献提出了动态网络的模型，这些模型要么是未标记的，要么是假定有一个单一的成员结构。另一方面，一类新的混合成员随机块模型(MMSBM)允许在混合成员聚类的假设下对静态标记网络进行建模。在这项工作中，我们建议扩展这类模型，以推断动态标记网络下的混合成员假设。我们的方法采用模型参数的时间先验的形式。它依赖于一个单一的假设，即动态不是突然的。我们展示了我们的方法与现有方法的显著不同，并且允许对更复杂的系统进行建模——动态标记网络。我们通过在合成数据集和真实数据集上的几个实验证明了该方法的鲁棒性。我们的方法的一个关键兴趣是它只需要很少的训练数据就能产生良好的结果。在具有挑战性的条件下，性能的提高扩大了自动学习工具的各种可能应用——如在社会科学领域，其中包括许多领域，小型数据集是引入机器学习方法的主要障碍。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamic+Mixed+Membership+Stochastic+Block+Model+for+Weighted+Labeled+Networks)|0|
|[DREAM: Adaptive Reinforcement Learning based on Attention Mechanism for Temporal Knowledge Graph Reasoning](https://doi.org/10.1145/3539618.3591671)|Shangfei Zheng, Hongzhi Yin, Tong Chen, Quoc Viet Hung Nguyen, Wei Chen, Lei Zhao|The University of Queensland, Brisbane, QLD, Australia; Griffith University, Gold Coast, Australia; Soochow University, Suzhou, China|Temporal knowledge graphs (TKGs) model the temporal evolution of events and have recently attracted increasing attention. Since TKGs are intrinsically incomplete, it is necessary to reason out missing elements. Although existing TKG reasoning methods have the ability to predict missing future events, they fail to generate explicit reasoning paths and lack explainability. As reinforcement learning (RL) for multi-hop reasoning on traditional knowledge graphs starts showing superior explainability and performance in recent advances, it has opened up opportunities for exploring RL techniques on TKG reasoning. However, the performance of RL-based TKG reasoning methods is limited due to: (1) lack of ability to capture temporal evolution and semantic dependence jointly; (2) excessive reliance on manually designed rewards. To overcome these challenges, we propose an adaptive reinforcement learning model based on attention mechanism (DREAM) to predict missing elements in the future. Specifically, the model contains two components: (1) a multi-faceted attention representation learning method that captures semantic dependence and temporal evolution jointly; (2) an adaptive RL framework that conducts multi-hop reasoning by adaptively learning the reward functions. Experimental results demonstrate DREAM outperforms state-of-the-art models on public dataset|时间知识图(TKGs)模拟事件的时间演化过程，近年来受到越来越多的关注。由于 TKG 本质上是不完整的，因此有必要对缺失的元素进行推理。现有的 TKG 推理方法虽然具有预测未来事件缺失的能力，但不能产生明确的推理路径，缺乏可解释性。由于传统知识图的多跳推理强化学习(rL)在最近的进展中显示出优越的可解释性和性能，它为探索传统知识图的多跳推理技术开辟了机会。然而，基于 RL 的 TKG 推理方法的性能受到以下因素的限制: (1)缺乏联合捕获时间演化和语义依赖的能力; (2)过度依赖手工设计的奖励。为了克服这些挑战，我们提出了一个基于注意力机制(dREAM)的自适应强化学习模型来预测未来缺失的元素。具体来说，该模型包含两个部分: (1)一个多方面的注意表征学习方法，它能够联合捕捉语义依赖和时间进化; (2)一个自适应 RL 框架，通过自适应学习奖励函数来进行多跳推理。实验结果表明，DREAM 在公共数据集上的性能优于最先进的模型|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DREAM:+Adaptive+Reinforcement+Learning+based+on+Attention+Mechanism+for+Temporal+Knowledge+Graph+Reasoning)|0|
|[SCHash: Speedy Simplicial Complex Neural Networks via Randomized Hashing](https://doi.org/10.1145/3539618.3591762)|Xuan Tan, Wei Wu, Chuan Luo|Beihang University, Beijing, China; Central South University, Changsha, China|Graphs, as a non-linear data structure, are ubiquitous in practice, and efficient graph analysis can benefit important information retrieval applications in the era of big data. Currently, one of the fundamental graph mining problems is graph embedding, which aims to represent the graph as a low-dimensional feature vector with the content and structural information in the graph preserved. Although the graph embedding technique has evolved considerably, traditional methods mainly focus on node pairwise relationship in graphs, which makes the representational power of such schemes limited. Recently, a number of works have explored the simplicial complexes, which describe the higher-order interactions between nodes in the graphs, and further proposed several Graph Neural Network (GNN) algorithms based on simplicial complexes. However, these GNN approaches are highly inefficient in terms of running time and space, due to massive parameter learning. In this paper, we propose a simple and speedy graph embedding algorithm dubbed SCHash. Through adopting the Locality Sensitive Hashing (LSH) technique, SCHash captures the higher-order information derived from the simplicial complex in the GNN framework, and it can achieve a good balance between accuracy and efficiency. Our extensive experiments clearly show that, in terms of accuracy, the performance of our proposed SCHash algorithm is comparable to that of state-of-the-art GNN algorithms; also, SCHash achieves higher accuracy than the existing LSH algorithms. In terms of efficiency, SCHash runs faster than GNN algorithms by 2 ~ 4 orders of magnitude, and is more efficient than the existing LSH algorithms.|图形作为一种非线性数据结构，在实践中无处不在，有效的图形分析可以使大数据时代的重要信息检索应用受益。目前，图挖掘的基本问题之一是图嵌入，其目的是将图表示为一个低维特征向量，并保留图中的内容和结构信息。虽然图嵌入技术已经有了很大的发展，但是传统的方法主要集中在图中的节点成对关系上，这使得这种方案的表示能力受到了限制。近年来，人们对描述图中节点之间高阶相互作用的单纯复形进行了研究，并进一步提出了几种基于单纯复形的图神经网络(GNN)算法。然而，由于大量的参数学习，这些 GNN 方法在运行时间和空间上都是非常低效的。本文提出了一种简单快速的图嵌入算法 SCHash。通过采用位置敏感哈希(Locality Sensor hash，LSH)技术，SCHash 捕获来自 GNN 框架中单纯复形的高阶信息，可以在准确性和效率之间取得良好的平衡。我们的大量实验清楚地表明，在准确性方面，我们提出的 SCHash 算法的性能与最先进的 GNN 算法相当; 而且，SCHash 比现有的 LSH 算法获得更高的准确性。在效率方面，SCHash 的运行速度比 GNN 算法快2 ~ 4个数量级，而且比现有的 LSH 算法更有效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SCHash:+Speedy+Simplicial+Complex+Neural+Networks+via+Randomized+Hashing)|0|
|[A Critical Reexamination of Intra-List Distance and Dispersion](https://doi.org/10.1145/3539618.3591623)|Naoto Ohsaka, Riku Togashi|CyberAgent, Inc., Tokyo, Japan|Diversification of recommendation results is a promising approach for coping with the uncertainty associated with users' information needs. Of particular importance in diversified recommendation is to define and optimize an appropriate diversity objective. In this study, we revisit the most popular diversity objective called intra-list distance (ILD), defined as the average pairwise distance between selected items, and a similar but lesser known objective called dispersion, which is the minimum pairwise distance. Owing to their simplicity and flexibility, ILD and dispersion have been used in a plethora of diversified recommendation research. Nevertheless, we do not actually know what kind of items are preferred by them. We present a critical reexamination of ILD and dispersion from theoretical and experimental perspectives. Our theoretical results reveal that these objectives have potential drawbacks: ILD may select duplicate items that are very close to each other, whereas dispersion may overlook distant item pairs. As a competitor to ILD and dispersion, we design a diversity objective called Gaussian ILD, which can interpolate between ILD and dispersion by tuning the bandwidth parameter. We verify our theoretical results by experimental results using real-world data and confirm the extreme behavior of ILD and dispersion in practice.|推荐结果的多样化是解决与用户信息需求相关的不确定性的一种有前途的方法。在多样化推荐中，定义和优化适当的多样化目标尤为重要。在这项研究中，我们重新审视了最流行的多样性目标，称为列表内距离(ILD) ，定义为选定项目之间的平均配对距离，以及一个类似但不太为人所知的目标，称为离散度，即最小配对距离。由于它们的简单性和灵活性，ILD 和分散性已经被用于各种各样的推荐研究中。尽管如此，我们实际上并不知道他们喜欢什么样的项目。我们从理论和实验的角度对 ILD 和离散度进行了批判性的重新审视。我们的理论结果揭示了这些目标具有潜在的缺点: ILD 可能会选择彼此非常接近的重复项目，而分散可能会忽略远距离项目对。作为 ILD 和色散的竞争对手，我们设计了一个分集目标——高斯 ILD，它可以通过调整带宽参数在 ILD 和色散之间进行插值。我们利用实际数据对理论结果进行了验证，并在实际应用中验证了 ILD 和色散的极端行为。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Critical+Reexamination+of+Intra-List+Distance+and+Dispersion)|0|
|[Contrastive Learning for Signed Bipartite Graphs](https://doi.org/10.1145/3539618.3591655)|Zeyu Zhang, Jiamou Liu, Kaiqi Zhao, Song Yang, Xianda Zheng, Yifei Wang|University of Electronic Science and Technology of China & The University of Auckland, Chengdu, China; The University of Auckland, Auckland, New Zealand|This paper is the first to use contrastive learning to improve the robustness of graph representation learning for signed bipartite graphs, which are commonly found in social networks, recommender systems, and paper review platforms. Existing contrastive learning methods for signed graphs cannot capture implicit relations between nodes of the same type in signed bipartite graphs, which have two types of nodes and edges only connect nodes of different types. We propose a Signed Bipartite Graph Contrastive Learning (SBGCL) method to learn robust node representation while retaining the implicit relations between nodes of the same type. SBGCL augments a signed bipartite graph with a novel two-level graph augmentation method. At the top level, we maintain two perspectives of the signed bipartite graph, one presents the original interactions between nodes of different types, and the other presents the implicit relations between nodes of the same type. At the bottom level, we employ stochastic perturbation strategies to create two perturbed graphs in each perspective. Then, we construct positive and negative samples from the perturbed graphs and design a multi-perspective contrastive loss to unify the node presentations learned from the two perspectives. Results show proposed model is effective over state-of-the-art methods on real-world datasets.|本文首次使用对比学习方法来提高有符号二部图的图表示学习的鲁棒性，这种学习方法在社交网络、推荐系统和论文评论平台中很常见。现有的有符号图的对比学习方法不能捕捉有符号二部图中同类节点之间的隐式关系，有两类节点，边只连接不同类型的节点。本文提出了一种符号二部图对比学习(SBGCL)方法来学习鲁棒节点表示，同时保留同类节点之间的隐式关系。SBGCL 用一种新的两层图增强方法对符号二部图进行增强。在顶层，我们保持了符号二部图的两个透视图，一个透视图表示不同类型节点之间的原始交互，另一个透视图表示同类型节点之间的隐式关系。在底层，我们使用随机扰动策略在每个透视图中创建两个扰动图。然后，我们从受扰图中构造正样本和负样本，并设计一个多视角对比损失来统一从这两个视角学到的节点表示。实验结果表明，该模型对于真实数据集是有效的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contrastive+Learning+for+Signed+Bipartite+Graphs)|0|
|[Uncertainty Quantification for Extreme Classification](https://doi.org/10.1145/3539618.3591780)|JyunYu Jiang, WeiCheng Chang, Jiong Zhang, ChoJui Hsieh, HsiangFu Yu|Amazon Search, Palo Alto, CA, USA; University of California, Los Angeles & Amazon Search, Los Angeles, CA, USA|Uncertainty quantification is one of the most crucial tasks to obtain trustworthy and reliable machine learning models for decision making. However, most research in this domain has only focused on problems with small label spaces and ignored eXtreme Multi-label Classification (XMC), which is an essential task in the era of big data for web-scale machine learning applications. Moreover, enormous label spaces could also lead to noisy retrieval results and intractable computational challenges for uncertainty quantification. In this paper, we aim to investigate general uncertainty quantification approaches for tree-based XMC models with a probabilistic ensemble-based framework. In particular, we analyze label-level and instance-level uncertainty in XMC, and propose a general approximation framework based on beam search to efficiently estimate the uncertainty with a theoretical guarantee under long-tail XMC predictions. Empirical studies on six large-scale real-world datasets show that our framework not only outperforms single models in predictive performance, but also can serve as strong uncertainty-based baselines for label misclassification and out-of-distribution detection, with significant speedup. Besides, our framework can further yield better state-of-the-art results based on deep XMC models with uncertainty quantification.|不确定性量化是获得可信可靠的决策机器学习模型的关键任务之一。然而，这一领域的大多数研究只关注于小标签空间的问题，而忽视了 eXtreme Multi-label 分类(XMC) ，这是当今大数据时代的一个重要任务，适用于网络规模的机器学习应用。此外，巨大的标签空间也可能导致噪声检索结果和棘手的计算不确定性量化的挑战。在本文中，我们的目的是研究一般的不确定性量化方法的树为基础的 XMC 模型与概率集成的框架。特别地，我们分析了 XMC 中的标签级和实例级不确定性，并提出了一个基于束搜索的通用近似框架，以有效地估计在长尾 XMC 预测下的不确定性和理论保证。对6个大规模实际数据集的实证研究表明，该框架不仅在预测性能上优于单个模型，而且可以作为标签错误分类和分布外检测的强不确定性基线，具有显著的加速效果。此外，我们的框架可以进一步产生更好的国家的最先进的结果基于深入的 XMC 模型与不确定性量化。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Uncertainty+Quantification+for+Extreme+Classification)|0|
|[A Mathematical Word Problem Generator with Structure Planning and Knowledge Enhancement](https://doi.org/10.1145/3539618.3591937)|Longhu Qin, Jiayu Liu, Zhenya Huang, Kai Zhang, Qi Liu, Binbin Jin, Enhong Chen|Huawei Cloud Computing Technologies Co., Ltd., Hangzhou, China; School of Computer Science and Technology, University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence, Hefei, China; School of Data Science, University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence, Hefei, China; Anhui Province Key Laboratory of Big Data Analysis and Application, University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence, Hefei, China|Automatically generating controllable and diverse mathematical word problems (MWPs) which conform to equations and topics is a crucial task in information retrieval and natural language generation. Recent deep learning models mainly focus on improving the problem readability but overlook the mathematical logic coherence, which tends to generate unsolvable problems. In this paper, we draw inspiration from the human problem-designing process and propose a Mathematical structure Planning and Knowledge enhanced Generation model (MaPKG), following the "plan-then-generate" steps. Specifically, we propose a novel dynamic planning module to make sentence-level equation plans and a dual-attention mechanism for word-level generation, incorporating equation structure representation and external commonsense knowledge. Extensive experiments on two MWP datasets show our model can guarantee more solvable, high-quality, and diverse problems. Our code is available at https://github.com/KenelmQLH/MaPKG.git|自动生成符合公式和主题的可控的多样化数学问题(MWPs)是信息检索和自然语言生成中的一项关键任务。目前的深度学习模式主要侧重于提高问题的可读性，而忽视了数学逻辑的一致性，这往往会产生无法解决的问题。本文从人类问题设计过程中得到的启示，按照“先规划后生成”的步骤，提出了一种数学结构规划与知识增强生成模型(MaPKG)。具体来说，我们提出了一个新的动态规划模块来制定句子水平的方程计划和一个双重注意机制的词水平生成，结合方程结构表示和外部常识知识。在两个 MWP 数据集上的大量实验表明，我们的模型可以保证更多可解决的、高质量的和多样化的问题。我们的代码可以在 https://github.com/kenelmqlh/mapkg.git 找到|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Mathematical+Word+Problem+Generator+with+Structure+Planning+and+Knowledge+Enhancement)|0|
|[A Simple yet Effective Framework for Few-Shot Aspect-Based Sentiment Analysis](https://doi.org/10.1145/3539618.3591940)|Zengzhi Wang, Qiming Xie, Rui Xia|Nanjing University of Science and Technology, Nanjing, China|The pre-training and fine-tuning paradigm has become the main-stream framework in the field of Aspect-Based Sentiment Analysis (ABSA). Although it has achieved sound performance in the domains containing enough fine-grained aspect-sentiment annotations, it is still challenging to conduct few-shot ABSA in domains where manual annotations are scarce. In this work, we argue that two kinds of gaps, i.e., domain gap and objective gap, hinder the transfer of knowledge from pre-training language models (PLMs) to ABSA tasks. To address this issue, we introduce a simple yet effective framework called FS-ABSA, which involves domain-adaptive pre-training and text-infilling fine-tuning. We approach the End-to-End ABSA task as a text-infilling problem and perform domain-adaptive pre-training with the text-infilling objective, narrowing the two gaps and consequently facilitating the knowledge transfer. Experiments show that the resulting model achieves more compelling performance than baselines under the few-shot setting while driving the state-of-the-art performance to a new level across datasets under the fully-supervised setting. Moreover, we apply our framework to two non-English low-resource languages to demonstrate its generality and effectiveness.|预训练和微调范式已经成为基于方面的情绪分析(ABSA)领域的主流框架。尽管它已经在包含足够细粒度的方面-情感注释的领域中取得了良好的性能，但是在缺乏手动注释的领域中进行少量的 ABSA 仍然具有挑战性。在本研究中，我们认为领域差距和目标差距两种差距阻碍了知识从培训前语言模型(PLM)到 ABSA 任务的转移。为了解决这个问题，我们引入了一个简单而有效的框架 FS-ABSA，它包括领域自适应预训练和文本填充微调。将端到端 ABSA 任务作为文本填充问题进行处理，并针对文本填充目标进行领域自适应预训练，缩小两者之间的差距，从而促进知识转移。实验结果表明，该模型比基准模型在少镜头情况下的性能更好，同时在全监督情况下将数据集间的性能提高到了一个新的水平。此外，我们将该框架应用于两种非英语低资源语言，以证明其通用性和有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Simple+yet+Effective+Framework+for+Few-Shot+Aspect-Based+Sentiment+Analysis)|0|
|[A Unified Formulation for the Frequency Distribution of Word Frequencies using the Inverse Zipf's Law](https://doi.org/10.1145/3539618.3591942)|Can Özbey, Talha Çolakoglu, M. Safak Bilici, Ekin Can Erkus|Huawei Turkey R&D Center, Istanbul, Turkey|The power-law approximation for the frequency distribution of words postulated by Zipf has been extensively studied for decades, which led to many variations on the theme. However, comparatively less attention has been paid to the investigation of the case of word frequencies. In this paper, we derive its analytical expression from the inverse of the underlying rank-size distribution as a function of total word count, vocabulary size and the shape parameter, thereby providing a unified framework to explain the nonlinear behavior of low frequencies on the log-log scale. We also present an efficient method based on relative entropy minimization for a robust estimation of the shape parameter using a small number of empirical low-frequency probabilities. Experiments were carried out for a selected set of languages with varying degrees of inflection in order to demonstrate the effectiveness of the proposed approach.|Zipf 假设的词频分布的幂律近似已经被广泛地研究了几十年，这导致了主题的许多变化。然而，对词频的研究却相对较少。本文从底层秩-大小分布与词汇总量、词汇量和形状参数的反向关系出发，推导出其解析表达式，从而为低频非线性行为在对数尺度上的解释提供了一个统一的框架。我们还提出了一种基于相对熵最小化的有效方法，使用少量的经验低频概率对形状参数进行稳健估计。为了验证该方法的有效性，对一组具有不同屈折度的选定语言进行了实验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Unified+Formulation+for+the+Frequency+Distribution+of+Word+Frequencies+using+the+Inverse+Zipf's+Law)|0|
|[Bayesian Knowledge-driven Critiquing with Indirect Evidence](https://doi.org/10.1145/3539618.3591954)|Armin Toroghi, Griffin Floto, Zhenwei Tang, Scott Sanner|University of Toronto, Toronto, ON, Canada|Conversational recommender systems (CRS) enhance the expressivity and personalization of recommendations through multiple turns of user-system interaction. Critiquing is a well-known paradigm for CRS that allows users to iteratively refine recommendations by providing feedback about attributes of recommended items. While existing critiquing methodologies utilize direct attributes of items to address user requests such as 'I prefer Western movies', the opportunity of incorporating richer contextual and side information about items stored in Knowledge Graphs (KG) into the critiquing paradigm has been overlooked. Employing this substantial knowledge together with a well-established reasoning methodology paves the way for critique-based recommenders to allow for complex knowledge-based feedback (e.g., 'I like movies featuring war side effects on veterans') which may arise in natural user-system conversations. In this work, we aim to increase the flexibility of critique-based recommendation by integrating KGs and propose a novel Bayesian inference framework that enables reasoning with relational knowledge-based feedback. We study and formulate the framework considering a Gaussian likelihood and evaluate it on two well-known recommendation datasets with KGs. Our evaluations demonstrate the effectiveness of our framework in leveraging indirect KG-based feedback (i.e., preferred relational properties of items rather than preferred items themselves), often improving personalized recommendations over a one-shot recommender by more than 15%. This work enables a new paradigm for using rich knowledge content and reasoning over indirect evidence as a mechanism for critiquing interactions with CRS.|会话推荐系统(CRS)通过多轮用户系统交互增强推荐的表达能力和个性化。批评是 CRS 的一个众所周知的范例，它允许用户通过提供关于推荐项目属性的反馈来迭代地完善推荐。虽然现有的批评方法利用项目的直接属性来满足用户的要求，例如“我更喜欢西部电影”，但是将知识图表(KG)中存储的项目的更丰富的上下文和侧面信息纳入批评范式的机会被忽视了。使用这些实质性的知识和一个完善的推理方法为基于评论的推荐者铺平了道路，以允许复杂的基于知识的反馈(例如，“我喜欢有退伍军人战争副作用的电影”) ，这可能出现在自然的用户系统对话中。在这项工作中，我们的目标是通过整合幼稚园来增加基于批判的推荐的灵活性，并提出一个新的贝叶斯推断框架，使推理与关系知识为基础的反馈。我们研究并制定了考虑高斯似然的框架，并在两个著名的 KG 推荐数据集上进行了评估。我们的评估表明，我们的框架在利用间接的基于 KG 的反馈(即，项目的首选关系属性，而不是首选项本身)方面的有效性，通常比一次性推荐提高个性化推荐超过15% 。这项工作为利用丰富的知识内容和推理间接证据作为一种机制批判与 CRS 的相互作用提供了一个新的范例。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bayesian+Knowledge-driven+Critiquing+with+Indirect+Evidence)|0|
|[BioAug: Conditional Generation based Data Augmentation for Low-Resource Biomedical NER](https://doi.org/10.1145/3539618.3591957)|Sreyan Ghosh, Utkarsh Tyagi, Sonal Kumar, Dinesh Manocha|University of Maryland College Park, USA, College Park, MD, USA|Biomedical Named Entity Recognition (BioNER) is the fundamental task of identifying named entities from biomedical text. However, BioNER suffers from severe data scarcity and lacks high-quality labeled data due to the highly specialized and expert knowledge required for annotation. Though data augmentation has shown to be highly effective for low-resource NER in general, existing data augmentation techniques fail to produce factual and diverse augmentations for BioNER. In this paper, we present BioAug, a novel data augmentation framework for low-resource BioNER. BioAug, built on BART, is trained to solve a novel text reconstruction task based on selective masking and knowledge augmentation. Post training, we perform conditional generation and generate diverse augmentations conditioning BioAug on selectively corrupted text similar to the training stage. We demonstrate the effectiveness of BioAug on 5 benchmark BioNER datasets and show that BioAug outperforms all our baselines by a significant margin (1.5%-21.5% absolute improvement) and is able to generate augmentations that are both more factual and diverse. Code: https://github.com/Sreyan88/BioAug.|生物医学命名实体识别(BioNER)是从生物医学文本中识别命名实体的基础性工作。然而，由于注释所需的高度专业化和专家知识，BioNER 存在严重的数据稀缺性，并且缺乏高质量的标记数据。虽然数据增强已被证明对于低资源 NER 一般来说是非常有效的，但现有的数据增强技术不能为 BioNER 产生实际的和多样的增强。在本文中，我们提出了 BioAug，一个新的低资源 BioNER 数据增强框架。基于 BART 的 BioAug 被训练来解决一个基于选择性掩蔽和知识增强的文本重建任务。训练后，我们执行条件生成和生成不同的增强条件 BioAug 对选择性损坏的文本类似于训练阶段。我们证明了 BioAug 在5个基准 BioNER 数据集上的有效性，并显示 BioAug 比我们所有的基线都有显着的提高(1.5% -21.5% 的绝对改善) ，并且能够产生更加实际和多样化的增强。密码:  https://github.com/sreyan88/bioaug。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BioAug:+Conditional+Generation+based+Data+Augmentation+for+Low-Resource+Biomedical+NER)|0|
|[Dimension-Prompts Boost Commonsense Consolidation](https://doi.org/10.1145/3539618.3591973)|Jiazhan Feng, Chongyang Tao, Tao Shen, Chang Liu, Dongyan Zhao|Peking University & National Key Laboratory of General Artificial Intelligence, BIGAI, Beijing, China; Microsoft Corporation, Beijing, China; University of Technology Sydney, Sydney, NSW, Australia; Peking University, Beijing, China|Neural knowledge models emerged and advanced common-sense-centric knowledge grounding. They parameterize a small seed curated commonsense knowledge graph (CS-KG) in a language model to generalize more. A current trend is to scale the seed up by directly mixing multiple sources of CS-KG (e.g., ATOMIC, ConceptNet) into one model. But, such brute-force mixing inevitably hinders effective knowledge consolidation due to i) ambiguous, polysemic, and/or inconsistent relations across sources and ii) knowledge learned in an entangled manner despite distinct types (e.g., causal, temporal). To mitigate this, we adopt a concept of commonsense knowledge dimension and propose a brand-new dimension-disentangled knowledge model (D2KM) learning paradigm with multiple sources. That is, a generative language model with dimension-specific soft prompts is trained to disentangle knowledge acquisitions along with different dimensions and facilitate potential intra-dimension consolidation across CS-KG sources. Experiments show our knowledge model outperforms its baselines in both standard and zero-shot scenarios.|神经知识模型的出现和先进的常识为中心的知识基础。他们在一个语言模型中参数化一个小的种子策划的常识知识图(CS-KG) ，以便进一步推广。目前的趋势是通过直接将 CS-KG 的多个来源(例如 ATOMIC、 ConcepeptNet)混合到一个模型中来扩大种子的规模。但是，这种蛮力混合不可避免地阻碍了有效的知识整合，因为 i)模棱两可，多义和/或来源之间的不一致关系，以及 ii)尽管有不同的类型(例如因果关系，时间关系) ，但以纠缠方式学习的知识。为了解决这一问题，我们采用常识知识维度的概念，提出了一种全新的多源维度分离知识模型(D2KM)学习范式。也就是说，训练一个具有特定维度的软提示的生成语言模型，以便将知识获取与不同维度一起分离，并促进跨 CS-KG 源的潜在维度内整合。实验表明，我们的知识模型在标准场景和零射击场景中都优于其基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dimension-Prompts+Boost+Commonsense+Consolidation)|0|
|[DeviceGPT: A Generative Pre-Training Transformer on the Heterogenous Graph for Internet of Things](https://doi.org/10.1145/3539618.3591972)|Yimo Ren, Jinfa Wang, Hong Li, Hongsong Zhu, Limin Sun|University of Chinese Academy of Science & Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China|Recently, Graph neural networks (GNNs) have been adopted to model a wide range of structured data from academic and industry fields. With the rapid development of Internet technology, there are more and more meaningful applications for Internet devices, including device identification, geolocation and others, whose performance needs improvement. To replicate the several claimed successes of GNNs, this paper proposes DeviceGPT based on a generative pre-training transformer on a heterogeneous graph via self-supervised learning to learn interactions-rich information of devices from its large-scale databases well. The experiments on the dataset constructed from the real world show DeviceGPT could achieve competitive results in multiple Internet applications.|近年来，图神经网络(GNN)已被广泛应用于学术和工业领域的结构化数据建模。随着互联网技术的飞速发展，互联网设备在设备识别、地理定位等方面的应用越来越多，其性能需要进一步提高。为了复制 GNN 的几个成功案例，本文提出了一种基于异构图上生成式预训练转换器的 DeviceGPT，通过自监督学习从其大规模数据库中很好地学习设备的交互信息。实验表明，DeviceGPT 可以在多种互联网应用中取得有竞争力的效果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DeviceGPT:+A+Generative+Pre-Training+Transformer+on+the+Heterogenous+Graph+for+Internet+of+Things)|0|
|[DocGraphLM: Documental Graph Language Model for Information Extraction](https://doi.org/10.1145/3539618.3591975)|Dongsheng Wang, Zhiqiang Ma, Armineh Nourbakhsh, Kang Gu, Sameena Shah|JPMorgan AI Research, New York, NY, USA; JPMorgan AI Research, London, United Kingdom; Dartmouth College, Hanover, NH, USA|Advances in Visually Rich Document Understanding (VrDU) have enabled information extraction and question answering over documents with complex layouts. Two tropes of architectures have emerged-transformer-based models inspired by LLMs, and Graph Neural Networks. In this paper, we introduce DocGraphLM, a novel framework that combines pre-trained language models with graph semantics. To achieve this, we propose 1) a joint encoder architecture to represent documents, and 2) a novel link prediction approach to reconstruct document graphs. DocGraphLM predicts both directions and distances between nodes using a convergent joint loss function that prioritizes neighborhood restoration and downweighs distant node detection. Our experiments on three SotA datasets show consistent improvement on IE and QA tasks with the adoption of graph features. Moreover, we report that adopting the graph features accelerates convergence in the learning process druing training, despite being solely constructed through link prediction.|视觉丰富文档理解技术的进步(vrdU)使得信息抽取和问题解答能够在复杂布局的文档上进行。出现了两种类型的体系结构——受 LLM 启发的基于变压器的模型和图形神经网络。在本文中，我们介绍了 DocGraphLM，这是一个结合了预训练语言模型和图语义的新框架。为了实现这一点，我们提出了1)联合编码器体系结构来表示文档，2)一种新的链接预测方法来重建文档图形。DocGraphLM 使用收敛的联合损失函数预测节点之间的方向和距离，该函数优先考虑邻域恢复并降低远程节点检测。我们在三个 SotA 数据集上的实验表明，随着图形特征的采用，IE 和 QA 任务得到了一致的改善。此外，我们报告采用图形特征加速收敛的学习过程中的训练，尽管只是通过链接预测构造。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DocGraphLM:+Documental+Graph+Language+Model+for+Information+Extraction)|0|
|[Exploiting Ubiquitous Mentions for Document-Level Relation Extraction](https://doi.org/10.1145/3539618.3591984)|Ruoyu Zhang, Yanzeng Li, Minhao Zhang, Lei Zou|Peking University, Beijing, China|Recent years have witnessed the transition from sentence-level to document-level in relation extraction (RE), with new formulation, new methods and new insights. Yet, the fundamental concept, mention, is not well-considered and well-defined. Current datasets usually use automatically-detected named entities as mentions, which leads to the missing reference problem. We show that such phenomenon hinders models' reasoning abilities. To address it, we propose to incorporate coreferences (e.g. pronouns and common nouns) into mentions, based on which we refine and re-annotate the widely-used DocRED benchmark as R-DocRED. We evaluate various methods and conduct thorough experiments to demonstrate the efficacy of our formula. Specifically, the results indicate that incorporating coreferences helps reduce the long-term dependencies, further improving models' robustness and generalization under adversarial and low-resource settings. The new dataset is made publicly available for future research.|近年来，关系抽取技术经历了从句子层次到文档层次的转变，出现了新的表述方式、新的方法和新的见解。然而，这里提到的基本概念并没有经过深思熟虑和明确定义。当前数据集通常使用自动检测到的命名实体作为提及，这导致缺少引用问题。我们发现这种现象阻碍了模型的推理能力。为了解决这个问题，我们建议将共引用(例如代词和普通名词)合并到提及中，在此基础上，我们将广泛使用的 DocRED 基准改进并重新注释为 R-DocRED。我们评估各种方法，并进行彻底的实验，以证明我们的公式的功效。具体地说，结果表明，合并参考文献有助于减少长期依赖，进一步提高模型的健壮性和泛化的对抗性和低资源设置。这个新的数据集可以公开用于未来的研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploiting+Ubiquitous+Mentions+for+Document-Level+Relation+Extraction)|0|
|[Faster Dynamic Pruning via Reordering of Documents in Inverted Indexes](https://doi.org/10.1145/3539618.3591987)|Erman Yafay, Ismail Sengor Altingovde|Middle East Technical University, Ankara, Turkey|Widely used dynamic pruning algorithms (such as MaxScore, WAND and BMW) keep track of the k-th highest score (i.e., heap threshold) among the documents that are scored so far, to avoid scoring the documents that cannot get into the top-k result list. Obviously, the faster the heap threshold converges to its final value, the larger will be the number of skipped documents and hence, the efficiency gains of the pruning algorithms. In this paper, we tailor approaches that reorder the documents in the inverted index based on their access counts and ranks for previous queries. By storing such frequently retrieved documents at front of the postings lists, we aim to compute the heap threshold earlier during the query processing. Our approach yields substantial speedups (up to 1.33x) for all three dynamic pruning algorithms and outperforms two strong baselines that have been employed for document reordering in the literature.|广泛使用的动态修剪算法(如 MaxScore、 WAND 和 BMW)跟踪目前为止得分最高的文档中的 k-th 最高得分(即堆阈值) ，以避免对无法进入 top-k 结果列表的文档进行得分。显然，堆阈值收敛到最终值的速度越快，跳过的文档数量就越多，因此，剪枝算法的效率也就越高。在本文中，我们针对以前的查询，根据文档的访问次数和排名，对倒排索引中的文档进行重新排序。通过将这些经常检索到的文档存储在发布列表的前面，我们的目标是在查询处理的早期计算堆阈值。我们的方法为所有三种动态剪枝算法提供了可观的加速(最高1.33 x) ，并且优于文献中用于文档重排序的两个强基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Faster+Dynamic+Pruning+via+Reordering+of+Documents+in+Inverted+Indexes)|0|
|[Gated Attention with Asymmetric Regularization for Transformer-based Continual Graph Learning](https://doi.org/10.1145/3539618.3591991)|Hongxiang Lin, Ruiqi Jia, Xiaoqing Lyu|Wangxuan Institute of Computer Technology, Peking University, Beijing, China|Continual graph learning (CGL) aims to mitigate the topological-feature-induced catastrophic forgetting problem (TCF) in graph neural networks, which plays an essential role in the field of information retrieval. The TCF is mainly caused by the forgetting of node features of old tasks and the forgetting of topological features shared by old and new tasks. Existing CGL methods do not pay enough attention to the forgetting of topological features shared between different tasks. In this paper, we propose a transformer-based CGL method (Trans-CGL), thereby taking full advantage of the transformer's properties to mitigate the TCF problem. Specifically, to alleviate forgetting of node features, we introduce a gated attention mechanism for Trans-CGL based on parameter isolation that allows the model to be independent of each other when learning old and new tasks. Furthermore, to address the forgetting of shared parameters that store topological information between different tasks, we propose an asymmetric mask attention regularization module to constrain the shared attention parameters ensuring that the shared topological information is preserved. Comparative experiments show that the method achieves competitive performance on four real-world datasets.|连续图学习(CGL)的目的是缓解图神经网络中由拓扑特征引起的灾难性遗忘问题(TCF) ，这在信息检索领域中起着至关重要的作用。TCF 主要是由于遗忘旧任务的节点特征和新旧任务共享的拓扑特征造成的。现有的 CGL 方法对不同任务之间共享的拓扑特征的遗忘没有给予足够的重视。本文提出了一种基于变压器的 CGL 方法(Trans-CGL) ，从而充分利用变压器的特性来缓解 TCF 问题。为了减少节点特征的遗忘，本文提出了一种基于参数隔离的 Trans-CGL 门控注意机制，该机制允许模型在学习新旧任务时相互独立。此外，为了解决存储不同任务间拓扑信息的共享参数的遗忘问题，本文提出了一种非对称掩码注意正则化模型来约束共享注意参数，以保证共享拓扑信息的保留。对比实验表明，该方法在四个实际数据集上都取得了较好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Gated+Attention+with+Asymmetric+Regularization+for+Transformer-based+Continual+Graph+Learning)|0|
|[HiPrompt: Few-Shot Biomedical Knowledge Fusion via Hierarchy-Oriented Prompting](https://doi.org/10.1145/3539618.3591997)|Jiaying Lu, Jiaming Shen, Bo Xiong, Wenjing Ma, Steffen Staab, Carl Yang|Google Research, New York, NY, USA; Emory University, Atlanta, GA, USA; University of Stuttgart; University of Southampton, Stuttgart, Germany; University of Stuttgart, Stuttgart, Germany|Medical decision-making processes can be enhanced by comprehensive biomedical knowledge bases, which require fusing knowledge graphs constructed from different sources via a uniform index system. The index system often organizes biomedical terms in a hierarchy to provide the aligned entities with fine-grained granularity. To address the challenge of scarce supervision in the biomedical knowledge fusion (BKF) task, researchers have proposed various unsupervised methods. However, these methods heavily rely on ad-hoc lexical and structural matching algorithms, which fail to capture the rich semantics conveyed by biomedical entities and terms. Recently, neural embedding models have proved effective in semantic-rich tasks, but they rely on sufficient labeled data to be adequately trained. To bridge the gap between the scarce-labeled BKF and neural embedding models, we propose HiPrompt, a supervision-efficient knowledge fusion framework that elicits the few-shot reasoning ability of large language models through hierarchy-oriented prompts. Empirical results on the collected KG-Hi-BKF benchmark datasets demonstrate the effectiveness of HiPrompt.|综合生物医学知识库可以增强医学决策过程，它需要通过统一的指标体系融合不同来源的知识图。索引系统通常将生物医学术语组织在一个层次结构中，以提供细粒度的对齐实体。为了解决生物医学知识融合(BKF)任务中缺乏监督的问题，研究人员提出了各种无监督方法。然而，这些方法严重依赖于特定的词法和结构匹配算法，无法捕获生物医学实体和术语所传达的丰富语义。最近，神经嵌入模型被证明在语义丰富的任务中是有效的，但是它们依赖于足够的标记数据来进行充分的训练。为了弥合稀缺标记 BKF 模型和神经嵌入模型之间的差距，我们提出了 HiPrompt，一个监督高效的知识融合框架，通过面向层次的提示来激发大型语言模型的少镜头推理能力。在 KG-Hi-BKF 基准数据集上的实证结果证明了 HiPrompt 的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HiPrompt:+Few-Shot+Biomedical+Knowledge+Fusion+via+Hierarchy-Oriented+Prompting)|0|
|[How Significant Attributes are in the Community Detection of Attributed Multiplex Networks](https://doi.org/10.1145/3539618.3591998)|Junwei Cheng, Chaobo He, Kunlin Han, Wenjie Ma, Yong Tang|University of Southern California, Los Angeles, CA, USA; China Mobile Group Zhejiang Company Limited, Ningbo, China; South China Normal University, Guangzhou, China|Existing community detection methods for attributed multiplex networks focus on exploiting the complementary information from different topologies, while they are paying little attention to the role of attributes. However, we observe that real attributed multiplex networks exhibit two unique features, namely, consistency and homogeneity of node attributes. Therefore, in this paper, we propose a novel method, called ACDM, which is based on these two characteristics of attributes, to detect communities on attributed multiplex networks. Specifically, we extract commonality representation of nodes through the consistency of attributes. The collaboration between the homogeneity of attributes and topology information reveals the particularity representation of nodes. The comprehensive experimental results on real attributed multiplex networks well validate that our method outperforms state-of-the-art methods in most networks.|现有的属性复用网络社区检测方法主要是利用不同拓扑的互补信息，而对属性的作用关注不够。然而，我们观察到实际的属性化多路复用网络表现出两个独特的特征，即节点属性的一致性和同质性。因此，本文提出了一种基于属性的两个特征的 ACDM 方法来检测属性复用网络上的社区。具体来说，我们通过属性的一致性来提取节点的公共性表示。属性的同质性和拓扑信息的协同作用揭示了节点的特殊性表示。在实际属性化多路复用网络上的综合实验结果验证了该方法在大多数网络中的性能优于现有的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+Significant+Attributes+are+in+the+Community+Detection+of+Attributed+Multiplex+Networks)|0|
|[HyperFormer: Learning Expressive Sparse Feature Representations via Hypergraph Transformer](https://doi.org/10.1145/3539618.3591999)|Kaize Ding, Albert Jiongqian Liang, Bryan Perozzi, Ting Chen, Ruoxi Wang, Lichan Hong, Ed H. Chi, Huan Liu, Derek Zhiyuan Cheng|Google, Tempe, USA; Google, Mountain View, USA|Learning expressive representations for high-dimensional yet sparse features has been a longstanding problem in information retrieval. Though recent deep learning methods can partially solve the problem, they often fail to handle the numerous sparse features, particularly those tail feature values with infrequent occurrences in the training data. Worse still, existing methods cannot explicitly leverage the correlations among different instances to help further improve the representation learning on sparse features since such relational prior knowledge is not provided. To address these challenges, in this paper, we tackle the problem of representation learning on feature-sparse data from a graph learning perspective. Specifically, we propose to model the sparse features of different instances using hypergraphs where each node represents a data instance and each hyperedge denotes a distinct feature value. By passing messages on the constructed hypergraphs based on our Hypergraph Transformer (HyperFormer), the learned feature representations capture not only the correlations among different instances but also the correlations among features. Our experiments demonstrate that the proposed approach can effectively improve feature representation learning on sparse features.|长期以来，学习高维稀疏特征的表达方式一直是信息检索领域的一个难题。目前的深度学习方法虽然能够部分解决这一问题，但往往无法处理大量稀疏特征，尤其是训练数据中不常出现的尾部特征值。更糟糕的是，现有的方法不能明确地利用不同实例之间的相关性来帮助进一步改善稀疏特征的表示学习，因为这样的关系先验知识是没有提供的。为了解决这些问题，本文从图学习的角度研究了特征稀疏数据的表示学习问题。具体来说，我们建议使用超图对不同实例的稀疏特征进行建模，其中每个节点代表一个数据实例，每个超边表示一个不同的特征值。通过在基于超图变换的构造的超图上传递消息，学习的特征表示不仅能够捕获不同实例之间的相关性，而且能够捕获特征之间的相关性。实验结果表明，该方法能够有效地改善稀疏特征的特征表示学习。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HyperFormer:+Learning+Expressive+Sparse+Feature+Representations+via+Hypergraph+Transformer)|0|
|[Best Prompts for Text-to-Image Models and How to Find Them](https://doi.org/10.1145/3539618.3592000)|Nikita Pavlichenko, Dmitry Ustalov||Recent progress in generative models, especially in text-guided diffusion models, has enabled the production of aesthetically-pleasing imagery resembling the works of professional human artists. However, one has to carefully compose the textual description, called the prompt, and augment it with a set of clarifying keywords. Since aesthetics are challenging to evaluate computationally, human feedback is needed to determine the optimal prompt formulation and keyword combination. In this paper, we present a human-in-the-loop approach to learning the most useful combination of prompt keywords using a genetic algorithm. We also show how such an approach can improve the aesthetic appeal of images depicting the same descriptions.|最近在生成模型，特别是在文本引导的扩散模型的进展，已经能够生产美观的形象类似于专业人类艺术家的作品。但是，必须仔细编写文本描述(称为提示符) ，并使用一组澄清关键字对其进行扩充。由于美学是具有挑战性的评价计算，人类反馈需要确定最佳的及时公式和关键字组合。在本文中，我们提出了一个人在循环的方法来学习最有用的组合提示关键字使用遗传算法。我们还展示了这种方法如何能够提高描绘相同描述的图像的审美吸引力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Best+Prompts+for+Text-to-Image+Models+and+How+to+Find+Them)|0|
|[LAPCA: Language-Agnostic Pretraining with Cross-Lingual Alignment](https://doi.org/10.1145/3539618.3592006)|Dmitry Abulkhanov, Nikita Sorokin, Sergey Nikolenko, Valentin Malykh|Ivannikov Institute for System Programming of the RAS & Steklov Institute of Mathematics of the RAS, Moscow, Russian Fed.; Huawei Noah's Ark Lab, Moscow, Russian Fed.|Data collection and mining is a crucial bottleneck for cross-lingual information retrieval (CLIR). While previous works used machine translation and iterative training, we present a novel approach to cross-lingual pretraining called LAPCA (language-agnostic pretraining with cross-lingual alignment). We train the LAPCA-LM model based on XLM-RoBERTa and łexa that significantly improves cross-lingual knowledge transfer for question answering and sentence retrieval on, e.g., XOR-TyDi and Mr. TyDi datasets, and in the zero-shot cross-lingual scenario performs on par with supervised methods, outperforming many of them on MKQA.|数据收集和挖掘是跨语言信息检索(CLIR)的一个关键瓶颈。虽然以前的工作使用机器翻译和迭代训练，我们提出了一种新的方法来跨语言预训练称为 LAPCA (语言无关预训练跨语言对齐)。我们训练了基于 XLM-RoBERTa 和 exa 的 LAPCA-LM 模型，该模型显着改善了问题回答和句子检索的跨语言知识转移，例如 XOR-TyDi 和 TyDi 先生数据集，并且在零击跨语言场景中表现与监督方法相当，在 MKQA 上优于其中许多方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LAPCA:+Language-Agnostic+Pretraining+with+Cross-Lingual+Alignment)|0|
|[Learning from Crowds with Annotation Reliability](https://doi.org/10.1145/3539618.3592007)|Zhi Cao, Enhong Chen, Ye Huang, Shuanghong Shen, Zhenya Huang|Anhui Province Key Laboratory of Big Data Analysis and Application, School of Computer Science and Technology, University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence, Hefei, China|Crowdsourcing provides a practical approach for obtaining annotated data to train supervised learning models. However, since the crowd annotators may have different expertise domain and cannot always guarantee the high-quality annotations, learning from crowds generally suffers from the problem of unreliable results of introducing some noises, which makes it hard to achieve satisfying performance. In this work, we investigate the reliability of annotations to improve learning from crowds. Specifically, we first project annotator and data instance to factor vectors and model the complex interaction between annotator expertise and instance difficulty to predict annotation reliability. The learned reliability can be used to evaluate the quality of crowdsourced data directly. Then, we construct a new annotation, namely soft annotation, which serves as the gold label during the training. To recognize the different strengths of annotators, we model each annotator's confusion in an end-to-end manner. Extensive experimental results on three real-world datasets demonstrate the effectiveness of our method.|众包为获取注释数据以训练监督式学习模型提供了一种实用的方法。然而，由于人群注释者的专业领域不同，并不能保证注释的质量，因此向人群学习通常存在引入一些噪声的结果不可靠的问题，难以达到令人满意的效果。在这项工作中，我们调查注释的可靠性，以改善从群体学习。具体来说，我们首先将注释者和数据实例投影到因子向量中，建立注释者专业知识和实例难度之间的复杂交互模型来预测注释的可靠性。学习可靠性可以直接用来评价众包数据的质量。然后，我们构建了一个新的注释，即软注释，作为培训过程中的金标签。为了识别注释者的不同优势，我们以端到端的方式对每个注释者的混淆进行建模。在三个实际数据集上的大量实验结果证明了该方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+from+Crowds+with+Annotation+Reliability)|0|
|[Learning Through Interpolative Augmentation of Dynamic Curvature Spaces](https://doi.org/10.1145/3539618.3592008)|Parth Chhabra, Atula Tejaswi Neerkaje, Shivam Agarwal, Ramit Sawhney, Megh Thakkar, Preslav Nakov, Sudheer Chava|BITS, Pilani, Pilani, India; University of Illinois at Urbana-Champaign, Urbana, IL, USA; Georgia Institute of Technology, Atlanta, GA, USA; Manipal Institute of Technology, Manipal, India; Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE; Indraprastha Institute of Information Technology, Delhi, Delhi, India|Mixup is an efficient data augmentation technique, which improves generalization by interpolating random examples. While numerous approaches have been developed for Mixup in the Euclidean and in the hyperbolic space, they do not fully use the intrinsic properties of the examples, i.e., they manually set the geometry (Euclidean or hyperbolic) based on the overall dataset, which may be sub-optimal since each example may require a different geometry. We propose DynaMix, a framework that automatically selects an example-specific geometry and performs Mixup between the different geometries to improve training dynamics and generalization. Through extensive experiments in image and text modalities we show that DynaMix outperforms state-of-the-art methods over six downstream applications. We find that DynaMix is more useful in low-resource and semi-supervised settings likely because it displays a probabilistic view of the geometry.|混合是一种有效的数据增强技术，它通过插值随机样本来提高泛化能力。虽然在欧几里得和双曲空间中已经开发了很多种方法用于 Mixup，但它们并没有充分利用示例的内在属性，即，它们基于整个数据集手动设置几何(欧几里得或双曲线) ，这可能是次优的，因为每个示例可能需要不同的几何。我们提出了 DynaMix，这是一个自动选择特定于示例的几何图形并在不同几何图形之间执行 Mixup 的框架，以改善训练动态性和泛化性。通过在图像和文本模式中的大量实验，我们显示 DynaMix 在六个下游应用程序中优于最先进的方法。我们发现 DynaMix 在低资源和半监督设置中更有用，因为它显示了几何的概率视图。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Through+Interpolative+Augmentation+of+Dynamic+Curvature+Spaces)|0|
|[Learning to Ask Clarification Questions with Spatial Reasoning](https://doi.org/10.1145/3539618.3592009)|Yang Deng, Shuaiyi Li, Wai Lam|The Chinese University of Hong Kong, Hong Kong, Hong Kong|Asking clarifying questions has become a key element of various conversational systems, allowing for an effective resolution of ambiguity and uncertainty through natural language questions. Despite the extensive applications of spatial information grounded dialogues, it remains an understudied area on learning to ask clarification questions with the capability of spatial reasoning. In this work, we propose a novel method, named SpatialCQ, for this problem. Specifically, we first align the representation space between textual and spatial information by encoding spatial states with textual descriptions. Then a multi-relational graph is constructed to capture the spatial relations and enable spatial reasoning with relational graph attention networks. Finally, a unified encoder is adopted to fuse the multimodal information for asking clarification questions. Experimental results on the latest IGLU dataset show the superiority of the proposed method over existing approaches.|提出明确的问题已经成为各种会话系统的一个关键因素，允许通过自然语言问题有效地解决歧义和不确定性。尽管基于空间信息的对话得到了广泛的应用，但是在学习提出具有空间推理能力的澄清问题方面，它仍然是一个尚未被研究的领域。在这项工作中，我们提出了一个新的方法，称为空间 CQ，为这个问题。具体来说，我们首先通过文本描述对空间状态进行编码，从而在文本信息和空间信息之间对齐表示空间。然后构造一个多关系图来捕获空间关系，并利用关系图注意网络进行空间推理。最后，采用统一的编码器对多模态信息进行融合，提出澄清问题。在最新的 IGLU 数据集上的实验结果表明了该方法相对于现有方法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Ask+Clarification+Questions+with+Spatial+Reasoning)|0|
|[Learning to Ask Questions for Zero-shot Dialogue State Tracking](https://doi.org/10.1145/3539618.3592010)|Diogo Tavares, David Semedo, Alexander Rudnicky, João Magalhães|NOVA University of Lisbon, Lisbon, Portugal; Carnegie Mellon University, Pittsburgh, PA, USA|We present a method for performing zero-shot Dialogue State Tracking (DST) by casting the task as a learning-to-ask-questions framework. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling annotations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation-given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation.|提出了一种将任务作为学习-提问框架进行对话状态跟踪(DST)的方法。该框架学习将最佳问题生成(QG)策略与领域内问题回答(QA)方法结合起来，在不需要任何人工干预的情况下从对话中提取插槽值。一个新的自监督 QA 预训练步骤使用域内数据是必不可少的结构学习不需要任何插槽填充注释。此外，我们表明 QG 方法需要与对话中使用的相同人称保持一致。对 MultiWOZ 2.1数据集的实证评估表明，我们的方法，当与健壮的 QA 模型一起使用时，在零拍跨域适应的挑战性任务中优于现有的零拍方法-在数据创建期间给定相当数量的领域知识。最后，我们分析了所使用问题类型的影响，并证明了算法的方法优于基于模板的问题生成。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Ask+Questions+for+Zero-shot+Dialogue+State+Tracking)|0|
|[Look Ahead: Improving the Accuracy of Time-Series Forecasting by Previewing Future Time Features](https://doi.org/10.1145/3539618.3592013)|Seonmin Kim, DongKyu Chae|Hanyang University, Seoul, Republic of Korea|Time-series forecasting has been actively studied and adopted in various real-world domains. Recently there have been two research mainstreams in this area: building Transformer-based architectures such as Informer, Autoformer and Reformer, and developing time-series representation learning frameworks based on contrastive learning such as TS2Vec and CoST. Both efforts have greatly improved the performance of time series forecasting. In this paper, we investigate a novel direction towards improving the forecasting performance even more, which is orthogonal to the aforementioned mainstreams as a model-agnostic scheme. We focus on time stamp embeddings that has been less-focused in the literature. Our idea is simple-yet-effective: based on given current time stamp, we predict embeddings of its near future time stamp and utilize the predicted embeddings in the time-series (value) forecasting task. We believe that if such future time information can be previewed at the time of prediction, they can be utilized by any time-series forecasting models as useful additional information. Our experimental results confirmed that our method consistently and significantly improves the accuracy of the recent Transformer-based models and time-series representation learning frameworks. Our code is available at: https://github.com/sunsunmin/Look_Ahead|时间序列预测在现实世界的各个领域都得到了积极的研究和应用。目前该领域的研究主流主要有两种: 一种是构建基于变压器的体系结构，如 Informer、 Autoformer 和 Reformer; 另一种是开发基于对比学习的时间序列表示学习框架，如 TS2Vec 和 CoST。这两种方法都大大提高了时间序列预测的性能。在本文中，我们研究了一个新的方向来提高预测性能，这是正交的上述主流作为一个模型不可知方案。我们关注的是时间戳嵌入，这在文献中已经很少被关注。我们的思想是简单而有效的: 在给定的当前时间戳的基础上，我们预测其近期时间戳的嵌入，并利用预测嵌入的时间序列(值)预测任务。我们相信，如果这种未来时间信息可以在预测时预测，它们可以被任何时间序列预测模型作为有用的附加信息加以利用。我们的实验结果证实，我们的方法一致和显著地提高了最近的变压器为基础的模型和时间序列表示学习框架的准确性。我们的代码可以在以下 https://github.com/sunsunmin/look_ahead 找到|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Look+Ahead:+Improving+the+Accuracy+of+Time-Series+Forecasting+by+Previewing+Future+Time+Features)|0|
|[MA-MRC: A Multi-answer Machine Reading Comprehension Dataset](https://doi.org/10.1145/3539618.3592015)|Zhiang Yue, Jingping Liu, Cong Zhang, Chao Wang, Haiyun Jiang, Yue Zhang, Xianyang Tian, Zhedong Cen, Yanghua Xiao, Tong Ruan|Fudan University, Shanghai, China; AECC Sichuan Gas Turbine Establishment, Mianyang City, China; Tencent AI Lab, Shenzhen, China; Shanghai University, Shanghai, China; East China University of Science and Technology, Shanghai, China|Machine reading comprehension (MRC) is an essential task for many question-answering applications. However, existing MRC datasets mainly focus on data with single answer and overlook multiple answers, which are common in the real world. In this paper, we aim to construct an MRC dataset with both data of single answer and multiple answers. To achieve this purpose, we design a novel pipeline method: data collection, data cleaning, question generation and test set annotation. Based on these procedures, we construct a high-quality multi-answer MRC dataset (MA-MRC) with 129K question-answer-context samples. We implement a sequence of baselines and carry out extensive experiments on MA-MRC. According to the experimental results, MA-MRC is a challenging dataset, which can facilitate the future research on the multi-answer MRC task.|机器阅读理解(MRC)是许多问答应用程序的基本任务。然而，现有的 MRC 数据集主要集中在单答案数据上，忽略了现实世界中常见的多答案数据。本文旨在构建一个同时包含单个答案和多个答案的 MRC 数据集。为此，我们设计了一种新的流水线方法: 数据采集、数据清理、问题生成和测试集注释。在此基础上，我们构建了一个129K 问答背景样本的高质量多答案 MRC 数据集(MA-MRC)。我们实现了一系列的基线，并在 MA-MRC 上进行了广泛的实验。实验结果表明，MA-MRC 是一个具有挑战性的数据集，有利于多答案 MRC 任务的进一步研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MA-MRC:+A+Multi-answer+Machine+Reading+Comprehension+Dataset)|0|
|[Multiple Topics Community Detection in Attributed Networks](https://doi.org/10.1145/3539618.3592026)|Chaobo He, Junwei Cheng, Guohua Chen, Yong Tang|South China Normal University & Pazhou Lab, Guangzhou, China; South China Normal University, Guangzhou, China|Since existing methods are often not effective to detect communities with multiple topics in attributed networks, we propose a method named SSAGCN via Autoencoder-style self-supervised learning. SSAGCN firstly designs an adaptive graph convolutional network (AGCN), which is treated as the encoder for fusing topology information and attribute information automatically, and then utilizes a dual decoder to simultaneously reconstruct network topology and attributes. By further introducing the modularity maximization and the joint optimization strategies, SSAGCN can detect communities with multiple topics in an end-to-end manner. Experimental results show that SSAGCN outperforms state-of-the-art approaches, and also can be used to conduct topic analysis well.|由于现有的方法往往不能有效地检测属性网络中具有多个主题的社区，我们提出了一种基于自动编码的自监督学习方法 SSAGCN。SSAGCN 首先设计一个自适应图卷积网络(AGCN) ，将其作为自动融合拓扑信息和属性信息的编码器，然后利用一个双解码器同时重构网络拓扑和属性。通过进一步引入模块化最大化和联合优化策略，SSAGCN 能够以端到端的方式检测具有多个主题的社区。实验结果表明，SSAGCN 方法的性能优于目前最先进的方法，并且可以很好地用于主题分析。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multiple+Topics+Community+Detection+in+Attributed+Networks)|0|
|[NC2T: Novel Curriculum Learning Approaches for Cross-Prompt Trait Scoring](https://doi.org/10.1145/3539618.3592027)|Yejin Lee, Seokwon Jeong, Hongjin Kim, Taeil Kim, SungWon Choi, Harksoo Kim|Konkuk University, Seoul, Republic of Korea; Konkuk University, Seoul, South Korea; Naver, Gyeonggi-do, Republic of Korea; Kangwon National University, Gangwon-do, Republic of Korea|Automated essay scoring (AES) is a crucial research area with potential applications in education and beyond. However, recent studies have primarily focused on AES models that evaluate essays within a specific domain or using a holistic score, leaving a gap in research and resources for more generalized models capable of assessing essays with detailed items from multiple perspectives. As evaluating and scoring essays based on complex traits is costly and time-consuming, datasets for such AES evaluations are limited. To address these issues, we developed a cross-prompt trait scoring AES model and proposed a suitable curriculum learning (CL) design. By devising difficulty scores and introducing the key curriculum method, we demonstrated its effectiveness compared to existing CL strategies in natural language understanding tasks.|自动论文评分(AES)是一个重要的研究领域，在教育和其他方面有潜在的应用。然而，最近的研究主要集中在 AES 模型评估论文在一个特定的领域或使用一个整体评分，留下了研究和资源的差距，更广泛的模型能够评估论文的详细项目从多个角度。由于基于复杂性状的论文评估和评分是昂贵的和耗时的，用于这种 AES 评估的数据集是有限的。为了解决这些问题，我们开发了一个交叉提示特质评分 AES 模型，并提出了一个合适的课程学习(CL)设计。在自然语言理解任务中，通过设计难度分数和引入关键课程方法，验证了该策略与现有的 CL 策略相比的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NC2T:+Novel+Curriculum+Learning+Approaches+for+Cross-Prompt+Trait+Scoring)|0|
|[On the Impact of Data Quality on Image Classification Fairness](https://doi.org/10.1145/3539618.3592031)|Aki Barry, Lei Han, Gianluca Demartini|The University of Queensland, Brisbane, QLD, Australia|With the proliferation of algorithmic decision-making, increased scrutiny has been placed on these systems. This paper explores the relationship between the quality of the training data and the overall fairness of the models trained with such data in the context of supervised classification. We measure key fairness metrics across a range of algorithms over multiple image classification datasets that have a varying level of noise in both the labels and the training data itself. We describe noise in the labels as inaccuracies in the labelling of the data in the training set and noise in the data as distortions in the data, also in the training set. By adding noise to the original datasets, we can explore the relationship between the quality of the training data and the fairness of the output of the models trained on that data.|随着算法决策的激增，对这些系统的审查也越来越多。本文探讨了在有监督分类的情况下，训练数据的质量与用这些数据训练的模型的总体公平性之间的关系。我们在多个图像分类数据集上测量一系列算法的关键公平性指标，这些图像分类数据在标签和训练数据本身中都有不同程度的噪声。我们将标签中的噪声描述为训练集中数据标注的不准确性，将数据中的噪声描述为数据中的畸变，同时也将数据中的噪声描述为训练集中的畸变。通过在原始数据集中添加噪声，我们可以探索训练数据的质量与模型输出的公平性之间的关系。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+the+Impact+of+Data+Quality+on+Image+Classification+Fairness)|0|
|[Power Norm Based Lifelong Learning for Paraphrase Generations](https://doi.org/10.1145/3539618.3592039)|Dingcheng Li, Peng Yang, Yue Zhang, Ping Li|Baidu Research, Sammamish, WA, USA; Baidu Research, Bellevue, WA, USA|Lifelong seq2seq language generation models are trained with multiple domains in a lifelong learning manner, with data from each domain being observed in an online fashion. It is a well-known problem that lifelong learning suffers from the catastrophic forgetting (CF). To handle this challenge, existing works have leveraged experience replay or dynamic architecture to consolidate the past knowledge, which however result in incremental memory space or high computational cost. In this work, we propose a novel framework name "power norm based lifelong learning" (PNLLL), which aims to remedy the catastrophic forgetting issues with a power normalization on NLP transformer models. Specifically, PNLLL leverages power norm to achieve a better balance between past experience rehearsal and new knowledge acquisition. These designs enable the knowledge adaptation onto new tasks while memorizing the experience of past tasks. Our experiments on paraphrase generation tasks show that PNLLL not only outperforms SOTA models by a considerable margin and but also largely alleviates forgetting.|终身 seq2seq 语言生成模型以终身学习的方式对多个域进行训练，每个域的数据以在线方式进行观察。众所周知，终身学习患有灾难性遗忘症(CF)。为了应对这一挑战，现有的工作已经利用经验重放或动态架构来巩固过去的知识，但这导致增加的内存空间或高计算成本。在这项工作中，我们提出了一个新的框架名称“基于功率规范的终身学习”(PNLL) ，旨在补救灾难性遗忘问题的功率规范化的自然语言处理变压器模型。具体来说，PNLL 利用权力规范，在过去的经验复习和新的知识获取之间实现更好的平衡。这些设计使知识适应新的任务，同时记忆过去的任务经验。我们在释义生成任务上的实验表明，PNLLL 不仅比 SOTA 模型有相当大的优势，而且在很大程度上减轻了遗忘。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Power+Norm+Based+Lifelong+Learning+for+Paraphrase+Generations)|0|
|[Private Meeting Summarization Without Performance Loss](https://doi.org/10.1145/3539618.3592042)|Seolhwa Lee, Anders Søgaard|Technical University of Darmstadt, Darmstadt, Germany; University of Copenhagen, Copenhagen, Denmark|Meeting summarization has an enormous business potential, but in addition to being a hard problem, roll-out is challenged by privacy concerns. We explore the problem of meeting summarization under differential privacy constraints and find, to our surprise, that while differential privacy leads to slightly lower performance on in-sample data, differential privacy improves performance when evaluated on unseen meeting types. Since meeting summarization systems will encounter a great variety of meeting types in practical employment scenarios, this observation makes safe meeting summarization seem much more feasible. We perform extensive error analysis and identify potential risks in meeting summarization under differential privacy, including a faithfulness analysis.|会议摘要具有巨大的商业潜力，但除了成为一个难题之外，它的推出还受到隐私问题的挑战。我们研究了在差分隐私约束下会议总结的问题，令我们惊讶的是，虽然差分隐私会导致样本内数据的性能稍微下降，但是当在看不见的会议类型上进行评估时，差分隐私会提高性能。由于会议摘要系统在实际的工作场景中会遇到各种各样的会议类型，这种观察使得安全的会议摘要看起来更加可行。我们进行广泛的错误分析，并识别在差分隐私下会议摘要的潜在风险，包括忠实性分析。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Private+Meeting+Summarization+Without+Performance+Loss)|0|
|[Read it Twice: Towards Faithfully Interpretable Fact Verification by Revisiting Evidence](https://doi.org/10.1145/3539618.3592049)|Xuming Hu, Zhaochen Hong, Zhijiang Guo, Lijie Wen, Philip S. Yu|Tsinghua University, Beijing, China; University of Cambridge, Cambridgeshire, England UK; University of Illinois at Chicago, Chicago, IL, USA|Real-world fact verification task aims to verify the factuality of a claim by retrieving evidence from the source document. The quality of the retrieved evidence plays an important role in claim verification. Ideally, the retrieved evidence should be faithful (reflecting the model's decision-making process in claim verification) and plausible (convincing to humans), and can improve the accuracy of verification task. Although existing approaches leverage the similarity measure of semantic or surface form between claims and documents to retrieve evidence, they all rely on certain heuristics that prevent them from satisfying all three requirements. In light of this, we propose a fact verification model named ReRead to retrieve evidence and verify claim that: (1) Train the evidence retriever to obtain interpretable evidence (i.e., faithfulness and plausibility criteria); (2) Train the claim verifier to revisit the evidence retrieved by the optimized evidence retriever to improve the accuracy. The proposed system is able to achieve significant improvements upon best-reported models under different settings.|现实世界中的事实验证任务是通过从原始文档中检索证据来验证索赔的真实性。检索到的证据的质量在索赔核实中起着重要作用。理想情况下，检索到的证据应该是忠实的(反映模型的决策过程中的索赔验证)和合理的(令人信服) ，并可以提高验证任务的准确性。尽管现有的方法利用声明和文档之间的语义或表面形式的相似性度量来检索证据，但是它们都依赖于某些启发式方法，这些启发式方法使它们无法满足所有三个要求。针对这种情况，我们提出了一种名为 ReRead 的事实验证模型来检索证据并验证索赔: (1)训练证据检索者获得可解释的证据(即忠实性和合理性标准) ; (2)训练索赔验证者重新检索优化后的证据检索者检索的证据以提高准确性。拟议的系统能够在不同环境下对最佳报告模型作出重大改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Read+it+Twice:+Towards+Faithfully+Interpretable+Fact+Verification+by+Revisiting+Evidence)|0|
|[RewardTLG: Learning to Temporally Language Grounding from Flexible Reward](https://doi.org/10.1145/3539618.3592054)|Yawen Zeng, Keyu Pan, Ning Han|ByteDance AI Lab, China, China; Xiangtan University, China, China|Given a textual sentence provided by a user, the Temporal Language Grounding (TLG) task is defined as the process of finding a semantically relevant video moment or clip from an untrimmed video. In recent years, localization-based TLG methods have been explored, which adopt reinforcement learning to locate a clip from the video. However, these methods are not stable enough due to the stochastic exploration mechanism of reinforcement learning, which is sensitive to the reward. Therefore, providing a more flexible and reasonable reward has become a focus of attention for both academia and industry. Inspired by the training process of chatGPT, we innovatively adopt a vision-language pre-training (VLP) model as a reward model, which provides flexible rewards to help the localization-based TLG task converge. Specifically, a reinforcement learning-based localization module is introduced to predict the start and end timestamps in multi-modal scenarios. Thereafter, we fine-tune a reward model based on a VLP model, even introducing some human feedback, which provides a flexible reward score for the localization module. In this way, our model is able to capture subtle differences of the untrimmed video. Extensive experiments on two datasets have well verified the effectiveness of our proposed solution.|给定一个用户提供的文本句子，时态语言基础(TLG)任务被定义为从未修剪的视频中寻找语义相关的视频片段或剪辑的过程。近年来，人们开始探索基于本地化的 TLG 方法，即采用强化学习来定位视频片段。然而，由于强化学习的随机勘探机制对报酬非常敏感，这些方法不够稳定。因此，提供更加灵活合理的报酬成为学术界和业界关注的焦点。本文受到聊天 GPT 训练过程的启发，创新性地采用视觉语言预训练(VLP)模型作为奖励模型，提供灵活的奖励，帮助基于定位的 TLG 任务收敛。特别地，引入了一个基于强化学习的定位模块来预测多模态场景中的开始和结束时间戳。在此基础上，对基于 VLP 模型的奖励模型进行了微调，甚至引入了一些人工反馈，为定位模块提供了灵活的奖励评分。通过这种方式，我们的模型能够捕捉未修剪视频的细微差别。在两个数据集上的大量实验已经很好地验证了我们提出的解决方案的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RewardTLG:+Learning+to+Temporally+Language+Grounding+from+Flexible+Reward)|0|
|[SelfLRE: Self-refining Representation Learning for Low-resource Relation Extraction](https://doi.org/10.1145/3539618.3592058)|Xuming Hu, Junzhe Chen, Shiao Meng, Lijie Wen, Philip S. Yu|Tsinghua University, Beijing, China; University of Illinois at Chicago, Chicago, IL, USA|Low-resource relation extraction (LRE) aims to extract potential relations from limited labeled corpus to handle the problem of scarcity of human annotations. Previous works mainly consist of two categories of methods: (1) Self-training methods, which improve themselves through the models' predictions, thus suffering from confirmation bias when the predictions are wrong. (2) Self-ensembling methods, which learn task-agnostic representations, therefore, generally do not work well for specific tasks. In our work, we propose a novel LRE architecture named SelfLRE, which leverages two complementary modules, one module uses self-training to obtain pseudo-labels for unlabeled data, and the other module uses self-ensembling learning to obtain the task-agnostic representations, and leverages the existing pseudo-labels to refine the better task-specific representations on unlabeled data. The two models are jointly trained through multi-task learning to iteratively improve the effect of LRE task. Experiments on three public datasets show that SelfLRE achieves 1.81% performance gain over the SOTA baseline. Source code is available at: https://github.com/THU-BPM/SelfLRE.|低资源关系抽取(LRE)的目的是从有限的标记语料库中提取潜在的关系，以解决人工标注稀缺的问题。以往的研究主要包括两类方法: (1)自我训练方法，它通过模型的预测来改善自身，当预测错误时就会产生确认偏差。(2)学习任务不可知表征的自组合方法对特定任务通常不起作用。在我们的工作中，我们提出了一种新的 LRE 体系结构 SelfLRE，它利用两个互补模块，一个模块使用自我训练来获得未标记数据的伪标签，另一个模块使用自我集成学习来获得任务无关的表示，并利用现有的伪标签来改进未标记数据的更好的任务特定表示。两种模型通过多任务联合训练，迭代地提高 LRE 任务的效果。在三个公共数据集上的实验表明，SelfLRE 在 SOTA 基线上获得了1.81% 的性能增益。源代码可在以下 https://github.com/thu-bpm/selflre 找到:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SelfLRE:+Self-refining+Representation+Learning+for+Low-resource+Relation+Extraction)|0|
|[Simple Approach for Aspect Sentiment Triplet Extraction Using Span-Based Segment Tagging and Dual Extractors](https://doi.org/10.1145/3539618.3592060)|Dongxu Li, Zhihao Yang, Yuquan Lan, Yunqi Zhang, Hui Zhao, Gang Zhao|East China Normal University, Shanghai, China; Shanghai Key Laboratory of Trustworthy Computing, Shanghai, China; Microsoft, Beijing, China|Aspect sentiment triplet extraction (ASTE) is a task which extracts aspect terms, opinion terms, and sentiment polarities as triplets from review sentences. Existing approaches have developed bidirectional structures for term interaction. Sentiment polarities are subsequently extracted from aspect-opinion pairs. These solutions suffer from: 1) high dependency on custom bidirectional structures, 2) inadequate representation of the information through existing tagging schemes, and 3) insufficient usage of all available sentiment data. To address the above issues, we propose a simple span-based solution named SimSTAR with Segment Tagging And dual extRactors. SimSTAR does not introduce any additional bidirectional mechanism. The segment tagging scheme is capable to indicate all possible cases of spans and reveals more information through negative labels. Dual extractors are employed to make the sentiment extraction independent of the term extraction. We evaluate our model on four ASTE datasets. The experimental results show that our simple method achieves state-of-the-art performance.|方面情感三联提取(ASTE)是一项从复习句中提取方面词、意见词和情感极性的任务。现有的方法已经开发了用于术语交互的双向结构。情感极性随后从方面-意见对中提取出来。这些解决方案受到以下因素的影响: 1)高度依赖定制的双向结构，2)通过现有的标记方案不能充分表示信息，3)不能充分利用所有可用的情感数据。为了解决上述问题，我们提出了一个简单的基于跨度的解决方案，名为 SimSTAR，它具有段标记和双提取器。SimSTAR 不引入任何额外的双向机制。分段标记方案能够指示所有可能的跨度情况，并通过负标记显示更多的信息。采用双抽取器使情感抽取独立于术语抽取。我们在四个 ASTE 数据集上评估我们的模型。实验结果表明，我们的简单方法达到了最先进的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Simple+Approach+for+Aspect+Sentiment+Triplet+Extraction+Using+Span-Based+Segment+Tagging+and+Dual+Extractors)|0|
|[Surprise: Result List Truncation via Extreme Value Theory](https://doi.org/10.1145/3539618.3592066)|Dara Bahri, Che Zheng, Yi Tay, Donald Metzler, Andrew Tomkins|Google Research, Mountain View, CA, USA|Work in information retrieval has largely been centered around ranking and relevance: given a query, return some number of results ordered by relevance to the user. The problem of result list truncation, or where to truncate the ranked list of results, however, has received less attention despite being crucial in a variety of applications. Such truncation is a balancing act between the overall relevance, or usefulness of the results, with the user cost of processing more results. Result list truncation can be challenging because relevance scores are often not well-calibrated. This is particularly true in large-scale IR systems where documents and queries are embedded in the same metric space and a query's nearest document neighbors are returned during inference. Here, relevance is inversely proportional to the distance between the query and candidate document, but what distance constitutes relevance varies from query to query and changes dynamically as more documents are added to the index. In this work, we propose Surprise scoring, a statistical method that leverages the Generalized Pareto distribution that arises in extreme value theory to produce interpretable and calibrated relevance scores at query time using nothing more than the ranked scores. We demonstrate its effectiveness on the result list truncation task across image, text, and IR datasets and compare it to both classical and recent baselines. We draw connections to hypothesis testing and $p$-values.|信息检索的工作主要围绕着排名和相关性: 给出一个查询，返回一些根据用户相关性排序的结果。然而，尽管结果列表截断问题在各种应用中都是至关重要的，但是它在结果排序列表中的截断位置却没有得到足够的重视。这种截断是在结果的整体相关性或有用性与处理更多结果的用户成本之间的一种平衡行为。结果列表的截断可能是具有挑战性的，因为相关性得分往往不能很好地校准。在大规模 IR 系统中尤其如此，其中文档和查询嵌入在相同的度量空间中，并且在推断过程中返回查询的最近文档邻居。在这里，相关性与查询和候选文档之间的距离成反比，但是构成相关性的距离因查询而异，并且随着向索引添加更多文档而动态变化。在这项工作中，我们提出了惊喜得分，一种统计方法，利用极值理论中出现的广义帕累托分布，在查询时仅仅使用排名得分就可以产生可解释和校准的相关得分。我们证明了它在跨图像、文本和红外数据集的结果列表截断任务中的有效性，并将其与经典和最近的基线进行了比较。我们把假设检验和 $p $- 值联系起来。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Surprise:+Result+List+Truncation+via+Extreme+Value+Theory)|0|
|[The Tale of Two MSMARCO - and Their Unfair Comparisons](https://doi.org/10.1145/3539618.3592071)|Carlos Lassance, Stéphane Clinchant|Naver Labs Europe, Meylan, France|The MS MARCO-passage dataset has been the main large-scale dataset open to the IR community and it has fostered successfully the development of novel neural retrieval models over the years. But, it turns out that two different corpora of MS MARCO are used in the literature, the official one and a second one where passages were augmented with titles, mostly due to the introduction of the Tevatron code base. However, the addition of titles actually leaks relevance information, while breaking the original guidelines of the MS MARCO-passage dataset. In this work, we investigate the differences between the two corpora and demonstrate empirically that they make a significant difference when evaluating a new method. In other words, we show that if a paper does not properly report which version is used, reproducing fairly its results is basically impossible. Furthermore, given the current status of reviewing, where monitoring state-of-the-art results is of great importance, having two different versions of a dataset is a large problem. This is why this paper aims to report the importance of this issue so that researchers can be made aware of this problem and appropriately report their results.|多年来，MS MARCO 通道数据集已成为向国际关系界开放的主要大规模数据集，并成功地促进了新型神经检索模型的发展。但是，事实证明，文献中使用了两个不同的 MS MARCO 语料库，第一个是官方语料库，第二个语料库中的段落增加了标题，这主要是由于 Tevatron 代码库的引入。然而，标题的添加实际上泄漏了相关信息，同时打破了 MS MARCO 通过数据集的原始指导方针。在这项工作中，我们调查了两个语料库之间的差异，并实证表明，它们在评价一种新的方法时有显著的差异。换句话说，我们表明，如果一篇论文没有正确地报告使用了哪个版本，公平地复制它的结果基本上是不可能的。此外，考虑到当前的审查状态(其中监视最先进的结果非常重要) ，拥有一个数据集的两个不同版本是一个大问题。这就是为什么本文旨在报告这个问题的重要性，使研究人员可以意识到这个问题，并适当地报告他们的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Tale+of+Two+MSMARCO+-+and+Their+Unfair+Comparisons)|0|
|[Towards Robust Knowledge Tracing Models via k-Sparse Attention](https://doi.org/10.1145/3539618.3592073)|Shuyan Huang, Zitao Liu, Xiangyu Zhao, Weiqi Luo, Jian Weng|TAL Education Group, Beijing, China; City University of Hong Kong, Hong Kong, China; Jinan University, Guangzhou, China|Knowledge tracing (KT) is the problem of predicting students' future performance based on their historical interaction sequences. With the advanced capability of capturing contextual long-term dependency, attention mechanism becomes one of the essential components in many deep learning based KT (DLKT) models. In spite of the impressive performance achieved by these attentional DLKT models, many of them are often vulnerable to run the risk of overfitting, especially on small-scale educational datasets. Therefore, in this paper, we propose sparseKT, a simple yet effective framework to improve the robustness and generalization of the attention based DLKT approaches. Specifically, we incorporate a k-selection module to only pick items with the highest attention scores. We propose two sparsification heuristics: (1) soft-thresholding sparse attention and (2) top-K sparse attention. We show that our sparseKT is able to help attentional KT models get rid of irrelevant student interactions and improve the predictive performance when compared to 11 state-of-the-art KT models on three publicly available real-world educational datasets. To encourage reproducible research, we make our data and code publicly available at https://github.com/pykt-team/pykt-toolkit1..|知识追踪(KT)是根据学生的历史交互序列预测其未来表现的问题。注意机制是深度学习 KT (DLKT)模型的重要组成部分，具有捕捉上下文长期依赖的能力。尽管这些注意力 DLKT 模型取得了令人印象深刻的性能，但其中许多模型往往容易出现过度拟合的风险，尤其是在小规模的教育数据集上。因此，在本文中，我们提出了稀疏 KT，一个简单而有效的框架，以提高基于注意的 DLKT 方法的鲁棒性和泛化能力。具体来说，我们合并了一个 k 选择模块，只选择注意力得分最高的项目。提出了两种稀疏化启发式算法: (1)软阈值稀疏注意和(2) top-K 稀疏注意。我们表明，我们的稀疏 KT 能够帮助注意 KT 模型摆脱不相关的学生互动，并提高预测性能相比，在三个公开可用的现实世界教育数据集的11个国家的最先进的 KT 模型。为了鼓励可重复的研究，我们将我们的数据和代码在 https://github.com/pykt-team/pykt-toolkit1公开。.|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Robust+Knowledge+Tracing+Models+via+k-Sparse+Attention)|0|
|[Think Rationally about What You See: Continuous Rationale Extraction for Relation Extraction](https://doi.org/10.1145/3539618.3592072)|Xuming Hu, Zhaochen Hong, Chenwei Zhang, Irwin King, Philip S. Yu|Tsinghua University, Beijing, China; Amazon, Seattle, WA, USA; The Chinese University of Hong Kong, Hong Kong, Hong Kong; University of Illinois at Chicago, Chicago, IL, USA|Relation extraction (RE) aims to extract potential relations according to the context of two entities, thus, deriving rational contexts from sentences plays an important role. Previous works either focus on how to leverage the entity information (e.g., entity types, entity verbalization) to inference relations, but ignore context-focused content, or use counterfactual thinking to remove the model's bias of potential relations in entities, but the relation reasoning process will still be hindered by irrelevant content. Therefore, how to preserve relevant content and remove noisy segments from sentences is a crucial task. In addition, retained content needs to be fluent enough to maintain semantic coherence and interpretability. In this work, we propose a novel rationale extraction framework named RE2, which leverages two continuity and sparsity factors to obtain relevant and coherent rationales from sentences. To solve the problem that the gold rationales are not labeled, RE2 applies an optimizable binary mask to each token in the sentence, and adjust the rationales that need to be selected according to the relation label. Experiments on four datasets show that RE2 surpasses baselines.|关系抽取(RE)的目的是根据两个实体的语境提取潜在的关系，因此，从句子中抽取合理的语境起着重要作用。以往的研究主要集中在如何利用实体信息(如实体类型、实体语言化)来推理关系，而忽视了关注上下文的内容，或者运用反事实思维来消除模型对实体中潜在关系的偏见，但是关系推理过程仍然会受到不相关内容的阻碍。因此，如何在句子中保留相关内容和去除噪声片段是一个非常关键的问题。此外，保留的内容需要足够流畅，以保持语义连贯性和可解释性。在这项工作中，我们提出了一个新的理论基础抽取框架，称为 RE2，它利用两个连续性和稀疏性因素来获得相关和连贯的理论基础从句子。为了解决黄金基本原理没有标记的问题，RE2对句子中的每个标记应用一个可优化的二进制掩码，并根据关系标签调整需要选择的基本原理。在四个数据集上的实验表明，RE2超过了基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Think+Rationally+about+What+You+See:+Continuous+Rationale+Extraction+for+Relation+Extraction)|0|
|[TrustSGCN: Learning Trustworthiness on Edge Signs for Effective Signed Graph Convolutional Networks](https://doi.org/10.1145/3539618.3592075)|MinJeong Kim, YeonChang Lee, SangWook Kim|Hanyang University, Seoul, Republic of Korea; Georgia Institute of Technology, Atlanta, USA|The problem of signed network embedding (SNE) aims to represent nodes in a given signed network as low-dimensional vectors. While several SNE methods based on graph convolutional networks (GCN) have been proposed, we point out that they significantly rely on the assumption that the decades-old balance theory always holds in the real world. To address this limitation, we propose a novel GCN-based SNE approach, named as TrustSGCN, which measures the trustworthiness on edge signs for high-order relationships inferred by balance theory and corrects incorrect embedding propagation based on the trustworthiness. The experiments on four real-world signed network datasets demonstrate that TrustSGCN consistently outperforms five state-of-the-art GCN-based SNE methods. The code is available at https://github.com/kmj0792/TrustSGCN.|有符号网络嵌入(SNE)问题旨在将给定有符号网络中的节点表示为低维向量。提出了几种基于图卷积网络(GCN)的 SNE 方法，指出它们主要依赖于几十年前的平衡理论在现实世界中仍然适用的假设。针对这一局限性，提出了一种新的基于 GCN 的 SNE 方法 TrustSGCN，该方法通过平衡理论对高阶关系的边缘符号进行可信度度量，并基于可信度校正不正确的嵌入传播。在四个真实世界签名网络数据集上的实验表明，TrustSGCN 始终优于基于 GCN 的五种最新的 SNE 方法。密码可在 https://github.com/kmj0792/trustsgcn 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TrustSGCN:+Learning+Trustworthiness+on+Edge+Signs+for+Effective+Signed+Graph+Convolutional+Networks)|0|
|[Unsupervised Dialogue Topic Segmentation with Topic-aware Contrastive Learning](https://doi.org/10.1145/3539618.3592081)|Haoyu Gao, Rui Wang, TingEn Lin, Yuchuan Wu, Min Yang, Fei Huang, Yongbin Li|Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, ShenZhen, China; University of Science and Technology of China & Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, HeFei, China; Alibaba Group, Beijing, China; Alibaba Group, New York, China|Dialogue Topic Segmentation (DTS) plays an essential role in a variety of dialogue modeling tasks. Previous DTS methods either focus on semantic similarity or dialogue coherence to assess topic similarity for unsupervised dialogue segmentation. However, the topic similarity cannot be fully identified via semantic similarity or dialogue coherence. In addition, the unlabeled dialogue data, which contains useful clues of utterance relationships, remains underexploited. In this paper, we propose a novel unsupervised DTS framework, which learns topic-aware utterance representations from unlabeled dialogue data through neighboring utterance matching and pseudo-segmentation. Extensive experiments on two benchmark datasets (i.e., DialSeg711 and Doc2Dial) demonstrate that our method significantly outperforms the strong baseline methods. For reproducibility, we provide our code and data at: https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/dial-start.|对话主题分割(DTS)在各种对话建模任务中起着至关重要的作用。以往的 DTS 方法都是基于语义相似性或对话连贯性来评估无监督对话分割的主题相似性。然而，主题相似性不能通过语义相似性或对话连贯来完全识别。此外，未标记的对话数据，其中包含有用的话语关系的线索，仍然没有得到充分利用。本文提出了一种新的无监督 DTS 框架，它通过邻近话语匹配和伪分割从未标记的对话数据中学习主题感知的话语表征。对两个基准数据集(即，DialSeg711和 Doc2Dial)的大量实验表明，我们的方法明显优于强基准方法。为了可重复性，我们在以下 https://github.com/alibabaresearch/damo-convai/tree/main/dial-start 提供代码和数据。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unsupervised+Dialogue+Topic+Segmentation+with+Topic-aware+Contrastive+Learning)|0|
|[Which Matters Most in Making Fund Investment Decisions? A Multi-granularity Graph Disentangled Learning Framework](https://doi.org/10.1145/3539618.3592088)|Chunjing Gan, Binbin Hu, Bo Huang, Tianyu Zhao, Yingru Lin, Wenliang Zhong, Zhiqiang Zhang, Jun Zhou, Chuan Shi|Ant Group, Hangzhou, China; Beijing University of Posts and Telecommunications, Beijing, China; Ant Group, Shanghai, China; Ant Group, Beijing, China|In this paper, we highlight that both conformity and risk preference matter in making fund investment decisions beyond personal interest and seek to jointly characterize these aspects in a disentangled manner. Consequently, we develop a novel Multi-granularity Graph Disentangled Learning framework named MGDL to effectively perform intelligent matching of fund investment products. Benefiting from the well-established fund graph and the attention module, multi-granularity user representations are derived from historical behaviors to separately express personal interest, conformity and risk preference in a fine-grained way. To attain stronger disentangled representations with specific semantics, MGDL explicitly involve two self-supervised signals, ie fund type based contrasts and fund popularity. Extensive experiments in offline and online environments verify the effectiveness of MGDL.|在本文中，我们强调了一致性和风险偏好在基金投资决策中的重要性，超越了个人利益，并试图以一种分离的方式共同描述这些方面。因此，我们提出了一个新的多粒度图解缠学习框架 MGDL，以有效地进行基金投资产品的智能匹配。利用已建立的基金图和注意模型，从历史行为中分离出多粒度的用户表征，以细粒度的方式分别表达个人兴趣、整合和风险偏好。为了获得具有特定语义的更强的分离表征，MGDL 明确涉及两个自我监督信号，即基金类型对比和基金受欢迎程度。在离线和在线环境中的大量实验验证了 MGDL 的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Which+Matters+Most+in+Making+Fund+Investment+Decisions?+A+Multi-granularity+Graph+Disentangled+Learning+Framework)|0|
|[Where Does Your News Come From? Predicting Information Pathways in Social Media](https://doi.org/10.1145/3539618.3592087)|Alexander K. Taylor, Nuan Wen, PoNien Kung, Jiaao Chen, Violet Peng, Wei Wang|Georgia Tech, Atlanta, GA, USA; University of California, Los Angeles, Los Angeles, CA, USA; University Southern California, Los Angeles, CA, USA|As social networks become further entrenched in modern society, it becomes increasingly important to understand and predict how information (e.g., news coverage of a given event) is propagated across social media (i.e., information pathway), which helps the understandings of the impact of real-world information. Thus, in this paper, we propose a novel task, Information Pathway Prediction (IPP), which depicts the propagation paths of a given passage as a community tree (rooted at the information source) on constructed community interaction graphs where we first aggregate individual users into communities formed around news sources and influential users, and then elucidate the patterns of information dissemination across media based on such community nodes. We argue that this is an important and useful task because, on one hand, community-level interactions offer more stability than those at the user level; on the other hand, individual users are often influenced by their community, and modeling community-level information propagation will help the traditional link-prediction problem. To tackle the IPP task, we introduce Lightning, a novel content-aware link prediction GNN model and demonstrate using a large Twitter dataset consisting of all COVID related tweets that Lightning outperforms state-of-the-art link prediction baselines by a significant margin.|随着社交网络在现代社会中越来越根深蒂固，理解和预测信息(例如，特定事件的新闻报道)如何通过社交媒体(例如，信息路径)传播变得越来越重要，这有助于理解现实世界信息的影响。因此，我们提出了一个新的任务，信息路径预测(IPP) ，它描述了一个给定的文章的传播路径作为一个社区树(根源于信息来源)在构建的社区互动图中，首先聚集个人用户周围新闻来源和有影响力的用户形成的社区，然后阐明基于这些社区节点的跨媒体信息传播模式。我们认为这是一个重要而有用的任务，因为一方面，社区层面的交互比用户层面的交互提供了更多的稳定性; 另一方面，个人用户经常受到他们的社区的影响，建模社区层面的信息传播将有助于传统的链接预测问题。为了解决 IPP 任务，我们引入了“闪电”，一个新颖的内容感知链接预测 GNN 模型，并使用一个包含所有与冠状病毒疾病相关的 tweet 的大型 Twitter 数据集演示了“闪电”比最先进的链接预测基线表现出了显著的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Where+Does+Your+News+Come+From?+Predicting+Information+Pathways+in+Social+Media)|0|
|[Automated Medical Coding on MIMIC-III and MIMIC-IV: A Critical Review and Replicability Study](https://doi.org/10.1145/3539618.3591918)|Joakim Edin, Alexander Junge, Jakob D. Havtorn, Lasse Borgholt, Maria Maistro, Tuukka Ruotsalo, Lars Maaløe|University of Copenhagen & University of Helsinki, Copenhagen, Denmark; Technical University of Denmark & Corti, Copenhagen, Denmark; University of Aalborg & Corti, Copenhagen, Denmark; University of Copenhagen, Copenhagen, Denmark; Corti, Copenhagen, Denmark; University of Copenhagen & Corti, Copenhagen, Denmark|Medical coding is the task of assigning medical codes to clinical free-text documentation. Healthcare professionals manually assign such codes to track patient diagnoses and treatments. Automated medical coding can considerably alleviate this administrative burden. In this paper, we reproduce, compare, and analyze state-of-the-art automated medical coding machine learning models. We show that several models underperform due to weak configurations, poorly sampled train-test splits, and insufficient evaluation. In previous work, the macro F1 score has been calculated sub-optimally, and our correction doubles it. We contribute a revised model comparison using stratified sampling and identical experimental setups, including hyperparameters and decision boundary tuning. We analyze prediction errors to validate and falsify assumptions of previous works. The analysis confirms that all models struggle with rare codes, while long documents only have a negligible impact. Finally, we present the first comprehensive results on the newly released MIMIC-IV dataset using the reproduced models. We release our code, model parameters, and new MIMIC-III and MIMIC-IV training and evaluation pipelines to accommodate fair future comparisons.|医学编码是为临床自由文本文档分配医学编码的任务。医疗保健专业人员手动指定这样的代码来跟踪患者的诊断和治疗。自动化医疗编码可以大大减轻这种管理负担。在本文中，我们再现，比较和分析国家的最先进的自动医疗编码机学习模型。我们表明，几个模型表现不佳，由于配置薄弱，采样列车测试分裂，和不充分的评估。在以前的工作中，宏 F1得分已经计算次优，我们的修正是它的两倍。我们使用分层抽样和相同的实验装置，包括超参数和决策边界调整，提供了一个修订后的模型比较。我们分析预测误差，以验证和证伪以往工作的假设。分析证实，所有的模型都与罕见的代码作斗争，而长文档只有微不足道的影响。最后，我们使用复制的模型对新发布的 MIMIC-IV 数据集提出了第一个全面的结果。我们发布我们的代码，模型参数，以及新的 MIMIC-III 和 MIMIC-IV 培训和评估管道，以适应公平的未来比较。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automated+Medical+Coding+on+MIMIC-III+and+MIMIC-IV:+A+Critical+Review+and+Replicability+Study)|0|
|[An Empirical Comparison of Web Content Extraction Algorithms](https://doi.org/10.1145/3539618.3591920)|Janek Bevendorff, Sanket Gupta, Johannes Kiesel, Benno Stein|Bauhaus-Universität Weimar, Weimar, Germany|Main content extraction from web pages-sometimes also called boilerplate removal-has been a research topic for over two decades. Yet despite web pages being delivered in a machine-readable markup format, extracting the actual content is still a challenge today. Even with the latest HTML5 standard, which defines many semantic elements to mark content areas, web page authors do not always use semantic markup correctly or to its full potential, making it hard for automated systems to extract the relevant information. A high-precision, high-recall content extraction is crucial for downstream applications such as search engines, AI language tools, distraction-free reader modes in users' browsers, and other general assistive technologies. For such a fundamental task, however, surprisingly few openly available extraction systems or training and benchmarking datasets exist. Even less research has gone into the rigorous evaluation and a true apples-to-apples comparison of the few extraction systems that do exist. To get a better grasp on the current state of the art in the field, we combine and clean eight existing human-labeled web content extraction datasets. On the combined dataset, we evaluate 14~competitive main content extraction systems and five baseline approaches. Finally, we build three ensembles as new state-of-the-art extraction baselines. We find that the performance of existing systems is quite genre-dependent and no single extractor performs best on all types of web pages.|从网页中提取主要内容——有时也称为样板删除——是二十多年来的一个研究课题。然而，尽管网页是以机器可读的标记格式发布的，提取实际内容在今天仍然是一个挑战。即使使用最新的 HTML5标准，定义了许多语义元素来标记内容区域，网页作者并不总是正确地使用语义标记或充分发挥其潜力，这使得自动化系统很难提取相关信息。一个高精度，高召回率的内容提取是至关重要的下游应用程序，如搜索引擎，人工智能语言工具，无干扰阅读模式在用户的浏览器，以及其他通用的辅助技术。然而，对于这样一个基本任务，令人惊讶的是，很少有公开可用的提取系统或培训和基准测试数据集存在。甚至更少的研究进入了严格的评估和一个真正的苹果对苹果的比较，几个提取系统是存在的。为了更好地掌握该领域的现状，我们组合并清理了8个现有的人类标记的 Web 内容提取数据集。在组合数据集上，我们评估了14种具有竞争力的主要内容提取系统和5种基线方法。最后，我们构建三个集合作为新的最先进的提取基线。我们发现，现有系统的性能是相当类型依赖，没有一个单一的提取器表现最好的所有类型的网页。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Empirical+Comparison+of+Web+Content+Extraction+Algorithms)|0|
|[Multimodal Neural Databases](https://doi.org/10.1145/3539618.3591930)|Giovanni Trappolini, Andrea Santilli, Emanuele Rodolà, Alon Y. Halevy, Fabrizio Silvestri|Sapienza University & ISTI-CNR, Rome, Italy; Meta AI, Menlo Park, CA, USA; Sapienza University, Rome, Italy|The rise in loosely-structured data available through text, images, and other modalities has called for new ways of querying them. Multimedia Information Retrieval has filled this gap and has witnessed exciting progress in recent years. Tasks such as search and retrieval of extensive multimedia archives have undergone massive performance improvements, driven to a large extent by recent developments in multimodal deep learning. However, methods in this field remain limited in the kinds of queries they support and, in particular, their inability to answer database-like queries. For this reason, inspired by recent work on neural databases, we propose a new framework, which we name Multimodal Neural Databases (MMNDBs). MMNDBs can answer complex database-like queries that involve reasoning over different input modalities, such as text and images, at scale. In this paper, we present the first architecture able to fulfill this set of requirements and test it with several baselines, showing the limitations of currently available models. The results show the potential of these new techniques to process unstructured data coming from different modalities, paving the way for future research in the area. Code to replicate the experiments will be released at https://github.com/GiovanniTRA/MultimodalNeuralDatabases|随着通过文本、图像和其他方式获得的松散结构数据的增加，需要用新的方式来查询这些数据。多媒体信息检索填补了这一空白，近年来取得了令人兴奋的进展。搜索和检索大量多媒体档案等任务在很大程度上受到多模式深度学习的最新发展的推动，已经取得了巨大的性能改进。但是，这个领域的方法仍然受到它们支持的查询类型的限制，特别是它们无法回答类似数据库的查询。基于这个原因，受最近神经数据库研究的启发，我们提出了一个新的框架，命名为多模态神经数据库(MMNDB)。MMNDB 可以回答复杂的数据库查询，这些查询涉及对不同输入模式(如文本和图像)进行大规模推理。在本文中，我们提出了第一个能够满足这组需求的体系结构，并用几个基线对其进行了测试，显示了当前可用模型的局限性。研究结果显示了这些新技术处理来自不同模式的非结构化数据的潜力，为该领域未来的研究铺平了道路。复制实验的代码将在 https://github.com/giovannitra/multimodalneuraldatabases 公布|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multimodal+Neural+Databases)|0|
|[SocialDial: A Benchmark for Socially-Aware Dialogue Systems](https://doi.org/10.1145/3539618.3591877)|Haolan Zhan, Zhuang Li, Yufei Wang, Linhao Luo, Tao Feng, Xiaoxi Kang, Yuncheng Hua, Lizhen Qu, LayKi Soon, Suraj Sharma, Ingrid Zukerman, Zhaleh SemnaniAzad, Gholamreza Haffari|Monash University, Subang Jaya, Malaysia; California State University, Northridge, Northridge, CA, USA; Monash University, Melbourne, VIC, Australia|Dialogue systems have been widely applied in many scenarios and are now more powerful and ubiquitous than ever before. With large neural models and massive available data, current dialogue systems have access to more knowledge than any people in their life. However, current dialogue systems still do not perform at a human level. One major gap between conversational agents and humans lies in their abilities to be aware of social norms. The development of socially-aware dialogue systems is impeded due to the lack of resources. In this paper, we present the first socially-aware dialogue corpus - SocialDial, based on Chinese social culture. SocialDial consists of two parts: 1,563 multi-turn dialogues between two human speakers with fine-grained labels, and 4,870 synthetic conversations generated by ChatGPT. The human corpus covers five categories of social norms, which have 14 sub-categories in total. Specifically, it contains social factor annotations including social relation, context, social distance, and social norms. However, collecting sufficient socially-aware dialogues is costly. Thus, we harness the power of ChatGPT and devise an ontology-based synthetic data generation framework. This framework is able to generate synthetic data at scale. To ensure the quality of synthetic dialogues, we design several mechanisms for quality control during data collection. Finally, we evaluate our dataset using several pre-trained models, such as BERT and RoBERTa. Comprehensive empirical results based on state-of-the-art neural models demonstrate that modeling of social norms for dialogue systems is a promising research direction. To the best of our knowledge, SocialDial is the first socially-aware dialogue dataset that covers multiple social factors and has fine-grained labels.|对话系统在许多情况下得到了广泛应用，现在比以往任何时候都更加强大和普遍。通过大型神经模型和大量可用数据，目前的对话系统比他们生活中的任何人都能获得更多的知识。然而，目前的对话系统仍然不能在人类的水平上运行。会话主体和人类之间的一个主要差距在于他们意识到社会规范的能力。由于缺乏资源，社会意识对话系统的发展受到阻碍。本文以中国社会文化为基础，提出了第一个具有社会意识的对话语料库——社会拨号。SocialDial 由两部分组成: 1,563个具有细粒度标签的人类说话者之间的多回合对话，以及由 ChatGPT 生成的4,870个合成对话。人类主体包括五类社会规范，共有14个子类。具体来说，它包含了社会因素的诠释，包括社会关系、语境、社会距离和社会规范。然而，收集足够的具有社会意识的对话是昂贵的。因此，我们利用 ChatGPT 的功能，设计了一个基于本体的综合数据生成框架。这个框架能够生成大规模的综合数据。为了保证综合对话的质量，我们设计了数据采集过程中的质量控制机制。最后，我们使用一些预先训练的模型来评估我们的数据集，例如 BERT 和 RoBERTa。基于最新神经模型的综合实证结果表明，对话系统的社会规范建模是一个有前途的研究方向。据我们所知，SocialDial 是第一个具有社会意识的对话数据集，它涵盖了多种社会因素，并有细粒度的标签。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SocialDial:+A+Benchmark+for+Socially-Aware+Dialogue+Systems)|0|
|[End-to-End Multimodal Fact-Checking and Explanation Generation: A Challenging Dataset and Models](https://doi.org/10.1145/3539618.3591879)|Barry Menglong Yao, Aditya Shah, Lichao Sun, JinHee Cho, Lifu Huang|Virginia Tech, Blacksburg, VA, USA; Lehigh University, Bethlehem, PA, USA|We propose the end-to-end multimodal fact-checking and explanation generation, where the input is a claim and a large collection of web sources, including articles, images, videos, and tweets, and the goal is to assess the truthfulness of the claim by retrieving relevant evidence and predicting a truthfulness label (i.e., support, refute and not enough information), and generate a rationalization statement to explain the reasoning and ruling process. To support this research, we construct Mocheg, a large-scale dataset that consists of 21,184 claims where each claim is assigned with a truthfulness label and ruling statement, with 58,523 evidence in the form of text and images. To establish baseline performances on Mocheg, we experiment with several state-of-the-art neural architectures on the three pipelined subtasks: multimodal evidence retrieval, claim verification, and explanation generation, and demonstrate the current state-of-the-art performance of end-to-end multimodal fact-checking is still far from satisfying. To the best of our knowledge, we are the first to build the benchmark dataset and solutions for end-to-end multimodal fact-checking and justification.|我们提出端到端的多模态事实检查和解释生成，其中输入是一个声明和大量的网络来源，包括文章，图像，视频和 tweet，目标是通过检索相关证据和预测真实性标签(即，支持，反驳和不足的信息)来评估声明的真实性，并生成合理化陈述来解释推理和裁决过程。为了支持这项研究，我们构建了 Mocheg，一个包含21,184个索赔的大型数据集，其中每个索赔都被分配了一个真实性标签和裁决声明，以文本和图像的形式提供了58,523个证据。为了建立 Mocheg 的基准表现，我们在三个流水线子任务(多模态证据检索、索赔验证和解释生成)上试验了几种最先进的神经结构，并证明目前端到端多模态事实检查的最先进表现仍然远远不能令人满意。据我们所知，我们是第一个建立基准数据集和解决方案的端到端多模式事实检查和论证。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=End-to-End+Multimodal+Fact-Checking+and+Explanation+Generation:+A+Challenging+Dataset+and+Models)|0|
|[The JOKER Corpus: English-French Parallel Data for Multilingual Wordplay Recognition](https://doi.org/10.1145/3539618.3591885)|Liana Ermakova, AnneGwenn Bosser, Adam Jatowt, Tristan Miller|Austrian Research Institute for Artificial Intelligence, Vienna, Austria; Université de Bretagne Occidentale, Brest, France; University of Innsbruck, Innsbruck, Austria; École Nationale d'Ingénieurs de Brest, Plouzané, France|Despite recent advances in information retrieval and natural language processing, rhetorical devices that exploit ambiguity or subvert linguistic rules remain a challenge for such systems. However, corpus-based analysis of wordplay has been a perennial topic of scholarship in the humanities, including literary criticism, language education, and translation studies. The immense data-gathering effort required for these studies points to the need for specialized text retrieval and classification technology, and consequently for appropriate test collections. In this paper, we introduce and analyze a new dataset for research and applications in the retrieval and processing of wordplay. Developed for the JOKER track at CLEF 2023, our annotated corpus extends and improves upon past English wordplay detection datasets in several ways. First, we introduce hundreds of additional positive examples of wordplay; second, we provide French translations for the examples; and third, we provide negative examples of non-wordplay with characteristics closely matching those of the positive examples. This last feature helps ensure that AI models learn to effectively distinguish wordplay from non-wordplay, and not simply texts differing in length, style, or vocabulary. Our test collection represents then a step towards wordplay-aware multilingual information retrieval.|尽管最近在信息检索和自然语言处理方面取得了进展，但利用歧义或颠覆语言规则的修辞手段仍然是这类系统面临的一个挑战。然而，基于语料库的文字游戏分析一直是人文学科研究的一个长期课题，包括文学批评、语言教育和翻译研究。这些研究需要大量的数据收集工作，这表明需要专门的文本检索和分类技术，因此需要适当的测试收集。本文介绍并分析了一个新的数据集，以便在文字游戏的检索和处理中进行研究和应用。在 CLEF 2023年为 JOKER 轨道开发，我们的注释语料库在几个方面扩展和改进了过去的英语文字游戏检测数据集。首先，我们介绍了数以百计的文字游戏的正面例子; 其次，我们提供了法语翻译的例子; 第三，我们提供了非文字游戏的负面例子，其特点与正面例子非常相似。最后一个特性有助于确保人工智能模型学会有效地区分文字游戏和非文字游戏，而不仅仅是文本的长度、风格或词汇不同。我们的测试集代表着向着支持文字游戏的多语言信息检索迈进了一步。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+JOKER+Corpus:+English-French+Parallel+Data+for+Multilingual+Wordplay+Recognition)|0|
|[Form-NLU: Dataset for the Form Natural Language Understanding](https://doi.org/10.1145/3539618.3591886)|Yihao Ding, Siqu Long, Jiabin Huang, Kaixuan Ren, Xingxiang Luo, Hyunsuk Chung, Soyeon Caren Han|The University of Western Australia, Perth, WA, Australia; The University of Sydney, Sydney, NSW, Australia; FortifyEdge, Sydney, NSW, Australia|Compared to general document analysis tasks, form document structure understanding and retrieval are challenging. Form documents are typically made by two types of authors; A form designer, who develops the form structure and keys, and a form user, who fills out form values based on the provided keys. Hence, the form values may not be aligned with the form designer's intention (structure and keys) if a form user gets confused. In this paper, we introduce Form-NLU, the first novel dataset for form structure understanding and its key and value information extraction, interpreting the form designer's intent and the alignment of user-written value on it. It consists of 857 form images, 6k form keys and values, and 4k table keys and values. Our dataset also includes three form types: digital, printed, and handwritten, which cover diverse form appearances and layouts. We propose a robust positional and logical relation-based form key-value information extraction framework. Using this dataset, Form-NLU, we first examine strong object detection models for the form layout understanding, then evaluate the key information extraction task on the dataset, providing fine-grained results for different types of forms and keys. Furthermore, we examine it with the off-the-shelf pdf layout extraction tool and prove its feasibility in real-world cases.|与一般的文档分析任务相比，表单文档结构的理解和检索具有挑战性。表单文档通常由两种类型的作者创建: 一种是开发表单结构和键的表单设计人员，另一种是根据提供的键填写表单值的表单用户。因此，如果表单用户弄混了，表单值可能与表单设计器的意图(结构和键)不一致。在本文中，我们将介绍第一个用于表单结构理解的新型数据集 Form-NLU 及其关键和价值信息抽取，解释表单设计者的意图以及用户书面价值的对齐。它由857个表单映像、6k 个表单键和值以及4k 个表单键和值组成。我们的数据集还包括三种形式类型: 数字、印刷和手写，涵盖了不同形式的外观和布局。我们提出了一个健壮的基于位置和逻辑关系的表单键值信息抽取框架。使用这个数据集 Form-nlU，我们首先检查表单布局理解的强目标检测模型，然后评估数据集上的关键信息抽取任务，为不同类型的表单和关键字提供细粒度的结果。此外，我们还利用现有的 pdf 版面抽取工具对其进行了检验，并在实际案例中证明了其可行性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Form-NLU:+Dataset+for+the+Form+Natural+Language+Understanding)|0|
|[MMEAD: MS MARCO Entity Annotations and Disambiguations](https://doi.org/10.1145/3539618.3591887)|Chris Kamphuis, Aileen Lin, Siwen Yang, Jimmy Lin, Arjen P. de Vries, Faegheh Hasibi|Radboud University, Nijmegen, Netherlands; University of Waterloo, Waterloo, Canada|MMEAD, or MS MARCO Entity Annotations and Disambiguations, is a resource for entity links for the MS MARCO datasets. We specify a format to store and share links for both document and passage collections of MS MARCO. Following this specification, we release entity links to Wikipedia for documents and passages in both MS MARCO collections (v1 and v2). Entity links have been produced by the REL and BLINK systems. MMEAD is an easy-to-install Python package, allowing users to load the link data and entity embeddings effortlessly. Using MMEAD takes only a few lines of code. Finally, we show how MMEAD can be used for IR research that uses entity information. We show how to improve recall@1000 and MRR@10 on more complex queries on the MS MARCO v1 passage dataset by using this resource. We also demonstrate how entity expansions can be used for interactive search applications.|MMEAD，或 MS MARCO 实体注释和消除歧义，是一个为 MS MARCO 数据集实体链接的资源。我们指定了一个格式，以存储和共享的文件和文章的微软 MARCO 集合链接。遵循此规范，我们发布到 Wikipedia 的实体链接，以获取 MS MARCO 集合(v1和 v2)中的文档和段落。实体链接已由 REL 和 BLINK 系统生成。MMEAD 是一个易于安装的 Python 包，允许用户毫不费力地加载链接数据和实体嵌入。使用 MMEAD 只需要几行代码。最后，我们展示了 MMEAD 如何用于使用实体信息的 IR 研究。我们展示了如何通过使用这个资源来提高对 MS MARCO v1通道数据集的更复杂查询的召回@1000和 MRR@10。我们还演示了如何将实体扩展用于交互式搜索应用程序。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MMEAD:+MS+MARCO+Entity+Annotations+and+Disambiguations)|0|
|[GammaGL: A Multi-Backend Library for Graph Neural Networks](https://doi.org/10.1145/3539618.3591891)|Yaoqi Liu, Cheng Yang, Tianyu Zhao, Hui Han, Siyuan Zhang, Jing Wu, Guangyu Zhou, Hai Huang, Hui Wang, Chuan Shi|Beijing University of Posts and Telecommunications & Peng Cheng Laboratory, Beijing, Shenzhen, China; Beijing University of Posts and Telecommunications, Beijing, China; Beijing University of Posts and Telecommunications & Peng Cheng Laboratory, Beijing, shenzhen, China; Peng Cheng Laboratory, Shenzhen, China|Graph Neural Networks (GNNs) have shown their superiority in modeling graph-structured data, and gained much attention over the last five years. Though traditional deep learning frameworks such as TensorFlow and PyTorch provide convenient tools for implementing neural network algorithms, they do not support the key operations of GNNs well, e.g., the message passing computation based on sparse matrices. To address this issue, GNN libraries such as PyG are proposed by introducing rich Application Programming Interfaces (APIs) specialized for GNNs. However, most current GNN libraries only support a specific deep learning framework as the backend, e.g., PyG is tied up with PyTorch. In practice, users usually need to combine GNNs with other neural network components, which may come from their co-workers or open-source codes with different deep-learning backends. Consequently, users have to be familiar with various GNN libraries, and rewrite their GNNs with corresponding APIs. To provide a more convenient user experience, we present Gamma Graph Library (GammaGL), a GNN library that supports multiple deep learning frameworks as backends. GammaGL uses a framework-agnostic design that allows users to easily switch between deep learning backends on top of existing components with a single line of code change. Following the tensor-centric design idea, GammaGL splits the graph data into several key tensors, and abstracts GNN computational processes (such as message passing and graph mini-batch operations) into a few key functions. We develop many efficient operators in GammaGL for acceleration. So far, GammaGL has provided more than 40 GNN examples that can be applied to a variety of downstream tasks. GammaGL also provides tools for heterogeneous graph neural networks and recommendations to facilitate research in related fields. We present the performance of models implemented by GammaGL and the time consumption of our optimized operators to show the efficiency. Our library is available at https://github.com/BUPT-GAMMA/GammaGL.|图形神经网络(GNN)在建立图形结构数据模型方面显示出其优越性，近五年来受到了广泛的关注。尽管传统的深度学习框架如 TensorFlow 和 PyTorch 为神经网络算法的实现提供了方便的工具，但它们并不能很好地支持 GNN 的关键操作，例如基于稀疏矩阵的消息传递计算。为了解决这个问题，通过引入专门用于 GNN 的丰富的应用程序编程接口(API) ，提出了像 PyG 这样的 GNN 库。然而，大多数当前的 GNN 库只支持特定的深度学习框架作为后端，例如，PyG 与 PyTorch 绑定在一起。在实践中，用户通常需要将 GNN 与其他神经网络组件结合起来，这些组件可能来自他们的同事或具有不同深度学习后端的开源代码。因此，用户必须熟悉各种 GNN 库，并用相应的 API 重写 GNN。为了提供更方便的用户体验，我们介绍了 GammaGraph Library (GammaGL) ，一个支持多个深度学习框架作为后端的 GNN 库。GammaGL 使用了一种与框架无关的设计，允许用户通过一行代码更改就可以轻松地在现有组件上的深度学习后端之间进行切换。遵循以张量为中心的设计思想，GammaGL 将图形数据分解为几个关键张量，并将 GNN 计算过程(如消息传递和图形小批处理操作)抽象为几个关键函数。我们在 GammaGL 中开发了许多有效的加速算子。到目前为止，GammaGL 已经提供了40多个 GNN 示例，可以应用于各种下游任务。GammaGL 还为异构图形神经网络提供了工具，并为相关领域的研究提供了建议。我们展示了由 GammaGL 实现的模型的性能和我们的优化算子的时间消耗，以显示效率。我们的图书馆 https://github.com/bupt-gamma/gammagl 有售。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GammaGL:+A+Multi-Backend+Library+for+Graph+Neural+Networks)|0|
|[tieval: An Evaluation Framework for Temporal Information Extraction Systems](https://doi.org/10.1145/3539618.3591892)|Hugo Sousa, Ricardo Campos, Alípio Mário Jorge|INESC TEC & University of Porto, Porto, Portugal; |Temporal information extraction (TIE) has attracted a great deal of interest over the last two decades, leading to the development of a significant number of datasets. Despite its benefits, having access to a large volume of corpora makes it difficult when it comes to benchmark TIE systems. On the one hand, different datasets have different annotation schemes, thus hindering the comparison between competitors across different corpora. On the other hand, the fact that each corpus is commonly disseminated in a different format requires a considerable engineering effort for a researcher/practitioner to develop parsers for all of them. This constraint forces researchers to select a limited amount of datasets to evaluate their systems which consequently limits the comparability of the systems. Yet another obstacle that hinders the comparability of the TIE systems is the evaluation metric employed. While most research works adopt traditional metrics such as precision, recall, and $F_1$, a few others prefer temporal awareness -- a metric tailored to be more comprehensive on the evaluation of temporal systems. Although the reason for the absence of temporal awareness in the evaluation of most systems is not clear, one of the factors that certainly weights this decision is the necessity to implement the temporal closure algorithm in order to compute temporal awareness, which is not straightforward to implement neither is currently easily available. All in all, these problems have limited the fair comparison between approaches and consequently, the development of temporal extraction systems. To mitigate these problems, we have developed tieval, a Python library that provides a concise interface for importing different corpora and facilitates system evaluation. In this paper, we present the first public release of tieval and highlight its most relevant features.|时间信息抽取(TIE)在过去的二十年中引起了人们的极大兴趣，导致了大量数据集的发展。尽管有这些好处，但是当涉及到基准 TIE 系统时，访问大量的语料库会变得困难。一方面，不同的数据集有不同的注释方案，从而阻碍了不同语料库的竞争对手之间的比较。另一方面，由于每个语料库通常以不同的格式传播，因此研究人员/从业人员需要付出大量的工程努力，为所有语料库开发解析器。这种约束迫使研究人员选择有限数量的数据集来评估他们的系统，从而限制了系统的可比性。妨碍 TIE 系统可比性的另一个障碍是所使用的评价标准。虽然大多数研究工作采用传统的度量方法，如精确度、召回率和 $F _ 1 $，但也有一些研究工作更喜欢时间感知——一种更全面地评估时间系统的度量方法。尽管在大多数系统的评估中缺乏时间感知的原因尚不清楚，但是确定这一决策权重的因素之一是为了计算时间感知而实现时间闭包算法的必要性，这种算法不容易实现，目前也不容易获得。总之，这些问题限制了各种方法之间的公平比较，从而限制了时间提取系统的发展。为了缓解这些问题，我们开发了 tieval，这是一个 Python 库，它为导入不同的语料库提供了一个简洁的接口，并有助于系统评估。在本文中，我们提出了第一次公开发布的时间和突出其最相关的特点。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=tieval:+An+Evaluation+Framework+for+Temporal+Information+Extraction+Systems)|0|
|[HC3: A Suite of Test Collections for CLIR Evaluation over Informal Text](https://doi.org/10.1145/3539618.3591893)|Dawn J. Lawrie, James Mayfield, Douglas W. Oard, Eugene Yang, Suraj Nair, Petra Galuscáková|Johns Hopkins University, Baltimore, MD, USA; University of Maryland, College Park, MD, USA; University of Maryland, College Park, MD, France; Université Grenoble Alpes, Grenoble, France|While there are many test collections for Cross-Language Information Retrieval (CLIR), none of the large public test collections focus on short informal text documents. This paper introduces a new pair of CLIR test collections with millions of Chinese or Persian Tweets or Tweet threads as documents, sixty event-motivated topics written both in English and in each of the two document languages, and three-point graded relevance judgments constructed using interactive search and active learning. The design and construction of these new test collections are described, and baseline results are presented that demonstrate the utility of the collections for system evaluation. Shallow pooling is used to assess the efficacy of active learning to select documents for judgment.|虽然有很多跨语检索测试集合(CLIR) ，但是没有一个大型的公共测试集合关注于简短的非正式文本文档。本文介绍了一组新的 CLIR 测试集，其中包括数百万条中文或波斯语推文或推文，六十个以事件为动机的英文和两种文档语言的主题，以及使用交互式搜索和主动学习构建的三点分级相关判断。描述了这些新的测试集合的设计和构造，并给出了基线结果，说明了这些集合用于系统评估的效用。浅池用于评估主动学习选择判断文档的效力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HC3:+A+Suite+of+Test+Collections+for+CLIR+Evaluation+over+Informal+Text)|0|
|[BioSift: A Dataset for Filtering Biomedical Abstracts for Drug Repurposing and Clinical Meta-Analysis](https://doi.org/10.1145/3539618.3591897)|David Kartchner, Irfan AlHussaini, Haydn Turner, Jennifer Deng, Shubham Lohiya, Prasanth Bathala, Cassie S. Mitchell|Georgia Institute of Technology, Atlanta, GA, USA|This work presents a new, original document classification dataset, BioSift, to expedite the initial selection and labeling of studies for drug repurposing. The dataset consists of 10,000 human-annotated abstracts from scientific articles in PubMed. Each abstract is labeled with up to eight attributes necessary to perform meta-analysis utilizing the popular patient-intervention-comparator-outcome (PICO) method: has human subjects, is clinical trial/cohort, has population size, has target disease, has study drug, has comparator group, has a quantitative outcome, and an "aggregate" label. Each abstract was annotated by 3 different annotators (i.e., biomedical students) and randomly sampled abstracts were reviewed by senior annotators to ensure quality. Data statistics such as reviewer agreement, label co-occurrence, and confidence are shown. Robust benchmark results illustrate neither PubMed advanced filters nor state-of-the-art document classification schemes (e.g., active learning, weak supervision, full supervision) can efficiently replace human annotation. In short, BioSift is a pivotal but challenging document classification task to expedite drug repurposing. The full annotated dataset is publicly available and enables research development of algorithms for document classification that enhance drug repurposing.|这项工作提出了一个新的，原始的文档分类数据集，生物筛选，以加快研究的初步选择和标签药物再利用。这个数据集包含 PubMed 上10,000篇科学文章的人工注释摘要。每个摘要被标记为使用流行的患者-干预-比较-结果(PICO)方法进行荟萃分析所必需的多达8个属性: 具有人类受试者，是临床试验/队列，具有人口大小，具有目标疾病，具有研究药物，具有比较组，具有定量结果和“聚合”标签。每个摘要由3个不同的注释者(即生物医学学生)进行注释，并由高级注释者随机抽取摘要进行审查，以确保质量。数据统计，例如审查者协议，标签共现，和置信度显示。强大的基准测试结果表明，PubMed 高级过滤器和最先进的文档分类计划(例如，主动学习、弱监督、全面监督)都不能有效地取代人工注释。简而言之，BioSift 是加速药物再利用的关键但具有挑战性的文档分类任务。完整的注释数据集是公开的，可以用来研究开发文档分类的算法，从而加强药物再利用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BioSift:+A+Dataset+for+Filtering+Biomedical+Abstracts+for+Drug+Repurposing+and+Clinical+Meta-Analysis)|0|
|[MoocRadar: A Fine-grained and Multi-aspect Knowledge Repository for Improving Cognitive Student Modeling in MOOCs](https://doi.org/10.1145/3539618.3591898)|Jifan Yu, Mengying Lu, Qingyang Zhong, Zijun Yao, Shangqing Tu, Zhengshan Liao, Xiaoya Li, Manli Li, Lei Hou, HaiTao Zheng, Juanzi Li, Jie Tang|Tsinghua University, Beijing, China; Tsinghua University, Shenzhen, China|Student modeling, the task of inferring a student's learning characteristics through their interactions with coursework, is a fundamental issue in intelligent education. Although the recent attempts from knowledge tracing and cognitive diagnosis propose several promising directions for improving the usability and effectiveness of current models, the existing public datasets are still insufficient to meet the need for these potential solutions due to their ignorance of complete exercising contexts, fine-grained concepts, and cognitive labels. In this paper, we present MoocRadar, a fine-grained, multi-aspect knowledge repository consisting of 2,513 exercise questions, 5,600 knowledge concepts, and over 12 million behavioral records. Specifically, we propose a framework to guarantee a high-quality and comprehensive annotation of fine-grained concepts and cognitive labels. The statistical and experimental results indicate that our dataset provides the basis for the future improvements of existing methods. Moreover, to support the convenient usage for researchers, we release a set of tools for data querying, model adaption, and even the extension of our repository, which are now available at https://github.com/THU-KEG/MOOC-Radar.|学生建模是智能教育中的一个基本问题，它是通过学生与课程的互动来推断学生学习特点的任务。尽管最近来自知识跟踪和认知诊断的尝试为改善当前模型的可用性和有效性提出了几个有希望的方向，但现有的公共数据集仍然不足以满足这些潜在解决方案的需要，因为它们对完整的锻炼背景，细粒度的概念和认知标签的无知。在本文中，我们介绍了 MoocRadar，它是一个细粒度的、多方面的知识库，由2,513个练习题、5,600个知识概念和超过1200万个行为记录组成。具体来说，我们提出了一个框架，以保证高质量和全面的细粒度概念和认知标签的注释。统计和实验结果表明，我们的数据集为未来现有方法的改进提供了基础。此外，为了支持研究人员方便的使用，我们发布了一系列工具，用于数据查询、模型适应，甚至扩展我们的知识库，这些工具现在都可以在 https://github.com/thu-keg/mooc-radar 上使用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MoocRadar:+A+Fine-grained+and+Multi-aspect+Knowledge+Repository+for+Improving+Cognitive+Student+Modeling+in+MOOCs)|0|
|[DICE: a Dataset of Italian Crime Event news](https://doi.org/10.1145/3539618.3591904)|Giovanni Bonisoli, Maria Pia di Buono, Laura Po, Federica Rollo|University of Napoli, Napoli, Italy; University of Modena and Reggio Emilia, Modena, Italy|Extracting events from news stories as the aim of several Natural Language Processing (NLP) applications (e.g., question answering, news recommendation, news summarization) is not a trivial task, due to the complexity of natural language and the fact that news reporting is characterized by journalistic style and norms. Those aspects entail scattering an event description over several sentences within one document (or more documents), applying a mechanism of gradual specification of event-related information. This implies a widespread use of co-reference relations among the textual elements, conveying non-linear temporal information. In addition to this, despite the achievement of state-of-the-art results in several tasks, high-quality training datasets for non-English languages are rarely available. This paper presents our preliminary study to develop an annotated Dataset for Italian Crime Event news (DICE). The contribution of the paper are: (1) the creation of a corpus of 10,395 crime news; (2) the annotation schema; (3) a dataset of 10,395 news with automatic annotations; (4) a preliminary manual annotation using the proposed schema of 1000 documents. The first tests on DICE have compared the performance of a manual annotator with that of single-span and multi-span question answering models and shown there is still a gap in the models, especially when dealing with more complex annotation tasks and limited training data. This underscores the importance of investing in the creation of high-quality annotated datasets like DICE, which can provide a solid foundation for training and testing a wide range of NLP models.|从新闻故事中提取事件作为几个自然语言处理(nLP)应用程序的目标(例如，问题回答，新闻推荐，新闻摘要)并不是一项微不足道的任务，因为自然语言的复杂性以及新闻报道是拥有属性的新闻风格和规范。这些方面需要将事件描述分散在一个文档(或多个文档)中的几个句子中，应用逐步规范事件相关信息的机制。这意味着文本成分之间广泛使用共指关系，传递非线性时间信息。除此之外，尽管在几项任务中取得了最先进的成果，但是很少有非英语语言的高质量培训数据集。本文对意大利犯罪事件新闻(DICE)注释数据集的开发进行了初步研究。本文的主要贡献是: (1)建立了10395条犯罪新闻的语料库; (2)注释模式; (3)10395条新闻的自动注释数据集; (4)使用提出的1000个文档模式进行初步的手工注释。在 DICE 上进行的首次测试比较了手动注释器与单跨和多跨问答模型的性能，发现两者之间仍存在差距，尤其是在处理较复杂的注释任务和有限的训练数据时。这强调了投资于创建高质量注释数据集(如 DICE)的重要性，它可以为广泛的 NLP 模型的培训和测试提供坚实的基础。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DICE:+a+Dataset+of+Italian+Crime+Event+news)|0|
|[BDI-Sen: A Sentence Dataset for Clinical Symptoms of Depression](https://doi.org/10.1145/3539618.3591905)|Anxo Pérez, Javier Parapar, Álvaro Barreiro, Silvia LopezLarrosa|Universidade da Coruña, A Coruña, Spain|People tend to consider social platforms as convenient media for expressing their concerns and emotional struggles. With their widespread use, researchers could access and analyze user-generated content related to mental states. Computational models that exploit that data show promising results in detecting at-risk users based on engineered features or deep learning models. However, recent works revealed that these approaches have a limited capacity for generalization and interpretation when considering clinical settings. Grounding the models' decisions on clinical and recognized symptoms can help to overcome these limitations. In this paper, we introduce BDI-Sen, a symptom-annotated sentence dataset for depressive disorder. BDI-Sen covers all the symptoms present in the Beck Depression Inventory-II (BDI-II), a reliable questionnaire used for detecting and measuring depression. The annotations in the collection reflect whether a statement about the specific symptom is informative (i.e., exposes traces about the individual's state regarding that symptom). We thoroughly analyze this resource and explore linguistic style, emotional attribution, and other psycholinguistic markers. Additionally, we conduct a series of experiments investigating the utility of BDI-Sen for various tasks, including the detection and severity classification of symptoms. We also examine their generalization when considering symptoms from other mental diseases. BDI-Sen may aid the development of future models that consider trustworthy and valuable depression markers.|人们往往认为社交平台是表达自己关切和情感挣扎的便利媒体。随着它们的广泛使用，研究人员可以访问和分析与心理状态有关的用户生成内容。利用这些数据的计算模型在基于工程特征或深度学习模型的高风险用户检测方面显示出有希望的结果。然而，最近的工作表明，这些方法在考虑临床情况时，其概括和解释能力有限。建立模型对临床和已知症状的决策基础可以帮助克服这些局限性。在本文中，我们介绍了 BDI-Sen，一个抑郁症的症状注释句子数据集。BDI-Sen 涵盖了 Beck 抑郁量表 II (BDI-II)中的所有症状，这是一个用于检测和测量抑郁的可靠问卷。集合中的注释反映关于特定症状的语句是否具有信息性(即，公开关于该症状的个人状态的跟踪)。我们深入分析了这一资源，并探讨了语言风格，情感归因和其他心理语言标记。此外，我们还进行了一系列实验，调查 BDI-Sen 在各种任务中的效用，包括症状的检测和严重程度分类。当我们考虑其他精神疾病的症状时，我们也检查它们的概括性。BDI-Sen 可能有助于开发未来的模型，认为值得信赖和有价值的抑郁症标志物。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BDI-Sen:+A+Sentence+Dataset+for+Clinical+Symptoms+of+Depression)|0|
|[REFinD: Relation Extraction Financial Dataset](https://doi.org/10.1145/3539618.3591911)|Simerjot Kaur, Charese Smiley, Akshat Gupta, Joy Sain, Dongsheng Wang, Suchetha Siddagangappa, Toyin Aguda, Sameena Shah|JPMorgan Chase and Co, Palo Alto, CA, USA; JPMorgan Chase and Co, London, United Kingdom; JPMorgan Chase and Co, Chicago, IL, USA; JPMorgan Chase and Co, New York, NY, USA; Wright State University, Dayton, OH, USA|A number of datasets for Relation Extraction (RE) have been created to aide downstream tasks such as information retrieval, semantic search, question answering and textual entailment. However, these datasets fail to capture financial-domain specific challenges since most of these datasets are compiled using general knowledge sources such as Wikipedia, web-based text and news articles, hindering real-life progress and adoption within the financial world. To address this limitation, we propose REFinD, the first large-scale annotated dataset of relations, with $\sim$29K instances and 22 relations amongst 8 types of entity pairs, generated entirely over financial documents. We also provide an empirical evaluation with various state-of-the-art models as benchmarks for the RE task and highlight the challenges posed by our dataset. We observed that various state-of-the-art deep learning models struggle with numeric inference, relational and directional ambiguity.|我们已经创建了一系列关系提取数据集，用于辅助下游任务，比如信息检索、语义搜索、问题回答和文字蕴涵。然而，这些数据集未能捕捉到金融领域的具体挑战，因为大多数这些数据集是使用一般知识来源(如维基百科、基于网络的文本和新闻文章)编译的，阻碍了金融领域的实际进展和采用。为了解决这一局限性，我们提出了 REFinD，第一个大规模的注释关系数据集，$sim $29K 实例和8种类型的实体对中的22个关系，完全在财务文档上生成。我们还提供了一个实证评估与各种国家的最先进的模型作为基准的可再生能源任务，并突出了我们的数据集所带来的挑战。我们观察到各种最先进的深度学习模型都在与数字推理、关系模糊和方向模糊做斗争。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=REFinD:+Relation+Extraction+Financial+Dataset)|0|
|[The BETTER Cross-Language Datasets](https://doi.org/10.1145/3539618.3591910)|Ian Soboroff|National Institute of Standards and Technology, Gaithersburg, MD, USA|The IARPA BETTER (Better Extraction from Text Through Enhanced Retrieval) program held three evaluations of information retrieval (IR) and information extraction (IE). For both tasks, the only training data available was in English, but systems had to perform cross-language retrieval and extraction from Arabic, Farsi, Chinese, Russian, and Korean. Pooled assessment and information extraction annotation were used to create reusable IR test collections. These datasets are freely available to researchers working in cross-language retrieval, information extraction, or the conjunction of IR and IE. This paper describes the datasets, how they were constructed, and how they might be used by researchers.|IARPA 的“更好的文本提取(通过增强检索)”项目对信息检索(IR)和信息抽取(IE)进行了三次评估。对于这两个任务，唯一可用的训练数据是英语，但系统必须执行跨语言检索和提取阿拉伯语、波斯语、中文、俄语和韩语。汇总评估和信息抽取注释用于创建可重用的 IR 测试集合。这些数据集免费提供给从事跨语言检索、信息抽取或 IR 和 IE 联合工作的研究人员。本文描述了数据集，它们是如何构建的，以及研究人员如何使用它们。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+BETTER+Cross-Language+Datasets)|0|
|[Linked-DocRED - Enhancing DocRED with Entity-Linking to Evaluate End-To-End Document-Level Information Extraction Pipelines](https://doi.org/10.1145/3539618.3591912)|PierreYves Genest, PierreEdouard Portier, Elöd EgyedZsigmond, Martino Lovisetto|Univ Lyon, INSA Lyon, CNRS, UCBL, LIRIS, UMR5205, Villeurbanne, France; Alteca & Univ Lyon, INSA Lyon, CNRS, UCBL, LIRIS, UMR5205, Lyon, France; Alteca, Lyon, France|Information Extraction (IE) pipelines aim to extract meaningful entities and relations from documents and structure them into a knowledge graph that can then be used in downstream applications. Training and evaluating such pipelines requires a dataset annotated with entities, coreferences, relations, and entity-linking. However, existing datasets either lack entity-linking labels, are too small, not diverse enough, or automatically annotated (that is, without a strong guarantee of the correction of annotations). Therefore, we propose Linked-DocRED, to the best of our knowledge, the first manually-annotated, large-scale, document-level IE dataset. We enhance the existing and widely-used DocRED dataset with entity-linking labels that are generated thanks to a semi-automatic process that guarantees high-quality annotations. In particular, we use hyperlinks in Wikipedia articles to provide disambiguation candidates. We also propose a complete framework of metrics to benchmark end-to-end IE pipelines, and we define an entity-centric metric to evaluate entity-linking. The evaluation of a baseline shows promising results while highlighting the challenges of an end-to-end IE pipeline. Linked-DocRED, the source code for the entity-linking, the baseline, and the metrics are distributed under an open-source license and can be downloaded from a public repository.|信息抽取(IE)管道旨在从文档中提取有意义的实体和关系，并将它们组织成一个知识图，然后用于下游应用程序。训练和评估这样的管道需要一个数据集，该数据集使用实体、共引用、关系和实体链接进行注释。然而，现有的数据集要么缺乏实体链接标签，要么太小，不够多样化，要么自动注释(也就是说，没有强有力的注释修正保证)。因此，我们提出了 Linked-DocRED，据我们所知，第一个手动注释的大型文档级 IE 数据集。我们使用实体链接标签来增强现有的和广泛使用的 DocRED 数据集，这些标签是通过一个半自动的过程生成的，这个过程保证了高质量的注释。特别是，我们在 Wikipedia 文章中使用超链接来提供消除歧义的候选者。我们还提出了一个完整的度量框架来基准端到端 IE 管道，并定义了一个以实体为中心的度量来评估实体链接。对基线的评估显示了有希望的结果，同时突出了端到端 IE 管道的挑战。LinkedDocRED，实体链接、基线和度量的源代码在开源许可下分发，可以从公共存储库下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Linked-DocRED+-+Enhancing+DocRED+with+Entity-Linking+to+Evaluate+End-To-End+Document-Level+Information+Extraction+Pipelines)|0|
|[A Preference Judgment Tool for Authoritative Assessment](https://doi.org/10.1145/3539618.3591801)|Mahsa Seifikar, Linh Nhi Phan Minh, Negar Arabzadeh, Charles L. A. Clarke, Mark D. Smucker|University of Waterloo, Waterloo, ON, Canada|Preference judgments have been established as an effective method for offline evaluation of information retrieval systems with advantages to graded or binary relevance judgments. Graded judgments assign each document a pre-defined grade level, while preference judgments involve assessing a pair of items presented side by side and indicating which is better. However, leveraging preference judgments may require a more extensive number of judgments, and there are limitations in terms of evaluation measures. In this study, we present a new preference judgment tool called JUDGO, designed for expert assessors and researchers. The tool is supported by a new heap-like preference judgment algorithm that assumes transitivity and allows for ties. An earlier version of the tool was employed by NIST to determine up to the top-10 best items for each of the 38 topics for the TREC 2022 Health Misinformation track, with over 2,200 judgments collected. The current version has been applied in a separate research study to collect almost 10,000 judgments, with multiple assessors completing each topic. The code and resources are available at https://judgo-system.github.io.|偏好判断已被确立为一种有效的方法，用于对信息检索系统进行离线评估，这种方法优于分级或二元相关判断。分级判断为每份文件分配一个预定义的等级水平，而偏好判断涉及评估一对项目并排出现，并指出哪一个更好。然而，利用偏好判断可能需要更广泛的判断数量，并且在评估措施方面存在局限性。在这项研究中，我们提出了一个新的偏好判断工具称为 JUDGO，专为专家评估员和研究人员设计。该工具由一个新的类似堆的偏好判断算法支持，该算法假定传递性并允许关系。NIST 使用了该工具的早期版本，为 TREC 2022健康错误信息跟踪的38个主题中的每个主题确定最多10个最佳项目，收集了超过2200个判断。目前的版本已经应用在一个单独的研究中，收集了近10,000个判断，由多个评估人员完成每个主题。代码和资源可在 https://judgo-system.github.io 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Preference+Judgment+Tool+for+Authoritative+Assessment)|0|
|[One Stop Shop for Question-Answering Dataset Selection](https://doi.org/10.1145/3539618.3591804)|Chang Nian Chuy, Qinmin Vivian Hu, Chen Ding|Toronto Metropolitan University, Toronto, ON, Canada|In this paper, we offer a new visualization tool -- Dataset Statistical View (DSV), to lower the barrier of research entry by providing easy access to the question-answering (QA) datasets that researchers can build their work upon. Our target users are new researchers to the QA domain with no prior knowledge nor programming skills. The system is populated with multiple QA datasets, which covers a wide range of QA tasks. It allows researchers to explore and compare existing QA datasets at a one-stop website. The system shows statistical graphs for each QA dataset to offer an overview and a visual comparison between datasets. Although this paper focuses mainly at the syntactic level comparison, integrating bias and semantic level analysis is our ongoing work. We believe our DSV system is a valuable contribution to the advancement of the QA field, as it provides a solid starting point for new researchers and practitioners. An overview of the framework is demonstrated in this paper and the introduction of the application system is available at https://cnchuy.github.io/images/demo.mp4.|本文提出了一种新的可视化工具——数据集统计视图(Dataset Statistics View，DSV) ，通过提供易于访问的问答(QA)数据集来降低研究进入的障碍，研究人员可以在此基础上开展工作。我们的目标用户是 QA 领域的新研究人员，既没有先前的知识，也没有编程技能。该系统由多个 QA 数据集填充，这些数据集涵盖了广泛的 QA 任务。它允许研究人员在一站式网站上探索和比较现有的质量保证数据集。该系统显示每个质量保证数据集的统计图表，以提供数据集之间的概述和可视化比较。虽然本文主要集中在句法层面的比较，但是整合偏倚和语义层面的分析是我们正在进行的工作。我们相信，我们的 DSV 系统是一个宝贵的贡献，提高质量保证领域，因为它提供了一个坚实的起点，新的研究人员和从业人员。本文概述这个架构，并介绍应用系统的 https://cnchuy.github.io/images/demo.mp4。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=One+Stop+Shop+for+Question-Answering+Dataset+Selection)|0|
|[Profiling and Visualizing Dynamic Pruning Algorithms](https://doi.org/10.1145/3539618.3591806)|Zhixuan Li, Joel Mackenzie|The University of Queensland, Brisbane, QLD, Australia|Efficiently retrieving the top-k documents for a given query is a fundamental operation in many search applications. Dynamic pruning algorithms accelerate top-k retrieval over inverted indexes by skipping documents that are not able to enter the current set of results. However, the performance of these algorithms depends on a number of variables such as the ranking function, the order of documents within the index, and the number of documents to be retrieved. In this paper, we propose a diagnostic framework, Dyno, for profiling and visualizing the performance of dynamic pruning algorithms. Our framework captures processing traces during retrieval, allowing the operations of the index traversal algorithm to be visualized. These visualizations support both query-level and system-to-system comparisons, enabling performance characteristics to be readily understood for different systems. Dyno benefits both academics and practitioners by furthering our understanding of the behavior of dynamic pruning algorithms, allowing better design choices to be made during experimentation and deployment.|有效地检索给定查询的 top-k 文档是许多搜索应用程序中的基本操作。动态剪枝算法通过跳过无法输入当前结果集的文档来加速对倒排索引的 top-k 检索。然而，这些算法的性能取决于许多变量，例如排名函数、索引中文档的顺序以及要检索的文档数量。在本文中，我们提出了一个诊断框架，Dyno，用于分析和可视化动态剪枝算法的性能。我们的框架在检索期间捕获处理跟踪，允许可视化索引遍历算法的操作。这些可视化支持查询级别和系统到系统的比较，从而能够容易地理解不同系统的性能特征。Dyno 通过加深我们对动态修剪算法行为的理解，使学者和实践者受益匪浅，从而允许在实验和部署过程中做出更好的设计选择。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Profiling+and+Visualizing+Dynamic+Pruning+Algorithms)|0|
|[NeuralKG-ind: A Python Library for Inductive Knowledge Graph Representation Learning](https://doi.org/10.1145/3539618.3591809)|Wen Zhang, Zhen Yao, Mingyang Chen, Zhiwei Huang, Huajun Chen|Zhejiang University, Hangzhou, China; Zhejiang University, Ningbo, China|Since the dynamic characteristics of knowledge graphs, many inductive knowledge graph representation learning (KGRL) works have been proposed in recent years, focusing on enabling prediction over new entities. NeuralKG-ind is the first library of inductive KGRL as an important update of NeuralKG library. It includes standardized processes, rich existing methods, decoupled modules, and comprehensive evaluation metrics. With NeuralKG-ind, it is easy for researchers and engineers to reproduce, redevelop, and compare inductive KGRL methods. The library, experimental methodologies, and model re-implementing results of NeuralKG-ind are all publicly released at https://github.com/zjukg/NeuralKG/tree/ind .|由于知识图的动态特性，近年来提出了许多归纳知识图表示学习(KGRL)的工作，重点是实现对新实体的预测。NeuralKG-ind 是第一个归纳 KGRL 文库，是 NeuralKG 文库的重要更新。它包括标准化过程、丰富的现有方法、解耦模块和全面的评估指标。使用 NeuralKG-ind，研究人员和工程师可以很容易地再现、重新开发和比较归纳 KGRL 方法。NeuralKG-ind 的数据库、实验方法和模型重新实现结果都在 https://github.com/zjukg/neuralkg/tree/ind 公开发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NeuralKG-ind:+A+Python+Library+for+Inductive+Knowledge+Graph+Representation+Learning)|0|
|[AMICA: Alleviating Misinformation for Chinese Americans](https://doi.org/10.1145/3539618.3591810)|Xiaoxiao Shang, Ye Chen, Yi Fang, Yuhong Liu, Subramaniam Vincent|Santa Clara University, Santa Clara, CA, USA|The increasing popularity of social media promotes the proliferation of misinformation, especially in the communities of Chinese-speaking diasporas, which has caused significant negative societal impacts. In addition, most of the existing efforts on misinformation mitigation have focused on English and other western languages, which makes numerous overseas Chinese a very vulnerable population to online disinformation campaigns. In this paper, we present AMICA, an information retrieval system for alleviating misinformation for Chinese Americans. AMICA dynamically collects data from popular social media platforms for Chinese Americans, including WeChat, Twitter, YouTube, and Chinese forums. The data are stored and indexed in Elasticsearch to provide advanced search functionalities. Given a user query, the ranking of social media posts considers both topical relevance and the likelihood of being misinformation.|社交媒体的日益普及促进了错误信息的扩散，特别是在讲华语的侨民社区，这已经造成了重大的负面社会影响。此外，现有的减少虚假信息的努力大多集中在英语和其他西方语言，这使得许多海外华人成为网络虚假信息活动的一个非常脆弱的群体。在这篇文章中，我们介绍了 AMICA，这是一个信息检索系统，用于减少对华裔美国人的错误信息。AMICA 为华裔美国人动态收集流行社交媒体平台的数据，包括微信、 Twitter、 YouTube 和中国论坛。这些数据在 Elasticsearch 存储和编制索引，以提供高级搜索功能。如果用户提出疑问，社交媒体帖子的排名既考虑了主题相关性，也考虑了错误信息的可能性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AMICA:+Alleviating+Misinformation+for+Chinese+Americans)|0|
|[PEPO: Petition Executing Processing Optimizer Based on Natural Language Processing](https://doi.org/10.1145/3539618.3591811)|YinWei Chiu, HsiaoChing Huang, ChengJu Lee, HsunPing Hsieh|National Cheng Kung University, Tainan, Taiwan Roc|In this paper, we propose "Petition Executing Process Optimizer (PEPO)," an AI-based petition processing system that features three components, (a) Department Classification, (b) Importance Assessment, and (c) Response Generation for improving the Public Work Bureau (PWB) 1999 Hotline petitions handling process in Taiwan. Our Department Classification algorithm has been evaluated with NDCG, achieving an impressive score of 86.48%, while the Important Assessment function has an accuracy rate of 85%. Besides, Response Generation enhances communication efficiency between the government and citizens. The PEPO system has been deployed as an online web service for the Public Works Bureau of the Tainan City Government. With PEPO, the PWB benefits greatly from the effectiveness and efficiency of handling citizens' petitions.|在本文中，我们提出了“请愿执行程序优化器(PEPO)”，一个基于人工智能的请愿处理系统，包括三个部分: (a)部门分类，(b)重要性评估，(c)响应生成，以改善台湾公共工程局(PWB)1999年热线请愿处理程序。我们的部门分类算法已经评估与 NDCG，取得了令人印象深刻的分数为86.48% ，而重要的评估功能的准确率为85% 。此外，响应生成提高了政府与市民之间的沟通效率。公共服务电子化系统是为台南市政府工务局提供的网上服务。有了 PEPO，工务局可以从处理公民请愿的效率和效力中获益匪浅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PEPO:+Petition+Executing+Processing+Optimizer+Based+on+Natural+Language+Processing)|0|
|[SEA: A Scalable Entity Alignment System](https://doi.org/10.1145/3539618.3591816)|Junyang Wu, Tianyi Li, Lu Chen, Yunjun Gao, Ziheng Wei|Aalborg University, Aalborg, Denmark; Zhejiang University, Hangzhou, China; Huawei, Hangzhou, China|Entity alignment (EA) aims to find equivalent entities in different knowledge graphs (KGs). State-of-the-art EA approaches generally use Graph Neural Networks (GNNs) to encode entities. However, most of them train the models and evaluate the results in a fullbatch fashion, which prohibits EA from being scalable on largescale datasets. To enhance the usability of GNN-based EA models in real-world applications, we present SEA, a scalable entity alignment system that enables to (i) train large-scale GNNs for EA, (ii) speed up the normalization and the evaluation process, and (iii) report clear results for users to estimate different models and parameter settings. SEA can be run on a computer with merely one graphic card. Moreover, SEA encompasses six state-of-the-art EA models and provides access for users to quickly establish and evaluate their own models. Thus, SEA allows users to perform EA without being involved in tedious implementations, such as negative sampling and GPU-accelerated evaluation. With SEA, users can gain a clear view of the model performance. In the demonstration, we show that SEA is user-friendly and is of high scalability even on computers with limited computational resources.|实体对齐(EA)的目的是在不同的知识图中寻找等价的实体。最先进的 EA 方法通常使用图形神经网络(GNN)对实体进行编码。然而，他们中的大多数以全批方式训练模型并评估结果，这阻碍了 EA 在大规模数据集上的可伸缩性。为了提高基于 GNN 的 EA 模型在实际应用中的可用性，我们提出了 SEA，这是一个可扩展的实体对齐系统，它能够(i)为 EA 训练大规模的 GNN，(ii)加速归一化和评估过程，以及(iii)报告清晰的结果，供用户估计不同的模型和参数设置。SEA 可以在一台计算机上运行，只需要一张图形卡。此外，SEA 还包括六种最先进的 EA 模型，并为用户提供了快速建立和评估自己模型的途径。因此，SEA 允许用户执行 EA，而不必参与繁琐的实现，例如负采样和 GPU 加速的评估。通过 SEA，用户可以清楚地看到模型的性能。在演示中，我们证明了 SEA 是用户友好的，即使在计算资源有限的计算机上也具有很高的可伸缩性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SEA:+A+Scalable+Entity+Alignment+System)|0|
|[TIB AV-Analytics: A Web-based Platform for Scholarly Video Analysis and Film Studies](https://doi.org/10.1145/3539618.3591820)|Matthias Springstein, Markos Stamatakis, Margret Plank, Julian Sittel, Roman Mauer, Oksana Bulgakowa, Ralph Ewerth, Eric MüllerBudack|TIB - Leibniz Information Centre for Science and Technology & Leibniz University Hannover, Hannover, Germany; Johannes Gutenberg University Mainz, Mainz, Germany; TIB - Leibniz Information Centre for Science and Technology, Hannover, Germany|Video analysis platforms that integrate automatic solutions for multimedia and information retrieval enable various applications in many disciplines including film and media studies, communication science, and education. However, current platforms for video analysis either focus on manual annotations or include only a few tools for automatic content analysis. In this paper, we present a novel web-based video analysis platform called TIB AV-Analytics (TIB-AV-A). Unlike previous platforms, TIB-AV-A integrates state-of-the-art approaches in the fields of computer vision, audio analysis, and natural language processing for many relevant video analysis tasks. To facilitate future extensions and to ensure interoperability with existing tools, the video analysis approaches are implemented in a plugin structure with appropriate interfaces and import-export functions. TIB-AV-A leverages modern web technologies to provide users with a responsive and interactive web interface that enables manual annotation and provides access to powerful deep learning tools without a requirement for specific hardware dependencies. Source code and demo are publicly available at: https://service.tib.eu/tibava.|集成了多媒体和信息检索自动解决方案的视频分析平台，使许多学科(包括电影和媒体研究、通信科学和教育)的各种应用成为可能。然而，目前用于视频分析的平台要么侧重于手动注释，要么只包括一些用于自动内容分析的工具。本文提出了一种新的基于网络的视频分析平台 TIB AV-Analytics (TIB-AV-A)。与以前的平台不同，TIB-AV-A 集成了计算机视觉、音频分析和自然语言处理领域的最先进的方法，用于许多相关的视频分析任务。为了促进未来的扩展，并确保与现有工具的互操作性，视频分析方法是在一个具有适当接口和导入导出功能的插件结构中实现的。TIB-AV-A 利用现代网络技术为用户提供响应和交互式的网络界面，支持手动注释，并提供访问强大的深度学习工具，而不需要特定的硬件依赖。源代码和演示可以在以下 https://service.tib.eu/tibava 公开获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TIB+AV-Analytics:+A+Web-based+Platform+for+Scholarly+Video+Analysis+and+Film+Studies)|0|
|[A Consumer Compensation System in Ride-hailing Service](https://doi.org/10.1145/3539618.3591829)|Zhe Yu, Chi Xia, Shaosheng Cao, Lin Zhou, Haibin Huang|DiDi Chuxing, Hangzhou, China|In the ride-hailing business, compensation is mostly used to motivate consumers to place more orders and grow the market scale. However, most of the previous studies focus on car-hailing services. Few works investigate localized smart transportation innovations, such as intra-city freight logistics and designated driving. In addition, satisfying consumer fairness and improving consumer surplus, with the objective of maximizing revenue, are also important. In this paper, we propose a consumer compensation system, where a transfer learning enhanced uplift modeling is designed to measure the elasticity, and a model predictive control based optimization is formulated to control the budget accurately. Our implementation is effective and can keep the online environment lightweight. The proposed system has been deployed in the production environment of the real-world ride-hailing platform for 300 days, which outperforms the expert strategy by using 0.5% less subsidy and achieving 14.4% more revenue.|在叫车业务中，薪酬主要用于激励消费者下更多的订单，扩大市场规模。然而，以前的大多数研究都集中在叫车服务上。很少有作品研究本地化的智能交通创新，如城市内的货运物流和指定驾驶。此外，满足消费者公平和改善以收益最大化为目标的消费者剩余也很重要。在这篇文章中，我们提出了一个消费者补偿系统，其中转移学习增强提升模型被设计来测量弹性，并且基于模型预估计控制的优化被制定来精确地控制预算。我们的实现是有效的，可以保持在线环境的轻量级。该系统已经在现实世界的叫车平台生产环境中部署了300天，比专家策略节省了0.5% 的补贴，获得了14.4% 的收入。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Consumer+Compensation+System+in+Ride-hailing+Service)|0|
|[Dialog-to-Actions: Building Task-Oriented Dialogue System via Action-Level Generation](https://doi.org/10.1145/3539618.3591832)|Yuncheng Hua, Xiangyu Xi, Zheng Jiang, Guanwei Zhang, Chaobo Sun, Guanglu Wan, Wei Ye|Meituan Group, Beijing, China; Peking University, Beijing, China|End-to-end generation-based approaches have been investigated and applied in task-oriented dialogue systems. However, in industrial scenarios, existing methods face the bottlenecks of controllability (e.g., domain-inconsistent responses, repetition problem, etc) and efficiency (e.g., long computation time, etc). In this paper, we propose a task-oriented dialogue system via action-level generation. Specifically, we first construct dialogue actions from large-scale dialogues and represent each natural language (NL) response as a sequence of dialogue actions. Further, we train a Sequence-to-Sequence model which takes the dialogue history as input and outputs sequence of dialogue actions. The generated dialogue actions are transformed into verbal responses. Experimental results show that our light-weighted method achieves competitive performance, and has the advantage of controllability and efficiency.|研究了基于端到端生成的方法，并将其应用于面向任务的对话系统。然而，在工业场景中，现有的方法面临着可控性(例如，领域不一致的响应、重复问题等)和效率(例如，长计算时间等)的瓶颈。在本文中，我们提出了一个面向任务的对话系统，通过行为级生成。具体来说，我们首先从大规模对话中构建对话行为，并将每个自然语言(NL)响应表示为一系列对话行为。进一步，我们训练了一个序列到序列模型，该模型以对话历史为输入，输出对话动作的序列。生成的对话动作转化为语言反应。实验结果表明，本文提出的轻量化方法具有良好的性能，并且具有可控性和高效性的优点。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dialog-to-Actions:+Building+Task-Oriented+Dialogue+System+via+Action-Level+Generation)|0|
|[Context-Aware Classification of Legal Document Pages](https://doi.org/10.1145/3539618.3591839)|Pavlos Fragkogiannis, Martina Forster, Grace E. Lee, Dell Zhang|Thomson Reuters Labs, Zug, Switzerland; Thomson Reuters Labs, London, United Kingdom|For many business applications that require the processing, indexing, and retrieval of professional documents such as legal briefs (in PDF format etc.), it is often essential to classify the pages of any given document into their corresponding types beforehand. Most existing studies in the field of document image classification either focus on single-page documents or treat multiple pages in a document independently. Although in recent years a few techniques have been proposed to exploit the context information from neighboring pages to enhance document page classification, they typically cannot be utilized with large pre-trained language models due to the constraint on input length. In this paper, we present a simple but effective approach that overcomes the above limitation. Specifically, we enhance the input with extra tokens carrying sequential information about previous pages - introducing recurrence - which enables the usage of pre-trained Transformer models like BERT for context-aware page classification. Our experiments conducted on two legal datasets in English and Portuguese respectively show that the proposed approach can significantly improve the performance of document page classification compared to the non-recurrent setup as well as the other context-aware baselines.|对于许多需要处理、索引和检索专业文档(如法律摘要(PDF 格式等))的业务应用程序，通常必须事先将任何给定文档的页面分类为相应的类型。现有的文档图像分类研究大多集中于单页文档，或者单独处理多页文档。尽管近年来提出了一些利用相邻页面的上下文信息来提高文档页面分类的技术，但由于输入长度的限制，这些技术通常不能用于大型预训练语言模型。在本文中，我们提出了一个简单而有效的方法，克服了上述限制。具体来说，我们使用额外的标记来增强输入，这些标记携带关于前面页面的连续信息——引入循环——这使得可以使用像 BERT 这样的预先训练的 Transformer 模型来进行上下文感知的页面分类。我们分别在英文和葡萄牙文的两个法律数据集上进行的实验表明，与非经常性设置和其他上下文感知基线相比，该方法可以显著提高文档页面分类的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Context-Aware+Classification+of+Legal+Document+Pages)|0|
|[Enhancing Dynamic Image Advertising with Vision-Language Pre-training](https://doi.org/10.1145/3539618.3591844)|Zhoufutu Wen, Xinyu Zhao, Zhipeng Jin, Yi Yang, Wei Jia, Xiaodong Chen, Shuanglong Li, Lin Liu|Peking University, Beijing, China; Baidu Inc., Beijing, China|In the multimedia era, image is an effective medium in search advertising. Dynamic Image Advertising (DIA), a system that matches queries with ad images and generates multimodal ads, is introduced to improve user experience and ad revenue. The core of DIA is a query-image matching module performing ad image retrieval and relevance modeling. Current query-image matching suffers from limited and inconsistent data, and insufficient cross-modal interaction. Also, the separate optimization of retrieval and relevance models affects overall performance. To address this issue, we propose a vision-language framework consisting of two parts. First, we train a base model on large-scale image-text pairs to learn general multimodal representation. Then, we fine-tune the base model on advertising business data, unifying relevance modeling and retrieval through multi-objective learning. Our framework has been implemented in Baidu search advertising system "Phoneix Nest". Online evaluation shows that it improves cost per mille (CPM) and click-through rate (CTR) by 1.04% and 1.865%.|在多媒体时代，图像是搜索广告的有效媒介。为了提高用户体验和广告收入，引入了动态图像广告(DIA)系统，将查询与广告图像进行匹配，生成多模式广告。DIA 的核心是一个查询-图像匹配模块，用于进行广告图像检索和相关性建模。当前的查询-图像匹配存在数据有限、不一致以及跨模式交互不足的问题。此外，检索和相关性模型的单独优化也会影响整体性能。为了解决这个问题，我们提出了一个由两部分组成的视觉语言框架。首先，我们训练一个基于大规模图像-文本对的模型来学习一般的多模态表示。然后，对广告业务数据的基础模型进行微调，通过多目标学习将相关性建模与检索统一起来。我们的架构已在百度搜寻广告系统“ Phoneix Nest”中实施。在线评估显示，它使每公里成本(CPM)和点进率(CTR)分别提高了1.04% 和1.865% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Dynamic+Image+Advertising+with+Vision-Language+Pre-training)|0|
|[Modeling Spoken Information Queries for Virtual Assistants: Open Problems, Challenges and Opportunities](https://doi.org/10.1145/3539618.3591849)|Christophe Van Gysel|Apple, Cambridge, MA, USA|Virtual assistants are becoming increasingly important speech-driven Information Retrieval platforms that assist users with various tasks. We discuss open problems and challenges with respect to modeling spoken information queries for virtual assistants, and list opportunities where Information Retrieval methods and research can be applied to improve the quality of virtual assistant speech recognition. We discuss how query domain classification, knowledge graphs and user interaction data, and query personalization can be helpful to improve the accurate recognition of spoken information domain queries. Finally, we also provide a brief overview of current problems and challenges in speech recognition.|虚拟助手正在成为越来越重要的语音驱动信息检索平台，帮助用户完成各种任务。我们讨论了在虚拟助理语音信息查询建模方面存在的问题和挑战，并列出了可以应用信息检索方法和研究来提高虚拟助理语音识别质量的机会。讨论了查询域分类、知识图和用户交互数据以及查询个性化如何有助于提高对语音信息域查询的准确识别。最后，我们还简要概述了当前语音识别中存在的问题和挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modeling+Spoken+Information+Queries+for+Virtual+Assistants:+Open+Problems,+Challenges+and+Opportunities)|0|
|[Extracting Complex Named Entities in Legal Documents via Weakly Supervised Object Detection](https://doi.org/10.1145/3539618.3591852)|HsiuWei Yang, Abhinav Agrawal|Thomson Reuters Labs, Bangalore, India; Thomson Reuters Labs, Toronto, ON, Canada|Accurate Named Entity Recognition (NER) is crucial for various information retrieval tasks in industry. However, despite significant progress in traditional NER methods, the extraction of Complex Named Entities remains a relatively unexplored area. In this paper, we propose a novel system that combines object detection for Document Layout Analysis (DLA) with weakly supervised learning to address the challenge of extracting discontinuous complex named entities in legal documents. Notably, to the best of our knowledge, this is the first work to apply weak supervision to DLA. Our experimental results show that the model trained solely on pseudo labels outperforms the supervised baseline when gold-standard data is limited, highlighting the effectiveness of our proposed approach in reducing the dependency on annotated data.|精确命名实体识别(NER)对于工业中的各种信息检索任务都是至关重要的。然而，尽管在传统的 NER 方法中取得了重大进展，但是复杂命名实体的提取仍然是一个相对未开发的领域。在本文中，我们提出了一个新的系统，它结合了文档布局分析(DLA)的目标检测和弱监督式学习，以解决在法律文档中提取不连续的复杂命名实体的难题。值得注意的是，据我们所知，这是第一项对 DLA 实施薄弱监管的工作。我们的实验结果表明，当金标准数据有限时，仅用伪标签训练的模型优于监督基线，突出了我们提出的方法在减少对注释数据的依赖方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Extracting+Complex+Named+Entities+in+Legal+Documents+via+Weakly+Supervised+Object+Detection)|0|
|[Improving Programming Q&A with Neural Generative Augmentation](https://doi.org/10.1145/3539618.3591860)|Suthee Chaidaroon, Xiao Zhang, Shruti Subramaniyam, Jeffrey Svajlenko, Tanya Shourya, Iman Keivanloo, Ria Joy|Amazon Web Service, Seattle, WA, USA|Knowledge-intensive programming Q&A is an active research area in industry. Its application boosts developer productivity by aiding developers in quickly finding programming answers from the vast amount of information on the Internet. In this study, we propose ProQANS and its variants ReProQANS and ReAugProQANS to tackle programming Q&A. ProQANS is a neural search approach that leverages unlabeled data on the Internet (such as StackOverflow) to mitigate the cold-start problem. ReProQANS extends ProQANS by utilizing reformulated queries with a novel triplet loss. We further use an auxiliary generative model to augment the training queries, and design a novel dual triplet loss function to adapt these generated queries, to build another variant of ReProQANS termed as ReAugProQANS. In our empirical experiments, we show ReProQANS has the best performance when evaluated on the in-domain test set, while ReAugProQANS has the superior performance on the out-of-domain real programming questions, by outperforming the state-of-the-art model by up to 477% lift on the MRR metric respectively. The results suggest their robustness to previously unseen questions and its wide application to real programming questions.|知识密集型规划问答是工业界一个活跃的研究领域。它的应用程序通过帮助开发人员从互联网上的大量信息中快速找到编程答案来提高开发人员的生产力。在这项研究中，我们提出 ProQANS 及其变体 ReProQANS 和 ReAugProQANS 来解决编程问答。ProQANS 是一种神经搜索方法，它利用 Internet 上的未标记数据(如 StackOverflow)来缓解冷启动问题。ReProQANS 通过使用具有新的三联体损失的重新制定的查询来扩展 ProQANS。我们进一步使用一个辅助生成模型来增加训练查询，并设计一个新颖的双三重丢失函数来适应这些生成的查询，以构建另一种称为 ReaugproQANS 的 ReProQANS。在我们的实证实验中，我们发现 ReProQANS 在域内测试集上的表现最好，而 ReAugProQANS 在域外实际编程问题上的表现更好，在 MRR 指标上的表现分别比最先进的模型高出477% 。结果表明，它们对以前看不见的问题的鲁棒性及其对真正的编程问题的广泛应用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Programming+Q&A+with+Neural+Generative+Augmentation)|0|
|[Uncertainty Quantification for Text Classification](https://doi.org/10.1145/3539618.3594243)|Dell Zhang, Murat Sensoy, Masoud Makrehchi, Bilyana TanevaPopova, Lin Gui, Yulan He|Amazon Alexa AI, London, United Kingdom; King's College London & The Alan Turing Institute, London, United Kingdom; King's College London, London, United Kingdom; Thomson Reuters Labs, London, United Kingdom; Thomson Reuters Labs, Zug, Switzerland; Thomson Reuters Labs, Toronto, ON, Canada|This full-day tutorial introduces modern techniques for practical uncertainty quantification specifically in the context of multi-class and multi-label text classification. First, we explain the usefulness of estimating aleatoric uncertainty and epistemic uncertainty for text classification models. Then, we describe several state-of-the-art approaches to uncertainty quantification and analyze their scalability to big text data: Virtual Ensemble in GBDT, Bayesian Deep Learning (including Deep Ensemble, Monte-Carlo Dropout, Bayes by Backprop, and their generalization Epistemic Neural Networks), Evidential Deep Learning (including Prior Networks and Posterior Networks), as well as Distance Awareness (including Spectral-normalized Neural Gaussian Process and Deep Deterministic Uncertainty). Next, we talk about the latest advances in uncertainty quantification for pre-trained language models (including asking language models to express their uncertainty, interpreting uncertainties of text classifiers built on large-scale language models, uncertainty estimation in text generation, calibration of language models, and calibration for in-context learning). After that, we discuss typical application scenarios of uncertainty quantification in text classification (including in-domain calibration, cross-domain robustness, and novel class detection). Finally, we list popular performance metrics for the evaluation of uncertainty quantification effectiveness in text classification. Practical hands-on examples/exercises are provided to the attendees for them to experiment with different uncertainty quantification methods on a few real-world text classification datasets such as CLINC150.|本教程介绍了在多类和多标签文本分类的背景下实用的不确定性量化的现代技术。首先，我们解释了文本分类模型中估计随机不确定性和认知不确定性的有用性。然后，我们描述了几种最先进的不确定性量化方法，并分析了它们对大文本数据的可伸缩性: GBDT 中的虚拟集合，贝叶斯深度学习(包括深度集合，Monte-Carlo 辍学，Bayes by Backsupport 及其泛化认知神经网络) ，证据深度学习(包括先验网络和后验网络) ，以及距离感知(包括谱归一化神经高斯过程和深度确定性不确定性)。接下来，我们将讨论预训练语言模型不确定性量化的最新进展(包括要求语言模型表达它们的不确定性，解释建立在大规模语言模型上的文本分类器的不确定性，文本生成中的不确定性估计，语言模型的校准，以及在上下文中学习的校准)。然后，我们讨论了不确定性量化在文本分类中的典型应用场景(包括域内校准、跨域鲁棒性和新的类检测)。最后，我们列出了文本分类中用于评估不确定性量化效果的常用性能指标。我们提供实际的例子/练习给与会者，让他们在一些真实世界的文本分类数据集(例如 CLINC150)上尝试不同的不确定性量化方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Uncertainty+Quantification+for+Text+Classification)|0|
|[Offline Pseudo Relevance Feedback for Efficient and Effective Single-pass Dense Retrieval](https://doi.org/10.1145/3539618.3592028)|Xueru Wen, Xiaoyang Chen, Xuanang Chen, Ben He, Le Sun|University of Chinese Academy of Sciences & Institute of Software, Chinese Academy of Sciences, Beijing, China; Institute of Software, Chinese Academy of Sciences, Beijing, China|Dense retrieval has made significant advancements in information retrieval (IR) by achieving high levels of effectiveness while maintaining online efficiency during a single-pass retrieval process. However, the application of pseudo relevance feedback (PRF) to further enhance retrieval effectiveness results in a doubling of online latency. To address this challenge, this paper presents a single-pass dense retrieval framework that shifts the PRF process offline through the utilization of pre-generated pseudo-queries. As a result, online retrieval is reduced to a single matching with the pseudo-queries, hence providing faster online retrieval. The effectiveness of the proposed approach is evaluated on the standard TREC DL and HARD datasets, and the results demonstrate its promise. Our code is openly available at https://github.com/Rosenberg37/OPRF.|密集检索通过在一次性检索过程中保持在线效率的同时实现高水平的有效性，在信息检索(IR)方面取得了重大进展。然而，应用伪关联反馈(PRF)进一步提高检索效率会导致在线延迟加倍。为了解决这个问题，本文提出了一个单通道密集检索框架，通过利用预生成的伪查询将 PRF 过程转移到离线状态。因此，在线检索减少为与伪查询的单一匹配，从而提供更快的在线检索。在标准 TREC DL 和 HARD 数据集上对该方法的有效性进行了评估，结果表明了该方法的有效性。我们的代码在 https://github.com/rosenberg37/oprf 公开可用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Offline+Pseudo+Relevance+Feedback+for+Efficient+and+Effective+Single-pass+Dense+Retrieval)|-1|
|[Exploring Scenarios of Uncertainty about the Users' Preferences in Interactive Recommendation Systems](https://doi.org/10.1145/3539618.3591684)|Nícollas Silva, Thiago Silva, Henrique Hott, Yan Ribeiro, Adriano C. M. Pereira, Leonardo Rocha|Federal University of São João Del Rei, São João del-Rei, Brazil; Universidade Federal de Minas Gerais, Belo Horizonte, Brazil|Interactive Recommender Systems have played a crucial role in distinct entertainment domains through a Contextual Bandit model. Despite the current advances, their personalisation level is still directly related to the information previously available about the users. However, there are at least two scenarios of uncertainty about the users' preferences over their journey: (1) when the user joins for the first time and (2) when the system continually makes wrong recommendations because of prior misleading assumptions. In this work, we introduce concepts from the Active Learning theory to mitigate the impact of such scenarios. We modify three traditional bandits to recommend items with a higher potential to get more user information without decreasing the model's accuracy when an uncertain scenario is observed. Our experiments show that the modified models outperform all baselines by increasing the cumulative reward in the long run. Moreover, a counterfactual evaluation validates that such improvements were not simply achieved due to the bias of offline datasets.|交互式推荐系统通过上下文强盗模式在不同的娱乐领域发挥了重要作用。尽管目前的进步，他们的个性化水平仍然直接相关的信息以前可用的用户。然而，至少存在两种不确定性情景: (1)当用户第一次加入时; (2)当系统由于先前的误导性假设而不断提出错误的建议时。在这项工作中，我们引入了主动学习理论的概念，以减轻这种情景的影响。在不确定情景下，通过修改三个传统的土匪推荐模型，在不降低模型精度的前提下，获得更多的用户信息。我们的实验表明，修改后的模型通过增加长期的累积报酬优于所有的基线。此外，一个反事实的评估验证了这样的改进不仅仅是由于离线数据集的偏见而实现的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+Scenarios+of+Uncertainty+about+the+Users'+Preferences+in+Interactive+Recommendation+Systems)|-1|
|[Exploring the Spatiotemporal Features of Online Food Recommendation Service](https://doi.org/10.1145/3539618.3591853)|Shaochuan Lin, Jiayan Pei, Taotao Zhou, Hengxu He, Jia Jia, Ning Hu||Online Food Recommendation Service (OFRS) has remarkable spatiotemporal characteristics and the advantage of being able to conveniently satisfy users' needs in a timely manner. There have been a variety of studies that have begun to explore its spatiotemporal properties, but a comprehensive and in-depth analysis of the OFRS spatiotemporal features is yet to be conducted. Therefore, this paper studies the OFRS based on three questions: how spatiotemporal features play a role; why self-attention cannot be used to model the spatiotemporal sequences of OFRS; and how to combine spatiotemporal features to improve the efficiency of OFRS. Firstly, through experimental analysis, we systemically extracted the spatiotemporal features of OFRS, identified the most valuable features and designed an effective combination method. Secondly, we conducted a detailed analysis of the spatiotemporal sequences, which revealed the shortcomings of self-attention in OFRS, and proposed a more optimized spatiotemporal sequence method for replacing self-attention. In addition, we also designed a Dynamic Context Adaptation Model to further improve the efficiency and performance of OFRS. Through the offline experiments on two large datasets and online experiments for a week, the feasibility and superiority of our model were proven.|在线食品推荐服务(OFRS)具有显著的时空特征和能够方便、及时地满足用户需求的优势。已经有各种各样的研究开始探索它的时空特性，但是对 OFRS 时空特性的全面和深入的分析还有待进行。为此，本文从时空特征如何发挥作用、为什么自我注意不能用于 OFRS 时空序列建模以及如何结合时空特征提高 OFRS 效率三个方面对 OFRS 进行了研究。首先，通过实验分析，系统地提取了 OFRS 的时空特征，识别出最有价值的特征，并设计了一种有效的组合方法。其次，我们对时空序列进行了详细的分析，揭示了 OFRS 中自我注意的缺点，并提出了一种更优化的时空序列替代自我注意的方法。此外，我们还设计了一个动态上下文适应模型来进一步提高 OFRS 的效率和性能。通过两个大型数据集的离线实验和为期一周的在线实验，验证了该模型的可行性和优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+the+Spatiotemporal+Features+of+Online+Food+Recommendation+Service)|-1|
|[MDKG: Graph-Based Medical Knowledge-Guided Dialogue Generation](https://doi.org/10.1145/3539618.3592019)|Usman Naseem, Surendrabikram Thapa, Qi Zhang, Liang Hu, Mehwish Nasim|University of Sydney, Sydney, NSW, Australia; Tongji University, Shanghai, China; Virginia Tech, Blacksburg, VA, USA; The University of Western Australia & Flinders University, Perth, WA, Australia|Medical dialogue systems (MDS) have shown promising abilities to diagnose through a conversation with a patient like a human doctor would. However, current systems are mostly based on sequence modeling, which does not account for medical knowledge. This makes the systems more prone to misdiagnosis in case of diseases with limited information. To overcome this issue, we present MDKG, an end-to-end dialogue system for medical dialogue generation (MDG) specifically designed to adapt to new diseases by quickly learning and evolving a meta-knowledge graph that allows it to reason about disease-symptom correlations. Our approach relies on a medical knowledge graph to extract disease-symptom relationships and uses a dynamic graph-based meta-learning framework to learn how to evolve the given knowledge graph to reason about disease-symptom correlations. Our approach incorporates medical knowledge and hence reduces the need for a large number of dialogues. Evaluations show that our system outperforms existing approaches when tested on benchmark datasets.|医学对话系统(MDS)已经显示出通过与病人交谈来诊断的有前途的能力，就像人类医生会做的那样。然而，目前的系统大多基于序列建模，没有考虑到医学知识。这使得系统在信息有限的情况下更容易误诊疾病。为了克服这个问题，我们提出了 MDKG，一个用于医疗对话生成(MDG)的端到端对话系统，专门设计用于通过快速学习和发展元知识图来适应新的疾病，使其能够推理疾病-症状相关性。我们的方法依赖于医学知识图来提取疾病-症状关系，并使用基于动态图的元学习框架来学习如何进化给定的知识图以推断疾病-症状相关性。我们的方法结合了医学知识，因此减少了对大量对话的需要。评估表明，当在基准数据集上进行测试时，我们的系统优于现有的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MDKG:+Graph-Based+Medical+Knowledge-Guided+Dialogue+Generation)|-1|
