# SIGIR2022 Paper List

|论文|作者|组织|摘要|翻译|代码|引用数|
|---|---|---|---|---|---|---|
|[Hypergraph Contrastive Collaborative Filtering](https://doi.org/10.1145/3477495.3532058)|Lianghao Xia, Chao Huang, Yong Xu, Jiashu Zhao, Dawei Yin, Jimmy X. Huang|South China University Of Technology, Guangzhou, China; York University, Toronto, Canada; University of Hong Kong, Hong Kong, Hong Kong; Wilfrid Laurier University, Waterloo, Canada; Baidu, Beijing, China|Collaborative Filtering (CF) has emerged as fundamental paradigms for parameterizing users and items into latent representation space, with their correlative patterns from interaction data. Among various CF techniques, the development of GNN-based recommender systems, e.g., PinSage and LightGCN, has offered the state-of-the-art performance. However, two key challenges have not been well explored in existing solutions: i) The over-smoothing effect with deeper graph-based CF architecture, may cause the indistinguishable user representations and degradation of recommendation results. ii) The supervision signals (i.e., user-item interactions) are usually scarce and skewed distributed in reality, which limits the representation power of CF paradigms. To tackle these challenges, we propose a new self-supervised recommendation framework Hypergraph Contrastive Collaborative Filtering (HCCF) to jointly capture local and global collaborative relations with a hypergraph-enhanced cross-view contrastive learning architecture. In particular, the designed hypergraph structure learning enhances the discrimination ability of GNN-based CF paradigm, in comprehensively capturing the complex high-order dependencies among users. Additionally, our HCCF model effectively integrates the hypergraph structure encoding with self-supervised learning to reinforce the representation quality of recommender systems, based on the hypergraph self-discrimination. Extensive experiments on three benchmark datasets demonstrate the superiority of our model over various state-of-the-art recommendation methods, and the robustness against sparse user interaction data. The implementation codes are available at https://github.com/akaxlh/HCCF.|协同过滤(CF)已经成为将用户和项目参数化为潜在表征空间的基本范例，其相关模式来自交互数据。在各种 CF 技术中，基于 GNN 的推荐系统(如 PinSage 和 LightGCN)的开发提供了最先进的性能。然而，在现有的解决方案中，有两个关键的挑战还没有得到很好的探索: i)基于更深层次的基于图的 CF 架构的过度平滑效应，可能导致难以区分的用户表示和推荐结果的退化。监督信号(即用户-项目交互)在现实生活中往往是稀缺的、偏态分布的，这限制了 CF 范式的表示能力。为了应对这些挑战，我们提出了一个新的自我监督推荐框架 Hypergraph 对比度协同过滤(hCCF) ，通过一个超图增强的跨视图对比度学习架构，联合捕捉本地和全球的协作关系。特别是，所设计的超图结构学习提高了基于 GNN 的 CF 范式的识别能力，能够全面捕获用户之间复杂的高阶依赖关系。此外，我们的 HCCF 模型有效地将超图结构编码与自监督学习相结合，以增强基于超图自辨识的推荐系统的表示质量。在三个基准数据集上的大量实验表明，该模型优于各种最新的推荐方法，并且对稀疏用户交互数据具有鲁棒性。实施守则可于 https://github.com/akaxlh/hccf 索取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hypergraph+Contrastive+Collaborative+Filtering)|22|
|[Are Graph Augmentations Necessary?: Simple Graph Contrastive Learning for Recommendation](https://doi.org/10.1145/3477495.3531937)|Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, Quoc Viet Hung Nguyen|The University of Queensland, Brisbane, QLD, Australia; Shandong University, Jinan, China; Griffith University, Gold Coast, Australia|Contrastive learning (CL) recently has spurred a fruitful line of research in the field of recommendation, since its ability to extract self-supervised signals from the raw data is well-aligned with recommender systems' needs for tackling the data sparsity issue. A typical pipeline of CL-based recommendation models is first augmenting the user-item bipartite graph with structure perturbations, and then maximizing the node representation consistency between different graph augmentations. Although this paradigm turns out to be effective, what underlies the performance gains is still a mystery. In this paper, we first experimentally disclose that, in CL-based recommendation models, CL operates by learning more uniform user/item representations that can implicitly mitigate the popularity bias. Meanwhile, we reveal that the graph augmentations, which used to be considered necessary, just play a trivial role. Based on this finding, we propose a simple CL method which discards the graph augmentations and instead adds uniform noises to the embedding space for creating contrastive views. A comprehensive experimental study on three benchmark datasets demonstrates that, though it appears strikingly simple, the proposed method can smoothly adjust the uniformity of learned representations and has distinct advantages over its graph augmentation-based counterparts in terms of recommendation accuracy and training efficiency. The code is released at https://github.com/Coder-Yu/QRec.|对比学习(CL)最近在推荐领域激发了一系列富有成效的研究，因为它从原始数据中提取自我监督信号的能力与推荐系统解决数据稀疏问题的需求非常一致。基于 CL 的推荐模型的一个典型流水线是首先用结构扰动对用户项二分图进行扩展，然后使不同图扩展之间的节点表示一致性最大化。尽管这个范例被证明是有效的，但是性能提升的基础是什么仍然是一个谜。在本文中，我们首先通过实验揭示了在基于 CL 的推荐模型中，CL 是通过学习更加统一的用户/项目表示来实现的，这种表示可以隐含地减少流行偏差。同时，我们发现，图的增广，过去被认为是必要的，只是起到一个微不足道的作用。基于这一发现，我们提出了一种简单的 CL 方法，该方法抛弃了图的增广，而是在嵌入空间中加入均匀的噪声来创建对比视图。通过对三个基准数据集的全面实验研究表明，该方法虽然看似简单，但能够平滑地调整学习表示的一致性，在推荐精度和训练效率方面明显优于基于图增强的推荐方法。密码在 https://github.com/coder-yu/qrec 发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Are+Graph+Augmentations+Necessary?:+Simple+Graph+Contrastive+Learning+for+Recommendation)|20|
|[From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective](https://doi.org/10.1145/3477495.3531857)|Thibault Formal, Carlos Lassance, Benjamin Piwowarski, Stéphane Clinchant|Naver Labs Europe, Meylan, France; Naver Labs Europe / Sorbonne Université, ISIR, Meylan, France; Sorbonne Université, ISIR / CNRS, Paris, France|Neural retrievers based on dense representations combined with Approximate Nearest Neighbors search have recently received a lot of attention, owing their success to distillation and/or better sampling of examples for training -- while still relying on the same backbone architecture. In the meantime, sparse representation learning fueled by traditional inverted indexing techniques has seen a growing interest, inheriting from desirable IR priors such as explicit lexical matching. While some architectural variants have been proposed, a lesser effort has been put in the training of such models. In this work, we build on SPLADE -- a sparse expansion-based retriever -- and show to which extent it is able to benefit from the same training improvements as dense models, by studying the effect of distillation, hard-negative mining as well as the Pre-trained Language Model initialization. We furthermore study the link between effectiveness and efficiency, on in-domain and zero-shot settings, leading to state-of-the-art results in both scenarios for sufficiently expressive models.|基于密集表示结合近似最近邻搜索的神经检索器最近受到了很多关注，因为它们的成功归功于提取和/或更好的训练样本采样——同时仍然依赖于相同的骨干架构。与此同时，传统的反向索引技术所激发的稀疏表示学习受到了越来越多的关注，它继承了外显词汇匹配等可取的信息检索先验。虽然已经提出了一些体系结构变体，但在这类模型的培训方面投入的努力较少。在这项工作中，我们建立在 SPLADE ——一个基于稀疏扩展的检索器——的基础上，通过研究蒸馏、硬负面挖掘以及预训练语言模型初始化的效果，展示了它在多大程度上能够从与密集模型相同的训练改进中受益。我们进一步研究了效率和效果之间的联系，在领域内和零射击设置，导致国家的最先进的结果，在两种情况下充分表达模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Distillation+to+Hard+Negative+Sampling:+Making+Sparse+Neural+IR+Models+More+Effective)|10|
|[Multi-level Cross-view Contrastive Learning for Knowledge-aware Recommender System](https://doi.org/10.1145/3477495.3532025)|Ding Zou, Wei Wei, XianLing Mao, Ziyang Wang, Minghui Qiu, Feida Zhu, Xin Cao|Beijing Institute of Technology, Beijing, China; Singapore Management University, Singapore, Singapore; The University of New South Wales, Sydney, NSW, Australia; Alibaba Group, Hangzhou, China; Huazhong University of Science and Technology, Wuhan, China|Knowledge graph (KG) plays an increasingly important role in recommender systems. Recently, graph neural networks (GNNs) based model has gradually become the theme of knowledge-aware recommendation (KGR). However, there is a natural deficiency for GNN-based KGR models, that is, the sparse supervised signal problem, which may make their actual performance drop to some extent. Inspired by the recent success of contrastive learning in mining supervised signals from data itself, in this paper, we focus on exploring the contrastive learning in KG-aware recommendation and propose a novel multi-level cross-view contrastive learning mechanism, named MCCLK. Different from traditional contrastive learning methods which generate two graph views by uniform data augmentation schemes such as corruption or dropping, we comprehensively consider three different graph views for KG-aware recommendation, including global-level structural view, local-level collaborative and semantic views. Specifically, we consider the user-item graph as a collaborative view, the item-entity graph as a semantic view, and the user-item-entity graph as a structural view. MCCLK hence performs contrastive learning across three views on both local and global levels, mining comprehensive graph feature and structure information in a self-supervised manner. Besides, in semantic view, a k-Nearest-Neighbor (k NN) item-item semantic graph construction module is proposed, to capture the important item-item semantic relation which is usually ignored by previous work. Extensive experiments conducted on three benchmark datasets show the superior performance of our proposed method over the state-of-the-arts. The implementations are available at: https://github.com/CCIIPLab/MCCLK.|知识图在推荐系统中起着越来越重要的作用。近年来，基于图神经网络(GNN)的知识推荐模型逐渐成为知识感知推荐(KGR)的主题。然而，基于 GNN 的 KGR 模型存在一个自然的缺陷，即稀疏监督信号问题，这可能使其实际性能有所下降。受近年来对比学习在数据挖掘中的成功启发，本文重点研究了 KG 推荐中的对比学习，并提出了一种新的多层次跨视图对比学习机制 MCCLK。与传统的对比学习方法不同，传统的对比学习方法通过统一的数据增强方案(如腐败或丢弃)生成两个图形视图，我们综合考虑了三种不同的 KG 感知推荐图形视图，包括全局层面的结构视图、局部层面的协作视图和语义视图。具体来说，我们将用户项目图视为协作视图，将项目实体图视为语义视图，将用户项目实体图视为结构视图。因此，MCCLK 在局部和全局层面上对三种视图进行对比学习，以自监督的方式挖掘综合图形特征和结构信息。此外，在语义视图中，提出了一个 k 近邻(k NN)项目-项目语义图的构造模块，以捕获以往工作中经常被忽略的重要项目-项目语义关系。在三个基准数据集上进行的大量实验表明，我们提出的方法的性能优于目前的技术水平。有关实施方案可参阅以下 https://github.com/cciiplab/mcclk  :。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-level+Cross-view+Contrastive+Learning+for+Knowledge-aware+Recommender+System)|10|
|[Knowledge Graph Contrastive Learning for Recommendation](https://doi.org/10.1145/3477495.3532009)|Yuhao Yang, Chao Huang, Lianghao Xia, Chenliang Li|University of Hong Kong, Hong Kong, Hong Kong; Wuhan University, Wuhan, China; University of Hong Kong, Hong Kong, China|Knowledge Graphs (KGs) have been utilized as useful side information to improve recommendation quality. In those recommender systems, knowledge graph information often contains fruitful facts and inherent semantic relatedness among items. However, the success of such methods relies on the high quality knowledge graphs, and may not learn quality representations with two challenges: i) The long-tail distribution of entities results in sparse supervision signals for KG-enhanced item representation; ii) Real-world knowledge graphs are often noisy and contain topic-irrelevant connections between items and entities. Such KG sparsity and noise make the item-entity dependent relations deviate from reflecting their true characteristics, which significantly amplifies the noise effect and hinders the accurate representation of user's preference. To fill this research gap, we design a general Knowledge Graph Contrastive Learning framework (KGCL) that alleviates the information noise for knowledge graph-enhanced recommender systems. Specifically, we propose a knowledge graph augmentation schema to suppress KG noise in information aggregation, and derive more robust knowledge-aware representations for items. In addition, we exploit additional supervision signals from the KG augmentation process to guide a cross-view contrastive learning paradigm, giving a greater role to unbiased user-item interactions in gradient descent and further suppressing the noise. Extensive experiments on three public datasets demonstrate the consistent superiority of our KGCL over state-of-the-art techniques. KGCL also achieves strong performance in recommendation scenarios with sparse user-item interactions, long-tail and noisy KG entities. Our implementation codes are available at https://github.com/yuh-yang/KGCL-SIGIR22.|知识图(KG)已经被用作提高推荐质量的有用的辅助信息。在这些推荐系统中，知识图信息往往包含丰富的事实和项目之间固有的语义关系。然而，这种方法的成功依赖于高质量的知识图，并且可能不会学习质量表示法，这有两个挑战: i)实体的长尾分布导致 KG 增强的项目表示的稀疏监督信号; ii)真实世界的知识图通常是噪音的，并且包含项目和实体之间的主题不相关的连接。这种 KG 稀疏性和噪声使得项目-实体依赖关系偏离了真实特征，显著放大了噪声效应，阻碍了用户偏好的准确表达。为了填补这一研究空白，我们设计了一个通用的知识图对比学习框架(KGCL) ，以减轻知识图增强的推荐系统的信息噪声。具体地说，我们提出了一个知识图增强模式来抑制信息聚合中的 KG 噪声，并且得到了更加健壮的知识感知项表示。此外，我们利用来自 KG 增强过程的额外监督信号来指导横向对比学习范式，让无偏见的用户项目交互在梯度下降法中发挥更大的作用，并进一步抑制噪音。在三个公共数据集上的大量实验表明，我们的 KGCL 相对于最先进的技术具有一致的优越性。KGCL 还可以在用户-项目交互稀疏、 KG 实体长尾和噪声较大的推荐场景中获得强大的性能。我们的执行守则可在 https://github.com/yuh-yang/kgcl-sigir22索取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Graph+Contrastive+Learning+for+Recommendation)|10|
|[Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding](https://doi.org/10.1145/3477495.3531757)|Mingyang Chen, Wen Zhang, Yushan Zhu, Hongting Zhou, Zonggang Yuan, Changliang Xu, Huajun Chen|State Key Laboratory of Media Convergence Production Technology and Systems, Beijing, China; Huawei Technologies Co., Ltd., Nanjing, China; Zhejiang University, Hangzhou, China; Zhejiang University & Alibaba-Zhejiang University Joint Institute of Frontier Technologies, Hangzhou, China|Knowledge graphs (KGs) consisting of a large number of triples have become widespread recently, and many knowledge graph embedding (KGE) methods are proposed to embed entities and relations of a KG into continuous vector spaces. Such embedding methods simplify the operations of conducting various in-KG tasks (e.g., link prediction) and out-of-KG tasks (e.g., question answering). They can be viewed as general solutions for representing KGs. However, existing KGE methods are not applicable to inductive settings, where a model trained on source KGs will be tested on target KGs with entities unseen during model training. Existing works focusing on KGs in inductive settings can only solve the inductive relation prediction task. They can not handle other out-of-KG tasks as general as KGE methods since they don't produce embeddings for entities. In this paper, to achieve inductive knowledge graph embedding, we propose a model MorsE, which does not learn embeddings for entities but learns transferable meta-knowledge that can be used to produce entity embeddings. Such meta-knowledge is modeled by entity-independent modules and learned by meta-learning. Experimental results show that our model significantly outperforms corresponding baselines for in-KG and out-of-KG tasks in inductive settings.|由大量三元组构成的知识图(KG)近年来得到了广泛的应用，为了将 KG 的实体和关系嵌入到连续向量空间中，提出了许多知识图嵌入(KGE)方法。这种嵌入方法简化了进行各种内 KG 任务(例如，链接预测)和外 KG 任务(例如，问题回答)的操作。他们可以被视为代表幼稚园的一般解决方案。然而，现有的 KGE 方法不适用于归纳环境，在这种情况下，以源幼儿园为培训对象的模型将在模型培训过程中看不到实体的目标幼儿园中进行测试。现有的针对归纳环境下幼儿园的工作只能解决归纳关系预测任务。它们不能像 KGE 方法那样处理其他超出 KG 的任务，因为它们不为实体生成嵌入。为了实现归纳知识图的嵌入，本文提出了一种 MorsE 模型，该模型不学习实体的嵌入，而是学习可转移的元知识，从而生成实体的嵌入。这种元知识由独立于实体的模块建模，并通过元学习来学习。实验结果表明，在感应环境下，我们的模型对于在 KG 内和在 KG 外的任务的性能明显优于相应的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Meta-Knowledge+Transfer+for+Inductive+Knowledge+Graph+Embedding)|8|
|[CPFair: Personalized Consumer and Producer Fairness Re-ranking for Recommender Systems](https://doi.org/10.1145/3477495.3531959)|Mohammadmehdi Naghiaei, Hossein A. Rahmani, Yashar Deldjoo|University of Southern California, California, CA, USA; University College London, London, United Kingdom; Polytechnic University of Bari, Bari, Italy|Recently, there has been a rising awareness that when machine learning (ML) algorithms are used to automate choices, they may treat/affect individuals unfairly, with legal, ethical, or economic consequences. Recommender systems are prominent examples of such ML systems that assist users in making high-stakes judgments. A common trend in the previous literature research on fairness in recommender systems is that the majority of works treat user and item fairness concerns separately, ignoring the fact that recommender systems operate in a two-sided marketplace. In this work, we present an optimization-based re-ranking approach that seamlessly integrates fairness constraints from both the consumer and producer-side in a joint objective framework. We demonstrate through large-scale experiments on 8 datasets that our proposed method is capable of improving both consumer and producer fairness without reducing overall recommendation quality, demonstrating the role algorithms may play in minimizing data biases.|最近，人们越来越意识到，当机器学习(ML)算法被用于自动选择时，它们可能会不公平地对待/影响个人，产生法律、道德或经济后果。推荐系统是这种机器学习系统的突出例子，它可以帮助用户做出高风险的判断。在以往关于推荐系统公平性的文献研究中，一个普遍的趋势是，大多数研究将用户和项目的公平性问题分开处理，忽略了推荐系统在双边市场中运行的事实。在这项工作中，我们提出了一个基于优化的重新排序方法，在一个联合目标框架中无缝地整合来自消费者和生产者方面的公平约束。我们通过在8个数据集上的大规模实验证明了我们提出的方法能够在不降低整体推荐质量的情况下提高消费者和生产者的公平性，并且证明了算法在最小化数据偏差方面可能发挥的作用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CPFair:+Personalized+Consumer+and+Producer+Fairness+Re-ranking+for+Recommender+Systems)|7|
|[Curriculum Learning for Dense Retrieval Distillation](https://doi.org/10.1145/3477495.3531791)|Hansi Zeng, Hamed Zamani, Vishwa Vinay|Adobe Research, Bangalore, AA, India; University of Massachusetts Amherst, Amherst, MA, USA|Recent work has shown that more effective dense retrieval models can be obtained by distilling ranking knowledge from an existing base re-ranking model. In this paper, we propose a generic curriculum learning based optimization framework called CL-DRD that controls the difficulty level of training data produced by the re-ranking (teacher) model. CL-DRD iteratively optimizes the dense retrieval (student) model by increasing the difficulty of the knowledge distillation data made available to it. In more detail, we initially provide the student model coarse-grained preference pairs between documents in the teacher's ranking, and progressively move towards finer-grained pairwise document ordering requirements. In our experiments, we apply a simple implementation of the CL-DRD framework to enhance two state-of-the-art dense retrieval models. Experiments on three public passage retrieval datasets demonstrate the effectiveness of our proposed framework.|最近的研究表明，从现有的基础重排序模型中提取排序知识可以获得更有效的稠密检索模型。本文提出了一个基于课程学习的通用优化框架 CL-DRD，该框架控制重新排序(教师)模型产生的训练数据的难易程度。CL-DRD 通过增加提供给它的知识提取数据的难度，迭代优化了密集检索(学生)模型。更详细地说，我们首先在教师排名中提供文档之间的学生模型粗粒度偏好对，然后逐步向更细粒度的成对文档排序需求转移。在我们的实验中，我们应用了一个简单的 CL-DRD 框架实现来增强两个最先进的稠密检索模型。在三个公共通道检索数据集上的实验证明了该框架的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Curriculum+Learning+for+Dense+Retrieval+Distillation)|7|
|[Constructing Better Evaluation Metrics by Incorporating the Anchoring Effect into the User Model](https://doi.org/10.1145/3477495.3531953)|Nuo Chen, Fan Zhang, Tetsuya Sakai|Wuhan University, Wuhan, China; Waseda University, Tokyo, Japan|Models of existing evaluation metrics assume that users are rational decision-makers trying to pursue maximised utility. However, studies in behavioural economics show that people are not always rational when making decisions. Previous studies showed that the anchoring effect can influence the relevance judgement of a document. In this paper, we challenge the rational user assumption and introduce the anchoring effect into user models. We first propose a framework for query-level evaluation metrics by incorporating the anchoring effect into the user model. In the framework, the magnitude of the anchoring effect is related to the quality of the previous document. We then apply our framework to several query-level evaluation metrics and compare them with their vanilla version as the baseline in terms of user satisfaction on a publicly available search dataset. As a result, our Anchoring-aware Metrics (AMs) outperformed their baselines in term of correlation with user satisfaction. The result suggests that we can better predict user query satisfaction feedbacks by incorporating the anchoring effect into user models of existing evaluating metrics. As far as we know, we are the first to introduce the anchoring effect into information retrieval evaluation metrics. Our findings provide a perspective from behavioural economics to better understand user behaviour and satisfaction in search interaction.|现有的评估指标模型假设用户是理性的决策者，试图追求效用最大化。然而，行为经济学的研究表明，人们在做决定时并不总是理性的。先前的研究表明，锚定效应可以影响文档的相关性判断。在本文中，我们挑战了理性的用户假设，并将锚定效应引入到用户模型中。我们首先通过将锚定效应整合到用户模型中，提出了一个查询级评估度量的框架。在框架中，锚定效应的大小与前一份文件的质量有关。然后，我们将框架应用于几个查询级评估指标，并将它们与普通版本进行比较，作为公开可用搜索数据集上用户满意度的基线。因此，我们的锚定感知度量(AMs)在与用户满意度的相关性方面表现优于他们的基线。结果表明，我们可以更好地预测用户查询满意度反馈，将锚定效应纳入用户模型的现有评估指标。据我们所知，我们是第一个将锚定效应引入信息检索评估指标的公司。我们的研究结果提供了一个从行为经济学的角度来更好地理解用户行为和搜索互动的满意度。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Constructing+Better+Evaluation+Metrics+by+Incorporating+the+Anchoring+Effect+into+the+User+Model)|7|
|[Graph Trend Filtering Networks for Recommendation](https://doi.org/10.1145/3477495.3531985)|Wenqi Fan, Xiaorui Liu, Wei Jin, Xiangyu Zhao, Jiliang Tang, Qing Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Trend+Filtering+Networks+for+Recommendation)|6|
|[An Efficiency Study for SPLADE Models](https://doi.org/10.1145/3477495.3531833)|Carlos Lassance, Stéphane Clinchant|Naver Labs Europe, Meylan, France|Latency and efficiency issues are often overlooked when evaluating IR models based on Pretrained Language Models (PLMs) in reason of multiple hardware and software testing scenarios. Nevertheless, efficiency is an important part of such systems and should not be overlooked. In this paper, we focus on improving the efficiency of the SPLADE model since it has achieved state-of-the-art zero-shot performance and competitive results on TREC collections. SPLADE efficiency can be controlled via a regularization factor, but solely controlling this regularization has been shown to not be efficient enough. In order to reduce the latency gap between SPLADE and traditional retrieval systems, we propose several techniques including L1 regularization for queries, a separation of document/query encoders, a FLOPS-regularized middle-training, and the use of faster query encoders. Our benchmark demonstrates that we can drastically improve the efficiency of these models while increasing the performance metrics on in-domain data. To our knowledge, we propose the first neural models that, under the same computing constraints, achieve similar latency (less than 4ms difference) as traditional BM25, while having similar performance (less than 10% [email protected] reduction) as the state-of-the-art single-stage neural rankers on in-domain data.|在评估基于预训练语言模型(PLM)的 IR 模型时，由于存在多种硬件和软件测试场景，延迟和效率问题常常被忽视。然而，效率是这些系统的一个重要组成部分，不应该被忽视。在本文中，我们关注于提高 SPLADE 模型的效率，因为它已经实现了最先进的零拍性能和 TREC 集合的竞争结果。SPLADE 的效率可以通过一个正则化因子来控制，但仅仅控制这个正则化因子是不够有效的。为了减少 SPLADE 与传统检索系统之间的延迟差距，我们提出了几种技术，包括查询的 L1正则化、文档/查询编码器的分离、 FLOPS 正则化的中间训练以及使用更快的查询编码器。我们的基准测试表明，我们可以大大提高这些模型的效率，同时提高域内数据的性能指标。据我们所知，我们提出了第一个神经模型，在相同的计算约束下，实现与传统 BM25相似的延迟(小于4ms 的差异) ，同时具有与领域内数据的最先进的单阶段神经排序相似的性能(少于10% [电子邮件保护]减少)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Efficiency+Study+for+SPLADE+Models)|6|
|[Improving Conversational Recommender Systems via Transformer-based Sequential Modelling](https://doi.org/10.1145/3477495.3531852)|Jie Zou, Evangelos Kanoulas, Pengjie Ren, Zhaochun Ren, Aixin Sun, Cheng Long|Shandong University, Qingdao, China; University of Amsterdam, Amsterdam, Netherlands; [email protected]|In Conversational Recommender Systems (CRSs), conversations usually involve a set of related items and entities e.g., attributes of items. These items and entities are mentioned in order following the development of a dialogue. In other words, potential sequential dependencies exist in conversations. However, most of the existing CRSs neglect these potential sequential dependencies. In this paper, we propose a Transformer-based sequential conversational recommendation method, named TSCR, which models the sequential dependencies in the conversations to improve CRS. We represent conversations by items and entities, and construct user sequences to discover user preferences by considering both mentioned items and entities. Based on the constructed sequences, we deploy a Cloze task to predict the recommended items along a sequence. Experimental results demonstrate that our TSCR model significantly outperforms state-of-the-art baselines.|在会话推荐系统(CRS)中，会话通常涉及一组相关的项目和实体，例如，项目的属性。这些项目和实体将在对话发展之后按顺序提及。换句话说，潜在的顺序依赖性存在于会话中。然而，大多数现有的 CRS 忽略了这些潜在的顺序依赖关系。本文提出了一种基于变压器的顺序会话推荐方法 TSCR，该方法通过建立会话中的顺序依赖关系来改进 CRS。我们通过条目和实体表示会话，并通过考虑上述条目和实体构造用户序列来发现用户偏好。基于构造的序列，我们部署完形填空任务来预测一个序列中推荐的项目。实验结果表明，我们的 TSCR 模型明显优于最先进的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Conversational+Recommender+Systems+via+Transformer-based+Sequential+Modelling)|5|
|[BERT-ER: Query-specific BERT Entity Representations for Entity Ranking](https://doi.org/10.1145/3477495.3531944)|Shubham Chatterjee, Laura Dietz|University of New Hampshire, Durham, NH, USA|Entity-oriented search systems often learn vector representations of entities via the introductory paragraph from the Wikipedia page of the entity. As such representations are the same for every query, our hypothesis is that the representations are not ideal for IR tasks. In this work, we present BERT Entity Representations (BERT-ER) which are query-specific vector representations of entities obtained from text that describes how an entity is relevant for a query. Using BERT-ER in a downstream entity ranking system, we achieve a performance improvement of 13-42% (Mean Average Precision) over a system that uses the BERT embedding of the introductory paragraph from Wikipedia on two large-scale test collections. Our approach also outperforms entity ranking systems using entity embeddings from Wikipedia2Vec, ERNIE, and E-BERT. We show that our entity ranking system using BERT-ER can increase precision at the top of the ranking by promoting relevant entities to the top. With this work, we release our BERT models and query-specific entity embeddings fine-tuned for the entity ranking task.|面向实体的搜索系统通常通过实体的 Wikipedia 页面中的介绍性段落学习实体的向量表示。由于这种表示对于每个查询都是相同的，我们的假设是，这种表示对于 IR 任务来说并不理想。在这项工作中，我们提出了 BERT 实体表示(BERT-ER) ，它是从文本中获得的实体的特定于查询的向量表示，描述了一个实体如何与一个查询相关。在下游实体排名系统中使用 BERT-ER，我们比在两个大规模测试集合中使用 BERT 嵌入的介绍性段落的系统获得了13-42% 的性能提高(均值平均精度)。我们的方法也优于使用 Wikipedia2Vec、 ERNIE 和 E-BERT 的实体嵌入的实体排序系统。结果表明，使用 BERT-ER 的实体排名系统可以通过将相关实体提升到顶端来提高排名的精度。通过这项工作，我们发布了我们的 BERT 模型和针对实体排序任务的查询特定实体嵌入。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BERT-ER:+Query-specific+BERT+Entity+Representations+for+Entity+Ranking)|5|
|[Reduce, Reuse, Recycle: Green Information Retrieval Research](https://doi.org/10.1145/3477495.3531766)|Harrisen Scells, Shengyao Zhuang, Guido Zuccon|The University of Queensland, Brisbane, QLD, Australia|Recent advances in Information Retrieval utilise energy-intensive hardware to produce state-of-the-art results. In areas of research highly related to Information Retrieval, such as Natural Language Processing and Machine Learning, there have been efforts to quantify and reduce the power and emissions produced by methods that depend on such hardware. Research that is conscious of the environmental impacts of its experimentation and takes steps to mitigate some of these impacts is considered 'Green'. Given the continuous demand for more data and power-hungry techniques, Green research is likely to become more important within the broader research community. Therefore, within the Information Retrieval community, the consequences of non-Green (in other words, Red) research should at least be considered and acknowledged. As such, the aims of this perspective paper are fourfold: (1) to review the Green literature not only for Information Retrieval but also for related domains in order to identify transferable Green techniques; (2) to provide measures for quantifying the power usage and emissions of Information Retrieval research; (3) to report the power usage and emission impacts for various current IR methods; and (4) to provide a framework to guide Green Information Retrieval research, taking inspiration from 'reduce, reuse, recycle' waste management campaigns, including salient examples from the literature that implement these concepts.|最近在信息检索方面的进展利用能源密集型硬件来产生最先进的结果。在与信息检索高度相关的研究领域，如自然语言处理和机器学习，一直在努力量化和减少依赖这些硬件的方法所产生的功率和排放。意识到实验对环境的影响并采取措施减轻其中一些影响的研究被认为是“绿色”的。鉴于对更多数据和耗电技术的持续需求，绿色研究可能在更广泛的研究领域变得更加重要。因此，在信息检索社区内，非绿色(换句话说，红色)研究的后果至少应该被考虑和承认。因此，本文件的目的有四个: (1)不仅为信息检索而且为相关领域查阅绿色文献，以便确定可转用的绿色技术; (2)提供量化信息检索研究的用电量和排放量的措施; (3)报告各种现行红外线方法的用电量和排放影响; (4)提供一个框架，以指导绿色信息检索研究，从“减少、再使用、回收”废物管理运动中获得启发，包括实施这些概念的文献中的突出例子。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reduce,+Reuse,+Recycle:+Green+Information+Retrieval+Research)|5|
|[On-Device Next-Item Recommendation with Self-Supervised Knowledge Distillation](https://doi.org/10.1145/3477495.3531775)|Xin Xia, Hongzhi Yin, Junliang Yu, Qinyong Wang, Guandong Xu, Quoc Viet Hung Nguyen|The University of Queensland, Brisbane, QLD, Australia; Griffith University, Gold Coast, Australia; Baidu Inc., Beijing, China; University of Technology Sydney, Sydney, NSW, Australia|Session-based recommender systems (SBR) are becoming increasingly popular because they can predict user interests without relying on long-term user profile and support login-free recommendation. Modern recommender systems operate in a fully server-based fashion. To cater to millions of users, the frequent model maintaining and the high-speed processing for concurrent user requests are required, which comes at the cost of a huge carbon footprint. Meanwhile, users need to upload their behavior data even including the immediate environmental context to the server, raising the public concern about privacy. On-device recommender systems circumvent these two issues with cost-conscious settings and local inference. However, due to the limited memory and computing resources, on-device recommender systems are confronted with two fundamental challenges: (1) how to reduce the size of regular models to fit edge devices? (2) how to retain the original capacity? Previous research mostly adopts tensor decomposition techniques to compress regular recommendation models with low compression rates so as to avoid drastic performance degradation. In this paper, we explore ultra-compact models for next-item recommendation, by loosing the constraint of dimensionality consistency in tensor decomposition. To compensate for the capacity loss caused by compression, we develop a self-supervised knowledge distillation framework which enables the compressed model (student) to distill the essential information lying in the raw data, and improves the long-tail item recommendation through an embedding-recombination strategy with the original model (teacher). The extensive experiments on two benchmarks demonstrate that, with 30x size reduction, the compressed model almost comes with no accuracy loss, and even outperforms its uncompressed counterpart. The code is released at https://github.com/xiaxin1998/OD-Rec.|基于会话的推荐系统正变得越来越流行，因为它们可以预测用户的兴趣而不依赖于长期的用户配置文件和支持免登录的推荐。现代推荐系统以完全基于服务器的方式运行。为了满足数百万用户的需求，需要频繁的模型维护和对并发用户请求的高速处理，这需要付出巨大的碳足印。与此同时，用户需要上传他们的行为数据，甚至包括即时的环境背景到服务器，引起公众对隐私的关注。设备上的推荐系统通过成本意识设置和本地推断来规避这两个问题。然而，由于有限的内存和计算资源，设备上的推荐系统面临着两个基本的挑战: (1)如何减少常规模型的大小，以适应边缘设备？(2)如何保留原有能力？以往的研究大多采用张量分解技术对低压缩率的常规推荐模型进行压缩，以避免性能的急剧下降。本文通过放松张量分解中维度一致性的约束，探索了下一项推荐的超紧模型。为了弥补压缩带来的容量损失，提出了一种自监督知识提取框架，使压缩模型(学生)能够提取原始数据中的关键信息，并通过与原始模型(教师)的嵌入-重组策略改进长尾项目推荐。在两个基准上的大量实验表明，在缩小了30倍尺寸的情况下，压缩模型几乎没有精度损失，甚至优于未压缩模型。密码在 https://github.com/xiaxin1998/od-rec 发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On-Device+Next-Item+Recommendation+with+Self-Supervised+Knowledge+Distillation)|5|
|[DisenCDR: Learning Disentangled Representations for Cross-Domain Recommendation](https://doi.org/10.1145/3477495.3531967)|Jiangxia Cao, Xixun Lin, Xin Cong, Jing Ya, Tingwen Liu, Bin Wang|Institute of Information Engineering, Chinese Academy of Sciences & University of Chinese Academy of Sciences, Beijing, China; Xiaomi Inc., Beijing, China|Data sparsity is a long-standing problem in recommender systems. To alleviate it, Cross-Domain Recommendation (CDR) has attracted a surge of interests, which utilizes the rich user-item interaction information from the related source domain to improve the performance on the sparse target domain. Recent CDR approaches pay attention to aggregating the source domain information to generate better user representations for the target domain. However, they focus on designing more powerful interaction encoders to learn both domains simultaneously, but fail to model different user preferences of different domains. Particularly, domain-specific preferences of the source domain usually provide useless information to enhance the performance in the target domain, and directly aggregating the domain-shared and domain-specific information together maybe hurts target domain performance. This work considers a key challenge of CDR: How do we transfer shared information across domains? Grounded in the information theory, we propose DisenCDR, a novel model to disentangle the domain-shared and domain-specific information. To reach our goal, we propose two mutual-information-based disentanglement regularizers. Specifically, an exclusive regularizer aims to enforce the user domain-shared representations and domain-specific representations encoding exclusive information. An information regularizer is to encourage the user domain-shared representations encoding predictive information for both domains. Based on them, we further derive a tractable bound of our disentanglement objective to learn desirable disentangled representations. Extensive experiments show that DisenCDR achieves significant improvements over state-of-the-art baselines on four real-world datasets.|数据稀疏是推荐系统中一个长期存在的问题。为了解决这一问题，跨域推荐技术(CDR)引起了人们的极大兴趣，它利用来自相关源域的丰富的用户项交互信息来提高稀疏目标域的性能。最近的 CDR 方法注重聚合源域信息，以便为目标域生成更好的用户表示。然而，他们专注于设计更强大的交互编码器来同时学习这两个域，但未能建模不同域的不同用户偏好。特别地，源域的特定领域偏好通常提供无用的信息来提高目标域的性能，直接将共享的和特定领域的信息聚合在一起可能会损害目标域的性能。这项工作认为 CDR 的一个关键挑战是: 我们如何跨域传输共享信息？在信息论的基础上，提出了一种新的区分领域共享信息和特定领域信息的模型 DisenCDR。为了达到这个目标，我们提出了两个基于互信息的解纠缠正则化器。具体地说，独占正则化程序旨在强制用户域共享表示和编码独占信息的特定于域的表示。信息正则化器是鼓励用户域共享表示，对两个域的预测信息进行编码。在此基础上，我们进一步推导出我们的解纠缠目标的一个易处理的界，以学习理想的解纠缠表示。大量的实验表明，DisenCDR 在四个真实世界的数据集上比最先进的基线获得了显著的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DisenCDR:+Learning+Disentangled+Representations+for+Cross-Domain+Recommendation)|5|
|[Personalized Fashion Compatibility Modeling via Metapath-guided Heterogeneous Graph Learning](https://doi.org/10.1145/3477495.3532038)|Weili Guan, Fangkai Jiao, Xuemeng Song, Haokun Wen, ChungHsing Yeh, Xiaojun Chang|Shandong University, Qingdao, China; University of Technology Sydney, Sydney, NSW, Australia; Monash University, Melbourne, VIC, Australia|Fashion Compatibility Modeling (FCM) is a new yet challenging task, which aims to automatically access the matching degree among a set of complementary items. Most of existing methods evaluate the fashion compatibility from the common perspective, but overlook the user's personal preference. Inspired by this, a few pioneers study the Personalized Fashion Compatibility Modeling (PFCM). Despite their significance, these PFCM methods mainly concentrate on the user and item entities, as well as their interactions, but ignore the attribute entities, which contain rich semantics. To address this problem, we propose to fully explore the related entities and their relations involved in PFCM to boost the PFCM performance. This is, however, non-trivial due to the heterogeneous contents of different entities, embeddings for new users, and various high-order relations. Towards these ends, we present a novel metapath-guided personalized fashion compatibility modeling, dubbed as MG-PFCM. In particular, we creatively build a heterogeneous graph to unify the three types of entities (i.e., users, items, and attributes) and their relations (i.e., user-item interactions, item-item matching relations, and item-attribute association relations). Thereafter, we design a multi-modal content-oriented user embedding module to learn user representations by inheriting the contents of their interacted items. Meanwhile, we define the user-oriented and item-oriented metapaths, and perform the metapath-guided heterogeneous graph learning to enhance the user and item embeddings. In addition, we introduce the contrastive regularization to improve the model performance. We conduct extensive experiments on the real-world benchmark dataset, which verifies the superiority of our proposed scheme over several cutting-edge baselines. As a byproduct, we have released our source codes to benefit other researchers.|时尚相容性建模(FCM)是一项新兴的具有挑战性的任务，其目标是自动获取一组互补项之间的匹配度。大多数现有的方法从一般的角度评估时尚兼容性，但是忽略了用户的个人偏好。受此启发，一些先驱者开始研究个性化时尚兼容性建模(PFCM)。尽管其重要性，这些 PFCM 方法主要集中在用户和项目实体，以及它们的交互，但是忽略了属性实体，其中包含丰富的语义。为了解决这一问题，我们建议充分探讨 PFCM 中涉及的相关实体及其关系，以提高 PFCM 的性能。然而，由于不同实体的内容异构、新用户的嵌入以及各种高阶关系，这是非常重要的。为此，我们提出了一种新的元路径引导的个性化时尚兼容性模型，称为 MG-PFCM。特别是，我们创造性地构建了一个异构图来统一三种类型的实体(即用户、项目和属性)及其关系(即用户-项目交互、项目-项目匹配关系和项目-属性关系)。然后，我们设计了一个面向多模态内容的用户嵌入模块，通过继承用户交互项的内容来学习用户表示。同时，定义了面向用户和面向项目的元路径，并通过元路径引导的异构图学习来增强用户和项目的嵌入。此外，为了提高模型的性能，我们引入了对比正则化方法。我们在现实世界的基准数据集上进行了广泛的实验，验证了我们提出的方案相对于几个前沿基线的优越性。作为一个副产品，我们已经发布了我们的源代码，以利于其他研究人员。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalized+Fashion+Compatibility+Modeling+via+Metapath-guided+Heterogeneous+Graph+Learning)|5|
|[Explainable Fairness in Recommendation](https://doi.org/10.1145/3477495.3531973)|Yingqiang Ge, Juntao Tan, Yan Zhu, Yinglong Xia, Jiebo Luo, Shuchang Liu, Zuohui Fu, Shijie Geng, Zelong Li, Yongfeng Zhang|Rutgers University, New Brunswick, NJ, USA; University of Rochester, Rochester, NY, USA; Meta Platforms, Inc., palo alto, CA, USA|Existing research on fairness-aware recommendation has mainly focused on the quantification of fairness and the development of fair recommendation models, neither of which studies a more substantial problem--identifying the underlying reason of model disparity in recommendation. This information is critical for recommender system designers to understand the intrinsic recommendation mechanism and provides insights on how to improve model fairness to decision makers. Fortunately, with the rapid development of Explainable AI, we can use model explainability to gain insights into model (un)fairness. In this paper, we study the problem ofexplainable fairness, which helps to gain insights about why a system is fair or unfair, and guides the design of fair recommender systems with a more informed and unified methodology. Particularly, we focus on a common setting with feature-aware recommendation and exposure unfairness, but the proposed explainable fairness framework is general and can be applied to other recommendation settings and fairness definitions. We propose a Counterfactual Explainable Fairness framework, called CEF, which generates explanations about model fairness that can improve the fairness without significantly hurting the performance. The CEF framework formulates an optimization problem to learn the "minimal'' change of the input features that changes the recommendation results to a certain level of fairness. Based on the counterfactual recommendation result of each feature, we calculate an explainability score in terms of the fairness-utility trade-off to rank all the feature-based explanations, and select the top ones as fairness explanations. Experimental results on several real-world datasets validate that our method is able to effectively provide explanations to the model disparities and these explanations can achieve better fairness-utility trade-off when using them for recommendation than all the baselines.|现有关于公平意识推荐的研究主要集中在公平性的量化和公平推荐模型的发展两个方面，这两个方面都没有研究更实质性的问题——确定推荐模型差异的根本原因。这些信息对于推荐系统设计者理解内在的推荐机制至关重要，并为如何提高模型对决策者的公平性提供了见解。幸运的是，随着可解释人工智能的快速发展，我们可以利用模型可解释性来深入了解模型(非)公平性。本文通过对解释公平问题的研究，有助于深入了解一个系统为什么是公平或不公平的，并以一种更为知情和统一的方法论指导公平推荐系统的设计。特别地，我们关注的是一个具有特征感知推荐和暴露不公平性的公共设置，但是提出的可解释公平性框架是通用的，可以应用于其他推荐设置和公平性定义。我们提出了一个反事实可解释的公平性框架，称为 CEF，它生成了对模型公平性的解释，可以在不严重损害绩效的情况下提高公平性。基金框架制定了一个最佳化问题，让学生了解输入功能的「最小」改变，会令推荐结果有一定程度的公平性。基于每个特征的反事实推荐结果，以公平-效用权衡的方式计算可解释性得分，对所有基于特征的解释进行排序，并选择最高的解释作为公平解释。在实际数据集上的实验结果验证了该方法能够有效地解释模型间的差异，并且这些解释在使用它们进行推荐时能够比所有基线更好地实现公平-效用的权衡。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explainable+Fairness+in+Recommendation)|5|
|[Measuring Fairness in Ranked Results: An Analytical and Empirical Comparison](https://doi.org/10.1145/3477495.3532018)|Amifa Raj, Michael D. Ekstrand|Boise State University, Boise, ID, USA|Information access systems, such as search and recommender systems, often use ranked lists to present results believed to be relevant to the user's information need. Evaluating these lists for their fairness along with other traditional metrics provides a more complete understanding of an information access system's behavior beyond accuracy or utility constructs. To measure the (un)fairness of rankings, particularly with respect to the protected group(s) of producers or providers, several metrics have been proposed in the last several years. However, an empirical and comparative analyses of these metrics showing the applicability to specific scenario or real data, conceptual similarities, and differences is still lacking. We aim to bridge the gap between theoretical and practical ap-plication of these metrics. In this paper we describe several fair ranking metrics from the existing literature in a common notation, enabling direct comparison of their approaches and assumptions, and empirically compare them on the same experimental setup and data sets in the context of three information access tasks. We also provide a sensitivity analysis to assess the impact of the design choices and parameter settings that go in to these metrics and point to additional work needed to improve fairness measurement.|信息访问系统，例如搜索和推荐系统，经常使用排名列表来显示被认为与用户信息需求相关的结果。评估这些列表的公平性以及其他传统指标，可以更全面地了解信息访问系统的行为，而不仅仅是准确性或效用结构。为了衡量排名的(不)公平性，特别是对于受保护的生产者或提供者群体，在过去几年中已经提出了几个指标。然而，对这些指标的实证和比较分析表明适用于具体情景或实际数据，概念上的相似性和差异仍然缺乏。我们的目标是弥合这些指标的理论和实际应用之间的差距。在本文中，我们描述了几个公平排序度量从现有的文献在一个共同的符号，使直接比较他们的方法和假设，并经验比较他们在相同的实验设置和数据集在三个信息访问任务的背景下。我们还提供了一个敏感度分析来评估设计选择和参数设置对这些指标的影响，并指出改善公平性度量所需的额外工作。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Measuring+Fairness+in+Ranked+Results:+An+Analytical+and+Empirical+Comparison)|5|
|[Webformer: Pre-training with Web Pages for Information Retrieval](https://doi.org/10.1145/3477495.3532086)|Yu Guo, Zhengyi Ma, Jiaxin Mao, Hongjin Qian, Xinyu Zhang, Hao Jiang, Zhao Cao, Zhicheng Dou|Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; Gaoling School of Artificial Intelligence, Renmin University of China & Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China; Distributed and Parallel Software Lab, Huawei, Beijing, China|Pre-trained language models (PLMs) have achieved great success in the area of Information Retrieval. Studies show that applying these models to ad-hoc document ranking can achieve better retrieval effectiveness. However, on the Web, most information is organized in the form of HTML web pages. In addition to the pure text content, the structure of the content organized by HTML tags is also an important part of the information delivered on a web page. Currently, such structured information is totally ignored by pre-trained models which are trained solely based on text content. In this paper, we propose to leverage large-scale web pages and their DOM (Document Object Model) tree structures to pre-train models for information retrieval. We argue that using the hierarchical structure contained in web pages, we can get richer contextual information for training better language models. To exploit this kind of information, we devise four pre-training objectives based on the structure of web pages, then pre-train a Transformer model towards these tasks jointly with traditional masked language model objective. Experimental results on two authoritative ad-hoc retrieval datasets prove that our model can significantly improve ranking performance compared to existing pre-trained models.|经过培训的语言模型(PLM)在信息检索领域取得了巨大的成功。研究表明，将这些模型应用于自组织文档排序可以获得更好的检索效果。然而，在 Web 上，大多数信息是以 HTML 网页的形式组织的。除了纯文本内容之外，HTML 标签组织的内容结构也是网页信息传递的重要组成部分。目前，这种结构化信息完全被预先训练的模型所忽视，而这些模型仅仅基于文本内容进行训练。在本文中，我们建议利用大规模网页及其 DOM (文档对象模型)树结构来预训练信息检索模型。我们认为，利用网页所包含的层次结构，我们可以获得更丰富的上下文信息来训练更好的语言模型。为了充分利用这种信息，我们根据网页的结构设计了四个预训练目标，然后结合传统的掩蔽语言模型目标，针对这些目标预训练了一个变换器模型。在两个权威的自组织检索数据集上的实验结果表明，与已有的预训练模型相比，该模型可以显著提高排序性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Webformer:+Pre-training+with+Web+Pages+for+Information+Retrieval)|5|
|[BARS: Towards Open Benchmarking for Recommender Systems](https://doi.org/10.1145/3477495.3531723)|Jieming Zhu, Quanyu Dai, Liangcai Su, Rong Ma, Jinyang Liu, Guohao Cai, Xi Xiao, Rui Zhang|Tsinghua University, Beijing, China; Huawei Noah's Ark Lab, Shenzhen, China; ruizhang.info, Shenzhen, China; Tsinghua University, Shenzhen, China; The Chinese University of Hong Kong, Hong Kong, China|The past two decades have witnessed the rapid development of personalized recommendation techniques. Despite the significant progress made in both research and practice of recommender systems, to date, there is a lack of a widely-recognized benchmarking standard in this field. Many of the existing studies perform model evaluations and comparisons in an ad-hoc manner, for example, by employing their own private data splits or using a different experimental setting. However, such conventions not only increase the difficulty in reproducing existing studies, but also lead to inconsistent experimental results among them. This largely limits the credibility and practical value of research results in this field. To tackle these issues, we present an initiative project aimed for open benchmarking for recommender systems. In contrast to some earlier attempts towards this goal, we take one further step by setting up a standardized benchmarking pipeline for reproducible research, which integrates all the details about datasets, source code, hyper-parameter settings, running logs, and evaluation results. The benchmark is designed with comprehensiveness and sustainability in mind. It spans both matching and ranking tasks, and also allows anyone to easily follow and contribute. We believe that our benchmark could not only reduce the redundant efforts of researchers to re-implement or re-run existing baselines, but also drive more solid and reproducible research on recommender systems.|近二十年来，个性化推荐技术发展迅速。尽管在推荐系统的研究和实践方面取得了重大进展，但迄今为止，在这一领域还缺乏一个得到广泛认可的基准标准。许多现有的研究以临时方式进行模型评估和比较，例如，通过使用自己的私人数据分割或使用不同的实验设置。然而，这些惯例不仅增加了复制现有研究的难度，而且导致它们之间的实验结果不一致。这在很大程度上限制了该领域研究成果的可信度和实用价值。为了解决这些问题，我们提出了一个倡议项目，旨在为推荐系统开放基准测试。与之前的一些尝试相比，我们更进一步，为可重复性研究建立了一个标准化的基准管道，其中集成了关于数据集、源代码、超参数设置、运行日志和评估结果的所有细节。基准的设计考虑了全面性和可持续性。它跨越了匹配和排序任务，并且允许任何人轻松地跟随和贡献。我们相信，我们的基准不仅可以减少研究人员重新实现或重新运行现有基准的冗余工作，而且还可以推动对推荐系统进行更可靠和可重复的研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BARS:+Towards+Open+Benchmarking+for+Recommender+Systems)|5|
|[V2P: Vision-to-Prompt based Multi-Modal Product Summary Generation](https://doi.org/10.1145/3477495.3532076)|Xuemeng Song, Liqiang Jing, Dengtian Lin, Zhongzhou Zhao, Haiqing Chen, Liqiang Nie|Shandong University, Qingdao, China; Alibaba Group, Hangzhou, China|Multi-modal Product Summary Generation is a new yet challenging task, which aims to generate a concise and readable summary for a product given its multi-modal content, e.g., its long text description and image. Although existing methods have achieved great success, they still suffer from three key limitations: 1) overlook the benefit of pre-training, 2) lack the representation-level supervision, and 3) ignore the diversity of the seller-generated data. To address these limitations, in this work, we propose a Vision-to-Prompt based multi-modal product summary generation framework, dubbed as V2P, where a Generative Pre-trained Language Model (GPLM) is adopted as the backbone. In particular, to maintain the original text capability of the GPLM and fully utilize the high-level concepts contained in the product image, we design V2P with two key components: vision-based prominent attribute prediction, and attribute prompt-guided summary generation. The first component works on obtaining the vital semantic attributes of the product from its image by the Swin Transformer, while the second component aims to generate the summary based on the product's long text description and the attribute prompts yielded by the first component with a GPLM. Towards comprehensive supervision over the second component, apart from the conventional output-level supervision, we introduce the representation-level regularization. Meanwhile, we design the data augmentation-based robustness regularization to handle the diverse inputs and improve the robustness of the second component. Extensive experiments on a large-scale Chinese dataset verify the superiority of our model over cutting-edge methods.|多模态产品摘要生成是一项新的具有挑战性的任务，其目标是根据产品的多模态内容(如长文本描述和图像)为其生成一个简明易读的摘要。虽然现有的方法已经取得了巨大的成功，但仍然存在三个关键的局限性: 1)忽视了预训练的好处，2)缺乏表征级别的监督，3)忽视了销售者生成的数据的多样性。为了解决这些局限性，在本文中，我们提出了一个基于视觉到提示的多模态产品摘要生成框架，称为 V2P，其中采用了生成预训练语言模型(GPLM)作为骨干。特别是，为了保持 GPLM 的原始文本能力，充分利用产品图像中包含的高层次概念，我们设计了 V2P 的两个关键组件: 基于视觉的突出属性预测和属性提示引导的摘要生成。第一个组件的工作是通过 Swin former 从产品图像中获取重要的语义属性，而第二个组件的目标是基于产品的长文本描述和由第一个组件通过 GPLM 产生的属性提示生成摘要。对于第二部分的综合监管，除了传统的产出层面的监管外，我们还引入了代表层面的规范化。同时，我们设计了基于数据增强的鲁棒正则化方法来处理不同的输入，提高了第二分量的鲁棒性。通过在大规模中文数据集上的大量实验，验证了该模型相对于前沿方法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=V2P:+Vision-to-Prompt+based+Multi-Modal+Product+Summary+Generation)|5|
|[A Weakly Supervised Propagation Model for Rumor Verification and Stance Detection with Multiple Instance Learning](https://doi.org/10.1145/3477495.3531930)|Ruichao Yang, Jing Ma, Hongzhan Lin, Wei Gao|Singapore Management University, Singapore, Singapore; Beijing University of Posts and Telecommunications, Hong Kong Baptist University, Beijing, Hong Kong SAR, China; Hong Kong Baptist University, Hong Kong SAR, Hong Kong|The diffusion of rumors on social media generally follows a propagation tree structure, which provides valuable clues on how an original message is transmitted and responded by users over time. Recent studies reveal that rumor verification and stance detection are two relevant tasks that can jointly enhance each other despite their differences. For example, rumors can be debunked by cross-checking the stances conveyed by their relevant posts, and stances are also conditioned on the nature of the rumor. However, stance detection typically requires a large training set of labeled stances at post level, which are rare and costly to annotate. Enlightened by Multiple Instance Learning (MIL) scheme, we propose a novel weakly supervised joint learning framework for rumor verification and stance detection which only requires bag-level class labels concerning the rumor's veracity. Specifically, based on the propagation trees of source posts, we convert the two multi-class problems into multiple MIL-based binary classification problems where each binary model is focused on differentiating a target class (of rumor or stance) from the remaining classes. Then, we propose a hierarchical attention mechanism to aggregate the binary predictions, including (1) a bottom-up/top-down tree attention layer to aggregate binary stances into binary veracity; and (2) a discriminative attention layer to aggregate the binary class into finer-grained classes. Extensive experiments conducted on three Twitter-based datasets demonstrate promising performance of our model on both claim-level rumor detection and post-level stance classification compared with state-of-the-art methods.|谣言在社交媒体上的传播通常遵循传播树结构，这种结构提供了有价值的线索，说明原始信息是如何随着时间的推移被用户传播和回应的。最近的研究表明，谣言验证和姿态识别是两个相互关联的任务，尽管它们之间存在差异，但可以相互促进。例如，谣言可以通过反复核对其相关帖子所传达的立场来揭穿，而立场也取决于谣言的性质。然而，姿态检测通常需要在后期水平上进行大量标记姿态的训练，这种训练很少，而且注释成本很高。在多实例学习(MIL)方案的启发下，我们提出了一种新的弱监督联合学习框架，用于谣言验证和姿态检测，该框架只需要袋级类标签就可以验证谣言的真实性。具体来说，基于源文章的传播树，我们将这两个多类问题转换为多个基于 MIL 的二进制分类问题，其中每个二进制模型的重点是区分目标类(谣言或立场)与其余类。然后，我们提出了一个分层的注意机制来聚集二元预测，包括(1)一个自下而上/自上而下的注意树层来聚集二元立场成为二元准确性; (2)一个区分性的注意层来聚集二元类成为更细粒度的类。在三个基于 Twitter 的数据集上进行的大量实验表明，与最先进的方法相比，我们的模型在索赔级谣言检测和后级立场分类方面具有良好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Weakly+Supervised+Propagation+Model+for+Rumor+Verification+and+Stance+Detection+with+Multiple+Instance+Learning)|5|
|[Positive, Negative and Neutral: Modeling Implicit Feedback in Session-based News Recommendation](https://doi.org/10.1145/3477495.3532040)|Shansan Gong, Kenny Q. Zhu|Shanghai Jiao Tong University, Shanghai, China|News recommendation for anonymous readers is a useful but challenging task for many news portals, where interactions between readers and articles are limited within a temporary login session. Previous works tend to formulate session-based recommendation as a next item prediction task, while they neglect the implicit feedback from user behaviors, which indicates what users really like or dislike. Hence, we propose a comprehensive framework to model user behaviors through positive feedback (i.e., the articles they spend more time on) and negative feedback (i.e., the articles they choose to skip without clicking in). Moreover, the framework implicitly models the user using their session start time, and the article using its initial publishing time, in what we call neutral feedback. Empirical evaluation on three real-world news datasets shows the framework's promising performance of more accurate, diverse and even unexpectedness recommendations than other state-of-the-art session-based recommendation approaches.|对于许多新闻门户网站来说，为匿名读者推荐新闻是一项有用但具有挑战性的任务，因为在临时登录会话中，读者和文章之间的交互受到限制。以往的研究倾向于将基于会话的推荐作为下一个项目的预测任务，而忽略了来自用户行为的隐性反馈，这表明用户真正喜欢或不喜欢什么。因此，我们提出了一个全面的框架，通过积极反馈(即，他们花费更多时间的文章)和消极反馈(即，他们选择不点击跳过的文章)来模拟用户行为。此外，框架使用用户的会话开始时间隐式地对用户进行建模，并使用文章的初始发布时间隐式地对文章进行建模，我们称之为中性反馈。对三个真实世界新闻数据集的实证评估表明，与其他基于会话的最新推荐方法相比，该框架具有更准确、更多样、甚至更出人意料的推荐性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Positive,+Negative+and+Neutral:+Modeling+Implicit+Feedback+in+Session-based+News+Recommendation)|4|
|[Multi-Behavior Sequential Transformer Recommender](https://doi.org/10.1145/3477495.3532023)|Enming Yuan, Wei Guo, Zhicheng He, Huifeng Guo, Chengkai Liu, Ruiming Tang|Shanghai Jiao Tong University, Shanghai, China; Noah's Ark Lab, Huawei, Shenzhen, China; Tsinghua University, Beijing, China|In most real-world recommender systems, users interact with items in a sequential and multi-behavioral manner. Exploring the fine-grained relationship of items behind the users' multi-behavior interactions is critical in improving the performance of recommender systems. Despite the great successes, existing methods seem to have limitations on modelling heterogeneous item-level multi-behavior dependencies, capturing diverse multi-behavior sequential dynamics, or alleviating data sparsity problems. In this paper, we show it is possible to derive a framework to address all the above three limitations. The proposed framework MB-STR, a Multi-Behavior Sequential Transformer Recommender, is equipped with the multi-behavior transformer layer (MB-Trans), the multi-behavior sequential pattern generator (MB-SPG) and the behavior-aware prediction module (BA-Pred). Compared with a typical transformer, we design MB-Trans to capture multi-behavior heterogeneous dependencies as well as behavior-specific semantics, propose MB-SPG to encode the diverse sequential patterns among multiple behaviors, and incorporate BA-Pred to better leverage multi-behavior supervision. Comprehensive experiments on three real-world datasets show the effectiveness of MB-STR by significantly boosting the recommendation performance compared with various competitive baselines. Further ablation studies demonstrate the superiority of different modules of MB-STR.|在大多数现实世界的推荐系统中，用户以一种顺序和多行为的方式与项目交互。探索用户多行为交互背后的细粒度关系对于提高推荐系统的性能至关重要。尽管取得了巨大的成功，但现有的方法似乎在建模异构项目级多行为依赖、捕获多种多行为顺序动态或缓解数据稀疏问题方面存在局限性。在本文中，我们展示了推导一个框架来解决上述三个限制是可能的。提出的多行为顺序变压器推荐系统框架 MB-STR 具有多行为顺序变压器层(MB-Trans)、多行为顺序模式发生器(MB-SPG)和行为感知预测模块(BA-Pred)。与典型的变压器相比，我们设计 MB-Trans 来捕获多行为异构依赖和行为特定的语义，提出 MB-SPG 来编码多行为之间的不同序列模式，并结合 BA-Pred 来更好地利用多行为监督。在三个实际数据集上的综合实验表明，与不同的竞争基线相比，MB-STR 能够显著提高推荐性能。进一步的消融研究证明了 MB-STR 不同模块的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Behavior+Sequential+Transformer+Recommender)|4|
|[News Recommendation with Candidate-aware User Modeling](https://doi.org/10.1145/3477495.3531778)|Tao Qi, Fangzhao Wu, Chuhan Wu, Yongfeng Huang|Microsoft Research Asia, Beijing, China; Tsinghua University, Beijing, China|News recommendation aims to match news with personalized user interest. Existing methods for news recommendation usually model user interest from historical clicked news without the consideration of candidate news. However, each user usually has multiple interests, and it is difficult for these methods to accurately match a candidate news with a specific user interest. In this paper, we present a candidate-aware user modeling method for personalized news recommendation, which can incorporate candidate news into user modeling for better matching between candidate news and user interest. We propose a candidate-aware self-attention network that uses candidate news as clue to model candidate-aware global user interest. In addition, we propose a candidate-aware CNN network to incorporate candidate news into local behavior context modeling and learn candidate-aware short-term user interest. Besides, we use a candidate-aware attention network to aggregate previously clicked news weighted by their relevance with candidate news to build candidate-aware user representation. Experiments on real-world datasets show the effectiveness of our method in improving news recommendation performance.|新闻推荐旨在使新闻与个性化的用户兴趣相匹配。现有的新闻推荐方法通常根据历史点击新闻建立用户兴趣模型，而不考虑候选新闻。然而，每个用户通常有多个兴趣，这些方法很难准确地匹配一个候选新闻和一个特定的用户兴趣。提出了一种基于候选者感知的个性化新闻推荐用户建模方法，该方法将候选新闻融入到用户建模中，以更好地匹配候选新闻和用户兴趣。我们提出了一个以候选新闻为线索的候选感知自我注意网络，该网络对全球候选感知用户兴趣进行建模。此外，我们提出了一个候选人感知的 CNN 网络，将候选人新闻纳入本地行为上下文建模，并学习候选人感知的短期用户兴趣。此外，我们利用一个候选人感知的注意网络来聚合先前点击过的新闻，并根据它们与候选人新闻的相关性来加权，从而建立一个候选人感知的用户表示。在实际数据集上的实验结果表明了该方法在提高新闻推荐性能方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=News+Recommendation+with+Candidate-aware+User+Modeling)|4|
|[Price DOES Matter!: Modeling Price and Interest Preferences in Session-based Recommendation](https://doi.org/10.1145/3477495.3532043)|Xiaokun Zhang, Bo Xu, Liang Yang, Chenliang Li, Fenglong Ma, Haifeng Liu, Hongfei Lin|Pennsylvania State University, Pennsylvania, PA, USA; Dalian University of Technology, Dalian, China; Wuhan University, Wuhan, China|Session-based recommendation aims to predict items that an anonymous user would like to purchase based on her short behavior sequence. The current approaches towards session-based recommendation only focus on modeling users' interest preferences, while they all ignore a key attribute of an item, i.e., the price. Many marketing studies have shown that the price factor significantly influences users' behaviors and the purchase decisions of users are determined by both price and interest preferences simultaneously. However, it is nontrivial to incorporate price preferences for session-based recommendation. Firstly, it is hard to handle heterogeneous information from various features of items to capture users' price preferences. Secondly, it is difficult to model the complex relations between price and interest preferences in determining user choices. To address the above challenges, we propose a novel method Co-guided Heterogeneous Hypergraph Network (CoHHN) for session-based recommendation. Towards the first challenge, we devise a heterogeneous hypergraph to represent heterogeneous information and rich relations among them. A dual-channel aggregating mechanism is then designed to aggregate various information in the heterogeneous hypergraph. After that, we extract users' price preferences and interest preferences via attention layers. As to the second challenge, a co-guided learning scheme is designed to model the relations between price and interest preferences and enhance the learning of each other. Finally, we predict user actions based on item features and users' price and interest preferences. Extensive experiments on three real-world datasets demonstrate the effectiveness of the proposed CoHHN. Further analysis reveals the significance of price for session-based recommendation.|基于会话的推荐旨在根据匿名用户的短行为序列预测其想要购买的商品。当前基于会话的推荐方法只关注于建模用户的兴趣偏好，而他们都忽略了一个项目的关键属性，即价格。许多市场研究表明，价格因素对用户行为有显著影响，用户的购买决策同时由价格和利益偏好决定。然而，为基于会话的推荐引入价格偏好并非易事。首先，很难处理来自商品不同特征的异构信息来捕捉用户的价格偏好;。其次，在决定用户选择时，很难建立价格和利益偏好之间的复杂关系模型。针对上述挑战，我们提出了一种基于会话的协同引导异构超图网络(CoHHN)的推荐方法。针对第一个挑战，我们设计了一个异构超图来表示异构信息和它们之间的丰富关系。然后设计了一种双通道聚集机制来聚集异构超图中的各种信息。然后，通过注意层提取用户的价格偏好和兴趣偏好。针对第二个挑战，设计了一个共同导向学习方案，以建立价格和兴趣偏好之间的关系模型，并加强相互之间的学习。最后，根据商品特征以及用户的价格和兴趣偏好对用户行为进行预测。在三个真实世界数据集上的大量实验证明了所提出的 CoHHN 算法的有效性。进一步的分析揭示了基于会话的推荐价格的重要性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Price+DOES+Matter!:+Modeling+Price+and+Interest+Preferences+in+Session-based+Recommendation)|4|
|[CORE: Simple and Effective Session-based Recommendation within Consistent Representation Space](https://doi.org/10.1145/3477495.3531955)|Yupeng Hou, Binbin Hu, Zhiqiang Zhang, Wayne Xin Zhao|Renmin University of China, Beijing, China; Ant Group, Hangzhou, China; Renmin University of China & Beijing Academy of Artificial Intelligence, Beijing, China|Session-based Recommendation (SBR) refers to the task of predicting the next item based on short-term user behaviors within an anonymous session. However, session embedding learned by a non-linear encoder is usually not in the same representation space as item embeddings, resulting in the inconsistent prediction issue while recommending items. To address this issue, we propose a simple and effective framework named CORE, which can unify the representation space for both the encoding and decoding processes. Firstly, we design a representation-consistent encoder that takes the linear combination of input item embeddings as session embedding, guaranteeing that sessions and items are in the same representation space. Besides, we propose a robust distance measuring method to prevent overfitting of embeddings in the consistent representation space. Extensive experiments conducted on five public real-world datasets demonstrate the effectiveness and efficiency of the proposed method. The code is available at: https://github.com/RUCAIBox/CORE.|基于会话的推荐(Session-based 汪汪)是指基于匿名会话中的短期用户行为来预测下一个项目的任务。然而，非线性编码器学习的会话嵌入通常与项目嵌入不在同一表示空间，在推荐项目时会产生不一致的预测问题。为了解决这个问题，我们提出了一个简单有效的框架 CORE，它可以统一编码和解码过程的表示空间。首先，我们设计了一个表示一致的编码器，将输入项嵌入的线性组合作为会话嵌入，保证会话和项处于相同的表示空间。此外，我们提出了一种鲁棒的距离测量方法，以防止过拟合嵌入在一致的表示空间。在五个公共实际数据集上进行的大量实验表明了该方法的有效性和高效性。密码可于以下 https://github.com/rucaibox/core 索取:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CORE:+Simple+and+Effective+Session-based+Recommendation+within+Consistent+Representation+Space)|4|
|[Unlearning Protected User Attributes in Recommendations with Adversarial Training](https://doi.org/10.1145/3477495.3531820)|Christian Ganhör, David Penz, Navid Rekabsaz, Oleg Lesota, Markus Schedl|Johannes Kepler University Linz & Linz Institute of Technology, Linz, Austria; Johannes Kepler University Linz & TU Wien, Linz and Vienna, Austria; Johannes Kepler University Linz, Linz, Austria|Collaborative filtering algorithms capture underlying consumption patterns, including the ones specific to particular demographics or protected information of users, e.g., gender, race, and location. These encoded biases can influence the decision of a recommendation system (RS) towards further separation of the contents provided to various demographic subgroups, and raise privacy concerns regarding the disclosure of users' protected attributes. In this work, we investigate the possibility and challenges of removing specific protected information of users from the learned interaction representations of a RS algorithm, while maintaining its effectiveness. Specifically, we incorporate adversarial training into the state-of-the-art MultVAE architecture, resulting in a novel model, Adversarial Variational Auto-Encoder with Multinomial Likelihood (Adv-MultVAE), which aims at removing the implicit information of protected attributes while preserving recommendation performance. We conduct experiments on the MovieLens-1M and LFM-2b-DemoBias datasets, and evaluate the effectiveness of the bias mitigation method based on the inability of external attackers in revealing the users' gender information from the model. Comparing with baseline MultVAE, the results show that Adv-MultVAE, with marginal deterioration in performance (w.r.t. NDCG and recall), largely mitigates inherent biases in the model on both datasets.|协同过滤算法捕捉潜在的消费模式，包括特定人群或受保护的用户信息，如性别、种族和地点。这些编码偏差可以影响推荐系统(RS)对进一步分离提供给不同人口亚群的内容的决策，并引起关于用户受保护属性披露的隐私问题。在这项工作中，我们研究的可能性和挑战，删除具体的保护信息的用户从学习交互表示的 RS 算法，同时保持其有效性。具体而言，我们将对抗性训练纳入最先进的 MultVAE 体系结构，产生了一种新的模型，具有多项式似然的对抗性变分自动编码器(Adv-MultVAE) ，其目的是消除受保护属性的隐含信息，同时保持推荐性能。我们在 MovieLens-1M 和 LFM-2b-DemoBias 数据集上进行了实验，评估了基于外部攻击者无法从模型中揭示用户性别信息的偏差缓解方法的有效性。与基线 MultVAE 相比，结果显示 Adv-MultVAE 的性能边际恶化(w.r.t.NDCG 和召回)在很大程度上减轻了两个数据集上模型的固有偏差。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unlearning+Protected+User+Attributes+in+Recommendations+with+Adversarial+Training)|4|
|[Deep Multi-Representational Item Network for CTR Prediction](https://doi.org/10.1145/3477495.3531845)|Jihai Zhang, Fangquan Lin, Cheng Yang, Wei Wang|Alibaba Group, Hangzhou, China|Click-through rate (CTR) prediction is essential in the modelling of a recommender system. Previous studies mainly focus on user behavior modelling, while few of them consider candidate item representations. This makes the models strongly dependent on user representations, and less effective when user behavior is sparse. Furthermore, most existing works regard the candidate item as one fixed embedding and ignore the multi-representational characteristics of the item. To handle the above issues, we propose a Deep multi-Representational Item NetworK (DRINK) for CTR prediction. Specifically, to tackle the sparse user behavior problem, we construct a sequence of interacting users and timestamps to represent the candidate item; to dynamically capture the characteristics of the item, we propose a transformer-based multi-representational item network consisting of a multi-CLS representation submodule and contextualized global item representation submodule. In addition, we propose to decouple the time information and item behavior to avoid information overwhelming. Outputs of the above components are concatenated and fed into a MLP layer to fit the CTR. We conduct extensive experiments on real-world datasets of Amazon and the results demonstrate the effectiveness of the proposed model.|点进率(CTR)预测是建立推荐系统模型的关键。以往的研究主要集中在用户行为建模，而很少考虑候选项表示。这使得模型强烈地依赖于用户表示，并且在用户行为稀疏时效率较低。此外，现有的大多数作品都将候选项作为一个固定的嵌入，而忽略了项目的多重表征特征。针对上述问题，本文提出了一种基于深度多表征项目网络(DRINK)的点击率预测方法。为了解决稀疏用户行为问题，我们构造了一个交互用户序列和时间戳来表示候选项目; 为了动态捕获候选项目的特征，我们提出了一个基于变换器的多表示项目网络，该网络由一个多 CLS 表示子模块和上下文化的全局项目表示子模块组成。此外，我们建议解耦的时间信息和项目行为，以避免信息铺天盖地。上述组件的输出被连接并输入 MLP 层以适应 CTR。我们在亚马逊的实际数据集上进行了广泛的实验，实验结果证明了该模型的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Multi-Representational+Item+Network+for+CTR+Prediction)|4|
|[GETNext: Trajectory Flow Map Enhanced Transformer for Next POI Recommendation](https://doi.org/10.1145/3477495.3531983)|Song Yang, Jiamou Liu, Kaiqi Zhao|Univ Auckland, Auckland, New Zealand|Next POI recommendation intends to forecast users' immediate future movements given their current status and historical information, yielding great values for both users and service providers. However, this problem is perceptibly complex because various data trends need to be considered together. This includes the spatial locations, temporal contexts, user's preferences, etc. Most existing studies view the next POI recommendation as a sequence prediction problem while omitting the collaborative signals from other users. Instead, we propose a user-agnostic global trajectory flow map and a novel Graph Enhanced Transformer model (GETNext) to better exploit the extensive collaborative signals for a more accurate next POI prediction, and alleviate the cold start problem in the meantime. GETNext incorporates the global transition patterns, user's general preference, spatio-temporal context, and time-aware category embeddings together into a transformer model to make the prediction of user's future moves. With this design, our model outperforms the state-of-the-art methods with a large margin and also sheds light on the cold start challenges within the spatio-temporal involved recommendation problems.|下一个 POI 建议旨在根据用户当前的状态和历史信息预测用户近期的动向，从而为用户和服务提供商带来巨大的价值。然而，这个问题显然很复杂，因为各种数据趋势需要一起考虑。这包括空间位置、时间上下文、用户偏好等。大多数现有的研究认为下一个 POI 建议是一个序列预测问题，而忽略了来自其他用户的协作信号。相反，我们提出了一个用户无关的全局轨迹流图和一个新的图形增强变压器模型(GETNext) ，以更好地利用广泛的协作信号来更准确地预测下一个 POI，同时缓解冷启动问题。GETNext 将全局转换模式、用户的一般偏好、时空上下文和时间感知类别嵌入到一个转换器模型中，对用户的未来动向进行预测。通过这种设计，我们的模型在很大程度上优于最先进的方法，同时也揭示了涉及时空的推荐问题中的冷启动挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GETNext:+Trajectory+Flow+Map+Enhanced+Transformer+for+Next+POI+Recommendation)|4|
|[ReCANet: A Repeat Consumption-Aware Neural Network for Next Basket Recommendation in Grocery Shopping](https://doi.org/10.1145/3477495.3531708)|Mozhdeh Ariannezhad, Sami Jullien, Ming Li, Min Fang, Sebastian Schelter, Maarten de Rijke|University of Amsterdam, Amsterdam, Netherlands; Albert Heijn, Zaandam, Netherlands|Retailers such as grocery stores or e-marketplaces often have vast selections of items for users to choose from. Predicting a user's next purchases has gained attention recently, in the form of next basket recommendation (NBR), as it facilitates navigating extensive assortments for users. Neural network-based models that focus on learning basket representations are the dominant approach in the recent literature. However, these methods do not consider the specific characteristics of the grocery shopping scenario, where users shop for grocery items on a regular basis, and grocery items are repurchased frequently by the same user. In this paper, we first gain a data-driven understanding of users' repeat consumption behavior through an empirical study on six public and proprietary grocery shopping transaction datasets. We discover that, averaged over all datasets, over 54% of NBR performance in terms of recall comes from repeat items: items that users have already purchased in their history, which constitute only 1% of the total collection of items on average. A NBR model with a strong focus on previously purchased items can potentially achieve high performance. We introduce ReCANet, a repeat consumption-aware neural network that explicitly models the repeat consumption behavior of users in order to predict their next basket. ReCANet significantly outperforms state-of-the-art models for the NBR task, in terms of recall and nDCG. We perform an ablation study and show that all of the components of ReCANet contribute to its performance, and demonstrate that a user's repetition ratio has a direct influence on the treatment effect of ReCANet.|零售商，如杂货店或电子市场往往有大量的商品供用户选择。最近，预测用户的下一次购买以下一篮子推荐(NBR)的形式引起了人们的注意，因为它有助于为用户导航广泛的分类。基于神经网络的模型集中在学习篮表示是最近文献中的主要方法。但是，这些方法没有考虑杂货店购物场景的具体特征，在这个场景中，用户定期购买杂货店商品，并且杂货店商品由同一个用户频繁地重新购买。本文首先通过对六个公共和私有杂货店购物交易数据集的实证研究，获得了用户重复消费行为的数据驱动理解。我们发现，在所有数据集中，平均而言，超过54% 的 NBR 的召回性能来自重复项目: 用户在历史上已经购买的项目，平均只占项目总数的1% 。NBR 模型强调先前购买的商品，可能获得高性能。我们介绍 ReCANet，一个重复消费感知的神经网络，明确地模拟用户的重复消费行为，以便预测他们的下一个篮子。就召回和 nDCG 而言，ReCANet 在 NBR 任务中的表现明显优于最先进的模型。我们进行了一个消融研究，表明所有组件的 ReCANet 有助于其性能，并表明用户的重复率直接影响治疗效果的 ReCANet。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ReCANet:+A+Repeat+Consumption-Aware+Neural+Network+for+Next+Basket+Recommendation+in+Grocery+Shopping)|4|
|[A Review-aware Graph Contrastive Learning Framework for Recommendation](https://doi.org/10.1145/3477495.3531927)|Jie Shuai, Kun Zhang, Le Wu, Peijie Sun, Richang Hong, Meng Wang, Yong Li|Hefei University of Technology, Hefei, China; Tsinghua University, Beijing, China|Most modern recommender systems predict users' preferences with two components: user and item embedding learning, followed by the user-item interaction modeling. By utilizing the auxiliary review information accompanied with user ratings, many of the existing review-based recommendation models enriched user/item embedding learning ability with historical reviews or better modeled user-item interactions with the help of available user-item target reviews. Though significant progress has been made, we argue that current solutions for review-based recommendation suffer from two drawbacks. First, as review-based recommendation can be naturally formed as a user-item bipartite graph with edge features from corresponding user-item reviews, how to better exploit this unique graph structure for recommendation? Second, while most current models suffer from limited user behaviors, can we exploit the unique self-supervised signals in the review-aware graph to guide two recommendation components better? To this end, in this paper, we propose a novel Review-aware Graph Contrastive Learning (RGCL) framework for review-based recommendation. Specifically, we first construct a review-aware user-item graph with feature-enhanced edges from reviews, where each edge feature is composed of both the user-item rating and the corresponding review semantics. This graph with feature-enhanced edges can help attentively learn each neighbor node weight for user and item representation learning. After that, we design two additional contrastive learning tasks (i.e., Node Discrimination and Edge Discrimination) to provide self-supervised signals for the two components in recommendation process. Finally, extensive experiments over five benchmark datasets demonstrate the superiority of our proposed RGCL compared to the state-of-the-art baselines.|大多数现代推荐系统通过两个组件来预测用户的偏好: 用户和项目嵌入学习，然后是用户-项目交互建模。许多现有的基于评论的推荐模型利用辅助评论信息和用户评分，通过历史评论丰富了用户/项目嵌入学习能力，或者利用现有的用户-项目目标评价更好地模拟了用户-项目交互。虽然取得了重大进展，但我们认为，目前基于审查的建议解决方案存在两个缺陷。首先，由于基于评论的推荐可以自然地形成一个具有相应用户评论边缘特征的用户项目二分图，如何更好地利用这种独特的图结构进行推荐？其次，当前大多数模型都受到用户行为的限制，我们能否利用评论感知图中唯一的自我监督信号来更好地指导两个推荐组件？为此，本文提出了一种新的基于评论的图形对比学习(RGCL)框架。具体来说，我们首先构建一个评论感知的用户项目图，该图具有来自评论的特征增强的边缘，其中每个边缘特征由用户项目评分和相应的评论语义组成。该图具有特征增强的边，可以帮助用户认真学习每个邻居节点的权重和项目表示学习。然后，我们设计了两个额外的对比学习任务(即节点判别和边缘判别) ，为推荐过程中的两个组件提供自我监督信号。最后，通过五个基准数据集的大量实验证明了我们提出的 RGCL 相对于最先进的基准线的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Review-aware+Graph+Contrastive+Learning+Framework+for+Recommendation)|4|
|[H-ERNIE: A Multi-Granularity Pre-Trained Language Model for Web Search](https://doi.org/10.1145/3477495.3531986)|Xiaokai Chu, Jiashu Zhao, Lixin Zou, Dawei Yin|Wilfrid Laurier University, waterloo, Canada; Baidu Inc., Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China|The pre-trained language models (PLMs), such as BERT and ERNIE, have achieved outstanding performance in many natural language understanding tasks. Recently, PLMs-based Information Retrieval models have also been investigated and showed substantially state-of-the-art effectiveness, e.g., MORES, PROP and ColBERT. Moreover, most of the PLMs-based rankers only focus on a single level relevance matching (e.g., character-level), while ignore the other granularity information (e.g., words and phrases), which easily lead to the ambiguity of query understanding and inaccurate matching issues in web search. In this paper, we aim to improve the state-of-the-art PLMs ERNIE for web search, by modeling multi-granularity context information with the awareness of word importance in queries and documents. In particular, we propose a novel H-ERNIE framework, which includes a query-document analysis component and a hierarchical ranking component. The query-document analysis component has several individual modules which generate the necessary variables, such as word segmentation, word importance analysis, and word tightness analysis. Based on these variables, the importance-aware multiple-level correspondences are sent to the ranking model. The hierarchical ranking model includes a multi-layer transformer module to learn the character-level representations, a word-level matching module, and a phrase-level matching module with word importance. Each of these modules models the query and the document matching from a different perspective. Also, these levels are inherently communicated to achieve the overall accurate matching. We discuss the time complexity of the proposed framework, and show that it can be efficiently implemented in real applications. The offline and online experiments on both public data sets and a commercial search engine illustrate the effectiveness of the proposed H-ERNIE framework.|预训练语言模型(PLM) ，如 BERT 和 ERNIE，在许多自然语言理解任务中都取得了优异的成绩。最近，基于 PLM 的信息检索模型也得到了研究，并显示了最先进的有效性，例如 MORES、 PROP 和 ColBERT。此外，大多数基于 PLM 的排名只关注单一层次的相关性匹配(例如，字符层次) ，而忽略了其他粒度信息(例如，单词和短语) ，这很容易导致查询理解的模糊性和网络搜索中不准确的匹配问题。本文旨在通过建立多粒度上下文信息模型，并考虑查询和文档中词的重要性，来改进目前最先进的面向 Web 搜索的 PLM ERNIE。特别地，我们提出了一个新的 H-ERNIE 框架，它包括一个查询-文档分析组件和一个层次化排序组件。查询-文档分析组件包含几个单独的模块，这些模块生成必要的变量，如分词、词重要性分析和词紧密性分析。基于这些变量，重要性感知的多层次通信被发送到排序模型。该层次排序模型包括学习字符级表示的多层转换器模块、词级匹配模块和具有词重要性的短语级匹配模块。这些模块中的每一个都从不同的角度对查询和文档匹配进行建模。此外，这些级别是固有的沟通，以实现整体精确匹配。我们讨论了该框架的时间复杂度，并表明它可以在实际应用中得到有效的实现。对公共数据集和商业搜索引擎的离线和在线实验说明了所提出的 H-ERNIE 框架的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=H-ERNIE:+A+Multi-Granularity+Pre-Trained+Language+Model+for+Web+Search)|4|
|[GERE: Generative Evidence Retrieval for Fact Verification](https://doi.org/10.1145/3477495.3531827)|Jiangui Chen, Ruqing Zhang, Jiafeng Guo, Yixing Fan, Xueqi Cheng|ICT, CAS & University of Chinese Academy of Sciences, Beijing, China|Fact verification (FV) is a challenging task which aims to verify a claim using multiple evidential sentences from trustworthy corpora, e.g., Wikipedia. Most existing approaches follow a three-step pipeline framework, including document retrieval, sentence retrieval and claim verification. High-quality evidences provided by the first two steps are the foundation of the effective reasoning in the last step. Despite being important, high-quality evidences are rarely studied by existing works for FV, which often adopt the off-the-shelf models to retrieve relevant documents and sentences in an "index-retrieve-then-rank'' fashion. This classical approach has clear drawbacks as follows: i) a large document index as well as a complicated search process is required, leading to considerable memory and computational overhead; ii) independent scoring paradigms fail to capture the interactions among documents and sentences in ranking; iii) a fixed number of sentences are selected to form the final evidence set. In this work, we proposeGERE, the first system that retrieves evidences in a generative fashion, i.e., generating the document titles as well as evidence sentence identifiers. This enables us to mitigate the aforementioned technical issues since: i) the memory and computational cost is greatly reduced because the document index is eliminated and the heavy ranking process is replaced by a light generative process; ii) the dependency between documents and that between sentences could be captured via sequential generation process; iii) the generative formulation allows us to dynamically select a precise set of relevant evidences for each claim. The experimental results on the FEVER dataset show that GERE achieves significant improvements over the state-of-the-art baselines, with both time-efficiency and memory-efficiency.|事实验证(FV)是一项具有挑战性的任务，其目的是利用可信语料库中的多个证据句来验证一个声明，例如维基百科。大多数现有的方法遵循三步流水线框架，包括文献检索、句子检索和索赔验证。前两步提供的高质量证据是最后一步有效推理的基础。尽管高质量的证据具有重要意义，但现有的 FV 研究很少对其进行研究，往往采用现成的模型，以“先索引后排名”的方式检索相关文档和句子。这种经典的方法有以下明显的缺点: i)需要大量的文档索引以及复杂的搜索过程，导致相当大的内存和计算开销; ii)独立的评分范例不能捕获排名中文档和句子之间的相互作用; iii)选择固定数量的句子来形成最终的证据集。在这项工作中，我们提出了 Gere，第一个以生成方式检索证据的系统，即生成文档标题以及证据句标识符。这使我们能够减轻上述技术问题，因为: i)内存和计算成本大大降低，因为文档索引被消除，重排序过程被轻生成过程取代; ii)文档之间的依赖性和句子之间的依赖性可以通过顺序生成过程捕获; iii)生成公式允许我们动态选择每个索赔的相关证据的精确集合。在 FEVER 数据集上的实验结果表明，在时间效率和内存效率方面，GARE 比最先进的基线有了显著的提高。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GERE:+Generative+Evidence+Retrieval+for+Fact+Verification)|4|
|[Towards Feature Selection for Ranking and Classification Exploiting Quantum Annealers](https://doi.org/10.1145/3477495.3531755)|Maurizio Ferrari Dacrema, Fabio Moroni, Riccardo Nembrini, Nicola Ferro, Guglielmo Faggioli, Paolo Cremonesi|Politecnico Di Milano, Milano, Italy; Università degli Studi di Padova, Padova, Italy; Politecnico di Milano, ContentWise, Milano, Italy|Feature selection is a common step in many ranking, classification, or prediction tasks and serves many purposes. By removing redundant or noisy features, the accuracy of ranking or classification can be improved and the computational cost of the subsequent learning steps can be reduced. However, feature selection can be itself a computationally expensive process. While for decades confined to theoretical algorithmic papers, quantum computing is now becoming a viable tool to tackle realistic problems, in particular special-purpose solvers based on the Quantum Annealing paradigm. This paper aims to explore the feasibility of using currently available quantum computing architectures to solve some quadratic feature selection algorithms for both ranking and classification. The experimental analysis includes 15 state-of-the-art datasets. The effectiveness obtained with quantum computing hardware is comparable to that of classical solvers, indicating that quantum computers are now reliable enough to tackle interesting problems. In terms of scalability, current generation quantum computers are able to provide a limited speedup over certain classical algorithms and hybrid quantum-classical strategies show lower computational cost for problems of more than a thousand features.|在许多排序、分类或预测任务中，特征选择是一个常见的步骤，有许多用途。通过去除冗余或噪声特征，可以提高排序或分类的准确性，降低后续学习步骤的计算成本。然而，特征选择本身就是一个计算开销很大的过程。数十年来，量子计算一直局限于理论算法论文，但现在已经成为解决现实问题的可行工具，特别是基于量子退火范式的特殊用途解决方案。本文旨在探讨利用现有的量子计算结构来解决一些二次特征选择算法的排序和分类的可行性。实验分析包括15个最先进的数据集。量子计算硬件获得的有效性与经典的解决方案相当，这表明量子计算机现在足够可靠，可以处理有趣的问题。在可扩展性方面，当前的量子计算机能够比某些经典算法提供有限的加速，混合量子-经典策略显示对于超过1000个特征的问题计算成本较低。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Feature+Selection+for+Ranking+and+Classification+Exploiting+Quantum+Annealers)|4|
|[Continual Learning Dialogue Systems - Learning during Conversation](https://doi.org/10.1145/3477495.3532677)|Sahisnu Mazumder, Bing Liu|Intel Labs, Santa Clara, CA, USA; University of Illinois at Chicago, Chicago, IL, USA|Dialogue systems, commonly known as Chatbots, have gained escalating popularity in recent years due to their wide-spread applications in carrying out chit-chat conversations with users and accomplishing various tasks as personal assistants. However, they still have some major weaknesses. One key weakness is that they are typically trained from pre-collected and manually-labeled data and/or written with handcrafted rules. Their knowledge bases (KBs) are also fixed and pre-compiled by human experts. Due to the huge amount of manual effort involved, they are difficult to scale and also tend to produce many errors ought to their limited ability to understand natural language and the limited knowledge in their KBs. Thus, when these systems are deployed, the level of user satisfactory is often low. In this tutorial, we introduce and discuss methods to give chatbots the ability to continuously and interactively learn new knowledge during conversation, i.e. "on-the-job" by themselves so that as the systems chat more and more with users, they become more and more knowledgeable and improve their performance over time. The first half of the tutorial focuses on introducing the paradigm of lifelong and continual learning and discuss various related problems and challenges in conversational AI applications. In the second half, we present recent advancements on the topic, with a focus on continuous lexical and factual knowledge learning in dialogues, open-domain dialogue learning after deployment and learning of new language expressions via user interactions for language grounding applications (e.g. natural language interfaces). Finally, we conclude with a discussion on the scopes for continual conversational skill learning and present some open challenges for future research.|通常被称为聊天机器人的对话系统，由于其广泛应用于与用户进行聊天对话以及作为个人助理完成各种任务，近年来越来越受欢迎。然而，他们仍然有一些主要的弱点。一个关键的弱点是，它们通常是从预先收集和手动标记的数据和/或用手工制定的规则编写而来的。它们的知识库(KB)也是由人类专家固定和预编译的。由于涉及大量的人工操作，它们难以扩展，并且由于它们理解自然语言的能力有限和知识库中的知识有限，往往会产生许多错误。因此，在部署这些系统时，用户满意度通常很低。在本教程中，我们将介绍和讨论一些方法，让聊天机器人能够在对话过程中不断地、交互式地学习新知识，比如“在工作中”自己学习，这样随着系统与用户聊天越来越多，它们就会变得越来越有知识，并随着时间的推移提高性能。本教程的前半部分重点介绍了终身学习和持续学习的范例，并讨论了会话 AI 应用中的各种相关问题和挑战。在下半部分，我们介绍了这一主题的最新进展，重点是在对话中持续的词汇和事实知识学习，部署后的开放领域对话学习，以及通过语言基础应用程序(如自然语言界面)的用户交互学习新的语言表达式。最后，我们讨论了继续会话技能学习的范围，并对未来的研究提出了一些开放性的挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Continual+Learning+Dialogue+Systems+-+Learning+during+Conversation)|4|
|[Conversational Information Seeking: Theory and Application](https://doi.org/10.1145/3477495.3532678)|Jeffrey Dalton, Sophie Fischer, Paul Owoicho, Filip Radlinski, Federico Rossetto, Johanne R. Trippas, Hamed Zamani|University of Glasgow, Glasgow, United Kingdom; Google, London, United Kingdom; RMIT University, Melbourne, Australia; University of Massachusetts Amherst, Amherst, MA, USA|Conversational information seeking (CIS) involves interaction sequences between one or more users and an information system. Interactions in CIS are primarily based on natural language dialogue, while they may include other types of interactions, such as click, touch, and body gestures. CIS recently attracted significant attention and advancements continue to be made. This tutorial follows the content of the recent Conversational Information Seeking book authored by several of the tutorial presenters. The tutorial aims to be an introduction to CIS for newcomers to CIS in addition to the recent advanced topics and state-of-the-art approaches for students and researchers with moderate knowledge of the topic. A significant part of the tutorial is dedicated to hands-on experiences based on toolkits developed by the presenters for conversational passage retrieval and multi-modal task-oriented dialogues. The outcomes of this tutorial include theoretical and practical knowledge, including a forum to meet researchers interested in CIS.|会话信息搜索(CIS)涉及一个或多个用户与信息系统之间的交互序列。CIS 中的交互主要基于自然语言对话，而它们可能包括其他类型的交互，如点击、触摸和肢体动作。独联体最近引起了重视，并继续取得进展。本教程遵循几位教程主持人最近撰写的《会话信息搜索》一书的内容。本教程的目的是除了为学生和研究人员提供最新的高级主题和最先进的方法之外，为初到 CIS 的人介绍 CIS。本教程的一个重要部分是专门介绍基于演讲者开发的工具包的实践经验，这些工具包用于会话文章检索和面向多模态任务的对话。本教程的成果包括理论和实践知识，包括一个论坛，以满足对 CIS 感兴趣的研究人员。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Conversational+Information+Seeking:+Theory+and+Application)|4|
|[MuMiN: A Large-Scale Multilingual Multimodal Fact-Checked Misinformation Social Network Dataset](https://doi.org/10.1145/3477495.3531744)|Dan Saattrup Nielsen, Ryan McConville|University of Bristol, Bristol, United Kingdom|Misinformation is becoming increasingly prevalent on social media and in news articles. It has become so widespread that we require algorithmic assistance utilising machine learning to detect such content. Training these machine learning models require datasets of sufficient scale, diversity and quality. However, datasets in the field of automatic misinformation detection are predominantly monolingual, include a limited amount of modalities and are not of sufficient scale and quality. Addressing this, we develop a data collection and linking system (MuMiN-trawl), to build a public misinformation graph dataset (MuMiN), containing rich social media data (tweets, replies, users, images, articles, hashtags) spanning 21 million tweets belonging to 26 thousand Twitter threads, each of which have been semantically linked to 13 thousand fact-checked claims across dozens of topics, events and domains, in 41 different languages, spanning more than a decade. The dataset is made available as a heterogeneous graph via a Python package (mumin). We provide baseline results for two node classification tasks related to the veracity of a claim involving social media, and demonstrate that these are challenging tasks, with the highest macro-average F1-score being 62.55% and 61.45% for the two tasks, respectively. The MuMiN ecosystem is available at https://mumin-dataset.github.io/, including the data, documentation, tutorials and leaderboards.|虚假信息在社交媒体和新闻文章中越来越普遍。它已经变得如此广泛，以至于我们需要利用机器学习来检测这些内容的算法辅助。训练这些机器学习模型需要有足够规模、多样性和质量的数据集。然而，自动错误信息检测领域的数据集主要是单语种的，包括数量有限的模式，规模和质量都不够。为了解决这个问题，我们开发了一个数据收集和链接系统(MuMiN-trawl) ，来建立一个公共错误信息图表数据集(MuMiN) ，其中包含丰富的社交媒体数据(tweet、回复、用户、图片、文章、标签) ，跨越2100万条 tweet，属于26000条 Twitter 线程，每条线程在语义上都与13000条事实核查声明相关联，涉及几十个主题、事件和域名，使用41种不同的语言，跨越10多年。数据集可以通过 Python 包(umin)作为异构图形提供。我们提供了两个节点分类任务的基线结果相关的准确性声明涉及社会媒体，并证明这些是具有挑战性的任务，最高的宏观平均 F1得分分别为62.55% 和61.45% 的两个任务。MumiN 的生态系统可以在 https://MuMiN-dataset.github.io/上使用，包括数据、文档、教程和排行榜。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MuMiN:+A+Large-Scale+Multilingual+Multimodal+Fact-Checked+Misinformation+Social+Network+Dataset)|4|
|[A Non-Factoid Question-Answering Taxonomy](https://doi.org/10.1145/3477495.3531926)|Valeria Bolotova, Vladislav Blinov, Falk Scholer, W. Bruce Croft, Mark Sanderson|Ural Federal University, Melbourne, VIC, Australia; RMIT University, Melbourne, VIC, Australia; University of Massachusetts Amherst, Amherst, MA, USA|Non-factoid question answering (NFQA) is a challenging and under-researched task that requires constructing long-form answers, such as explanations or opinions, to open-ended non-factoid questions - NFQs. There is still little understanding of the categories of NFQs that people tend to ask, what form of answers they expect to see in return, and what the key research challenges of each category are. This work presents the first comprehensive taxonomy of NFQ categories and the expected structure of answers. The taxonomy was constructed with a transparent methodology and extensively evaluated via crowdsourcing. The most challenging categories were identified through an editorial user study. We also release a dataset of categorised NFQs and a question category classifier. Finally, we conduct a quantitative analysis of the distribution of question categories using major NFQA datasets, showing that the NFQ categories that are the most challenging for current NFQA systems are poorly represented in these datasets. This imbalance may lead to insufficient system performance for challenging categories. The new taxonomy, along with the category classifier, will aid research in the area, helping to create more balanced benchmarks and to focus models on addressing specific categories.|非事实性问题回答(NFQA)是一个具有挑战性和研究不足的任务，需要构建长形式的答案，如解释或意见，开放式非事实性问题-NFQs。对于人们倾向于询问的 NFQ 类别，他们期望看到的回答形式，以及每个类别的关键研究挑战是什么，人们仍然知之甚少。这项工作提出了第一个全面的分类 NFQ 类别和预期的答案结构。分类法采用透明的方法，并通过众包进行广泛评估。最具挑战性的类别是通过编辑用户研究确定的。我们还发布了一个分类的 NFQ 数据集和一个问题类别分类器。最后，我们使用主要的 NFQA 数据集对问题类别的分布进行了定量分析，表明对当前 NFQA 系统最具挑战性的 NFQ 类别在这些数据集中表现不佳。这种不平衡可能导致具有挑战性的类别的系统性能不足。新的分类法，连同类别分类器，将有助于该领域的研究，有助于创建更平衡的基准，并将重点放在解决特定类别的模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Non-Factoid+Question-Answering+Taxonomy)|4|
|[A Multitask Framework for Sentiment, Emotion and Sarcasm aware Cyberbullying Detection from Multi-modal Code-Mixed Memes](https://doi.org/10.1145/3477495.3531925)|Krishanu Maity, Prince Jha, Sriparna Saha, Pushpak Bhattacharyya|Indian Institute of Technology Patna, Patna, India; Indian Institute of Technology Bombay, Bombay, India|Detecting cyberbullying from memes is highly challenging, because of the presence of the implicit affective content which is also often sarcastic, and multi-modality (image + text). The current work is the first attempt, to the best of our knowledge, in investigating the role of sentiment, emotion and sarcasm in identifying cyberbullying from multi-modal memes in a code-mixed language setting. As a contribution, we have created a benchmark multi-modal meme dataset called MultiBully annotated with bully, sentiment, emotion and sarcasm labels collected from open-source Twitter and Reddit platforms. Moreover, the severity of the cyberbullying posts is also investigated by adding a harmfulness score to each of the memes. The created dataset consists of two modalities, text and image. Most of the texts in our dataset are in code-mixed form, which captures the seamless transitions between languages for multilingual users. Two different multimodal multitask frameworks (BERT+ResNET-Feedback and CLIP-CentralNet) have been proposed for cyberbullying detection (CD), the three auxiliary tasks being sentiment analysis (SA), emotion recognition (ER) and sarcasm detection (SAR). Experimental results indicate that compared to uni-modal and single-task variants, the proposed frameworks improve the performance of the main task, i.e., CD, by 3.18% and 3.10% in terms of accuracy and F1 score, respectively.|网络欺凌的模因检测具有很大的挑战性，因为网络欺凌的内隐情感内容往往是讽刺性的，而且是多模态的(图片 + 文本)。目前的工作是第一次尝试，就我们所知，在调查的作用，情感，情绪和讽刺识别网络欺凌从多模态模因在编码混合的语言环境。作为一个贡献，我们创建了一个基准的多模态 Meme 数据集，称为 MultiBully，注释了从开源 Twitter 和 Reddit 平台收集的恶霸、情绪、情感和讽刺标签。此外，网络欺凌帖子的严重性也通过添加一个危害性评分到每个模因进行调查。创建的数据集由文本和图像两种模式组成。我们数据集中的大多数文本都是代码混合形式的，它捕获了多语言用户语言之间的无缝转换。针对网络欺凌检测(CD) ，提出了两种不同的多模态多任务框架(BERT + R esNET-Feeback 和 CLIP-CentralNet) ，即情绪分析(SA)、情绪识别(ER)和讽刺检测(SAR)三种辅助任务。实验结果表明，与单模态和单任务变体相比，提出的框架在准确性和 F1评分方面分别提高了主要任务 CD 的执行效率3.18% 和3.10% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Multitask+Framework+for+Sentiment,+Emotion+and+Sarcasm+aware+Cyberbullying+Detection+from+Multi-modal+Code-Mixed+Memes)|4|
|[MetaCVR: Conversion Rate Prediction via Meta Learning in Small-Scale Recommendation Scenarios](https://doi.org/10.1145/3477495.3531733)|Xiaofeng Pan, Ming Li, Jing Zhang, Keren Yu, Hong Wen, Luping Wang, Chengjun Mao, Bo Cao|Alibaba Group, Hangzhou, China; The University of Sydney, Sydney, NSW, China|Different from large-scale platforms such as Taobao and Amazon, CVR modeling in small-scale recommendation scenarios is more challenging due to the severe Data Distribution Fluctuation (DDF) issue. DDF prevents existing CVR models from being effective since 1) several months of data are needed to train CVR models sufficiently in small scenarios, leading to considerable distribution discrepancy between training and online serving; and 2) e-commerce promotions have significant impacts on small scenarios, leading to distribution uncertainty of the upcoming time period. In this work, we propose a novel CVR method named MetaCVR from a perspective of meta learning to address the DDF issue. Firstly, a base CVR model which consists of a Feature Representation Network (FRN) and output layers is designed and trained sufficiently with samples across months. Then we treat time periods with different data distributions as different occasions and obtain positive and negative prototypes for each occasion using the corresponding samples and the pre-trained FRN. Subsequently, a Distance Metric Network (DMN) is devised to calculate the distance metrics between each sample and all prototypes to facilitate mitigating the distribution uncertainty. At last, we develop an Ensemble Prediction Network (EPN) which incorporates the output of FRN and DMN to make the final CVR prediction. In this stage, we freeze the FRN and train the DMN and EPN with samples from recent time period, therefore effectively easing the distribution discrepancy. To the best of our knowledge, this is the first study of CVR prediction targeting the DDF issue in small-scale recommendation scenarios. Experimental results on real-world datasets validate the superiority of our MetaCVR and online A/B test also shows our model achieves impressive gains of 11.92% on PCVR and 8.64% on GMV.|与淘宝和亚马逊等大型平台不同，由于数据分布波动(DDF)问题严重，小规模推荐场景下的 CVR 建模更具挑战性。DDF 阻止了现有的 CVR 模型的有效性，因为1)需要几个月的数据来充分训练 CVR 模型在小场景中，导致训练和在线服务之间的相当大的分布差异; 2)电子商务促销对小场景有重大影响，导致即将到来的时间段的分布不确定性。本文从元学习的角度出发，提出了一种新的 CVR 方法 MetaCVR 来解决 DDF 问题。首先，设计了一个由特征表示网络(FRN)和输出层组成的基本 CVR 模型，并对该模型进行了数月的样本训练。然后将不同时间段的数据分布视为不同的场合，利用相应的样本和预先训练好的 FRN，得到每个场合的正负原型。随后，设计了一个距离度量网络(DMN)来计算每个样本和所有原型之间的距离度量，以减少分布的不确定性。最后，我们开发了一个集成预测网络(EPN) ，将 FRN 和 DMN 的输出结合起来进行 CVR 的最终预测。在这一阶段，我们冻结 FRN，并用最近时间段的样本训练 DMN 和 EPN，从而有效地缓解了分布差异。据我们所知，这是第一个针对小规模推荐场景中 DDF 问题的 CVR 预测研究。在实际数据集上的实验结果验证了我们的 MetaCVR 模型的优越性，在线 A/B 测试也表明我们的模型在 PCVR 和 GMV 上分别取得了令人印象深刻的11.92% 和8.64% 的增益。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MetaCVR:+Conversion+Rate+Prediction+via+Meta+Learning+in+Small-Scale+Recommendation+Scenarios)|3|
|[Can Clicks Be Both Labels and Features?: Unbiased Behavior Feature Collection and Uncertainty-aware Learning to Rank](https://doi.org/10.1145/3477495.3531948)|Tao Yang, Chen Luo, Hanqing Lu, Parth Gupta, Bing Yin, Qingyao Ai|University of Utah, Salt Lake City, UT, USA; Amazon Search, Palo Alto, CA, USA|Using implicit feedback collected from user clicks as training labels for learning-to-rank algorithms is a well-developed paradigm that has been extensively studied and used in modern IR systems. Using user clicks as ranking features, on the other hand, has not been fully explored in existing literature. Despite its potential in improving short-term system performance, whether the incorporation of user clicks as ranking features is beneficial for learning-to-rank systems in the long term is still questionable. Two of the most important problems are (1) the explicit bias introduced by noisy user behavior, and (2) the implicit bias, which we refer to as the exploitation bias, introduced by the dynamic training and serving of learning-to-rank systems with behavior features. In this paper, we explore the possibility of incorporating user clicks as both training labels and ranking features for learning to rank. We formally investigate the problems in feature collection and model training, and propose a counterfactual feature projection function and a novel uncertainty-aware learning to rank framework. Experiments on public datasets show that ranking models learned with the proposed framework can significantly outperform models built with raw click features and algorithms that rank items without considering model uncertainty.|使用从用户点击收集的隐式反馈作为学习到排序算法的训练标签是一个发展良好的范例，已经在现代 IR 系统中得到了广泛的研究和应用。使用用户点击作为排名功能，另一方面，还没有充分探讨现有的文献。尽管它在提高短期系统性能方面具有潜力，但是从长远来看，将用户点击作为排名功能的结合是否有利于学习排名系统仍然值得怀疑。其中最重要的两个问题是: (1)噪声用户行为引入的显性偏差和(2)具有行为特征的学习排序系统的动态训练和服务引入的隐性偏差，我们称之为剥削偏差。在这篇文章中，我们探讨了将用户点击作为训练标签和排名特征来学习排名的可能性。我们正式研究了特征收集和模型训练中的问题，提出了一种反事实特征投影函数和一种新的不确定性学习排序框架。对公共数据集的实验表明，使用所提出的框架学习的排序模型可以显著优于使用原始点击特征和算法建立的模型，这些模型不考虑模型的不确定性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+Clicks+Be+Both+Labels+and+Features?:+Unbiased+Behavior+Feature+Collection+and+Uncertainty-aware+Learning+to+Rank)|3|
|[User-Centric Conversational Recommendation with Multi-Aspect User Modeling](https://doi.org/10.1145/3477495.3532074)|Shuokai Li, Ruobing Xie, Yongchun Zhu, Xiang Ao, Fuzhen Zhuang, Qing He|Institute of Artificial Intelligence, Beihang University, Beijing, China; WeChat Search Application Department, Tencent, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China|Conversational recommender systems (CRS) aim to provide highquality recommendations in conversations. However, most conventional CRS models mainly focus on the dialogue understanding of the current session, ignoring other rich multi-aspect information of the central subjects (i.e., users) in recommendation. In this work, we highlight that the user's historical dialogue sessions and look-alike users are essential sources of user preferences besides the current dialogue session in CRS. To systematically model the multi-aspect information, we propose a User-Centric Conversational Recommendation (UCCR) model, which returns to the essence of user preference learning in CRS tasks. Specifically, we propose a historical session learner to capture users' multi-view preferences from knowledge, semantic, and consuming views as supplements to the current preference signals. A multi-view preference mapper is conducted to learn the intrinsic correlations among different views in current and historical sessions via self-supervised objectives. We also design a temporal look-alike user selector to understand users via their similar users. The learned multi-aspect multi-view user preferences are then used for the recommendation and dialogue generation. In experiments, we conduct comprehensive evaluations on both Chinese and English CRS datasets. The significant improvements over competitive models in both recommendation and dialogue generation verify the superiority of UCCR.|会话推荐系统(CRS)旨在提供高质量的会话推荐。然而，大多数传统的 CRS 模型主要侧重于对当前会话的对话理解，而忽略了推荐中心主体(即用户)的其他丰富的多方面信息。在这项工作中，我们强调用户的历史对话会话和外观相似的用户是用户偏好的重要来源，除了当前的对话会话在 CRS。为了系统地建立多方面信息的模型，本文提出了一种以用户为中心的会话推荐(UCCR)模型，该模型回归了 CRS 任务中用户偏好学习的本质。具体来说，我们提出了一个历史会话学习者来捕捉用户的多视图偏好，从知识，语义和消费观点作为补充的当前偏好信号。通过自监督目标，提出了一种多视点偏好映射算法，用于研究当前会议和历史会议中不同视点之间的内在相关性。我们还设计了一个时间外观相似的用户选择器，通过相似的用户来理解用户。然后利用所学习的多方面多视图用户偏好进行推荐和对话生成。在实验中，我们对中英文 CRS 数据集进行了综合评价。在推荐和对话生成方面对竞争模式的重大改进证明了 UCCR 的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=User-Centric+Conversational+Recommendation+with+Multi-Aspect+User+Modeling)|3|
|[When Multi-Level Meets Multi-Interest: A Multi-Grained Neural Model for Sequential Recommendation](https://doi.org/10.1145/3477495.3532081)|Yu Tian, Jianxin Chang, Yanan Niu, Yang Song, Chenliang Li|Wuhan University, Wuhan, China; Kuaishou.com, Beijing, China|Sequential recommendation aims at identifying the next item that is preferred by a user based on their behavioral history. Compared to conventional sequential models that leverage attention mechanisms and RNNs, recent efforts mainly follow two directions for improvement: multi-interest learning and graph convolutional aggregation. Specifically, multi-interest methods such as ComiRec and MIMN, focus on extracting different interests for a user by performing historical item clustering, while graph convolution methods including TGSRec and SURGE elect to refine user preferences based on multilevel correlations between historical items. Unfortunately, neither of them realizes that these two types of solutions can mutually complement each other, by aggregating multi-level user preference to achieve more precise multi-interest extraction for a better recommendation. To this end, in this paper, we propose a unified multi-grained neural model (named MGNM) via a combination of multi-interest learning and graph convolutional aggregation. Concretely, MGNM first learns the graph structure and information aggregation paths of the historical items for a user. It then performs graph convolution to derive item representations in an iterative fashion, in which the complex preferences at different levels can be well captured. Afterwards, a novel sequential capsule network is proposed to inject the sequential patterns into the multi-interest extraction process, leading to a more precise interest learning in a multi-grained manner. Experiments on three real-world datasets from different scenarios demonstrate the superiority of MGNM against several state-of-the-art baselines. The performance gain over the best baseline is up to 27.10% and 25.17% in terms of [email protected] and [email protected] respectively, which is one of the largest gains in recent development of sequential recommendation. Further analysis also demonstrates that MGNM is robust and effective at user preference understanding at multi-grained levels.|顺序推荐的目的是根据用户的行为历史来确定他们喜欢的下一个项目。与利用注意机制和 RNN 的传统序列模型相比，最近的努力主要遵循两个改进方向: 多兴趣学习和图卷积聚合。具体来说，ComiRec 和 MIMN 等多兴趣方法侧重于通过对历史项目进行聚类来为用户提取不同的兴趣，而 TGSRec 和 SurGE 等图形卷积方法则选择基于历史项目之间的多级相关性来改进用户偏好。不幸的是，他们都没有意识到这两种解决方案可以相互补充，通过聚合多级用户偏好来实现更精确的多兴趣提取以获得更好的推荐。为此，本文将多兴趣学习和图卷积聚合相结合，提出了一种统一的多粒度神经网络模型(MGNM)。具体来说，MGNM 首先为用户学习历史项的图形结构和信息聚合路径。然后，它执行图形卷积，以迭代的方式派生项表示，在这种方式中，可以很好地捕获不同级别的复杂首选项。然后，提出了一种新的序列胶囊网络，将序列模式引入到多兴趣提取过程中，从而实现多粒度的兴趣学习。通过对来自不同场景的三个真实世界数据集的实验，证明了 MGNM 算法对几个最新基线的优越性。就[ email protected ]和[ email protected ]而言，超过最佳基线的性能提升分别高达27.10% 和25.17% ，这是最近顺序推荐发展中最大的提升之一。进一步的分析还表明，MGNM 在多粒度级别上对用户偏好的理解是健壮的和有效的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=When+Multi-Level+Meets+Multi-Interest:+A+Multi-Grained+Neural+Model+for+Sequential+Recommendation)|3|
|[AutoGSR: Neural Architecture Search for Graph-based Session Recommendation](https://doi.org/10.1145/3477495.3531940)|Jingfan Chen, Guanghui Zhu, Haojun Hou, Chunfeng Yuan, Yihua Huang|Nanjing University, Nanjing, China|Session-based recommendation aims to predict next click action (e.g., item) of anonymous users based on a fixed number of previous actions. Recently, Graph Neural Networks (GNNs) have shown superior performance in various applications. Inspired by the success of GNNs, tremendous endeavors have been devoted to introduce GNNs into session-based recommendation and have achieved significant results. Nevertheless, due to the highly diverse types of potential information in sessions, existing GNNs-based methods perform differently on different session datasets, leading to the need for efficient design of neural networks adapted to various session recommendation scenarios. To address this problem, we propose Automated neural architecture search for Graph-based Session Recommendation, namely AutoGSR, a framework that provides a practical and general solution to automatically find the optimal GNNs-based session recommendation model. In AutoGSR, we propose two novel GNN operations to build an expressive and compact search space. Building upon the search space, we employ a differentiable search algorithm to search for the optimal graph neural architecture. Furthermore, to consider all types of session information together, we propose to learn the item meta knowledge, which acts as a priori knowledge for guiding the optimization of final session representations. Comprehensive experiments on three real-world datasets demonstrate that AutoGSR is able to find effective neural architectures and achieve state-of-the-art results. To the best of our knowledge, we are the first to study the neural architecture search for the session-based recommendation.|基于会话的推荐旨在预测匿名用户的下一次点击操作(例如，条目) ，该推荐基于一个固定数量的以前的操作。近年来，图形神经网络(GNN)在各种应用中表现出了优越的性能。在 GNN 取得成功的启发下，为将 GNN 引入会议建议作出了巨大努力，并取得了重大成果。然而，由于会议中潜在信息的类型多种多样，现有的基于全球导航系统的方法在不同的会议数据集上表现不同，因此需要有效设计适应各种会议推荐情景的神经网络。为了解决这个问题，我们提出了基于图的会话推荐的自动神经网络体系结构搜索，即 AutoGSR，这个框架提供了一个实用的和通用的解决方案来自动找到最佳的基于 GNN 的会话推荐模型。在 AutoGSR 中，我们提出了两种新的 GNN 操作来构建一个表达式的、紧凑的搜索空间。在搜索空间的基础上，采用可微搜索算法来搜索最优的图神经网络结构。此外，为了一起考虑所有类型的会话信息，我们建议学习项目元知识，它作为指导最终会话表示优化的先验知识。对三个实际数据集的综合实验表明，AutoGSR 能够找到有效的神经结构，并取得最先进的结果。据我们所知，我们是第一个研究神经结构搜索的会话为基础的建议。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AutoGSR:+Neural+Architecture+Search+for+Graph-based+Session+Recommendation)|3|
|[How Does Feedback Signal Quality Impact Effectiveness of Pseudo Relevance Feedback for Passage Retrieval](https://doi.org/10.1145/3477495.3531822)|Hang Li, Ahmed Mourad, Bevan Koopman, Guido Zuccon|The University of Queensland, Brisbane, QLD, Australia; CSIRO, Brisbane, QLD, Australia|Pseudo-Relevance Feedback (PRF) assumes that the top results retrieved by a first-stage ranker are relevant to the original query and uses them to improve the query representation for a second round of retrieval. This assumption however is often not correct: some or even all of the feedback documents may be irrelevant. Indeed, the effectiveness of PRF methods may well depend on the quality of the feedback signal and thus on the effectiveness of the first-stage ranker. This aspect however has received little attention before. In this paper we control the quality of the feedback signal and measure its impact on a range of PRF methods, including traditional bag-of-words methods (Rocchio), and dense vector-based methods (learnt and not learnt). Our results show the important role the quality of the feedback signal plays on the effectiveness of PRF methods. Importantly, and surprisingly, our analysis reveals that not all PRF methods are the same when dealing with feedback signals of varying quality. These findings are critical to gain a better understanding of the PRF methods and of which and when they should be used, depending on the feedback signal quality, and set the basis for future research in this area.|伪相关反馈(PRF)假设第一阶段排序器检索到的最高结果与原始查询相关，并利用这些结果改善查询表示以进行第二轮检索。然而，这种假设往往是不正确的: 一些甚至所有的反馈文档可能是不相关的。事实上，PRF 方法的有效性很大程度上取决于反馈信号的质量，因此也取决于第一阶段排序器的有效性。然而，这方面以前很少受到关注。在本文中，我们控制反馈信号的质量，并测量其对一系列 PRF 方法的影响，包括传统的词袋方法(Rocchio)和基于密集向量的方法(学习和不学习)。结果表明，反馈信号的质量对 PRF 方法的有效性起着重要作用。重要的是，令人惊讶的是，我们的分析表明，并非所有的 PRF 方法是相同的，当处理不同质量的反馈信号。这些发现对于更好地理解 PRF 方法以及根据反馈信号的质量，什么时候应该使用这些方法是至关重要的，并且为这一领域的未来研究奠定了基础。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+Does+Feedback+Signal+Quality+Impact+Effectiveness+of+Pseudo+Relevance+Feedback+for+Passage+Retrieval)|3|
|[Revisiting Two-tower Models for Unbiased Learning to Rank](https://doi.org/10.1145/3477495.3531837)|Le Yan, Zhen Qin, Honglei Zhuang, Xuanhui Wang, Michael Bendersky, Marc Najork|Google, Mountain View, CA, USA|Two-tower architecture is commonly used in real-world systems for Unbiased Learning to Rank (ULTR), where a Deep Neural Network (DNN) tower models unbiased relevance predictions, while another tower models observation biases inherent in the training data like user clicks. This two-tower architecture introduces inductive biases to allow more efficient use of limited observational logs and better generalization during deployment than single-tower architecture that may learn spurious correlations between relevance predictions and biases. However, despite their popularity, it is largely neglected in the literature that existing two-tower models assume that the joint distribution of relevance prediction and observation probabilities are completely factorizable. In this work, we revisit two-tower models for ULTR. We rigorously show that the factorization assumption can be too strong for real-world user behaviors, and existing methods may easily fail under slightly milder assumptions. We then propose several novel ideas that consider a wider spectrum of user behaviors while still under the two-tower framework to maintain simplicity and generalizability. Our concerns of existing two-tower models and the effectiveness of our proposed methods are validated on both controlled synthetic and large-scale real-world datasets.|双塔架构通常用于现实世界的无偏学习排名(ULTR)系统，其中一个深度神经网络(DNN)塔模型无偏相关性预测，而另一个塔模型观察偏差固有的训练数据，如用户点击。与单塔架构相比，这种双塔架构引入了归纳偏差，以便更有效地使用有限的观测日志，并在部署期间更好地进行概括，而单塔架构可以学习相关性预测和偏差之间的伪相关性。然而，尽管现有的双塔模型很流行，但它们大多被忽视了，现有的双塔模型假定相关预测和观测概率的联合分布是完全可分解的。在这项工作中，我们重新讨论了 ULTR 的双塔模型。我们严格地证明了因子分解假设对于真实世界的用户行为来说可能过于强烈，并且在稍微温和的假设下，现有的方法可能很容易失败。然后，我们提出了几个新的想法，考虑更广泛的用户行为，同时仍然在双塔框架下，以保持简单性和普遍性。我们对现有的双塔模型的关注和我们提出的方法的有效性是在受控的合成和大规模的真实世界数据集上得到验证。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+Two-tower+Models+for+Unbiased+Learning+to+Rank)|3|
|[Mitigating Bias in Search Results Through Contextual Document Reranking and Neutrality Regularization](https://doi.org/10.1145/3477495.3531891)|George Zerveas, Navid Rekabsaz, Daniel Cohen, Carsten Eickhoff|Johannes Kepler University Linz & Linz Institute of Technology, Linz, Austria; Brown University, Providence, RI, USA|Societal biases can influence Information Retrieval system results, and conversely, search results can potentially reinforce existing societal biases. Recent research has therefore focused on developing methods for quantifying and mitigating bias in search results and applied them to contemporary retrieval systems that leverage transformer-based language models. In the present work, we expand this direction of research by considering bias mitigation within a framework for contextual document embedding reranking. In this framework, the transformer-based query encoder is optimized for relevance ranking through a list-wise objective, by jointly scoring for the same query a large set of candidate document embeddings in the context of one another, instead of in isolation. At the same time, we impose a regularization loss which penalizes highly scoring documents that deviate from neutrality with respect to a protected attribute (e.g., gender). Our approach for bias mitigation is end-to-end differentiable and efficient. Compared to the existing alternatives for deep neural retrieval architectures, which are based on adversarial training, we demonstrate that it can attain much stronger bias mitigation/fairness. At the same time, for the same amount of bias mitigation, it offers significantly better relevance performance (utility). Crucially, our method allows for a more finely controllable and predictable intensity of bias mitigation, which is essential for practical deployment in production systems.|社会偏见可以影响信息检索系统的结果，相反，搜索结果可能会加强现有的社会偏见。因此，最近的研究侧重于开发用于量化和减轻搜索结果偏差的方法，并将其应用于利用基于转换器的语言模型的当代检索系统。在本文的工作中，我们扩展了这个研究方向，在上下文文档嵌入重排序的框架内考虑了偏差缓解。在这个框架中，基于转换器的查询编码器通过列表目标优化相关性排序，为同一个查询联合评分一大组候选文档嵌入在另一个上下文中，而不是孤立。与此同时，我们强制规范化的损失，惩罚高得分的文件偏离中立方面的保护属性(例如，性别)。我们的减少偏差的方法是端到端可微和有效的。与已有的基于对抗训练的深度神经检索结构相比，我们证明了该结构可以获得更强的偏差缓解/公平性。同时，对于相同数量的偏差缓解，它提供了明显更好的相关性能(效用)。至关重要的是，我们的方法允许更精细地控制和可预测的偏差缓解强度，这对于生产系统中的实际部署是必不可少的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mitigating+Bias+in+Search+Results+Through+Contextual+Document+Reranking+and+Neutrality+Regularization)|3|
|[Curriculum Contrastive Context Denoising for Few-shot Conversational Dense Retrieval](https://doi.org/10.1145/3477495.3531961)|Kelong Mao, Zhicheng Dou, Hongjin Qian|Renmin University of China, Beijing, China; Renmin University of China & Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China|Conversational search is a crucial and promising branch in information retrieval. In this paper, we reveal that not all historical conversational turns are necessary for understanding the intent of the current query. The redundant noisy turns in the context largely hinder the improvement of search performance. However, enhancing the context denoising ability for conversational search is quite challenging due to data scarcity and the steep difficulty for simultaneously learning conversational query encoding and context denoising. To address these issues, in this paper, we present a novel Curriculum cOntrastive conTExt Denoising framework, COTED, towards few-shot conversational dense retrieval. Under a curriculum training order, we progressively endow the model with the capability of context denoising via contrastive learning between noised samples and denoised samples generated by a new conversation data augmentation strategy. Three curriculums tailored to conversational search are exploited in our framework. Extensive experiments on two few-shot conversational search datasets, i.e., CAsT-19 and CAsT-20, validate the effectiveness and superiority of our method compared with the state-of-the-art baselines.|会话搜索是信息检索中一个重要而有前途的分支。本文揭示了并非所有的历史会话转折都是理解当前查询意图的必要条件。上下文中的冗余噪声转折很大程度上阻碍了搜索性能的提高。然而，由于数据的稀缺性和同时学习会话查询编码和上下文去噪的困难性，提高会话搜索的上下文去噪能力是一个相当具有挑战性的问题。为了解决这些问题，本文提出了一种新的课程对比语境去噪框架 COTED，该框架旨在实现少镜头的会话密集检索。在课程训练顺序下，我们逐步赋予该模型通过对比学习去除噪声样本和通过一种新的会话数据增强策略产生的去噪样本的上下文去噪能力。在我们的框架中开发了三个适合会话搜索的课程。在两个少量会话搜索数据集，即 CAsT-19和 CAsT-20上的广泛实验验证了我们的方法与最先进的基线相比的有效性和优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Curriculum+Contrastive+Context+Denoising+for+Few-shot+Conversational+Dense+Retrieval)|3|
|[Decoupled Side Information Fusion for Sequential Recommendation](https://doi.org/10.1145/3477495.3531963)|Yueqi Xie, Peilin Zhou, Sunghun Kim|HKUST, Hong Kong, Hong Kong; Upstage, Hong Kong, Hong Kong|Side information fusion for sequential recommendation (SR) aims to effectively leverage various side information to enhance the performance of next-item prediction. Most state-of-the-art methods build on self-attention networks and focus on exploring various solutions to integrate the item embedding and side information embeddings before the attention layer. However, our analysis shows that the early integration of various types of embeddings limits the expressiveness of attention matrices due to a rank bottleneck and constrains the flexibility of gradients. Also, it involves mixed correlations among the different heterogeneous information resources, which brings extra disturbance to attention calculation. Motivated by this, we propose Decoupled Side Information Fusion for Sequential Recommendation (DIF-SR), which moves the side information from the input to the attention layer and decouples the attention calculation of various side information and item representation. We theoretically and empirically show that the proposed solution allows higher-rank attention matrices and flexible gradients to enhance the modeling capacity of side information fusion. Also, auxiliary attribute predictors are proposed to further activate the beneficial interaction between side information and item representation learning. Extensive experiments on four real-world datasets demonstrate that our proposed solution stably outperforms state-of-the-art SR models. Further studies show that our proposed solution can be readily incorporated into current attention-based SR models and significantly boost performance. Our source code is available at https://github.com/AIM-SE/DIF-SR.|序贯推荐侧信息融合是为了有效地利用各种侧信息来提高下一个项目的预测性能。大多数最新的方法都是建立在自我注意网络的基础上，着重于探索各种解决方案，以整合注意层之前的项目嵌入和侧信息嵌入。然而，我们的分析表明，由于等级瓶颈的存在，各种嵌入类型的早期集成限制了注意矩阵的表达能力，同时也限制了梯度的灵活性。此外，它还涉及到不同异质信息资源之间的混合相关，给注意计算带来额外的干扰。在此基础上，提出了基于解耦的序贯推荐侧信息融合方法(DIF-SR) ，该方法将侧信息从输入层移动到注意层，并对各侧信息的注意计算和项目表示进行解耦。理论和实验结果表明，该方法允许高阶注意矩阵和灵活的梯度，提高了侧向信息融合的建模能力。同时，提出了辅助属性预测器，以进一步激活侧信息与项目表征学习之间的有益交互作用。在四个真实世界数据集上的大量实验表明，我们提出的解决方案稳定地优于最先进的 SR 模型。进一步的研究表明，我们提出的解决方案可以很容易地纳入目前的注意为基础的 SR 模型，并显着提高性能。我们的源代码可以在 https://github.com/aim-se/dif-sr 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Decoupled+Side+Information+Fusion+for+Sequential+Recommendation)|3|
|[Is News Recommendation a Sequential Recommendation Task?](https://doi.org/10.1145/3477495.3531862)|Chuhan Wu, Fangzhao Wu, Tao Qi, Chenliang Li, Yongfeng Huang|York University, Toronto, Canada; Renmin University of China, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China|For sequential recommendation, it is essential to capture and predict future or long-term user preference for generating accurate recommendation over time. To improve the predictive capacity, we adopt reinforcement learning (RL) for developing effective sequential recommenders. However, user-item interaction data is likely to be sparse, complicated and time-varying. It is not easy to directly apply RL techniques to improve the performance of sequential recommendation. Inspired by the availability of knowledge graph (KG), we propose a novel Knowledge-guidEd Reinforcement Learning model (KERL for short) for fusing KG information into a RL framework for sequential recommendation. Specifically, we formalize the sequential recommendation task as a Markov Decision Process (MDP), and make three major technical extensions in this framework, including state representation, reward function and learning algorithm. First, we propose to enhance the state representations with KG information considering both exploitation and exploration. Second, we carefully design a composite reward function that is able to compute both sequence- and knowledge-level rewards. Third, we propose a new algorithm for more effectively learning the proposed model. To our knowledge, it is the first time that knowledge information has been explicitly discussed and utilized in RL-based sequential recommenders, especially for the exploration process. Extensive experiment results on both next-item and next-session recommendation tasks show that our model can significantly outperform the baselines on four real-world datasets.|对于连续推荐，必须捕获和预测未来或长期的用户偏好，以便随着时间的推移产生准确的推荐。为了提高预测能力，我们采用强化学习(RL)来开发有效的顺序推荐系统。然而，用户项交互数据可能是稀疏的、复杂的和时变的。直接应用 RL 技术来提高顺序推荐的性能并不容易。受到知识图表(kG)的启发，我们提出了一种新的知识引导强化学习模型(简称 KERL) ，用于将 kG 信息融合到一个 RL 框架中，用于连续推荐。具体来说，我们将顺序推荐任务形式化为一个马可夫决策过程(mDP) ，并在这个框架中进行了三个主要的技术扩展，包括状态表示、奖励函数和学习算法。首先，我们提出了利用 KG 信息同时考虑开发和探索的方法来增强状态表示。其次，我们仔细设计了一个复合奖励函数，它能够计算序列和知识水平的奖励。第三，我们提出了一个新的算法，以更有效地学习所提出的模型。据我们所知，知识信息首次被明确地讨论和利用在基于 RL 的顺序推荐系统中，特别是在探索过程中。对下一个项目和下一个会话推荐任务的大量实验结果表明，我们的模型可以显著优于四个真实世界数据集的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Is+News+Recommendation+a+Sequential+Recommendation+Task?)|3|
|[Dual Contrastive Network for Sequential Recommendation](https://doi.org/10.1145/3477495.3531918)|Guanyu Lin, Chen Gao, Yinfeng Li, Yu Zheng, Zhiheng Li, Depeng Jin, Yong Li|Tsinghua University, Beijing, China|Widely applied in today's recommender systems, sequential recommendation predicts the next interacted item for a given user via his/her historical item sequence. However, sequential recommendation suffers data sparsity issue like most recommenders. To extract auxiliary signals from the data, some recent works exploit self-supervised learning to generate augmented data via dropout strategy, which, however, leads to sparser sequential data and obscure signals. In this paper, we propose D ual C ontrastive N etwork (DCN) to boost sequential recommendation, from a new perspective of integrating auxiliary user-sequence for items. Specifically, we propose two kinds of contrastive learning. The first one is the dual representation contrastive learning that minimizes the distances between embeddings and sequence-representations of users/items. The second one is the dual interest contrastive learning which aims to self-supervise the static interest with the dynamic interest of next item prediction via auxiliary training. We also incorporate the auxiliary task of predicting next user for a given item's historical user sequence, which can capture the trends of items preferred by certain types of users. Experiments on benchmark datasets verify the effectiveness of our proposed method. Further ablation study also illustrates the boosting effect of the proposed components upon different sequential models.|顺序推荐在当今的推荐系统中被广泛应用，它通过用户的历史项目顺序来预测给定用户的下一个交互项目。然而，与大多数推荐程序一样，顺序推荐也存在数据稀疏的问题。为了从数据中提取辅助信号，最近的一些工作利用自监督学习通过丢失策略生成增强数据，但是这样会导致序列数据更稀疏，信号更模糊。本文从集成项目辅助用户序列的新角度出发，提出了 D-C 对比 N 网络(DCN)来增强序列推荐。具体来说，我们提出了两种对比学习。第一种是双重表示对比学习，它最小化了嵌入和用户/项目序列表示之间的距离。第二种是双兴趣对比学习，目的是通过辅助训练对静态兴趣和下一项预测的动态兴趣进行自我监督。我们还结合了辅助任务，即预测给定项目的历史用户序列的下一个用户，这可以捕获某些类型的用户喜欢的项目的趋势。在基准数据集上的实验验证了该方法的有效性。进一步的消融研究也说明了所提出的组件对不同序列模型的增强效应。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dual+Contrastive+Network+for+Sequential+Recommendation)|3|
|[PKG: A Personal Knowledge Graph for Recommendation](https://doi.org/10.1145/3477495.3531671)|Yu Yang, Jiangxu Lin, Xiaolian Zhang, Meng Wang|Southeast University, Nanjing, China; Huawei Technologies Co. Ltd., Shenzhen, China|Mobile internet users generate personal data on the devices all the time in this era. In this paper, we demonstrate a novel system for integrating the data of a user from different sources into a Personal Knowledge Graph, i.e., PKG. We show how a user's intention can be detected and how the personal data can be aligned and connected by the user behaviors. The constructed PKG allows the system makes reasonable and accurate recommendations for users by a "neural + symbolic'' approach across different services. Our system is shown in https://youtu.be/hWuo8KCDrto.|在这个时代，移动互联网用户一直在设备上生成个人数据。在本文中，我们展示了一个新的系统来整合来自不同来源的用户的数据到一个个人知识图，即 PKG。我们展示了如何检测用户的意图，以及如何通过用户行为来校准和连接个人数据。构建的 PKG 允许系统通过“神经 + 符号”的方法跨越不同的服务为用户提供合理和准确的建议。我们的系统以 https://youtu.be/hwuo8kcdrto 显示。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PKG:+A+Personal+Knowledge+Graph+for+Recommendation)|3|
|[Co-training Disentangled Domain Adaptation Network for Leveraging Popularity Bias in Recommenders](https://doi.org/10.1145/3477495.3531952)|Zhihong Chen, Jiawei Wu, Chenliang Li, Jingxu Chen, Rong Xiao, Binqiang Zhao|Alibaba Group, Hangzhou, China; Wuhan University, Wuhan, China|Recommender system usually faces popularity bias. From the popularity distribution shift perspective, the normal paradigm trained on exposed items (most are hot items) identifies that recommending popular items more frequently can achieve lower loss, thus injecting popularity information into item property embedding, e.g., id embedding. From the long-tail distribution shift perspective, the sparse interactions of long-tail items lead to insufficient learning of them. The resultant distribution discrepancy between hot and long-tail items would not only inherit the bias, but also amplify the bias. Existing work addresses this issue with inverse propensity scoring (IPS) or causal embeddings. However, we argue that not all popularity biases mean bad effects, i.e., some items show higher popularity due to better quality or conform to current trends, which deserve more recommendations. Blindly seeking unbiased learning may inhibit high-quality or fashionable items. To make better use of the popularity bias, we propose a co-training disentangled domain adaptation network (CD$^2$AN), which can co-train both biased and unbiased models. Specifically, for popularity distribution shift, CD$^2$AN disentangles item property representation and popularity representation from item property embedding. For long-tail distribution shift, we introduce additional unexposed items (most are long-tail items) to align the distribution of hot and long-tail item property representations. Further, from the instances perspective, we carefully design the item similarity regularization to learn comprehensive item representation, which encourages item pairs with more effective co-occurrences patterns to have more similar item property representations. Based on offline evaluations and online A/B tests, we show that CD$^2$AN outperforms the existing debiased solutions. Currently, CD$^2$AN has been successfully deployed at Mobile Taobao App and handling major online traffic.|推荐系统通常面临受欢迎程度的偏见。从受欢迎度分布转移的角度来看，通常对曝光项目(大多数是热门项目)进行训练的范式认为，更频繁地推荐受欢迎项目可以获得更低的损失，从而将受欢迎信息注入项目属性嵌入，例如，ID 嵌入。从长尾分布偏移的角度来看，长尾项目之间稀疏的交互作用导致对它们的学习不足。由此产生的热点项目与长尾项目之间的分布差异不仅会继承偏差，而且会放大偏差。现有的工作解决这个问题的逆倾向评分(IPS)或因果嵌入。然而，我们认为并非所有的流行偏见都意味着负面影响，例如，一些项目由于质量更好或符合当前趋势而显示出更高的流行度，这值得更多的推荐。盲目追求无偏见的学习可能会抑制高质量或时尚的项目。为了更好地利用流行度偏差，我们提出了一种协同训练的去纠缠域自适应网络(CD $^ 2 $AN) ，它可以同时训练有偏和无偏模型。具体地说，对于流行度分布转移，CD $^ 2 $AN 将项目属性表示和流行度表示从项目属性嵌入中分离出来。对于长尾分布转移，我们引入了额外的未公开项(大多数是长尾项)来对齐热点和长尾项属性表示的分布。此外，从实例的角度，我们仔细设计了项目相似性正则化，以学习综合项目表示，鼓励具有更有效的共现模式的项目对具有更多相似的项目属性表示。基于离线评估和在线 A/B 测试，我们表明 CD $^ 2 $AN 优于现有的去偏解决方案。目前，CD $^ 2 $AN 已经成功部署在移动淘宝应用程序上，并处理主要的在线流量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Co-training+Disentangled+Domain+Adaptation+Network+for+Leveraging+Popularity+Bias+in+Recommenders)|3|
|[Multi-Level Interaction Reranking with User Behavior History](https://doi.org/10.1145/3477495.3532026)|Yunjia Xi, Weiwen Liu, Jieming Zhu, Xilong Zhao, Xinyi Dai, Ruiming Tang, Weinan Zhang, Rui Zhang, Yong Yu|ruizhang.info, Shenzhen, China; Shanghai Jiao Tong University, Shanghai, China; Huawei Noah's Ark Lab, Shenzhen, China|As the final stage of the multi-stage recommender system (MRS), reranking directly affects users' experience and satisfaction, thus playing a critical role in MRS. Despite the improvement achieved in the existing work, three issues are yet to be solved. First, users' historical behaviors contain rich preference information, such as users' long and short-term interests, but are not fully exploited in reranking. Previous work typically treats items in history equally important, neglecting the dynamic interaction between the history and candidate items. Second, existing reranking models focus on learning interactions at the item level while ignoring the fine-grained feature-level interactions. Lastly, estimating the reranking score on the ordered initial list before reranking may lead to the early scoring problem, thereby yielding suboptimal reranking performance. To address the above issues, we propose a framework named Multi-level Interaction Reranking (MIR). MIR combines low-level cross-item interaction and high-level set-to-list interaction, where we view the candidate items to be reranked as a set and the users' behavior history in chronological order as a list. We design a novel SLAttention structure for modeling the set-to-list interactions with personalized long-short term interests. Moreover, feature-level interactions are incorporated to capture the fine-grained influence among items. We design MIR in such a way that any permutation of the input items would not change the output ranking, and we theoretically prove it. Extensive experiments on three public and proprietary datasets show that MIR significantly outperforms the state-of-the-art models using various ranking and utility metrics.|作为多阶段推荐系统(MRS)的最后阶段，重新排名直接影响用户的体验和满意度，因此在 MRS 中发挥着关键作用。尽管现有工作已有所改善，但仍有三个问题有待解决。首先，用户的历史行为包含了丰富的偏好信息，如用户的长期和短期兴趣，但在重新排序时没有得到充分的利用。以往的研究通常认为历史条目同等重要，而忽视了历史条目与候选条目之间的动态互动。其次，现有的重新排序模型侧重于项目层面的学习交互，而忽略了细粒度的特征层面的交互。最后，在重新排序之前估计排序初始列表上的重新排序得分可能会导致早期得分问题，从而产生次优的重新排序性能。为了解决上述问题，我们提出了一个名为多级交互重排(MIR)的框架。MIR 结合了低层次的跨项目交互和高层次的集合-列表交互，我们将待重新排序的候选项作为一个集合，将用户的行为历史按照时间顺序作为一个列表。我们设计了一个新颖的空间注意结构，用于建模具有个性化长期短期兴趣的集合列表交互。此外，特征层次的交互作用被合并来捕获项目之间的细粒度影响。我们设计 MIR 的方法使得输入项的任何排列都不会改变输出的排名，并且我们从理论上证明了这一点。对三个公共和专有数据集的大量实验表明，使用各种排名和效用指标，MIR 显著优于最先进的模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Level+Interaction+Reranking+with+User+Behavior+History)|3|
|[RankFlow: Joint Optimization of Multi-Stage Cascade Ranking Systems as Flows](https://doi.org/10.1145/3477495.3532050)|Jiarui Qin, Jiachen Zhu, Bo Chen, Zhirong Liu, Weiwen Liu, Ruiming Tang, Rui Zhang, Yong Yu, Weinan Zhang|ruizhang.info, Shenzhen, China; Shanghai Jiao Tong University, Shanghai, China; Huawei Noah's Ark Lab, Shenzhen, China|Building a multi-stage cascade ranking system is a commonly used solution to balance the efficiency and effectiveness in modern information retrieval (IR) applications, such as recommendation and web search. Despite the popularity in practice, the literature specific on multi-stage cascade ranking systems is relatively scarce. The common practice is to train rankers of each stage independently using the same user feedback data (a.k.a., impression data), disregarding the data flow and the possible interactions between stages. This straightforward solution could lead to a sub-optimal system because of the sample selection bias (SSB) issue, which is especially damaging for cascade rankers due to the negative effect accumulated in the multiple stages. Worse still, the interactions between the rankers of each stage are not fully exploited. This paper provides an elaborate analysis of this commonly used solution to reveal its limitations. By studying the essence of cascade ranking, we propose a joint training framework named RankFlow to alleviate the SSB issue and exploit the interactions between the cascade rankers, which is the first systematic solution for this topic. We propose a paradigm of training cascade rankers that emphasizes the importance of fitting rankers on stage-specific data distributions instead of the unified user feedback distribution. We design the RankFlow framework based on this paradigm: The training data of each stage is generated by its preceding stages while the guidance signals not only come from the logs but its successors. Extensive experiments are conducted on various IR scenarios, including recommendation, web search and advertisement. The results verify the efficacy and superiority of RankFlow.|建立一个多阶段的级联排名系统是现代信息检索应用(如推荐和网络搜索)中一个常用的平衡效率和有效性的解决方案。尽管在实践中很流行，但是关于多级级联排序系统的文献相对较少。通常的做法是使用相同的用户反馈数据(也就是印象数据)独立训练每个阶段的排名，忽略数据流和阶段之间可能的交互作用。由于样本选择偏差(SSB)问题，这种直接的解决方案可能会导致系统的次优化，而由于多阶段累积的负面效应，SSB 问题对级联排序尤其有害。更糟糕的是，每个阶段的排名之间的相互作用没有得到充分利用。本文对这种常用的解决方案进行了详细的分析，以揭示其局限性。通过研究级联排序的本质，提出了一种联合训练框架 RankFlow 来缓解 SSB 问题，并利用级联排序器之间的相互作用，这是本课题的第一个系统解决方案。我们提出了一个训练级联排名的范式，强调拟合排名的重要性阶段特定的数据分布，而不是统一的用户反馈分布。在此基础上，我们设计了 RankFlow 框架: 每个阶段的训练数据由前一阶段生成，而制导信号不仅来自日志，还来自后一阶段。在不同的信息检索场景中进行了广泛的实验，包括推荐、网络搜索和广告。结果验证了 RankFlow 的有效性和优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RankFlow:+Joint+Optimization+of+Multi-Stage+Cascade+Ranking+Systems+as+Flows)|3|
|[Towards Suicide Ideation Detection Through Online Conversational Context](https://doi.org/10.1145/3477495.3532068)|Ramit Sawhney, Shivam Agarwal, Atula Tejaswi Neerkaje, Nikolaos Aletras, Preslav Nakov, Lucie Flek|University of Illinois at Urbana-Champaign & Conversational AI and Social Analytics (CAISA) Lab, University of Marburg, Urbana-Champaign, IL, USA; Conversational AI and Social Analytics (CAISA) Lab, University of Marburg, Marburg, Germany; University of Sheffield, Sheffield, United Kingdom; Qatar Computing Research Institute, HBKU, Doha, Qatar; Georgia Institute of Technology & Conversational AI and Social Analytics (CAISA) Lab, University of Marburg, Atlanta, GA, USA|Social media enable users to share their feelings and emotional struggles. They also offer an opportunity to provide community support to suicidal users. Recent studies on suicide risk assessment have explored the user's historic timeline and information from their social network to analyze their emotional state. However, such methods often require a large amount of user-centric data. A less intrusive alternative is to only use conversation trees arising from online community responses. Modeling such online conversations between the community and a person in distress is an important context for understanding that person's mental state. However, it is not trivial to model the vast number of conversation trees on social media, since each comment has a diverse influence on a user in distress. Typically, a handful of comments/posts receive a significantly high number of replies, which results in scale-free dynamics in the conversation tree. Moreover, psychological studies suggested that it is important to capture the fine-grained temporal irregularities in the release of vast volumes of comments, since suicidal users react quickly to online community support. Building on these limitations and psychological studies, we propose HCN, a Hyperbolic Conversation Network, which is a less user-intrusive method for suicide ideation detection. HCN leverages the hyperbolic space to represent the scale-free dynamics of online conversations. Through extensive quantitative, qualitative, and ablative experiments on real-world Twitter data, we find that HCN outperforms state-of-the art methods, while using 98% less user-specific data, and while maintaining a 74% lower carbon footprint and a 94% smaller model size. We also find that the comments within the first half an hour are most important to identify at-risk users.|社交媒体使用户能够分享他们的感受和情感挣扎。他们还提供了一个机会，为自杀使用者提供社区支持。最近有关自杀风险评估的研究探讨了使用者的历史时间线和来自他们的社交网络的信息，以分析他们的情绪状态。然而，这样的方法通常需要大量以用户为中心的数据。一个较少干扰的替代方法是只使用在线社区响应中产生的对话树。在社区和一个处于困境中的人之间建立这样的在线对话模型是理解这个人的精神状态的重要环境。然而，在社交媒体上建立大量的对话树并非易事，因为每条评论都会对处于困境中的用户产生不同的影响。通常，少量的评论/帖子会收到大量的回复，这导致了会话树中的无标度动态。此外，心理学研究表明，重要的是捕捉细粒度的时间不规则的发布大量的评论，因为自杀用户反应迅速在线社区支持。在这些局限性和心理学研究的基础上，我们提出了双曲对话网络 HCN，这是一种用户侵入性较低的自杀意念检测方法。HCN 利用双曲空间来表现在线对话的无标度动态。通过对真实世界的 Twitter 数据进行广泛的定量、定性和消融实验，我们发现 HCN 的表现优于最先进的方法，同时使用的用户特定数据减少了98% ，同时保持了74% 的低碳足印和94% 的小模型尺寸。我们还发现，在前半个小时内的评论对于识别高危用户是最重要的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Suicide+Ideation+Detection+Through+Online+Conversational+Context)|3|
|[Socially-aware Dual Contrastive Learning for Cold-Start Recommendation](https://doi.org/10.1145/3477495.3531780)|Jing Du, Zesheng Ye, Lina Yao, Bin Guo, Zhiwen Yu|Northwestern Polytechnical University, Xi'an, Shaanxi, China; The University of New South Wales, Sydney, NSW, Australia|Social recommendation with Graph Neural Networks(GNNs) learns to represent cold users by fusing user-user social relations with user-item interactions, thereby alleviating the cold-start problem associated with recommender systems. Despite being well adapted to social relations and user-item interactions, these supervised models are still susceptible to popularity bias. Contrastive learning helps resolve this dilemma by identifying the properties that distinguish positive from negative samples. In its previous combinations with recommender systems, social relationships and cold-start cases in this context are not considered. Also, they primarily focus on collaborative features between users and items, leaving the similarity between items under-utilized. In this work, we propose socially-aware dual contrastive learning for cold-start recommendation, where cold users can be modeled in the same way as warm users. To take full advantage of social relations, we create dynamic node embeddings for each user by aggregating information from different neighbors according to each different query item, in the form of user-item pairs. We further design a dual-branch self-supervised contrastive objective to account for user-item collaborative features and item-item mutual information, respectively. On one hand, our framework eliminates popularity bias with proper negative sampling in contrastive learning, without extra ground-truth supervision. On the other hand, we extend previous contrastive learning methods to provide a solution to cold-start problem with social relations included. Extensive experiments on two real-world social recommendation datasets demonstrate its effectiveness.|图形神经网络的社会推荐学习通过融合用户-用户社会关系和用户-项目交互来表示冷用户，从而缓解推荐系统的冷启动问题。尽管这些被监督的模型很好地适应了社会关系和用户项目交互，但是仍然容易受到流行偏见的影响。对比学习有助于解决这一困境，识别的属性，区分积极的样本和消极的样本。在其以往与推荐系统的结合中，社会关系和这方面的冷启动案例没有得到考虑。此外，他们主要关注用户和项目之间的协作特性，而没有充分利用项目之间的相似性。在这项工作中，我们提出了具有社会意识的双重对比学习的冷启动推荐，其中冷用户可以按照与暖用户相同的方式建模。为了充分利用社会关系，我们根据不同的查询项目，以用户-项目对的形式聚合来自不同邻居的信息，为每个用户创建动态节点嵌入。进一步设计了一个双分支自监督对比目标，分别考虑了用户项目的协同特征和项目项目间的相互信息。一方面，我们的框架消除了流行偏见与适当的负面抽样在对比学习，没有额外的地面真相监督。另一方面，我们扩展了以往的对比学习方法，提供了一个解决冷启动问题，其中包括社会关系。在两个真实世界的社交推荐数据集上进行的大量实验证明了该方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Socially-aware+Dual+Contrastive+Learning+for+Cold-Start+Recommendation)|3|
|[A Multi-Task Based Neural Model to Simulate Users in Goal Oriented Dialogue Systems](https://doi.org/10.1145/3477495.3531814)|To Eun Kim, Aldo Lipani|University College London, London, United Kingdom|A human-like user simulator that anticipates users' satisfaction scores, actions, and utterances can help goal-oriented dialogue systems in evaluating the conversation and refining their dialogue strategies. However, little work has experimented with user simulators which can generate users' utterances. In this paper, we propose a deep learning-based user simulator that predicts users' satisfaction scores and actions while also jointly generating users' utterances in a multi-task manner. In particular, we show that 1) the proposed deep text-to-text multi-task neural model achieves state-of-the-art performance in the users' satisfaction scores and actions prediction tasks, and 2) in an ablation analysis, user satisfaction score prediction, action prediction, and utterance generation tasks can boost the performance with each other via positive transfers across the tasks. The source code and model checkpoints used for the experiments run in this paper are available at the following weblink: \urlhttps://github.com/kimdanny/user-simulation-t5.|一个类人的用户模拟器，可以预测用户的满意度分数、行为和话语，可以帮助目标导向的对话系统评估对话和完善他们的对话策略。然而，很少有工作已经试验用户模拟器，可以产生用户的话语。在本文中，我们提出了一个基于深度学习的用户模拟器，预测用户的满意度分数和行为，同时也联合生成用户的话语在多任务的方式。实验结果表明: (1)本文提出的深度文本-文本多任务神经模型在用户满意度分数和行为预测任务方面达到了最高水平; (2)在消融分析方面，用户满意度分数预测、行为预测和话语生成任务可以通过任务之间的正向传递相互提高性能。本文中用于实验的源代码和模型检查点可以在以下网站找到: urlhttps:// github.com/kimdanny/user-simulation-t5。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Multi-Task+Based+Neural+Model+to+Simulate+Users+in+Goal+Oriented+Dialogue+Systems)|3|
|[Investigating Accuracy-Novelty Performance for Graph-based Collaborative Filtering](https://doi.org/10.1145/3477495.3532005)|Minghao Zhao, Le Wu, Yile Liang, Lei Chen, Jian Zhang, Qilin Deng, Kai Wang, Xudong Shen, Tangjie Lv, Runze Wu|Hefei University of Technology, Hefei, China; Zhejiang University of Technology, Hangzhou, China; Fuxi AI Lab, NetEase Games, Hangzhou, China|Recent years have witnessed the great accuracy performance of graph-based Collaborative Filtering (CF) models for recommender systems. By taking the user-item interaction behavior as a graph, these graph-based CF models borrow the success of Graph Neural Networks (GNN), and iteratively perform neighborhood aggregation to propagate the collaborative signals. While conventional CF models are known for facing the challenges of the popularity bias that favors popular items, one may wonder "Whether the existing graph-based CF models alleviate or exacerbate the popularity bias of recommender systems?" To answer this question, we first investigate the two-fold performances w.r.t. accuracy and novelty for existing graph-based CF methods. The empirical results show that symmetric neighborhood aggregation adopted by most existing graph-based CF models exacerbates the popularity bias and this phenomenon becomes more serious as the depth of graph propagation increases. Further, we theoretically analyze the cause of popularity bias for graph-based CF. Then, we propose a simple yet effective plugin, namely r-AdjNorm, to achieve an accuracy-novelty trade-off by controlling the normalization strength in the neighborhood aggregation process. Meanwhile, r-AdjNorm can be smoothly applied to the existing graph-based CF backbones without additional computation. Finally, experimental results on three benchmark datasets show that our proposed method can improve novelty without sacrificing accuracy under various graph-based CF backbones.|近年来，基于图形的协同过滤(CF)模型在推荐系统中的准确性表现非常出色。这些基于图的协同过程模型借鉴了图神经网络(GNN)的成功之处，以用户项交互行为为图形，迭代地进行邻域聚合来传播协同信号。虽然传统的 CF 模型面临着流行偏好的挑战，有人可能会问: “现有的基于图表的 CF 模型是否减轻或加剧了推荐系统的流行偏好?”为了回答这个问题，我们首先研究了现有的基于图的 CF 方法的双重性能。实验结果表明，现有的基于图的 CF 模型所采用的对称邻域聚集加剧了流行偏差，并且随着图的传播深度的增加，这种现象变得更加严重。进一步从理论上分析了基于图的流行度偏差产生的原因，提出了一种简单有效的插件 r-AdjNorm，通过控制邻域聚合过程中的归一化强度来实现精度-新颖性的权衡。同时，r-AdjNorm 可以顺利地应用于现有的基于图的 CF 骨干，而不需要额外的计算。最后，在三个基准数据集上的实验结果表明，在不同的基于图的 CF 骨架下，本文提出的方法可以在不牺牲精度的前提下提高新颖性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Investigating+Accuracy-Novelty+Performance+for+Graph-based+Collaborative+Filtering)|3|
|[Learning to Denoise Unreliable Interactions for Graph Collaborative Filtering](https://doi.org/10.1145/3477495.3531889)|Changxin Tian, Yuexiang Xie, Yaliang Li, Nan Yang, Wayne Xin Zhao|Renmin University of China, Beijing, China; Alibaba Group, Hangzhou, China; Alibaba Group, Bellevue, WA, USA|Recently, graph neural networks (GNN) have been successfully applied to recommender systems as an effective collaborative filtering (CF) approach. However, existing GNN-based CF models suffer from noisy user-item interaction data, which seriously affects the effectiveness and robustness in real-world applications. Although there have been several studies on data denoising in recommender systems, they either neglect direct intervention of noisy interaction in the message-propagation of GNN, or fail to preserve the diversity of recommendation when denoising. To tackle the above issues, this paper presents a novel GNN-based CF model, named Robust Graph Collaborative Filtering (RGCF), to denoise unreliable interactions for recommendation. Specifically, RGCF consists of a graph denoising module and a diversity preserving module. The graph denoising module is designed for reducing the impact of noisy interactions on the representation learning of GNN, by adopting both a hard denoising strategy (i.e., discarding interactions that are confidently estimated as noise) and a soft denoising strategy (i.e., assigning reliability weights for each remaining interaction). In the diversity preserving module, we build up a diversity augmented graph and propose an auxiliary self-supervised task based on mutual information maximization (MIM) for enhancing the denoised representation and preserving the diversity of recommendation. These two modules are integrated in a multi-task learning manner that jointly improves the recommendation performance. We conduct extensive experiments on three real-world datasets and three synthesized datasets. Experiment results show that RGCF is more robust against noisy interactions and achieves significant improvement compared with baseline models.|最近，图形神经网络(GNN)已成功应用于推荐系统，作为一种有效的协同过滤(CF)方法。然而，现有的基于 GNN 的 CF 模型存在着用户项交互数据的噪声，严重影响了实际应用的有效性和鲁棒性。对于推荐系统中的数据去噪问题，目前已经有了一些研究，但是这些研究要么忽略了噪声交互对 GNN 信息传播的直接干预，要么在去噪时未能保持推荐信息的多样性。为了解决上述问题，本文提出了一种新的基于 GNN 的 CF 模型，称为鲁棒图协同过滤(rgCF) ，用于去除不可靠的推荐交互。具体来说，RGCF 由图的去噪模块和多样性保持模块组成。图形去噪模块是为了减少噪声交互对 GNN 表示学习的影响而设计的，它采用了硬去噪策略(即放弃可信地估计为噪声的交互)和软去噪策略(即为每个剩余交互指定可靠性权重)。在多样性保持模块中，我们建立了一个多样性增强图，并提出了一个基于互信息最大化(MIM)的辅助自监督任务，以提高去噪表示和保持推荐的多样性。这两个模块以多任务学习方式集成，共同提高了推荐性能。我们在三个真实数据集和三个合成数据集上进行了广泛的实验。实验结果表明，与基线模型相比，RGCF 对噪声干扰具有更强的鲁棒性，并取得了显著的改善。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Denoise+Unreliable+Interactions+for+Graph+Collaborative+Filtering)|3|
|[DAWAR: Diversity-aware Web APIs Recommendation for Mashup Creation based on Correlation Graph](https://doi.org/10.1145/3477495.3531962)|Wenwen Gong, Xuyun Zhang, Yifei Chen, Qiang He, Amin Beheshti, Xiaolong Xu, Chao Yan, Lianyong Qi|China Agricultural University, Beijing, China; Swinburne University of Technology, Melbourne, VIC, Australia; Qufu Normal University, Rizhao, China; Nanjing University of Information Science and Technology, Nanjing, China; Macquarie University, Sydney, NSW, Australia; Qufu Normal University & Nanjing University, Rizhao, China|With the ever-increasing popularity of microservice architecture, a considerable number of enterprises or organizations have encapsulated their complex business services into various lightweight functions as published them accessible APIs (Application Programming Interfaces). Through keyword search, a software developer could select a set of APIs from a massive number of candidates to implement the functions of a complex mashup, which reduces the development cost significantly. However, traditional keyword search methods for APIs often suffer from several critical issues such as functional compatibility and limited diversity in search results, which may lead to mashup creation failures and lower development productivity. To deal with these challenges, this paper designs DAWAR, a diversity-aware Web APIs recommendation approach that finds diversified and compatible APIs for mashup creation. Specifically, the APIs recommendation problem for mashup creating is modelled as a graph search problem that aims to find the minimal group Steiner trees in a correlation graph of APIs. DAWAR innovatively employs the determinantal point processes to diversify the recommended results. Empirical evaluation is performed on commonly-used real-world datasets, and the statistic results show that DAWAR is able to achieve significant improvements in terms of recommendation diversity, accuracy, and compatibility.|随着微服务体系结构的日益普及，相当数量的企业或组织已经将其复杂的业务服务封装成各种轻量级功能，并将其发布为可访问的 API (应用程序编程接口)。通过关键字搜索，软件开发人员可以从大量候选 API 中选择一组 API 来实现复杂 mashup 的功能，这大大降低了开发成本。然而，用于 API 的传统关键字搜索方法经常遇到一些关键问题，如功能兼容性和搜索结果的多样性有限，这可能导致 mashup 创建失败和开发效率降低。为了应对这些挑战，本文设计了 DAWAR，这是一种多样性感知的 Web API 推荐方法，它为 mashup 创建寻找多样性和兼容的 API。具体来说，用于 mashup 创建的 API 推荐问题被建模为一个图搜索问题，其目的是在 API 的相关图中找到最小组 Steiner 树。DAWAR 创新地使用决定点过程来使推荐的结果多样化。对现实世界中常用的数据集进行了实证评估，统计结果表明，DAWAR 在推荐多样性、准确性和兼容性方面都有显著的提高。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DAWAR:+Diversity-aware+Web+APIs+Recommendation+for+Mashup+Creation+based+on+Correlation+Graph)|3|
|[Post Processing Recommender Systems with Knowledge Graphs for Recency, Popularity, and Diversity of Explanations](https://doi.org/10.1145/3477495.3532041)|Giacomo Balloccu, Ludovico Boratto, Gianni Fenu, Mirko Marras|University of Cagliari, Cagliari, Italy|Existing explainable recommender systems have mainly modeled relationships between recommended and already experienced products, and shaped explanation types accordingly (e.g., movie "x" starred by actress "y" recommended to a user because that user watched other movies with "y" as an actress). However, none of these systems has investigated the extent to which properties of a single explanation (e.g., the recency of interaction with that actress) and of a group of explanations for a recommended list (e.g., the diversity of the explanation types) can influence the perceived explaination quality. In this paper, we conceptualized three novel properties that model the quality of the explanations (linking interaction recency, shared entity popularity, and explanation type diversity) and proposed re-ranking approaches able to optimize for these properties. Experiments on two public data sets showed that our approaches can increase explanation quality according to the proposed properties, fairly across demographic groups, while preserving recommendation utility. The source code and data are available at https://github.com/giacoballoccu/explanation-quality-recsys.|现有的可解释推荐系统主要模拟推荐产品和已经有经验的产品之间的关系，并相应地形成解释类型(例如，由女演员“ y”主演的电影“ x”被推荐给用户，因为该用户观看了其他以“ y”为女演员的电影)。然而，这些系统都没有研究单一解释(例如，与女演员互动的近期性)和推荐列表的一组解释(例如，解释类型的多样性)的特性在多大程度上影响感知的解释质量。在本文中，我们概念化了三个新的性质来模拟解释的质量(连接互动的新近性，共享实体的流行性和解释类型的多样性) ，并提出了重新排序的方法，能够优化这些性质。对两个公共数据集的实验表明，我们的方法可以提高解释质量根据建议的性质，相当跨人口组，同时保留推荐效用。源代码和数据可在 https://github.com/giacoballoccu/explanation-quality-recsys 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Post+Processing+Recommender+Systems+with+Knowledge+Graphs+for+Recency,+Popularity,+and+Diversity+of+Explanations)|3|
|[Learning Graph-based Disentangled Representations for Next POI Recommendation](https://doi.org/10.1145/3477495.3532012)|Zhaobo Wang, Yanmin Zhu, Haobing Liu, Chunyang Wang|Shanghai Jiao Tong University, Shanghai, China|Next Point-of-Interest (POI) recommendation plays a critical role in many location-based applications as it provides personalized suggestions on attractive destinations for users. Since users' next movement is highly related to the historical visits, sequential methods such as recurrent neural networks are widely used in this task for modeling check-in behaviors. However, existing methods mainly focus on modeling the sequential regularity of check-in sequences but pay little attention to the intrinsic characteristics of POIs, neglecting the entanglement of the diverse influence stemming from different aspects of POIs. In this paper, we propose a novel Disentangled Representation-enhanced Attention Network (DRAN) for next POI recommendation, which leverages the disentangled representations to explicitly model different aspects and corresponding influence for representing a POI more precisely. Specifically, we first design a propagation rule to learn graph-based disentangled representations by refining two types of POI relation graphs, making full use of the distance-based and transition-based influence for representation learning. Then, we extend the attention architecture to aggregate personalized spatio-temporal information for modeling dynamic user preferences on the next timestamp, while maintaining the different components of disentangled representations independent. Extensive experiments on two real-world datasets demonstrate the superior performance of our model to state-of-the-art approaches. Further studies confirm the effectiveness of DRAN in representation disentanglement.|下一个兴趣点(POI)推荐在许多基于位置的应用程序中起着至关重要的作用，因为它为用户提供有吸引力的目的地的个性化建议。由于用户的下一步行动与历史访问量密切相关，因此在这项任务中广泛采用了循环神经网络等顺序方法来建立签入行为模型。然而，现有的方法主要侧重于对检入序列的顺序规律性进行建模，很少关注检入序列的内在特性，忽视了检入序列不同方面所产生的不同影响的纠缠。本文提出了一种新的面向下一个 POI 推荐的分离表示增强注意网络(DRAN) ，该网络利用分离表示对不同方面和相应影响进行显式建模，以更精确地表示一个 POI。具体来说，我们首先通过对两类 POI 关系图的细化，充分利用基于距离和基于转移的影响来学习基于图的分离表示，设计了一种基于图的传播规则来学习基于图的分离表示。然后，我们将注意结构扩展到聚合个性化的时空信息，以便在下一个时间戳上建立动态用户偏好模型，同时保持分离表征的不同组件之间的独立性。在两个真实世界数据集上的大量实验表明，我们的模型的性能优于最先进的方法。进一步的研究证实了 DRAN 在表征解纠缠中的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Graph-based+Disentangled+Representations+for+Next+POI+Recommendation)|3|
|[AutoLossGen: Automatic Loss Function Generation for Recommender Systems](https://doi.org/10.1145/3477495.3531941)|Zelong Li, Jianchao Ji, Yingqiang Ge, Yongfeng Zhang|Rutgers University, New Brunswick, NJ, USA|In recommendation systems, the choice of loss function is critical since a good loss may significantly improve the model performance. However, manually designing a good loss is a big challenge due to the complexity of the problem. A large fraction of previous work focuses on handcrafted loss functions, which needs significant expertise and human effort. In this paper, inspired by the recent development of automated machine learning, we propose an automatic loss function generation framework, AutoLossGen, which is able to generate loss functions directly constructed from basic mathematical operators without prior knowledge on loss structure. More specifically, we develop a controller model driven by reinforcement learning to generate loss functions, and develop iterative and alternating optimization schedule to update the parameters of both the controller model and the recommender model. One challenge for automatic loss generation in recommender systems is the extreme sparsity of recommendation datasets, which leads to the sparse reward problem for loss generation and search. To solve the problem, we further develop a reward filtering mechanism for efficient and effective loss generation. Experimental results show that our framework manages to create tailored loss functions for different recommendation models and datasets, and the generated loss gives better recommendation performance than commonly used baseline losses. Besides, most of the generated losses are transferable, i.e., the loss generated based on one model and dataset also works well for another model or dataset. Source code of the work is available at https://github.com/rutgerswiselab/AutoLossGen.|在推荐系统中，损失函数的选择至关重要，因为一个好的损失可以显著提高模型的性能。然而，由于问题的复杂性，手工设计一个好的损失是一个很大的挑战。以前的大部分工作集中在手工制作的损失功能，这需要大量的专业知识和人力。本文受机器学习的启发，提出了一种自动损失函数生成框架 AutoLossGen，它可以直接由基本的数学算子构造损失函数，而不需要知道损失结构。更具体地说，我们开发了一个由强化学习驱动的控制器模型来产生损失函数，并开发了迭代和交替优化时间表来更新控制器模型和推荐模型的参数。推荐系统中自动丢失生成的一个挑战是推荐数据集的极端稀疏性，这导致了丢失生成和搜索的稀疏奖励问题。为了解决这个问题，我们进一步开发了一个有效的奖励过滤机制，有效地产生损失。实验结果表明，我们的框架能够为不同的推荐模型和数据集创建量身定制的损失函数，并且产生的损失比常用的基准损失提供了更好的推荐性能。此外，产生的损失大部分是可转移的，即基于一个模型和数据集产生的损失也适用于另一个模型或数据集。作品的源代码可于 https://github.com/rutgerswiselab/autolossgen 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AutoLossGen:+Automatic+Loss+Function+Generation+for+Recommender+Systems)|3|
|[MGPolicy: Meta Graph Enhanced Off-policy Learning for Recommendations](https://doi.org/10.1145/3477495.3532021)|Xiangmeng Wang, Qian Li, Dianer Yu, Zhichao Wang, Hongxu Chen, Guandong Xu|University of New South Wales, Sydney, NSW, Australia; Curtin University, Perth, WA, Australia; University of Technology Sydney, Sydney, NSW, Australia|Off-policy learning has drawn huge attention in recommender systems (RS), which provides an opportunity for reinforcement learning to abandon the expensive online training. However, off-policy learning from logged data suffers biases caused by the policy shift between the target policy and the logging policy. Consequently, most off-policy learning resorts to inverse propensity scoring (IPS) which however tends to be over-fitted over exposed (or recommended) items and thus fails to explore unexposed items. In this paper, we propose meta graph enhanced off-policy learning (MGPolicy), which is the first recommendation model for correcting the off-policy bias via contextual information. In particular, we explicitly leverage rich semantics in meta graphs for user state representation, and then train the candidate generation model to promote an efficient search in the action space. lMoreover, our MGpolicy is designed with counterfactual risk minimization, which can correct poicy learning bias and ultimately yield an effective target policy to maximize the long-run rewards for the recommendation. We extensively evaluate our method through a series of simulations and large-scale real-world datasets, achieving favorable results compared with state-of-the-art methods. Our code is currently available online.|非政策性学习在推荐系统(RS)中引起了巨大的关注，它为强化学习提供了一个放弃昂贵的在线培训的机会。然而，由于目标策略和日志策略之间的策略转移，从日志数据中进行的非策略学习会产生偏差。因此，大多数非政策学习诉诸于逆倾向评分(IPS) ，然而倾向于过度拟合暴露(或推荐)项目，因此未能探索未暴露项目。本文提出了元图增强的非策略学习(MGPolicy)模型，这是第一个通过上下文信息来纠正非策略偏差的推荐模型。特别地，我们显式地利用元图中丰富的语义来表示用户的状态，然后训练候选生成模型来促进行动空间中的有效搜索。此外，我们的 MG 策略是设计与反事实风险最小化，这可以纠正策略学习偏差，并最终产生一个有效的目标策略，以最大限度地长期回报的建议。我们通过一系列的模拟和大规模的真实世界数据集对我们的方法进行了广泛的评估，与最先进的方法相比取得了良好的结果。我们的代码目前可在线使用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MGPolicy:+Meta+Graph+Enhanced+Off-policy+Learning+for+Recommendations)|3|
|[Privacy-Preserving Synthetic Data Generation for Recommendation Systems](https://doi.org/10.1145/3477495.3532044)|Fan Liu, Zhiyong Cheng, Huilin Chen, Yinwei Wei, Liqiang Nie, Mohan S. Kankanhalli|Qilu University of Technology (Shandong Artificial Intelligence Institute), Jinan, China; National University of Singapore, Singapore, Singapore; Shandong University, Jinan, China; Tianjin University of Technology, Tianjin, China|Recommendation systems make predictions chiefly based on users' historical interaction data (e.g., items previously clicked or purchased). There is a risk of privacy leakage when collecting the users' behavior data for building the recommendation model. However, existing privacy-preserving solutions are designed for tackling the privacy issue only during the model training [32] and results collection [40] phases. The problem of privacy leakage still exists when directly sharing the private user interaction data with organizations or releasing them to the public. To address this problem, in this paper, we present a User Privacy Controllable Synthetic Data Generation model (short for UPC-SDG), which generates synthetic interaction data for users based on their privacy preferences. The generation model aims to provide certain privacy guarantees while maximizing the utility of the generated synthetic data at both data level and item level. Specifically, at the data level, we design a selection module that selects those items that contribute less to a user's preferences from the user's interaction data. At the item level, a synthetic data generation module is proposed to generate a synthetic item corresponding to the selected item based on the user's preferences. Furthermore, we also present a privacy-utility trade-off strategy to balance the privacy and utility of the synthetic data. Extensive experiments and ablation studies have been conducted on three publicly accessible datasets to justify our method, demonstrating its effectiveness in generating synthetic data under users' privacy preferences.|推荐系统主要根据用户的历史交互数据(例如，以前点击或购买的项目)进行预测。在收集用户行为数据以构建推荐模型时，存在隐私泄漏风险。然而，现有的保护隐私的解决方案只是为了在模型培训[32]和结果收集[40]阶段解决隐私问题而设计的。在与组织直接共享或向公众发布用户交互数据时，仍然存在隐私泄露问题。针对这一问题，本文提出了一种用户隐私可控合成数据生成模型(UPC-SDG) ，该模型根据用户的隐私偏好为用户生成合成交互数据。该生成模型旨在提供一定的隐私保护，同时最大限度地利用生成的合成数据在数据级和项级。具体来说，在数据级别，我们设计一个选择模块，从用户的交互数据中选择那些对用户偏好贡献较小的项目。在项目级，提出了一个综合数据生成模块，该模块根据用户的偏好生成与所选项目相对应的综合项目。此外，我们还提出了一个隐私-效用权衡策略来平衡合成数据的隐私和效用。在三个公开的数据集上进行了广泛的实验和消融研究，证明了我们的方法在用户隐私偏好下生成合成数据的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Privacy-Preserving+Synthetic+Data+Generation+for+Recommendation+Systems)|3|
|[Self-Guided Learning to Denoise for Robust Recommendation](https://doi.org/10.1145/3477495.3532059)|Yunjun Gao, Yuntao Du, Yujia Hu, Lu Chen, Xinjun Zhu, Ziquan Fang, Baihua Zheng|Singapore Management University, Singapore, Singapore; Zhejiang University, Hangzhou, China|The ubiquity of implicit feedback makes them the default choice to build modern recommender systems. Generally speaking, observed interactions are considered as positive samples, while unobserved interactions are considered as negative ones. However, implicit feedback is inherently noisy because of the ubiquitous presence of noisy-positive and noisy-negative interactions. Recently, some studies have noticed the importance of denoising implicit feedback for recommendations, and enhanced the robustness of recommendation models to some extent. Nonetheless, they typically fail to (1) capture the hard yet clean interactions for learning comprehensive user preference, and (2) provide a universal denoising solution that can be applied to various kinds of recommendation models. In this paper, we thoroughly investigate the memorization effect of recommendation models, and propose a new denoising paradigm, i.e., Self-Guided Denoising Learning (SGDL), which is able to collect memorized interactions at the early stage of the training (i.e., ''noise-resistant'' period), and leverage those data as denoising signals to guide the following training (i.e., ''noise-sensitive'' period) of the model in a meta-learning manner. Besides, our method can automatically switch its learning phase at the memorization point from memorization to self-guided learning, and select clean and informative memorized data via a novel adaptive denoising scheduler to improve the robustness. We incorporate SGDL with four representative recommendation models (i.e., NeuMF, CDAE, NGCF and LightGCN) and different loss functions (i.e., binary cross-entropy and BPR loss). The experimental results on three benchmark datasets demonstrate the effectiveness of SGDL over the state-of-the-art denoising methods like T-CE, IR, DeCA, and even state-of-the-art robust graph-based methods like SGCN and SGL.|无处不在的隐式反馈使它们成为构建现代推荐系统的默认选择。一般来说，观察到的相互作用被认为是积极的样本，而未观察到的相互作用被认为是消极的。然而，由于噪声-正向和噪声-负向相互作用的普遍存在，隐式反馈本质上是有噪声的。近年来，一些研究已经注意到去除推荐内隐反馈的重要性，并在一定程度上增强了推荐模型的鲁棒性。尽管如此，它们通常无法(1)捕捉到用于学习全面的用户偏好的硬而干净的交互，(2)提供一个可以应用于各种推荐模型的通用去噪解决方案。本文对推荐模型的记忆效应进行了深入研究，提出了一种新的去噪范式，即自我引导去噪学习(SGDL) ，它能够在训练的早期阶段(即“噪声抵抗期”)收集记忆交互信息，并利用这些数据作为去噪信号，以元学习的方式指导模型的后续训练(即“噪声敏感期”)。此外，该方法可以在记忆点自动切换学习阶段，由记忆阶段转变为自我引导学习阶段，并通过一种新的自适应去噪调度器选择清晰、信息丰富的记忆数据，提高了算法的鲁棒性。我们将 SGDL 与四个代表性的推荐模型(即 NeuMF，CDAE，NGCF 和 LightGCN)和不同的损失函数(即二进制交叉熵和 BPR 损失)结合起来。在三个基准数据集上的实验结果证明了 SGDL 相对于最先进的去噪方法如 T-CE、 IR、 DeCA，甚至是最先进的基于图的鲁棒性方法如 SGCN 和 SGL 的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Guided+Learning+to+Denoise+for+Robust+Recommendation)|3|
|[Faster Learned Sparse Retrieval with Guided Traversal](https://doi.org/10.1145/3477495.3531774)|Antonio Mallia, Joel Mackenzie, Torsten Suel, Nicola Tonellotto|New York University, Brooklyn, NY, USA; University of Pisa, Pisa, Italy; The University of Queensland, Brisbane, Australia|Neural information retrieval architectures based on transformers such as BERT are able to significantly improve system effectiveness over traditional sparse models such as BM25. Though highly effective, these neural approaches are very expensive to run, making them difficult to deploy under strict latency constraints. To address this limitation, recent studies have proposed new families of learned sparse models that try to match the effectiveness of learned dense models, while leveraging the traditional inverted index data structure for efficiency. Current learned sparse models learn the weights of terms in documents and, sometimes, queries; however, they exploit different vocabulary structures, document expansion techniques, and query expansion strategies, which can make them slower than traditional sparse models such as BM25. In this work, we propose a novel indexing and query processing technique that exploits a traditional sparse model's "guidance" to efficiently traverse the index, allowing the more effective learned model to execute fewer scoring operations. Our experiments show that our guided processing heuristic is able to boost the efficiency of the underlying learned sparse model by a factor of four without any measurable loss of effectiveness.|基于诸如 BERT 等变压器的神经信息检索架构能够比传统的稀疏模型如 BM25显著提高系统效率。尽管这些神经方法非常有效，但是运行成本非常高，因此在严格的延迟约束下很难部署它们。为了解决这一局限性，最近的研究提出了新的学习稀疏模型系列，试图匹配学习密集模型的有效性，同时利用传统的倒排索引数据结构来提高效率。目前学习的稀疏模型学习文档中的术语权重，有时也学习查询; 然而，它们利用不同的词汇结构、文档扩展技术和查询扩展策略，这使得它们比传统的稀疏模型如 BM25更慢。在这项工作中，我们提出了一个新的索引和查询处理技术，利用传统的稀疏模型的“指导”，有效地遍历索引，使更有效的学习模型执行更少的评分操作。我们的实验表明，我们的引导处理启发式能够提高效率的基础学习稀疏模型的四倍，没有任何可测量的效率损失。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Faster+Learned+Sparse+Retrieval+with+Guided+Traversal)|3|
|[Analysing the Robustness of Dual Encoders for Dense Retrieval Against Misspellings](https://doi.org/10.1145/3477495.3531818)|Georgios Sidiropoulos, Evangelos Kanoulas|University of Amsterdam, Amsterdam, Netherlands|Dense retrieval is becoming one of the standard approaches for document and passage ranking. The dual-encoder architecture is widely adopted for scoring question-passage pairs due to its efficiency and high performance. Typically, dense retrieval models are evaluated on clean and curated datasets. However, when deployed in real-life applications, these models encounter noisy user-generated text. That said, the performance of state-of-the-art dense retrievers can substantially deteriorate when exposed to noisy text. In this work, we study the robustness of dense retrievers against typos in the user question. We observe a significant drop in the performance of the dual-encoder model when encountering typos and explore ways to improve its robustness by combining data augmentation with contrastive learning. Our experiments on two large-scale passage ranking and open-domain question answering datasets show that our proposed approach outperforms competing approaches. Additionally, we perform a thorough analysis on robustness. Finally, we provide insights on how different typos affect the robustness of embeddings differently and how our method alleviates the effect of some typos but not of others.|密集检索正在成为文献和段落排序的标准方法之一。双编码器结构以其高效、高性能的特点被广泛应用于问答题-段落对的评分。通常，密集检索模型是在干净和精选的数据集上进行评估的。然而，当部署到实际应用程序中时，这些模型会遇到用户生成的嘈杂文本。也就是说，当暴露在嘈杂的文本中时，最先进的稠密检索器的性能会大大恶化。在这项工作中，我们研究了密集检索器对用户问题中的输入错误的鲁棒性。我们观察到当遇到拼写错误时，双编码器模型的性能显著下降，并探索通过数据增强和对比学习相结合来提高其鲁棒性的方法。我们在两个大规模段落排序和开放领域问答数据集上的实验表明，我们提出的方法优于竞争方法。此外，我们还对鲁棒性进行了全面的分析。最后，我们提供了关于不同类型错误如何不同地影响嵌入的稳健性的见解，以及我们的方法如何减轻一些类型错误的影响，而不是其他的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Analysing+the+Robustness+of+Dual+Encoders+for+Dense+Retrieval+Against+Misspellings)|3|
|[Cross-Probe BERT for Fast Cross-Modal Search](https://doi.org/10.1145/3477495.3531826)|Tan Yu, Hongliang Fei, Ping Li|Baidu Research, Bellevue, WA, USA|Owing to the effectiveness of cross-modal attentions, text-vision BERT models have achieved excellent performance in text-image retrieval. Nevertheless, cross-modal attentions in text-vision BERT models require expensive computation cost when tackling text-vision retrieval due to their pairwise input. Therefore, normally, it is impractical for deploying them for large-scale cross-modal retrieval in real applications. To address the inefficiency issue in exiting text-vision BERT models, in this work, we develop a novel architecture, cross-probe BERT. It devises a small number of text and vision probes, and the cross-modal attentions are efficiency achieved through the interactions between text and vision probes. It takes lightweight computation cost, and meanwhile effectively exploits cross-modal attention. Systematic experiments on public benchmarks demonstrate the excellent effectiveness and efficiency of our cross-probe BERT.|由于跨模态注意的有效性，文本视觉 BERT 模型在文本图像检索中取得了良好的效果。然而，文本视觉 BERT 模型中的跨模式注意力由于其成对输入而在处理文本视觉检索时需要昂贵的计算成本。因此，在实际应用中部署它们进行大规模的跨模态检索通常是不切实际的。为了解决现有文本视觉 BERT 模型效率低下的问题，本文提出了一种新的体系结构——交叉探测 BERT。它设计了少量的文本和视觉探测器，通过文本和视觉探测器之间的交互实现了跨模态注意的有效性。它具有计算量小的特点，同时有效地利用了交叉模态注意力。通过对公共基准测试的系统实验，验证了交叉探测误码率测试的有效性和高效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cross-Probe+BERT+for+Fast+Cross-Modal+Search)|3|
|[DH-HGCN: Dual Homogeneity Hypergraph Convolutional Network for Multiple Social Recommendations](https://doi.org/10.1145/3477495.3531828)|Jiadi Han, Qian Tao, Yufei Tang, Yuhan Xia|Florida Atlantic University, Boca Raton, FL, USA; Northeastern University, Shenyang, China; South China University of Technology, Guangzhou, China|Social relations are often used as auxiliary information to improve recommendations. In the real-world, social relations among users are complex and diverse. However, most existing recommendation methods assume only single social relation (i.e., exploit pairwise relations to mine user preferences), ignoring the impact of multifaceted social relations on user preferences (i.e., high order complexity of user relations). Moreover, an observing fact is that similar items always have similar attractiveness when exposed to users, indicating a potential connection among the static attributes of items. Here, we advocate modeling the dual homogeneity from social relations and item connections by hypergraph convolution networks, named DH-HGCN, to obtain high-order correlations among users and items. Specifically, we use sentiment analysis to extract comment relation and use the k-means clustering to construct item-item correlations, and we then optimize those heterogeneous graphs in a unified framework. Extensive experiments on two real-world datasets demonstrate the effectiveness of our model.|社会关系往往被用作辅助信息，以改善建议。在现实世界中，用户之间的社会关系是复杂多样的。然而，大多数现有的推荐方法只假设单一的社会关系(即，利用成对关系挖掘用户偏好) ，忽略了多方面的社会关系对用户偏好的影响(即，用户关系的高度复杂性)。此外，一个观察事实是，相似的项目总是具有相似的吸引力时，暴露给用户，表明项目的静态属性之间的潜在联系。本文主张利用超图卷积网络(DH-HGCN)从社会关系和项目连接两方面建立双同质性模型，以获得用户和项目之间的高阶相关性。具体来说，我们使用情绪分析来提取评论关系，并使用 K平均算法来构建项目-项目关联，然后在一个统一的框架内优化这些异构图。在两个实际数据集上的大量实验证明了该模型的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DH-HGCN:+Dual+Homogeneity+Hypergraph+Convolutional+Network+for+Multiple+Social+Recommendations)|3|
|[To Interpolate or not to Interpolate: PRF, Dense and Sparse Retrievers](https://doi.org/10.1145/3477495.3531884)|Hang Li, Shuai Wang, Shengyao Zhuang, Ahmed Mourad, Xueguang Ma, Jimmy Lin, Guido Zuccon|The University of Queensland, Brisbane, QLD, Australia; University of Waterloo, Waterloo, ON, Canada; The University of Queensland, Brisbane, VIC, Australia|Current pre-trained language model approaches to information retrieval can be broadly divided into two categories: sparse retrievers (to which belong also non-neural approaches such as bag-of-words methods, e.g., BM25) and dense retrievers. Each of these categories appears to capture different characteristics of relevance. Previous work has investigated how relevance signals from sparse retrievers could be combined with those from dense retrievers via interpolation. Such interpolation would generally lead to higher retrieval effectiveness. In this paper we consider the problem of combining the relevance signals from sparse and dense retrievers in the context of Pseudo Relevance Feedback (PRF). This context poses two key challenges: (1) When should interpolation occur: before, after, or both before and after the PRF process? (2) Which sparse representation should be considered: a zero-shot bag-of-words model (BM25), or a learned sparse representation? To answer these questions we perform a thorough empirical evaluation considering an effective and scalable neural PRF approach (Vector-PRF), three effective dense retrievers (ANCE, TCTv2, DistillBERT), and one state-of-the-art learned sparse retriever (uniCOIL). The empirical findings from our experiments suggest that, regardless of sparse representation and dense retriever, interpolation both before and after PRF achieves the highest effectiveness across most datasets and metrics.|目前预先训练的信息检索语言模型方法可以大致分为两类: 稀疏检索器(也属于非神经方法，如单词袋法，例如，BM25)和稠密检索器。这些类别中的每一个似乎都捕获了不同的相关性特征。先前的工作已经研究了如何通过插值将稀疏检索器的相关信号与密集检索器的相关信号相结合。这样的插值通常会导致更高的检索效率。在本文中，我们考虑了在伪关联反馈(PRF)的情况下合并稀疏和密集检索器的相关信号的问题。这种情况提出了两个关键的挑战: (1)什么时候应该插值发生: 之前，之后，或两者之前和之后的 PRF 过程？(2)应该考虑哪种稀疏表示法: 零词袋模型表示法(BM25) ，还是学习型稀疏表示法？为了回答这些问题，我们考虑到有效和可扩展的神经 PRF 方法(Vector-PRF) ，三个有效的稠密检索器(ANCE，TCTv2，DistillBERT)和一个最先进的稀疏检索器(uniCOIL)进行了彻底的经验评估。我们的实验结果表明，不管稀疏表示和密集检索，在 PRF 之前和之后的插值在大多数数据集和度量中都达到了最高的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=To+Interpolate+or+not+to+Interpolate:+PRF,+Dense+and+Sparse+Retrievers)|3|
|[Selective Fairness in Recommendation via Prompts](https://doi.org/10.1145/3477495.3531913)|Yiqing Wu, Ruobing Xie, Yongchun Zhu, Fuzhen Zhuang, Xiang Ao, Xu Zhang, Leyu Lin, Qing He|Beihang University, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences & University of Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, University of Chinese Academy of Sciences, & Tencent, Beijing, China; Tencent, Beijing, China|Recommendation fairness has attracted great attention recently. In real-world systems, users usually have multiple sensitive attributes (e.g. age, gender, and occupation), and users may not want their recommendation results influenced by those attributes. Moreover, which of and when these user attributes should be considered in fairness-aware modeling should depend on users' specific demands. In this work, we define the selective fairness task, where users can flexibly choose which sensitive attributes should the recommendation model be bias-free. We propose a novel parameter-efficient prompt-based fairness-aware recommendation (PFRec) framework, which relies on attribute-specific prompt-based bias eliminators with adversarial training, enabling selective fairness with different attribute combinations on sequential recommendation. Both task-specific and user-specific prompts are considered. We conduct extensive evaluations to verify PFRec's superiority in selective fairness. The source codes are released in \urlhttps://github.com/wyqing20/PFRec.|推荐公平性近年来引起了人们的广泛关注。在现实世界的系统中，用户通常有多个敏感属性(例如年龄、性别和职业) ，用户可能不希望他们的推荐结果受到这些属性的影响。此外，在公平感知的建模中，应该考虑哪些用户属性以及何时考虑这些用户属性，应该取决于用户的具体需求。在本文中，我们定义了选择性公平任务，用户可以灵活地选择推荐模型无偏差的敏感属性。我们提出了一种新的参数高效的基于提示的公平感知推荐(PFRec)框架，该框架依赖于具有对抗性训练的特定属性的基于提示的偏差消除器，使得在顺序推荐上具有不同属性组合的选择性公平性成为可能。同时考虑特定于任务和特定于用户的提示。我们进行了广泛的评估，以验证 PFRec 在选择公平性方面的优越性。源代码以 urlhttps:// github.com/wyqing20/pfrec 发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Selective+Fairness+in+Recommendation+via+Prompts)|3|
|[ReLoop: A Self-Correction Continual Learning Loop for Recommender Systems](https://doi.org/10.1145/3477495.3531922)|Guohao Cai, Jieming Zhu, Quanyu Dai, Zhenhua Dong, Xiuqiang He, Ruiming Tang, Rui Zhang|www.ruizhang.info, Shenzhen, China; Shenzhen, Huawei Noah's Ark Lab, China; Huawei Noah's Ark Lab, Shenzhen, China|Deep learning-based recommendation has become a widely adopted technique in various online applications. Typically, a deployed model undergoes frequent re-training to capture users' dynamic behaviors from newly collected interaction logs. However, the current model training process only acquires users' feedbacks as labels, but fails to take into account the errors made in previous recommendations. Inspired by the intuition that humans usually reflect and learn from mistakes, in this paper, we attempt to build a self-correction continual learning loop (dubbed ReLoop) for recommender systems. In particular, a new customized loss is employed to encourage every new model version to reduce prediction errors over the previous model version during training. Our ReLoop learning framework enables a continual self-correction process in the long run and thus is expected to obtain better performance over existing training strategies. Both offline experiments and an online A/B test have been conducted to validate the effectiveness of ReLoop.|基于深度学习的推荐已经成为各种在线应用中广泛采用的技术。通常，已部署的模型要经过频繁的重新训练，以从新收集的交互日志中捕获用户的动态行为。然而，目前的模型培训过程只是获取用户的反馈作为标签，而没有考虑到以前的建议中的错误。受到人类常常反思和从错误中学习的直觉的启发，本文尝试为推荐系统构建一个自我修正的连续学习循环(称为 ReLoop)。特别是，一个新的定制损失被用来鼓励每一个新的模型版本，以减少训练期间的预测错误超过以前的模型版本。我们的 ReLoop 学习框架能够在长期内实现持续的自我修正过程，因此预计将获得比现有培训策略更好的性能。离线实验和在线 A/B 测试都验证了 ReLoop 的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ReLoop:+A+Self-Correction+Continual+Learning+Loop+for+Recommender+Systems)|3|
|[SoChainDB: A Database for Storing and Retrieving Blockchain-Powered Social Network Data](https://doi.org/10.1145/3477495.3531735)|Hoang H. Nguyen, Dmytro Bozhkov, Zahra Ahmadi, NhatMinh Nguyen, ThanhNam Doan|Independent Researcher, Atlanta, GA, USA; L3S Research Center, Leibniz Universität Hannover, Hannover, Germany; Sunshine Tech Ho Chi Minh, Ho Chi Minh City, Vietnam|Social networks have become an inseparable part of human activities. Most existing social networks follow a centralized system model, which despite storing valuable information of users, arise many critical concerns such as content ownership and over-commercialization. Recently, decentralized social networks, built primarily on blockchain technology, have been proposed as a substitution to eliminate these concerns. Since decentralized architectures are mature enough to be on par with the centralized ones, decentralized social networks are becoming more and more popular. Decentralized social networks can offer both common options like writing posts and comments and more advanced options such as reward systems and voting mechanisms. They provide rich eco-systems for the influencers to interact with their followers and other users via staking systems based on cryptocurrency tokens. The vast and valuable data of the decentralized social networks open several new directions for the research community to extend human behavior knowledge. However, accessing and collecting data from these social networks is not easy because it requires strong blockchain knowledge, which is not the main focus of computer science and social science researchers. Hence, our work proposes the SoChainDB framework that facilitates obtaining data from these new social networks. To show the capacity and strength of SoChainDB, we crawl and publish Hive data - one of the largest blockchain-based social networks. We conduct extensive analyses to understand the insight of Hive data and discuss some interesting applications, e.g., game, non-fungible tokens market built upon Hive. It is worth mentioning that our framework is well-adaptable to other blockchain social networks with minimal modification. SoChainDB is publicly accessible at http://sochaindb.com and the dataset is available under the CC BY-SA 4.0 license.|社交网络已经成为人类活动不可分割的一部分。大多数现有的社交网络遵循一种集中的系统模型，尽管它存储了用户的有价值的信息，但是也引起了许多关键的问题，如内容所有权和过度商业化。最近，分散的社会网络，主要建立在区块链技术，已提出作为一种替代，以消除这些关注。由于分散式体系结构已经足够成熟，可以与集中式体系结构相媲美，分散式社交网络正变得越来越流行。分散的社交网络既可以提供一般的选择，比如写帖子和评论，也可以提供更高级的选择，比如奖励制度和投票机制。它们为影响者提供了丰富的生态系统，通过基于加密货币令牌的标注系统与其追随者和其他用户进行交互。去中心化的社会网络的大量有价值的数据为研究团体拓展人类行为知识开辟了几个新的方向。然而，从这些社交网络访问和收集数据并不容易，因为它需要强大的区块链知识，这并不是计算机科学和社会科学研究人员的主要重点。因此，我们的工作提出了 SoChainDB 框架，该框架有助于从这些新的社交网络获取数据。为了展示 SoChainDB 的能力和强度，我们抓取并发布了 Hive 数据——最大的基于区块链的社交网络之一。我们进行了广泛的分析，以了解蜂巢数据的洞察力，并讨论了一些有趣的应用程序，例如，游戏，不可替代的令牌市场建立在蜂巢。值得一提的是，我们的框架可以很好地适应其他区块链社交网络，只需要最小限度的修改。SochainDB 可以在 http://SoChainDB.com 上公开访问，并且数据集可以在 CC BY-SA 4.0许可下获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SoChainDB:+A+Database+for+Storing+and+Retrieving+Blockchain-Powered+Social+Network+Data)|3|
|[From Little Things Big Things Grow: A Collection with Seed Studies for Medical Systematic Review Literature Search](https://doi.org/10.1145/3477495.3531748)|Shuai Wang, Harrisen Scells, Justin Clark, Bevan Koopman, Guido Zuccon|Bond Institute for Evidence-Based Healthcare, Gold Coast, Australia; CSIRO, Herston, Australia; The University of Queensland, St Lucia, QLD, Australia|Medical systematic review query formulation is a highly complex task done by trained information specialists. Complexity comes from the reliance on lengthy Boolean queries, which express a detailed research question. To aid query formulation, information specialists use a set of exemplar documents, called 'seed studies', prior to query formulation. Seed studies help verify the effectiveness of a query prior to the full assessment of retrieved studies. Beyond this use of seeds, specific IR methods can exploit seed studies for guiding both automatic query formulation and new retrieval models. One major limitation of work to date is that these methods exploit 'pseudo seed studies' through retrospective use of included studies (i.e., relevance assessments). However, we show pseudo seed studies are not representative of real seed studies used by information specialists. Hence, we provide a test collection with real world seed studies used to assist with the formulation of queries. To support our collection, we provide an analysis, previously not possible, on how seed studies impact retrieval and perform several experiments using seed study based methods to compare the effectiveness of using seed studies versus pseudo seed studies. We make our test collection and the results of all of our experiments and analysis available at http://github.com/ielab/sysrev-seed-collection.|医疗系统综述查询制定是一项高度复杂的任务，由训练有素的信息专家完成。复杂性来自于对冗长的布尔查询的依赖，布尔查询表达了一个详细的研究问题。为了帮助查询公式化，信息专家在查询公式化之前使用一组示例文档，称为“种子研究”。种子研究有助于在对检索到的研究进行全面评估之前验证查询的有效性。除了使用种子之外，特定的 IR 方法还可以利用种子研究来指导自动查询表达和新的检索模型。迄今为止工作的一个主要局限性是，这些方法通过回顾性使用纳入研究(即相关性评估)来利用“伪种子研究”。然而，我们表明，伪种子研究并不代表真正的种子研究所使用的信息专家。因此，我们提供了一个具有真实世界种子研究的测试集合，这些种子研究用于帮助制定查询。为了支持我们的收集，我们提供了一个以前不可能的关于种子研究如何影响检索的分析，并使用基于种子研究的方法进行了几个实验，以比较使用种子研究和伪种子研究的有效性。我们把我们的测试收集和所有实验和分析的结果在 http://github.com/ielab/sysrev-seed-collection 上公布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Little+Things+Big+Things+Grow:+A+Collection+with+Seed+Studies+for+Medical+Systematic+Review+Literature+Search)|3|
|[Assessing Student's Dynamic Knowledge State by Exploring the Question Difficulty Effect](https://doi.org/10.1145/3477495.3531939)|Shuanghong Shen, Zhenya Huang, Qi Liu, Yu Su, Shijin Wang, Enhong Chen|School of Computer Science and Technology, Hefei Normal University & Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, Hefei, China; Anhui Province Key Laboratory of Big Data Analysis and Application, School of Computer Science and Technology, University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence, Hefei, China; Anhui Province Key Laboratory of Big Data Analysis and Application, School of Data Science, University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence, Hefei, China; State Key Laboratory of Cognitive Intelligence & iFLYTEK AI Research (Central China), iFLYTEK Co., Ltd, Hefei, China|Knowledge Tracing (KT), which aims to assess students' dynamic knowledge states when practicing on various questions, is a fundamental research task for offering intelligent services in online learning systems. Researchers have devoted significant efforts to developing KT models with impressive performance. However, in existing KT methods, the related question difficulty level, which directly affects students' knowledge state in learning, has not been effectively explored and employed. In this paper, we focus on exploring the question difficulty effect on learning to improve student's knowledge state assessment and propose the DIfficulty Matching Knowledge Tracing (DIMKT) model. Specifically, we first explicitly incorporate the difficulty level into the question representation. Then, to establish the relation between students' knowledge state and the question difficulty level during the practice process, we accordingly design an adaptive sequential neural network in three stages: (1) measuring students' subjective feelings of the question difficulty before practice; (2) estimating students' personalized knowledge acquisition while answering questions of different difficulty levels; (3) updating students' knowledge state in varying degrees to match the question difficulty level after practice. Finally, we conduct extensive experiments on real-world datasets, and the results demonstrate that DIMKT outperforms state-of-the-art KT models. Moreover, DIMKT shows superior interpretability by exploring the question difficulty effect when making predictions. Our codes are available at https://github.com/shshen-closer/DIMKT.|知识追踪(KT)是在线学习系统中提供智能服务的基础性研究课题，其目的是评估学生在各种问题上的动态知识状态。研究人员已经投入了大量的努力来开发具有令人印象深刻的性能的 KT 模型。然而，在现有的 KT 教学方法中，直接影响学生学习知识状态的相关问题难度水平还没有得到有效的探索和应用。本文着重探讨了问题难度对学习的影响，以提高学生的知识状态评价，并提出了难度匹配知识跟踪模型。具体来说，我们首先明确地将难度水平纳入问题表征。然后，为了建立学生在练习过程中的知识状态与问题难度水平之间的关系，我们设计了一个自适应序贯神经网络: (1)在练习前测量学生对问题难度的主观感受; (2)在回答不同难度水平的问题时估计学生的个性化知识获得; (3)在练习后不同程度地更新学生的知识状态以匹配问题难度水平。最后，我们在实际数据集上进行了广泛的实验，结果表明 DIMKT 模型的性能优于最先进的 KT 模型。此外，DIMKT 在预测问题时通过探索问题难度效应显示出更好的可解释性。我们的代码可以在 https://github.com/shshen-closer/dimkt 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Assessing+Student's+Dynamic+Knowledge+State+by+Exploring+the+Question+Difficulty+Effect)|3|
|[MetaCare++: Meta-Learning with Hierarchical Subtyping for Cold-Start Diagnosis Prediction in Healthcare Data](https://doi.org/10.1145/3477495.3532020)|Yanchao Tan, Carl Yang, Xiangyu Wei, Chaochao Chen, Weiming Liu, Longfei Li, Jun Zhou, Xiaolin Zheng|Ant Group, Hangzhou, China; Emory University, Atlanta, GA, USA; Zhejiang University, Hangzhou, China|Cold-start diagnosis prediction is a challenging task for AI in healthcare, where often only a few visits per patient and a few observations per disease can be exploited. Although meta-learning is widely adopted to address the data sparsity problem in general domains, directly applying it to healthcare data is less effective, since it is unclear how to capture both the temporal relations in clinical visits and the complicated relations among syndromic diseases for precise personalized diagnosis. To this end, we first propose a novel Meta-learning framework for cold-start diagnosis prediction in healthCare data (MetaCare). By explicitly encoding the effects of disease progress over time as a generalization prior, MetaCare dynamically predicts future diagnosis and timestamp for infrequent patients. Then, to model complicated relations among rare diseases, we propose to utilize domain knowledge of hierarchical relations among diseases, and further perform diagnosis subtyping to mine the latent syndromic relations among diseases. Finally, to tailor the generic meta-learning framework with personalized parameters, we design a hierarchical patient subtyping mechanism and bridge the modeling of both infrequent patients and rare diseases. We term the joint model as MetaCare++. Extensive experiments on two real-world benchmark datasets show significant performance gains brought by MetaCare++, yielding average improvements of 7.71% for diagnosis prediction and 13.94% for diagnosis time prediction over the state-of-the-art baselines.|冷启动诊断预测是医疗保健中 AI 的一项具有挑战性的任务，通常每个病人只有几次就诊，每个疾病只有几次观察。尽管元学习被广泛用于解决一般领域的数据稀疏问题，但直接应用于医疗保健数据的效果较差，因为目前还不清楚如何捕获临床访视中的时间关系和综合征疾病之间的复杂关系以进行精确的个性化诊断。为此，我们首先提出了一个新的元学习框架，用于医疗保健数据(MetaCare)中的冷启动诊断预测。通过明确编码疾病进展随时间推移的影响作为一个普遍的先例，MetaCare 动态预测未来的诊断和时间戳的罕见病人。然后，利用疾病之间等级关系的领域知识，进一步进行诊断分型，挖掘疾病之间潜在的综合征关系，建立罕见病之间复杂关系的模型。最后，为了使通用的元学习框架具有个性化的参数，我们设计了一个分层的患者亚型机制，并在罕见患者和罕见疾病的建模之间架起了桥梁。我们将联合模型称为 MetaCare + + 。在两个真实世界基准数据集上的广泛实验显示 MetaCare + + 带来了显着的性能提高，在最先进的基线上诊断预测平均提高了7.71% ，诊断时间预测平均提高了13.94% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MetaCare++:+Meta-Learning+with+Hierarchical+Subtyping+for+Cold-Start+Diagnosis+Prediction+in+Healthcare+Data)|3|
|[Generalizing to the Future: Mitigating Entity Bias in Fake News Detection](https://doi.org/10.1145/3477495.3531816)|Yongchun Zhu, Qiang Sheng, Juan Cao, Shuokai Li, Danding Wang, Fuzhen Zhuang|Beihang University, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences & University of Chinese Academy of Sciences, Beijing, China|The wide dissemination of fake news is increasingly threatening both individuals and society. Fake news detection aims to train a model on the past news and detect fake news of the future. Though great efforts have been made, existing fake news detection methods overlooked the unintended entity bias in the real-world data, which seriously influences models' generalization ability to future data. For example, 97% of news pieces in 2010-2017 containing the entity 'Donald Trump' are real in our data, but the percentage falls down to merely 33% in 2018. This would lead the model trained on the former set to hardly generalize to the latter, as it tends to predict news pieces about 'Donald Trump' as real for lower training loss. In this paper, we propose an entity debiasing framework (ENDEF) which generalizes fake news detection models to the future data by mitigating entity bias from a cause-effect perspective. Based on the causal graph among entities, news contents, and news veracity, we separately model the contribution of each cause (entities and contents) during training. In the inference stage, we remove the direct effect of the entities to mitigate entity bias. Extensive offline experiments on the English and Chinese datasets demonstrate that the proposed framework can largely improve the performance of base fake news detectors, and online tests verify its superiority in practice. To the best of our knowledge, this is the first work to explicitly improve the generalization ability of fake news detection models to the future data. The code has been released at https://github.com/ICTMCG/ENDEF-SIGIR2022.|假新闻的广泛传播日益威胁着个人和社会。假新闻检测的目的是训练一个模型，以过去的新闻和检测假新闻的未来。现有的假新闻检测方法忽视了现实数据中存在的无意识实体偏差，严重影响了模型对未来数据的泛化能力。例如，在我们的数据中，2010-2017年包含实体“唐纳德 · 特朗普”的新闻报道有97% 是真实的，但这一比例在2018年下降到仅有33% 。这将导致在前者上训练的模型很难推广到后者，因为它往往预测关于“唐纳德 · 特朗普”的新闻报道是真实的，以降低训练损失。本文提出了一个实体去偏框架(ENDEF) ，从因果关系的角度将虚假新闻检测模型推广到未来数据，减轻实体偏差。基于实体、新闻内容和新闻准确性之间的因果图，我们分别建立了训练过程中各个因素(实体和内容)的贡献模型。在推理阶段，我们消除了实体的直接影响，以减轻实体偏差。在中英文数据集上进行的大量离线实验表明，该框架能够显著提高基本假新闻检测器的性能，在线测试验证了该框架在实际应用中的优越性。据我们所知，这是第一项明确提高假新闻检测模型对未来数据的泛化能力的工作。密码已经在 https://github.com/ictmcg/endef-sigir2022公布了。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generalizing+to+the+Future:+Mitigating+Entity+Bias+in+Fake+News+Detection)|3|
|[ORCAS-I: Queries Annotated with Intent using Weak Supervision](https://doi.org/10.1145/3477495.3531737)|Daria Alexander, Wojciech Kusa, Arjen P. de Vries|Radboud University, Nijmegen, Netherlands; Radboud University & Spinque, Nijmegen, Netherlands; TU Wien, Vienna, Austria|User intent classification is an important task in information retrieval. In this work, we introduce a revised taxonomy of user intent. We take the widely used differentiation between navigational, transactional and informational queries as a starting point, and identify three different sub-classes for the informational queries: instrumental, factual and abstain. The resulting classification of user queries is more fine-grained, reaches a high level of consistency between annotators, and can serve as the basis for an effective automatic classification process. The newly introduced categories help distinguish between types of queries that a retrieval system could act upon, for example by prioritizing different types of results in the ranking. We have used a weak supervision approach based on Snorkel to annotate the ORCAS dataset according to our new user intent taxonomy, utilising established heuristics and keywords to construct rules for the prediction of the intent category. We then present a series of experiments with a variety of machine learning models, using the labels from the weak supervision stage as training data, but find that the results produced by Snorkel are not outperformed by these competing approaches and can be considered state-of-the-art. The advantage of a rule-based approach like Snorkel's is its efficient deployment in an actual system, where intent classification would be executed for every query issued. The resource released with this paper is the ORCAS-I dataset: a labelled version of the ORCAS click-based dataset of Web queries, which provides 18 million connections to 10 million distinct queries. We anticipate the usage of this resource in a scenario where the retrieval system would change its internal workings and search user interface to match the type of information request. For example, a navigational query could trigger just a short result list; and, for instrumental intent the system could rank tutorials and instructions higher than for other types of queries.|用户意图分类是信息检索的一项重要工作。在这项工作中，我们介绍了修订后的用户意图分类法。我们以广泛使用的导航查询、事务性查询和信息性查询之间的区别为出发点，确定了信息性查询的三个不同的子类: 工具性查询、事实性查询和弃权查询。由此产生的用户查询分类更加细粒度，达到了注释者之间的高度一致性，可以作为一个有效的自动分类过程的基础。新引入的类别有助于区分检索系统可以处理的查询类型，例如，通过在排名中对不同类型的结果进行优先排序。我们使用了基于 Snorkel 的弱监督方法来根据我们新的用户意图分类法对 ORCAS 数据集进行注释，使用已建立的启发式和关键字来构建用于预测意图类别的规则。然后，我们用各种机器学习模型进行了一系列的实验，使用来自弱监督阶段的标签作为训练数据，但是发现 Snorkel 产生的结果并没有被这些相互竞争的方法所超越，并且可以被认为是最先进的。像 Snorkel 这样的基于规则的方法的优势在于它在实际系统中的高效部署，在实际系统中，将对发出的每个查询执行意图分类。与本文一起发布的资源是 ORCAS-I 数据集: 基于 ORCAS 单击的 Web 查询数据集的标记版本，它提供了1800万个对1000万个不同查询的连接。我们预期在检索系统将改变其内部工作方式并搜索用户界面以匹配信息请求类型的场景中使用此资源。例如，导航查询只能触发一个简短的结果列表; 而且，对于工具意图，系统可以将教程和指令排序得比其他类型的查询更高。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ORCAS-I:+Queries+Annotated+with+Intent+using+Weak+Supervision)|3|
|[Unified Dialog Model Pre-training for Task-Oriented Dialog Understanding and Generation](https://doi.org/10.1145/3477495.3532069)|Wanwei He, Yinpei Dai, Min Yang, Jian Sun, Fei Huang, Luo Si, Yongbin Li|Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Alibaba Group, Beijing, China|Recently, pre-training methods have shown remarkable success in task-oriented dialog (TOD) systems. However, most existing pre-trained models for TOD focus on either dialog understanding or dialog generation, but not both. In this paper, we propose SPACE, a novel unified pre-trained dialog model learning from large-scale dialog corpora with limited annotations, which can be effectively fine-tuned on a wide range of downstream dialog tasks. Specifically, SPACE consists of four successive components in a single transformer to maintain a task-flow in TOD systems: (i) a dialog encoding module to encode dialog history, (ii) a dialog understanding module to extract semantic vectors from either user queries or system responses, (iii) a dialog policy module to generate a policy vector that contains high-level semantics of the response, and (iv) a dialog generation module to produce appropriate responses. We design a dedicated pre-training objective for each component. Concretely, we pre-train the dialog encoding module with span mask language modeling to learn contextualized dialog information. To capture the structured dialog semantics, we pre-train the dialog understanding module via a novel tree-induced semi-supervised contrastive learning objective with the help of extra dialog annotations. In addition, we pre-train the dialog policy module by minimizing the ℒ2 distance between its output policy vector and the semantic vector of the response for policy optimization. Finally, the dialog generation model is pre-trained by language modeling. Results show that SPACE achieves state-of-the-art performance on eight downstream dialog benchmarks, including intent prediction, dialog state tracking, and end-to-end dialog modeling. We also show that SPACE has a stronger few-shot ability than existing models under the low-resource setting.|近年来，在任务导向对话(TOD)系统中，预训练方法取得了显著的成功。然而，大多数现有的预先训练的 TOD 模型只关注对话理解或对话生成，而不是两者兼顾。在本文中，我们提出了一种新的统一的预训练对话模型 SPACE，该模型基于有限注释的大规模对话语料库，可以在大范围的下游对话任务中进行有效的微调。具体而言，SPACE 由单个转换器中的四个连续组件组成，以维持 TOD 系统中的任务流: (i)用于编码对话历史的对话编码模块，(ii)用于从用户查询或系统响应中提取语义向量的对话理解模块，(iii)用于生成包含响应的高级语义的策略向量的对话策略模块，以及(iv)用于产生适当响应的对话生成模块。我们为每个部分设计一个专门的培训前目标。具体来说，我们使用跨度掩码语言模型对对话框编码模块进行了预训练，以学习上下文化的对话框信息。为了捕获结构化对话语义，我们利用额外的对话注释，通过一个新的树引导的半监督对比学习目标对对话理解模块进行预训练。此外，我们预先训练对话策略模块，使其输出策略向量与策略优化响应的语义向量之间的 L2距离最小。最后，通过语言建模对对话生成模型进行预训练。结果表明，SPACE 在8个下游对话框基准测试中取得了最佳性能，包括意图预测、对话框状态跟踪和端到端对话框建模。我们还表明，在低资源设置下，SPACE 比现有的模型具有更强的少拍摄能力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unified+Dialog+Model+Pre-training+for+Task-Oriented+Dialog+Understanding+and+Generation)|3|
|[Introducing Problem Schema with Hierarchical Exercise Graph for Knowledge Tracing](https://doi.org/10.1145/3477495.3532004)|Hanshuang Tong, Zhen Wang, Yun Zhou, Shiwei Tong, Wenyuan Han, Qi Liu|School of Computer Science and Technology, University of Science and Technology of China, Beijing, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; Microsoft Corporation, Beijing, China, Beijing, China; AIXUEXI Education Group Ltd, Beijing, China, Beijing, China|Knowledge tracing (KT) which aims at predicting learner's knowledge mastery plays an important role in the computer-aided educational system. The goal of KT is to provide personalized learning paths for learners by diagnosing the mastery of each knowledge, thus improving the learning efficiency. In recent years, many deep learning models have been applied to tackle the KT task, which has shown promising results. However, most existing methods simplify the exercising records as knowledge sequences, which fail to explore the rich information that existed in exercises. Besides, the existing diagnosis results of knowledge tracing are not convincing enough since they neglect hierarchical relations between exercises. To solve the above problems, we propose a hierarchical graph knowledge tracing model called HGKT to explore the latent complex relations between exercises. Specifically, we introduce the concept of problem schema to construct a hierarchical exercise graph that could model the exercise learning dependencies. Moreover, we employ two attention mechanisms to highlight important historical states of learners. In the testing stage, we present a knowledge&schema diagnosis matrix that could trace the transition of mastery of knowledge and problem schema, which can be more easily applied to different applications. Extensive experiments show the effectiveness and interpretability of our proposed model.|知识追踪是计算机辅助教育系统中的一个重要组成部分，其目的是预测学习者的知识掌握情况。KT 的目标是通过诊断学习者对各种知识的掌握情况，为学习者提供个性化的学习途径，从而提高学习效率。近年来，许多深度学习模型被应用于解决 KT 问题，并取得了良好的效果。然而，现有的方法大多将习题记录简化为知识序列，未能探索习题中存在的丰富信息。此外，现有的知识追踪诊断结果由于忽视了习题之间的层次关系而不够令人信服。为了解决上述问题，本文提出了一种层次化的图形知识跟踪模型 HGKT，用于探索习题之间潜在的复杂关系。具体地说，我们引入了问题模式的概念，构造了一个层次化的练习图，可以对练习学习的依赖关系进行建模。此外，我们采用了两种注意机制来突出学习者的重要历史状态。在测试阶段，我们提出了一个知识和图式诊断矩阵，它可以跟踪知识掌握和问题图式的转换，可以更容易地应用到不同的应用程序中。大量的实验表明了我们提出的模型的有效性和可解释性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Introducing+Problem+Schema+with+Hierarchical+Exercise+Graph+for+Knowledge+Tracing)|3|
|[A Flexible Framework for Offline Effectiveness Metrics](https://doi.org/10.1145/3477495.3531924)|Alistair Moffat, Joel Mackenzie, Paul Thomas, Leif Azzopardi|The University of Melbourne, Melbourne, VIC, Australia; University of Strathclyde, Glasgow, United Kingdom; The University of Queensland, Brisbane, QLD, Australia; Microsoft, Canberra, Australia|The use of offline effectiveness metrics is one of the cornerstones of evaluation in information retrieval. Static resources that include test collections and sets of topics, the corresponding relevance judgments connecting them, and metrics that map document rankings from a retrieval system to numeric scores have been used for multiple decades as an important way of comparing systems. The basis behind this experimental structure is that the metric score for a system can serve as a surrogate measurement for user satisfaction. Here we introduce a user behavior framework that extends the C/W/L family. The essence of the new framework - which we call C/W/L/A - is that the user actions that are undertaken while reading the ranking can be considered separately from the benefit that each user will have derived as they exit the ranking. This split structure allows the great majority of current effectiveness metrics to be systematically categorized, and thus their relative properties and relationships to be better understood; and at the same time permits a wide range of novel combinations to be considered. We then carry out experiments using relevance judgments, document rankings, and user satisfaction data from two distinct sources, comparing the patterns of metric scores generated, and showing that those metrics vary quite markedly in terms of their ability to predict user satisfaction.|离线效能指标的使用是信息检索评估的基石之一。静态资源，包括测试集合和主题集合，相应的相关性判断连接它们，以及将文档排名从检索系统映射到数字分数的指标，已经作为比较系统的一个重要方法使用了几十年。这个实验结构的基础是系统的度量分数可以作为用户满意度的替代指标。这里我们介绍一个扩展 C/W/L 家族的用户行为框架。我们称之为 C/W/L/A 的新框架的本质是，用户在阅读排名时所采取的行动可以与每个用户在退出排名时获得的好处分开考虑。这种分离结构允许对大多数当前有效性指标进行系统分类，从而更好地理解它们的相对属性和关系; 同时允许考虑范围广泛的新颖组合。然后，我们使用来自两个不同来源的相关性判断、文档排名和用户满意度数据进行实验，比较产生的度量得分模式，并显示这些度量在预测用户满意度方面的能力差异非常显著。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Flexible+Framework+for+Offline+Effectiveness+Metrics)|3|
|[Incorporating Context Graph with Logical Reasoning for Inductive Relation Prediction](https://doi.org/10.1145/3477495.3531996)|Qika Lin, Jun Liu, Fangzhi Xu, Yudai Pan, Yifan Zhu, Lingling Zhang, Tianzhe Zhao|National Engineering Lab for Big Data Analytics, Xi'an, China; Xi'an Jiaotong University, Xi'an, China; Tsinghua University, Beijing, China|Relation prediction on knowledge graphs (KGs) aims to infer missing valid triples from observed ones. Although this task has been deeply studied, most previous studies are limited to the transductive setting and cannot handle emerging entities. Actually, the inductive setting is closer to real-life scenarios because it allows entities in the testing phase to be unseen during training. However, it is challenging to precisely conduct inductive relation prediction as there exists requirements of entity-independent relation modeling and discrete logical reasoning for interoperability. To this end, we propose a novel model ConGLR to incorporate context graph with logical reasoning. Firstly, the enclosing subgraph w.r.t. target head and tail entities are extracted and initialized by the double radius labeling. And then the context graph involving relational paths, relations and entities is introduced. Secondly, two graph convolutional networks (GCNs) with the information interaction of entities and relations are carried out to process the subgraph and context graph respectively. Considering the influence of different edges and target relations, we introduce edge-aware and relation-aware attention mechanisms for the subgraph GCN. Finally, by treating the relational path as rule body and target relation as rule head, we integrate neural calculating and logical reasoning to obtain inductive scores. And to focus on the specific modeling goals of each module, the stop-gradient is utilized in the information interaction between context graph and subgraph GCNs in the training process. In this way, ConGLR satisfies two inductive requirements at the same time. Extensive experiments demonstrate that ConGLR obtains outstanding performance against state-of-the-art baselines on twelve inductive dataset versions of three common KGs.|知识图关系预测的目的是从观察到的三元组中推断出缺失的有效三元组。虽然这项任务已经深入研究，大多数以前的研究仅限于传导性的设置，不能处理新兴的实体。实际上，归纳设置更接近于实际场景，因为它允许测试阶段的实体在训练期间不被看到。然而，精确地进行归纳关系预测是具有挑战性的，因为存在实体无关的关系建模和互操作性的离散逻辑推理的要求。为此，我们提出了一个新的模型 ConGLR，将上下文图和逻辑推理结合起来。首先，通过双半径标记提取封闭子图的目标头部和尾部实体，并对其进行初始化;。然后介绍了涉及关系路径、关系和实体的上下文图。其次，分别对子图和上下文图进行处理，构造了两个具有实体和关系信息交互的图卷积网络。考虑到不同边和目标关系的影响，我们在子图 GCN 中引入了边感知和关系感知的注意机制。最后，我们把关系路径当作规则体，把目标关系当作规则头，结合神经计算和逻辑推理来获得归纳分数。针对每个模块的特定建模目标，在训练过程中，利用停止梯度进行上下文图和子图 GCNs 之间的信息交互。这样，ConGLR 可以同时满足两个感应要求。大量的实验表明，ConGLR 在三个常见 KG 的12个归纳数据集版本上对最先进的基线获得了出色的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Incorporating+Context+Graph+with+Logical+Reasoning+for+Inductive+Relation+Prediction)|3|
|[Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion](https://doi.org/10.1145/3477495.3531992)|Xiang Chen, Ningyu Zhang, Lei Li, Shumin Deng, Chuanqi Tan, Changliang Xu, Fei Huang, Luo Si, Huajun Chen|State Key Laboratory of Media Convergence Production Technology and Systems, Beijing, China; Alibaba Group, Hangzhou, China; Zhejiang University, Hangzhou, China|Multimodal Knowledge Graphs (MKGs), which organize visual-text factual knowledge, have recently been successfully applied to tasks such as information retrieval, question answering, and recommendation system. Since most MKGs are far from complete, extensive knowledge graph completion studies have been proposed focusing on the multimodal entity, relation extraction and link prediction. However, different tasks and modalities require changes to the model architecture, and not all images/objects are relevant to text input, which hinders the applicability to diverse real-world scenarios. In this paper, we propose a hybrid transformer with multi-level fusion to address those issues. Specifically, we leverage a hybrid transformer architecture with unified input-output for diverse multimodal knowledge graph completion tasks. Moreover, we propose multi-level fusion, which integrates visual and text representation via coarse-grained prefix-guided interaction and fine-grained correlation-aware fusion modules. We conduct extensive experiments to validate that our MKGformer can obtain SOTA performance on four datasets of multimodal link prediction, multimodal RE, and multimodal NER1. https://github.com/zjunlp/MKGformer.|多模态知识图(multimodalKnowledge Graphs，MKGs)组织视觉文本事实知识，最近已成功应用于信息检索、问答和推荐系统等任务。由于大多数 MKG 还不完备，因此人们提出了广泛的知识图完备性研究，主要集中在多模态实体、关系提取和链路预测方面。然而，不同的任务和模式需要对模型架构进行更改，并非所有的图像/对象都与文本输入相关，这阻碍了对不同的现实世界场景的适用性。本文提出了一种多电平融合的混合变压器来解决这些问题。具体来说，我们利用一个具有统一输入输出的混合变压器架构来完成不同的多模态知识图完成任务。此外，我们还提出了多级融合，通过粗粒度前缀引导交互和细粒度相关感知融合模块，实现了视觉和文本表示的集成。我们进行了广泛的实验，以验证我们的 MKGformer 可以获得 SOTA 性能的四个数据集的多模态链接预测，多模态 RE 和多模态 NER1。Https://github.com/zjunlp/mkgformer.|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hybrid+Transformer+with+Multi-level+Fusion+for+Multimodal+Knowledge+Graph+Completion)|3|
|[HTKG: Deep Keyphrase Generation with Neural Hierarchical Topic Guidance](https://doi.org/10.1145/3477495.3531990)|Yuxiang Zhang, Tao Jiang, Tianyu Yang, Xiaoli Li, Suge Wang|Civil Aviation University of China, Tianjin, China; Shanxi University, Taiyuan, China; A*STAR, Singapore, Singapore, Singapore|Keyphrases can concisely describe the high-level topics discussed in a document that usually possesses hierarchical topic structures. Thus, it is crucial to understand the hierarchical topic structures and employ it to guide the keyphrase identification. However, integrating the hierarchical topic information into a deep keyphrase generation model is unexplored. In this paper, we focus on how to effectively exploit the hierarchical topic to improve the keyphrase generation performance (HTKG). Specifically, we propose a novel hierarchical topic-guided variational neural sequence generation method for keyphrase generation, which consists of two major modules: a neural hierarchical topic model that learns the latent topic tree across the whole corpus of documents, and a variational neural keyphrase generation model to generate keyphrases under hierarchical topic guidance. Finally, these two modules are jointly trained to help them learn complementary information from each other. To the best of our knowledge, this is the first attempt to leverage the neural hierarchical topic to guide keyphrase generation. The experimental results demonstrate that our method significantly outperforms the existing state-of-the-art methods across five benchmark datasets.|关键词可以简洁地描述文档中讨论的高级主题，该文档通常具有分层主题结构。因此，理解分层主题结构并利用它来指导关键词的识别是至关重要的。然而，将层次化的主题信息集成到一个深层次的关键词生成模型中还没有被探索。本文主要研究如何有效地利用分层主题来提高关键词生成性能。提出了一种基于层次主题引导的变分神经序列生成方法，该方法包括两个主要模块: 学习整个文档语料库中潜在主题树的神经层次主题模型和层次主题引导下的变分神经关键词生成模型。最后，对这两个模块进行联合培训，以帮助它们相互学习互补的信息。据我们所知，这是第一次尝试利用神经分层主题来指导关键词生成。实验结果表明，该方法在五个基准数据集上的性能明显优于现有的最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HTKG:+Deep+Keyphrase+Generation+with+Neural+Hierarchical+Topic+Guidance)|3|
|[Logiformer: A Two-Branch Graph Transformer Network for Interpretable Logical Reasoning](https://doi.org/10.1145/3477495.3532016)|Fangzhi Xu, Jun Liu, Qika Lin, Yudai Pan, Lingling Zhang|National Engineering Lab for Big Data Analytics, Xi'an, China; Xi'an Jiaotong University, Xi'an, China|Machine reading comprehension has aroused wide concerns, since it explores the potential of model for text understanding. To further equip the machine with the reasoning capability, the challenging task of logical reasoning is proposed. Previous works on logical reasoning have proposed some strategies to extract the logical units from different aspects. However, there still remains a challenge to model the long distance dependency among the logical units. Also, it is demanding to uncover the logical structures of the text and further fuse the discrete logic to the continuous text embedding. To tackle the above issues, we propose an end-to-end model Logiformer which utilizes a two-branch graph transformer network for logical reasoning of text. Firstly, we introduce different extraction strategies to split the text into two sets of logical units, and construct the logical graph and the syntax graph respectively. The logical graph models the causal relations for the logical branch while the syntax graph captures the co-occurrence relations for the syntax branch. Secondly, to model the long distance dependency, the node sequence from each graph is fed into the fully connected graph transformer structures. The two adjacent matrices are viewed as the attention biases for the graph transformer layers, which map the discrete logical structures to the continuous text embedding space. Thirdly, a dynamic gate mechanism and a question-aware self-attention module are introduced before the answer prediction to update the features. The reasoning process provides the interpretability by employing the logical units, which are consistent with human cognition. The experimental results show the superiority of our model, which outperforms the state-of-the-art single model on two logical reasoning benchmarks.|机器阅读理解已经引起了广泛的关注，因为它探索了文本理解模型的潜力。为了进一步提高机器的推理能力，提出了具有挑战性的逻辑推理任务。以往关于逻辑推理的研究提出了一些策略来从不同方面提取逻辑单元。然而，在逻辑单元之间建立长距离依赖关系仍然是一个挑战。此外，还需要揭示文本的逻辑结构，进一步将离散逻辑与连续文本嵌入相融合。为了解决上述问题，我们提出了一个端到端模型 Logigformer，它利用一个两分支的图形转换网络进行文本逻辑推理。首先，引入不同的抽取策略，将文本分割成两组逻辑单元，分别构造逻辑图和句法图。逻辑图模拟逻辑分支的因果关系，而语法图捕获语法分支的共现关系。其次，为了建立长距离依赖关系模型，将每个图的节点序列输入到完全连通的图变换结构中。将两个相邻矩阵视为图形转换层的注意偏差，将离散逻辑结构映射到连续文本嵌入空间。第三，在答案预测之前引入动态门机制和问题感知自我注意模块来更新特征。推理过程通过使用与人类认知相一致的逻辑单元来提供可解释性。实验结果显示了我们的模型的优越性，它在两个逻辑推理基准上优于最先进的单一模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Logiformer:+A+Two-Branch+Graph+Transformer+Network+for+Interpretable+Logical+Reasoning)|3|
|[Constrained Sequence-to-Tree Generation for Hierarchical Text Classification](https://doi.org/10.1145/3477495.3531765)|Chao Yu, Yi Shen, Yue Mao||Hierarchical Text Classification (HTC) is a challenging task where a document can be assigned to multiple hierarchically structured categories within a taxonomy. The majority of prior studies consider HTC as a flat multi-label classification problem, which inevitably leads to ''label inconsistency'' problem. In this paper, we formulate HTC as a sequence generation task and introduce a sequence-to-tree framework (Seq2Tree) for modeling the hierarchical label structure. Moreover, we design a constrained decoding strategy with dynamic vocabulary to secure the label consistency of the results. Compared with previous works, the proposed approach achieves significant and consistent improvements on three benchmark datasets.|层次化文本分类(HTC)是一个具有挑战性的任务，其中一个文档可以分配到一个分类法中的多个层次结构化类别。以往的大多数研究认为 HTC 是一个扁平的多标签分类问题，这不可避免地导致了“标签不一致”问题。本文将 HTC 描述为一个序列生成任务，并引入了一个序列到树的框架(Seq2Tree)来建模分层标签结构。此外，我们设计了一个具有动态词汇的约束解码策略，以保证解码结果的标签一致性。与以往的工作相比，该方法在三个基准数据集上取得了显著和一致的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Constrained+Sequence-to-Tree+Generation+for+Hierarchical+Text+Classification)|3|
|[Answering Count Queries with Explanatory Evidence](https://doi.org/10.1145/3477495.3531870)|Shrestha Ghosh, Simon Razniewski, Gerhard Weikum|Max Planck Institute for Informatics, Saarbruecken, Germany; Max Planck Institute for Informatics & Saarland University, Saarbruecken, Germany|A challenging case in web search and question answering are count queries, such as"number of songs by John Lennon''. Prior methods merely answer these with a single, and sometimes puzzling number or return a ranked list of text snippets with different numbers. This paper proposes a methodology for answering count queries with inference, contextualization and explanatory evidence. Unlike previous systems, our method infers final answers from multiple observations, supports semantic qualifiers for the counts, and provides evidence by enumerating representative instances. Experiments with a wide variety of queries show the benefits of our method. To promote further research on this underexplored topic, we release an annotated dataset of 5k queries with 200k relevant text spans.|在网络搜索和问题回答中一个具有挑战性的例子是计数查询，例如“约翰 · 列侬的歌曲数量”。先前的方法只用一个单独的、有时令人费解的数字来回答这些问题，或者返回一个具有不同数字的文本片段的排序列表。本文提出了一种用推理、上下文化和解释性证据来回答计数查询的方法。与以前的系统不同，我们的方法从多个观察结果中推断出最终答案，支持计数的语义限定符，并通过枚举代表性实例提供证据。对各种查询的实验显示了我们的方法的优点。为了促进对这个未被充分探索的主题的进一步研究，我们发布了一个有注释的5k 查询数据集，其相关文本跨度为200k。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Answering+Count+Queries+with+Explanatory+Evidence)|3|
|[Space4HGNN: A Novel, Modularized and Reproducible Platform to Evaluate Heterogeneous Graph Neural Network](https://doi.org/10.1145/3477495.3531720)|Tianyu Zhao, Cheng Yang, Yibo Li, Quan Gan, Zhenyi Wang, Fengqi Liang, Huan Zhao, Yingxia Shao, Xiao Wang, Chuan Shi|AWS Shanghai AI Lab, Shanghai, China; 4Paradigm Inc., Beijing, China; Beijing University of Posts and Telecommunications & Peng Cheng Laboratory, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China|Heterogeneous Graph Neural Network (HGNN) has been successfully employed in various tasks, but we cannot accurately know the importance of different design dimensions of HGNNs due to diverse architectures and applied scenarios. Besides, in the research community of HGNNs, implementing and evaluating various tasks still need much human effort. To mitigate these issues, we first propose a unified framework covering most HGNNs, consisting of three components: heterogeneous linear transformation, heterogeneous graph transformation, and heterogeneous message passing layer. Then we build a platform Space4HGNN by defining a design space for HGNNs based on the unified framework, which offers modularized components, reproducible implementations, and standardized evaluation for HGNNs. Finally, we conduct experiments to analyze the effect of different designs. With the insights found, we distill a condensed design space and verify its effectiveness.|异构图形神经网络(HGNN)已经成功地应用于各种任务中，但由于其结构和应用场景的多样性，我们无法准确地了解 HGNN 不同设计维数的重要性。此外，在 HGNN 的研究领域中，实施和评估各种任务仍然需要大量的人力。为了缓解这些问题，我们首先提出了一个涵盖大多数 HGNN 的统一框架，该框架由三个组件组成: 异构线性映射、异构图转换和异构消息传递层。然后基于统一框架定义了 HGNN 的设计空间，提供了 HGNN 的模块化组件、可重复实现和标准化评估，构建了一个平台 Space4HGNN。最后，通过实验分析了不同设计方案的效果。根据所发现的见解，我们提炼出一个浓缩的设计空间并验证其有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Space4HGNN:+A+Novel,+Modularized+and+Reproducible+Platform+to+Evaluate+Heterogeneous+Graph+Neural+Network)|3|
|[NeuralKG: An Open Source Library for Diverse Representation Learning of Knowledge Graphs](https://doi.org/10.1145/3477495.3531669)|Wen Zhang, Xiangnan Chen, Zhen Yao, Mingyang Chen, Yushan Zhu, Hongtao Yu, Yufeng Huang, Yajing Xu, Ningyu Zhang, Zezhong Xu, Zonggang Yuan, Feiyu Xiong, Huajun Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NeuralKG:+An+Open+Source+Library+for+Diverse+Representation+Learning+of+Knowledge+Graphs)|3|
|[Deep Knowledge Graph Representation Learning for Completion, Alignment, and Question Answering](https://doi.org/10.1145/3477495.3532679)|Soumen Chakrabarti||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Knowledge+Graph+Representation+Learning+for+Completion,+Alignment,+and+Question+Answering)|3|
|[HIEN: Hierarchical Intention Embedding Network for Click-Through Rate Prediction](https://doi.org/10.1145/3477495.3531988)|Zuowu Zheng, Changwang Zhang, Xiaofeng Gao, Guihai Chen|Tencent Inc., Shenzhen, China; Shanghai Jiao Tong University, Shanghai, China|Click-through rate (CTR) prediction plays an important role in online advertising and recommendation systems, which aims at estimating the probability of a user clicking on a specific item. Feature interaction modeling and user interest modeling methods are two popular domains in CTR prediction, and they have been studied extensively in recent years. However, these methods still suffer from two limitations. First, traditional methods regard item attributes as ID features, while neglecting structure information and relation dependencies among attributes. Second, when mining user interests from user-item interactions, current models ignore user intents and item intents for different attributes, which lacks interpretability. Based on this observation, in this paper, we propose a novel approach Hierarchical Intention Embedding Network (HIEN), which considers dependencies of attributes based on bottom-up tree aggregation in the constructed attribute graph. HIEN also captures user intents for different item attributes as well as item intents based on our proposed hierarchical attention mechanism. Extensive experiments on both public and production datasets show that the proposed model significantly outperforms the state-of-the-art methods. In addition, HIEN can be applied as an input module to state-of-the-art CTR prediction methods, bringing further performance lift for these existing models that might already be intensively used in real systems.|在在线广告和推荐系统中，点进率预测(ctrl)扮演着重要的角色，其目的是估计用户点击特定项目的概率。特征交互建模和用户兴趣建模是 CTR 预测的两个热门领域，近年来得到了广泛的研究。然而，这些方法仍然存在两个局限性。首先，传统的方法把项目属性看作 ID 特征，而忽略了结构信息和属性之间的关系依赖。其次，当从用户-项目交互中挖掘用户兴趣时，目前的模型忽略了不同属性的用户意图和项目意图，缺乏可解释性。在此基础上，本文提出了一种新的层次意图嵌入网络(HIEN)方法，该方法在构造的属性图中考虑基于自底向上树聚集的属性依赖关系。HIEN 还根据我们提出的分层注意机制捕获不同项目属性的用户意图以及项目意图。在公共数据集和生产数据集上的大量实验表明，所提出的模型明显优于最先进的方法。此外，HIEN 还可以作为最先进的 CTR 预测方法的输入模块，为这些可能已经在实际系统中广泛使用的现有模型带来进一步的性能提升。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HIEN:+Hierarchical+Intention+Embedding+Network+for+Click-Through+Rate+Prediction)|2|
|[Neural Query Synthesis and Domain-Specific Ranking Templates for Multi-Stage Clinical Trial Matching](https://doi.org/10.1145/3477495.3531853)|Ronak Pradeep, Yilin Li, Yuetong Wang, Jimmy Lin|University of Waterloo, Waterloo, ON, Canada; University of Waterloo, Waterloo, Canada|In this work, we propose an effective multi-stage neural ranking system for the clinical trial matching problem. First, we introduce NQS, a neural query synthesis method that leverages a zero-shot document expansion model to generate multiple sentence-long queries from lengthy patient descriptions. These queries are independently issued to a search engine and the results are fused. We find that on the TREC 2021 Clinical Trials Track, this method outperforms strong traditional baselines like BM25 and BM25 + RM3 by about 12 points in [email protected] , a relative improvement of 34%. This simple method is so effective that even a state-of-the-art neural relevance ranking method trained on the medical subset of MS MARCO passage, when reranking the results of NQS, fails to improve on the ranked list. Second, we introduce a two-stage neural reranking pipeline trained on clinical trial matching data using tailored ranking templates. In this setting, we can train a pointwise reranker using just 1.1k positive examples and obtain effectiveness improvements over NQS by 24 points. This end-to-end multi-stage system demonstrates a 20% relative effectiveness gain compared to the second-best submission at TREC 2021, making it an important step towards better automated clinical trial matching.|在这项工作中，我们提出了一个有效的多阶段神经排序系统的临床试验匹配问题。首先，我们介绍了 NQS，这是一种神经查询合成方法，它利用一个零拍文档扩展模型，从冗长的患者描述中生成多个句子长的查询。这些查询被独立地发送给搜索引擎，并且结果被融合。我们发现，在 TREC 2021临床试验跟踪中，这种方法比强大的传统基线如 BM25和 BM25 + RM3在[ email protected ]中高出约12分，相对提高了34% 。这种简单的方法是如此有效，以至于即使是在 MS MARCO 通道的医学子集上训练的最先进的神经相关性排序方法，在对 NQS 的结果重新排序时，也不能改进排序列表。其次，我们介绍了一个两阶段的神经重新排序管道训练临床试验匹配数据使用定制的排序模板。在这种情况下，我们可以训练一个点态的重新排序使用只有1.1 k 正的例子，并获得24点的有效性改进超过 NQS。这种端到端的多阶段系统与 TREC 2021年第二好的提交相比，显示出20% 的相对有效性增益，这使得它朝着更好的自动化临床试验匹配迈出了重要的一步。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Query+Synthesis+and+Domain-Specific+Ranking+Templates+for+Multi-Stage+Clinical+Trial+Matching)|2|
|[Improving Contrastive Learning of Sentence Embeddings with Case-Augmented Positives and Retrieved Negatives](https://doi.org/10.1145/3477495.3531823)|Wei Wang, Liangzhu Ge, Jingqiao Zhang, Cheng Yang|Alibaba Group, Hangzhou, China|Following SimCSE, contrastive learning based methods have achieved the state-of-the-art (SOTA) performance in learning sentence embeddings. However, the unsupervised contrastive learning methods still lag far behind the supervised counterparts. We attribute this to the quality of positive and negative samples, and aim to improve both. Specifically, for positive samples, we propose switch-case augmentation to flip the case of the first letter of randomly selected words in a sentence. This is to counteract the intrinsic bias of pre-trained token embeddings to frequency, word cases and subwords. For negative samples, we sample hard negatives from the whole dataset based on a pre-trained language model. Combining the above two methods with SimCSE, our proposed Contrastive learning with Augmented and Retrieved Data for Sentence embedding (CARDS) method significantly surpasses the current SOTA on STS benchmarks in the unsupervised setting.|继 SimCSE 之后，基于对比学习的方法在学习句子嵌入方面取得了先进的性能。然而，无监督对比学习方法仍然远远落后于有监督对比学习方法。我们将此归因于阳性和阴性样本的质量，并致力于改善这两种情况。特别地，对于正样本，我们提出开关格增强来翻转句子中随机选择的单词的第一个字母的情况。这是为了抵消预先训练的标记嵌入对频率、词例和子词的内在偏差。对于阴性样本，我们基于预训练语言模型从整个数据集中抽取硬阴性样本。将上述两种方法与 SimCSE 相结合，我们提出的对比学习与增强和检索数据的句子嵌入(CARDS)方法显着超过目前的 SOTA 的 STS 基准在无监督的设置。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Contrastive+Learning+of+Sentence+Embeddings+with+Case-Augmented+Positives+and+Retrieved+Negatives)|2|
|[Diversity Vs Relevance: A Practical Multi-objective Study in Luxury Fashion Recommendations](https://doi.org/10.1145/3477495.3531866)|João Sá, Vanessa Queiroz Marinho, Ana Rita Magalhães, Tiago Lacerda, Diogo Gonçalves|Farfetch, London, United Kingdom|Personalized algorithms focusing uniquely on accuracy might provide highly relevant recommendations, but the recommended items could be too similar to current users' preferences. Therefore, recommenders might prevent users from exploring new products and brands (filter bubbles). This is especially critical for luxury fashion recommendations because luxury shoppers expect to discover exclusive and rare items. Thus, recommender systems for fashion need to consider diversity and elevate the shopping experience by recommending new brands and products from the catalog. In this work, we explored a handful of diversification strategies to rerank the output of a relevance-focused recommender system. Subsequently, we conducted a multi-objective offline experiment optimizing for relevance and diversity simultaneously. We measured diversity with commonly used metrics such as coverage, serendipity, and neighborhood distance, whereas, for relevance, we selected ranking metrics such as recall. The best diversification strategy offline improved user engagement by 2% in click-through rate and presented an uplift of 46% in distinct brands recommended when AB tested against real users. These results reinforced the importance of considering accuracy and diversity metrics when developing a recommender system.|专注于准确性的个性化算法可能会提供高度相关的推荐，但推荐的项目可能与当前用户的偏好过于相似。因此，推荐者可能会阻止用户探索新的产品和品牌(过滤气泡)。这对于奢侈品时尚推荐来说尤其重要，因为奢侈品消费者希望发现独一无二的稀有物品。因此，时尚推荐系统需要考虑多样性，并通过推荐目录中的新品牌和产品来提升购物体验。在这项工作中，我们探索了一些多样化策略，以重新排列以相关性为中心的推荐系统的产出。随后，我们进行了多目标离线实验，同时对相关性和多样性进行了优化。我们使用常用的指标(如覆盖率、意外发现和邻近距离)来衡量多样性，而对于相关性，我们选择了排名指标(如回忆)。线下最佳多样化策略提高了2% 的用户参与点进率，当 AB 公司对真实用户进行测试时，推荐的不同品牌的用户参与度提高了46% 。这些结果强调了在开发推荐系统时考虑准确性和多样性指标的重要性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Diversity+Vs+Relevance:+A+Practical+Multi-objective+Study+in+Luxury+Fashion+Recommendations)|2|
|[Exploiting Variational Domain-Invariant User Embedding for Partially Overlapped Cross Domain Recommendation](https://doi.org/10.1145/3477495.3531975)|Weiming Liu, Xiaolin Zheng, Jiajie Su, Mengling Hu, Yanchao Tan, Chaochao Chen|Zhejiang University, Hangzhou, China|Cross-Domain Recommendation (CDR) has been popularly studied to utilize different domain knowledge to solve the cold-start problem in recommender systems. Most of the existing CDR models assume that both the source and target domains share the same overlapped user set for knowledge transfer. However, only few proportion of users simultaneously activate on both the source and target domains in practical CDR tasks. In this paper, we focus on the Partially Overlapped Cross-Domain Recommendation (POCDR) problem, that is, how to leverage the information of both the overlapped and non-overlapped users to improve recommendation performance. Existing approaches cannot fully utilize the useful knowledge behind the non-overlapped users across domains, which limits the model performance when the majority of users turn out to be non-overlapped. To address this issue, we propose an end-to-end Dual-autoencoder with Variational Domain-invariant Embedding Alignment (VDEA) model, a cross-domain recommendation framework for the POCDR problem, which utilizes dual variational autoencoders with both local and global embedding alignment for exploiting domain-invariant user embedding. VDEA first adopts variational inference to capture collaborative user preferences, and then utilizes Gromov-Wasserstein distribution co-clustering optimal transport to cluster the users with similar rating interaction behaviors. Our empirical studies on Douban and Amazon datasets demonstrate that VDEA significantly outperforms the state-of-the-art models, especially under the POCDR setting.|跨域推荐(CDR)是一种利用不同领域知识解决推荐系统冷启动问题的方法。现有的 CDR 模型大多假设源域和目标域共享相同的重叠用户集进行知识转移。然而，在实际的 CDR 任务中，只有少数用户在源域和目标域同时激活。本文主要研究部分重叠跨域推荐(POCDR)问题，即如何利用重叠用户和非重叠用户的信息来提高推荐性能。现有的方法不能充分利用跨领域非重叠用户背后的有用知识，这限制了大多数用户不重叠时的模型性能。针对这一问题，本文提出了一种基于变分域不变嵌入对齐(VDEA)模型的端到端双变分自动编码器，该模型利用具有局部和全局嵌入对齐的双变分自动编码器进行域不变用户嵌入。VDEA 首先采用变分推理获取协同用户偏好，然后利用 Gromov-Wasserstein 分布协聚类最优传输对具有相似评分交互行为的用户进行聚类。我们对 Douban 和亚马逊数据集的实证研究表明，VDEA 显著优于最先进的模型，特别是在 POCDR 设置下。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploiting+Variational+Domain-Invariant+User+Embedding+for+Partially+Overlapped+Cross+Domain+Recommendation)|2|
|[Variational Reasoning about User Preferences for Conversational Recommendation](https://doi.org/10.1145/3477495.3532077)|Zhaochun Ren, Zhi Tian, Dongdong Li, Pengjie Ren, Liu Yang, Xin Xin, Huasheng Liang, Maarten de Rijke, Zhumin Chen|Shandong University, Qingdao, China; University of Amsterdam, Amsterdam, Netherlands; WeChat, Tencent, Shenzhen, China|Conversational recommender systems (CRSs) provide recommendations through interactive conversations. CRSs typically provide recommendations through relatively straightforward interactions, where the system continuously inquires about a user's explicit attribute-aware preferences and then decides which items to recommend. In addition, topic tracking is often used to provide naturally sounding responses. However, merely tracking topics is not enough to recognize a user's real preferences in a dialogue. In this paper, we address the problem of accurately recognizing and maintaining user preferences in CRSs. Three challenges come with this problem: (1) An ongoing dialogue only provides the user's short-term feedback; (2) Annotations of user preferences are not available; and (3) There may be complex semantic correlations among items that feature in a dialogue. We tackle these challenges by proposing an end-to-end variational reasoning approach to the task of conversational recommendation. We model both long-term preferences and short-term preferences as latent variables with topical priors for explicit long-term and short-term preference exploration, respectively. We use an efficient stochastic gradient variational Bayesian (SGVB) estimator for optimizing the derived evidence lower bound. A policy network is then used to predict topics for a clarification utterance or items for a recommendation response. The use of explicit sequences of preferences with multi-hop reasoning in a heterogeneous knowledge graph helps to provide more accurate conversational recommendation results. Extensive experiments conducted on two benchmark datasets show that our proposed method outperforms state-of-the-art baselines in terms of both objective and subjective evaluation metric|会话推荐系统(CRS)通过交互式对话提供推荐。CRS 通常通过相对简单的交互提供推荐，系统不断地查询用户的明确的属性感知偏好，然后决定推荐哪些项目。此外，主题跟踪通常用于提供听起来很自然的回答。但是，仅仅跟踪主题不足以识别用户在对话中的真实偏好。在本文中，我们解决了准确识别和维护用户偏好的问题。这个问题带来了三个挑战: (1)持续的对话只提供用户的短期反馈; (2)用户偏好的注释不可用; (3)对话中的项目之间可能存在复杂的语义关联。我们通过提出一种端到端的变分推理方法来解决这些挑战。我们将长期偏好和短期偏好分别建模为具有主题先验的潜在变量，用于显性长期和短期偏好探索。我们使用一个有效的随机梯度变分贝叶斯(SGVB)估计器来优化导出的证据下界。然后使用策略网络来预测澄清话语的主题或推荐响应的项目。在异构知识图中使用多跳推理的显式偏好序列有助于提供更准确的会话推荐结果。在两个基准数据集上进行的大量实验表明，我们提出的方法在客观和主观评价指标方面都优于最先进的基准|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Variational+Reasoning+about+User+Preferences+for+Conversational+Recommendation)|2|
|[Analyzing and Simulating User Utterance Reformulation in Conversational Recommender Systems](https://doi.org/10.1145/3477495.3531936)|Shuo Zhang, MuChun Wang, Krisztian Balog|University of Science and Technology of China, Hefei, China; Bloomberg, London, United Kingdom; University of Stavanger, Stavanger, Norway|User simulation has been a cost-effective technique for evaluating conversational recommender systems. However, building a human-like simulator is still an open challenge. In this work, we focus on how users reformulate their utterances when a conversational agent fails to understand them. First, we perform a user study, involving five conversational agents across different domains, to identify common reformulation types and their transition relationships. A common pattern that emerges is that persistent users would first try to rephrase, then simplify, before giving up. Next, to incorporate the observed reformulation behavior in a user simulator, we introduce the task of reformulation sequence generation: to generate a sequence of reformulated utterances with a given intent (rephrase or simplify). We develop methods by extending transformer models guided by the reformulation type and perform further filtering based on estimated reading difficulty. We demonstrate the effectiveness of our approach using both automatic and human evaluation.|用户仿真已经成为评估会话推荐系统的一种经济有效的技术。然而，建立一个类似人的模拟器仍然是一个公开的挑战。在这项工作中，我们的重点是如何用户重新组织他们的话语时，一个会话代理无法理解他们。首先，我们进行了一个用户研究，涉及五个不同领域的会话代理，以确定常见的重构类型及其转换关系。出现的一种常见模式是，持久用户在放弃之前会首先尝试重新措辞，然后进行简化。接下来，为了在用户模拟器中整合观察到的重新表述行为，我们引入了重新表述序列生成的任务: 生成具有给定意图(重新表述或简化)的重新表述话语序列。我们发展的方法，扩展变压器模型指导下的重新公式类型和进一步滤波的基础上估计读取困难。我们使用自动评估和人工评估证明了我们的方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Analyzing+and+Simulating+User+Utterance+Reformulation+in+Conversational+Recommender+Systems)|2|
|[Learning to Infer User Implicit Preference in Conversational Recommendation](https://doi.org/10.1145/3477495.3531844)|Chenhao Hu, Shuhua Huang, Yansen Zhang, Yubao Liu|Sun Yat-Sen University & Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, China; Sun Yat-Sen University, Guangzhou, China|Conversational recommender systems (CRS) enable traditional recommender systems to interact with users by asking questions about attributes and recommending items. The attribute-level and item-level feedback of users can be utilized to estimate users' preferences. However, existing works do not fully exploit the advantage of explicit item feedback --- they only use the item feedback in rather implicit ways such as updating the latent user and item representation. Since CRS has multiple chances to interact with users, leveraging the context in the conversation may help infer users' implicit feedback (e.g., some specific attributes) when recommendations get rejected. To address the limitations of existing methods, we propose a new CRS framework called Conversational Recommender with Implicit Feedback (CRIF). CRIF formulates the conversational recommendation scheme as a four-phase process consisting of offline representation learning, tracking, decision, and inference. In the inference module, by fully utilizing the relation between users' attribute-level and item-level feedback, our method can explicitly deduce users' implicit preferences. Therefore, CRIF is able to achieve more accurate user preference estimation. Besides, in the decision module, to better utilize the attribute-level and item-level feedback, we adopt inverse reinforcement learning to learn a flexible decision strategy that selects the suitable action at each conversation turn. Through extensive experiments on four benchmark CRS datasets, we validate the effectiveness of our approach, which significantly outperforms the state-of-the-art CRS methods.|会话推荐系统(CRS)使得传统的推荐系统能够通过询问关于属性的问题和推荐项目与用户进行交互。用户的属性级反馈和项目级反馈可以用来估计用户的偏好。然而，现有的作品并没有充分利用显式项目反馈的优势——他们只是以相当隐式的方式使用项目反馈，比如更新潜在用户和项目表示。由于 CRS 有多个与用户交互的机会，当推荐被拒绝时，利用会话中的上下文可能有助于推断用户的隐式反馈(例如，一些特定的属性)。为了解决现有方法的局限性，我们提出了一个新的 CRS 框架，称为隐式反馈会话推荐(CRIF)。CRIF 将会话推荐方案设计为离线表征学习、跟踪、决策和推理四个阶段。在推理模块中，通过充分利用用户属性级反馈和项目级反馈之间的关系，可以明确推断出用户的隐性偏好。因此，CRIF 能够实现更准确的用户偏好估计。此外，在决策模块中，为了更好地利用属性级别和项目级别的反馈，我们采用逆向强化学习学习灵活的决策策略，在每次谈话转折点选择合适的行动。通过对四个基准 CRS 数据集的大量实验，我们验证了该方法的有效性，其性能明显优于目前最先进的 CRS 方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Infer+User+Implicit+Preference+in+Conversational+Recommendation)|2|
|[Recognizing Medical Search Query Intent by Few-shot Learning](https://doi.org/10.1145/3477495.3531789)|Yaqing Wang, Song Wang, Yanyan Li, Dejing Dou|Baidu Inc., Beijing, China; Baidu Inc. & University of Virginia, Beijing, China|Online healthcare services can provide unlimited and in-time medical information to users, which promotes social goods and breaks the barriers of locations. However, understanding the user intents behind the medical related queries is a challenging problem. Medical search queries are usually short and noisy, lack strict syntactic structure, and also require professional background to understand the medical terms. The medical intents are fine-grained, making them hard to recognize. In addition, many intents only have a few labeled data. To handle these problems, we propose a few-shot learning method for medical search query intent recognition called MEDIC. We extract co-click queries from user search logs as weak supervision to compensate for the lack of labeled data. We also design a new query encoder which learns to represent queries as a combination of semantic knowledge recorded in an external medical knowledge graph, syntactic knowledge which marks the grammatical role of each word in the query, and generic knowledge which is captured by language models pretrained from large-scale text corpus. Experimental results on a real medical search query intent recognition dataset validate the effectiveness of MEDIC.|在线医疗服务可以向用户提供无限制和及时的医疗信息，促进社会商品，打破地理位置的障碍。然而，理解医疗相关查询背后的用户意图是一个具有挑战性的问题。医学检索查询通常短小而嘈杂，缺乏严格的句法结构，而且需要专业背景才能理解医学术语。医学意图是细粒度的，很难识别。此外，许多意图只有少量带标签的数据。针对这些问题，本文提出了一种基于少镜头学习的医学搜索查询意图识别方法 MEDIC。我们从用户搜索日志中提取共同点击查询作为薄弱的监督，以弥补标记数据的缺乏。我们还设计了一种新的查询编码器，该编码器学习将外部医学知识图中记录的语义知识、标记查询中每个词语法角色的句法知识以及从大规模文本语料库中预先训练的语言模型获取的通用知识组合起来来表示查询。在一个真实的医学搜索查询意图识别数据集上的实验结果验证了 MEDIC 算法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Recognizing+Medical+Search+Query+Intent+by+Few-shot+Learning)|2|
|[PERD: Personalized Emoji Recommendation with Dynamic User Preference](https://doi.org/10.1145/3477495.3531779)|Xuanzhi Zheng, Guoshuai Zhao, Li Zhu, Xueming Qian|Xi'an Jiaotong University, Xi'an, China|Emoji recommendation is an important task to help users find appropriate emojis from thousands of candidates based on a short tweet text. Traditional emoji recommendation methods lack personalized recommendation and ignore user historical information in selecting emojis. In this paper, we propose a personalized emoji recommendation with dynamic user preference (PERD) which contains a text encoder and a personalized attention mechanism. In text encoder, a BERT model is contained to learn dense and low-dimensional representations of tweets. In personalized attention, user dynamic preferences are learned according to semantic and sentimental similarity between historical tweets and the tweet which is waiting for emoji recommendation. Informative historical tweets are selected and highlighted. Experiments are carried out on two real-world datasets from Sina Weibo and Twitter. Experimental results validate the superiority of our approach on personalized emoji recommendation.|表情符号推荐是一项重要的任务，它可以帮助用户根据一条短短的推文从数千名候选人中找到合适的表情符号。传统的表情符号推荐方法缺乏个性化推荐，在选择表情符号时忽略了用户的历史信息。本文提出了一种基于动态用户偏好的个性化表情推荐(PERD) ，它包含一个文本编码器和一个个性化的注意机制。在文本编码器中，BERT 模型用于学习 tweet 的稠密和低维表示。在个性化关注中，根据历史推文和等待表情推荐的推文之间的语义和情感相似性来学习用户的动态偏好。选择并突出显示信息丰富的历史 tweet。实验是在来自新浪微博和推特的两个真实世界的数据集上进行的。实验结果验证了该方法在个性化表情符号推荐中的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PERD:+Personalized+Emoji+Recommendation+with+Dynamic+User+Preference)|2|
|[Sequential/Session-based Recommendations: Challenges, Approaches, Applications and Opportunities](https://doi.org/10.1145/3477495.3532685)|Shoujin Wang, Qi Zhang, Liang Hu, Xiuzhen Zhang, Yan Wang, Charu Aggarwal|IBM T. J. Watson Research Center, Yorktown Heights, NY, USA; Macquarie University, Sydney , Australia; RMIT University, Melbourne, Australia; DeepBlue Academy of Sciences, Shanghai, China; RMIT University & Macquarie University, Melbourne , Australia; Tongji University, Shanghai , China|In recent years, sequential recommender systems (SRSs) and session-based recommender systems (SBRSs) have emerged as a new paradigm of RSs to capture users' short-term but dynamic preferences for enabling more timely and accurate recommendations. Although SRSs and SBRSs have been extensively studied, there are many inconsistencies in this area caused by the diverse descriptions, settings, assumptions and application domains. There is no work to provide a unified framework and problem statement to remove the commonly existing and various inconsistencies in the area of SR/SBR. There is a lack of work to provide a comprehensive and systematic demonstration of the data characteristics, key challenges, most representative and state-of-the-art approaches, typical real- world applications and important future research directions in the area. This work aims to fill in these gaps so as to facilitate further research in this exciting and vibrant area.|近年来，顺序推荐系统(SRS)和基于会话的推荐系统(SBRS)已经成为一种新的 RSS 模式，它们可以捕获用户短期的动态偏好，从而实现更及时、更准确的推荐。尽管 SRS 和 SBRS 已经得到了广泛的研究，但是由于描述、设置、假设和应用领域的不同，在这个领域还存在着许多不一致之处。目前还没有提供一个统一的框架和问题说明来消除 SR/SBR 领域中普遍存在的各种不一致之处。目前缺乏对数据特征、主要挑战、最具代表性和最先进的方法、典型的现实世界应用和该领域未来重要研究方向进行全面和系统的论证的工作。这项工作旨在填补这些空白，以促进在这个令人兴奋和充满活力的领域的进一步研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sequential/Session-based+Recommendations:+Challenges,+Approaches,+Applications+and+Opportunities)|2|
|[Probabilistic Permutation Graph Search: Black-Box Optimization for Fairness in Ranking](https://doi.org/10.1145/3477495.3532045)|Ali Vardasbi, Fatemeh Sarvi, Maarten de Rijke|University of Amsterdam, Amsterdam, Netherlands|There are several measures for fairness in ranking, based on different underlying assumptions and perspectives. \acPL optimization with the REINFORCE algorithm can be used for optimizing black-box objective functions over permutations. In particular, it can be used for optimizing fairness measures. However, though effective for queries with a moderate number of repeating sessions, \acPL optimization has room for improvement for queries with a small number of repeating sessions. In this paper, we present a novel way of representing permutation distributions, based on the notion of permutation graphs. Similar to~\acPL, our distribution representation, called~\acPPG, can be used for black-box optimization of fairness. Different from~\acPL, where pointwise logits are used as the distribution parameters, in~\acPPG pairwise inversion probabilities together with a reference permutation construct the distribution. As such, the reference permutation can be set to the best sampled permutation regarding the objective function, making~\acPPG suitable for both deterministic and stochastic rankings. Our experiments show that~\acPPG, while comparable to~\acPL for larger session repetitions (i.e., stochastic ranking), improves over~\acPL for optimizing fairness metrics for queries with one session (i.e., deterministic ranking). Additionally, when accurate utility estimations are available, e.g., in tabular models, the performance of \acPPG in fairness optimization is significantly boosted compared to lower quality utility estimations from a learning to rank model, leading to a large performance gap with PL. Finally, the pairwise probabilities make it possible to impose pairwise constraints such as "item $d_1$ should always be ranked higher than item $d_2$.'' Such constraints can be used to simultaneously optimize the fairness metric and control another objective such as ranking performance.|根据不同的基本假设和观点，有几种衡量排名公平性的方法。使用 REINFORCE 算法的 acPL 优化可以用于优化黑盒目标函数。特别是可以用来优化公平性措施。然而，尽管对于具有中等数量重复会话的查询有效，但是对于具有少量重复会话的查询，acPL 优化还有改进的空间。本文基于置换图的概念，提出了一种表示置换分布的新方法。与 ~ acPL 类似，我们的分布表示(称为 ~ acPPG)可以用于公平性的黑盒优化。与以逐点对数作为分布参数的 ~ acPPG 不同，在 ~ acPPG 中成对反演概率与参考置换构成分布。因此，对于目标函数，可以将参考排序设置为最佳采样排序，使 ~ acPPG 既适用于确定性排序，也适用于随机排序。我们的实验表明 ~ acPPG 在较大的会话重复(即随机排名)方面与 ~ acPL 相当，但在优化一个会话查询(即确定性排名)的公平性指标方面优于 ~ acPL。此外，当准确的效用估计可用时，例如在表格模型中，acPPG 在公平优化中的表现显着提高，与来自学习到等级模型的较低质量效用估计相比，导致与 PL 的巨大性能差距。最后，成对概率使得可以施加成对约束，例如“项 $d _ 1 $应该总是排在项 $d _ 2 $之前”这种约束可以用来同时优化公平性度量和控制另一个目标，如排名性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Probabilistic+Permutation+Graph+Search:+Black-Box+Optimization+for+Fairness+in+Ranking)|2|
|[Less is More: Reweighting Important Spectral Graph Features for Recommendation](https://doi.org/10.1145/3477495.3532014)|Shaowen Peng, Kazunari Sugiyama, Tsunenori Mine|Kyushu University, Fukuoka, Japan; Kyoto University, Kyoto, Japan|As much as Graph Convolutional Networks (GCNs) have shown tremendous success in recommender systems and collaborative filtering (CF), the mechanism of how they, especially the core components (\textiti.e., neighborhood aggregation) contribute to recommendation has not been well studied. To unveil the effectiveness of GCNs for recommendation, we first analyze them in a spectral perspective and discover two important findings: (1) only a small portion of spectral graph features that emphasize the neighborhood smoothness and difference contribute to the recommendation accuracy, whereas most graph information can be considered as noise that even reduces the performance, and (2) repetition of the neighborhood aggregation emphasizes smoothed features and filters out noise information in an ineffective way. Based on the two findings above, we propose a new GCN learning scheme for recommendation by replacing neihgborhood aggregation with a simple yet effective Graph Denoising Encoder (GDE), which acts as a band pass filter to capture important graph features. We show that our proposed method alleviates the over-smoothing and is comparable to an indefinite-layer GCN that can take any-hop neighborhood into consideration. Finally, we dynamically adjust the gradients over the negative samples to expedite model training without introducing additional complexity. Extensive experiments on five real-world datasets show that our proposed method not only outperforms state-of-the-arts but also achieves 12x speedup over LightGCN.|尽管图形卷积网络在推荐系统和协同过滤(CF)方面取得了巨大的成功，但其核心组件(textititi.e. 邻域聚合)对推荐的贡献机制还没有得到很好的研究。为了揭示 GCNs 在推荐中的有效性，我们首先从谱的角度分析它们，发现两个重要的结果: (1)只有一小部分强调邻域平滑和差异的谱图特征有助于推荐精度，而大多数图形信息可以被视为噪声，甚至降低性能; (2)重复的邻域聚合强调平滑的特征，并以无效的方式过滤掉噪声信息。基于以上两个发现，我们提出了一种新的 GCN 推荐学习方案，用一种简单而有效的图形去噪编码器(GDE)代替邻域聚合，GDE 作为带通滤波器来捕捉重要的图形特征。结果表明，我们提出的方法减轻了过度平滑，并可比拟一个不确定层 GCN，可以考虑任何跳邻居。最后，我们动态调整负样本上的梯度，以加快模型训练而不引入额外的复杂性。在五个真实世界数据集上的大量实验表明，我们提出的方法不仅优于最新技术，而且比 LightGCN 提高了12倍的速度。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Less+is+More:+Reweighting+Important+Spectral+Graph+Features+for+Recommendation)|2|
|[Interpolative Distillation for Unifying Biased and Debiased Recommendation](https://doi.org/10.1145/3477495.3532002)|Sihao Ding, Fuli Feng, Xiangnan He, Jinqiu Jin, Wenjie Wang, Yong Liao, Yongdong Zhang|University of Science and Technology of China & CCCD Key Lab of MCT, Hefei, China; University of Science and Technology of China, Hefei, China; National University of Singapore, Singapore, China|Most recommender systems evaluate model performance offline through either: 1) normal biased test on factual interactions; or 2) debiased test with records from the randomized controlled trial. In fact, both tests only reflect part of the whole picture: factual interactions are collected from the recommendation policy, fitting them better implies benefiting the platform with higher click or conversion rate; in contrast, debiased test eliminates system-induced biases and thus is more reflective of user true preference. Nevertheless, we find that existing models exhibit trade-off on the two tests, and there lacks methods that perform well on both tests. In this work, we aim to develop a win-win recommendation method that is strong on both tests. It is non-trivial, since it requires to learn a model that can make accurate prediction in both factual environment (ie normal biased test) and counterfactual environment (ie debiased test). Towards the goal, we perform environment-aware recommendation modeling by considering both environments. In particular, we propose an Interpolative Distillation (InterD) framework, which interpolates the biased and debiased models at user-item pair level by distilling a student model. We conduct experiments on three real-world datasets with both tests. Empirical results justify the rationality and effectiveness of InterD, which stands out on both tests especially demonstrates remarkable gains on less popular items.|大多数推荐系统通过以下两种方式评估模型的离线性能: 1)对实际交互进行正常偏向测试; 或者2)利用随机对照试验记录进行偏向测试。事实上，这两个测试只反映了整体情况的一部分: 实际的交互是从推荐策略中收集的，更好地适应它们意味着更高的点击率或转化率有利于平台; 相反，去偏向测试消除了系统引起的偏见，因此更能反映用户的真实偏好。尽管如此，我们发现现有的模型在两个测试中表现出了折衷，并且缺乏在两个测试中都表现良好的方法。在这项工作中，我们的目标是开发一个双赢的推荐方法，在两个测试中都很强。它是非平凡的，因为它需要学习一个模型，可以作出准确的预测既在事实环境(即正常偏向测试)和反事实环境(即去偏向测试)。为了实现这个目标，我们通过考虑两种环境来执行环境感知的推荐建模。特别地，我们提出了一个插值蒸馏(InterD)框架，它通过提取学生模型在用户项目对水平上插值有偏和无偏模型。我们用这两个测试在三个真实世界的数据集上进行实验。实证结果证明了 InterD 的合理性和有效性，它在两个测试中都表现突出，尤其是在不太受欢迎的项目上表现出显著的收益。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interpolative+Distillation+for+Unifying+Biased+and+Debiased+Recommendation)|2|
|[Enhancing Hypergraph Neural Networks with Intent Disentanglement for Session-based Recommendation](https://doi.org/10.1145/3477495.3531794)|Yinfeng Li, Chen Gao, Hengliang Luo, Depeng Jin, Yong Li|Meituan Inc., Beijing, China; Tsinghua University, Beijing, China|Session-based recommendation (SBR) aims at the next-item prediction with a short behavior session. Existing solutions fail to address two main challenges: 1) user interests are shown as dynamically coupled intents, and 2) sessions always contain noisy signals. To address them, in this paper, we propose a hypergraph-based solution, HIDE. Specifically, HIDE first constructs a hypergraph for each session to model the possible interest transitions from distinct perspectives. HIDE then disentangles the intents under each item click in micro and macro manners. In the micro-disentanglement, we perform intent-aware embedding propagation on session hypergraph to adaptively activate disentangled intents from noisy data. In the macro-disentanglement, we introduce an auxiliary intent-classification task to encourage the independence of different intents. Finally, we generate the intent-specific representations for the given session to make the final recommendation. Benchmark evaluations demonstrate the significant performance gain of our HIDE over the state-of-the-art methods.|基于会话的推荐(SBR)针对具有短行为会话的下一项预测。现有的解决方案未能解决两个主要挑战: 1)用户兴趣以动态耦合意图的形式显示，2)会话总是包含噪声信号。为了解决这些问题，本文提出了一种基于超图的解决方案—— HIDE。具体来说，HIDE 首先为每个会话构造一个超图，从不同的角度对可能的兴趣转换进行建模。然后隐藏解开每个项目下的微观和宏观方式点击的意图。在微分离中，我们在会话超图上进行意图感知的嵌入传播，以自适应地激活噪声数据中的意图。在宏观分离中，我们引入了一个辅助的意图分类任务，以鼓励不同意图的独立性。最后，我们为给定的会话生成特定于意图的表示以提出最终的建议。基准评估显示了我们的 HIDE 相对于最先进的方法的显著性能增益。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Hypergraph+Neural+Networks+with+Intent+Disentanglement+for+Session-based+Recommendation)|2|
|[Generative Adversarial Framework for Cold-Start Item Recommendation](https://doi.org/10.1145/3477495.3531897)|Hao Chen, Zefan Wang, Feiran Huang, Xiao Huang, Yue Xu, Yishi Lin, Peng He, Zhoujun Li|Beihang University, Beijing, China; Jinan University, Guangzhou, China; Tencent Inc., Shenzhen, China; Alibaba Group, Hangzhou, China; The Hong Kong Polytechnic University, Hong Kong, Hong Kong|The cold-start problem has been a long-standing issue in recommendation. Embedding-based recommendation models provide recommendations by learning embeddings for each user and item from historical interactions. Therefore, such embedding-based models perform badly for cold items which haven't emerged in the training set. The most common solutions are to generate the cold embedding for the cold item from its content features. However, the cold embeddings generated from contents have different distribution as the warm embeddings are learned from historical interactions. In this case, current cold-start methods are facing an interesting seesaw phenomenon, which improves the recommendation of either the cold items or the warm items but hurts the opposite ones. To this end, we propose a general framework named Generative Adversarial Recommendation (GAR). By training the generator and the recommender adversarially, the generated cold item embeddings can have similar distribution as the warm embeddings that can even fool the recommender. Simultaneously, the recommender is fine-tuned to correctly rank the "fake'' warm embeddings and the real warm embeddings. Consequently, the recommendation of the warms and the colds will not influence each other, thus avoiding the seesaw phenomenon. Additionally, GAR could be applied to any off-the-shelf recommendation model. Experiments on two datasets present that GAR has strong overall recommendation performance in cold-starting both the CF-based model (improved by over 30.18%) and the GNN-based model (improved by over 17.78%).|冷启动问题一直是建议中的一个长期存在的问题。基于嵌入的推荐模型通过从历史交互中学习每个用户和项目的嵌入来提供推荐。因此，这种基于嵌入的模型对于训练集中没有出现的冷项目表现很差。最常见的解决方案是根据冷藏项目的内容特征生成冷藏嵌入。然而，由内容所产生的冷嵌体在历史互动中学习到的热嵌体，其分布是不同的。在这种情况下，目前的冷启动方法正面临着一个有趣的跷跷板现象，这改善了推荐的冷项目或温暖的项目，但伤害了相反的。为此，我们提出了一个名为生成对抗性建议(GAR)的通用框架。通过对生成器和推荐器进行对抗性训练，生成的冷嵌入项可以像暖嵌入项一样具有相似的分布，甚至可以欺骗推荐器。同时，对推荐进行微调，以正确排列“假的”暖嵌入和真正的暖嵌入。因此，建议的温暖和寒冷将不会相互影响，从而避免跷跷板现象。此外，GAR 可以应用于任何现成的推荐模型。在两个数据集上的实验表明，GAR 在冷启动基于 CF 的模型(改进超过30.18%)和基于 GNN 的模型(改进超过17.78%)方面具有很强的总体推荐性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+Adversarial+Framework+for+Cold-Start+Item+Recommendation)|2|
|[Dynamics-Aware Adaptation for Reinforcement Learning Based Cross-Domain Interactive Recommendation](https://doi.org/10.1145/3477495.3531969)|Junda Wu, Zhihui Xie, Tong Yu, Handong Zhao, Ruiyi Zhang, Shuai Li|Adobe Research, San Jose, CA, USA; Shanghai Jiao Tong University, Shanghai, China; New York University, New York, NY, USA|Interactive recommender systems (IRS) have received wide attention in recent years. To capture users' dynamic preferences and maximize their long-term engagement, IRS are usually formulated as reinforcement learning (RL) problems. Despite the promise to solve complex decision-making problems, RL-based methods generally require a large amount of online interaction, restricting their applications due to economic considerations. One possible direction to alleviate this issue is cross-domain recommendation that aims to leverage abundant logged interaction data from a source domain (e.g., adventure genre in movie recommendation) to improve the recommendation quality in the target domain (e.g., crime genre). Nevertheless, prior studies mostly focus on adapting the static representations of users/items. Few have explored how the temporally dynamic user-item interaction patterns transform across domains. Motivated by the above consideration, we propose DACIR, a novel Doubly-Adaptive deep RL-based framework for Cross-domain Interactive Recommendation. We first pinpoint how users behave differently in two domains and highlight the potential to leverage the shared user dynamics to boost IRS. To transfer static user preferences across domains, DACIR enforces consistency of item representation by aligning embeddings into a shared latent space. In addition, given the user dynamics in IRS, DACIR calibrates the dynamic interaction patterns in two domains via reward correlation. Once the double adaptation narrows the cross-domain gap, we are able to learn a transferable policy for the target recommender by leveraging logged data. Experiments on real-world datasets validate the superiority of our approach, which consistently achieves significant improvements over the baselines.|交互式推荐系统(IRS)近年来受到了广泛的关注。为了捕捉用户的动态偏好并最大化他们的长期参与，IRS 通常被定义为强化学习(RL)问题。尽管有望解决复杂的决策问题，但基于 RL 的方法通常需要大量的在线交互，出于经济考虑，限制了它们的应用。缓解这个问题的一个可能的方向是跨域推荐，旨在利用来自源域的大量日志交互数据(例如，电影推荐中的冒险类型)来提高目标域的推荐质量(例如，犯罪类型)。尽管如此，以往的研究主要集中在调整用户/项目的静态表征。很少有人研究过时态动态的用户项交互模式如何跨域转换。基于上述考虑，我们提出了一种新的基于双重自适应深度 RL 的跨域交互推荐框架 DACIR。我们首先精确地指出用户在两个领域中的不同行为，并强调利用共享用户动态来提高 IRS 的潜力。为了跨域传输静态用户首选项，DACIR 通过将嵌入对齐到共享的潜在空间来实现项表示的一致性。此外，给定 IRS 中的用户动态，DACIR 通过奖励相关校正两个域中的动态交互模式。一旦双重适应缩小了跨域间的差距，我们就能够通过利用已记录的数据为目标推荐程序学习一种可转移的策略。在实际数据集上的实验验证了该方法的优越性，在基线上取得了明显的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamics-Aware+Adaptation+for+Reinforcement+Learning+Based+Cross-Domain+Interactive+Recommendation)|2|
|[A Study of Cross-Session Cross-Device Search Within an Academic Digital Library](https://doi.org/10.1145/3477495.3531929)|Sebastian Gomes, Miriam Boon, Orland Hoeber|University of Regina, Regina, SK, Canada|Information seeking in an academic digital library is complex in nature, often spanning multiple search sessions. Resuming academic search tasks requires significant cognitive effort as searchers must re-acquaint themselves with previous search session activities and previously discovered documents before resuming their search. Further, some academic searchers may find it convenient to initiate such searches on their mobile devices during short gaps in time (e.g., between classes), and resume them later in a desktop environment when they can use the extra screen space and more convenient document storage capabilities of their computers. To support such searching, we have developed an academic digital library search interface that assists searchers in managing cross-session search tasks even when moving between mobile and desktop environments. Using a controlled laboratory study we compared our approach (Dilex) to a standard academic digital library search interface. We found increased user engagement in both the initial (mobile) and resumed (desktop) search activities, and that participants spent more time on the search results pages and had an increased degree of interaction with information and personalization features during the resumed tasks. These results provide evidence that the participants were able to make effective use of the visualization features in Dilex, which enabled them to readily resume their search tasks and stay engaged in the search activities. This work represents an example of how semi-automatic search task/session management and visualization features can support cross-session search, and how designing for both mobile and desktop use can support cross-device search.|学术数字图书馆的信息搜索本质上是复杂的，通常跨越多个搜索环节。恢复学术搜索任务需要大量的认知努力，因为搜索者必须在恢复搜索之前重新熟悉以前的搜索会话活动和以前发现的文档。此外，部分学术搜寻人士可能会觉得在短时间内(例如课间)在流动装置上进行这类搜寻比较方便，稍后当他们可以使用电脑额外的屏幕空间和更方便的文件储存功能时，便可在桌面环境中继续进行这类搜寻。为了支持这种搜索，我们开发了一个学术数字图书馆搜索界面，协助搜索人员管理跨会话搜索任务，即使在移动和桌面环境之间移动。使用一个受控的实验室研究，我们比较了我们的方法(Dilex)与标准的学术数字图书馆搜索界面。我们发现，用户在初始(移动)和恢复(桌面)搜索活动中的参与度都有所提高，参与者在搜索结果页面上花费的时间更多，在恢复任务期间与信息和个性化功能的交互程度也有所提高。这些结果证明参与者能够有效地利用 Dilex 的可视化功能，使他们能够随时恢复搜索任务并继续参与搜索活动。这项工作代表了一个半自动搜索任务/会话管理和可视化特性如何支持跨会话搜索的例子，以及如何设计为移动和桌面使用都可以支持跨设备搜索。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Study+of+Cross-Session+Cross-Device+Search+Within+an+Academic+Digital+Library)|2|
|[Locality-Sensitive State-Guided Experience Replay Optimization for Sparse Rewards in Online Recommendation](https://doi.org/10.1145/3477495.3532015)|Xiaocong Chen, Lina Yao, Julian J. McAuley, Weili Guan, Xiaojun Chang, Xianzhi Wang|University of New South Wales, Sydney, NSW, Australia; University of California, San Diego, San Diego, CA, USA; Monash University, Melbourne, VIC, Australia; University of Technology Sydney, Sydney, NSW, Australia|Online recommendation requires handling rapidly changing user preferences. Deep reinforcement learning (DRL) is an effective means of capturing users' dynamic interest during interactions with recommender systems. Generally, it is challenging to train a DRL agent in online recommender systems because of the sparse rewards caused by the large action space (e.g., candidate item space) and comparatively fewer user interactions. Leveraging experience replay (ER) has been extensively studied to conquer the issue of sparse rewards. However, they adapt poorly to the complex environment of online recommender systems and are inefficient in learning an optimal strategy from past experience. As a step to filling this gap, we propose a novel state-aware experience replay model, in which the agent selectively discovers the most relevant and salient experiences and is guided to find the optimal policy for online recommendations. In particular, a locality-sensitive hashing method is proposed to selectively retain the most meaningful experience at scale and a prioritized reward-driven strategy is designed to replay more valuable experiences with higher chance. We formally show that the proposed method guarantees the upper and lower bound on experience replay and optimizes the space complexity, as well as empirically demonstrate our model's superiority to several existing experience replay methods over three benchmark simulation platforms.|在线推荐需要处理快速变化的用户首选项。深度强化学习(DRL)是在与推荐系统交互时捕捉用户动态兴趣的有效手段。一般来说，在在线推荐系统中训练 DRL 代理是一个挑战，因为大的动作空间(例如候选项空间)和相对较少的用户交互造成了稀疏的奖励。杠杆经验重放(ER)已被广泛研究，以克服稀疏奖励的问题。然而，它们不能很好地适应在线推荐系统的复杂环境，并且在从过去的经验中学习最佳策略方面效率低下。作为填补这一空白的一个步骤，我们提出了一个新的状态感知经验重放模型，其中代理人选择性地发现最相关和突出的经验，并指导寻找最佳策略的在线推荐。特别是，一个局部性敏感哈希的方法被提出来选择性地保留大规模的最有意义的经验，一个优先奖励驱动的策略被设计来回放更有价值的经验和更高的机会。在三个基准仿真平台上，我们正式证明了该方法保证了经验重放的上下界，优化了空间复杂度，并且实证证明了该模型相对于现有的几种经验重放方法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Locality-Sensitive+State-Guided+Experience+Replay+Optimization+for+Sparse+Rewards+in+Online+Recommendation)|2|
|[An Attribute-Driven Mirror Graph Network for Session-based Recommendation](https://doi.org/10.1145/3477495.3531935)|Siqi Lai, Erli Meng, Fan Zhang, Chenliang Li, Bin Wang, Aixin Sun|Xiaomi.com, Beijing, China; Nanyang Technological University, Singapore, Singapore, Singapore; Wuhan University, Wuhan, China|Session-based recommendation (SBR) aims to predict a user's next clicked item based on an anonymous yet short interaction sequence. Previous SBR models, which rely only on the limited short-term transition information without utilizing extra valuable knowledge, have suffered a lot from the problem of data sparsity. This paper proposes a novel mirror graph enhanced neural model for session-based recommendation (MGS), to exploit item attribute information over item embeddings for more accurate preference estimation. Specifically, MGS utilizes two kinds of graphs to learn item representations. One is a session graph generated from the user interaction sequence describing users' preference based on transition patterns. Another is a mirror graph built by an attribute-aware module that selects the most attribute-representative information for each session item by integrating items' attribute information. We applied an iterative dual refinement mechanism to propagate information between the session and mirror graphs. To further guide the training process of the attribute-aware module, we also introduce a contrastive learning strategy that compares two mirror graphs generated for the same session by randomly sampling the attribute-same neighbors. Experiments on three real-world datasets exhibit that the performance of MGS surpasses many state-of-the-art models.|基于会话(Session-based)的推荐(SBR)旨在基于一个匿名但很短的交互序列来预测用户的下一个点击项目。以往的 SBR 模型仅仅依赖于有限的短期转换信息，而没有利用额外的有价值的知识，已经遭受了很多数据稀疏的问题。提出了一种新的基于会话推荐(MGS)的镜像图增强神经网络模型，利用项目属性信息对项目嵌入进行更精确的偏好估计。具体来说，MGS 利用两种图来学习项目表示。一个是由用户交互序列生成的会话图，描述基于转换模式的用户偏好。另一个是由属性感知模块构建的镜像图，该模块通过集成会话项的属性信息来为每个会话项选择最具代表性的属性信息。我们应用一个迭代对偶精化机制来传播会话和镜像图之间的信息。为了进一步指导属性感知模块的训练过程，我们还引入了一种对比学习策略，通过对属性相同的邻居进行随机抽样，比较同一会话中生成的两个镜像图。在三个实际数据集上的实验表明，MGS 的性能优于许多最先进的模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Attribute-Driven+Mirror+Graph+Network+for+Session-based+Recommendation)|2|
|[FUM: Fine-grained and Fast User Modeling for News Recommendation](https://doi.org/10.1145/3477495.3531790)|Tao Qi, Fangzhao Wu, Chuhan Wu, Yongfeng Huang|Microsoft Research Asia, Beijing, China; Tsinghua Unvisersity, Beijing, China; Tsinghua University, Beijing, China|User modeling is important for news recommendation. Existing methods usually first encode user's clicked news into news embeddings independently and then aggregate them into user embedding. However, the word-level interactions across different clicked news from the same user, which contain rich detailed clues to infer user interest, are ignored by these methods. In this paper, we propose a fine-grained and fast user modeling framework (FUM) to model user interest from fine-grained behavior interactions for news recommendation. The core idea of FUM is to concatenate the clicked news into a long document and transform user modeling into a document modeling task with both intra-news and inter-news word-level interactions. Since vanilla transformer cannot efficiently handle long document, we apply an efficient transformer named Fastformer to model fine-grained behavior interactions. Extensive experiments on two real-world datasets verify that FUM can effectively and efficiently model user interest for news recommendation.|用户建模对新闻推荐非常重要。现有方法通常首先将用户点击新闻独立编码为新闻嵌入，然后将其聚合为用户嵌入。但是，这些方法忽略了来自同一用户的不同点击新闻之间的词级交互，这些新闻包含丰富的细节线索来推断用户的兴趣。在本文中，我们提出了一个细粒度和快速的用户建模框架(FUM)来模型用户的兴趣从细粒度的行为交互的新闻推荐。FUM 的核心思想是将点击的新闻连接到一个长文档中，并将用户建模转换为具有内部新闻和内部新闻字级交互的文档建模任务。由于普通的转换器不能有效地处理长文档，因此我们应用一个名为 Fastformer 的高效转换器来对细粒度的行为交互进行建模。在两个实际数据集上的大量实验证明，FUM 可以有效地为新闻推荐建立用户兴趣模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FUM:+Fine-grained+and+Fast+User+Modeling+for+News+Recommendation)|2|
|[Experiments on Generalizability of User-Oriented Fairness in Recommender Systems](https://doi.org/10.1145/3477495.3531718)|Hossein A. Rahmani, Mohammadmehdi Naghiaei, Mahdi Dehghan, Mohammad Aliannejadi|University of Southern California, California, CA, USA; University College London, London, United Kingdom; University of Amsterdam, Amsterdam, Netherlands; Shahid Beheshti University, Tehran, Iran|Recent work in recommender systems mainly focuses on fairness in recommendations as an important aspect of measuring recommendations quality. A fairness-aware recommender system aims to treat different user groups similarly. Relevant work on user-oriented fairness highlights the discriminant behavior of fairness-unaware recommendation algorithms towards a certain user group, defined based on users' activity level. Typical solutions include proposing a user-centered fairness re-ranking framework applied on top of a base ranking model to mitigate its unfair behavior towards a certain user group i.e., disadvantaged group. In this paper, we re-produce a user-oriented fairness study and provide extensive experiments to analyze the dependency of their proposed method on various fairness and recommendation aspects, including the recommendation domain, nature of the base ranking model, and user grouping method. Moreover, we evaluate the final recommendations provided by the re-ranking framework from both user- (e.g., NDCG, user-fairness) and item-side (e.g., novelty, item-fairness) metrics. We discover interesting trends and trade-offs between the model's performance in terms of different evaluation metrics. For instance, we see that the definition of the advantaged/disadvantaged user groups plays a crucial role in the effectiveness of the fairness algorithm and how it improves the performance of specific base ranking models. Finally, we highlight some important open challenges and future directions in this field. We release the data, evaluation pipeline, and the trained models publicly on https://github.com/rahmanidashti/FairRecSys.|推荐系统最近的工作主要侧重于推荐的公平性，这是衡量推荐质量的一个重要方面。公平意识推荐系统的目标是对不同的用户群体采取类似的对待方式。面向用户的公平性的相关工作强调了公平性无意识推荐算法对特定用户群的区分行为，这种区分行为是根据用户的活动水平来定义的。典型的解决方案包括提出一种以用户为中心的公平性重新排序框架，该框架应用于基本排序模型之上，以减轻其对特定用户群体(即弱势群体)的不公平行为。本文重现了一个面向用户的公平性研究，并通过大量实验分析了他们提出的方法对各种公平性和推荐方面的依赖性，包括推荐域、基本排名模型的性质和用户分组方法。此外，我们从用户(如 NDCG，用户公平性)和项目方(如新颖性，项目公平性)指标评估重新排名框架提供的最终建议。我们发现了一些有趣的趋势，以及模型在不同评估指标方面的表现之间的权衡。例如，我们发现优势/劣势用户群的定义对于公平算法的有效性以及如何提高特定基本排序模型的性能起着至关重要的作用。最后，我们强调一些重要的公开挑战和未来的方向在这一领域。我们在 https://github.com/rahmanidashti/fairrecsys 上公布数据、评估流程和训练有素的模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Experiments+on+Generalizability+of+User-Oriented+Fairness+in+Recommender+Systems)|2|
|[Geometric Disentangled Collaborative Filtering](https://doi.org/10.1145/3477495.3531982)|Yiding Zhang, Chaozhuo Li, Xing Xie, Xiao Wang, Chuan Shi, Yuming Liu, Hao Sun, Liangjie Zhang, Weiwei Deng, Qi Zhang|Microsoft Research Asia, Beijing, China; Microsoft, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China|Learning informative representations of users and items from the historical interactions is crucial to collaborative filtering (CF). Existing CF approaches usually model interactions solely within the Euclidean space. However, the sophisticated user-item interactions inherently present highly non-Euclidean anatomy with various types of geometric patterns (i.e., tree-likeness and cyclic structures). The Euclidean-based models may be inadequate to fully uncover the intent factors beneath such hybrid-geometry interactions. To remedy this deficiency, in this paper, we study the novel problem of Geometric Disentangled Collaborative Filtering (GDCF), which aims to reveal and disentangle the latent intent factors across multiple geometric spaces. A novel generative GDCF model is proposed to learn geometric disentangled representations by inferring the high-level concepts associated with user intentions and various geometries. Empirically, our proposal is extensively evaluated over five real-world datasets, and the experimental results demonstrate the superiority of GDCF.|从历史交互中学习用户和项目的信息表示对于协同过滤(CF)至关重要。现有的 CF 方法通常仅在欧几里得空间内模拟交互作用。然而，复杂的用户-项目交互本质上呈现出高度非欧几里德解剖学和各种几何模式(例如，树状结构和循环结构)。基于欧几里得的模型可能不足以完全揭示这种混合几何相互作用下的意图因素。为了弥补这一不足，本文研究了几何解缠协同过滤(gdCF)这一新问题，旨在揭示和解缠跨多个几何空间的潜在意图因素。提出了一种新的生成性 GDCF 模型，通过推导与用户意图和各种几何形状相关的高层概念来学习几何分离表示。实验结果证明了 GDCF 算法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Geometric+Disentangled+Collaborative+Filtering)|2|
|[Generating Clarifying Questions with Web Search Results](https://doi.org/10.1145/3477495.3531981)|Ziliang Zhao, Zhicheng Dou, Jiaxin Mao, JiRong Wen|Renmin University of China, Beijing, China; Beijing Key Laboratory of Big Data Management and Analysis Methods & Key Laboratory of Data Engineering and Knowledge Engineering, MOE, Beijing, China|Asking clarifying questions is an interactive way to effectively clarify user intent. When a user submits a query, the search engine will return a clarifying question with several clickable items of sub-intents for clarification. According to the existing definition, the key to asking high-quality questions is to generate good descriptions for submitted queries and provided items. However, existing methods mainly based on static knowledge bases are difficult to find descriptions for many queries because of the lack of entities within these queries and their corresponding items. For such a query, it is unable to generate an informative question. To alleviate this problem, we propose leveraging top search results of the query to help generate better descriptions because we deem that the top retrieved documents contain rich and relevant contexts of the query. Specifically, we first design a rule-based algorithm to extract description candidates from search results and rank them by various human-designed features. Then, we apply an learning-to-rank model and another generative model for generalization and further improve the quality of clarifying questions. Experimental results show that our proposed methods can generate more readable and informative questions compared with existing methods. The results prove that search results can be utilized to improve users' search experience for search clarification in conversational search systems.|提出澄清问题是一种有效澄清用户意图的互动方式。当用户提交查询时，搜索引擎将返回一个澄清问题，其中包含若干可单击的子意图项以便澄清。根据现有的定义，提出高质量问题的关键是为提交的查询和提供的项目生成良好的描述。然而，现有的基于静态知识库的查询方法由于缺乏查询实体及其对应的项目，很难为许多查询找到描述。对于这样的查询，它无法生成提供信息的问题。为了缓解这个问题，我们建议利用查询的顶部搜索结果来帮助生成更好的描述，因为我们认为顶部检索的文档包含查询的丰富和相关的上下文。具体来说，我们首先设计了一个基于规则的算法，从搜索结果中提取描述候选者，并根据各种人工设计的特征对它们进行排序。然后，我们应用一个学习排序模型和另一个生成模型进行归纳，进一步提高澄清问题的质量。实验结果表明，与现有方法相比，本文提出的方法能够产生更具可读性和信息性的问题。实验结果表明，在会话搜索系统中，利用搜索结果可以提高用户的搜索体验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generating+Clarifying+Questions+with+Web+Search+Results)|2|
|[Enhancing CTR Prediction with Context-Aware Feature Representation Learning](https://doi.org/10.1145/3477495.3531970)|Fangye Wang, Yingxu Wang, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, Ning Gu|Independent, Seattle, WA, USA; Fudan University, Shanghai, China; Microsoft Research Asia, Shanghai, China|CTR prediction has been widely used in the real world. Many methods model feature interaction to improve their performance. However, most methods only learn a fixed representation for each feature without considering the varying importance of each feature under different contexts, resulting in inferior performance. Recently, several methods tried to learn vector-level weights for feature representations to address the fixed representation issue. However, they only produce linear transformations to refine the fixed feature representations, which are still not flexible enough to capture the varying importance of each feature under different contexts. In this paper, we propose a novel module named Feature Refinement Network (FRNet), which learns context-aware feature representations at bit-level for each feature in different contexts. FRNet consists of two key components: 1) Information Extraction Unit (IEU), which captures contextual information and cross-feature relationships to guide context-aware feature refinement; and 2) Complementary Selection Gate (CSGate), which adaptively integrates the original and complementary feature representations learned in IEU with bit-level weights. Notably, FRNet is orthogonal to existing CTR methods and thus can be applied in many existing methods to boost their performance. Comprehensive experiments are conducted to verify the effectiveness, efficiency, and compatibility of FRNet.|CTR 预测在现实世界中得到了广泛的应用。许多方法对特征交互进行建模以提高其性能。然而，大多数方法只学习每个特征的固定表示，而不考虑每个特征在不同环境下的不同重要性，导致性能较差。最近，一些方法尝试学习特征表示的矢量级权重，以解决固定表示问题。然而，它们仅仅产生线性变换来细化固定特征表示，这些线性变换仍然不够灵活，不足以捕获不同上下文环境下每个特征的不同重要性。本文提出了一种新颖的特征精化网络(FRNet)模型，该模型能够在不同的上下文环境下对每个特征进行位级的上下文感知特征表示。FRNet 由两个关键组成部分组成: 1)信息抽取单元(IEU) ，它捕获上下文信息和跨特征关系，以指导上下文感知特征细化; 2)补充选择门(CSGate) ，它自适应地将在 IEU 中学到的原始和补充特征表示与位级权重结合起来。值得注意的是，FRNet 与现有的 CTR 方法是正交的，因此可以应用在许多现有的方法中来提高它们的性能。通过综合实验验证了 FRNet 的有效性、高效性和兼容性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+CTR+Prediction+with+Context-Aware+Feature+Representation+Learning)|2|
|[Joint Multisided Exposure Fairness for Recommendation](https://doi.org/10.1145/3477495.3532007)|Haolun Wu, Bhaskar Mitra, Chen Ma, Fernando Diaz, Xue Liu|McGill University, Montréal, PQ, Canada; Microsoft, Montréal, PQ, Canada; Canadian CIFAR AI Chair & Google, Montréal, PQ, Canada; City University of Hong Kong, Hong Kong SAR, Hong Kong|Prior research on exposure fairness in the context of recommender systems has focused mostly on disparities in the exposure of individual or groups of items to individual users of the system. The problem of how individual or groups of items may be systemically under or over exposed to groups of users, or even all users, has received relatively less attention. However, such systemic disparities in information exposure can result in observable social harms, such as withholding economic opportunities from historically marginalized groups (allocative harm) or amplifying gendered and racialized stereotypes (representational harm). Previously, Diaz et al. developed the expected exposure metric---that incorporates existing user browsing models that have previously been developed for information retrieval---to study fairness of content exposure to individual users. We extend their proposed framework to formalize a family of exposure fairness metrics that model the problem jointly from the perspective of both the consumers and producers. Specifically, we consider group attributes for both types of stakeholders to identify and mitigate fairness concerns that go beyond individual users and items towards more systemic biases in recommendation. Furthermore, we study and discuss the relationships between the different exposure fairness dimensions proposed in this paper, as well as demonstrate how stochastic ranking policies can be optimized towards said fairness goals.|以往关于推荐系统中曝光公平性的研究主要侧重于系统个别用户对个别或组别项目的曝光方面的差异。个人或项目组如何可能系统地暴露于一组用户甚至所有用户之下或过度暴露于这些用户之下的问题，受到的关注相对较少。然而，这种信息暴露的系统性差异可能导致可观察到的社会危害，如阻止历史上被边缘化的群体获得经济机会(分配伤害)或放大性别和种族化的刻板印象(代表性伤害)。此前，迪亚兹等人开发了预期的曝光度量标准——该标准结合了以前为信息检索开发的现有用户浏览模型——以研究内容曝光对个人用户的公平性。我们扩展了他们提出的框架，从消费者和生产者的角度建立了一系列的暴露公平度量模型。具体来说，我们考虑两种类型的利益相关者的群体属性，以确定和减轻超越个人用户和项目的建议中更系统的偏见的公平性问题。此外，我们还研究和讨论了本文提出的不同暴露公平维度之间的关系，并论证了随机排序策略是如何朝着公平目标进行优化的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Joint+Multisided+Exposure+Fairness+for+Recommendation)|2|
|[Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction](https://doi.org/10.1145/3477495.3531772)|Xinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Xueqi Cheng|ICT, CAS & University of Chinese Academy of Sciences, Beijing, China|Dense retrieval has shown promising results in many information retrieval (IR) related tasks, whose foundation is high-quality text representation learning for effective search. Some recent studies have shown that autoencoder-based language models are able to boost the dense retrieval performance using a weak decoder. However, we argue that 1) it is not discriminative to decode all the input texts and, 2) even a weak decoder has the bypass effect on the encoder. Therefore, in this work, we introduce a novel contrastive span prediction task to pre-train the encoder alone, but still retain the bottleneck ability of the autoencoder. In this way, we can 1) learn discriminative text representations efficiently with the group-wise contrastive learning over spans and, 2) avoid the bypass effect of the decoder thoroughly. Comprehensive experiments over publicly available retrieval benchmark datasets show that our approach can outperform existing pre-training methods for dense retrieval significantly.|密集检索在许多与信息检索(IR)相关的任务中显示出有希望的结果，其基础是高质量的文本表示学习，以便有效地搜索。最近的一些研究表明，基于自动编码器的语言模型能够通过弱解码器提高密集检索性能。然而，我们认为: 1)解码所有的输入文本是没有区别的，2)即使是弱解码器对编码器也有旁路效应。因此，本文提出了一种新的对比跨度预测任务来单独对编码器进行预训练，同时保留了自动编码器的瓶颈能力。这样，我们可以1)利用跨区域的分组对比学习有效地学习歧视性文本表示，2)彻底避免解码器的旁路效应。对公开检索基准数据集的综合实验表明，该方法在密集检索方面的性能明显优于现有的预训练方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pre-train+a+Discriminative+Text+Encoder+for+Dense+Retrieval+via+Contrastive+Span+Prediction)|2|
|[CenterCLIP: Token Clustering for Efficient Text-Video Retrieval](https://doi.org/10.1145/3477495.3531950)|Shuai Zhao, Linchao Zhu, Xiaohan Wang, Yi Yang|University of Technology Sydney, Sydney, NSW, Australia; Zhejiang University, Hangzhou, China|Recently, large-scale pre-training methods like CLIP have made great progress in multi-modal research such as text-video retrieval. In CLIP, transformers are vital for modeling complex multi-modal relations. However, in the vision transformer of CLIP, the essential visual tokenization process, which produces discrete visual token sequences, generates many homogeneous tokens due to the redundancy nature of consecutive and similar frames in videos. This significantly increases computation costs and hinders the deployment of video retrieval models in web applications. In this paper, to reduce the number of redundant video tokens, we design a multi-segment token clustering algorithm to find the most representative tokens and drop the non-essential ones. As the frame redundancy occurs mostly in consecutive frames, we divide videos into multiple segments and conduct segment-level clustering. Center tokens from each segment are later concatenated into a new sequence, while their original spatial-temporal relations are well maintained. We instantiate two clustering algorithms to efficiently find deterministic medoids and iteratively partition groups in high dimensional space. Through this token clustering and center selection procedure, we successfully reduce computation costs by removing redundant visual tokens. This method further enhances segment-level semantic alignment between video and text representations, enforcing the spatio-temporal interactions of tokens from within-segment frames. Our method, coined as CenterCLIP, surpasses existing state-of-the-art by a large margin on typical text-video benchmarks, while reducing the training memory cost by 35% and accelerating the inference speed by 14% at the best case. The code is available at https://github.com/mzhaoshuai/CenterCLIP https://github.com/mzhaoshuai/CenterCLIP.|近年来，CLIP 等大规模预训练方法在文本-视频检索等多模态研究方面取得了很大进展。在 CLIP 中，变压器对于建立复杂的多模态关系至关重要。然而，在 CLIP 的视觉转换器中，由于视频中连续帧和相似帧的冗余性，产生离散视觉标记序列的基本视觉标记化过程会产生许多同质标记。这大大增加了计算成本，并阻碍了在 Web 应用程序中部署视频检索模型。为了减少冗余视频令牌的数量，本文设计了一种多段令牌聚类算法来寻找最有代表性的令牌并去除非必需的令牌。由于帧冗余主要发生在连续的帧中，我们将视频分割成多个片段，进行片段级聚类。来自每个片段的中心标记后来被连接成一个新的序列，同时它们原来的时空关系得到很好的保持。我们实例化了两个聚类算法，以有效地发现确定性中介体和迭代划分群在高维空间。通过这种令牌聚类和中心选择过程，我们成功地消除了冗余的可视令牌，降低了计算成本。该方法进一步增强了视频和文本表示之间的分段级语义对齐，增强了分段帧内标记的时空交互作用。我们的方法被称为 CenterCLIP，它在典型的文本视频基准测试上大大超越了现有的最新技术，同时降低了35% 的训练记忆成本，并且在最好的情况下加快了14% 的推理速度。密码可在 https://github.com/mzhaoshuai/centerclip  https://github.com/mzhaoshuai/centerclip 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CenterCLIP:+Token+Clustering+for+Efficient+Text-Video+Retrieval)|2|
|[ProFairRec: Provider Fairness-aware News Recommendation](https://doi.org/10.1145/3477495.3532046)|Tao Qi, Fangzhao Wu, Chuhan Wu, Peijie Sun, Le Wu, Xiting Wang, Yongfeng Huang, Xing Xie|Microsoft Research Asia, Beijing, China; Hefei University of Technology, Hefei, China; Tsinghua University, Beijing, China|News recommendation aims to help online news platform users find their preferred news articles. Existing news recommendation methods usually learn models from historical user behaviors on news. However, these behaviors are usually biased on news providers. Models trained on biased user data may capture and even amplify the biases on news providers, and are unfair for some minority news providers. In this paper, we propose a provider fairness-aware news recommendation framework (named ProFairRec), which can learn news recommendation models fair for different news providers from biased user data. The core idea of ProFairRec is to learn provider-fair news representations and provider-fair user representations to achieve provider fairness. To learn provider-fair representations from biased data, we employ provider-biased representations to inherit provider bias from data. Provider-fair and -biased news representations are learned from news content and provider IDs respectively, which are further aggregated to build fair and biased user representations based on user click history. All of these representations are used in model training while only fair representations are used for user-news matching to achieve fair news recommendation. Besides, we propose an adversarial learning task on news provider discrimination to prevent provider-fair news representation from encoding provider bias. We also propose an orthogonal regularization on provider-fair and -biased representations to better reduce provider bias in provider-fair representations. Moreover, ProFairRec is a general framework and can be applied to different news recommendation methods. Extensive experiments on a public dataset verify that our ProFairRec approach can effectively improve the provider fairness of many existing methods and meanwhile maintain their recommendation accuracy.|新闻推荐旨在帮助在线新闻平台用户找到他们喜欢的新闻文章。现有的新闻推荐方法通常借鉴新闻历史用户行为的模型。然而，这些行为通常对新闻提供者有偏见。对有偏见的用户数据进行培训的模式可能捕捉甚至放大对新闻提供者的偏见，这对一些少数群体的新闻提供者是不公平的。本文提出了一个提供者公平感知的新闻推荐框架(ProFairRec) ，该框架可以从有偏差的用户数据中学习不同新闻提供者的新闻推荐模型。ProFairRec 的核心思想是通过学习提供者-公平新闻表示和提供者-公平用户表示来实现提供者公平。为了从有偏差的数据中学习提供者公平表示，我们使用提供者偏好表示从数据中继承提供者偏好。分别从新闻内容和提供者 ID 中学习提供者公平和有偏见的新闻表示，并进一步聚合它们，构建基于用户点击历史的公平和有偏见的用户表示。所有这些表示都用于模型训练，只有公平表示用于用户新闻匹配以实现公平新闻推荐。此外，我们提出了一个关于新闻提供者歧视的对抗性学习任务，以防止提供者-公平的新闻表征对提供者偏见进行编码。我们还提出了一个正交正则化的提供者公平和偏见的表示，以更好地减少提供者偏见提供者公平的表示。此外，ProFairRec 是一个通用框架，可以应用于不同的新闻推荐方法。在一个公共数据集上的大量实验证明，我们的 ProFairRec 方法能够有效地提高许多现有方法的提供者公平性，同时保持它们的推荐准确性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ProFairRec:+Provider+Fairness-aware+News+Recommendation)|2|
|[Rethinking Reinforcement Learning for Recommendation: A Prompt Perspective](https://doi.org/10.1145/3477495.3531714)|Xin Xin, Tiago Pimentel, Alexandros Karatzoglou, Pengjie Ren, Konstantina Christakopoulou, Zhaochun Ren|Google Research, London, United Kingdom; Google, Mountain View, CA, USA; University of Cambridge, Cambridge, United Kingdom; Shandong University, Qingdao City, China|Modern recommender systems aim to improve user experience. As reinforcement learning (RL) naturally fits this objective---maximizing an user's reward per session---it has become an emerging topic in recommender systems. Developing RL-based recommendation methods, however, is not trivial due to the offline training challenge. Specifically, the keystone of traditional RL is to train an agent with large amounts of online exploration making lots of 'errors' in the process. In the recommendation setting, though, we cannot afford the price of making 'errors' online. As a result, the agent needs to be trained through offline historical implicit feedback, collected under different recommendation policies; traditional RL algorithms may lead to sub-optimal policies under these offline training settings. Here we propose a new learning paradigm---namely Prompt-Based Reinforcement Learning (PRL)---for the offline training of RL-based recommendation agents. While traditional RL algorithms attempt to map state-action input pairs to their expected rewards (e.g., Q-values), PRL directly infers actions (i.e., recommended items) from state-reward inputs. In short, the agents are trained to predict a recommended item given the prior interactions and an observed reward value---with simple supervised learning. At deployment time, this historical (training) data acts as a knowledge base, while the state-reward pairs are used as a prompt. The agents are thus used to answer the question: Which item should be recommended given the prior interactions & the prompted reward value? We implement PRL with four notable recommendation models and conduct experiments on two real-world e-commerce datasets. Experimental results demonstrate the superior performance of our proposed methods.|现代推荐系统旨在改善用户体验。由于强化学习(rL)自然符合这一目标——最大化用户每次会话的回报——它已经成为推荐系统中一个新兴的话题。然而，由于离线培训的挑战，开发基于 RL 的推荐方法并非易事。具体来说，传统 RL 的核心是训练一个具有大量在线探索并在过程中犯下大量“错误”的代理。然而，在推荐设置中，我们承担不起在网上犯“错误”的代价。因此，代理需要通过在不同推荐策略下收集的离线历史隐式反馈进行训练; 传统的 RL 算法在这些离线训练设置下可能导致策略次优。在这里，我们提出了一个新的学习范式——即基于提示的强化学习(PRL)——用于基于提示的推荐代理的离线培训。传统的 RL 算法试图将状态-动作输入对映射到它们的预期奖励(例如，Q 值) ，而 PRL 直接从状态-奖励输入推断动作(例如，推荐项)。简而言之，经纪人接受训练，根据之前的互动和观察到的奖励价值，预测一个推荐的项目——用简单的监督式学习。在部署时，这个历史(培训)数据充当知识库，而状态-奖励对用作提示符。因此，代理人被用来回答这个问题: 鉴于之前的互动和提示的奖励价值，应该推荐哪个项目？我们使用四个值得注意的推荐模型来实现 PRL，并在两个真实的电子商务数据集上进行了实验。实验结果表明，该方法具有良好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rethinking+Reinforcement+Learning+for+Recommendation:+A+Prompt+Perspective)|2|
|[Alleviating Spurious Correlations in Knowledge-aware Recommendations through Counterfactual Generator](https://doi.org/10.1145/3477495.3531934)|Shanlei Mu, Yaliang Li, Wayne Xin Zhao, Jingyuan Wang, Bolin Ding, JiRong Wen|Renmin University of China & Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China; Alibaba Group, Bellevue, WA, USA; Peng Cheng Laboratory, Shenzhen, Shenzhen, China|Limited by the statistical-based machine learning framework, a spurious correlation is likely to appear in existing knowledge-aware recommendation methods. It refers to a knowledge fact that appears causal to the user behaviors (inferred by the recommender) but is not in fact. For tackling this issue, we present a novel approach to discovering and alleviating the potential spurious correlations from a counterfactual perspective. To be specific, our approach consists of two counterfactual generators and a recommender. The counterfactual generators are designed to generate counterfactual interactions via reinforcement learning, while the recommender is implemented with two different graph neural networks to aggregate the information from KG and user-item interactions respectively. The counterfactual generators and recommender are integrated in a mutually collaborative way. With this approach, the recommender helps the counterfactual generators better identify potential spurious correlations and generate high-quality counterfactual interactions, while the counterfactual generators help the recommender weaken the influence of the potential spurious correlations simultaneously. Extensive experiments on three real-world datasets have shown the effectiveness of the proposed approach by comparing it with a number of competitive baselines. Our implementation code is available at: https://github.com/RUCAIBox/CGKR.|由于受到基于统计的机器学习框架的限制，现有的知识推荐方法可能会出现伪相关。它指的是一个知识事实，似乎是因果关系的用户行为(推荐) ，但不是事实。为了解决这个问题，我们提出了一个新颖的方法来发现和减轻潜在的虚假相关性从一个反事实的角度。具体来说，我们的方法由两个反事实生成器和一个推荐器组成。反事实生成器的设计目的是通过强化学习生成反事实交互，而推荐器是通过两种不同的图形神经网络实现的，分别聚合来自 KG 和用户项目交互的信息。反事实生成器和推荐器以一种相互协作的方式集成在一起。通过这种方法，推荐器可以帮助反事实生成器更好地识别潜在的虚假关联并产生高质量的反事实交互，而反事实生成器可以帮助推荐器同时削弱潜在虚假关联的影响。在三个真实世界数据集上的大量实验表明，通过与一些竞争性基线进行比较，该方法是有效的。我们的实施守则可于以下 https://github.com/rucaibox/cgkr 索取:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Alleviating+Spurious+Correlations+in+Knowledge-aware+Recommendations+through+Counterfactual+Generator)|2|
|[HAKG: Hierarchy-Aware Knowledge Gated Network for Recommendation](https://doi.org/10.1145/3477495.3531987)|Yuntao Du, Xinjun Zhu, Lu Chen, Baihua Zheng, Yunjun Gao|Singapore Management University, Singapore, Singapore; Zhejiang University, Ningbo, China; Zhejiang University, Hangzhou, China|Knowledge graph (KG) plays an increasingly important role to improve the recommendation performance and interpretability. A recent technical trend is to design end-to-end models based on the information propagation schemes. However, existing propagation-based methods fail to (1) model the underlying hierarchical structures and relations, and (2) capture the high-order collaborative signals of items for learning high-quality user and item representations. In this paper, we propose a new model, called Hierarchy-Aware Knowledge Gated Network (HAKG), to tackle the aforementioned problems. Technically, we model users and items (that are captured by a user-item graph), as well as entities and relations (that are captured in a KG) in hyperbolic space, and design a new hyperbolic aggregation scheme to gather relational contexts over KG. Meanwhile, we introduce a novel angle constraint to preserve characteristics of items in the embedding space. Furthermore, we propose the dual item embeddings design to represent and propagate collaborative signals and knowledge associations separately, and leverage the gated aggregation to distill discriminative information for better capturing user behavior patterns. Experimental results on three benchmark datasets show that, HAKG achieves significant improvement over the state-of-the-art methods like CKAN, Hyper-Know, and KGIN. Further analyses on the learned hyperbolic embeddings confirm that HAKG can offer meaningful insights into the hierarchies of data.|知识图(KG)在提高推荐性能和可解释性方面发挥着越来越重要的作用。基于信息传播方案的端到端模型设计是近年来的一个技术趋势。然而，现有的基于传播的方法无法(1)对底层的层次结构和关系进行建模，(2)捕获项目的高阶协作信号来学习高质量的用户和项目表示。本文提出了一种新的层次感知知识门限网络(HAKG)模型来解决上述问题。从技术上讲，我们对用户和项目(通过用户项目图捕获)以及实体和关系(通过 KG 捕获)进行双曲空间建模，并设计一个新的双曲线聚合方案，通过 KG 收集关系上下文。同时，我们引入了一种新的角度约束来保持嵌入空间中项目的特征。此外，我们提出了双项嵌入设计，分别表示和传播协作信号和知识关联，并利用门限聚合提取区分信息，以更好地捕捉用户行为模式。在三个基准数据集上的实验结果表明，HAKG 比最先进的 CKAN、 Hyper-Know 和 KGIN 方法有明显的改进。进一步分析学习双曲嵌入，证实 HAKG 可以提供有意义的洞察数据层次。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HAKG:+Hierarchy-Aware+Knowledge+Gated+Network+for+Recommendation)|2|
|[Entity-aware Transformers for Entity Search](https://doi.org/10.1145/3477495.3531971)|Emma J. Gerritse, Faegheh Hasibi, Arjen P. de Vries|Radboud University, Nijmegen, Netherlands|Pre-trained language models such as BERT have been a key ingredient to achieve state-of-the-art results on a variety of tasks in natural language processing and, more recently, also in information retrieval. Recent research even claims that BERT is able to capture factual knowledge about entity relations and properties, the information that is commonly obtained from knowledge graphs. This paper investigates the following question: Do BERT-based entity retrieval models benefit from additional entity information stored in knowledge graphs? To address this research question, we map entity embeddings into the same input space as a pre-trained BERT model and inject these entity embeddings into the BERT model. This entity-enriched language model is then employed on the entity retrieval task. We show that the entity-enriched BERT model improves effectiveness on entity-oriented queries over a regular BERT model, establishing a new state-of-the-art result for the entity retrieval task, with substantial improvements for complex natural language queries and queries requesting a list of entities with a certain property. Additionally, we show that the entity information provided by our entity-enriched model particularly helps queries related to less popular entities. Last, we observe empirically that the entity-enriched BERT models enable fine-tuning on limited training data, which otherwise would not be feasible due to the known instabilities of BERT in few-sample fine-tuning, thereby contributing to data-efficient training of BERT for entity search.|在自然语言处理以及最近的信息检索中，像 BERT 这样经过预先训练的语言模型已经成为在各种任务中取得最先进结果的关键因素。最近的研究甚至声称 BERT 能够捕获关于实体关系和属性的实际知识，这些信息通常是从知识图表中获得的。本文研究了以下问题: 基于 BERT 的实体检索模型是否受益于存储在知识图中的附加实体信息？为了解决这个问题，我们将实体嵌入映射到与预先训练的 BERT 模型相同的输入空间中，并将这些实体嵌入注入到 BERT 模型中。然后将这种实体丰富的语言模型应用于实体检索任务。实验结果表明，与常规的 BERT 模型相比，实体增强的 BERT 模型提高了面向实体查询的效率，为实体检索任务建立了一个新的最先进的结果，对于复杂的自然语言查询和请求具有特定属性的实体列表的查询有了实质性的改进。此外，我们还展示了实体丰富模型提供的实体信息特别有助于与不太流行的实体相关的查询。最后，我们实验观察到实体增强的 BERT 模型能够对有限的训练数据进行微调，否则由于已知的 BERT 在少样本微调中的不稳定性，这是不可行的，从而有助于为实体搜索进行数据有效的 BERT 训练。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Entity-aware+Transformers+for+Entity+Search)|2|
|[CharacterBERT and Self-Teaching for Improving the Robustness of Dense Retrievers on Queries with Typos](https://doi.org/10.1145/3477495.3531951)|Shengyao Zhuang, Guido Zuccon|The University of Queensland, Brisbane, QLD, Australia|Current dense retrievers are not robust to out-of-domain and outlier queries, i.e. their effectiveness on these queries is much poorer than what one would expect. In this paper, we consider a specific instance of such queries: queries that contain typos. We show that a small character level perturbation in queries (as caused by typos) highly impacts the effectiveness of dense retrievers. We then demonstrate that the root cause of this resides in the input tokenization strategy employed by BERT. In BERT, tokenization is performed using the BERT's WordPiece tokenizer and we show that a token with a typo will significantly change the token distributions obtained after tokenization. This distribution change translates to changes in the input embeddings passed to the BERT-based query encoder of dense retrievers. We then turn our attention to devising dense retriever methods that are robust to such queries with typos, while still being as performant as previous methods on queries without typos. For this, we use CharacterBERT as the backbone encoder and an efficient yet effective training method, called Self-Teaching (ST), that distills knowledge from queries without typos into the queries with typos. Experimental results show that CharacterBERT in combination with ST achieves significantly higher effectiveness on queries with typos compared to previous methods. Along with these results and the open-sourced implementation of the methods, we also provide a new passage retrieval dataset consisting of real-world queries with typos and associated relevance assessments on the MS MARCO corpus, thus supporting the research community in the investigation of effective and robust dense retrievers. Code, experimental results and dataset are made available at https://github.com/ielab/CharacterBERT-DR.|当前的密集检索器对域外查询和离群查询不够健壮，即它们对这些查询的有效性比人们预期的要差得多。在本文中，我们考虑这种查询的一个特定实例: 包含输入错误的查询。我们表明，查询中的小字符级别扰动(由输入错误引起)会严重影响稠密检索器的有效性。然后，我们证明了造成这种情况的根本原因在于 BERT 采用的输入标记化策略。在 BERT 中，标记化是使用 BERT 的 WordPiece 标记化程序执行的，我们展示了带有输入错误的标记将显著改变标记化后获得的标记分发。这种分布更改转化为传递给稠密检索器的基于 BERT 的查询编码器的输入嵌入的更改。然后，我们将注意力转向设计密集的检索方法，这些方法对于这种带有输入错误的查询是健壮的，同时在没有输入错误的查询上仍然和以前的方法一样性能良好。为此，我们使用 CHARTERBERT 作为骨干编码器和一种有效的训练方法，称为自学(ST) ，它将知识从没有拼写错误的查询中提取到有拼写错误的查询中。实验结果表明，与以前的方法相比，采用与 ST 相结合的方法对拼写错误的查询具有更高的查询效率。随着这些结果和方法的开源实现，我们还提供了一个新的通道检索数据集，包括在 MS MARCO 语料库上输入错误的真实世界查询和相关的相关性评估，从而支持研究社区调查有效和健壮的密集检索器。代码、实验结果和数据集可在 https://github.com/ielab/characterbert-dr 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CharacterBERT+and+Self-Teaching+for+Improving+the+Robustness+of+Dense+Retrievers+on+Queries+with+Typos)|2|
|[Thinking inside The Box: Learning Hypercube Representations for Group Recommendation](https://doi.org/10.1145/3477495.3532066)|Tong Chen, Hongzhi Yin, Jing Long, Quoc Viet Hung Nguyen, Yang Wang, Meng Wang|The University of Queensland, Brisbane, QLD, Australia; Griffith University, Gold Coast, Australia; Hefei University of Technology, Hefei, China|As a step beyond traditional personalized recommendation, group recommendation is the task of suggesting items that can satisfy a group of users. In group recommendation, the core is to design preference aggregation functions to obtain a quality summary of all group members' preferences. Such user and group preferences are commonly represented as points in the vector space (i.e., embeddings), where multiple user embeddings are compressed into one to facilitate ranking for group-item pairs. However, the resulted group representations, as points, lack adequate flexibility and capacity to account for the multi-faceted user preferences. Also, the point embedding-based preference aggregation is a less faithful reflection of a group's decision-making process, where all users have to agree on a certain value in each embedding dimension instead of a negotiable interval. In this paper, we propose a novel representation of groups via the notion of hypercubes, which are subspaces containing innumerable points in the vector space. Specifically, we design the hypercube recommender (CubeRec) to adaptively learn group hypercubes from user embeddings with minimal information loss during preference aggregation, and to leverage a revamped distance metric to measure the affinity between group hypercubes and item points. Moreover, to counteract the long-standing issue of data sparsity in group recommendation, we make full use of the geometric expressiveness of hypercubes and innovatively incorporate self-supervision by intersecting two groups. Experiments on four real-world datasets have validated the superiority of CubeRec over state-of-the-art baselines.|作为超越传统个性化推荐的一个步骤，群体推荐的任务是推荐能够满足一组用户的项目。在群组推荐中，核心是设计偏好聚合函数，以获得群组成员偏好的高质量汇总。这样的用户和组首选项通常表示为向量空间中的点(即嵌入) ，其中多个用户嵌入被压缩为一个，以便于对组-项目对进行排序。然而，由此产生的群体表示，作为点，缺乏足够的灵活性和能力来说明多方面的用户偏好。另外，基于点嵌入的偏好聚合是群体决策过程的一种不那么忠实的反映，即所有用户都必须在每个嵌入维度上同意一个确定的值，而不是一个可协商的区间。本文利用超立方体的概念，提出了一种新的群表示方法，超立方体是向量空间中包含无数个点的子空间。具体来说，我们设计了超立方体推荐器(CubeRec)来自适应地从用户嵌入中学习组超立方体，在偏好聚合过程中以最小的信息损失，并利用改进的距离度量来度量组超立方体和项目点之间的亲和力。此外，为了解决群体推荐中长期存在的数据稀疏问题，我们充分利用了超立方体的几何表达能力，创新性地将自我监督融入到群体推荐中。在四个实际数据集上的实验验证了 CubeRec 相对于最先进的基线的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Thinking+inside+The+Box:+Learning+Hypercube+Representations+for+Group+Recommendation)|2|
|[Multi-modal Graph Contrastive Learning for Micro-video Recommendation](https://doi.org/10.1145/3477495.3532027)|Zixuan Yi, Xi Wang, Iadh Ounis, Craig MacDonald|University of Glasgow, Glasgow, United Kingdom|Recently micro-videos have become more popular in social media platforms such as TikTok and Instagram. Engagements in these platforms are facilitated by multi-modal recommendation systems. Indeed, such multimedia content can involve diverse modalities, often represented as visual, acoustic, and textual features to the recommender model. Existing works in micro-video recommendation tend to unify the multi-modal channels, thereby treating each modality with equal importance. However, we argue that these approaches are not sufficient to encode item representations with multiple modalities, since the used methods cannot fully disentangle the users' tastes on different modalities. To tackle this problem, we propose a novel learning method named Multi-Modal Graph Contrastive Learning (MMGCL), which aims to explicitly enhance multi-modal representation learning in a self-supervised learning manner. In particular, we devise two augmentation techniques to generate the multiple views of a user/item: modality edge dropout and modality masking. Furthermore, we introduce a novel negative sampling technique that allows to learn the correlation between modalities and ensures the effective contribution of each modality. Extensive experiments conducted on two micro-video datasets demonstrate the superiority of our proposed MMGCL method over existing state-of-the-art approaches in terms of both recommendation performance and training convergence speed.|最近，微视频在 TikTok 和 Instagram 等社交媒体平台上变得越来越流行。多模式推荐系统促进了这些平台的参与。事实上，这样的多媒体内容可以涉及不同的形式，通常表示为视觉，声学和文本特征的推荐模型。现有的微视频推荐作品倾向于统一多通道，从而对各种通道给予同等重视。然而，我们认为这些方法不足以用多种模式来编码项目表示，因为所使用的方法不能完全区分使用者对不同模式的喜好。为了解决这一问题，我们提出了一种新的学习方法——多模态图形对比学习(MMGCL) ，该方法旨在以自监督学习的方式明确地增强多模态表示学习。特别地，我们设计了两种增强技术来产生一个用户/项目的多视图: 模态边缘丢失和模态掩蔽。此外，我们还引入了一种新颖的负抽样技术，它可以学习模式之间的相关性，并确保每种模式的有效贡献。在两个微视频数据集上进行的大量实验表明，我们提出的 MMGCL 方法在推荐性能和训练收敛速度方面都优于现有的最新方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-modal+Graph+Contrastive+Learning+for+Micro-video+Recommendation)|2|
|[InPars: Unsupervised Dataset Generation for Information Retrieval](https://doi.org/10.1145/3477495.3531863)|Luiz Henrique Bonifacio, Hugo Abonizio, Marzieh Fadaee, Rodrigo Frassetto Nogueira|Zeta Alpha, Amsterdam, Netherlands; Zeta Alpha, NeuralMind, University of Campinas, & University of Waterloo, Amsterdam, Netherlands; Zeta Alpha, NeuralMind, & University of Campinas, Amsterdam, Netherlands; Zeta Alpha & NeuralMind, Amsterdam, Netherlands|The Information Retrieval (IR) community has recently witnessed a revolution due to large pretrained transformer models. Another key ingredient for this revolution was the MS MARCO dataset, whose scale and diversity has enabled zero-shot transfer learning to various tasks. However, not all IR tasks and domains can benefit from one single dataset equally. Extensive research in various NLP tasks has shown that using domain-specific training data, as opposed to a general-purpose one, improves the performance of neural models. In this work, we harness the few-shot capabilities of large pretrained language models as synthetic data generators for IR tasks. We show that models finetuned solely on our synthetic datasets outperform strong baselines such as BM25 as well as recently proposed self-supervised dense retrieval methods. Code, models, and data are available at https://github.com/zetaalphavector/inpars.|由于大型预先训练的变压器模型，信息检索(IR)社区最近经历了一场革命。这场革命的另一个关键因素是微软 MARCO 数据集，它的规模和多样性使得零镜头转移学习能够应用于各种任务。然而，并不是所有的 IR 任务和领域都能从一个单一的数据集中获益。对各种自然语言处理任务的广泛研究表明，与通用训练数据相比，使用领域特定的训练数据可以提高神经模型的性能。在这项工作中，我们利用大型预先训练的语言模型作为 IR 任务的合成数据生成器的少数镜头功能。我们表明，模型微调仅在我们的合成数据集优于强基线，如 BM25以及最近提出的自我监督密集检索方法。代码、模型和数据可在 https://github.com/zetaalphavector/inpars 获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=InPars:+Unsupervised+Dataset+Generation+for+Information+Retrieval)|2|
|[Addressing Gender-related Performance Disparities in Neural Rankers](https://doi.org/10.1145/3477495.3531882)|Shirin Seyedsalehi, Amin Bigdeli, Negar Arabzadeh, Morteza Zihayat, Ebrahim Bagheri|Ryerson University, Toronto, Canada; Ryerson University, Toronto, ON, Canada; University of Waterloo, Waterloo, Canada|While neural rankers continue to show notable performance improvements over a wide variety of information retrieval tasks, there have been recent studies that show such rankers may intensify certain stereotypical biases. In this paper, we investigate whether neural rankers introduce retrieval effectiveness (performance) disparities over queries related to different genders. We specifically study whether there are significant performance differences between male and female queries when retrieved by neural rankers. Through our empirical study over the MS MARCO collection, we find that such performance disparities are notable and that the performance disparities may be due to the difference between how queries and their relevant judgements are collected and distributed for different gendered queries. More specifically, we observe that male queries are more closely associated with their relevant documents compared to female queries and hence neural rankers are able to more easily learn associations between male queries and their relevant documents. We show that it is possible to systematically balance relevance judgment collections in order to reduce performance disparity between different gendered queries without negatively compromising overall model performance.|虽然神经系统排名在各种信息检索任务中表现显著，但最近的研究表明，这种排名可能会加剧某些刻板的偏见。在本文中，我们研究了神经排序器是否引入检索效率(性能)差异相关的查询不同性别。我们专门研究是否有显着性能差异的男性和女性查询时，检索的神经排序。通过对 MS MARCO 集合的实证研究，我们发现这种性能差异是显著的，性能差异可能是由于不同性别的查询如何收集和分配查询及其相关判断之间的差异造成的。更具体地说，我们观察到，与女性查询相比，男性查询与其相关文档关联更密切，因此神经排序器能够更容易地学习男性查询与其相关文档之间的关联。我们表明，系统地平衡相关性判断集合是可能的，以减少不同性别查询之间的性能差异，而不会对整体模型性能造成负面影响。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Addressing+Gender-related+Performance+Disparities+in+Neural+Rankers)|2|
|[State Encoders in Reinforcement Learning for Recommendation: A Reproducibility Study](https://doi.org/10.1145/3477495.3531716)|Jin Huang, Harrie Oosterhuis, Bunyamin Cetinkaya, Thijs Rood, Maarten de Rijke|Radboud University, Nijmegen, Netherlands; University of Amsterdam, Amsterdam, Netherlands|Methods for reinforcement learning for recommendation are increasingly receiving attention as they can quickly adapt to user feedback. A typical RL4Rec framework consists of (1) a state encoder to encode the state that stores the users' historical interactions, and (2) an RL method to take actions and observe rewards. Prior work compared four state encoders in an environment where user feedback is simulated based on real-world logged user data. An attention-based state encoder was found to be the optimal choice as it reached the highest performance. However, this finding is limited to the actor-critic method, four state encoders, and evaluation-simulators that do not debias logged user data. In response to these shortcomings, we reproduce and expand on the existing comparison of attention-based state encoders (1) in the publicly available debiased RL4Rec SOFA simulator with (2) a different RL method, (3) more state encoders, and (4) a different dataset. Importantly, our experimental results indicate that existing findings do not generalize to the debiased SOFA simulator generated from a different dataset and a DQN-based method when compared with more state encoders.|强化学习推荐方法越来越受到关注，因为它们能够迅速适应用户的反馈。典型的 RL4Rec 框架包括(1)一个状态编码器来编码存储用户历史交互的状态，(2)一个 RL 方法来采取行动和观察奖励。先前的工作比较了四个状态编码器在一个环境中，其中用户反馈是基于真实世界的日志用户数据模拟的。结果表明，基于注意的状态编码器性能最好。然而，这一发现仅限于参与者批评方法、四个状态编码器和评估模拟器，它们不会减少记录的用户数据。针对这些缺点，我们复制和扩展了公开可用的无偏 RL4Rec SOFA 模拟器中基于注意力的状态编码器(1)与(2)不同的 RL 方法，(3)更多的状态编码器和(4)不同的数据集的现有比较。重要的是，我们的实验结果表明，与更多的状态编码器相比，现有的发现不能推广到由不同数据集和基于 DQN 的方法产生的去偏 SOFA 模拟器。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=State+Encoders+in+Reinforcement+Learning+for+Recommendation:+A+Reproducibility+Study)|2|
|[Wikimarks: Harvesting Relevance Benchmarks from Wikipedia](https://doi.org/10.1145/3477495.3531731)|Laura Dietz, Shubham Chatterjee, Connor Lennox, Sumanta Kashyapi, Pooja Oza, Ben Gamari|Well-Typed LLP, London, United Kingdom; University of New Hampshire, Durham, NH, USA|We provide a resource for automatically harvesting relevance benchmarks from Wikipedia -- which we refer to as "Wikimarks" to differentiate them from manually created benchmarks. Unlike simulated benchmarks, they are based on manual annotations of Wikipedia authors. Studies on the TREC Complex Answer Retrieval track demonstrated that leaderboards under Wikimarks and manually annotated benchmarks are very similar. Because of their availability, Wikimarks can fill an important need for Information Retrieval research. We provide a meta-resource to harvest Wikimarks for several information retrieval tasks across different languages: paragraph retrieval, entity ranking, query-specific clustering, outline prediction, and relevant entity linking and many more. In addition, we provide example Wikimarks for English, Simple English, and Japanese derived from the 01/01/2022 Wikipedia dump. Resource available: https://trema-unh.github.io/wikimarks/|我们提供了一个从 Wikipedia 自动获取相关性基准的资源——我们称之为“ Wikimark”，以区分它们与手工创建的基准。与模拟基准测试不同，它们基于 Wikipedia 作者的手动注释。TREC 复杂答案检索跟踪的研究表明，Wikimark 下的排行榜和手动注释的基准测试非常相似。由于其可用性，wikimark 可以满足信息检索研究的重要需求。我们提供了一个元资源来收集维基信息检索，用于不同语言的多项任务: 段落检索、实体排名、特定于查询的聚类、大纲预测、相关实体链接等等。此外，我们还提供了源自01/01/2022 Wikipedia 转储的英语、简单英语和日语的维基标记示例。可供使用的资源:  https://trema-unh.github.io/wikimarks/|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Wikimarks:+Harvesting+Relevance+Benchmarks+from+Wikipedia)|2|
|[Gender Fairness in Information Retrieval Systems](https://doi.org/10.1145/3477495.3532680)|Amin Bigdeli, Negar Arabzadeh, Shirin Seyedsalehi, Morteza Zihayat, Ebrahim Bagheri|Ryerson University, Toronto, Canada; University of Waterloo, Waterloo, Canada|Recent studies have shown that it is possible for stereotypical gender biases to find their way into representational and algorithmic aspects of retrieval methods; hence, exhibit themselves in retrieval outcomes. In this tutorial, we inform the audience of various studies that have systematically reported the presence of stereotypical gender biases in Information Retrieval (IR) systems. We further classify existing work on gender biases in IR systems as being related to (1) relevance judgement datasets, (2) structure of retrieval methods, and (3) representations learnt for queries and documents. We present how each of these components can be impacted by or cause intensified biases during retrieval. Based on these identified issues, we then present a collection of approaches from the literature that have discussed how such biases can be measured, controlled, or mitigated. Additionally, we introduce publicly available datasets that are often used for investigating gender biases in IR systems as well as evaluation methodology adopted for determining the utility of gender bias mitigation strategies.|最近的研究表明，陈规定型的性别偏见有可能进入检索方法的表征和算法方面; 因此，在检索结果中表现出来。在这个教程中，我们告诉观众的各种研究，已经系统地报告了存在的陈规定型性别偏见的信息检索(IR)系统。我们进一步将关于 IR 系统中性别偏见的现有工作分类为: (1)相关性判断数据集，(2)检索方法的结构，(3)查询和文档所学到的表征。我们介绍了这些组件中的每一个在检索过程中是如何受到偏差的影响或引起偏差加剧的。基于这些已确定的问题，我们然后从文献中提出了一系列方法，这些方法讨论了如何可以测量、控制或减轻这些偏差。此外，我们还介绍了公开可用的数据集，这些数据集通常用于调查 IR 系统中的性别偏见，以及用于确定性别偏见缓解策略效用的评估方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Gender+Fairness+in+Information+Retrieval+Systems)|2|
|[Fairness of Exposure in Light of Incomplete Exposure Estimation](https://doi.org/10.1145/3477495.3531977)|Maria Heuss, Fatemeh Sarvi, Maarten de Rijke|University of Amsterdam, Amsterdam, Netherlands; AIRLab & University of Amsterdam, Amsterdam , Netherlands|Fairness of exposure is a commonly used notion of fairness for ranking systems. It is based on the idea that all items or item groups should get exposure proportional to the merit of the item or the collective merit of the items in the group. Often, stochastic ranking policies are used to ensure fairness of exposure. Previous work unrealistically assumes that we can reliably estimate the expected exposure for all items in each ranking produced by the stochastic policy. In this work, we discuss how to approach fairness of exposure in cases where the policy contains rankings of which, due to inter-item dependencies, we cannot reliably estimate the exposure distribution. In such cases, we cannot determine whether the policy can be considered fair. % Our contributions in this paper are twofold. First, we define a method called \method for finding stochastic policies that avoid showing rankings with unknown exposure distribution to the user without having to compromise user utility or item fairness. Second, we extend the study of fairness of exposure to the top-k setting and also assess \method in this setting. We find that \method can significantly reduce the number of rankings with unknown exposure distribution without a drop in user utility or fairness compared to existing fair ranking methods, both for full-length and top-k rankings. This is an important first step in developing fair ranking methods for cases where we have incomplete knowledge about the user's behaviour.|曝光公平是排名系统中常用的公平概念。它是基于这样的想法，即所有项目或项目组应获得与项目的价值或集体价值的项目组成比例的曝光。通常，随机排序策略用于确保公平的曝光。以往的工作不切实际地假设我们可以可靠地估计所有项目的预期风险在每个排名所产生的随机政策。在这项工作中，我们讨论了如何接近公平的情况下，政策包含排名，由于项目间的相关性，我们不能可靠地估计曝光分布。在这种情况下，我们无法确定这项政策是否公平。% 我们在这篇论文中的贡献是双重的。首先，我们定义了一种方法，叫做随机策略发现方法，避免显示排名与未知的暴露分布给用户，而不必妥协用户效用或项目的公平性。其次，我们将公平性的研究扩展到 Top-k 环境，并在此环境下进行了评价方法的探讨。我们发现，与现有的公平排名方法相比，该方法可以显著减少未知曝光分布的排名数量，而不会降低用户效用或公平性，无论是全长排名还是前 K 排名。对于我们不完全了解用户行为的情况，这是开发公平排名方法的重要第一步。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fairness+of+Exposure+in+Light+of+Incomplete+Exposure+Estimation)|2|
|[Few-Shot Stance Detection via Target-Aware Prompt Distillation](https://doi.org/10.1145/3477495.3531979)|Yan Jiang, Jinhua Gao, Huawei Shen, Xueqi Cheng|Data Intelligence System Research Center, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences & University of Chinese Academy of Sciences, Beijing, China; Data Intelligence System Research Center, Institute of Computing Technology, Chinese Academy of Sciences & University of Chinese Academy of Sciences, Beijing, China|Stance detection aims to identify whether the author of a text is in favor of, against, or neutral to a given target. The main challenge of this task comes two-fold: few-shot learning resulting from the varying targets and the lack of contextual information of the targets. Existing works mainly focus on solving the second issue by designing attention-based models or introducing noisy external knowledge, while the first issue remains under-explored. In this paper, inspired by the potential capability of pre-trained language models (PLMs) serving as knowledge bases and few-shot learners, we propose to introduce prompt-based fine-tuning for stance detection. PLMs can provide essential contextual information for the targets and enable few-shot learning via prompts. Considering the crucial role of the target in stance detection task, we design target-aware prompts and propose a novel verbalizer. Instead of mapping each label to a concrete word, our verbalizer maps each label to a vector and picks the label that best captures the correlation between the stance and the target. Moreover, to alleviate the possible defect of dealing with varying targets with a single hand-crafted prompt, we propose to distill the information learned from multiple prompts. Experimental results show the superior performance of our proposed model in both full-data and few-shot scenarios.|立场检测的目的是确定文本的作者是否赞成、反对或中立给定的目标。这项任务的主要挑战来自两个方面: 由于目标的不同和缺乏目标的上下文信息导致的少量学习。现有的研究主要集中在通过设计基于注意的模型或引入噪声外部知识来解决第二个问题，而第一个问题仍然没有得到充分的研究。本文受预训练语言模型(PLM)作为知识库和少镜头学习者的潜在能力的启发，提出了一种基于提示的姿态检测微调方法。PLM 可以为目标提供必要的上下文信息，并通过提示实现少量学习。考虑到目标在姿态检测任务中的重要作用，本文设计了目标感知提示，并提出了一种新的语言表达方法。代替将每个标签映射到一个具体的单词，我们的语言表达器将每个标签映射到一个向量，并选择最好地捕获立场和目标之间相关性的标签。此外，为了减少单一手工提示处理不同目标的可能缺陷，我们建议提取从多个提示中学到的信息。实验结果表明，该模型在全数据场景和少镜头场景下均具有良好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Few-Shot+Stance+Detection+via+Target-Aware+Prompt+Distillation)|2|
|[Unsupervised Belief Representation Learning with Information-Theoretic Variational Graph Auto-Encoders](https://doi.org/10.1145/3477495.3532072)|Jinning Li, Huajie Shao, Dachun Sun, Ruijie Wang, Yuchen Yan, Jinyang Li, Shengzhong Liu, Hanghang Tong, Tarek F. Abdelzaher||This paper develops a novel unsupervised algorithm for belief representation learning in polarized networks that (i) uncovers the latent dimensions of the underlying belief space and (ii) jointly embeds users and content items (that they interact with) into that space in a manner that facilitates a number of downstream tasks, such as stance detection, stance prediction, and ideology mapping. Inspired by total correlation in information theory, we propose the Information-Theoretic Variational Graph Auto-Encoder (InfoVGAE) that learns to project both users and content items (e.g., posts that represent user views) into an appropriate disentangled latent space. To better disentangle latent variables in that space, we develop a total correlation regularization module, a Proportional-Integral (PI) control module, and adopt rectified Gaussian distribution to ensure the orthogonality. The latent representation of users and content can then be used to quantify their ideological leaning and detect/predict their stances on issues. We evaluate the performance of the proposed InfoVGAE on three real-world datasets, of which two are collected from Twitter and one from U.S. Congress voting records. The evaluation results show that our model outperforms state-of-the-art unsupervised models by reducing 10.5% user clustering errors and achieving 12.1% higher F1 scores for stance separation of content items. In addition, InfoVGAE produces a comparable result with supervised models. We also discuss its performance on stance prediction and user ranking within ideological groups.|本文提出了一种新的极化网络信念表示学习的无监督算法，该算法(i)揭示潜在信念空间的潜在维度，(ii)联合嵌入用户和内容项(他们交互的)到该空间中，以促进一些下游任务，如姿态检测，姿态预测和意识形态映射。受到信息论中的完全相关性的启发，我们提出了信息论变分图自动编码器(InfoVGAE) ，它学会将用户和内容项(例如，代表用户视图的帖子)投射到适当的分离潜在空间中。为了更好地分离该空间中的潜变量，我们开发了一个全相关正则化模块，一个比例积分(PI)控制模块，并采用校正正态分布来确保正交性。用户和内容的潜在表征可以用来量化他们的意识形态倾向，并发现/预测他们对问题的立场。我们在三个真实世界的数据集上评估提议的 InfoVGAE 的性能，其中两个数据集是从 Twitter 收集的，一个来自美国国会的投票记录。评估结果表明，我们的模型优于国家的最先进的无监督模型，减少了10.5% 的用户聚类错误，实现了12.1% 更高的 F1得分的立场分离的内容项目。此外，InfoVGAE 产生与监督模型相当的结果。并讨论了它在思想群体中的姿态预测和用户排名方面的表现。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unsupervised+Belief+Representation+Learning+with+Information-Theoretic+Variational+Graph+Auto-Encoders)|2|
|[Towards Motivational and Empathetic Response Generation in Online Mental Health Support](https://doi.org/10.1145/3477495.3531912)|Tulika Saha, Vaibhav Gakhreja, Anindya Sundar Das, Souhitya Chakraborty, Sriparna Saha|Indian Institute of Technology Patna, Patna, India|The scarcity of Mental Health Professionals (MHPs) available to assist patients underlines the need for developing automated systems to help MHPs combat the grievous mental illness called Major Depressive Disorder. In this paper, we develop a Virtual Assistant (VA) that serves as a first point of contact for users who are depressed or disheartened. In support based conversations, two primary components have been identified to produce positive outcomes,empathy andmotivation. While empathy necessitates acknowledging the feelings of the users with a desire to help, imparting hope and motivation uplifts the spirit of support seekers in distress. A combination of these aspects will ensure generalized positive outcome and beneficial alliance in mental health support. The VA, thus, should be capable of generating empathetic and motivational responses, continuously demonstrating positive sentiment by the VA. The end-to-end system employs two mechanisms in a pipe-lined manner : (i)Motivational Response Generator (MRG) : a sentiment driven Reinforcement Learning (RL) based motivational response generator; and (ii)Empathetic Rewriting Framework (ERF) : a transformer based model that rewrites the response from MRG to induce empathy. Experimental results indicate that our proposed VA outperforms several of its counterparts. To the best of our knowledge, this is the first work that seeks to incorporate these aspects together in an end-to-end system.|可用于帮助患者的精神卫生专业人员(MHPs)的稀缺性突出表明，需要开发自动化系统来帮助 MHPs 对抗称为重性抑郁障碍的严重精神疾病。在本文中，我们开发了一个虚拟助手(VA) ，作为用户的第一联系点谁是沮丧或心灰意冷。在以支持为基础的对话中，两个主要的组成部分已经被确定来产生积极的结果，移情和动机。虽然移情需要承认用户的感受与愿望帮助，传递希望和动机提升精神的支持寻求者在困境中。这些方面的结合将确保普遍积极的结果和有益的联盟在心理健康支持。因此，退伍军人事务部应该能够产生同理心和激励性的反应，不断表现出 VA. 的积极情绪。端到端系统以管道式的方式使用两种机制: (i)动机反应生成器(MRG) : 基于情感驱动的强化学习(RL)的动机反应生成器; (ii)移情重写框架(ERF) : 基于转换器的模型，重写来自 MRG 的反应以诱导移情。实验结果表明，我们提出的 VA 性能优于其他几个竞争对手。据我们所知，这是第一个试图将这些方面整合到一个端到端系统中的工作。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Motivational+and+Empathetic+Response+Generation+in+Online+Mental+Health+Support)|2|
|[Multimodal Entity Linking with Gated Hierarchical Fusion and Contrastive Training](https://doi.org/10.1145/3477495.3531867)|Peng Wang, Jiangheng Wu, Xiaohang Chen|Southeast University, Nanjing, China|Previous entity linking methods in knowledge graphs (KGs) mostly link the textual mentions to corresponding entities. However, they have deficiencies in processing numerous multimodal data, when the text is too short to provide enough context. Consequently, we conceive the idea of introducing valuable information of other modalities, and propose a novel multimodal entity linking method with gated hierarchical multimodal fusion and contrastive training (GHMFC). Firstly, in order to discover the fine-grained inter-modal correlations, GHMFC extracts the hierarchical features of text and visual co-attention through the multi-modal co-attention mechanism: textual-guided visual attention and visual-guided textual attention. The former attention obtains weighted visual features under the guidance of textual information. In contrast, the latter attention produces weighted textual features under the guidance of visual information. Afterwards, gated fusion is used to evaluate the importance of hierarchical features of different modalities and integrate them into the final multimodal representations of mentions. Subsequently, contrastive training with two types of contrastive losses is designed to learn more generic multimodal features and reduce noise. Finally, the linking entities are selected by calculating the cosine similarity between representations of mentions and entities in KGs. To evaluate the proposed method, this paper releases two new open multimodal entity linking datasets: WikiMEL and Richpedia-MEL. Experimental results demonstrate that GHMFC can learn meaningful multimodal representation and significantly outperforms most of the baseline methods.|以往知识图中的实体链接方法大多将文本提及链接到相应的实体。然而，当文本太短而无法提供足够的上下文时，它们在处理大量多模式数据方面存在缺陷。因此，我们提出了引入其他模式的有价值信息的思想，并提出了一种新的多模式实体连接方法与门限层次多模式融合和对比训练(GHMFC)。首先，为了发现细粒度的多模态关联，GHMFC 通过文本引导的视觉注意和视觉引导的文本注意这两种多模态共注意机制提取文本和视觉共注意的层次特征。前者在文本信息的引导下获得加权视觉特征。相反，后者在视觉信息的引导下产生加权的文本特征。然后，门限融合被用来评估不同模式的等级特征的重要性，并将它们整合到提及的最终多模式表示中。随后，设计了两种对比损失的对比训练，以学习更一般的多模态特征和降低噪声。最后，通过计算幼儿园中提及和实体的表示之间的余弦距离来选择链接实体。为了评估提出的方法，本文发布了两个新的开放的多模式实体链接数据集: WikiMEL 和 Richpedia-MEL。实验结果表明，GHMFC 能够学习有意义的多模态表示，其性能明显优于大多数基线方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multimodal+Entity+Linking+with+Gated+Hierarchical+Fusion+and+Contrastive+Training)|2|
|[Tag-assisted Multimodal Sentiment Analysis under Uncertain Missing Modalities](https://doi.org/10.1145/3477495.3532064)|Jiandian Zeng, Tianyi Liu, Jiantao Zhou|University of Macau, Macau, China; Shanghai Jiao Tong University, Shanghai, China|Multimodal sentiment analysis has been studied under the assumption that all modalities are available. However, such a strong assumption does not always hold in practice, and most of multimodal fusion models may fail when partial modalities are missing. Several works have addressed the missing modality problem; but most of them only considered the single modality missing case, and ignored the practically more general cases of multiple modalities missing. To this end, in this paper, we propose a Tag-Assisted Transformer Encoder (TATE) network to handle the problem of missing uncertain modalities. Specifically, we design a tag encoding module to cover both the single modality and multiple modalities missing cases, so as to guide the network's attention to those missing modalities. Besides, we adopt a new space projection pattern to align common vectors. Then, a Transformer encoder-decoder network is utilized to learn the missing modality features. At last, the outputs of the Transformer encoder are used for the final sentiment classification. Extensive experiments are conducted on CMU-MOSI and IEMOCAP datasets, showing that our method can achieve significant improvements compared with several baselines.|多模态情绪分析已研究的假设下，所有的模式是可用的。然而，这样一个强大的假设并不总是适用于实践，并且大多数多模态融合模型可能会失败时，部分模式缺失。有几部著作论述了缺失模式问题; 但大多数著作只考虑了单一模式缺失的情况，而忽略了实际上更为普遍的多模式缺失的情况。为此，本文提出了一种标签辅助变压器编码器(TATE)网络来处理丢失不确定模式的问题。具体来说，我们设计了一个标签编码模块来同时涵盖单个模态和多个模态的缺失情况，从而引导网络对这些缺失模态的注意。此外，我们还采用了一种新的空间投影模式来对齐公共向量。然后，利用变压器编译码网络来学习缺失的模态特征。最后，利用变压器编码器的输出进行最终的情感分类。在 CMU-MOSI 和 IEMOCAP 数据集上进行了大量的实验，结果表明，与几个基线相比，本文提出的方法可以取得显著的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tag-assisted+Multimodal+Sentiment+Analysis+under+Uncertain+Missing+Modalities)|2|
|[Enhancing Zero-Shot Stance Detection via Targeted Background Knowledge](https://doi.org/10.1145/3477495.3531807)|Qinglin Zhu, Bin Liang, Jingyi Sun, Jiachen Du, Lanjun Zhou, Ruifeng Xu|None, Shenzhen, China; Harbin Institute of Technology, Shenzhen, Shenzhen, China; Harbin Institute of Technology, Shenzhen & Peng Cheng Laboratory, Shenzhen, China|Stance detection aims to identify the stance of the text towards a target. Different from conventional stance detection, Zero-Shot Stance Detection (ZSSD) needs to predict the stances of the unseen targets during the inference stage. For human beings, we generally tend to reason the stance of a new target by linking it with the related knowledge learned from the known ones. Therefore, in this paper, to better generalize the target-related stance features learned from the known targets to the unseen ones, we incorporate the targeted background knowledge from Wikipedia into the model. The background knowledge can be considered as a bridge for connecting the meanings between known targets and the unseen ones, which enables the generalization and reasoning ability of the model to be improved in dealing with ZSSD. Extensive experimental results demonstrate that our model outperforms the state-of-the-art methods on the ZSSD task.|姿态检测的目的是识别文本对目标的姿态。与传统的姿态检测不同，零拍姿态检测(ZSSD)需要在推理阶段预测未知目标的姿态。对于人类来说，我们通常倾向于将新目标的立场与从已知目标中学到的相关知识联系起来进行推理。因此，为了更好地将从已知目标中学到的与目标相关的姿态特征推广到未知目标中，本文将维基百科中的目标背景知识引入到模型中。背景知识可以作为连接已知目标和未知目标意义的桥梁，提高模型在处理 ZSSD 时的推理能力。大量的实验结果表明，我们的模型在 ZSSD 任务上优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Zero-Shot+Stance+Detection+via+Targeted+Background+Knowledge)|2|
|[Relation-Guided Few-Shot Relational Triple Extraction](https://doi.org/10.1145/3477495.3531831)|Xin Cong, Jiawei Sheng, Shiyao Cui, Bowen Yu, Tingwen Liu, Bin Wang|Institute of Information Engineering, Chinese Academy of Sciences & University of Chinese Academy of Sciences, Beijing, China; Xiaomi Inc., Beijing, China|In few-shot relational triple extraction (FS-RTE), one seeks to extract relational triples from plain texts by utilizing only few annotated samples. Recent work first extracts all entities and then classifies their relations. Such an entity-then-relation paradigm ignores the entity discrepancy between relations. To address it, we propose a novel task decomposition strategy, Relation-then-Entity, for FS-RTE. It first detects relations occurred in a sentence and then extracts the corresponding head/tail entities of the detected relations. To instantiate this strategy, we further propose a model, RelATE, which builds a dual-level attention to aggregate relation-relevant information to detect the relation occurrence and utilizes the annotated samples of the detected relations to extract the corresponding head/tail entities. Experimental results show that our model outperforms previous work by an absolute gain (18.98%, 28.85% in F1 in two few-shot settings).|在少镜头关系三元组提取(few-shot Relations Triples，FS-RTE)中，人们试图通过只使用少量带注释的样本从纯文本中提取关系三元组。最近的工作首先提取所有的实体，然后分类它们的关系。这种先实体后关系的范式忽略了关系之间的实体差异。为了解决这个问题，我们提出了一个新的任务分解策略，即关系然后实体，用于 FS-RTE。它首先检测句子中出现的关系，然后提取被检测关系的相应头尾实体。为了实例化这一策略，我们进一步提出了一个名为 RelATE 的模型，该模型构建了对聚合关系相关信息的双层关注来检测关系的发生，并利用检测到的关系的注释样本来提取相应的头/尾实体。实验结果表明，我们的模型优于以前的工作的绝对增益(18.98% ，在 F1在两个少拍设置28.85%)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Relation-Guided+Few-Shot+Relational+Triple+Extraction)|2|
|[Summarizing Legal Regulatory Documents using Transformers](https://doi.org/10.1145/3477495.3531872)|Svea Klaus, Ria Van Hecke, Kaweh Djafari Naini, Ismail Sengor Altingovde, Juan BernabéMoreno, Enrique HerreraViedma|University of Granada & E.ON Digital Technology GmbH, Granada, Spain; E.ON Digital Technology GmbH, Hannover, Germany; University of Granada, Granada, Spain; Middle East Technical University, Ankara, Turkey|Companies invest a substantial amount of time and resources in ensuring the compliance to the existing regulations or in the form of fines when compliance cannot be proven in auditing procedures. The topic is not only relevant, but also highly complex, given the frequency of changes and amendments, the complexity of the cases and the difficulty of the juristic language. This paper aims at applying advanced extractive summarization to democratize the understanding of regulations, so that non-jurists can decide which regulations deserve further follow-up. To achieve that, we first create a corpus named EUR-LexSum EUR-LexSum containing 4595 curated European regulatory documents and their corresponding summaries. We then fine-tune transformer-based models which, applied to this corpus, yield a superior performance (in terms of ROUGE metrics) compared to a traditional extractive summarization baseline. Our experiments reveal that even with limited amounts of data such transformer-based models are effective in the field of legal document summarization.|公司投入大量时间和资源，确保遵守现行条例，或在审计程序无法证明遵守情况时以罚款的形式进行。鉴于变更和修正的频率、案件的复杂性和法律语言的困难，这一专题不仅相关，而且非常复杂。本文旨在运用先进的抽象概括方法，使法规的理解民主化，使非法学家能够决定哪些法规值得进一步跟进。为了实现这一目标，我们首先创建了一个名为 EUR-LexSum 的语料库，其中包含4595份精选的欧洲监管文件及其相应的摘要。然后，我们对基于转换器的模型进行微调，这些模型应用于这个语料库，与传统的提取摘要基线相比，产生了更好的性能(在 ROUGE 指标方面)。我们的实验表明，即使在数据量有限的情况下，这种基于转换器的模型在法律文档摘要领域仍然是有效的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Summarizing+Legal+Regulatory+Documents+using+Transformers)|2|
|[An Inspection of the Reproducibility and Replicability of TCT-ColBERT](https://doi.org/10.1145/3477495.3531721)|Xiao Wang, Sean MacAvaney, Craig Macdonald, Iadh Ounis|University of Glasgow, Glasgow, United Kingdom|Dense retrieval approaches are of increasing interest because they can better capture contextualised similarity compared to sparse retrieval models such as BM25. Among the most prominent of these approaches is TCT-ColBERT, which trains a light-weight "student'' model from a more expensive "teacher'' model. In this work, we take a closer look into TCT-ColBERT concerning its reproducibility and replicability. To structure our study, we propose a three-stage perspective on reproducing the training, inference, and evaluation of model-focused papers, each using artefacts produced from different stages in the pipeline. We find that --- perhaps as expected --- precise reproduction is more challenging when the complete training process is conducted, rather than just inference from a released trained model. Each stage provides the opportunity to perform replication and ablation experiments. We are able to replicate (i.e., produce an effective independent implementation) for model inference and dense indexing/retrieval, but are unable to replicate the training process. We conduct several ablations to cover gaps in the original paper, and make the following observations: (1) the model can function as an inexpensive re-ranker, establishing a new Pareto-optimal result; (2) the index size can be reduced by using lower-precision floating point values, but only if ties in scores are handled appropriately; (3) training needs to be conducted for the entire suggested duration to achieve optimal performance; and (4) student initialisation from the teacher is not necessary.|密集检索方法越来越受到人们的关注，因为与稀疏检索模型(如 BM25)相比，密集检索方法能够更好地捕获上下文相似性。其中最突出的方法是 TCT-ColBERT，它从更昂贵的“教师”模型中培训轻量级的“学生”模型。在这项工作中，我们对 TCT-ColBERT 的重复性和可复制性进行了更深入的研究。为了构建我们的研究，我们提出了一个三阶段的视角来重现模型关注论文的训练、推理和评估，每一个阶段都使用流水线中不同阶段产生的人工制品。我们发现——也许正如预期的那样——当完整的训练过程进行时，精确的复制比仅仅从已发布的训练模型中推断更具挑战性。每个阶段都提供了进行复制和消融实验的机会。我们能够复制(即，生成一个有效的独立实现)模型推理和密集索引/检索，但无法复制培训过程。我们进行了几次消融，以弥补原始论文中的空白，并做出以下观察: (1)该模型可以作为一个廉价的重新排序，建立一个新的帕累托最优结果; (2)指数大小可以通过使用较低精度的浮点数值减少，但只有当分数的关系处理得当时; (3)培训需要进行整个建议的持续时间，以达到最佳的表现; (4)学生初始化是不必要的教师。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Inspection+of+the+Reproducibility+and+Replicability+of+TCT-ColBERT)|2|
|[MET-Meme: A Multimodal Meme Dataset Rich in Metaphors](https://doi.org/10.1145/3477495.3532019)|Bo Xu, Tingting Li, Junzhe Zheng, Mehdi Naseriparsa, Zhehuan Zhao, Hongfei Lin, Feng Xia|Dalian University of Technology, Dalian, China; Federation University Australia, Ballarat, VIC, Australia|Memes have become the popular means of communication for Internet users worldwide. Understanding the Internet meme is one of the most tricky challenges in natural language processing (NLP) tasks due to its convenient non-standard writing and network vocabulary. Recently, many linguists suggested that memes contain rich metaphorical information. However, the existing researches ignore this key feature. Therefore, to incorporate informative metaphors into the meme analysis, we introduce a novel multimodal meme dataset called MET-Meme, which is rich in metaphorical features. It contains 10045 text-image pairs, with manual annotations of the metaphor occurrence, sentiment categories, intentions, and offensiveness degree. Moreover, we propose a range of strong baselines to demonstrate the importance of combining metaphorical features for meme sentiment analysis and semantic understanding tasks, respectively. MET-Meme, and its code are released publicly for research in \urlhttps://github.com/liaolianfoka/MET-Meme-A-Multi-modal-Meme-Dataset-Rich-in-Metaphors.|文化基因已经成为世界范围内互联网用户流行的交流方式。自然语言处理(NLP)任务中最棘手的挑战之一就是理解网络文化基因，因为它具有方便的非标准写作和网络词汇。近年来，许多语言学家认为模因包含丰富的隐喻信息。然而，现有的研究忽视了这一关键特征。因此，为了将信息隐喻引入到模因分析中，我们引入了一个新的多模式模因数据集 MET-Meme，它具有丰富的隐喻特征。它包含10045个文本-图像对，手动注释的隐喻出现，情感类别，意图，和冒犯程度。此外，我们提出了一系列强基线来证明隐喻特征相结合对于模因情感分析和语义理解任务的重要性。MET-meme 及其代码在 urlhttps:// github.com/liaolianfoka/MET-Meme-a-multi-modal-meme-dataset-rich-in-metaphors 上公开发布以供研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MET-Meme:+A+Multimodal+Meme+Dataset+Rich+in+Metaphors)|2|
|[Fostering Coopetition While Plugging Leaks: The Design and Implementation of the MS MARCO Leaderboards](https://doi.org/10.1145/3477495.3531725)|Jimmy Lin, Daniel Campos, Nick Craswell, Bhaskar Mitra, Emine Yilmaz||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fostering+Coopetition+While+Plugging+Leaks:+The+Design+and+Implementation+of+the+MS+MARCO+Leaderboards)|2|
|[Too Many Relevants: Whither Cranfield Test Collections?](https://doi.org/10.1145/3477495.3531728)|Ellen M. Voorhees, Nick Craswell, Jimmy Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Too+Many+Relevants:+Whither+Cranfield+Test+Collections?)|2|
|[Document Expansion Baselines and Learned Sparse Lexical Representations for MS MARCO V1 and V2](https://doi.org/10.1145/3477495.3531749)|Xueguang Ma, Ronak Pradeep, Rodrigo Frassetto Nogueira, Jimmy Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Document+Expansion+Baselines+and+Learned+Sparse+Lexical+Representations+for+MS+MARCO+V1+and+V2)|2|
|[ClueWeb22: 10 Billion Web Documents with Rich Information](https://doi.org/10.1145/3477495.3536321)|Arnold Overwijk, Chenyan Xiong, Jamie Callan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ClueWeb22:+10+Billion+Web+Documents+with+Rich+Information)|2|
|[User-Aware Multi-Interest Learning for Candidate Matching in Recommenders](https://doi.org/10.1145/3477495.3532073)|Zheng Chai, Zhihong Chen, Chenliang Li, Rong Xiao, Houyi Li, Jiawei Wu, Jingxu Chen, Haihong Tang|Wuhan University, Wuhan, China; Alibaba Group, Hangzhou, China; Zhejiang University, Hangzhou, China|Recommender systems have become a fundamental service in most E-Commerce platforms, in which the matching stage aims to retrieve potentially relevant candidate items to users for further ranking. Recently, some efforts on extracting multi-interests from user's historical behaviors have demonstrated superior performance. However, the historical behaviors are not noise-free due to the possible misclicks or disturbances. Existing works mainly overlook the fact that the interests of a user are not only reflected by the historical behaviors, but also inherently regulated by the profile information. Hence, we are interested in exploiting the benefit of user profile in multi-interest learning to enhance candidate matching performance. To this end, a user-aware multi-interest learning framework (named UMI) is proposed in this paper to exploit both user profile and behavior information for candidate matching. Specifically, UMI consists of two main components: dual-attention routing and interest refinement. In the dual-attention routing, we firstly introduce a user-guided attention network to identify the important historical items with respect to the user profile. Then, the resultant importance weights are leveraged via the dual-attentive capsule network to extract the user's multi-interests. Afterwards, the extracted interests are utilized to highlight the corresponding user profile features for interest refinement, such that different user profiles can be incorporated into interest learning for diverse user preference understanding. Besides, to improve the model's discriminative capacity, we further devise a harder-negatives strategy to support model optimization. Extensive experiments show that UMI significantly outperforms state-of-the-art multi-interest modeling alternatives. Currently, UMI has been successfully deployed at Taobao App in Alibaba, serving hundreds of millions of users.|推荐系统已经成为大多数电子商务平台的基本服务，其中匹配阶段的目的是检索潜在的相关候选项目，以便用户进一步排名。最近，一些从用户历史行为中提取多重利益的尝试已经显示出了卓越的性能。然而，由于可能的错误或干扰，历史行为并不是无噪声的。现有的研究工作主要忽视了用户的利益不仅反映在历史行为上，而且内在地受到个人资料信息的调节。因此，我们有兴趣在多兴趣学习中利用用户资料的好处来提高候选人匹配性能。为此，本文提出了一种基于用户感知的多兴趣学习框架(UMI) ，该框架利用用户信息和行为信息进行候选人匹配。具体来说，UMI 包括两个主要组成部分: 双注意路由和兴趣细化。在双注意路由中，我们首先引入一个用户引导的注意网络来识别与用户资料相关的重要历史项目。然后，通过双注意胶囊网络利用得到的重要性权重来提取用户的多重兴趣。然后利用提取出的兴趣特征突出相应的用户兴趣特征进行兴趣细化，从而将不同的用户兴趣特征融入到兴趣学习中以获得不同的用户偏好理解。此外，为了提高模型的判别能力，我们进一步设计了一个较难否定的策略来支持模型优化。大量的实验表明，UMI 明显优于最先进的多重兴趣建模方案。目前，用户界面已经在阿里巴巴的淘宝应用上成功部署，为数亿用户提供服务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=User-Aware+Multi-Interest+Learning+for+Candidate+Matching+in+Recommenders)|1|
|[ESCM2: Entire Space Counterfactual Multi-Task Model for Post-Click Conversion Rate Estimation](https://doi.org/10.1145/3477495.3531972)|Hao Wang, TaiWei Chang, Tianqiao Liu, Jianmin Huang, Zhichao Chen, Chao Yu, Ruopeng Li, Wei Chu|Ant Group, Hangzhou, China|Accurate estimation of post-click conversion rate is critical for building recommender systems, which has long been confronted with sample selection bias and data sparsity issues. Methods in the Entire Space Multi-task Model (ESMM) family leverage the sequential pattern of user actions, \ie $impression\rightarrow click \rightarrow conversion$ to address data sparsity issue. However, they still fail to ensure the unbiasedness of CVR estimates. In this paper, we theoretically demonstrate that ESMM suffers from the following two problems: (1) Inherent Estimation Bias (IEB) for CVR estimation, where the CVR estimate is inherently higher than the ground truth; (2) Potential Independence Priority (PIP) for CTCVR estimation, where ESMM might overlook the causality from click to conversion. To this end, we devise a principled approach named Entire Space Counterfactual Multi-task Modelling (ESCM$^2$), which employs a counterfactual risk miminizer as a regularizer in ESMM to address both IEB and PIP issues simultaneously. Extensive experiments on offline datasets and online environments demonstrate that our proposed ESCM$^2$ can largely mitigate the inherent IEB and PIP issues and achieve better performance than baseline models.|准确估计点击后的转换率是建立推荐系统的关键，长期以来推荐系统一直面临样本选择偏差和数据稀疏问题。整个空间多任务模型(ESMM)家族中的方法利用了用户操作的顺序模式，例如: $pressionright-tarrow 单击 right-tarrow 转换 $来解决数据稀疏问题。然而，他们仍然不能确保 CVR 估计的公正性。在本文中，我们从理论上证明了 ESMM 存在以下两个问题: (1) CVR 估计的内在估计偏差(IEB) ，其中 CVR 估计固有地高于地面真值; (2) CTCVR 估计的潜在独立优先级(PIP) ，其中 ESMM 可能忽略从点击到转换的因果关系。为此，我们设计了一种名为“整个空间反事实多任务建模”(ESCM $^ 2 $)的原则性方法，该方法使用反事实风险模拟器作为 ESMM 中的规则化器，同时解决 IEB 和 PIP 问题。在离线数据集和在线环境上的大量实验表明，我们提出的 ESCM $^ 2 $可以在很大程度上缓解内在的 IEB 和 PIP 问题，并取得比基线模型更好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ESCM2:+Entire+Space+Counterfactual+Multi-Task+Model+for+Post-Click+Conversion+Rate+Estimation)|1|
|[Single-shot Embedding Dimension Search in Recommender System](https://doi.org/10.1145/3477495.3532060)|Liang Qu, Yonghong Ye, Ningzhi Tang, Lixin Zhang, Yuhui Shi, Hongzhi Yin|The University of Queensland, Brisbane, QLD, Australia; Southern University of Science and Technology, Shenzhen, China; WeChat, Tencent, Shenzhen, China|As a crucial component of most modern deep recommender systems, feature embedding maps high-dimensional sparse user/item features into low-dimensional dense embeddings. However, these embeddings are usually assigned a unified dimension, which suffers from the following issues: (1) high memory usage and computation cost. (2) sub-optimal performance due to inferior dimension assignments. In order to alleviate the above issues, some works focus on automated embedding dimension search by formulating it as hyper-parameter optimization or embedding pruning problems. However, they either require well-designed search space for hyperparameters or need time-consuming optimization procedures. In this paper, we propose a Single-Shot Embedding Dimension Search method, called SSEDS, which can efficiently assign dimensions for each feature field via a single-shot embedding pruning operation while maintaining the recommendation accuracy of the model. Specifically, it introduces a criterion for identifying the importance of each embedding dimension for each feature field. As a result, SSEDS could automatically obtain mixed-dimensional embeddings by explicitly reducing redundant embedding dimensions based on the corresponding dimension importance ranking and the predefined parameter budget. Furthermore, the proposed SSEDS is model-agnostic, meaning that it could be integrated into different base recommendation models. The extensive offline experiments are conducted on two widely used public datasets for CTR (Click Through Rate) prediction task, and the results demonstrate that SSEDS can still achieve strong recommendation performance even if it has reduced 90% parameters. Moreover, SSEDS has also been deployed on the WeChat Subscription platform for practical recommendation services. The 7-day online A/B test results show that SSEDS can significantly improve the performance of the online recommendation model while reducing resource consumption.|作为现代深度推荐系统的重要组成部分，特征嵌入将高维稀疏用户/项目特征映射为低维密集嵌入。然而，这些嵌入通常被分配一个统一的维度，这受到以下问题: (1)高内存使用和计算成本。(2)由于尺寸分配不合理而导致性能次优。为了解决上述问题，一些工作将嵌入维搜索问题转化为超参数优化问题或嵌入剪枝问题。然而，它们要么需要设计良好的超参数搜索空间，要么需要耗时的优化过程。本文提出了一种单镜头嵌入维度搜索方法 SSEDS，该方法通过单镜头嵌入剪枝操作，可以有效地为每个特征域分配维度，同时保持模型的推荐精度。具体地说，它引入了一个标准来识别每个特征字段的每个嵌入维的重要性。因此，SSEDS 可以根据相应的维重要性排序和预定义的参数预算，通过显式地减少冗余嵌入维数来自动获得混合维嵌入。此外，提出的 SSEDS 是模型无关的，这意味着它可以集成到不同的基本推荐模型中。在两个广泛使用的公共数据集上进行了广泛的离线实验，结果表明，即使 SSEDS 减少了90% 的参数，仍然可以获得很好的推荐性能。此外，SSEDS 还被部署在微信订阅平台上，提供实用的推荐服务。为期7天的在线 A/B 测试结果表明，SSEDS 在降低资源消耗的同时，可以显著提高在线推荐模型的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Single-shot+Embedding+Dimension+Search+in+Recommender+System)|1|
|[Zero-shot Query Contextualization for Conversational Search](https://doi.org/10.1145/3477495.3531769)|Antonios Minas Krasakis, Andrew Yates, Evangelos Kanoulas|University of Amsterdam, Amsterdam, Netherlands|Current conversational passage retrieval systems cast conversational search into ad-hoc search by using an intermediate query resolution step that places the user's question in context of the conversation. While the proposed methods have proven effective, they still assume the availability of large-scale question resolution and conversational search datasets. To waive the dependency on the availability of such data, we adapt a pre-trained token-level dense retriever on ad-hoc search data to perform conversational search with no additional fine-tuning. The proposed method allows to contextualize the user question within the conversation history, but restrict the matching only between question and potential answer. Our experiments demonstrate the effectiveness of the proposed approach. We also perform an analysis that provides insights of how contextualization works in the latent space, in essence introducing a bias towards salient terms from the conversation.|当前的会话文本检索系统通过使用一个中间查询解析步骤，将用户的问题置于会话上下文中，从而将会话搜索转换为特定搜索。虽然提出的方法已被证明是有效的，但它们仍然假设大规模的问题解决和会话搜索数据集的可用性。为了摆脱对这些数据可用性的依赖，我们在自组织搜索数据上采用了一个预先训练的令牌级密集检索器来执行会话搜索，而不需要进行额外的微调。该方法允许在会话历史中上下文化用户问题，但只限制问题和潜在答案之间的匹配。实验证明了该方法的有效性。我们还进行了一个分析，提供了如何在潜在空间的情境化工作的见解，在本质上引入了一个从会话突出术语的偏见。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Zero-shot+Query+Contextualization+for+Conversational+Search)|1|
|[DisenCTR: Dynamic Graph-based Disentangled Representation for Click-Through Rate Prediction](https://doi.org/10.1145/3477495.3531851)|Yifan Wang, Yifang Qin, Fang Sun, Bo Zhang, Xuyang Hou, Ke Hu, Jia Cheng, Jun Lei, Ming Zhang|Meituan, Beijing, China; Peking University, Beijing, China|Click-through rate (CTR) prediction plays a critical role in recommender systems and other applications. Recently, modeling user behavior sequences attracts much attention and brings great improvements in the CTR field. Many existing works utilize attention mechanism or recurrent neural networks to exploit user interest from the sequence, but fail to recognize the simple truth that a user's real-time interests are inherently diverse and fluid. In this paper, we propose DisenCTR, a novel dynamic graph-based disentangled representation framework for CTR prediction. The key novelty of our method compared with existing approaches is to model evolving diverse interests of users. Specifically, we construct a time-evolving user-item interaction graph induced by historical interactions. And based on the rich dynamics supplied by the graph, we propose a disentangled graph representation module to extract diverse user interests. We further exploit the fluidity of user interests and model the temporal effect of historical behaviors using Mixture of Hawkes Process. Extensive experiments on three real-world datasets demonstrate the superior performance of our method comparing to state-of-the-art approaches.|在推荐系统和其他应用程序中，点进率(ctrl)预测起着至关重要的作用。近年来，用户行为序列建模引起了人们的广泛关注，并在 CTR 领域得到了很大的发展。现有的许多作品利用注意机制或反复神经网络从序列中挖掘用户兴趣，但未能认识到用户的实时兴趣具有内在的多样性和流动性这一简单事实。本文提出了一种新的基于动态图的分离表示框架 DisenCTR，用于 CTR 预测。与现有方法相比，我们的方法的关键新颖之处在于对用户的不同兴趣进行建模。具体来说，我们构造了一个由历史交互作用引起的时间演化的用户-项目交互图。基于图所提供的丰富的动态性，我们提出了一个分离的图表示模块来提取不同的用户兴趣。进一步利用用户兴趣的流动性，利用霍克斯过程混合模型对历史行为的时间效应进行建模。在三个真实世界数据集上的大量实验表明，与最先进的方法相比，我们的方法具有更好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DisenCTR:+Dynamic+Graph-based+Disentangled+Representation+for+Click-Through+Rate+Prediction)|1|
|[BERT-based Dense Intra-ranking and Contextualized Late Interaction via Multi-task Learning for Long Document Retrieval](https://doi.org/10.1145/3477495.3531856)|Minghan Li, Éric Gaussier|Univ. Grenoble Alpes, CNRS, LIG, Grenoble, France|Combining query tokens and document tokens and inputting them to pre-trained transformer models like BERT, an approach known as interaction-based, has shown state-of-the-art effectiveness for information retrieval. However, the computational complexity of this approach is high due to the online self-attention computation. In contrast, dense retrieval methods in representation-based approaches are known to be efficient, however less effective. A tradeoff between the two is reached with late interaction methods like ColBERT, which attempt to benefit from both approaches: contextualized token embeddings can be pre-calculated over BERT for fine-grained effective interaction while preserving efficiency. However, despite its success in passage retrieval, it's not straightforward to use this approach for long document retrieval. In this paper, we propose a cascaded late interaction approach using a single model for long document retrieval. Fast intra-ranking by dot product is used to select relevant passages, then fine-grained interaction of pre-stored token embeddings is used to generate passage scores which are aggregated to the final document score. Multi-task learning is used to train a BERT model to optimize both a dot product and a fine-grained interaction loss functions. Our experiments reveal that the proposed approach obtains near state-of-the-art level effectiveness while being efficient on such collections as TREC 2019.|将查询令牌和文档令牌结合起来，并将它们输入到像 BERT 这样的经过预先训练的转换器模型中，这种方法被称为基于交互的方法，已经显示出对于信息检索的最先进的有效性。然而，由于在线自注意计算，这种方法的计算复杂度很高。相比之下，基于表示的方法中的密集检索方法已知是有效的，但是效率较低。这两种方法之间的折衷是通过 ColBERT 这样的后期交互方法实现的，它们试图从两种方法中受益: 上下文化的令牌嵌入可以在 BERT 上预先计算出细粒度的有效交互，同时保持效率。然而，尽管这种方法在文章检索方面取得了成功，但是要长时间地使用这种方法并不是一件简单的文献检索。在这篇文章中，我们提出了一个级联的晚期交互方法，使用单一模型的长期文献检索。首先利用点乘快速内排序来选择相关段落，然后利用预存储令牌嵌入的细粒度交互来生成段落分数，并将这些分数聚合为最终的文档分数。多任务学习用于训练 BERT 模型，以优化网点积和细粒度交互损失函数。我们的实验表明，所提出的方法获得接近最先进水平的效率，同时对 TREC 2019这样的集合是有效的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BERT-based+Dense+Intra-ranking+and+Contextualized+Late+Interaction+via+Multi-task+Learning+for+Long+Document+Retrieval)|1|
|[RESETBERT4Rec: A Pre-training Model Integrating Time And User Historical Behavior for Sequential Recommendation](https://doi.org/10.1145/3477495.3532054)|Qihang Zhao|University of Science and Technology of China & JD AI Research, Hefei & Shanghai, China|Sequential recommendation methods are very important in modern recommender systems because they can well capture users' dynamic interests from their interaction history, and make accurate recommendations for users, thereby helping enterprises succeed in business. However, despite the great success of existing sequential recommendation-based methods, they focus too much on item-level modeling of users' click history and lack information about the user's entire click history (such as click order, click time, etc.). To tackle this problem, inspired by recent advances in pre-training techniques in the field of natural language processing, we build a new pre-training task based on the original BERT pre-training framework and incorporate temporal information. Specifically, we propose a new model called the RE arrange S equence prE -training and T ime embedding model via BERT for sequential R ecommendation (RESETBERT4Rec ) \footnoteThis work was completed during JD internship., it further captures the information of the user's whole click history by adding a rearrange sequence prediction task to the original BERT pre-training framework, while it integrates different views of time information. Comprehensive experiments on two public datasets as well as one e-commerce dataset demonstrate that RESETBERT4Rec achieves state-of-the-art performance over existing baselines.|序贯推荐方法在现代推荐系统中具有重要意义，因为它能够很好地从用户的交互历史中捕捉用户的动态兴趣，为用户提供准确的推荐，从而帮助企业获得成功。然而，尽管现有的基于顺序推荐的方法取得了巨大的成功，但它们过于关注用户点击历史的项目级建模，缺乏关于用户整个点击历史的信息(如点击顺序、点击时间等)。为了解决这一问题，受自然语言处理领域预训练技术的最新进展的启发，我们在原有的 BERT 预训练框架的基础上，结合时间信息构建了一个新的预训练任务。具体来说，我们提出了一个新的模型，称为 RE 安排 S 序列预训练和 T 时间嵌入模型，通过 BERT 进行顺序 R 推荐(RESETBERT4Rec)注释这项工作是在 JD 实习期间完成的，它通过在原有的 BERT 预训练框架中增加一个重排序列预测任务，进一步获取用户的整个点击历史信息，同时整合不同的时间信息视图。对两个公共数据集和一个电子商务数据集的综合实验表明，RESETBERT4Rec 在现有的基线上取得了最先进的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RESETBERT4Rec:+A+Pre-training+Model+Integrating+Time+And+User+Historical+Behavior+for+Sequential+Recommendation)|1|
|[Clustering based Behavior Sampling with Long Sequential Data for CTR Prediction](https://doi.org/10.1145/3477495.3531829)|Yuren Zhang, Enhong Chen, Binbin Jin, Hao Wang, Min Hou, Wei Huang, Runlong Yu|University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence, Hefei, Anhui, China; Huawei Cloud Computing Technologies Co., Ltd., Hangzhou, Zhejiang, China|Click-through rate (CTR) prediction is fundamental in many industrial applications, such as online advertising and recommender systems. With the development of the online platforms, the sequential user behaviors grow rapidly, bringing us great opportunity to better understand user preferences.However, it is extremely challenging for existing sequential models to effectively utilize the entire behavior history of each user. First, there is a lot of noise in such long histories, which can seriously hurt the prediction performance. Second, feeding the long behavior sequence directly results in infeasible inference time and storage cost. In order to tackle these challenges, in this paper we propose a novel framework, which we name as User Behavior Clustering Sampling (UBCS). In UBCS, short sub-sequences will be obtained from the whole user history sequence with two cascaded modules: (i) Behavior Sampling module samples short sequences related to candidate items using a novel sampling method which takes relevance and temporal information into consideration; (ii) Item Clustering module clusters items into a small number of cluster centroids, mitigating the impact of noise and improving efficiency. Then, the sampled short sub-sequences will be fed into the CTR prediction module for efficient prediction. Moreover, we conduct a self-supervised consistency pre-training task to extract user persona preference and optimize the sampling module effectively. Experiments on real-world datasets demonstrate the superiority and efficiency of our proposed framework.|在许多工业应用中，如在线广告和推荐系统中，点进率(ctrl)预测是基础。随着在线平台的发展，连续用户行为迅速增长，为我们更好地理解用户偏好带来了巨大的机遇。然而，对于现有的顺序模型来说，有效地利用每个用户的整个行为历史是极具挑战性的。首先，在如此长的历史中存在大量的噪声，这会严重影响预测性能。其次，长行为序列直接导致不可行的推理时间和存储成本。为了应对这些挑战，本文提出了一个新的框架，我们称之为用户行为聚类抽样(UBCS)。在 UBCS 中，通过两个级联模块从整个用户历史序列中获取短子序列: (1)行为采样模块采用一种新的考虑相关性和时间信息的采样方法对与候选项相关的短子序列进行采样; (2)项目聚类模块将项目聚类为少量的聚类质心，减少噪声的影响，提高效率。然后，将采样的短子序列输入 CTR 预测模块进行有效预测。此外，我们进行了自我监督的一致性预训练任务，以提取用户的人物偏好，并有效地优化抽样模块。在实际数据集上的实验表明了该框架的优越性和有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Clustering+based+Behavior+Sampling+with+Long+Sequential+Data+for+CTR+Prediction)|1|
|[Matching Search Result Diversity with User Diversity Acceptance in Web Search Sessions](https://doi.org/10.1145/3477495.3531880)|Jiqun Liu, Fangyuan Han|The University of Oklahoma, Norman, OK, USA; Xiamen University, Xiamen, China|Promoting diversity in ranking while maintaining the relevance of ranked results is critical for enhancing human-centered search systems. While existing ranking algorithm and diversity IR metrics provide a solid basis for evaluating and improving search result diversification in offline experiments, it misses out possible divergences and temporal changes of users' levels of Diversity Acceptance, which in this work refers to the extent to which users actually prefer to interact with topically diversified search results. To address this gap between offline evaluations and users' expectations, we proposed an intuitive diversity acceptance measure and ran experiments for diversity acceptance prediction and diversity-aware re-ranking based on datasets from both controlled lab and naturalistic settings. Our results demonstrate that: 1) user diversity acceptance change across different query segments and session contexts, and can be predicted from search interaction signals; 2) our diversity-aware re-ranking algorithm utilizing predicted diversity acceptance and estimated relevance labels can effectively minimize the gap between diversity acceptance and result diversity, while maintaining SERP relevance levels. Our research presents an initial attempt on balancing user needs, result diversity, and SERP relevance in sessions and highlights the importance of studying diversity acceptance in promoting effective result diversification.|促进排名的多样性，同时保持排名结果的相关性，对于加强以人为中心的搜索系统至关重要。虽然现有的排名算法和多样性 IR 指标为评估和改善离线实验中的搜索结果多样性提供了坚实的基础，但它忽略了用户多样性接受水平的可能的分歧和时间变化，这在本文中指的是用户实际上更喜欢与主题多样化的搜索结果交互的程度。为了解决离线评估和用户期望之间的差距，我们提出了一个直观的多样性接受度量，并进行了基于受控实验室和自然环境数据集的多样性接受预测和多样性感知重新排序的实验。研究结果表明: 1)用户多样性接受度在不同查询段和会话上下文之间的变化，可以通过搜索交互信号进行预测; 2)我们的多样性感知重排算法利用预测的多样性接受度和估计的相关标签，可以有效地最小化多样性接受度和结果多样性之间的差距，同时保持 SERP 相关水平。我们的研究提出了在会议中平衡用户需求、结果多样性和 SERP 相关性的初步尝试，并强调了研究多样性接受在促进有效结果多样化中的重要性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Matching+Search+Result+Diversity+with+User+Diversity+Acceptance+in+Web+Search+Sessions)|1|
|[Distill-VQ: Learning Retrieval Oriented Vector Quantization By Distilling Knowledge from Dense Embeddings](https://doi.org/10.1145/3477495.3531799)|Shitao Xiao, Zheng Liu, Weihao Han, Jianjin Zhang, Defu Lian, Yeyun Gong, Qi Chen, Fan Yang, Hao Sun, Yingxia Shao, Xing Xie||Vector quantization (VQ) based ANN indexes, such as Inverted File System (IVF) and Product Quantization (PQ), have been widely applied to embedding based document retrieval thanks to the competitive time and memory efficiency. Originally, VQ is learned to minimize the reconstruction loss, i.e., the distortions between the original dense embeddings and the reconstructed embeddings after quantization. Unfortunately, such an objective is inconsistent with the goal of selecting ground-truth documents for the input query, which may cause severe loss of retrieval quality. Recent works identify such a defect, and propose to minimize the retrieval loss through contrastive learning. However, these methods intensively rely on queries with ground-truth documents, whose performance is limited by the insufficiency of labeled data. In this paper, we propose Distill-VQ, which unifies the learning of IVF and PQ within a knowledge distillation framework. In Distill-VQ, the dense embeddings are leveraged as "teachers'', which predict the query's relevance to the sampled documents. The VQ modules are treated as the "students'', which are learned to reproduce the predicted relevance, such that the reconstructed embeddings may fully preserve the retrieval result of the dense embeddings. By doing so, Distill-VQ is able to derive substantial training signals from the massive unlabeled data, which significantly contributes to the retrieval quality. We perform comprehensive explorations for the optimal conduct of knowledge distillation, which may provide useful insights for the learning of VQ based ANN index. We also experimentally show that the labeled data is no longer a necessity for high-quality vector quantization, which indicates Distill-VQ's strong applicability in practice. The evaluations are performed on MS MARCO and Natural Questions benchmarks, where Distill-VQ notably outperforms the SOTA VQ methods in Recall and MRR. Our code is avaliable at https://github.com/staoxiao/LibVQ.|基于向量量化(vQ)的人工神经网络索引，例如倒置文件系统(IVF)和产品量化(PQ) ，由于具有竞争性的时间和存储效率，已被广泛应用于基于嵌入的文献检索。最初，学习 VQ 来最小化重构损失，即原始密集嵌入和量化后重构嵌入之间的失真。遗憾的是，这样的目标不符合为输入查询选择地面真实文档的目标，这可能导致检索质量的严重损失。最近的工作发现了这一缺陷，并提出通过对比学习来最小化检索损失。然而，这些方法主要依赖于对地面真相文档的查询，其性能受到标记数据不足的限制。在本文中，我们提出了提取 VQ，它在一个知识提取框架内将 IVF 和 PQ 的学习结合起来。在蒸馏 VQ 中，密集嵌入被用作“教师”，用于预测查询与采样文档的相关性。将 VQ 模块视为“学生”，学习再现预测的相关性，使得重构嵌入能够完全保留密集嵌入的检索结果。通过这种方法，DistilVQ 能够从海量的未标记数据中提取出大量的训练信号，从而大大提高了检索质量。本文对知识提取的最优化进行了全面的探索，为基于矢量量化的神经网络指标的学习提供了有益的启示。我们还通过实验表明，标记数据不再是高质量向量量化的必要条件，这表明蒸馏 VQ 在实践中的强大适用性。评估是在微软 MARCO 和自然问题基准上进行的，其中蒸馏 VQ 明显优于召回和 MRR 中的 SOTA VQ 方法。我们的代码 https://github.com/staoxiao/libvq 可用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distill-VQ:+Learning+Retrieval+Oriented+Vector+Quantization+By+Distilling+Knowledge+from+Dense+Embeddings)|1|
|[Neural Pseudo-Relevance Feedback Models for Sparse and Dense Retrieval](https://doi.org/10.1145/3477495.3531685)|Xiao Wang|University of Glasgow, Glasgow, Scotland, United Kingdom|Pseudo-relevance feedback mechanisms have long served as an effective technique to improve the retrieval effectiveness in information retrieval. Recently, large pre-trained language models, such as T5 and BERT, have shown a strong capacity to capture the latent traits of texts. Given the success of these models, we seek to study the capacity of these models for query reformulation. In addition, the BERT models have demonstrated further promise for dense retrieval, where the query and documents are encoded into the contextualised embeddings and relevant documents are retrieved by conducting the semantic matching operation. Although the success of pseudo-relevance feedback for sparse retrieval is well documented, effective pseudo-relevance feedback approaches for dense retrieval paradigm are still in their infancy. Thus, we are concerned with excavating the potential of the pseudo-relevance feedback information combined with the large pre-trained models to conduct effective query reformulation operating on both sparse retrieval and dense retrieval.|长期以来，伪相关反馈机制一直是提高信息检索检索效率的有效方法。近年来，大型的预训练语言模型，如 T5和 BERT，已经显示出很强的捕捉文本潜在特征的能力。鉴于这些模型的成功，我们寻求研究这些模型的查询重构能力。此外，BERT 模型还进一步展示了密集检索的前景，其中查询和文档被编码到上下文嵌入中，相关文档通过进行语义匹配操作进行检索。虽然伪相关反馈在稀疏检索方面的成功已有文献记载，但有效的伪相关反馈方法在密集检索范式中仍处于起步阶段。因此，我们致力于挖掘伪相关反馈信息与大型预训练模型相结合的潜力，对稀疏检索和密集检索进行有效的查询重构。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Pseudo-Relevance+Feedback+Models+for+Sparse+and+Dense+Retrieval)|1|
|[Neighbour Interaction based Click-Through Rate Prediction via Graph-masked Transformer](https://doi.org/10.1145/3477495.3532031)|Erxue Min, Yu Rong, Tingyang Xu, Yatao Bian, Da Luo, Kangyi Lin, Junzhou Huang, Sophia Ananiadou, Peilin Zhao|The University of Manchester, Manchester, UNK, United Kingdom; Tencent AI Lab, Shenzhen, UNK, China; Weixin Open Platform, Tencent, Guangzhou, UNK, China; Tencent AI lab, Shenzhen, UNK, China; University of Texas at Arlington, Arlington, UNK, USA; University of Manchester, Manchester, UNK, United Kingdom|Click-Through Rate (CTR) prediction, which aims to estimate the probability that a user will click an item, is an essential component of online advertising. Existing methods mainly attempt to mine user interests from users' historical behaviours, which contain users' directly interacted items. Although these methods have made great progress, they are often limited by the recommender system's direct exposure and inactive interactions, and thus fail to mine all potential user interests. To tackle these problems, we propose Neighbor-Interaction based CTR prediction (NI-CTR), which considers this task under a Heterogeneous Information Network (HIN) setting. In short, Neighbor-Interaction based CTR prediction involves the local neighborhood of the target user-item pair in the HIN to predict their linkage. In order to guide the representation learning of the local neighbourhood, we further consider different kinds of interactions among the local neighborhood nodes from both explicit and implicit perspective, and propose a novel Graph-Masked Transformer (GMT) to effectively incorporates these kinds of interactions to produce highly representative embeddings for the target user-item pair. Moreover, in order to improve model robustness against neighbour sampling, we enforce a consistency regularization loss over the neighbourhood embedding. We conduct extensive experiments on two real-world datasets with millions of instances and the experimental results show that our proposed method outperforms state-of-the-art CTR models significantly. Meanwhile, the comprehensive ablation studies verify the effectiveness of every component of our model. Furthermore, we have deployed this framework on the WeChat Official Account Platform with billions of users. The online A/B tests demonstrate an average CTR improvement of 21.9% against all online baselines.|点进率预测是在线广告的一个重要组成部分，其目的是估计用户点击某个项目的概率。现有的方法主要是从用户的历史行为中挖掘用户兴趣，这些历史行为包含了用户直接交互的项目。尽管这些方法已经取得了很大的进步，但它们往往受到推荐系统直接暴露和非活动交互的限制，因此无法挖掘所有潜在的用户兴趣。为了解决这些问题，我们提出了基于邻居交互的点击率预测(NI-CTR) ，它考虑了异构信息网络(HIN)环境下的任务。简而言之，基于邻域交互的 CTR 预测涉及到 HIN 中目标用户-项目对的局部邻域来预测它们之间的联系。为了指导局部邻域的表示学习，我们进一步从显式和隐式两个角度考虑局部邻域节点之间的不同交互，并提出了一种新的图掩盖变换器(GMT) ，以有效地整合这些交互，为目标用户项对产生高度代表性的嵌入。此外，为了提高模型对邻域采样的鲁棒性，我们对邻域嵌入增加了一个一致性正则化损失。我们在两个实际数据集上进行了大量的实验，实验结果表明，我们提出的方法明显优于最先进的 CTR 模型。同时，综合烧蚀研究验证了模型各组成部分的有效性。此外，我们已经在微信官方账号平台上部署了这个框架，拥有数十亿用户。在线 A/B 测试显示，与所有在线基线相比，点击率平均提高了21.9% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neighbour+Interaction+based+Click-Through+Rate+Prediction+via+Graph-masked+Transformer)|1|
|[Why Don't You Click: Understanding Non-Click Results in Web Search with Brain Signals](https://doi.org/10.1145/3477495.3532082)|Ziyi Ye, Xiaohui Xie, Yiqun Liu, Zhihong Wang, Xuancheng Li, Jiaji Li, Xuesong Chen, Min Zhang, Shaoping Ma|The Chinese University of Hong Kong, Shenzhen, Shenzhen, China; Tsinghua University, Beijing, China|Web search heavily relies on click-through behavior as an essential feedback signal for performance evaluation and improvement. Traditionally, click is usually treated as a positive implicit feedback signal of relevance or usefulness, while non-click is regarded as a signal of irrelevance or uselessness. However, there are many cases where users satisfy their information need with the contents shown on the Search Engine Result Page (SERP). This raises the problem of measuring the usefulness of non-click results and modeling user satisfaction in such circumstances. For a long period, understanding non-click results is challenging owing to the lack of user interactions. In recent years, the rapid development of neuroimaging technologies constitutes a paradigm shift in various industries, e.g., search, entertainment, and education. Therefore, we benefit from these technologies and apply them to bridge the gap between the human mind and the external search system in non-click situations. To this end, we analyze the differences in brain signals between the examination of non-click search results in different usefulness levels. Inspired by these findings, we conduct supervised learning tasks to estimate the usefulness of non-click results with brain signals and conventional information (i.e., content and context factors). Furthermore, we devise two re-ranking methods, i.e., a Personalized Method (PM) and a Generalized Intent modeling Method (GIM), for search result re-ranking with the estimated usefulness. Results show that it is feasible to utilize brain signals to improve usefulness estimation performance and enhance human-computer interactions by search result re-ranking.|网络搜索在很大程度上依赖于点击行为作为性能评估和改进的重要反馈信号。传统上，点击通常被视为相关性或有用性的积极隐性反馈信号，而非点击则被视为无关性或无用性的信号。但是，在许多情况下，用户使用搜索引擎结果页(SERP)上显示的内容来满足他们的信息需求。这就提出了测量非点击结果的有用性以及在这种情况下建立用户满意度模型的问题。长期以来，由于缺乏用户交互，理解非点击结果是一项挑战。近年来，神经影像技术的快速发展构成了搜索、娱乐和教育等多个行业的范式转变。因此，我们从这些技术中受益，并应用它们在非点击情况下架起人类思维和外部搜索系统之间的桥梁。为此，我们分析了不同有用性水平的非点击检索结果在大脑信号方面的差异。受这些发现的启发，我们进行了一些监督式学习的任务，用大脑信号和传统信息(即内容和上下文因素)来评估非点击结果的有用性。此外，我们设计了两个重新排序的方法，即个性化方法(PM)和广义意图建模方法(GIM) ，用于搜索结果的重新排序与估计的有用性。实验结果表明，利用大脑信号进行搜索结果重排可以提高有用性估计性能，增强人机交互。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Why+Don't+You+Click:+Understanding+Non-Click+Results+in+Web+Search+with+Brain+Signals)|1|
|[Hierarchical Multi-Task Graph Recurrent Network for Next POI Recommendation](https://doi.org/10.1145/3477495.3531989)|Nicholas Lim, Bryan Hooi, SeeKiong Ng, Yong Liang Goh, Renrong Weng, Rui Tan|National University of Singapore, Singapore, Singapore; GrabTaxi Holdings, Singapore, Singapore|Learning which Point-of-Interest (POI) a user will visit next is a challenging task for personalized recommender systems due to the large search space of possible POIs in the region. A recurring problem among existing works that makes it difficult to learn and perform well is the sparsity of the User-POI matrix. In this paper, we propose our Hierarchical Multi-Task Graph Recurrent Network (HMT-GRN) approach, which alleviates the data sparsity problem by learning different User-Region matrices of lower sparsities in a multi-task setting. We then perform a Hierarchical Beam Search (HBS) on the different region and POI distributions to hierarchically reduce the search space with increasing spatial granularity and predict the next POI. Our HBS provides efficiency gains by reducing the search space, resulting in speedups of 5 to 7 times over an exhaustive approach. In addition, we also propose a novel selectivity layer to predict if the next POI has been visited before by the user to balance between personalization and exploration. Experimental results on two real-world Location-Based Social Network (LBSN) datasets show that our model significantly outperforms baseline and the state-of-the-art methods.|由于该地区可能存在的 POI 搜索空间很大，因此了解用户下一步将访问哪个 POI 对于个性化推荐系统来说是一项具有挑战性的任务。现有作品中反复出现的一个难以学习和执行的问题是 User-POI 矩阵的稀疏性。本文提出了一种分层多任务图回归网络(HMT-GRN)方法，通过在多任务环境下学习不同稀疏度较低的用户区域矩阵来解决数据稀疏问题。然后对不同区域和 POI 分布进行分层束搜索(HBS) ，随着空间粒度的增加逐步减少搜索空间，并预测下一个 POI。我们的 HBS 通过减少搜索空间提供了效率增益，在一个详尽的方法中导致5到7倍的加速。此外，我们还提出了一个新的选择层来预测下一个 POI 是否已经被用户访问过，以便在个性化和探索之间取得平衡。在两个实际的基于位置的社会网络(LBSN)数据集上的实验结果表明，我们的模型明显优于基线和最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Multi-Task+Graph+Recurrent+Network+for+Next+POI+Recommendation)|1|
|[Progressive Self-Attention Network with Unsymmetrical Positional Encoding for Sequential Recommendation](https://doi.org/10.1145/3477495.3531800)|Yuehua Zhu, Bo Huang, Shaohua Jiang, Muli Yang, Yanhua Yang, Wenliang Zhong|AntGroup, Hangzhou, China; Xidian University, Xian, China|In real-world recommendation systems, the preferences of users are often affected by long-term constant interests and short-term temporal needs. The recently proposed Transformer-based models have proved superior in the sequential recommendation, modeling temporal dynamics globally via the remarkable self-attention mechanism. However, all equivalent item-item interactions in original self-attention are cumbersome, failing to capture the drifting of users' local preferences, which contain abundant short-term patterns. In this paper, we propose a novel interpretable convolutional self-attention, which efficiently captures both short- and long-term patterns with a progressive attention distribution. Specifically, a down-sampling convolution module is proposed to segment the overall long behavior sequence into a series of local subsequences. Accordingly, the segments are interacted with each item in the self-attention layer to produce locality-aware contextual representations, during which the quadratic complexity in original self-attention is reduced to nearly linear complexity. Moreover, to further enhance the robust feature learning in the context of Transformers, an unsymmetrical positional encoding strategy is carefully designed. Extensive experiments are carried out on real-world datasets, \eg ML-1M, Amazon Books, and Yelp, indicating that the proposed method outperforms the state-of-the-art methods w.r.t. both effectiveness and efficiency.|在实际的推荐系统中，用户的偏好往往受到长期不变的利益和短期的时间需求的影响。最近提出的变压器为基础的模型已被证明优越的顺序推荐，建模全球时间动态通过显着的自我注意机制。然而，原始自我注意中的所有等效项目-项目交互都是繁琐的，未能捕捉到用户本地偏好的漂移，其中包含了丰富的短期模式。在本文中，我们提出了一种新的可解释的卷积自我注意，有效地捕捉短期和长期的模式与逐步注意分布。特别地，提出了一种下采样卷积模块，将整个长行为序列分割成一系列局部子序列。相应地，这些片段与自我注意层中的每个项目相互作用，产生具有局部感知的上下文表征，在此过程中，原始自我注意的二次复杂度降低到接近线性复杂度。此外，为了进一步提高变压器环境下的鲁棒性特征学习，设计了一种非对称的位置编码策略。在现实世界的数据集上进行了大量的实验，例如 ML-1M，Amazon Books 和 Yelp，表明所提出的方法在效率和效果上都优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Progressive+Self-Attention+Network+with+Unsymmetrical+Positional+Encoding+for+Sequential+Recommendation)|1|
|[A New Sequential Prediction Framework with Spatial-temporal Embedding](https://doi.org/10.1145/3477495.3531846)|Jihai Zhang, Fangquan Lin, Cheng Yang, Wei Jiang|Alibaba Group, Hangzhou, China|Sequential prediction is one of the key components in recommendation. In online e-commerce recommendation system, user behavior consists of the sequential visiting logs and item behavior contains the interacted user list in order. Most of the existing state-of-the-art sequential prediction methods only consider the user behavior while ignoring the item behavior. In addition, we find that user behavior varies greatly at different time, and most existing models fail to characterize the rich temporal information. To address the above problems, we propose a transformer-based spatial-temporal recommendation framework (STEM). In the STEM framework, we first utilize attention mechanisms to model user behavior and item behavior, and then exploit spatial and temporal information through a transformer-based model. The STEM framework, as a plug-in, is able to be incorporated into many neural network-based sequential recommendation methods to improve performance. We conduct extensive experiments on three real-world Amazon datasets. The results demonstrate the effectiveness of our proposed framework.|序贯预测是推荐系统的关键组成部分之一。在在线电子商务推荐系统中，用户行为由顺序访问日志组成，项目行为按顺序包含交互式用户列表。现有的大多数最先进的顺序预测方法只考虑用户行为，而忽略项目行为。此外，我们发现用户行为在不同的时间变化很大，现有的模型不能刻画丰富的时间信息。为了解决上述问题，我们提出了一个基于变压器的时空推荐框架(STEM)。在 STEM 框架中，我们首先利用注意机制对用户行为和项目行为进行建模，然后通过一个基于转换器的模型来利用空间和时间信息。STEM 框架作为一个插件，能够被整合到许多基于神经网络的顺序推荐方法中，以提高性能。我们在三个真实的亚马逊数据集上进行了广泛的实验。仿真结果表明了该框架的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+New+Sequential+Prediction+Framework+with+Spatial-temporal+Embedding)|1|
|[Mitigating the Filter Bubble While Maintaining Relevance: Targeted Diversification with VAE-based Recommender Systems](https://doi.org/10.1145/3477495.3531890)|Zhaolin Gao, Tianshu Shen, Zheda Mai, Mohamed Reda Bouadjenek, Isaac Waller, Ashton Anderson, Ron Bodkin, Scott Sanner|Optimy AI, Toronto, ON, Canada; University of Toronto, Toronto, ON, Canada; Deakin University, Geelong, Australia; Vector Institute for Artificial Intelligence, Toronto, ON, Canada|Online recommendation systems are prone to create filter bubbles, whereby users are only recommended content narrowly aligned with their historical interests. In the case of media recommendation, this can reinforce political polarization by recommending topical content (e.g., on the economy) at one extreme end of the political spectrum even though this topic has broad coverage from multiple political viewpoints that would provide a more balanced and informed perspective for the user. Historically, Maximal Marginal Relevance (MMR) has been used to diversify result lists and even mitigate filter bubbles, but suffers from three key drawbacks: (1)~MMR directly sacrifices relevance for diversity, (2)~MMR typically diversifies across all content and not just targeted dimensions (e.g., political polarization), and (3)~MMR is inefficient in practice due to the need to compute pairwise similarities between recommended items. To simultaneously address these limitations, we propose a novel methodology that trains Concept Activation Vectors (CAVs) for targeted topical dimensions (e.g., political polarization). We then modulate the latent embeddings of user preferences in a state-of-the-art VAE-based recommender system to diversify along the targeted dimension while preserving topical relevance across orthogonal dimensions. Our experiments show that our Targeted Diversification VAE-based Collaborative Filtering (TD-VAE-CF) methodology better preserves relevance of content to user preferences across a range of diversification levels in comparison to both untargeted and targeted variations of Maximum Marginal Relevance (MMR); TD-VAE-CF is also much more computationally efficient than the post-hoc re-ranking approach of MMR.|在线推荐系统容易产生过滤气泡，用户只能被推荐与他们的历史兴趣狭窄地一致的内容。就媒体推荐而言，这可能会加剧政治两极分化，推荐政治光谱一端的主题内容(例如经济) ，尽管这个主题有多种政治观点的广泛覆盖，可以为用户提供一个更加平衡和知情的视角。从历史上看，最大边际相关(MMR)一直被用来使结果列表多样化，甚至减轻过滤器泡沫，但遭受三个关键的缺点: (1) ~ MMR 直接牺牲相关性的多样性，(2) ~ MMR 通常多样化跨所有内容，而不仅仅是有针对性的维度(例如，政治极化) ，和(3) ~ MMR 在实践中是低效的，因为需要计算推荐项目之间的成对相似性。为了同时解决这些局限性，我们提出了一种新的方法，训练概念激活向量(CAV)的目标主题维度(例如，政治极化)。然后，我们调整潜在的嵌入用户偏好在一个国家的最先进的 VAE 为基础的推荐系统，以多样化沿着目标的维度，同时保持跨正交维度的主题相关性。我们的实验表明，我们的基于目标多样化 VAE 的协同过滤(TD-VAE-CF)方法更好地保留了多样化水平范围内的用户偏好的内容相关性，与非目标和有针对性的最大边际相关性(MMR)变化相比，TD-VAE-CF 也比 MMR 的事后重新排序方法计算效率高得多。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mitigating+the+Filter+Bubble+While+Maintaining+Relevance:+Targeted+Diversification+with+VAE-based+Recommender+Systems)|1|
|[Modality-Balanced Embedding for Video Retrieval](https://doi.org/10.1145/3477495.3531899)|Xun Wang, Bingqing Ke, Xuanping Li, Fangyu Liu, Mingyu Zhang, Xiao Liang, Qiushi Xiao||Video search has become the main routine for users to discover videos relevant to a text query on large short-video sharing platforms. During training a query-video bi-encoder model using online search logs,\textit we identify a modality bias phenomenon that the video encoder almost entirely relies on text matching, neglecting other modalities of the videos such as vision, audio, \etc This modality imbalance results from a) modality gap: the relevance between a query and a video text is much easier to learn as the query is also a piece of text, with the same modality as the video text; b) data bias: most training samples can be solved solely by text matching. Here we share our practices to improve the first retrieval stage including our solution for the modality imbalance issue. We propose \modelname (short for Modality Balanced Video Retrieval) with two key components: manually generated modality-shuffled (MS) samples and a dynamic margin (DM) based on visual relevance. They can encourage the video encoder to pay balanced attentions to each modality. Through extensive experiments on a real world dataset, we show empirically that our method is both effective and efficient in solving modality bias problem. We have also deployed our ~\modelname~ in a large video platform and observed statistically significant boost over a highly optimized baseline in an A/B test and manual GSB evaluations.|视频搜索已成为用户在大型短视频分享平台上发现与文本查询相关的视频的主要程序。在使用在线搜索日志(texttit)对查询-视频双编码器模型进行训练时，我们发现了一种模态偏差现象，即视频编码器几乎完全依赖于文本匹配，而忽略了视频的其他模态，如视觉、音频等。这种模态偏差源于: a)模态差异: 查询和视频文本之间的相关性更容易学习，因为查询也是一段文本，具有与视频文本相同的模态; b)数据偏差: 大多数训练样本可以单独通过文本匹配来解决。在这里，我们分享我们的做法，以改善第一个检索阶段，包括我们的解决方案的形式不平衡的问题。提出了一种基于视觉相关性的动态边界检索模型，该模型由两个关键部分组成: 手动生成的模态混合样本(MS)和动态边界(DM)。它们可以鼓励视频编码器对每种模式给予均衡的关注。通过对实际数据集的大量实验，证明了该方法在解决模态偏差问题上的有效性。我们还在一个大型视频平台上部署了我们的“模型名”，并在 A/B 测试和手动 GSB 评估中观察到统计学上显著提高了高度优化的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modality-Balanced+Embedding+for+Video+Retrieval)|1|
|[Expanded Lattice Embeddings for Spoken Document Retrieval on Informal Meetings](https://doi.org/10.1145/3477495.3531921)|Esaú VillatoroTello, Srikanth R. Madikeri, Petr Motlícek, Aravind Ganapathiraju, Alexei V. Ivanov|Idiap Research Institute, Martigny, Switzerland; Uniphore Software Systems Inc., Palo Alto, CA, USA|In this paper, we evaluate different alternatives to process richer forms of Automatic Speech Recognition (ASR) output based on lattice expansion algorithms for Spoken Document Retrieval (SDR). Typically, SDR systems employ ASR transcripts to index and retrieve relevant documents. However, ASR errors negatively affect the retrieval performance. Multiple alternative hypotheses can also be used to augment the input to document retrieval to compensate for the erroneous one-best hypothesis. In Weighted Finite State Transducer-based ASR systems, using the n-best output (i.e. the top "n'' scoring hypotheses) for the retrieval task is common, since they can easily be fed to a traditional Information Retrieval (IR) pipeline. However, the n-best hypotheses are terribly redundant, and do not sufficiently encapsulate the richness of the ASR output, which is represented as an acyclic directed graph called the lattice. In particular, we utilize the lattice's constrained minimum path cover to generate a minimum set of hypotheses that serve as input to the reranking phase of IR. The novelty of our proposed approach is the incorporation of the lattice as an input for neural reranking by considering a set of hypotheses that represents every arc in the lattice. The obtained hypotheses are encoded through sentence embeddings using BERT-based models, namely SBERT and RoBERTa, and the final ranking of the retrieved segments is obtained with a max-pooling operation over the computed scores among the input query and the hypotheses set. We present our evaluation on the publicly available AMI meeting corpus. Our results indicate that the proposed use of hypotheses from the expanded lattice improves the SDR performance significantly over the n-best ASR output.|在这篇文章中，我们评估了处理更丰富形式的自动语音识别(ASR)输出的不同方案，这些方案是基于文献检索的格展开算法。通常，SDR 系统使用 ASR 记录来索引和检索相关文档。但是，ASR 错误会对检索性能产生负面影响。多重替代假设也可以用来增加对文献检索的输入，以弥补错误的最佳假设。在基于加权有限状态转换器的 ASR 系统中，对于检索任务使用 n 个最佳输出(即顶部的“ n”评分假设)是很常见的，因为它们可以很容易地提供给传统的信息检索(IR)流水线。然而，n 个最佳假设是非常多余的，并且没有充分封装 ASR 输出的丰富性，ASR 输出被表示为一个称为格的非循环有向图。特别地，我们利用格子的约束最小路覆盖生成一组最小假设，作为 IR 重新排序阶段的输入。我们提出的方法的新颖之处在于，通过考虑一组代表格子中每个弧的假设，将格子作为神经重新排序的输入。所得到的假设通过使用基于 BERT 的模型(即 SBERT 和 RoBERTa)的句子嵌入进行编码，并且通过对输入查询和假设集之间计算得分的最大池操作获得检索段的最终排序。我们提出了我们的评价公开可用的 AMI 会议语料库。我们的结果表明，提出的假设使用从扩展格提高软件无线电性能显着超过 n 最佳的 ASR 输出。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Expanded+Lattice+Embeddings+for+Spoken+Document+Retrieval+on+Informal+Meetings)|1|
|[Improving Item Cold-start Recommendation via Model-agnostic Conditional Variational Autoencoder](https://doi.org/10.1145/3477495.3531902)|Xu Zhao, Yi Ren, Ying Du, Shenzheng Zhang, Nian Wang|Tencent News, Beijing, China|Embedding & MLP has become a paradigm for modern large-scale recommendation system. However, this paradigm suffers from the cold-start problem which will seriously compromise the ecological health of recommendation systems. This paper attempts to tackle the item cold-start problem by generating enhanced warmed-up ID embeddings for cold items with historical data and limited interaction records. From the aspect of industrial practice, we mainly focus on the following three points of item cold-start: 1) How to conduct cold-start without additional data requirements and make strategy easy to be deployed in online recommendation scenarios. 2) How to leverage both historical records and constantly emerging interaction data of new items. 3) How to model the relationship between item ID and side information stably from interaction data. To address these problems, we propose a model-agnostic Conditional Variational Autoencoder based Recommendation(CVAR) framework with some advantages including compatibility on various backbones, no extra requirements for data, utilization of both historical data and recent emerging interactions. CVAR uses latent variables to learn a distribution over item side information and generates desirable item ID embeddings using a conditional decoder. The proposed method is evaluated by extensive offline experiments on public datasets and online A/B tests on Tencent News recommendation platform, which further illustrate the advantages and robustness of CVAR.|嵌入式 MLP 已经成为现代大规模推荐系统的典范。然而，这种模式受到冷启动问题的影响，这将严重损害推荐系统的生态健康。本文试图利用历史数据和有限的交互记录为冷藏物品生成增强的预热 ID 嵌入，从而解决物品冷启动问题。从工业实践的角度出发，重点研究了项目冷启动的三个方面: 1)如何在不增加额外数据需求的情况下进行冷启动，使策略易于在在线推荐场景中部署。2)如何利用历史记录和新项目不断涌现的交互数据。3)如何从交互数据中稳定地建立项目 ID 与侧信息之间的关系模型。为了解决这些问题，我们提出了一个基于模型无关的条件变分自动编码器推荐(CVAR)框架，该框架具有一些优点，包括在不同骨干上的兼容性，对数据没有额外的要求，利用历史数据和最近出现的交互。CVAR 使用潜变量学习项目边信息的分布，并使用条件解码器生成所需的项目 ID 嵌入。该方法通过在公共数据集上的大量离线实验和在腾讯新闻推荐平台上的在线 A/B 测试进行了评估，进一步说明了 CVAR 的优势和稳健性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Item+Cold-start+Recommendation+via+Model-agnostic+Conditional+Variational+Autoencoder)|1|
|[Multi-Faceted Global Item Relation Learning for Session-Based Recommendation](https://doi.org/10.1145/3477495.3532024)|Qilong Han, Chi Zhang, Rui Chen, Riwei Lai, Hongtao Song, Li Li|University of Delaware, Newark, DE, USA; Harbin Engineering University, Harbin, China|As an emerging paradigm, session-based recommendation is aimed at recommending the next item based on a set of anonymous sessions. Effectively representing a session that is normally a short interaction sequence renders a major technical challenge. In view of the limitations of pioneering studies that explore collaborative information from other sessions, in this paper we propose a new direction to enhance session representations by learning multi-faceted session-independent global item relations. In particular, we identify three types of advantageous global item relations, including negative relations that have not been studied before, and propose different graph construction methods to capture such relations. We then devise a novel multi-faceted global item relation (MGIR) model to encode different relations using different aggregation layers and generate enhanced session representations by fusing positive and negative relations. Our solution is flexible to accommodate new item relations and can easily integrate existing session representation learning methods to generate better representations from global relation enhanced session information. Extensive experiments on three benchmark datasets demonstrate the superiority of our model over a large number of state-of-the-art methods. Specifically, we show that learning negative relations is critical for session-based recommendation.|作为一种新兴的模式，基于会议的建议旨在根据一组匿名会议推荐下一个项目。有效地表示一个通常是短交互序列的会话会带来重大的技术挑战。针对开拓性研究在探索其他会议协作信息方面的局限性，本文提出了一个通过学习多方面会议独立的全局项目关系来增强会议表征的新方向。特别地，我们确定了三种有利的全局项目关系，包括以前没有研究过的负关系，并提出了不同的图构造方法来捕获这些关系。然后设计了一个新的多方面全局项目关系(MGIR)模型，利用不同的聚合层对不同的关系进行编码，并通过融合正负关系生成增强的会话表示。我们的解决方案是灵活的，以适应新的项目关系，可以很容易地集成现有的会话表示学习方法，从全局关系增强会话信息生成更好的表示。在三个基准数据集上的大量实验表明，我们的模型优于大量的最先进的方法。具体来说，我们表明，学习负关系是至关重要的会话为基础的推荐。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Faceted+Global+Item+Relation+Learning+for+Session-Based+Recommendation)|1|
|[Item Similarity Mining for Multi-Market Recommendation](https://doi.org/10.1145/3477495.3531839)|Jiangxia Cao, Xin Cong, Tingwen Liu, Bin Wang|Institute of Information Engineering, CAS & UCAS, Beijing, China; Xiaomi AI Lab, Xiaomi Inc., Beijing, China|Real-world web applications such as Amazon and Netflix often provide services in multiple countries and regions (i.e., markets) around the world. Generally, different markets share similar item sets while containing different amounts of interaction data. Some markets are data-scarce and others are data-rich and leveraging those data from similar and data-rich auxiliary markets could enhance the data-scarce markets. In this paper, we explore multi-market recommendation (MMR), and propose a novel model called M$^3$Rec to improve all markets recommendation simultaneously. Since items play the role to bridge different markets, we argue that mining the similarities among items is the key point of MMR. Our M^3Rec preprocess two global item similarities: intra- and inter- market similarities. Specifically, we first learn the second-order intra-market similarity by adopting linear models with closed-form solutions, and then capture the high-order inter-market similarity by the random walk. Afterward, we incorporate the global item similarities for each local market. We conduct extensive experiments on five public available markets and compare with several state-of-the-art methods. Detailed experimental results demonstrate the effectiveness of our proposed method.|现实世界的网络应用程序，如亚马逊和 Netflix，通常在世界各地的多个国家和地区(即市场)提供服务。通常，不同的市场共享相似的项目集，同时包含不同数量的交互数据。一些市场数据稀缺，另一些市场数据丰富，利用类似和数据丰富的辅助市场的数据可以加强数据稀缺市场。在本文中，我们探讨了多市场推荐(MMR) ，并提出了一种新的模型 M $^ 3 $Rec 来同时改进所有市场的推荐。由于产品在不同市场之间起着桥梁作用，我们认为挖掘产品之间的相似性是 MMR 的关键所在。我们的 M ^ 3Rec 预处理两个全球项目的相似性: 市场内部和市场间的相似性。具体来说，我们首先通过采用具有封闭解的线性模型来学习二阶市场内部相似性，然后通过随机游走来获取高阶市场内部相似性。然后，我们为每个当地市场整合全球产品的相似性。我们在五个公开市场上进行了广泛的实验，并与几种最先进的方法进行了比较。详细的实验结果证明了该方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Item+Similarity+Mining+for+Multi-Market+Recommendation)|1|
|[Interpreting Patient Descriptions using Distantly Supervised Similar Case Retrieval](https://doi.org/10.1145/3477495.3532003)|Israa Alghanmi, Luis Espinosa Anke, Steven Schockaert|Cardiff University, Cardiff, United Kingdom|Biomedical natural language processing often involves the interpretation of patient descriptions, for instance for diagnosis or for recommending treatments. Current methods, based on biomedical language models, have been found to struggle with such tasks. Moreover, retrieval augmented strategies have only had limited success, as it is rare to find sentences which express the exact type of knowledge that is needed for interpreting a given patient description. For this reason, rather than attempting to retrieve explicit medical knowledge, we instead propose to rely on a nearest neighbour strategy. First, we retrieve text passages that are similar to the given patient description, and are thus likely to describe patients in similar situations, while also mentioning some hypothesis (e.g.\ a possible diagnosis of the patient). We then judge the likelihood of the hypothesis based on the similarity of the retrieved passages. Identifying similar cases is challenging, however, as descriptions of similar patients may superficially look rather different, among others because they often contain an abundance of irrelevant details. To address this challenge, we propose a strategy that relies on a distantly supervised cross-encoder. Despite its conceptual simplicity, we find this strategy to be effective in practice.|生物医学自然语言处理通常涉及对患者描述的解释，例如用于诊断或推荐治疗。目前的方法，基于生物医学语言模型，已被发现与这样的任务斗争。此外，提取增强策略只取得了有限的成功，因为很少能找到表达解释给定患者描述所需的确切知识类型的句子。基于这个原因，我们不尝试检索显性医学知识，而是建议依赖于最近邻策略。首先，我们检索与给定患者描述相似的文本段落，因此可能描述处于相似情况的患者，同时也提到一些假设(例如患者的可能诊断)。然后，我们根据检索到的段落的相似性来判断假设的可能性。然而，鉴别相似病例是具有挑战性的，因为对相似病例的描述可能在表面上看起来相当不同，因为它们往往包含大量不相关的细节。为了应对这一挑战，我们提出了一种依赖于远程监督交叉编码器的策略。尽管其概念简单，我们发现这种策略在实践中是有效的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interpreting+Patient+Descriptions+using+Distantly+Supervised+Similar+Case+Retrieval)|1|
|[Towards Explainable Search Results: A Listwise Explanation Generator](https://doi.org/10.1145/3477495.3532067)|Puxuan Yu, Razieh Rahimi, James Allan|University of Massachusetts Amherst, Amherst, MA, USA|It has been shown that the interpretability of search results is enhanced when query aspects covered by documents are explicitly provided. However, existing work on aspect-oriented explanation of search results explains each document independently. These explanations thus cannot describe the differences between documents. This issue is also true for existing models on query aspect generation. Furthermore, these models provide a single query aspect for each document, even though documents often cover multiple query aspects. To overcome these limitations, we propose LiEGe, an approach that jointly explains all documents in a search result list. LiEGe provides semantic representations at two levels of granularity -- documents and their tokens -- using different interaction signals including cross-document interactions. These allow listwise modeling of a search result list as well as the generation of coherent explanations for documents. To appropriately explain documents that cover multiple query aspects, we introduce two settings for search result explanation: comprehensive and novelty explanation generation. LiEGe is trained and evaluated for both settings. We evaluate LiEGe on datasets built from Wikipedia and real query logs of the Bing search engine. Our experimental results demonstrate that LiEGe outperforms all baselines, with improvements that are substantial and statistically significant.|研究表明，如果明确提供文件所涉查询方面，搜索结果的可解释性就会得到提高。但是，现有的面向方面的搜索结果解释工作独立地解释每个文档。因此，这些解释无法描述文档之间的差异。对于生成查询方面的现有模型，也存在这个问题。此外，这些模型为每个文档提供单个查询方面，即使文档通常包含多个查询方面。为了克服这些限制，我们提出了 LiEGe，一种联合解释搜索结果列表中所有文档的方法。LiEGe 使用不同的交互信号(包括跨文档交互)在两个粒度级别(文档及其标记)提供语义表示。它们允许对搜索结果列表进行列表建模，以及为文档生成连贯的解释。为了恰当地解释涵盖多个查询方面的文档，我们引入了两种用于搜索结果解释的设置: 全面解释生成和新颖解释生成。对 LiEGe 进行了两种设置的培训和评估。我们根据维基百科建立的数据集和 Bing 搜索引擎的实际查询日志来评估 LiEGe。我们的实验结果表明，LiEGe 的性能优于所有基线，具有实质性的改进和统计学意义。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Explainable+Search+Results:+A+Listwise+Explanation+Generator)|1|
|[Bit-aware Semantic Transformer Hashing for Multi-modal Retrieval](https://doi.org/10.1145/3477495.3531947)|Wentao Tan, Lei Zhu, Weili Guan, Jingjing Li, Zhiyong Cheng|Shandong Normal University, Jinan, China; University of Electronic Science and Technology of China, Chengdu, China; Shandong Artificial Intelligence Institute, Jinan, China; Monash University, Clayton, Australia|Multi-modal hashing learns binary hash codes with extremely low storage cost and high retrieval speed. It can support efficient multi-modal retrieval well. However, most existing methods still suffer from three important problems: 1) Limited semantic representation capability with shallow learning. 2) Mandatory feature-level multi-modal fusion ignores heterogeneous multi-modal semantic gaps. 3) Direct coarse pairwise semantic preserving cannot effectively capture the fine-grained semantic correlations. For solving these problems, in this paper, we propose a Bit-aware Semantic Transformer Hashing (BSTH) framework to excavate bit-wise semantic concepts and simultaneously align the heterogeneous modalities for multi-modal hash learning on the concept-level. Specifically, the bit-wise implicit semantic concepts are learned with the transformer in a self-attention manner, which can achieve implicit semantic alignment on the fine-grained concept-level and reduce the heterogeneous modality gaps. Then, the concept-level multi-modal fusion is performed to enhance the semantic representation capability of each implicit concept and the fused concept representations are further encoded to the corresponding hash bits via bit-wise hash functions. Further, to supervise the bit-aware transformer module, a label prototype learning module is developed to learn prototype embeddings for all categories that capture the explicit semantic correlations on the category-level by considering the co-occurrence priors. Experiments on three widely tested multi-modal retrieval datasets demonstrate the superiority of the proposed method from various aspects.|多模态哈希学习二进制哈希码具有极低的存储成本和极高的检索速度。它能很好地支持有效的多模态检索。然而，现有的大多数方法仍然存在三个重要问题: 1)语义表示能力有限，浅层学习。2)强制特征级多模态融合忽略异质多模态语义缺口。3)直接粗对语义保持不能有效地捕获细粒度的语义关联。为了解决这些问题，本文提出了一种比特感知的语义变换哈希(BSTH)框架来挖掘比特感知的语义概念，同时在概念层次上对多模态哈希学习的异构模式进行校准。具体来说，通过变换器以自我注意的方式学习位隐含语义概念，可以在细粒度的概念水平上实现隐含语义对齐，减少异质情态差异。然后进行概念级多模态融合以提高每个隐式概念的语义表示能力，并通过逐位哈希函数将融合后的概念表示进一步编码到相应的哈希位。此外，为了监督位感知转换器模块，开发了一个标签原型学习模块来学习所有类别的原型嵌入，这些类别通过考虑共现先验在类别层面上捕获显式的语义相关性。在三个被广泛测试的多模态检索数据集上的实验从各个方面证明了该方法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bit-aware+Semantic+Transformer+Hashing+for+Multi-modal+Retrieval)|1|
|[Multimodal Disentanglement Variational AutoEncoders for Zero-Shot Cross-Modal Retrieval](https://doi.org/10.1145/3477495.3532028)|Jialin Tian, Kai Wang, Xing Xu, Zuo Cao, Fumin Shen, Heng Tao Shen|University of Electronic Science and Technology of China, Chengdu, China; Meituan, Shanghai, China|Zero-Shot Cross-Modal Retrieval (ZS-CMR) has recently drawn increasing attention as it focuses on a practical retrieval scenario, i.e., the multimodal test set consists of unseen classes that are disjoint with seen classes in the training set. The recently proposed methods typically adopt the generative model as the main framework to learn a joint latent embedding space to alleviate the modality gap. Generally, these methods largely rely on auxiliary semantic embeddings for knowledge transfer across classes and unconsciously neglect the effect of the data reconstruction manner in the adopted generative model. To address this issue, we propose a novel ZS-CMR model termed Multimodal Disentanglement Variational AutoEncoders (MDVAE), which consists of two coupled disentanglement variational autoencoders (DVAEs) and a fusion-exchange VAE (FVAE). Specifically, DVAE is developed to disentangle the original representations of each modality into modality-invariant and modality-specific features. FVAE is designed to fuse and exchange information of multimodal data by the reconstruction and alignment process without pre-extracted semantic embeddings. Moreover, an advanced counter-intuitive cross-reconstruction scheme is further proposed to enhance the informativeness and generalizability of the modality-invariant features for more effective knowledge transfer. The comprehensive experiments on four image-text retrieval and two image-sketch retrieval datasets consistently demonstrate that our method establishes the new state-of-the-art performance.|零拍交叉模态检索(Zero-Shot Cross-Modal Retrieval，ZS-CMR)近年来受到越来越多的关注，因为它集中在一个实际的检索场景，即多模态测试集包含不相交的看不见的类和训练集中的看得见的类。最近提出的方法通常采用生成模型作为主要框架，学习一个联合潜在嵌入空间，以缓解模态差距。一般而言，这些方法主要依赖于辅助语义嵌入来跨类别传递知识，而无意识地忽略了所采用的生成模型中数据重构方式的影响。为了解决这个问题，我们提出了一种新的 ZS-CMR 模型，称为多模态解缠变分自动编码器(MDVAE) ，它由两个耦合的解缠变分自动编码器(DVAE)和一个融合交换 VAE (FVAE)组成。具体来说，DVAE 的开发是为了将每种模态的原始表示分解为模态不变和模态特定的特征。FVAE 通过重构和对齐过程实现多模态数据信息的融合和交换，不需要预先提取语义嵌入。进一步提出了一种改进的反直观交叉重构方案，以提高模态不变特征的信息量和泛化能力，实现更有效的知识转移。在四个图像文本检索和两个图像素描检索数据集上的综合实验表明，该方法建立了新的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multimodal+Disentanglement+Variational+AutoEncoders+for+Zero-Shot+Cross-Modal+Retrieval)|1|
|[User-controllable Recommendation Against Filter Bubbles](https://doi.org/10.1145/3477495.3532075)|Wenjie Wang, Fuli Feng, Liqiang Nie, TatSeng Chua|Shandong University, Qingdao, China; National University of Singapore, Singapore, Singapore; University of Science and Technology of China, Hefei, China|Recommender systems usually face the issue of filter bubbles: over-recommending homogeneous items based on user features and historical interactions. Filter bubbles will grow along the feedback loop and inadvertently narrow user interests. Existing work usually mitigates filter bubbles by incorporating objectives apart from accuracy such as diversity and fairness. However, they typically sacrifice accuracy, hurting model fidelity and user experience. Worse still, users have to passively accept the recommendation strategy and influence the system in an inefficient manner with high latency, e.g., keeping providing feedback (e.g., like and dislike) until the system recognizes the user intention. This work proposes a new recommender prototype called User-Controllable Recommender System (UCRS), which enables users to actively control the mitigation of filter bubbles. Functionally, 1) UCRS can alert users if they are deeply stuck in filter bubbles. 2) UCRS supports four kinds of control commands for users to mitigate the bubbles at different granularities. 3) UCRS can respond to the controls and adjust the recommendations on the fly. The key to adjusting lies in blocking the effect of out-of-date user representations on recommendations, which contains historical information inconsistent with the control commands. As such, we develop a causality-enhanced User-Controllable Inference (UCI) framework, which can quickly revise the recommendations based on user controls in the inference stage and utilize counterfactual inference to mitigate the effect of out-of-date user representations. Experiments on three datasets validate that the UCI framework can effectively recommend more desired items based on user controls, showing promising performance w.r.t. both accuracy and diversity.|推荐系统通常面临过滤气泡的问题: 过度推荐基于用户特征和历史交互的同类项目。过滤气泡会沿着反馈回路增长，不经意间会缩小用户的兴趣。现有的工作通常通过将目标与准确性(如多样性和公平性)相结合来减少过滤泡沫。然而，他们通常牺牲准确性，损害模型的保真度和用户体验。更糟糕的是，用户必须被动地接受推荐策略，并以高延迟的低效方式影响系统，例如，不断提供反馈(例如，喜欢和不喜欢) ，直到系统认识到用户的意图。这项工作提出了一个新的推荐原型，称为用户可控推荐系统(UCRS) ，它使用户能够积极控制过滤气泡的缓解。在功能上，1) UCRS 可以提醒用户，如果他们深深陷入过滤泡。2) UCRS 支持四种控制命令，用户可以根据不同的粒度缓解气泡。3) UCRS 可以对控制措施做出响应，并在运行中调整建议。调整的关键在于阻止过时的用户表示对建议的影响，其中包含与控制命令不一致的历史信息。因此，我们开发了一个因果关系增强的用户可控推理(UCI)框架，它可以在推理阶段快速修改基于用户控制的推荐，并利用反事实推理来减轻过时用户表示的影响。在三个数据集上的实验验证了 UCI 框架基于用户控件能够有效地推荐更多期望的项目，在准确性和多样性方面都显示出良好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=User-controllable+Recommendation+Against+Filter+Bubbles)|1|
|[Evaluation of Herd Behavior Caused by Population-scale Concept Drift in Collaborative Filtering](https://doi.org/10.1145/3477495.3531792)|Chenglong Ma, Yongli Ren, Pablo Castells, Mark Sanderson|Universidad Autónoma de Madrid, Madrid , Spain; RMIT University, Melbourne, VIC, Australia|Concept drift in stream data has been well studied in machine learning applications. In the field of recommender systems, this issue is also widely observed, as known as temporal dynamics in user behavior. Furthermore, in the context of COVID-19 pandemic related contingencies, people shift their behavior patterns extremely and tend to imitate others' opinions. The changes in user behavior may not be always rational. Thus, irrational behavior may impair the knowledge learned by the algorithm. It can cause herd effects and aggravate the popularity bias in recommender systems due to the irrational behavior of users. However, related research usually pays attention to the concept drift of individuals and overlooks the synergistic effect among users in the same social group. We conduct a study on user behavior to detect the collaborative concept drifts among users. Also, we empirically study the increase of experience of individuals can weaken herding effects. Our results suggest the CF models are highly impacted by the herd behavior and our findings could provide useful implications for the design of future recommender algorithms.|流数据中的概念漂移已经在机器学习应用中得到了很好的研究。在推荐系统领域，这个问题也被广泛观察到，被称为用户行为的时间动态。此外，在与2019冠状病毒疾病相关的突发事件中，人们的行为模式会发生极大的变化，并倾向于模仿他人的观点。用户行为的变化可能并不总是理性的。因此，非理性行为可能会损害算法所学到的知识。在推荐系统中，由于用户的非理性行为，会引起羊群效应，加剧推荐系统的受欢迎程度偏差。然而，相关研究往往关注个体的概念漂移，忽视了同一社会群体中用户之间的协同效应。本文以用户行为为研究对象，检测用户之间的协作概念漂移。同时，我们实证研究了个体经验的增加会削弱羊群效应。我们的研究结果表明 CF 模型受到群体行为的高度影响，我们的研究结果可以为未来推荐算法的设计提供有用的启示。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Evaluation+of+Herd+Behavior+Caused+by+Population-scale+Concept+Drift+in+Collaborative+Filtering)|1|
|[A 'Pointwise-Query, Listwise-Document' based Query Performance Prediction Approach](https://doi.org/10.1145/3477495.3531821)|Suchana Datta, Sean MacAvaney, Debasis Ganguly, Derek Greene|Univ Coll Dublin, Dublin, Ireland; Univ Glasgow, Glasgow, Lanark, Scotland|The task of Query Performance Prediction (QPP) in Information Retrieval (IR) involves predicting the relative effectiveness of a search system for a given input query. Supervised approaches for QPP, such as NeuralQPP [24] are often trained on pairs of queries to capture their relative retrieval performance. However, point-wise approaches, such as the recently proposed BERT-QPP [1], are generally preferable for efficiency reasons. In this paper, we propose a novel end-to-end neural cross-encoder-based approach that is trained pointwise on individual queries, but listwise over the top ranked documents (split into chunks). In contrast to prior work, the network is then trained to predict the number of relevant documents in each chunk for a given query. Our method is thus a split-n-merge technique that instead of predicting the likely number of relevant documents in the top-k [1], rather predicts the number of relevant documents for each fixed chunk size p (p < k) and then aggregates them for QPP on top-k. Experiments demonstrate that our method is significantly more effective than other supervised and unsupervised QPP approaches yielding improvements of up to 30% on the TREC-DL'20 dataset and by nearly 9% for the MS MARCO Dev set.|查询性能预测的任务(QPP)在信息检索(IR)包括预测一个给定输入查询的搜索系统的相对有效性。QPP 的有监督的方法，如 NeuralQPP [24] ，经常在查询对上进行训练，以捕获它们的相对检索性能。然而，出于效率的考虑，最近提出的 BERT-QPP [1]等逐点方法通常是可取的。在本文中，我们提出了一种新的端到端神经交叉编码器为基础的方法，是点式训练的个别查询，但列表上的排名最高的文档(分成块)。与先前的工作相反，网络然后被训练来预测给定查询的每个块中相关文档的数量。因此，我们的方法是一种拆分-n-merge 技术，它不是预测 top-k [1]中相关文档的可能数量，而是预测每个固定块大小 p (p < k)的相关文档的数量，然后将它们聚合为 top-k 上的 QPP。实验表明，我们的方法比其他有监督和无监督的 QPP 方法显着更有效，在 TREC-DL’20数据集上提高了30% ，在 MS MARCO Dev 集上提高了近9% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+'Pointwise-Query,+Listwise-Document'+based+Query+Performance+Prediction+Approach)|1|
|[AHP: Learning to Negative Sample for Hyperedge Prediction](https://doi.org/10.1145/3477495.3531836)|Hyunjin Hwang, Seungwoo Lee, Chanyoung Park, Kijung Shin|KAIST, Daejeon, Republic of Korea; KAIST, Seoul, Republic of Korea|Hypergraphs (i.e., sets of hyperedges) naturally represent group relations (e.g., researchers co-authoring a paper and ingredients used together in a recipe), each of which corresponds to a hyperedge (i.e., a subset of nodes). Predicting future or missing hyperedges bears significant implications for many applications (e.g., collaboration and recipe recommendation). What makes hyperedge prediction particularly challenging is the vast number of non-hyperedge subsets, which grows exponentially with the number of nodes. Since it is prohibitive to use all of them as negative examples for model training, it is inevitable to sample a very small portion of them, and to this end, heuristic sampling schemes have been employed. However, trained models suffer from poor generalization capability for examples of different natures. In this paper, we propose AHP, an adversarial training-based hyperedge-prediction method. It learns to sample negative examples without relying on any heuristic schemes. Using six real hypergraphs, we show that AHP generalizes better to negative examples of various natures. It yields up to 28.2% higher AUROC than the best existing methods and often even outperforms its variants with sampling schemes tailored to test sets.|超图(即超边集合)自然地表示群体关系(例如，研究人员共同撰写一篇论文和一个食谱中的成分) ，每一个都对应于一个超边(即节点的子集)。预测未来或缺失的超边界对于许多应用程序(例如，协作和菜谱推荐)具有重要意义。使得超边缘预测特别具有挑战性的是大量的非超边缘子集，它们随着节点的数量呈指数增长。由于在模型训练中禁止使用所有的负例子，因此不可避免地需要对其中的一小部分进行抽样，为此，采用了启发式抽样方案。然而，经过训练的模型对于不同性质的例子的泛化能力较差。本文提出了一种基于对抗训练的 AHP 超边缘预测方法。它学习不依赖任何启发式方案来抽样否定的例子。利用六个实超图，我们证明了层次分析法能够更好地推广各种性质的负例子。与现有的最佳方法相比，它的 AUROC 提高了28.2% ，而且通常在根据测试集量身定制的抽样方案上甚至优于其变体。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AHP:+Learning+to+Negative+Sample+for+Hyperedge+Prediction)|1|
|[Interactive Query Clarification and Refinement via User Simulation](https://doi.org/10.1145/3477495.3531871)|Pierre Erbacher, Ludovic Denoyer, Laure Soulier|Sorbonne Université, Paris, France|When users initiate search sessions, their query are often ambiguous or might lack of context; this resulting in non-efficient document ranking. Multiple approaches have been proposed by the Information Retrieval community to add context and retrieve documents aligned with users' intents. While some work focus on query disambiguation using users' browsing history, a recent line of work proposes to interact with users by asking clarification questions or/and proposing clarification panels. However, these approaches count either a limited number (i.e., 1) of interactions with user or log-based interactions. In this paper, we propose and evaluate a fully simulated query clarification framework allowing multi-turn interactions between IR systems and user agents.|当用户启动搜索会话时，他们的查询通常是模棱两可的，或者可能缺乏上下文; 这导致文档排序效率低下。信息检索社区提出了多种方法来添加上下文和检索符合用户意图的文档。虽然一些工作侧重于利用用户的浏览历史消除查询歧义，但最近的一项工作建议通过提出澄清问题或/和提出澄清面板与用户进行交互。但是，这些方法只计算与用户或基于日志的交互的有限数量(即1)。本文提出并评估了一个完全模拟的查询澄清框架，该框架允许信息检索系统和用户代理之间进行多次交互。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interactive+Query+Clarification+and+Refinement+via+User+Simulation)|1|
|[Explainable Session-based Recommendation with Meta-path Guided Instances and Self-Attention Mechanism](https://doi.org/10.1145/3477495.3531895)|Jiayin Zheng, Juanyun Mai, Yanlong Wen|Nankai University, Tianjin, China|Session-based recommendation (SR) gains increasing popularity because it helps greatly maintain users' privacy. Aside from its efficacy, explainability is also critical for developing a successful SR model, since it can improve the persuasiveness of the results, the users' satisfaction, and the debugging efficiency. However, the majority of current SR models are unexplainable and even those that claim to be interpretable cannot provide clear and convincing explanations of users' intentions and how they influence the models' decisions. To solve this problem, in this research, we propose a meta-path guided model which uses path instances to capture item dependencies, explicitly reveal the underlying motives, and illustrate the entire reasoning process. To begin with, our model explores meta-path guided instances and leverages the multi-head self-attention mechanism to disclose the hidden motivations beneath these path instances. To comprehensively model the user interest and interest shifting, we search paths in both adjacent and non-adjacent items. Then, we update item representations by incorporating the user-item interactions and meta-path-based context sequentially. Compared with recent strong baselines, our method is competent to the SOTA performance on three datasets and meanwhile provides sound and clear explanations.|基于会话的推荐(SR)越来越受欢迎，因为它极大地保护了用户的隐私。除了功效之外，可解释性对于建立一个成功的 SR 模型也是至关重要的，因为它可以提高结果的说服力、用户的满意度和调试效率。然而，目前大多数 SR 模型是无法解释的，甚至那些声称可以解释的模型也不能提供清晰和令人信服的解释，说明用户的意图以及他们如何影响模型的决策。为了解决这一问题，本研究提出了一个元路径引导模型，该模型利用路径实例来捕获项目依赖，明确揭示项目依赖背后的动机，并说明整个推理过程。首先，我们的模型探索元路径引导实例，并利用多头自我关注机制来揭示这些路径实例下隐藏的动机。为了对用户兴趣和兴趣转移进行综合建模，我们在相邻和非相邻项目中搜索路径。然后，通过合并用户-项目交互和基于元路径的上下文顺序更新项目表示。与最近的强基线相比，我们的方法能够胜任三个数据集的 SOTA 性能，同时提供了合理而清晰的解释。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explainable+Session-based+Recommendation+with+Meta-path+Guided+Instances+and+Self-Attention+Mechanism)|1|
|[QSG Transformer: Transformer with Query-Attentive Semantic Graph for Query-Focused Summarization](https://doi.org/10.1145/3477495.3531901)|Choongwon Park, Youngjoong Ko|Sungkyunkwan University, Suwon-si, Gyeonggi-do, Republic of Korea|Query-Focused Summarization (QFS) is a task that aims to extract essential information from a long document and organize it into a summary that can answer a query. Recently, Transformer-based summarization models have been widely used in QFS. However, the simple Transformer architecture cannot utilize the relationships between distant words and information from a query directly. In this study, we propose the QSG Transformer, a novel QFS model that leverages structure information on Query-attentive Semantic Graph (QSG) to address these issues. Specifically, in the QSG Transformer, QSG node representation is improved by a proposed query-attentive graph attention network, which spreads the information of the query node into QSG using Personalized PageRank, and it is used to generate a summary that better reflects the information from the relationships of a query and document. The proposed method is evaluated on two QFS datasets, and it achieves superior performances over the state-of-the-art models.|查询聚焦摘要(Query-Focus Summarization，QFS)是一项任务，旨在从长文档中提取重要信息，并将其组织成可以回答查询的摘要。近年来，基于变压器的汇总模型在 QFS 中得到了广泛的应用。但是，简单的 Transformer 体系结构不能直接利用远程单词和来自查询的信息之间的关系。在这项研究中，我们提出了 QSG 转换器，一个新的 QFS 模型，利用查询注意语义图(QSG)的结构信息来解决这些问题。提出了一种基于查询-注意图注意网络的 QSG 节点表示方法，该方法利用个性化 PageRank 将查询节点的信息扩展到 QSG 中，生成能够更好地反映查询和文档关系信息的摘要。在两个 QFS 数据集上对该方法进行了评估，结果表明该方法的性能优于现有的模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=QSG+Transformer:+Transformer+with+Query-Attentive+Semantic+Graph+for+Query-Focused+Summarization)|1|
|[Next Point-of-Interest Recommendation with Auto-Correlation Enhanced Multi-Modal Transformer Network](https://doi.org/10.1145/3477495.3531905)|Yanjun Qin, Yuchen Fang, Haiyong Luo, Fang Zhao, Chenxing Wang|Institute of Computing Technology Chinese Academy Of Sciences, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China|Next Point-of-Interest (POI) recommendation is a pivotal issue for researchers in the field of location-based social networks. While many recent efforts show the effectiveness of recurrent neural network-based next POI recommendation algorithms, several important challenges have not been well addressed yet: (i) The majority of previous models only consider the dependence of consecutive visits, while ignoring the intricate dependencies of POIs in traces; (ii) The nature of hierarchical and the matching of sub-sequence in POI sequences are hardly model in prior methods; (iii) Most of the existing solutions neglect the interactions between two modals of POI and the density category. To tackle the above challenges, we propose an auto-correlation enhanced multi-modal Transformer network (AutoMTN) for the next POI recommendation. Particularly, AutoMTN uses the Transformer network to explicitly exploits connections of all the POIs along the trace. Besides, to discover the dependencies at the sub-sequence level and attend to cross-modal interactions between POI and category sequences, we replace self-attention in Transformer with the auto-correlation mechanism and design a multi-modal network. Experiments results on two real-world datasets demonstrate the ascendancy of AutoMTN contra state-of-the-art methods in the next POI recommendation.|下一个兴趣点(POI)推荐是基于位置的社交网络研究领域的一个关键问题。尽管最近的许多努力显示了基于循环神经网络的下一个 POI 推荐算法的有效性，但是一些重要的挑战还没有得到很好的解决: (i)以前的大多数模型只考虑连续访问的依赖性，而忽略了跟踪中 POI 的复杂依赖性; (ii) POI 序列中分层的性质和子序列的匹配在以前的方法中几乎不是模型; (iii)大多数现有的解决方案忽略了 POI 的两个模态和密度类别之间的相互作用。为了应对上述挑战，我们提出了一个自相关增强型多模态变压器网络(AutoMTN)作为下一个 POI 建议。特别是，AutoMTN 使用 Transformer 网络显式地利用跟踪过程中所有 POI 的连接。此外，为了发现子序列层次上的依赖关系，并处理 POI 与类别序列之间的跨模态交互作用，我们用自相关机制取代了主变压器中的自注意机制，并设计了一个多模态网络。在两个实际数据集上的实验结果表明，AutoMTN 方法在下一个 POI 推荐中占优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Next+Point-of-Interest+Recommendation+with+Auto-Correlation+Enhanced+Multi-Modal+Transformer+Network)|1|
|[Neutralizing Popularity Bias in Recommendation Models](https://doi.org/10.1145/3477495.3531907)|Guipeng Xv, Chen Lin, Hui Li, Jinsong Su, Weiyao Ye, Yewang Chen|Huaqiao University, Xiamen, China; Xiamen University, Xiamen, China|Most existing recommendation models learn vectorized representations for items, i.e., item embeddings to make predictions. Item embeddings inherit popularity bias from the data, which leads to biased recommendations. We use this observation to design two simple and effective strategies, which can be flexibly plugged into different backbone recommendation models, to learn popularity neutral item representations. One strategy isolates popularity bias in one embedding direction and neutralizes the popularity direction post-training. The other strategy encourages all embedding directions to be disentangled and popularity neutral. We demonstrate that the proposed strategies outperform state-of-the-art debiasing methods on various real-world datasets, and improve recommendation quality of shallow and deep backbone models.|大多数现有的推荐模型学习项目的向量化表示，即项目嵌入来进行预测。项目嵌入从数据中继承了受欢迎程度的偏差，从而导致偏差推荐。利用这一观察结果，我们设计了两个简单有效的策略，可以灵活地插入到不同的骨干推荐模型中，来学习流行度中性的项目表示。一种策略是在一个嵌入方向上隔离流行偏差，在训练后中和流行方向。另一种策略鼓励所有的嵌入方向都是分离的和流行中性的。实验结果表明，所提出的策略优于现有的各种实际数据集的去偏方法，提高了浅层和深层骨架模型的推荐质量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neutralizing+Popularity+Bias+in+Recommendation+Models)|1|
|[ROGUE: A System for Exploratory Search of GANs](https://doi.org/10.1145/3477495.3531675)|Yang Liu, Alan Medlar, Dorota Glowacka|University of Helsinki, Helsinki, Finland|Image retrieval from generative adversarial networks (GANs) is challenging for several reasons. First, there are no clear mappings between the GAN's latent space and useful semantic features, making it difficult for users to navigate. Second, the number of unique images that can be generated is exceptionally high, taxing the scaling properties of existing search algorithms. In this article, we present ROGUE, a system to support exploratory search of images generated from GANs. We demonstrate how to implement features that are commonly found in exploratory search interfaces, such as faceted search and relevance feedback, in the context of GAN search. We additionally use reinforcement learning to help users navigate the image space [8], trading off exploration (showing diverse images) and exploitation (showing images predicted to receive positive relevance feedback). Finally, we present a usability study where participants were situated in the role of a casting director who needs to explore actors' headshots for an upcoming movie. The system obtained an average SUS score of 72.8 and all participants reported being either satisfied or very satisfied with the images they identified with the system. The system is shown in this accompanying video: https://vimeo.com/680036160.|基于生成对抗网络(GAN)的图像检索是一个具有挑战性的问题。首先，在 GAN 的潜在空间和有用的语义特性之间没有清晰的映射，这使得用户很难导航。其次，可以生成的独特图像的数量异常之高，这对现有搜索算法的缩放特性造成了压力。在本文中，我们提出了 ROGUE，一个系统，以支持探索性搜索的图像生成的 GAN。我们将演示如何在广域网搜索环境中实现探索性搜索界面中常见的特性，例如分面搜索和关联反馈搜索。此外，我们还使用强化学习来帮助用户浏览图片空间，在探索(显示不同的图片)和利用(显示预计会收到正面关联反馈的图片)之间进行权衡。最后，我们提出了一个可用性研究，其中的参与者位于一个角色的选角导演谁需要探索演员的头像为即将到来的电影。该系统获得的 SUS 平均分为72.8分，所有参与者报告说，他们对系统识别的图像要么感到满意，要么非常满意。该系统在下面的视频中显示:  https://vimeo.com/680036160。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ROGUE:+A+System+for+Exploratory+Search+of+GANs)|1|
|[Searching for a New and Better Future of Work](https://doi.org/10.1145/3477495.3532088)|Jaime Teevan|Microsoft, Redmond, WA, USA|Search engines were one of the first intelligent cloud-based applications that people used to get things done, and they have since become an extremely important productivity tool. This is in part because much of what a person is doing when they search is thinking. Search engines do not merely support that thinking, however, but can also actually shape it. For example, some search results are more likely to spur learning than others and influence a person's future queries [1]. This means that as information retrieval researchers the new approaches that we develop can actively shape the future of work. The world is now in the middle of the most significant change to work practices in a generation, and it is one that will make search technology even more central to work in the years to come. For the past several millennia, space was the primary technology that people used to get things done. The coming Hybrid Work Era, however, will be shaped by digital technology. The recent rapid shift to remote work significantly accelerated the digital transformation already underway at many companies, and new types of work-related data are now being generated at an unprecedented rate. For example, the use of meeting recordings in Microsoft Stream has more than doubled from March 2020 to February 2022 [2]. Knowledge exists in this newly captured data, but figuring out how to make sense of it is overwhelming. The information retrieval community knows how to process large amounts of data, build usable intelligent systems, and learn from behavioral data, and we have a unique opportunity right now to apply this expertise to new corpora and in new scenarios in a meaningful way. In this talk I will give an overview of what research tells us about emerging work practices, and explore how the SIGIR community can build on these findings to help create a new - and better - future of work.|搜索引擎是人们用来完成工作的第一批基于云的智能应用程序之一，自那以后，它们已经成为极其重要的生产力工具。这在一定程度上是因为一个人在搜索时所做的大部分事情都是在思考。然而，搜索引擎不仅仅支持这种想法，而且实际上还可以塑造这种想法。例如，一些搜索结果比其他结果更有可能刺激学习，并影响一个人未来的查询[1]。这意味着，作为信息检索研究人员，我们开发的新方法可以积极地塑造未来的工作。这个世界现在正处于一代人以来最重大的工作实践变革之中，这将使搜索技术在未来几年的工作中变得更加重要。在过去的几千年里，太空是人们用来完成任务的主要技术。然而，即将到来的混合工作时代将由数字技术塑造。最近向远程工作的迅速转变极大地加速了许多公司已经在进行的数字化转变，现在正以前所未有的速度产生新类型的与工作有关的数据。例如，从2020年3月到2022年2月，微软 Stream 会议录音的使用量增加了一倍多[2]。知识存在于这些新捕获的数据中，但是弄清楚如何理解这些数据是非常困难的。信息检索社区知道如何处理大量的数据，构建可用的智能系统，并从行为数据中学习，我们现在有一个独特的机会，可以将这些专业知识以一种有意义的方式应用到新的语料库和新的场景中。在这个演讲中，我将概述研究告诉我们的新兴工作实践，并探讨 SIGIR 社区如何能够建立在这些发现的基础上，帮助创造一个新的、更好的工作未来。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Searching+for+a+New+and+Better+Future+of+Work)|1|
|[INMO: A Model-Agnostic and Scalable Module for Inductive Collaborative Filtering](https://doi.org/10.1145/3477495.3532000)|Yunfan Wu, Qi Cao, Huawei Shen, Shuchang Tao, Xueqi Cheng|Institute of Computing Technology, Chinese Academy of Sciences & University of Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China|Collaborative filtering is one of the most common scenarios and popular research topics in recommender systems. Among existing methods, latent factor models, i.e., learning a specific embedding for each user/item by reconstructing the observed interaction matrix, have shown excellent performances. However, such user-specific and item-specific embeddings are intrinsically transductive, making it difficult for them to deal with new users and new items unseen during training. Besides, the number of model parameters heavily depends on the number of all users and items, restricting their scalability to real-world applications. To solve the above challenges, in this paper, we propose a novel model-agnostic and scalable Inductive Embedding Module for collaborative filtering, namely INMO. INMO generates the inductive embeddings for users (items) by characterizing their interactions with some template items (template users), instead of employing an embedding lookup table. Under the theoretical analysis, we further propose an effective indicator for the selection of template users and template items. Our proposed INMO can be attached to existing latent factor models as a pre-module, inheriting the expressiveness of backbone models, while bringing the inductive ability and reducing model parameters. We validate the generality of INMO by attaching it to Matrix Factorization (MF) and LightGCN, which are two representative latent factor models for collaborative filtering. Extensive experiments on three public benchmarks demonstrate the effectiveness and efficiency of INMO in both transductive and inductive recommendation scenarios.|协同过滤是推荐系统中最常见的场景和流行的研究主题之一。在已有的方法中，潜因子模型(即通过重构观察到的交互矩阵学习每个用户/项目的特定嵌入)表现出了良好的性能。然而，这种特定于用户和特定于项目的嵌入在本质上具有传导性，使他们难以处理培训期间看不到的新用户和新项目。此外，模型参数的数量很大程度上取决于所有用户和项目的数量，限制了它们对现实应用程序的可伸缩性。为了解决上述挑战，本文提出了一种新颖的模型无关性和可扩展性的感应嵌入模块，即 INMO 协同过滤。INMO 通过描述用户与某些模板项(模板用户)的交互来为用户(项)生成归纳嵌入，而不是使用嵌入查找表。在理论分析的基础上，进一步提出了模板用户和模板项选择的有效指标。我们提出的 INMO 可以作为一个预模块附加到现有的潜因子模型，继承骨干模型的表达能力，同时带来归纳能力和减少模型参数。我们通过将 INMO 附加到矩阵分解(MF)和 LightGCN (这两个是协同过滤的两个代表性潜在因素模型)来验证 INMO 的普遍性。对三个公共基准进行的广泛试验表明，INMO 在推导和归纳推荐两种情况下都具有有效性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=INMO:+A+Model-Agnostic+and+Scalable+Module+for+Inductive+Collaborative+Filtering)|1|
|[Conversational Question Answering on Heterogeneous Sources](https://doi.org/10.1145/3477495.3531815)|Philipp Christmann, Rishiraj Saha Roy, Gerhard Weikum|Max Planck Institute for Informatics, Saarbrücken, Germany|Conversational question answering (ConvQA) tackles sequential information needs where contexts in follow-up questions are left implicit. Current ConvQA systems operate over homogeneous sources of information: either a knowledge base (KB), or a text corpus, or a collection of tables. This paper addresses the novel issue of jointly tapping into all of these together, this way boosting answer coverage and confidence. We present CONVINSE, an end-to-end pipeline for ConvQA over heterogeneous sources, operating in three stages: i) learning an explicit structured representation of an incoming question and its conversational context, ii) harnessing this frame-like representation to uniformly capture relevant evidences from KB, text, and tables, and iii) running a fusion-in-decoder model to generate the answer. We construct and release the first benchmark, ConvMix, for ConvQA over heterogeneous sources, comprising 3000 real-user conversations with 16000 questions, along with entity annotations, completed question utterances, and question paraphrases. Experiments demonstrate the viability and advantages of our method, compared to state-of-the-art baselines.|会话问题回答(ConvQA)处理后续问题中的上下文隐含的连续信息需求。当前的 ConvQA 系统是在同一信息源上运行的: 知识库(KB)、文本语料库或表集合。本文提出了一个新颖的问题，即共同利用所有这些问题，这样可以提高答案的覆盖面和信心。我们提出 CONVINSE，一种针对异构来源的 ConvQA 端到端管道，分三个阶段进行操作: i)学习传入问题及其会话上下文的显式结构化表示，ii)利用这种框架样表示来统一地从知识库，文本和表格中捕获相关证据，以及 iii)运行融合解码模型来生成答案。我们构建并发布了第一个基准，ConvMix，用于异构来源的 ConvQA，包括3000个实际用户对话和16000个问题，以及实体注释，完成的问题语句和问题转述。与最先进的基线相比，实验证明了我们方法的可行性和优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Conversational+Question+Answering+on+Heterogeneous+Sources)|1|
|[COSPLAY: Concept Set Guided Personalized Dialogue Generation Across Both Party Personas](https://doi.org/10.1145/3477495.3531957)|Chen Xu, Piji Li, Wei Wang, Haoran Yang, Siyun Wang, Chuangbai Xiao|Tencent AI Lab, Shenzhen, AA, China; Beijing University of Technology, Beijing, AA, China; Tsinghua University, Beijing, AA, China; University of Southern California, Los Angeles, AA, USA; The Chinese University of Hong Kong, Hong Kong, AA, China|Maintaining a consistent persona is essential for building a human-like conversational model. However, the lack of attention to the partner makes the model more egocentric: they tend to show their persona by all means such as twisting the topic stiffly, pulling the conversation to their own interests regardless, and rambling their persona with little curiosity to the partner. In this work, we propose COSPLAY(COncept Set guided PersonaLized dialogue generation Across both partY personas) that considers both parties as a "team": expressing self-persona while keeping curiosity toward the partner, leading responses around mutual personas, and finding the common ground. Specifically, we first represent self-persona, partner persona and mutual dialogue all in the concept sets. Then, we propose the Concept Set framework with a suite of knowledge-enhanced operations to process them such as set algebras, set expansion, and set distance. Based on these operations as medium, we train the model by utilizing 1) concepts of both party personas, 2) concept relationship between them, and 3) their relationship to the future dialogue. Extensive experiments on a large public dataset, Persona-Chat, demonstrate that our model outperforms state-of-the-art baselines for generating less egocentric, more human-like, and higher quality responses in both automatic and human evaluations.|保持一致的人物角色对于构建类似人类的会话模型至关重要。然而，缺乏对伴侣的关注使得这种模式更加自我中心: 他们倾向于通过各种方式来展示自己的角色，比如僵硬地扭曲话题，不顾一切地把谈话拉向自己的兴趣，对伴侣毫无好奇心地胡扯自己的角色。在这项工作中，我们提出 COSPLAY (概念集引导的个性化对话生成跨越双方角色) ，认为双方是一个“团队”: 表达自我角色，同时保持对伴侣的好奇心，引导对相互角色的反应，并找到共同点。具体来说，我们首先在概念集中表现自我人格、伴侣人格和相互对话。然后，我们提出了概念集框架和一套知识增强操作来处理它们，例如集代数、集展开和集距离。基于这些操作作为媒介，我们利用1)双方角色的概念，2)他们之间的概念关系，3)他们与未来对话的关系来训练模型。在一个大型公共数据集“人物聊天”上进行的大量实验表明，我们的模型在自动和人工评估中产生更少的自我中心，更多的人性化和更高质量的响应方面优于最先进的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=COSPLAY:+Concept+Set+Guided+Personalized+Dialogue+Generation+Across+Both+Party+Personas)|1|
|[KETCH: Knowledge Graph Enhanced Thread Recommendation in Healthcare Forums](https://doi.org/10.1145/3477495.3532008)|Limeng Cui, Dongwon Lee|The Pennsylvania State University, University Park, PA, USA|Health thread recommendation methods aim to suggest the most relevant existing threads for a user. Most of the existing methods tend to rely on modeling the post contents to retrieve relevant answers. However, some posts written by users with different clinical conditions can be lexically similar, as unrelated diseases (e.g., Angina and Osteoporosis) may have the same symptoms (e.g., back pain), yet irrelevant threads to a user. Therefore, it is critical to not only consider the connections between users and threads, but also the descriptions of users' symptoms and clinical conditions. In this paper, towards this problem of thread recommendation in online healthcare forums, we propose a knowledge graph enhanced Threads Recommendation (KETCH) model, which leverages graph neural networks to model the interactions among users and threads, and learn their representations. In our model, the users, threads and posts are three types of nodes in a graph, linked through their associations. KETCH uses the message passing strategy by aggregating information along with the network. In addition, we introduce a knowledge-enhanced attention mechanism to capture the latent conditions and symptoms. We also apply the method to the task of predicting the side effects of drugs, to show that KETCH has the potential to complement the medical knowledge graph. Comparing with the best results of seven competing methods, in terms of MRR, KETCH outperforms all methods by at least 0.125 on the MedHelp dataset, 0.048 on the Patient dataset and 0.092 on HealthBoards dataset, respectively. We release the source code of KETCH at: https://github.com/cuilimeng/KETCH.|健康线程推荐方法旨在为用户推荐最相关的现有线程。现有的大多数方法倾向于依赖于对文章内容进行建模来检索相关的答案。然而，不同临床状况的用户写的一些帖子在词汇上可能是相似的，因为不相关的疾病(如心绞痛和骨质疏松症)可能有相同的症状(如背痛) ，但对用户来说是不相关的。因此，不仅要考虑用户和线程之间的连接，还要考虑用户症状和临床状况的描述。针对在线医疗论坛中的线程推荐问题，提出了一种知识图增强型线程推荐(KETCH)模型，该模型利用图神经网络对用户和线程之间的交互进行建模，并学习用户和线程之间的表示。在我们的模型中，用户、线程和帖子是图中的三种类型的节点，通过它们的关联进行链接。KETCH 通过将信息与网络一起聚合来使用消息传递策略。此外，我们还引入了一种知识增强的注意机制来捕捉潜在的条件和症状。我们还将该方法应用于预测药物副作用的任务，表明 KETCH 具有补充医学知识图谱的潜力。与七种竞争方法的最佳结果相比，就 MRR 而言，KETCH 在 MedHelp 数据集上至少优于所有方法0.125，在患者数据集上优于0.048，在 HealthBoards 数据集上优于0.092。我们在 https://github.com/cuilimeng/KETCH 发布 KETCH 的源代码。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=KETCH:+Knowledge+Graph+Enhanced+Thread+Recommendation+in+Healthcare+Forums)|1|
|[Explainable Legal Case Matching via Inverse Optimal Transport-based Rationale Extraction](https://doi.org/10.1145/3477495.3531974)|Weijie Yu, Zhongxiang Sun, Jun Xu, Zhenhua Dong, Xu Chen, Hongteng Xu, JiRong Wen|Renmin University of China, Beijing, China; Huawei Noah's Ark Lab, Shenzhen, China|As an essential operation of legal retrieval, legal case matching plays a central role in intelligent legal systems. This task has a high demand on the explainability of matching results because of its critical impacts on downstream applications --- the matched legal cases may provide supportive evidence for the judgments of target cases and thus influence the fairness and justice of legal decisions. Focusing on this challenging task, we propose a novel and explainable method, namely IOT-Match, with the help of computational optimal transport, which formulates the legal case matching problem as an inverse optimal transport (IOT) problem. Different from most existing methods, which merely focus on the sentence-level semantic similarity between legal cases, our IOT-Match learns to extract rationales from paired legal cases based on both semantics and legal characteristics of their sentences. The extracted rationales are further applied to generate faithful explanations and conduct matching. Moreover, the proposed IOT-Match is robust to the alignment label insufficiency issue commonly in practical legal case matching tasks, which is suitable for both supervised and semi-supervised learning paradigms. To demonstrate the superiority of our IOT-Match method and construct a benchmark of explainable legal case matching task, we not only extend the well-known Challenge of AI in Law (CAIL) dataset but also build a new Explainable Legal cAse Matching (ELAM) dataset, which contains lots of legal cases with detailed and explainable annotations. Experiments on these two datasets show that our IOT-Match outperforms state-of-the-art methods consistently on matching prediction, rationale extraction, and explanation generation.|法律案件匹配作为法律检索的一项基本操作，在智能法律系统中起着核心作用。这项任务对匹配结果的解释性要求很高，因为它对下游应用产生了重要影响——匹配的法律案件可以为目标案件的判决提供支持性证据，从而影响法律判决的公正性和正义性。针对这一具有挑战性的任务，我们提出了一种新颖的解释方法，即物联网匹配(IOT-Match) ，借助于计算最优传输，将法律案例匹配问题表述为一个逆最优传输(IOT)问题。不同于现有的方法，我们的 IOT-Match 仅仅着眼于法律案件之间的句子层面的语义相似性，而是学习从成对的法律案件中根据其句子的语义和法律特征提取理据。提取的理论基础进一步应用于产生忠实的解释和进行匹配。此外，建议的物联网匹配对于实际案例匹配任务中常见的校准标签不足问题具有稳健性，适用于监督范式和半监督学习范式。为了证明我们的物联网匹配方法的优越性，构建一个可解释的法律案例匹配任务的基准，我们不仅扩展了著名的 CAIL 数据集，而且建立了一个新的可解释的法律案例匹配(ELAM)数据集，其中包含了大量的法律案例和详细的可解释的注释。在这两个数据集上的实验表明，我们的 IOT-Match 方法在匹配预测、理论提取和解释生成方面都优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explainable+Legal+Case+Matching+via+Inverse+Optimal+Transport-based+Rationale+Extraction)|1|
|[Optimizing Generalized Gini Indices for Fairness in Rankings](https://doi.org/10.1145/3477495.3532035)|Virginie Do, Nicolas Usunier|Meta AI Research, Paris, France; Meta AI Research & Université Paris Dauphine-PSL, Paris, France|There is growing interest in designing recommender systems that aim at being fair towards item producers or their least satisfied users. Inspired by the domain of inequality measurement in economics, this paper explores the use of generalized Gini welfare functions (GGFs) as a means to specify the normative criterion that recommender systems should optimize for. GGFs weight individuals depending on their ranks in the population, giving more weight to worse-off individuals to promote equality. Depending on these weights, GGFs minimize the Gini index of item exposure to promote equality between items, or focus on the performance on specific quantiles of least satisfied users. GGFs for ranking are challenging to optimize because they are non-differentiable. We resolve this challenge by leveraging tools from non-smooth optimization and projection operators used in differentiable sorting. We present experiments using real datasets with up to 15k users and items, which show that our approach obtains better trade-offs than the baselines on a variety of recommendation tasks and fairness criteria.|人们对设计推荐系统越来越感兴趣，这种系统旨在公平对待项目生产者或其最不满意的用户。受经济学中不平等测度领域的启发，本文探讨了广义基尼福利函数(GGFs)作为指定推荐系统优化标准的一种方法。GGFs 根据个体在人口中的等级来衡量个体的重量，将更多的重量赋予经济状况较差的个体，以促进平等。根据这些权重，GGFs 最小化项目曝光的基尼指数，以促进项目之间的平等，或者关注最不满意用户的特定分位数的性能。用于排名的 GGF 很难优化，因为它们是不可微的。我们通过利用非光滑优化工具和可微排序中使用的投影运算符来解决这一挑战。我们提出的实验使用真实的数据集多达15000个用户和项目，这表明我们的方法获得更好的权衡比基线的各种推荐任务和公平性标准。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+Generalized+Gini+Indices+for+Fairness+in+Rankings)|1|
|[Video Moment Retrieval from Text Queries via Single Frame Annotation](https://doi.org/10.1145/3477495.3532078)|Ran Cui, Tianwen Qian, Pai Peng, Elena Daskalaki, Jingjing Chen, Xiaowei Guo, Huyang Sun, YuGang Jiang|bilibili, Shanghai, China; Fudan University, Shanghai, China; The Australian National University, Canberra, Australia|Video moment retrieval aims at finding the start and end timestamps of a moment (part of a video) described by a given natural language query. Fully supervised methods need complete temporal boundary annotations to achieve promising results, which is costly since the annotator needs to watch the whole moment. Weakly supervised methods only rely on the paired video and query, but the performance is relatively poor. In this paper, we look closer into the annotation process and propose a new paradigm called "glance annotation". This paradigm requires the timestamp of only one single random frame, which we refer to as a "glance", within the temporal boundary of the fully supervised counterpart. We argue this is beneficial because comparing to weak supervision, trivial cost is added yet more potential in performance is provided. Under the glance annotation setting, we propose a method named as Video moment retrieval via Glance Annotation (ViGA) based on contrastive learning. ViGA cuts the input video into clips and contrasts between clips and queries, in which glance guided Gaussian distributed weights are assigned to all clips. Our extensive experiments indicate that ViGA achieves better results than the state-of-the-art weakly supervised methods by a large margin, even comparable to fully supervised methods in some cases.|视频矩检索的目的是查找给定的自然语言查询所描述的矩(视频的一部分)的开始和结束时间戳。完全监督的方法需要完整的时间边界注释来获得有希望的结果，这是昂贵的，因为注释者需要观察整个时刻。弱监督方法只依赖于配对视频和查询，但性能相对较差。在本文中，我们仔细研究了注释过程，并提出了一个新的范式称为“一瞥注释”。这个范例只需要一个随机帧的时间戳，我们称之为“一瞥”，在完全监督的对应物的时间边界内。我们认为这是有益的，因为相对于薄弱的监督，微不足道的成本增加了更多的性能潜力提供。在目视注释设置下，提出了一种基于对比学习的目视注释视频矩检索方法。ViGA 将输入的视频剪切成片段，并在片段和查询之间进行对比，给所有片段分配目视导引的高斯分布权值。我们的大量实验表明，ViGA 比最先进的弱监督方法获得了更好的结果，甚至在某些情况下可以与全监督方法相媲美。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Video+Moment+Retrieval+from+Text+Queries+via+Single+Frame+Annotation)|1|
|[You Need to Read Again: Multi-granularity Perception Network for Moment Retrieval in Videos](https://doi.org/10.1145/3477495.3532083)|Xin Sun, Xuan Wang, Jialin Gao, Qiong Liu, Xi Zhou|Shanghai Jiaotong University, Shanghai, China; CloudWalk Technology Co., Ltd, Shanghai, China; Peking University, Beijing, China|Moment retrieval in videos is a challenging task that aims to retrieve the most relevant video moment in an untrimmed video given a sentence description. Previous methods tend to perform self-modal learning and cross-modal interaction in a coarse manner, which neglect fine-grained clues contained in video content, query context, and their alignment. To this end, we propose a novel Multi-Granularity Perception Network (MGPN) that perceives intra-modality and inter-modality information at a multi-granularity level. Specifically, we formulate moment retrieval as a multi-choice reading comprehension task and integrate human reading strategies into our framework. A coarse-grained feature encoder and a co-attention mechanism are utilized to obtain a preliminary perception of intra-modality and inter-modality information. Then a fine-grained feature encoder and a conditioned interaction module are introduced to enhance the initial perception inspired by how humans address reading comprehension problems. Moreover, to alleviate the huge computation burden of some existing methods, we further design an efficient choice comparison module and reduce the hidden size with imperceptible quality loss. Extensive experiments on Charades-STA, TACoS, and ActivityNet Captions datasets demonstrate that our solution outperforms existing state-of-the-art methods.|视频中的瞬间检索是一项具有挑战性的任务，其目的是在给定句子描述的未修剪视频中检索最相关的视频瞬间。以往的方法倾向于以粗糙的方式进行自模态学习和跨模态交互，忽略了视频内容、查询上下文及其对齐中包含的细粒度线索。为此，我们提出了一种新的多粒度感知网络(MGPN) ，它在多粒度级别上感知模式内和模式间的信息。具体来说，我们把瞬间提取作为一项多项选择的阅读理解任务，并将人类阅读策略整合到我们的框架中。利用粗粒度特征编码器和共注意机制获得初步的模式内和模式间信息感知。然后引入细粒度特征编码器和条件交互模块，以增强受人类处理阅读理解问题启发而产生的初始感知。此外，为了减轻现有方法的巨大计算负担，我们进一步设计了一个有效的选择比较模块，减少了隐藏的大小和不易察觉的质量损失。对 Charades-STA、 TACoS 和 ActivityNet Captions 数据集的大量实验表明，我们的解决方案优于现有的最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=You+Need+to+Read+Again:+Multi-granularity+Perception+Network+for+Moment+Retrieval+in+Videos)|1|
|[Personalized Abstractive Opinion Tagging](https://doi.org/10.1145/3477495.3532037)|Mengxue Zhao, Yang Yang, Miao Li, Jingang Wang, Wei Wu, Pengjie Ren, Maarten de Rijke, Zhaochun Ren|Shandong University, Qingdao, China; University of Amsterdam, Amsterdam, Netherlands; The University of Melbourne, Melbourne, Australia; Meituan, Beijing, China|An opinion tag is a sequence of words on a specific aspect of a product or service. Opinion tags reflect key characteristics of product reviews and help users quickly understand their content in e-commerce portals. The task of abstractive opinion tagging has previously been proposed to automatically generate a ranked list of opinion tags for a given review. However, current models for opinion tagging are not personalized, even though personalization is an essential ingredient of engaging user interactions, especially in e-commerce. In this paper, we focus on the task of personalized abstractive opinion tagging. There are two main challenges when developing models for the end-to-end generation of personalized opinion tags: sparseness of reviews and difficulty to integrate multi-type signals, i.e., explicit review signals and implicit behavioral signals. To address these challenges, we propose an end-to-end model, named POT, that consists of three main components: (1) a review-based explicit preference tracker component based on a hierarchical heterogeneous review graph to track user preferences from reviews; (2)a behavior-based implicit preference tracker component using a heterogeneous behavior graph to track the user preferences from implicit behaviors; and (3) a personalized rank-aware tagging component to generate a ranked sequence of personalized opinion tags. In our experiments, we evaluate POT on a real-world dataset collected from e-commerce platforms and the results demonstrate that it significantly outperforms strong baselines.|意见标签是关于产品或服务特定方面的一系列单词。意见标签反映了产品评论的关键特征，帮助用户快速理解电子商务门户中的内容。抽象意见标签的任务以前曾被提议为给定的评论自动生成一个意见标签的排序列表。然而，目前的意见标签模型并不个性化，即使个性化是参与用户交互的一个重要组成部分，特别是在电子商务中。本文主要研究个性化抽象意见标注的任务。在开发端到端的个性化评价标签生成模型时，存在两个主要的挑战: 评价的稀疏性和整合多类型信号的困难，即显性评价信号和隐性行为信号。为了应对这些挑战，我们提出了一个端到端模型，命名为 POT，该模型由三个主要组成部分组成: (1)一个基于评论的显性偏好跟踪组件，该组件基于分层异构评论图来跟踪来自评论的用户偏好; (2)一个基于行为的隐性偏好跟踪组件，该组件使用一个异构行为图来跟踪来自隐性行为的用户偏好; (3)一个个性化的等级感知标签组件来生成一个个性。在我们的实验中，我们评估了从电子商务平台收集的真实世界数据集上的 POT，结果表明，它明显优于强基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalized+Abstractive+Opinion+Tagging)|1|
|[Contrastive Learning with Hard Negative Entities for Entity Set Expansion](https://doi.org/10.1145/3477495.3531954)|Yinghui Li, Yangning Li, Yuxin He, Tianyu Yu, Ying Shen, HaiTao Zheng|Harbin Institute of Technology, Shenzhen, Guangdong, China; Sun-Yat Sen University, Shenzhen, Guangdong, China; Tsinghua University, Shenzhen, Guangdong, China|Entity Set Expansion (ESE) is a promising task which aims to expand entities of the target semantic class described by a small seed entity set. Various NLP and IR applications will benefit from ESE due to its ability to discover knowledge. Although previous ESE methods have achieved great progress, most of them still lack the ability to handle hard negative entities (i.e., entities that are difficult to distinguish from the target entities), since two entities may or may not belong to the same semantic class based on different granularity levels we analyze on. To address this challenge, we devise an entity-level masked language model with contrastive learning to refine the representation of entities. In addition, we propose the ProbExpan, a novel probabilistic ESE framework utilizing the entity representation obtained by the aforementioned language model to expand entities. Extensive experiments and detailed analyses on three datasets show that our method outperforms previous state-of-the-art methods.|实体集扩展(ESE)是一个很有前途的任务，其目的是扩展由一个小的种子实体集描述的目标语义类的实体。由于 ESE 具有发现知识的能力，各种自然语言处理和红外应用程序将从中受益。尽管以前的 ESE 方法已经取得了很大的进步，但是它们中的大多数仍然缺乏处理硬负实体的能力(例如，难以区分目标实体的实体) ，因为两个实体可能属于或不属于同一个语义类，基于我们分析的不同粒度级别。为了解决这个问题，我们设计了一个带有对比学习的实体级掩蔽语言模型来改进实体的表示。此外，我们还提出了一种新的概率 ESE 框架，该框架利用上述语言模型获得的实体表示来扩展实体。对三个数据集的大量实验和详细分析表明，我们的方法优于以前的最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contrastive+Learning+with+Hard+Negative+Entities+for+Entity+Set+Expansion)|1|
|[EFLEC: Efficient Feature-LEakage Correction in GNN based Recommendation Systems](https://doi.org/10.1145/3477495.3531770)|Ishaan Kumar, Yaochen Hu, Yingxue Zhang|Huawei Technologies Canada, Montreal, PQ, Canada|Graph Convolutional Neural Networks (GNN) based recommender systems are state-of-the-art since they can capture the high order collaborative signals between users and items. However, they suffer from the feature leakage problem since label information determined by edges can be leaked into node embeddings through the GNN aggregation procedure guided by the same set of edges, leading to poor generalization. We propose the accurate removal algorithm to generate the final embedding. For each edge, the embeddings of the two end nodes are evaluated on a graph with that edge removed. We devise an algebraic trick to efficiently compute this procedure without explicitly constructing separate graphs for the LightGCN model. Experiments on four datasets demonstrate that our algorithm can perform better on datasets with sparse interactions, while the training time is significantly reduced.|基于图形卷积神经网络(GNN)的推荐系统能够捕获用户和商品之间的高阶协作信号，因而是目前最先进的推荐系统。然而，由于由边确定的标签信息可以通过由同一组边引导的 GNN 聚合过程泄漏到节点嵌入中，从而导致特征泄漏问题，从而导致特征泛化能力差。我们提出了精确去除算法来生成最终的嵌入。对于每个边，两端节点的嵌入在一个去除了该边的图上进行计算。我们设计了一个代数技巧来有效地计算这个过程，而不显式地为 LightGCN 模型构造单独的图。在四个数据集上的实验结果表明，该算法能够较好地处理交互稀疏的数据集，同时显著地减少了训练时间。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EFLEC:+Efficient+Feature-LEakage+Correction+in+GNN+based+Recommendation+Systems)|1|
|[P3 Ranker: Mitigating the Gaps between Pre-training and Ranking Fine-tuning with Prompt-based Learning and Pre-finetuning](https://doi.org/10.1145/3477495.3531786)|Xiaomeng Hu, Shi Yu, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu, Ge Yu|Microsoft, Redmond, WA, USA; Northeastern University, Shenyang, China; Tsinghua University, Beijing, China|Compared to other language tasks, applying pre-trained language models (PLMs) for search ranking often requires more nuances and training signals. In this paper, we identify and study the two mismatches between pre-training and ranking fine-tuning: the training schema gap regarding the differences in training objectives and model architectures, and the task knowledge gap considering the discrepancy between the knowledge needed in ranking and that learned during pre-training. To mitigate these gaps, we propose Pre-trained, Prompt-learned and Pre-finetuned Neural Ranker (P3 Ranker). P3 Ranker leverages prompt-based learning to convert the ranking task into a pre-training like schema and uses pre-finetuning to initialize the model on intermediate supervised tasks. Experiments on MS MARCO and Robust04 show the superior performances of P3 Ranker in few-shot ranking. Analyses reveal that P3 Ranker is able to better accustom to the ranking task through prompt-based learning and retrieve necessary ranking-oriented knowledge gleaned in pre-finetuning, resulting in data-efficient PLM adaptation. Our code is available at https://github.com/NEUIR/P3Ranker.|与其他语言任务相比，应用预训练语言模型(PLM)进行搜索排序往往需要更多的细微差别和训练信号。本文通过分析训练前和排序微调之间的两种不匹配现象: 训练目标和模型结构差异的训练模式差异和排序需要的知识与训练前学习的知识差异的任务知识差异。为了弥补这些差距，我们提出预训练，即时学习和预微调神经排序(P3排序)。P3 Ranker 利用基于提示的学习将排序任务转换为类似于预训练的模式，并使用预微调来初始化中间监督任务的模型。在 MSMARCO 和 Robust04上的实验表明，P3 Ranker 在少镜头排序方面具有优越的性能。分析表明，P3 Ranker 能够更好地适应排序任务，通过提示式学习和检索必要的排序导向的知识收集预微调，导致数据高效的 PLM 适应。我们的代码可以在 https://github.com/neuir/p3ranker 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=P3+Ranker:+Mitigating+the+Gaps+between+Pre-training+and+Ranking+Fine-tuning+with+Prompt-based+Learning+and+Pre-finetuning)|1|
|[Empowering Next POI Recommendation with Multi-Relational Modeling](https://doi.org/10.1145/3477495.3531801)|Zheng Huang, Jing Ma, Yushun Dong, Natasha Zhang Foutz, Jundong Li|University of Virginia, Charlottesville, VA, USA|With the wide adoption of mobile devices and web applications, location-based social networks (LBSNs) offer large-scale individual-level location-related activities and experiences. Next point-of-interest (POI) recommendation is one of the most important tasks in LBSNs, aiming to make personalized recommendations of next suitable locations to users by discovering preferences from users' historical activities. Noticeably, LBSNs have offered unparalleled access to abundant heterogeneous relational information about users and POIs (including user-user social relations, such as families or colleagues; and user-POI visiting relations). Such relational information holds great potential to facilitate the next POI recommendation. However, most existing methods either focus on merely the user-POI visits, or handle different relations based on over-simplified assumptions while neglecting relational heterogeneities. To fill these critical voids, we propose a novel framework, MEMO, which effectively utilizes the heterogeneous relations with a multi-network representation learning module, and explicitly incorporates the inter-temporal user-POI mutual influence with the coupled recurrent neural networks. Extensive experiments on real-world LBSN data validate the superiority of our framework over the state-of-the-art next POI recommendation methods.|随着移动设备和网络应用的广泛应用，基于位置的社交网络(LBSNs)提供了大规模的个人层面的位置相关活动和体验。下一个兴趣点(POI)推荐是 LBSNs 中最重要的任务之一，目的是通过发现用户历史活动的偏好，为用户提供下一个合适位置的个性化推荐。值得注意的是，LBSNs 提供了对关于用户和 POI (包括用户-用户社会关系，如家庭或同事; 以及用户-POI 访问关系)的大量异构关系信息的无与伦比的访问。这样的关系信息对于促进下一个 POI 建议具有很大的潜力。然而，大多数现有的方法要么仅仅关注用户 POI 访问，要么在过于简化的假设基础上处理不同的关系，而忽略了关系异构性。为了填补这些关键空白，我们提出了一种新的框架 MEMO，它有效地利用了多网络表示学习模块的异构关系，并将用户间 POI 的相互影响明确地与耦合的递归神经网络结合起来。对实际 LBSN 数据的大量实验验证了该框架相对于下一代 POI 推荐方法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Empowering+Next+POI+Recommendation+with+Multi-Relational+Modeling)|1|
|[Learning Trustworthy Web Sources to Derive Correct Answers and Reduce Health Misinformation in Search](https://doi.org/10.1145/3477495.3531812)|Dake Zhang, Amir Vakili Tahami, Mustafa Abualsaud, Mark D. Smucker|University of Waterloo, Waterloo, ON, Canada; Thomson Reuters Labs, Toronto, ON, Canada|When searching the web for answers to health questions, people can make incorrect decisions that have a negative effect on their lives if the search results contain misinformation. To reduce health misinformation in search results, we need to be able to detect documents with correct answers and promote them over documents containing misinformation. Determining the correct answer has been a difficult hurdle to overcome for participants in the TREC Health Misinformation Track. In the 2021 track, automatic runs were not allowed to use the known answer to a topic's health question, and as a result, the top automatic run had a compatibility-difference score of 0.043 while the top manual run, which used the known answer, had a score of 0.259. The compatibility-difference measures the ability of methods to rank correct and credible documents before incorrect and non-credible documents. By using an existing set of health questions and their known answers, we show it is possible to learn which web hosts are trustworthy, from which we can predict the correct answers to the 2021 health questions with an accuracy of 76%. Using our predicted answers, we can promote documents that we predict contain this answer and achieve a compatibility-difference score of 0.129, which is a three-fold increase in performance over the best previous automatic method.|在网上搜索健康问题的答案时，如果搜索结果包含错误信息，人们可能会做出对他们的生活有负面影响的错误决定。为了减少搜索结果中的健康错误信息，我们需要能够发现带有正确答案的文档，并将其推广到含有错误信息的文档之上。对于 TREC 健康错误信息跟踪的参与者来说，确定正确的答案是一个难以克服的障碍。在2021年的赛道上，自动跑步不允许使用已知的答案来回答某个话题的健康问题，结果，自动跑步得分最高的相容性差异得分为0.043，而使用已知答案的手动跑步得分最高的相容性差异得分为0.259。兼容性差异衡量的是将正确和可信的文件排在不正确和不可信的文件之前的方法的能力。通过使用现有的一组健康问题及其已知答案，我们可以了解哪些网站主机是值得信赖的，从中我们可以预测2021年健康问题的正确答案，准确率为76% 。使用我们预测的答案，我们可以推广我们预测包含这个答案的文档，并获得0.129的兼容性差异评分，这比以前最好的自动化方法的性能提高了三倍。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Trustworthy+Web+Sources+to+Derive+Correct+Answers+and+Reduce+Health+Misinformation+in+Search)|1|
|[On Optimizing Top-K Metrics for Neural Ranking Models](https://doi.org/10.1145/3477495.3531849)|Rolf Jagerman, Zhen Qin, Xuanhui Wang, Michael Bendersky, Marc Najork|Google Research, Mountain View, CA, USA|Top-K metrics such as [email protected] are frequently used to evaluate ranking performance. The traditional tree-based models such as LambdaMART, which are based on Gradient Boosted Decision Trees (GBDT), are designed to optimize [email protected] using the LambdaRank losses. Recently, there is a good amount of research interest on neural ranking models for learning-to-rank tasks. These models are fundamentally different from the decision tree models and behave differently with respect to different loss functions. For example, the most popular ranking losses used in neural models are the Softmax loss and the GumbelApproxNDCG loss. These losses do not connect to top-K metrics such as [email protected] naturally. It remains a question on how to effectively optimize [email protected] for neural ranking models. In this paper, we follow the LambdaLoss framework and design novel and theoretically sound losses for [email protected] metrics, while the original LambdaLoss paper can only do so using an unsound heuristic. We study the new losses on the LETOR benchmark datasets and show that the new losses work better than other losses for neural ranking models.|诸如[ email protected ]之类的 Top-K 指标经常被用来评估排名表现。传统的基于树的模型，如 LambdaMART，基于梯度增强决策树(GBDT) ，旨在利用 LambdaRank 损失优化[ email protected ]。近年来，针对学习排序任务的神经排序模型的研究引起了人们的广泛兴趣。这些模型与决策树模型有着根本的不同，对于不同的损失函数表现也不同。例如，在神经模型中最常用的排名损失是 Softmax 损失和 GumbelApprovxNDCG 损失。这些损失不会自然而然地与诸如[ email protected ]之类的 top-K 指标联系起来。这仍然是一个问题，如何有效地优化[电子邮件保护]的神经排序模型。在本文中，我们遵循 LambdaLoss 框架，为[电子邮件保护]指标设计新颖且理论上可靠的损失，而原始的 LambdaLoss 论文只能使用不可靠的启发式方法来做到这一点。我们研究了 LETOR 基准数据集上的新损失，发现新的损失比其他神经排序模型的损失要好。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Optimizing+Top-K+Metrics+for+Neural+Ranking+Models)|1|
|[Identifying Argumentative Questions in Web Search Logs](https://doi.org/10.1145/3477495.3531864)|Yamen Ajjour, Pavel Braslavski, Alexander Bondarenko, Benno Stein|Bauhaus-Universität Weimar, Weimar, Germany; Leipzig University & Bauhaus-Universität Weimar, Leipzig, Germany; Martin Luther Universität Halle-Wittenberg, Halle, Germany; Ural Federal University & HSE University, Yekaterinburg, Russian Fed.|We present an approach to identify argumentative questions among web search queries. Argumentative questions ask for reasons to support a certain stance on a controversial topic, such as ''Should marijuana be legalized?'' Controversial topics entail opposing stances, and hence can be supported or opposed by various arguments. Argumentative questions pose a challenge for search engines since they should be answered with both pro and con arguments in order to not bias a user toward a certain stance. To further analyze the problem, we sampled questions about 19 controversial topics from a large Yandex search log and let human annotators label them as one of factual, method, or argumentative. The result is a collection of 39,340 labeled questions, 28% of which are argumentative, demonstrating the need to develop dedicated systems for this type of questions. A comparative analysis of the three question types shows that asking for reasons and predictions are among the most important features of argumentative questions. To demonstrate the feasibility of the classification task, we developed a BERT-based classifier to map questions to the question types, reaching a promising macro-averaged F>sub>1-score of 0.78.|我们提出了一种识别网络搜索查询中争议性问题的方法。有争议的问题会询问支持某种立场的理由，比如“大麻是否应该合法化?”有争议的话题包含相反的立场，因此可以支持或反对的各种论点。争议性问题对搜索引擎来说是一个挑战，因为它们应该同时回答正反两方面的争议，以避免用户偏向某一立场。为了进一步分析这个问题，我们从一个大型 Yandex 搜索日志中抽取了19个有争议话题的问题，并让人工注释者将它们标记为事实、方法或论证性的问题。结果是收集了39,340个带标签的问题，其中28% 是有争议的，表明需要为这类问题开发专门的系统。对三种问题类型的比较分析表明，提问原因和预测是议论题的最重要特征之一。为了验证分类任务的可行性，我们开发了一个基于 BERT 的分类器来将问题映射到问题类型，得到了一个很有前途的宏观平均 F > 小于1的得分为0.78。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Identifying+Argumentative+Questions+in+Web+Search+Logs)|1|
|[An MLP-based Algorithm for Efficient Contrastive Graph Recommendations](https://doi.org/10.1145/3477495.3531874)|Siwei Liu, Iadh Ounis, Craig Macdonald|University of Glasgow, Glasgow, United Kingdom|Graph-based recommender systems (GBRSs) have achieved promising performance by incorporating the user-item bipartite graph using the Graph Neural Network (GNN). Among GBRSs, the information from each user and item's multi-hop neighbours is effectively conveyed between nodes through neighbourhood aggregation and message passing. Although effective, existing neighbourhood information aggregation and passing functions are usually computationally expensive. Motivated by the emerging contrastive learning technique, we design a simple neighbourhood construction method in conjunction with the contrastive objective function to simulate the neighbourhood information processing of GNN. In addition, we propose a simple algorithm based on Multilayer Perceptron (MLP) for learning users and items' representations with extra non-linearity while lowering computational burden compared with multi-layers GNNs. Our extensive empirical experiments on three public datasets demonstrate that our proposed model, i.e. MLP-CGRec, can reduce the GPU memory consumption and training time by up to 24.0% and 33.1%, respectively, without significantly degenerating the recommendation accuracy in comparison with competitive baselines.|基于图形的推荐系统(GBRS)通过使用图形神经网络(GNN)结合用户项二分图已经取得了良好的性能。在 GBRS 中，每个用户和项目的多跳邻居信息通过邻居聚合和消息传递在节点之间有效地传递。虽然有效，但现有的邻里信息聚合和传递函数通常计算开销很大。受新兴的对比学习技术的启发，我们设计了一种简单的邻域构造方法，结合对比目标函数来模拟 GNN 的邻域信息处理。此外，我们提出了一个简单的基于多层感知机(MLP)的学习算法，用于学习用户和项目的额外非线性表示，同时与多层 GNN 相比，降低了计算负担。我们在三个公共数据集上的广泛实验表明，我们提出的模型，即 MLP-CGRec，可以分别减少 GPU 内存消耗和训练时间高达24.0% 和33.1% ，与竞争性基线相比，不会显着降低推荐准确性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+MLP-based+Algorithm+for+Efficient+Contrastive+Graph+Recommendations)|1|
|[Entity-Conditioned Question Generation for Robust Attention Distribution in Neural Information Retrieval](https://doi.org/10.1145/3477495.3531878)|Revanth Gangi Reddy, Md. Arafat Sultan, Martin Franz, Avirup Sil, Heng Ji|UIUC, Champaign, IL, USA; IBM Research AI, Yorktown Heights, NY, USA|We show that supervised neural information retrieval (IR) models are prone to learning sparse attention patterns over passage tokens, which can result in key phrases including named entities receiving low attention weights, eventually leading to model under-performance. Using a novel targeted synthetic data generation method that identifies poorly attended entities and conditions the generation episodes on those, we teach neural IR to attend more uniformly and robustly to all entities in a given passage. On two public IR benchmarks, we empirically show that the proposed method helps improve both the model's attention patterns and retrieval performance, including in zero-shot settings.|我们发现监督神经信息检索(IR)模型更倾向于学习通道标记上的稀疏注意模式，这可能导致包括命名实体在内的关键短语注意力权重较低，最终导致模型表现不佳。使用一种新的有针对性的合成数据生成方法，识别参与程度较低的实体，并对这些实体上的生成事件进行条件化处理，我们教导神经 IR 在给定的段落中更加统一和稳健地参与所有实体。实验结果表明，该方法有助于提高模型的注意模式和检索性能，包括在零镜头设置下的检索性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Entity-Conditioned+Question+Generation+for+Robust+Attention+Distribution+in+Neural+Information+Retrieval)|1|
|[C3: Continued Pretraining with Contrastive Weak Supervision for Cross Language Ad-Hoc Retrieval](https://doi.org/10.1145/3477495.3531886)|Eugene Yang, Suraj Nair, Ramraj Chandradevan, Rebecca IglesiasFlores, Douglas W. Oard|University of Pennsylvania, Philadelphia, PA, USA; University of Maryland, College Park, College Park, MD, USA; Johns Hopkins University, Baltimore, MD, USA; Emory University, Atlanta, GA, USA|Pretrained language models have improved effectiveness on numerous tasks, including ad-hoc retrieval. Recent work has shown that continuing to pretrain a language model with auxiliary objectives before fine-tuning on the retrieval task can further improve retrieval effectiveness. Unlike monolingual retrieval, designing an appropriate auxiliary task for cross-language mappings is challenging. To address this challenge, we use comparable Wikipedia articles in different languages to further pretrain off-the-shelf multilingual pretrained models before fine-tuning on the retrieval task. We show that our approach yields improvements in retrieval effectiveness.|预先训练的语言模型可以提高包括即席检索在内的许多任务的有效性。最近的研究表明，在对检索任务进行微调之前，继续预训练带有辅助目标的语言模型可以进一步提高检索效率。与单语检索不同，为跨语言映射设计合适的辅助任务具有挑战性。为了解决这个问题，我们使用不同语言的维基百科文章来进一步预训练现成的多语言预训模型，然后再对检索任务进行微调。我们展示了我们的方法在检索效率方面的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=C3:+Continued+Pretraining+with+Contrastive+Weak+Supervision+for+Cross+Language+Ad-Hoc+Retrieval)|1|
|[A Meta-learning Approach to Fair Ranking](https://doi.org/10.1145/3477495.3531892)|Yuan Wang, Zhiqiang Tao, Yi Fang|Santa Clara University, Santa Clara, CA, USA|In recent years, the fairness in information retrieval (IR) system has received increasing research attention. While the data-driven ranking models achieve significant improvements over traditional methods, the dataset used to train such models is usually biased, which causes unfairness in the ranking models. For example, the collected imbalance dataset on the subject of the expert search usually leads to systematic discrimination on the specific demographic groups such as race, gender, etc, which further reduces the exposure for the minority group. To solve this problem, we propose a Meta-learning based Fair Ranking (MFR) model that could alleviate the data bias for protected groups through an automatically-weighted loss. Specifically, we adopt a meta-learning framework to explicitly train a meta-learner from an unbiased sampled dataset (meta-dataset), and simultaneously, train a listwise learning-to-rank (LTR) model on the whole (biased) dataset governed by "fair" loss weights. The meta-learner serves as a weighting function to make the ranking loss attend more on the minority group. To update the parameters of the weighting function and the ranking model, we formulate the proposed MFR as a bilevel optimization problem and solve it using the gradients through gradients. Experimental results on several real-world datasets demonstrate that the proposed method achieves a comparable ranking performance and significantly improves the fairness metric compared with state-of-the-art methods.|近年来，信息检索制度的公平性越来越受到研究人员的关注。虽然数据驱动的排序模型比传统的方法有了显著的改进，但是用于训练这些模型的数据集往往存在偏差，从而导致排序模型的不公平性。例如，所收集的关于专家搜索主题的不平衡数据集通常导致对特定人口群体如种族、性别等的系统性歧视，从而进一步减少少数群体的暴露程度。为了解决这一问题，我们提出了一种基于元学习的公平排序(MFR)模型，该模型通过自动加权损失来减轻受保护群体的数据偏差。具体而言，我们采用元学习框架来显式地从无偏的采样数据集(元数据集)中训练元学习者，同时在受“公平”损失权重管理的整个(有偏的)数据集上训练列表学习到排名(LTR)模型。元学习者作为一个权重函数，使排名的损失更多地出现在少数群体中。为了更新权重函数和排名模型的参数，我们将建议的最小生成最佳化问题(mFR)表示为一个双层次模型，并使用梯度来解决这个问题。在实际数据集上的实验结果表明，与现有方法相比，该方法具有可比较的排序性能，并显著提高了公平性度量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Meta-learning+Approach+to+Fair+Ranking)|1|
|[Where Does the Performance Improvement Come From?: - A Reproducibility Concern about Image-Text Retrieval](https://doi.org/10.1145/3477495.3531715)|Jun Rao, Fei Wang, Liang Ding, Shuhan Qi, Yibing Zhan, Weifeng Liu, Dacheng Tao|China University of Petroleum (East China), qingdao, China; Harbin Institute of Technology, Shenzhen, shenzhen, China; JD Explore Academy, beijing, China|This article aims to provide the information retrieval community with some reflections on recent advances in retrieval learning by analyzing the reproducibility of image-text retrieval models. Due to the increase of multimodal data over the last decade, image-text retrieval has steadily become a major research direction in the field of information retrieval. Numerous researchers train and evaluate image-text retrieval algorithms using benchmark datasets such as MS-COCO and Flickr30k. Research in the past has mostly focused on performance, with multiple state-of-the-art methodologies being suggested in a variety of ways. According to their assertions, these techniques provide improved modality interactions and hence more precise multimodal representations. In contrast to previous works, we focus on the reproducibility of the approaches and the examination of the elements that lead to improved performance by pretrained and nonpretrained models in retrieving images and text. To be more specific, we first examine the related reproducibility concerns and explain why our focus is on image-text retrieval tasks. Second, we systematically summarize the current paradigm of image-text retrieval models and the stated contributions of those approaches. Third, we analyze various aspects of the reproduction of pretrained and nonpretrained retrieval models. To complete this, we conducted ablation experiments and obtained some influencing factors that affect retrieval recall more than the improvement claimed in the original paper. Finally, we present some reflections and challenges that the retrieval community should consider in the future. Our source code is publicly available at https://github.com/WangFei-2019/Image-text-Retrieval.|本文旨在通过分析图像-文本检索模型的可重复性，为信息检索提供有关检索学习最新进展的一些反思。由于过去十年来多模态数据的增加，图像-文本检索逐渐成为信息检索领域的一个主要研究方向。许多研究人员使用基准数据集(如 MS-COCO 和 Flickr30k)训练和评估图像-文本检索算法。过去的研究主要集中在性能方面，以各种方式提出了多种最先进的方法。根据他们的断言，这些技术提供了改进的模态交互，因此更精确的多模态表示。与以往的工作相比，我们关注的方法的可重复性和检查的元素，导致改善性能的预训练和未经预训练的模型检索图像和文本。更具体地说，我们首先考察相关的重复性问题，并解释为什么我们的重点是图像文本检索任务。其次，系统地总结了当前图像文本检索模型的研究范式以及这些方法的贡献。第三，我们分析了预训练和非预训练检索模型复制的各个方面。为此，我们进行了消融实验，得到了一些影响检索回忆的因素，这些因素对检索回忆的影响大于原文中提出的改进。最后，我们提出了一些反思和挑战，检索社区应该考虑在未来。我们的源代码可以在 https://github.com/wangfei-2019/image-text-retrieval 上公开。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Where+Does+the+Performance+Improvement+Come+From?:+-+A+Reproducibility+Concern+about+Image-Text+Retrieval)|1|
|[Competitive Search](https://doi.org/10.1145/3477495.3532771)|Oren Kurland, Moshe Tennenholtz|European Univ Inst, Fiesole, Italy; Univ New South Wales, Business Sch, Sydney, NSW, Australia; Zhejiang Univ, Hangzhou, Peoples R China; Univ Chicago, Booth Sch Business, Chicago, IL 60637 USA|This essay surveys the literature on directed search and competitive search equilibrium, covering theory and a variety of applications. These models share features with traditional search theory, but also differ in important ways. They share features with general equilibrium theory, but with explicit frictions. Equilibria are often efficient, mainly because markets price goods plus the time required to get them. The approach is tractable and arguably realistic. Results are presented for finite and continuum economies. Private information and sorting with heterogeneity are analyzed. While emphasizing issues and applications, we also provide several hard-to-find technical results.|本文综述了有关定向搜索和竞争搜索均衡、覆盖理论和各种应用的文献。这些模型具有与传统搜索理论相同的特点，但在一些重要方面又有所不同。它们与一般均衡理论有共同的特点，但存在明显的摩擦。均衡往往是有效的，主要是因为市场对商品定价，加上购买商品所需的时间。这种方法容易处理，而且可以说是现实的。给出了有限经济体和连续经济体的结果。分析了私有信息和异构排序问题。在强调问题和应用程序的同时，我们还提供了一些难以找到的技术结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Competitive+Search)|1|
|[A Dataset for Sentence Retrieval for Open-Ended Dialogues](https://doi.org/10.1145/3477495.3531727)|Itay Harel, Hagai Taitelbaum, Idan Szpektor, Oren Kurland|Technion - Israel Institute of Technology, Haifa, Israel; TSG IT Advanced Systems Ltd., Tel Aviv, Israel; Google Research, Tel Aviv, Israel|We address the task of sentence retrieval for open-ended dialogues. The goal is to retrieve sentences from a document corpus that contain information useful for generating the next turn in a given dialogue. Prior work on dialogue-based retrieval focused on specific types of dialogues: either conversational QA or conversational search. To address a broader scope of this task where any type of dialogue can be used, we constructed a dataset that includes open-ended dialogues from Reddit, candidate sentences from Wikipedia for each dialogue and human annotations for the sentences. We report the performance of several retrieval baselines, including neural retrieval models, over the dataset. To adapt neural models to the types of dialogues in the dataset, we explored an approach to induce a large-scale weakly supervised training data from Reddit. Using this training set significantly improved the performance over training on the MS MARCO dataset.|我们讨论开放式对话的句子检索任务。其目的是从文档语料库中检索句子，这些句子包含有用的信息，用于在给定的对话中生成下一个回合。先前的基于对话的检索工作集中在特定类型的对话: 会话 QA 或会话搜索。为了扩大任何类型的对话都可以使用的范围，我们构建了一个数据集，其中包括来自 Reddit 的开放式对话、来自 Wikipedia 的每个对话的候选句子以及句子的人工注释。我们报告了在数据集上的几个检索基线的性能，包括神经检索模型。为了使神经模型适应数据集中的对话类型，我们探索了一种从 Reddit 引入大规模弱监督训练数据的方法。使用这个训练集显著提高了在 MS MARCO 数据集上的训练性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Dataset+for+Sentence+Retrieval+for+Open-Ended+Dialogues)|1|
|[iRec: An Interactive Recommendation Framework](https://doi.org/10.1145/3477495.3531754)|Thiago Silva, Nícollas Silva, Heitor Werneck, Carlos Mito, Adriano C. M. Pereira, Leonardo Rocha|Universidade Federal de São João Del Rei, São João Del Rei, Brazil; Universidade Federal de Minas Gerais, Belo Horizonte, Brazil|Nowadays, most e-commerce and entertainment services have adopted interactive Recommender Systems (RS) to guide the entire journey of users into the system. This task has been addressed as a Multi-Armed Bandit problem where systems must continuously learn and recommend at each iteration. However, despite the recent advances, there is still a lack of consensus on the best practices to evaluate such bandit solutions. Several variables might affect the evaluation process, but most of the works have only been concerned about the accuracy of each method. Thus, this work proposes an interactive RS framework named iRec. It covers the whole experimentation process by following the main RS guidelines. The iRec provides three modules to prepare the dataset, create new recommendation agents, and simulate the interactive scenario. Moreover, it also contains several state-of-the-art algorithms, a hyperparameter tuning module, distinct evaluation metrics, different ways of visualizing the results, and statistical validation.|目前，大多数电子商务和娱乐服务都采用交互式推荐系统(RS)来引导用户进入系统的整个过程。这个任务已经被解决为一个多臂老虎机问题，系统必须在每次迭代中不断学习和推荐。然而，尽管最近取得了一些进展，但是在评估这种土匪解决方案的最佳实践方面仍然缺乏共识。有几个变量可能会影响评价过程，但大多数工作只关心每种方法的准确性。因此，本文提出了一个交互式 RS 框架 iRec。它涵盖了遵循 RS 主要准则的整个实验过程。IRec 提供了三个模块来准备数据集、创建新的推荐代理和模拟交互场景。此外，它还包含一些最先进的算法、一个超参数调整模块、不同的评估指标、不同的结果可视化方法以及统计验证。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=iRec:+An+Interactive+Recommendation+Framework)|1|
|[RecDelta: An Interactive Dashboard on Top-k Recommendation for Cross-model Evaluation](https://doi.org/10.1145/3477495.3531674)|YiShyuan Chiang, YuZe Liu, ChenFeng Tsai, JingKai Lou, MingFeng Tsai, ChuanJu Wang|KKStream Technologies, Taipei, Taiwan Roc; Academia Sinica, Taipei, Taiwan Roc; National Chengchi University, Taipei, Taiwan Roc|In this demonstration, we present RecDelta, an interactive tool for the cross-model evaluation of top-k recommendation. RecDelta is a web-based information system where people visually compare the performance of various recommendation algorithms and their recommended items. In the proposed system, we visualize the distribution of the δ scores between algorithms--a distance metric measuring the intersection between recommendation lists. Such visualization allows for rapid identification of users for whom the items recommended by different algorithms diverge or vice versa; then, one can further select the desired user to present the relationship between recommended items and his/her historical behavior. RecDelta benefits both academics and practitioners by enhancing model explainability as they develop recommendation algorithms with their newly gained insights. Note that while the system is now online at https://cfda.csie.org/recdelta, we also provide a video recording at https://tinyurl.com/RecDelta to introduce the concept and the usage of our system.|在这个演示中，我们展示了 RecDelta，一个用于 top-k 推荐的跨模型评估的交互式工具。RecDelta 是一个基于网络的信息系统，人们可以在这个系统中直观地比较各种推荐算法及其推荐项目的性能。在所提出的系统中，我们可视化算法之间 δ 分数的分布——一个测量推荐列表之间交集的距离度量。这种可视化允许快速识别由不同算法推荐的条目有差异的用户，反之亦然; 然后，人们可以进一步选择所需的用户来表示推荐条目和他/她的历史行为之间的关系。RecDelta 通过增强模型的可解释性使学者和从业者受益，因为他们利用新获得的见解开发推荐算法。请注意，虽然该系统现已上网，但我们亦提供 https://cfda.csie.org/recdelta 录像 https://tinyurl.com/recdelta  ，介绍该系统的概念及用途。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RecDelta:+An+Interactive+Dashboard+on+Top-k+Recommendation+for+Cross-model+Evaluation)|1|
|[Asyncval: A Toolkit for Asynchronously Validating Dense Retriever Checkpoints During Training](https://doi.org/10.1145/3477495.3531658)|Shengyao Zhuang, Guido Zuccon|The University of Queensland, Brisbane, QLD, Australia|The process of model checkpoint validation refers to the evaluation of the performance of a model checkpoint executed on a held-out portion of the training data while learning the hyperparameters of the model. This model checkpoint validation process is used to avoid over-fitting and determine when the model has converged so as to stop training. A simple and efficient strategy to validate deep learning checkpoints is the addition of validation loops to execute during training. However, the validation of dense retrievers (DR) checkpoints is not as trivial -- and the addition of validation loops is not efficient. This is because, in order to accurately evaluate the performance of a DR checkpoint, the whole document corpus needs to be encoded into vectors using the current checkpoint before any actual retrieval operation for checkpoint validation can be performed. This corpus encoding process can be very time-consuming if the document corpus contains millions of documents (e.g., 8.8M for MS MARCO v1 and 21M for Natural Questions). Thus, a naïve use of validation loops during training will significantly increase training time. To address this issue, we propose Asyncval: a Python-based toolkit for efficiently validating DR checkpoints during training. Instead of pausing the training loop for validating DR checkpoints, Asyncval decouples the validation loop from the training loop, uses another GPU to automatically validate new DR checkpoints and thus permits to perform validation asynchronously from training. Asyncval also implements a range of different corpus subset sampling strategies for validating DR checkpoints; these strategies allow to further speed up the validation process. We provide an investigation of these methods in terms of their impact on validation time and validation fidelity. Asyncval is made available as an open-source project at https://github.com/ielab/asyncval.|模型检查点验证过程是指在学习模型超参数的同时，对在训练数据的保持部分上执行的模型检查点的性能进行评估。该模型检查点验证过程用于避免过拟合，判断模型何时收敛，从而停止训练。验证深度学习检查点的一个简单而有效的策略是在训练期间添加验证循环来执行。然而，密集检索器(DR)检查点的验证并不那么简单——并且添加验证循环的效率也不高。这是因为，为了准确地评估 DR 检查点的性能，在执行检查点验证的任何实际检索操作之前，需要使用当前检查点将整个文档语料库编码到向量中。如果文档语料库包含数百万个文档(例如，MS MARCO v1为8.8 M，自然问题为21M) ，那么这个语料库编码过程可能非常耗时。因此，在训练期间天真地使用验证循环将显著增加训练时间。为了解决这个问题，我们提出了 Asyncval: 一个基于 Python 的工具包，用于在培训期间有效地验证 DR 检查点。Asyncval 没有暂停用于验证 DR 检查点的训练循环，而是将验证循环与训练循环解耦，使用另一个 GPU 自动验证新的 DR 检查点，从而允许从训练中异步执行验证。Asyncval 还实现了一系列不同的语料库子集采样策略来验证 DR 检查点; 这些策略允许进一步加快验证过程。我们根据这些方法对验证时间和验证保真度的影响对它们进行了研究。Asyncval 作为一个开源项目在 https://github.com/ielab/Asyncval 上可以使用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Asyncval:+A+Toolkit+for+Asynchronously+Validating+Dense+Retriever+Checkpoints+During+Training)|1|
|[TaskMAD: A Platform for Multimodal Task-Centric Knowledge-Grounded Conversational Experimentation](https://doi.org/10.1145/3477495.3531679)|Alessandro Speggiorin, Jeffrey Dalton, Anton Leuski|University of Glasgow, Glasgow, United Kingdom; University of Southern California, Los Angeles, CA, USA|The role of conversational assistants continues to evolve, beyond simple voice commands to ones that support rich and complex tasks in the home, car, and even virtual reality. Going beyond simple voice command and control requires agents and datasets blending structured dialogue, information seeking, grounded reasoning, and contextual question-answering in a multimodal environment with rich image and video content. In this demo, we introduce Task-oriented Multimodal Agent Dialogue (TaskMAD), a new platform that supports the creation of interactive multimodal and task-centric datasets in a Wizard-of-Oz experimental setup. TaskMAD includes support for text and voice, federated retrieval from text and knowledge bases, and structured logging of interactions for offline labeling. Its architecture supports a spectrum of tasks that span open-domain exploratory search to traditional frame-based dialogue tasks. It's open-source and offers rich capability as a platform used to collect data for the Amazon Alexa Prize Taskbot challenge, TREC Conversational Assistance track, undergraduate student research, and others. TaskMAD is distributed under the MIT license.|会话助理的角色不断演变，从简单的语音命令到支持家庭、汽车甚至虚拟现实中丰富而复杂的任务的语音命令。超越简单的语音命令和控制需要代理和数据集在一个具有丰富图像和视频内容的多通道环境中，将结构化对话、信息搜索、基础推理和上下文问答相结合。在这个演示中，我们介绍了面向任务的多通道 Agent 对话(TaskMAD) ，这是一个新的平台，支持在 Wizard-of-Oz 实验设置中创建交互式多通道和以任务为中心的数据集。TaskMAD 包括对文本和语音的支持、对文本和知识库的联合检索，以及用于离线标记的交互的结构化日志记录。它的体系结构支持一系列任务，从开放领域的探索性搜索到传统的基于框架的对话任务。它是开源的，提供了丰富的能力，作为一个平台，用于收集数据的亚马逊 Alexa 奖任务机器人挑战，TREC 对话援助轨道，本科生研究，和其他。TaskMAD 是在 MIT 许可下发布的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TaskMAD:+A+Platform+for+Multimodal+Task-Centric+Knowledge-Grounded+Conversational+Experimentation)|1|
|[IRVILAB: Gamified Searching on Multilingual Wikipedia](https://doi.org/10.1145/3477495.3531662)|Paavo Arvola, Tuulikki Alamettälä|Tampere University, Tampere, Finland|Information retrieval (IR) evaluation can be considered as a form of competition in matching documents and queries. This paper introduces a learning environment based on gamification of query construction for document retrieval, called IRVILAB (Information Retrieval Virtual Lab). The lab has modules for creating standard evaluation settings, one for topic creation including relevance assessments and another for performance evaluation of user queries. In addition, multilingual Wikipedia online collection enables a module, where relevance assessments are translated to other languages. The underlying game utilizes IR performance metrics to measure and give feedback on participants' information retrieval performance. It aims to improve participants' search skills, subject knowledge and contributes to science education by introducing an experimental method. Distinctive features of the system include algorithmic relevance assessments and automatic recall base translation.|信息检索评估可被视为一种在配对文件和查询方面的竞争形式。本文介绍了一个基于文献检索查询结构游戏化的学习环境 IRVILAB (信息检索虚拟实验室)。该实验室有用于创建标准评估设置的模块，一个用于主题创建，包括相关性评估，另一个用于用户查询的性能评估。此外，多语言维基百科在线收集支持一个模块，其中相关性评估被翻译成其他语言。基础游戏利用红外表现指标来衡量和反馈参与者的信息检索表现。它旨在提高参与者的搜索技能，学科知识和有助于科学教育的实验方法。该系统的显著特点包括算法相关性评估和自动回忆库翻译。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IRVILAB:+Gamified+Searching+on+Multilingual+Wikipedia)|1|
|[Improving Efficiency and Robustness of Transformer-based Information Retrieval Systems](https://doi.org/10.1145/3477495.3532681)|Edmon Begoli, Sudarshan Srinivasan, Maria Mahbub|Oak Ridge National Laboratory (ORNL), Oak Ridge, TN, USA|This tutorial focuses on both theoretical and practical aspects of improving the efficiency and robustness of transformer-based approaches, so that these can be effectively used in practical, high-scale, and high-volume information retrieval (IR) scenarios. The tutorial is inspired and informed by our work and experience while working with massive narrative datasets (8.5 billion medical notes), and by our basic research and academic experience with transformer-based IR tasks. Additionally, the tutorial focuses on techniques for making transformer-based IR robust against adversarial (AI) exploitation. This is a recent concern in the IR domain that we needed to take into concern, and we want to want to share some of the lessons learned and applicable principles with our audience. Finally, an important, if not critical, element of this tutorial is its focus on didacticism -- delivering tutorial content in a clear, intuitive, plain-speak fashion. Transformers are a challenging subject, and, through our teaching experience, we observed a great value and a great need to explain all relevant aspects of this architecture and related principles in the most straightforward, precise, and intuitive manner. That is the defining style of our proposed tutorial.|本教程重点介绍提高基于变压器的方法的效率和稳健性的理论和实践方面，以便这些方法能够有效地用于实际的、大规模的和大容量的信息检索(IR)场景。本教程的灵感来自于我们在处理大量叙述性数据集(85亿医学笔记)时的工作和经验，以及我们在基于变压器的 IR 任务方面的基础研究和学术经验。此外，本教程还重点介绍了使基于变压器的 IR 针对对手(AI)开发具有鲁棒性的技术。这是国际关系领域最近关注的一个问题，我们需要加以关注，我们希望与我们的听众分享一些经验教训和适用的原则。最后，本教程的一个重要的(如果不是关键的)元素是它对教学法的关注——以一种清晰、直观、直白的方式提供教程内容。变压器是一个具有挑战性的课题，并且，通过我们的教学经验，我们观察到一个巨大的价值和一个巨大的需要，以最直接，精确和直观的方式解释这个架构和相关原则的所有相关方面。这就是我们提议的教程的定义风格。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Efficiency+and+Robustness+of+Transformer-based+Information+Retrieval+Systems)|1|
|[Self-Supervised Learning for Recommender System](https://doi.org/10.1145/3477495.3532684)|Chao Huang, Xiang Wang, Xiangnan He, Dawei Yin|University of Hong Kong, Hong Kong, Hong Kong; Baidu Inc., Beijing, China; University of Science and Technology of China, Hefei, China|Recommender systems have become key components for a wide spectrum of web applications (e.g., E-commerce sites, video sharing platforms, lifestyle applications, etc), so as to alleviate the information overload and suggest items for users. However, most existing recommendation models follow a supervised learning manner, which notably limits their representation ability with the ubiquitous sparse and noisy data in practical applications. Recently, self-supervised learning (SSL) has become a promising learning paradigm to distill informative knowledge from unlabeled data, without the heavy reliance on sufficient supervision signals. Inspired by the effectiveness of self-supervised learning, recent efforts bring SSL's superiority into various recommendation representation learning scenarios with augmented auxiliary learning tasks. In this tutorial, we aim to provide a systemic review of existing self-supervised learning frameworks and analyze the corresponding challenges for various recommendation scenarios, such as general collaborative filtering paradigm, social recommendation, sequential recommendation, and multi-behavior recommendation. We then raise discussions and future directions of this area. With the introduction of this emerging and promising topic, we expect the audience to have a deep understanding of this domain. We also seek to promote more ideas and discussions, which facilitates the development of self-supervised learning recommendation techniques.|推荐系统已成为一系列网上应用程式(例如电子商贸网站、影片分享平台、生活方式应用程式等)的重要组成部分，以纾缓信息超载及为使用者提供建议。然而，现有的大多数推荐模型都遵循监督式学习的方式，这明显地限制了它们在实际应用中对普遍存在的稀疏和嘈杂数据的表示能力。近年来，自监督学习(SSL)已成为从未标记数据中提取信息知识的一种很有前途的学习方法，而不需要过多地依赖于足够的监督信号。受自我监督学习效果的启发，最近的研究将 SSL 的优越性应用到各种推荐表示学习场景中，并增加了辅助学习任务。在本教程中，我们的目标是提供一个现有的自我监督学习框架的系统回顾，并分析各种推荐场景的相应挑战，如一般协同过滤范式，社会推荐，顺序推荐和多行为推荐。然后，我们提出这一领域的讨论和未来方向。随着这个新兴的和有前途的主题的介绍，我们希望听众对这个领域有一个深刻的理解。我们也寻求促进更多的想法和讨论，以促进自我监督学习推荐技术的发展。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Supervised+Learning+for+Recommender+System)|1|
|[ReNeuIR: Reaching Efficiency in Neural Information Retrieval](https://doi.org/10.1145/3477495.3531704)|Sebastian Bruch, Claudio Lucchese, Franco Maria Nardini|Ca' Foscary University of Venice, Venice, Italy; Pinecone, New York, NY, USA; ISTI-CNR, Pisa, Italy|Perhaps the applied nature of information retrieval research goes some way to explain the community's rich history of evaluating machine learning models holistically, understanding that efficacy matters but so does the computational cost incurred to achieve it. This is evidenced, for example, by more than a decade of research on efficient training and inference of large decision forest models in learning-to-rank. As the community adopts even more complex, neural network-based models in a wide range of applications, questions on efficiency have once again become relevant. We propose this workshop as a forum for a critical discussion of efficiency in the era of neural information retrieval, to encourage debate on the current state and future directions of research in this space, and to promote more sustainable research by identifying best practices in the development and evaluation of neural models for information retrieval.|也许信息检索研究的应用性质在某种程度上可以解释社区整体评估机器学习模型的丰富历史，理解功效很重要，但实现它所需的计算成本也很重要。例如，十多年来对大型决策森林模型在学习排序中的有效训练和推理的研究就证明了这一点。随着社会采用更为复杂的、基于神经网络的模型在广泛的应用中，有关效率的问题再次变得相关。我们建议这个研讨会作为一个论坛，就神经信息检索时代的效率进行批判性讨论，鼓励就该领域的研究现状和未来方向展开辩论，并通过确定开发和评估神经信息检索模型的最佳做法，促进更可持续的研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ReNeuIR:+Reaching+Efficiency+in+Neural+Information+Retrieval)|1|
|[Generating Knowledge-based Explanation for Recommendation from Review](https://doi.org/10.1145/3477495.3531683)|Zuoxi Yang|South China University of Technology, Tianhe, Guangzhou, China|Reasonable explanation is helpful to increase the trust and satisfaction of user to the recommender system. Among many previous studies, there is growing concern about generating explanation based on review text. Collaborative filtering is one of the most successful approaches to predict user's preference. However, most of them suffer from data sparsity problem. Researcher often utilizes auxiliary data to address this problem, such as review, knowledge graph (KG), image and so on. Some researchers have proven that recommendation accuracy can be improved via incorporating rating and review data. Besides, neural network is also applied to learn more powerful representations for user and item from the review data. For example, convolution neural network (CNN) is used to extract representation from review text by using convolutional filters. Recurrent neural network (RNN) is another widely used model, which can encode the sequential behaviours as hidden states. However, most of them lack the ability to generate explanation. In order to generate explanation, there are two main approaches are used, i.e., template-based approach and generation-based approach. It is usually necessary for the templated-based approach to define serval templates. Then, these templates will be further filled with different personalized features/words. Although they can offer readable explanations, they rely heavily on pre-defined templates. It causes large manual efforts, limiting their explanation expression. Due to the strong generation ability of natural language model, the generation-based approach is capable to generate explanation without templates, which can largely enhance the expression of the generated sentence. Although they can generate more free and flexible explanation, the explanation might tend to be uninformative. To tackle these challenges of the above-mentioned work, we propose a Generating Knowldge-based Explanation for Recommendation from Review (GKER) to provide informative explanation. Unlike the traditional generation-based approach with a multi-task framework, we design a single-task framework to simultaneously model user's preference and explanation generation. The multi-task training usually needs more manual effort and time overhead. In this unitary framework, we inject the user's sentiment preference into the explanation generation, aiming at capturing the user's interest while producing high-quality explanation. Specifically, we build three graphs, including a bipartite graph, a KG and a co-occur graph. All of them are integrated to form a unitary graph, thus bringing the semantic among user-item interaction, KG and review. Based on this integrated graph, it is possible to learn more effective representations for user and item. To make better use of the integrated KG, a graph convolution network (GCN) is utilized to obtain improved embeddings due to its superior representation learning ability. We argue that these embeddings can contain more semantic interaction signals with the help of the integrated KG and GCN. After obtaining these extensive embeddings, a multilayer perceptron (MLP) layer is further employed to capture non-linear interaction signals between user and item, aiming at predicting user's rating accurately. The predicted rating would be regarded as a sentiment indicator to explore why the user likes or dislikes the target item. To investigate the association between sentiment indicator and the related review data, a transformer-enhanced encoder-decoder architecture is designed to produce informative and topic-relevant explanation. Besides, the aspect semantic is added in this architecture through an attention mechanism. In this framework, the transformer is utilized as a "teacher" model to supervise the generation of the encoder-decoder process. Finally, experiments conducted on three datasets have shown the state-of-the-art performance of GKER. There are some research issues for discussion: 1) although KG is a useful tool for recommendation accuracy and explainability, it is always incomplete in the real world. Hence, it is worth completing it for the recommendation. 2) Besides, as for explainable, it still needs more metrics to evaluate the quality of its explanation.|合理的解释有助于提高用户对推荐系统的信任和满意度。在以往的许多研究中，人们越来越关注基于评论文本生成解释。协同过滤是预测用户偏好最成功的方法之一。然而，它们中的大多数都存在数据稀疏问题。研究人员经常利用辅助数据来解决这个问题，如复习、知识图(KG)、图像等。一些研究人员已经证明，通过整合评分和评论数据，可以提高推荐的准确性。此外，还应用神经网络从复习数据中学习更强大的用户和项目表示。例如，卷积神经网络(CNN)通过使用卷积滤波器从复习文本中提取表示。递归神经网络(RNN)是另一种广泛使用的模型，它可以将序列行为编码为隐藏状态。然而，他们中的大多数缺乏产生解释的能力。为了产生解释，主要使用了两种方法，即基于模板的方法和基于生成的方法。基于模板的方法通常需要定义几个模板。然后，这些模板将进一步填充不同的个性化特征/词语。尽管它们可以提供可读的解释，但它们严重依赖于预定义的模板。它导致大量的人工操作，限制了它们的解释表达。由于自然语言模型具有很强的生成能力，基于生成的方法能够在没有模板的情况下生成解释，从而大大提高了生成句子的表达能力。虽然他们可以产生更自由和灵活的解释，解释可能往往是无益的。为了应对上述工作的这些挑战，我们提出了一个基于知识生成的评审推荐解释(GKER)来提供信息解释。与传统的基于生成的多任务框架方法不同，我们设计了一个单任务框架来同时模拟用户的偏好和解释生成。多任务训练通常需要更多的人工努力和时间开销。在这个统一的框架中，我们将用户的情感偏好引入到解释生成中，目的是在获取用户兴趣的同时生成高质量的解释。具体来说，我们构造了三个图，包括一个二部图、一个 KG 图和一个共现图。它们集成在一起形成一个统一的图形，从而实现了用户-项目交互、 KG 和评论之间的语义关系。基于这个集成图表，可以学习更有效的表示用户和项目。为了更好地利用集成 KG，利用图卷积网络(GCN)优越的表示学习能力来获得改进的嵌入。我们认为，这些嵌入可以包含更多的语义交互信号的帮助下，整合 KG 和 GCN。在获得这些广泛的嵌入之后，一个多层感知机(MLP)层被进一步用来捕获用户和项目之间的非线性交互信号，旨在准确预测用户的评分。预测的评分将被视为一个情绪指标，以探索为什么用户喜欢或不喜欢的目标项目。为了研究情绪指标与相关评论数据之间的关联，设计了一个变压器增强型编解码器结构，以产生信息丰富和与主题相关的解释。此外，通过注意机制在体系结构中添加了方面语义。在这个框架中，变压器被用作“教师”模型来监督编码器-解码器过程的生成。最后，在三个数据集上进行的实验显示了 GKER 的最新性能。有一些研究问题值得讨论: 1)虽然 KG 是一个有效的推荐准确性和可解释性的工具，但在现实世界中它总是不完整的。因此，为了获得推荐，完成它是值得的。2)对于可解释性，还需要更多的指标来评价其解释的质量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generating+Knowledge-based+Explanation+for+Recommendation+from+Review)|1|
|[Improving Fairness and Transparency for Artists in Music Recommender Systems](https://doi.org/10.1145/3477495.3531681)|Karlijn Dinnissen|Utrecht University, Utrecht, Netherlands|Streaming services have become one of today's main sources of music consumption, with music recommender systems (MRS) as important components. The MRS' choices strongly influence what users consume, and vice versa. Therefore, there is a growing interest in ensuring the fairness of these choices for all stakeholders involved. Firstly, for users, unfairness might result in some users receiving lower-quality recommendations in terms of accuracy and coverage. Secondly, item provider (i.e. artist) unfairness might result in some artists receiving less exposure, and therefore less revenue. However, it is challenging to improve fairness without a decrease in, for instance, overall recommendation quality or user satisfaction. Additional complications arise when balancing possibly domain-specific objectives for multiple stakeholders at once. While fairness research exists from both the user and artist perspective in the music domain, there is a lack of research directly consulting artists---with Ferraro et al. (2021) as an exception. When interacting with recommendation systems and evaluating their fairness, the many factors influencing recommendation system decisions can cause another difficulty: lack of transparency. Artists indicate they would appreciate more transparency in MRS---both towards the user and themselves. While e.g. Millecamp et al. (2019) use explanations to increase transparency for MRS users, to the best of our knowledge, no research has addressed improving transparency for artists this way.|流媒体服务已经成为当今音乐消费的主要来源之一，音乐推荐系统(MRS)是其中的重要组成部分。MRS 的选择强烈地影响用户的消费，反之亦然。因此，确保这些选择对所有相关利益攸关方的公平性越来越受到关注。首先，对于用户来说，不公平可能会导致一些用户在准确性和覆盖率方面得到低质量的推荐。其次，项目提供者(即艺术家)的不公平可能导致一些艺术家获得较少的曝光率，从而减少收入。然而，在不降低例如整体推荐质量或用户满意度的情况下提高公平性是具有挑战性的。当同时为多个涉众平衡可能的领域特定目标时，会出现额外的复杂性。虽然公平性研究存在于音乐领域的用户和艺术家两个角度，但缺乏直接咨询艺术家的研究——费拉罗等人(2021)是一个例外。当与推荐系统进行交互并评估其公平性时，影响推荐系统决策的诸多因素会导致另一个困难: 缺乏透明度。艺术家们表示，他们希望 MRS 更加透明——无论是对用户还是对他们自己。虽然 Millecamp 等人(2019)使用解释来增加 MRS 用户的透明度，据我们所知，还没有研究以这种方式提高艺术家的透明度。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Fairness+and+Transparency+for+Artists+in+Music+Recommender+Systems)|1|
|[Exploring Modular Task Decomposition in Cross-domain Named Entity Recognition](https://doi.org/10.1145/3477495.3531976)|Xinghua Zhang, Bowen Yu, Yubin Wang, Tingwen Liu, Taoyu Su, Hongbo Xu|Institute of Information Engineering, Chinese Academy of Sciences & University of Chinese Academy of Sciences, Beijing, China|Cross-domain Named Entity Recognition (NER) aims to transfer knowledge from the source domain to the target, alleviating expensive labeling costs in the target domain. Most prior studies acquire domain-invariant features under the end-to-end sequence-labeling framework where each token is assigned a compositional label (e.g., B-LOC). However, the complexity of cross-domain transfer may be increased over this complicated labeling scheme, which leads to sub-optimal results, especially when there are significantly distinct entity categories across domains. In this paper, we aim to explore the task decomposition in cross-domain NER. Concretely, we suggest a modular learning approach in which two sub-tasks (entity span detection and type classification) are learned by separate functional modules to perform respective cross-domain transfer with corresponding strategies. Compared with the compositional labeling scheme, the label spaces are smaller and closer across domains especially in entity span detection, leading to easier transfer in each sub-task. And then we combine two sub-tasks to achieve the final result with modular interaction mechanism, and deploy the adversarial regularization for generalized and robust learning in low-resource target domains. Extensive experiments over 10 diverse domain pairs demonstrate that the proposed method is superior to state-of-the-art cross-domain NER methods in an end-to-end fashion (about average 6.4% absolute F1 score increase). Further analyses show the effectiveness of modular task decomposition and its great potential in cross-domain NER.|跨域命名实体识别(NER)旨在将知识从源域传递到目标域，减少目标域中昂贵的标记代价。大多数先前的研究在端到端序列标记框架下获得域不变特征，其中每个标记被分配一个组合标签(例如，B-LOC)。然而，这种复杂的标记方案可能会增加跨域传输的复杂性，从而导致次优结果，特别是当跨域存在明显不同的实体类别时。本文旨在研究跨域 NER 中的任务分解问题。具体地说，我们提出了一种模块化学习方法，其中两个子任务(实体跨度检测和类型分类)由不同的功能模块学习，以执行各自的跨域传输和相应的策略。与组合标记方案相比，标记空间更小，跨域更紧密，特别是在实体跨度检测中，使得每个子任务之间的传递更加容易。然后将两个子任务结合起来，采用模块化交互机制实现最终结果，并在低资源目标域上部署广义鲁棒学习的对抗正则化。超过10个不同域对的广泛实验表明，所提出的方法以端到端的方式优于最先进的跨域 NER 方法(平均绝对 F1评分增加约6.4%)。进一步的分析表明模块化任务分解的有效性及其在跨域 NER 中的巨大潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+Modular+Task+Decomposition+in+Cross-domain+Named+Entity+Recognition)|1|
|[Graph Adaptive Semantic Transfer for Cross-domain Sentiment Classification](https://doi.org/10.1145/3477495.3531984)|Kai Zhang, Qi Liu, Zhenya Huang, Mingyue Cheng, Kun Zhang, Mengdi Zhang, Wei Wu, Enhong Chen|Anhui Province Key Lab. of Big Data Analysis and Application, University of S&T of China & State Key Laboratory of Cognitive Intelligence, Hefei, China; Hefei University of Technology, Hefei, China; Anhui Province Key Lab. of Big Data Analysis and Application, University of S&T of China, Hefei, China; Meituan, Beijing, China|Cross-domain sentiment classification (CDSC) aims to use the transferable semantics learned from the source domain to predict the sentiment of reviews in the unlabeled target domain. Existing studies in this task attach more attention to the sequence modeling of sentences while largely ignoring the rich domain-invariant semantics embedded in graph structures (i.e., the part-of-speech tags and dependency relations). As an important aspect of exploring characteristics of language comprehension, adaptive graph representations have played an essential role in recent years. To this end, in the paper, we aim to explore the possibility of learning invariant semantic features from graph-like structures in CDSC. Specifically, we present Graph Adaptive Semantic Transfer (GAST) model, an adaptive syntactic graph embedding method that is able to learn domain-invariant semantics from both word sequences and syntactic graphs. More specifically, we first raise a POS-Transformer module to extract sequential semantic features from the word sequences as well as the part-of-speech tags. Then, we design a Hybrid Graph Attention (HGAT) module to generate syntax-based semantic features by considering the transferable dependency relations. Finally, we devise an Integrated aDaptive Strategy (IDS) to guide the joint learning process of both modules. Extensive experiments on four public datasets indicate that GAST achieves comparable effectiveness to a range of state-of-the-art models.|跨域情感分类(CDSC)的目的是利用从源域学到的可转移语义来预测未标记目标域中的评论情感。现有的研究更多地关注句子的序列建模，而忽略了图结构中的丰富的领域不变语义(即词性标签和依赖关系)。自适应图表示作为探索语言理解特征的一个重要方面，近年来发挥了重要作用。为此，本文旨在探索在 CDSC 中从类图结构中学习不变语义特征的可能性。具体来说，我们提出了一种自适应语义转移(GAST)模型，这是一种自适应语法图嵌入方法，能够从词序列和语法图中学习领域不变语义。更具体地说，我们首先提出一个 POS 转换器模块来从词序列和词性标签中提取序列语义特征。然后，我们设计了一个混合图注意(HGAT)模块，通过考虑可转移的依赖关系来生成基于语法的语义特征。最后，我们设计了一个综合自适应策略(IDS)来指导两个模块的联合学习过程。对四个公共数据集的大量实验表明，GAST 达到了与一系列最先进的模型相当的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Adaptive+Semantic+Transfer+for+Cross-domain+Sentiment+Classification)|1|
|[Hybrid CNN Based Attention with Category Prior for User Image Behavior Modeling](https://doi.org/10.1145/3477495.3531854)|Xin Chen, Qingtao Tang, Ke Hu, Yue Xu, Shihang Qiu, Jia Cheng, Jun Lei|Meituan, Shanghai, China|User historical behaviors are proved useful for Click Through Rate (CTR) prediction in online advertising system. In Meituan, one of the largest e-commerce platform in China, an item is typically displayed with its image and whether a user clicks the item or not is usually influenced by its image, which implies that user's image behaviors are helpful for understanding user's visual preference and improving the accuracy of CTR prediction. Existing user image behavior models typically use a two-stage architecture, which extracts visual embeddings of images through off-the-shelf Convolutional Neural Networks (CNNs) in the first stage, and then jointly trains a CTR model with those visual embeddings and non-visual features. We find that the two-stage architecture is sub-optimal for CTR prediction. Meanwhile, precisely labeled categories in online ad systems contain abundant visual prior information, which can enhance the modeling of user image behaviors. However, off-the-shelf CNNs without category prior may extract category unrelated features, limiting CNN's expression ability. To address the two issues, we propose a hybrid CNN based attention module, unifying user's image behaviors and category prior, for CTR prediction. Our approach achieves significant improvements in both online and offline experiments on a billion scale real serving dataset.|在线广告系统中，用户的历史行为对点击率(CTR)的预测非常有用。美团是中国最大的电子商务平台之一，一个商品通常与其图像一起显示，用户是否点击该商品通常受其图像的影响，这意味着用户的图像行为有助于理解用户的视觉偏好，提高点击率预测的准确性。现有的用户图像行为模型通常采用两阶段结构，在第一阶段通过现成的卷积神经网络(CNN)提取图像的视觉嵌入，然后将这些视觉嵌入和非视觉特征联合训练 CTR 模型。我们发现两阶段结构对于 CTR 预测是次优的。同时，在线广告系统中精确标注的类别含有丰富的视觉先验信息，可以增强用户图像行为的建模能力。然而，没有类别先验的现成 CNN 可能会提取类别不相关的特征，限制了 CNN 的表达能力。针对这两个问题，提出了一种基于混合细胞神经网络的注意模块，统一用户的图像行为和类别先验，用于 CTR 预测。我们的方法在10亿规模的实际服务数据集的两个在线和离线实验中都取得了显著的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hybrid+CNN+Based+Attention+with+Category+Prior+for+User+Image+Behavior+Modeling)|1|
|[When Online Meets Offline: Exploring Periodicity for Travel Destination Prediction](https://doi.org/10.1145/3477495.3531859)|Wanjie Tao, Liangyue Li, Chen Chen, Zulong Chen, Hong Wen|University of Virginia, Petersburg, UNK, USA; Alibaba Group, Hangzhou, UNK, China|Online travel platforms (OTPs), e.g., booking.com and Ctrip.com, deliver travel experiences to online users by providing travel-related products. One key problem facing OTPs is to predict users' future travel destination, which has many important applications, e.g., proactively recommending users flight tickets or hotels in the destination city. Although much progress has been made for the next POI recommendation, they are largely sub-optimal for travel destination prediction on OTPs, due to the unique characteristics exhibited from users' travel behaviors such as offline spatial-temporal periodicity and online multi-interest exploration. In this paper, we propose an online-offline periodicity-aware information gain network, OOPIN, for travel destination prediction on OTPs. The key components of the model are (1) an offline mobility pattern extractor, which extracts spatial-temporal periodicity along with the sequential dependencies from the visited city sequence; and (2) an online multi-interests exploration module that discovers destinations that the user might be interested in but not yet visited from their online interaction data.Comprehensive experiments on real-world OTP demonstrate the superior performance of the proposed model for travel destination prediction compared with state-of-the-art methods.|在线旅游平台(OTP) ，例如 booking.com 和 Trip.com，通过提供与旅游相关的产品，为在线用户提供旅游体验。OTP 面临的一个关键问题是预测用户未来的旅游目的地，这有许多重要的应用程序，例如，主动向用户推荐目的地城市的机票或酒店。尽管下一个 POI 推荐已经取得了很大的进展，但由于用户的出行行为表现出离线时空周期性和在线多兴趣探索等独特特征，它们在 OTP 上对旅游目的地的预测大多是次优的。在本文中，我们提出了一个在线-离线周期感知信息增益网络，OOPIN，用于 OTP 上的旅游目的地预测。该模型的关键组成部分是: (1)离线移动模式提取器，它从访问的城市序列中提取出时空周期以及顺序依赖关系; (2)在线多兴趣探索模块，它从用户的在线交互数据中发现用户可能感兴趣但尚未访问的目的地。在现实生活中的 OTP 实验表明，与现有的预测方法相比，本文提出的旅游目的地预测模型具有更好的预测性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=When+Online+Meets+Offline:+Exploring+Periodicity+for+Travel+Destination+Prediction)|1|
|[Modeling User Behavior With Interaction Networks for Spam Detection](https://doi.org/10.1145/3477495.3531875)|Prabhat Agarwal, Manisha Srivastava, Vishwakarma Singh, Charles Rosenberg|Pinterest, San Francisco, CA, USA|Spam is a serious problem plaguing web-scale digital platforms which facilitate user content creation and distribution. It compromises platform's integrity, performance of services like recommendation and search, and overall business. Spammers engage in a variety of abusive and evasive behavior which are distinct from non-spammers. Users' complex behavior can be well represented by a heterogeneous graph rich with node and edge attributes. Learning to identify spammers in such a graph for a web-scale platform is challenging because of its structural complexity and size. In this paper, we propose SEINE (Spam DEtection using Interaction NEtworks), a spam detection model over a novel graph framework. Our graph simultaneously captures rich users' details and behavior and enables learning on a billion-scale graph. Our model considers neighborhood along with edge types and attributes, allowing it to capture a wide range of spammers. SEINE, trained on a real dataset of tens of millions of nodes and billions of edges, achieves a high performance of 80% recall with 1% false positive rate. SEINE achieves comparable performance to the state-of-the-art techniques on a public dataset while being pragmatic to be used in a large-scale production system.|垃圾邮件是困扰网络规模的数字平台的一个严重问题，这些平台促进了用户内容的创建和分发。它损害了平台的完整性、推荐和搜索等服务的性能以及整体业务。垃圾邮件发送者与非垃圾邮件发送者有所不同，他们从事各种辱骂和回避行为。用户的复杂行为可以很好地用一个具有丰富节点和边属性的异构图来表示。由于其结构的复杂性和规模，学习在这样一个网络规模平台的图表中识别垃圾邮件发送者是具有挑战性的。在本文中，我们提出了 SEINE (使用交互网络的垃圾邮件检测) ，一个新的图形框架下的垃圾邮件检测模型。我们的图形同时捕捉丰富用户的细节和行为，并支持在十亿级图形上学习。我们的模型考虑了邻域以及边缘类型和属性，允许它捕获范围广泛的垃圾邮件发送者。SEINE 在数千万个节点和数十亿条边的真实数据集上进行训练，实现了80% 的高性能召回率和1% 的假阳性率。SEINE 在公共数据集上实现了与最先进技术相当的性能，同时在大规模生产系统中使用也很实用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modeling+User+Behavior+With+Interaction+Networks+for+Spam+Detection)|1|
|[ArchivalQA: A Large-scale Benchmark Dataset for Open-Domain Question Answering over Historical News Collections](https://doi.org/10.1145/3477495.3531734)|Jiexin Wang, Adam Jatowt, Masatoshi Yoshikawa|University of Innsbruck, Innsbruck, Austria; Kyoto University, Kyoto, Japan|In the last few years, open-domain question answering (ODQA) has advanced rapidly due to the development of deep learning techniques and the availability of large-scale QA datasets. However, the current datasets are essentially designed for synchronic document collections (e.g., Wikipedia). Temporal news collections such as long-term news archives spanning decades are rarely used in training the models despite they are quite valuable for our society. To foster the research in the field of ODQA on such historical collections, we present ArchivalQA, a large question answering dataset consisting of 532,444 question-answer pairs which is designed for temporal news QA. We divide our dataset into four subparts based on the question difficulty levels and the containment of temporal expressions, which we believe are useful for training and testing ODQA systems characterized by different strengths and abilities. The novel QA dataset-constructing framework that we introduce can be also applied to generate high-quality, non-ambiguous questions over other types of temporal document collections.|近年来，随着深度学习技术的发展和大规模问答数据集的出现，开放域问答技术得到了迅速的发展。然而，当前的数据集基本上是为同步文档集合而设计的(例如，Wikipedia)。时态新闻集合，如跨越数十年的长期新闻档案，很少用于训练模型，尽管它们对我们的社会有相当大的价值。为了促进 ODQA 领域对这些历史文献的研究，我们提出了 ArchivalQA，一个由532,444个问答对组成的大型问答数据集，它是为时事新闻 QA 而设计的。我们根据问题的难度水平和时间表达式的限制将数据集分为四个子部分，我们相信这对于训练和测试具有不同拥有属性和能力的 ODQA 系统是有用的。我们介绍的新的 QA 数据集构建框架也可以应用于生成高质量的、非歧义的问题，而不是其他类型的时态文档集合。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ArchivalQA:+A+Large-scale+Benchmark+Dataset+for+Open-Domain+Question+Answering+over+Historical+News+Collections)|1|
|[Structure and Semantics Preserving Document Representations](https://doi.org/10.1145/3477495.3532062)|Natraj Raman, Sameena Shah, Manuela Veloso|J.P. Morgan AI Research, New York, NY, USA; J.P. Morgan AI Research, London, United Kingdom|Retrieving relevant documents from a corpus is typically based on the semantic similarity between the document content and query text. The inclusion of structural relationship between documents can benefit the retrieval mechanism by addressing semantic gaps. However, incorporating these relationships requires tractable mechanisms that balance structure with semantics and take advantage of the prevalent pre-train/fine-tune paradigm. We propose here a holistic approach to learning document representations by integrating intra-document content with inter-document relations. Our deep metric learning solution analyzes the complex neighborhood structure in the relationship network to efficiently sample similar/dissimilar document pairs and defines a novel quintuplet loss function that simultaneously encourages document pairs that are semantically relevant to be closer and structurally unrelated to be far apart in the representation space. Furthermore, the separation margins between the documents are varied flexibly to encode the heterogeneity in relationship strengths. The model is fully fine-tunable and natively supports query projection during inference. We demonstrate that it outperforms competing methods on multiple datasets for document retrieval tasks.|从语料库中检索相关文档通常是基于文档内容和查询文本之间的语义相似性。文档之间结构化关系的引入有利于解决语义空白，提高检索机制的效率。然而，合并这些关系需要易处理的机制，平衡结构与语义，并利用流行的预训练/微调范式的优势。在这里，我们提出了一种通过整合文档内容和文档间关系来学习文档表示的整体方法。我们的深度度量学习解决方案分析了关系网络中复杂的邻域结构，以有效地采样相似/不相似的文档对，并定义了一种新的五元组丢失函数，同时鼓励语义相关的文档对在表示空间中更加紧密，结构上不相关。此外，文档之间的分离边界可以灵活变化，以编码关系强度的异质性。该模型是完全可调的，并且在推理过程中本身支持查询投影。我们展示了它在多个数据集的文献检索任务中优于竞争方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Structure+and+Semantics+Preserving+Document+Representations)|1|
|[Aspect Feature Distillation and Enhancement Network for Aspect-based Sentiment Analysis](https://doi.org/10.1145/3477495.3531938)|Rui Liu, Jiahao Cao, Nannan Sun, Lei Jiang|Institute of Information Engineering, Chinese Academy of Sciences & University of Chinese Academy of Sciences, Beijing, China|Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis task designed to identify the polarity of a target aspect. Some works introduce various attention mechanisms to fully mine the relevant context words of different aspects, and use the traditional cross-entropy loss to fine-tune the models for the ABSA task. However, the attention mechanism paying partial attention to aspect-unrelated words inevitably introduces irrelevant noise. Moreover, the cross-entropy loss lacks discriminative learning of features, which makes it difficult to exploit the implicit information of intra-class compactness and inter-class separability. To overcome these challenges, we propose an Aspect Feature Distillation and Enhancement Network (AFDEN) for the ABSA task. We first propose a dual-feature extraction module to extract aspect-related and aspect-unrelated features through the attention mechanisms and graph convolutional networks. Then, to eliminate the interference of aspect-unrelated words, we design a novel aspect-feature distillation module containing a gradient reverse layer that learns aspect-unrelated contextual features through adversarial training, and an aspect-specific orthogonal projection layer to further project aspect-related features into the orthogonal space of aspect-unrelated features. Finally, we propose an aspect-feature enhancement module that leverages supervised contrastive learning to capture the implicit information between the same sentiment labels and between different sentiment labels. Experimental results on three public datasets demonstrate that our AFDEN model achieves state-of-the-art performance and verify the effectiveness and robustness of our model.|基于方面的情绪分析(ABSA)是一个细粒度的情绪分析任务，旨在识别目标方面的极性。一些工作引入了各种注意机制来充分挖掘不同方面的相关上下文词，并利用传统的交叉熵损失对 ABSA 任务的模型进行了微调。然而，部分关注体无关词的注意机制不可避免地会引入不相关的噪声。此外，交叉熵损失缺乏对特征的判别学习，这使得利用类内紧性和类间可分性的隐含信息变得困难。为了克服这些挑战，我们提出了一个面向特征提取和增强网络(AFDEN)的 ABSA 任务。首先提出一个双特征提取模块，通过注意机制和图卷积网络提取与方面相关和与方面无关的特征。然后，为了消除方面无关词的干扰，设计了一个新的方面特征提取模块，该模块包含一个梯度反向层，通过对抗性训练学习方面无关的上下文特征，以及一个方面特定的正交投影层，进一步将方面相关特征投影到方面无关特征的正交空间中。最后，我们提出了一个侧面特征增强模块，该模块利用监督对比学习来捕获相同情感标签之间和不同情感标签之间的隐含信息。在三个公共数据集上的实验结果表明，我们的 AFDEN 模型达到了最先进的性能，验证了模型的有效性和鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Aspect+Feature+Distillation+and+Enhancement+Network+for+Aspect-based+Sentiment+Analysis)|1|
|[Detecting Frozen Phrases in Open-Domain Question Answering](https://doi.org/10.1145/3477495.3531793)|Mostafa Yadegari, Ehsan Kamalloo, Davood Rafiei|University of Alberta, Edmonton, AB, Canada|There is essential information in the underlying structure of words and phrases in natural language questions, and this structure has been extensively studied. In this paper, we study one particular structure, referred to as frozen phrases, that is highly expected to transfer as a whole from questions to answer passages. Frozen phrases, if detected, can be helpful in open-domain Question Answering (QA) where identifying the localized context of a given input question is crucial. An interesting question is if frozen phrases can be accurately detected. We cast the problem as a sequence-labeling task and create synthetic data from existing QA datasets to train a model. We further plug this model into a sparse retriever that is made aware of the detected phrases. Our experiments reveal that detecting frozen phrases whose presence in answer documents are highly plausible yields significant improvements in retrievals as well as in the end-to-end accuracy of open-domain QA models.|自然语言问题中的词和短语的基本结构蕴含着重要的信息，这种结构已经得到了广泛的研究。在本文中，我们研究了一个特殊的结构，称为冻结短语，这是高度期望转移作为一个整体从问题，以回答段落。如果检测到冻结的短语，在开放域问答(QA)中可能会有所帮助，因为识别给定输入问题的本地化上下文是至关重要的。一个有趣的问题是，是否可以准确地检测到冻结的短语。我们将问题转换为序列标记任务，并从现有的 QA 数据集创建合成数据来训练模型。我们进一步将这个模型插入到一个稀疏的检索器中，这个检索器可以识别检测到的短语。我们的实验表明，检测在应答文档中出现的高度合理的冻结短语可以显著提高检索效率以及开放域 QA 模型的端到端准确性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+Frozen+Phrases+in+Open-Domain+Question+Answering)|1|
|[Understanding User Satisfaction with Task-oriented Dialogue Systems](https://doi.org/10.1145/3477495.3531798)|Clemencia Siro, Mohammad Aliannejadi, Maarten de Rijke|University of Amsterdam, Amsterdam, Netherlands|\beginabstract \AcpDS are evaluated depending on their type and purpose. Two categories are often distinguished: \beginenumerate* \item \acpTDS, which are typically evaluated on utility, i.e., their ability to complete a specified task, and \item open-domain chat-bots, which are evaluated on the user experience, i.e., based on their ability to engage a person. \endenumerate* What is the influence of user experience on the user satisfaction rating of \acpTDS as opposed to, or in addition to, utility ? We collect data by providing an additional annotation layer for dialogues sampled from the ReDial dataset, a widely used conversational recommendation dataset. Unlike prior work, we annotate the sampled dialogues at both the turn and dialogue level on six dialogue aspects: relevance, interestingness, understanding, task completion, efficiency, and interest arousal. The annotations allow us to study how different dialogue aspects influence user satisfaction. We introduce a comprehensive set of user experience aspects derived from the annotators' open comments that can influence users' overall impression. We find that the concept of satisfaction varies across annotators and dialogues, and show that a relevant turn is significant for some annotators, while for others, an interesting turn is all they need. Our analysis indicates that the proposed user experience aspects provide a fine-grained analysis of user satisfaction that is not captured by a monolithic overall human rating. \endabstract|根据其类型和用途对 AcpDS 进行评估。通常区分为两类: 初始列举 * 项目 acpTDS，通常根据实用性进行评估，例如，它们完成指定任务的能力; 以及项目开放域聊天机器人，根据用户体验进行评估，例如，根据它们吸引人的能力进行评估。相对于效用，用户体验对 acpTDS 的用户满意度有什么影响？我们通过为从 ReDial 数据集(一个广泛使用的会话推荐数据集)采样的对话提供额外的注释层来收集数据。与之前的工作不同，我们在转向和对话两个层面对样本对话进行了注释: 相关性、趣味性、理解性、任务完成、效率和兴趣激发。注释允许我们研究不同的对话方面如何影响用户满意度。我们介绍了一套全面的用户体验方面来自注释者的开放评论，可以影响用户的整体印象。我们发现满意度的概念在不同的注释者和对话中是不同的，并且表明相关的转向对于一些注释者是重要的，而对于其他人，一个有趣的转向是他们所需要的。我们的分析表明，建议的用户体验方面提供了一个细粒度的用户满意度分析，而不是由单一的整体人类评分。结束摘要|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+User+Satisfaction+with+Task-oriented+Dialogue+Systems)|1|
|[On Survivorship Bias in MS MARCO](https://doi.org/10.1145/3477495.3531832)|Prashansa Gupta, Sean MacAvaney|University of Glasgow, Glasgow, United Kingdom|Survivorship bias is the tendency to concentrate on the positive outcomes of a selection process and overlook the results that generate negative outcomes. We observe that this bias could be present in the popular MS MARCO dataset, given that annotators could not find answers to 38--45% of the queries, leading to these queries being discarded in training and evaluation processes. Although we find that some discarded queries in MS MARCO are ill-defined or otherwise unanswerable, many are valid questions that could be answered had the collection been annotated more completely (around two thirds using modern ranking techniques). This survivability problem distorts the MS MARCO collection in several ways. We find that it affects the natural distribution of queries in terms of the type of information needed. When used for evaluation, we find that the bias likely yields a significant distortion of the absolute performance scores observed. Finally, given that MS MARCO is frequently used for model training, we train models based on subsets of MS MARCO that simulates more survivorship bias. We find that models trained in this setting are up to 9.9% worse when evaluated on versions of the dataset with more complete annotations, and up to 3.5% worse at zero-shot transfer. Our findings are complementary to other recent suggestions for further annotation of MS MARCO, but with a focus on discarded queries.|倖存者偏差就是倾向于专注于选择过程的积极结果，而忽视产生消极结果的结果。我们观察到，这种偏见可能存在于流行的 MS MARCO 数据集中，因为注释者无法找到38-45% 的查询的答案，导致这些查询在培训和评估过程中被丢弃。尽管我们发现 MS MARCO 中的一些丢弃的查询是不明确的或者无法回答的，但是许多是有效的问题，如果集合被更完整地注释(使用现代排序技术约占三分之二) ，则可以回答这些问题。这个生存性问题在几个方面扭曲了 MS MARCO 集合。我们发现，根据所需信息的类型，它会影响查询的自然分布。当用于评估时，我们发现这种偏差很可能导致观察到的绝对绩效得分的显著扭曲。最后，鉴于微软 MARCO 经常被用于模型训练，我们基于微软 MARCO 的子集来训练模型，以模拟更多的倖存者偏差。我们发现，在这种情况下训练的模型在使用更完整注释的数据集版本进行评估时差异高达9.9% ，而在零镜头传输时差异高达3.5% 。我们的发现是对其他最近的建议进一步注释微软 MARCO 的补充，但重点放在丢弃的查询。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Survivorship+Bias+in+MS+MARCO)|1|
|[Bias Mitigation for Evidence-aware Fake News Detection by Causal Intervention](https://doi.org/10.1145/3477495.3531850)|Junfei Wu, Qiang Liu, Weizhi Xu, Shu Wu|Institute of Automation, Chinese Academy of Sciences & University of Chinese Academy of Sciences, Beijing, China|Evidence-based fake news detection is to judge the veracity of news against relevant evidences. However, models tend to memorize the dataset biases within spurious correlations between news patterns and veracity labels as shortcuts, rather than learning how to integrate the information behind them to reason. As a consequence, models may suffer from a serious failure when facing real-life conditions where most news has different patterns. Inspired by the success of causal inference, we propose a novel framework for debiasing evidence-based fake news detection\footnoteCode available at https://github.com/CRIPAC-DIG/CF-FEND by causal intervention. Under this framework, the model is first trained on the original biased dataset like ordinary work, then it makes conventional predictions and counterfactual predictions simultaneously in the testing stage, where counterfactual predictions are based on the intervened evidence. Relatively unbiased predictions are obtained by subtracting intervened outputs from the conventional ones. Extensive experiments conducted on several datasets demonstrate our method's effectiveness and generality on debiased datasets.|基于证据的假新闻检测就是根据相关证据来判断新闻的真实性。然而，模型倾向于记住新闻模式和准确性标签之间的虚假相关性中的数据集偏差作为快捷方式，而不是学习如何整合它们背后的信息进行推理。因此，模型可能会遭受严重的失败，当面对现实生活的情况下，大多数新闻有不同的模式。受到因果推理成功的启发，我们提出了一个新的框架，用于消除基于证据的假新闻检测脚注通过因果干预可以获得的 https://github.com/cripac-dig/cf-fend 代码。在此框架下，该模型首先像普通工作一样对原始偏差数据集进行训练，然后在测试阶段同时进行常规预测和反事实预测，反事实预测是基于介入的证据。相对无偏的预测是通过减去传统的干预输出得到的。在多个数据集上进行的大量实验证明了该方法在去偏数据集上的有效性和通用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bias+Mitigation+for+Evidence-aware+Fake+News+Detection+by+Causal+Intervention)|1|
|[Preference Enhanced Social Influence Modeling for Network-Aware Cascade Prediction](https://doi.org/10.1145/3477495.3532042)|Likang Wu, Hao Wang, Enhong Chen, Zhi Li, Hongke Zhao, Jianhui Ma|University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence, Hefei, China; Tianjin University, Hefei, China|Network-aware cascade size prediction aims to predict the final reposted number of user-generated information via modeling the propagation process in social networks. Estimating the user's reposting probability by social influence, namely state activation plays an important role in the information diffusion process. Therefore, Graph Neural Networks (GNN), which can simulate the information interaction between nodes, has been proved as an effective scheme to handle this prediction task. However, existing studies including GNN-based models usually neglect a vital factor of user's preference which influences the state activation deeply. To that end, we propose a novel framework to promote cascade size prediction by enhancing the user preference modeling according to three stages, i.e., preference topics generation, preference shift modeling, and social influence activation. Our end-to-end method makes the user activating process of information diffusion more adaptive and accurate. Extensive experiments on two large-scale real-world datasets have clearly demonstrated the effectiveness of our proposed model compared to state-of-the-art baselines.|网络感知级联规模预测的目的是通过建立社交网络中的传播过程模型来预测用户生成信息的最终转发数量。通过社会影响即状态激活来估计用户的转发概率在信息传播过程中起着重要作用。因此，能够模拟节点间信息交互的图神经网络(GNN)已被证明是处理这一预测任务的有效方案。然而，现有的研究，包括基于 GNN 的模型，往往忽略了用户偏好的一个重要因素，这个因素对状态激活有着深刻的影响。为此，我们提出了一个新的框架来促进级联规模预测，通过增强用户偏好建模的三个阶段，即偏好主题的生成，偏好转移模型和社会影响激活。我们的端到端方法使得信息传播的用户激活过程更具适应性和准确性。在两个大规模真实世界数据集上的大量实验已经清楚地证明了我们提出的模型相对于最先进的基线的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Preference+Enhanced+Social+Influence+Modeling+for+Network-Aware+Cascade+Prediction)|1|
|[Users and Contemporary SERPs: A (Re-)Investigation](https://doi.org/10.1145/3477495.3531719)|Nirmal Roy, David Maxwell, Claudia Hauff|Delft University of Technology, Delft, Netherlands|TheSearch Engine Results Page (SERP) has evolved significantly over the last two decades, moving away from the simple ten blue links paradigm to considerably more complex presentations that contain results from multiple verticals and granularities of textual information. Prior works have investigated how user interactions on the SERP are influenced by the presence or absence of heterogeneous content (e.g., images, videos, or news content), the layout of the SERP (\emphlist vs. grid layout), and task complexity. In this paper, we reproduce the user studies conducted in prior works---specifically those of~\citetarguello2012task and~\citetsiu2014first ---to explore to what extent the findings from research conducted five to ten years ago still hold today as the average web user has become accustomed to SERPs with ever-increasing presentational complexity. To this end, we designed and ran a user study with four different SERP interfaces:(i) ~\empha heterogeneous grid ;(ii) ~\empha heterogeneous list ;(iii) ~\empha simple grid ; and(iv) ~\empha simple list. We collected the interactions of $41$ study participants over $12$ search tasks for our analyses. We observed that SERP types and task complexity affect user interactions with search results. We also find evidence to support most (6 out of 8) observations from~\citearguello2012task,siu2014first indicating that user interactions with different interfaces and to solve tasks of different complexity have remained mostly similar over time.|搜索引擎结果页面(SERP)在过去的二十年中发生了巨大的变化，从简单的十个蓝色链接范式转变为包含多个垂直结果和文本信息粒度的更复杂的表示。先前的工作已经研究了用户在 SERP 上的交互是如何受到异构内容(如图像、视频或新闻内容)、 SERP 布局(强调列表与网格布局)和任务复杂性的影响的。在这篇论文中，我们重现了在以前的研究中进行的用户研究——特别是那些 ~ citetguello2012task 和 ~ citetsiu2014first 的研究——来探索五到十年前的研究结果在多大程度上仍然适用于今天，因为普通的网络用户已经习惯了不断增加的表现复杂度的 SERP。为此，我们设计并运行了一个使用四种不同的 SERP 接口的用户研究: (i) ~ empa 异构网格; (ii) ~ empa 异构列表; (iii) ~ empa 简单网格; 和(iv) ~ empa 简单列表。我们收集了 $41 $研究参与者在 $12 $搜索任务中的交互作用，用于我们的分析。我们观察到 SERP 类型和任务复杂性影响用户与搜索结果的交互。我们还发现支持 ~ citearguello2012task，siu2014的大多数(8个中的6个)观察的证据首先表明，用户与不同界面的交互以及解决不同复杂度的任务随着时间的推移大多保持相似。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Users+and+Contemporary+SERPs:+A+(Re-)Investigation)|1|
|[ReMeDi: Resources for Multi-domain, Multi-service, Medical Dialogues](https://doi.org/10.1145/3477495.3531809)|Guojun Yan, Jiahuan Pei, Pengjie Ren, Zhaochun Ren, Xin Xin, Huasheng Liang, Maarten de Rijke, Zhumin Chen|Shandong University, Qingdao, China; WeChat Tencent, Qingdao, China; University of Amsterdam, Amsterdam, Netherlands|\AcpMDS aim to assist doctors and patients with a range of professional medical services, i.e., diagnosis, treatment and consultation. The development of \acpMDS is hindered because of a lack of resources. In particular. \beginenumerate* [label=(\arabic*) ] \item there is no dataset with large-scale medical dialogues that covers multiple medical services and contains fine-grained medical labels (i.e., intents, actions, slots, values), and \item there is no set of established benchmarks for \acpMDS for multi-domain, multi-service medical dialogues. \endenumerate* In this paper, we present \acsReMeDi, a set of \aclReMeDi \acusedReMeDi. ØurResources consists of two parts, the ØurResources dataset and the ØurResources benchmarks. The ØurResources dataset contains 96,965 conversations between doctors and patients, including 1,557 conversations with fine-gained labels. It covers 843 types of diseases, 5,228 medical entities, and 3 specialties of medical services across 40 domains. To the best of our knowledge, the ØurResources dataset is the only medical dialogue dataset that covers multiple domains and services, and has fine-grained medical labels. The second part of the ØurResources resources consists of a set of state-of-the-art models for (medical) dialogue generation. The ØurResources benchmark has the following methods: \beginenumerate* \item pretrained models (i.e., BERT-WWM, BERT-MED, GPT2, and MT5) trained, validated, and tested on the ØurResources dataset, and \item a \acfSCL method to expand the ØurResources dataset and enhance the training of the state-of-the-art pretrained models. \endenumerate* We describe the creation of the ØurResources dataset, the ØurResources benchmarking methods, and establish experimental results using the ØurResources benchmarking methods on the ØurResources dataset for future research to compare against. With this paper, we share the dataset, implementations of the benchmarks, and evaluation scripts.|该计划旨在为医生和病人提供一系列的专业医疗服务，包括诊断、治疗和咨询。由于缺乏资源，阻碍了 ACpMDS 的发展。尤其是。开始列举 * [ label = (阿拉伯语 *)]项目没有涵盖多种医疗服务并包含细粒度医疗标签(即意图，行动，插槽，价值)的大规模医疗对话的数据集，并且没有一套针对多领域，多服务医疗对话的 acpMDS 的既定基准。在本文中，我们介绍 acsReMeDi，一组 aclReMeDi acusedReMeDi。ØurResources 由两部分组成，ØurResources 数据集和 ØurResources 基准。ØurResources 的数据集包含96,965次医生与病人之间的对话，其中包括1557次带有罚款标签的对话。它涵盖了40个领域的843种疾病，5228个医疗实体和3个医疗服务专业。就我们所知，ØurResources 数据集是唯一一个涵盖多个领域和服务的医疗对话数据集，并具有细粒度的医疗标签。ØurResources 资源的第二部分包含一组用于(医疗)对话生成的最先进模型。ØurResources 基准有以下方法: 在 ØurResources 数据集上训练、验证和测试的 * 项目预训模型(即 BERT-WMM、 BERT-MED、 GPT2和 MT5) ，以及扩展 ØurResources 数据集和加强最先进的预训模型的训练的 acfSCL 方法。* 我们描述了 ØurResources 数据集的创建、 ØurResources 基准测试方法，并使用 ØurResources 基准测试方法建立了实验结果，以便将来的研究与之进行比较。在本文中，我们共享数据集、基准测试的实现和评估脚本。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ReMeDi:+Resources+for+Multi-domain,+Multi-service,+Medical+Dialogues)|1|
|[Online DATEing: A Web Interface for Temporal Annotations](https://doi.org/10.1145/3477495.3531670)|Dennis Aumiller, Satya Almasian, David Pohl, Michael Gertz|Heidelberg University, Heidelberg, Germany|Despite more than two decades of research on temporal tagging and temporal relation extraction, usable tools for annotating text remain very basic and hard to set up from an average end-user perspective, limiting the applicability of developments to a selected group of invested researchers. In this work, we aim to increase the accessibility of temporal tagging systems by presenting an intuitive web interface, called "Online DATEing", which simplifies the interaction with existing temporal annotation frameworks. Our system integrates several approaches in a single interface and streamlines the process of importing (and tagging) groups of documents, as well as making it accessible through a programmatic API. It further enables users to interactively investigate and visualize tagged texts, and is designed with an extensible API for the inclusion of new models or data formats. A web demonstration of our tool is available at https://onlinedating.ifi.uni-heidelberg.de and public code accessible at https://github.com/satya77/Temporal_Tagger_Service.|尽管在时间标签和时间关系提取方面进行了二十多年的研究，但用于文本注释的可用工具仍然非常基础，从一般最终用户的角度来看很难建立起来，这限制了开发的适用性，使其只能适用于一组经过投资的研究人员。在这项工作中，我们的目标是通过提出一个直观的网络界面，称为“在线 DATEing”，简化与现有的时态标注框架的交互，提高时态标注系统的可访问性。我们的系统在单个接口中集成了多种方法，并简化了导入(和标记)文档组的过程，同时还通过编程 API 使其可访问。它进一步使用户能够交互式地调查和可视化标记的文本，并且设计了一个可扩展的 API，用于包含新的模型或数据格式。我们的工具的网页演示可在 https://onlinedating.ifi.uni-heidelberg.de 下载，公众代码亦可在 https://github.com/satya77/temporal_tagger_service 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+DATEing:+A+Web+Interface+for+Temporal+Annotations)|1|
|[Few-shot Node Classification on Attributed Networks with Graph Meta-learning](https://doi.org/10.1145/3477495.3531978)|Yonghao Liu, Mengyu Li, Ximing Li, Fausto Giunchiglia, Xiaoyue Feng, Renchu Guan|Jilin University, Changchun, China; University of Trento, Trento, Italy|Attributed networks, as a manifestation of data in non-Euclidean domains, have a wide range of applications in the real world, such as molecular property prediction, social network analysis and anomaly detection. Node classification, as a fundamental research problem in attributed networks, has attracted increasing attention among research communities. However, most existing models cannot be directly applied to the data with limited labeled instances (\textiti.e., the few-shot scenario). Few-shot node classification on attributed networks is gradually becoming a research hotspot. Although several methods aim to integrate meta-learning with graph neural networks to address this problem, some limitations remain. First, they all assume node representation learning using graph neural networks in homophilic graphs. %Hence, suboptimal performance is obtained when these models are applied to heterophilic graphs. Second, existing models based on meta-learning entirely depend on instance-based statistics. %which in few-shot settings are unavoidably degraded by data noise or outliers. Third, most previous models treat all sampled tasks equally and fail to adapt their uniqueness. %which has a significant impact on the overall performance of the model. To solve the above three limitations, we propose a novel graph Meta -learning framework called G raph learning based on P rototype and S caling & shifting transformation (Meta-GPS ). More specifically, we introduce an efficient method for learning expressive node representations even on heterophilic graphs and propose utilizing a prototype-based approach to initialize parameters in meta-learning. Moreover, we also leverage S$^2$ (scaling & shifting) transformation to learn effective transferable knowledge from diverse tasks. Extensive experimental results on six real-world datasets demonstrate the superiority of our proposed framework, which outperforms other state-of-the-art baselines by up to 13% absolute improvement in terms of related metrics.|属性网络作为非欧几里德领域数据的表现形式，在现实世界中有着广泛的应用，如分子特性预测、社会网络分析和异常检测分析。节点分类作为属性网络的一个基础研究问题，越来越受到研究界的重视。然而，大多数现有的模型不能直接应用于带有有限标签的实例的数据(textti.e.a few-shot 场景)。基于属性网络的少镜头节点分类正逐渐成为研究热点。虽然有几种方法旨在将元学习与图神经网络相结合来解决这个问题，但仍然存在一些局限性。首先，它们都假定在同态图中使用图神经网络进行节点表示学习。因此，当这些模型应用于异质图时，得到了次优的性能。其次，现有的基于元学习的模型完全依赖于基于实例的统计。在少镜头设置中，不可避免地会受到数据噪声或异常值的影响。第三，大多数以前的模型对所有抽样任务一视同仁，不能适应它们的唯一性。% ，这对模型的整体性能有重大影响。为了解决这三个问题，提出了一种新的图元学习框架——基于 P 原型和 S 标定与移位变换的 G 图学习(Meta-GPS)。更具体地说，我们介绍了一种有效的学习表达式节点表示的方法，甚至在异质图上，并提出了利用基于原型的方法来初始化元学习中的参数。此外，我们还利用 S $^ 2 $(缩放和转移)转换来学习不同任务中有效的可转移知识。在六个真实世界数据集上的大量实验结果证明了我们提出的框架的优越性，它在相关指标方面比其他最先进的基线表现出高达13% 的绝对改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Few-shot+Node+Classification+on+Attributed+Networks+with+Graph+Meta-learning)|1|
|[Co-clustering Interactions via Attentive Hypergraph Neural Network](https://doi.org/10.1145/3477495.3531868)|Tianchi Yang, Cheng Yang, Luhao Zhang, Chuan Shi, Maodi Hu, Huaijun Liu, Tao Li, Dong Wang|Meituan, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China|With the rapid growth of interaction data, many clustering methods have been proposed to discover interaction patterns as prior knowledge beneficial to downstream tasks. Considering that an interaction can be seen as an action occurring among multiple objects, most existing methods model the objects and their pair-wise relations as nodes and links in graphs. However, they only model and leverage part of the information in real entire interactions, i.e., either decompose the entire interaction into several pair-wise sub-interactions for simplification, or only focus on clustering some specific types of objects, which limits the performance and explainability of clustering. To tackle this issue, we propose to Co-cluster the Interactions via Attentive Hypergraph neural network (CIAH). Particularly, with more comprehensive modeling of interactions by hypergraph, we propose an attentive hypergraph neural network to encode the entire interactions, where an attention mechanism is utilized to select important attributes for explanations. Then, we introduce a salient method to guide the attention to be more consistent with real importance of attributes, namely saliency-based consistency. Moreover, we propose a novel co-clustering method to perform a joint clustering for the representations of interactions and the corresponding distributions of attribute selection, namely cluster-based consistency. Extensive experiments demonstrate that our CIAH significantly outperforms state-of-the-art clustering methods on both public datasets and real industrial datasets.|随着交互数据的快速增长，人们提出了许多聚类方法来发现交互模式作为有利于下游任务的先验知识。考虑到一个交互可以看作是多个对象之间的一个动作，现有的方法大多将对象及其成对关系建模为图中的节点和链接。然而，他们只是在真实的整个交互中对部分信息进行建模和利用，也就是说，要么将整个交互分解为几个成对的子交互以进行简化，要么只关注某些特定类型的对象的聚类，这限制了聚类的性能和可解释性。为了解决这一问题，我们提出了通过注意超图神经网络(CIAH)对相互作用进行共聚类。特别是，随着超图对交互作用建模的不断深入，我们提出了一种注意超图神经网络对整个交互作用进行编码，利用注意机制选择重要属性进行解释。然后，我们介绍了一种引导注意与属性的真实重要性更加一致的显著方法，即基于显著性的一致性。此外，我们提出了一种新的共聚类方法，即基于聚类的一致性，对交互作用的表示和相应的属性选择分布进行联合聚类。大量的实验表明，我们的 CIAH 在公共数据集和实际工业数据集上都显著优于最先进的聚类方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Co-clustering+Interactions+via+Attentive+Hypergraph+Neural+Network)|1|
|[Mutual Disentanglement Learning for Joint Fine-Grained Sentiment Classification and Controllable Text Generation](https://doi.org/10.1145/3477495.3532029)|Hao Fei, Chenliang Li, Donghong Ji, Fei Li|Wuhan University, Wuhan, China|Fine-grained sentiment classification (FGSC) task and fine-grained controllable text generation (FGSG) task are two representative applications of sentiment analysis, two of which together can actually form an inverse task prediction, i.e., the former aims to infer the fine-grained sentiment polarities given a text piece, while the latter generates text content that describes the input fine-grained opinions. Most of the existing work solves the FGSC and the FGSG tasks in isolation, while ignoring the complementary benefits in between. This paper combines FGSC and FGSG as a joint dual learning system, encouraging them to learn the advantages from each other. Based on the dual learning framework, we further propose decoupling the feature representations in two tasks into fine-grained aspect-oriented opinion variables and content variables respectively, by performing mutual disentanglement learning upon them. We also propose to transform the difficult "data-to-text'' generation fashion widely used in FGSG into an easier text-to-text generation fashion by creating surrogate natural language text as the model inputs. Experimental results on 7 sentiment analysis benchmarks including both the document-level and sentence-level datasets show that our method significantly outperforms the current strong-performing baselines on both the FGSC and FGSG tasks. Automatic and human evaluations demonstrate that our FGSG model successfully generates fluent, diverse and rich content conditioned on fine-grained sentiments.|细粒度情绪分类任务(FGSC)和细粒度可控文本生成任务(FGSG)是情绪分析的两个具有代表性的应用，两者结合起来实际上可以形成一个反向的任务预测，即前者旨在推断给定文本片段的细粒度情绪极性，而后者生成描述输入细粒度意见的文本内容。现有的大多数工作是孤立地解决 FGSC 和 FGSG 任务，而忽略了它们之间的互补性。本文将 FGSC 和 FGSG 作为一个联合的双学习系统结合起来，鼓励它们相互学习对方的优势。在对偶学习框架的基础上，进一步提出将两个任务的特征表示分别解耦为细粒度面向方面的观点变量和内容变量，并对它们进行相互解缠学习。我们还建议通过创建替代自然语言文本作为模型输入，将 FGSG 中广泛使用的困难的“数据到文本”生成方式转换为更容易的文本到文本生成方式。对包括文档级和句子级数据集在内的7个情绪分析基准的实验结果表明，我们的方法在 FGSC 和 FGSG 任务上都显著优于目前表现强劲的基准。自动和人工评估表明，我们的 FGSG 模型成功地产生流畅，多样和丰富的内容条件下细粒度的情感。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mutual+Disentanglement+Learning+for+Joint+Fine-Grained+Sentiment+Classification+and+Controllable+Text+Generation)|1|
|[Learning Disentangled Representations for Counterfactual Regression via Mutual Information Minimization](https://doi.org/10.1145/3477495.3532011)|Mingyuan Cheng, Xinru Liao, Quan Liu, Bin Ma, Jian Xu, Bo Zheng|Alibaba Group, Hangzhou, China; Alibaba Group, Beijing, China|Learning individual-level treatment effect is a fundamental problem in causal inference and has received increasing attention in many areas, especially in the user growth area which concerns many internet companies. Recently, disentangled representation learning methods that decompose covariates into three latent factors, including instrumental, confounding and adjustment factors, have witnessed great success in treatment effect estimation. However, it remains an open problem how to learn the underlying disentangled factors precisely. Specifically, previous methods fail to obtain independent disentangled factors, which is a necessary condition for identifying treatment effect. In this paper, we propose Disentangled Representations for Counterfactual Regression via Mutual Information Minimization (MIM-DRCFR), which uses a multi-task learning framework to share information when learning the latent factors and incorporates MI minimization learning criteria to ensure the independence of these factors. Extensive experiments including public benchmarks and real-world industrial user growth datasets demonstrate that our method performs much better than state-of-the-art methods.|个体水平治疗效应的学习是因果推理中的一个基本问题，在许多领域，特别是在涉及到许多互联网公司的用户增长领域受到越来越多的关注。近年来，将协变量分解为工具因素、混杂因素和调整因素的分离表征学习方法在评估治疗效果方面取得了很大的成功。然而，如何准确地认识这些潜在的分离因素仍然是一个悬而未决的问题。具体来说，以往的方法不能获得独立的解缠因子，这是确定治疗效果的必要条件。本文提出了基于互信息最小化的反事实回归分离表示方法(MIM-DRCFR) ，该方法采用多任务学习框架，在学习潜在因素时共享信息，并结合 MI 最小化学习准则，保证了这些因素的独立性。包括公共基准和真实世界工业用户增长数据集在内的大量实验表明，我们的方法比最先进的方法表现得更好。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Disentangled+Representations+for+Counterfactual+Regression+via+Mutual+Information+Minimization)|1|
|[L3E-HD: A Framework Enabling Efficient Ensemble in High-Dimensional Space for Language Tasks](https://doi.org/10.1145/3477495.3531761)|Fangxin Liu, Haomin Li, Xiaokang Yang, Li Jiang|Shanghai Jiao Tong University, Shanghai , China; Shanghai Jiao Tong University, Shanghai, China; Tianjin University, Tianjin, China|Brain-inspired hyperdimensional computing (HDC) has been introduced as an alternative computing paradigm to achieve efficient and robust learning. HDC simulates cognitive tasks by mapping all data points to patterns of neural activity in the high-dimensional space, which has demonstrated promising performances in a wide range of applications such as robotics, biomedical signal processing, and genome sequencing. Language tasks, generally solved using machine learning methods, are widely deployed on low-power embedded devices. However, existing HDC solutions suffer from major challenges that impede the deployment of low-power embedded devices: the storage and computation overhead of HDC models grows dramatically with (i) the number of dimensions and (ii) the complex similarity metric during the inference. In this paper, we proposed a novel ensemble framework for the language task, termed L3E-HD, which enables efficient HDC on low-power edge devices. L3E-HD accelerates the inference by mapping data points to a high-dimensional binary space to simplify similarity search, which dominates costly and frequent operation in HDC. Through marrying HDC with the ensemble technique, L3E-HD also addresses the severe accuracy degradation induced by the compression of the dimension and precision of the model. Our experiments show that the ensemble technique is naturally a perfect fit to boost HDCs. We find that our L3E-HD, which is faster, more efficient, and more accurate than conventional machine learning methods, can even surpass the accuracy of the full-precision model at a smaller model size. Code is released at: https://github.com/MXHX7199/SIGIR22-EnsembleHDC.|大脑启发的高维计算(HDC)已被引入作为一种替代计算范式，以实现有效和健壮的学习。HDC 通过将所有数据点映射到高维空间中的神经活动模式来模拟认知任务，这已经在机器人、生医信号处理和基因组测序等广泛的应用中展示了有前途的性能。语言任务通常采用机器学习方法来解决，在低功耗嵌入式设备上得到了广泛的应用。然而，现有的 HDC 解决方案面临着阻碍低功耗嵌入式设备部署的主要挑战: HDC 模型的存储和计算开销随着(i)维数和(ii)推断期间复杂的相似度量急剧增长。在本文中，我们提出了一个新的语言任务集成框架，称为 L3E-HD，它可以在低功耗边缘设备上实现有效的 HDC。通过将数据点映射到一个高维的二进制空间来简化最近邻搜索，L3E-HD 加快了推理速度。通过将 HDC 与集成技术相结合，L3E-HD 还解决了由于模型尺寸和精度的压缩而引起的精度严重下降的问题。我们的实验表明，集成技术自然是一个完美的适合增强 HDC。我们发现我们的 L3E-HD 比传统的机器学习方法更快、更有效、更精确，甚至可以在更小的模型尺寸下超过全精度模型的精度。密码发布于:  https://github.com/mxhx7199/sigir22-ensemblehdc。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=L3E-HD:+A+Framework+Enabling+Efficient+Ensemble+in+High-Dimensional+Space+for+Language+Tasks)|1|
|[Graph Capsule Network with a Dual Adaptive Mechanism](https://doi.org/10.1145/3477495.3531764)|Xiangping Zheng, Xun Liang, Bo Wu, Yuhui Guo, Xuan Zhang|Renmin University of China, Beijing, China|While Graph Convolutional Networks (GCNs) have been extended to various fields of artificial intelligence with their powerful representation capabilities, recent studies have revealed that their ability to capture the part-whole structure of the graph is limited. Furthermore, though many GCNs variants have been proposed and obtained state-of-the-art results, they face the situation that much early information may be lost during the graph convolution step. To this end, we innovatively present an Graph Capsule Network with a Dual Adaptive Mechanism (DA-GCN) to tackle the above challenges. Specifically, this powerful mechanism is a dual-adaptive mechanism to capture the part-whole structure of the graph. One is an adaptive node interaction module to explore the potential relationship between interactive nodes. The other is an adaptive attention-based graph dynamic routing to select appropriate graph capsules, so that only favorable graph capsules are gathered and redundant graph capsules are restrained for better capturing the whole structure between graphs. Experiments demonstrate that our proposed algorithm has achieved the most advanced or competitive results on all datasets.|虽然图卷积网络(GCNs)以其强大的表示能力已经扩展到人工智能的各个领域，但最近的研究表明，它们捕获图的部分-整体结构的能力是有限的。此外，虽然已经提出了许多 GCNs 变体，并获得了最先进的结果，但是它们面临的情况是，在图卷积步骤中，许多早期信息可能丢失。为此，我们创新性地提出了一个具有双重适应机制的图胶囊网络(DA-GCN)来应对上述挑战。具体来说，这种强大的机制是一种双自适应机制，用于捕获图的部分-整体结构。一种是自适应节点交互模块，用于探索交互节点之间的潜在关系。另一种是基于自适应注意的图动态路由选择合适的图胶囊，从而只收集有利的图胶囊，抑制冗余的图胶囊，以便更好地捕捉图间的整体结构。实验结果表明，本文提出的算法在所有数据集上都取得了最先进或最有竞争力的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Capsule+Network+with+a+Dual+Adaptive+Mechanism)|1|
|[Training Entire-Space Models for Target-oriented Opinion Words Extraction](https://doi.org/10.1145/3477495.3531768)|Yuncong Li, Fang Wang, ShengHua Zhong|Tencent Inc, Shenzhen, China; Shenzhen University, Shenzhen, China|Target-oriented opinion words extraction (TOWE) is a subtask of aspect-based sentiment analysis (ABSA). Given a sentence and an aspect term occurring in the sentence, TOWE extracts the corresponding opinion words for the aspect term. TOWE has two types of instance. In the first type, aspect terms are associated with at least one opinion word, while in the second type, aspect terms do not have corresponding opinion words. However, previous researches trained and evaluated their models with only the first type of instance, resulting in a sample selection bias problem. Specifically, TOWE models were trained with only the first type of instance, while these models would be utilized to make inference on the entire space with both the first type of instance and the second type of instance. Thus, the generalization performance will be hurt. Moreover, the performance of these models on the first type of instance cannot reflect their performance on entire space. To validate the sample selection bias problem, four popular TOWE datasets containing only aspect terms associated with at least one opinion word are extended and additionally include aspect terms without corresponding opinion words. Experimental results on these datasets show that training TOWE models on entire space will significantly improve model performance and evaluating TOWE models only on the first type of instance will overestimate model performance.|面向目标的意见词提取(TOWE)是基于方面的情感分析(ABSA)的一个子任务。给定一个句子和一个体项出现在句子中，TOWE 提取相应的体项意见词。TOWE 有两种类型的实例。在第一种类型中，体词至少与一个意见词相关联，而在第二种类型中，体词没有相应的意见词。然而，以往的研究仅用第一类实例对模型进行训练和评价，导致样本选择偏差问题。具体来说，TOWE 模型只用第一种类型的实例进行训练，而这些模型将被用来对整个空间进行第一种类型的实例和第二种类型的实例的推断。因此，泛化性能将受到影响。此外，这些模型在第一类实例上的性能不能反映它们在整个空间上的性能。为了验证样本选择偏差问题，扩展了四个常用的 TOWE 数据集，其中只包含与至少一个意见词相关的方面词，并且还包含了没有相应意见词的方面词。在这些数据集上的实验结果表明，在整个空间上训练 TOWE 模型将显著提高模型的性能，只有在第一类实例上评估 TOWE 模型才会高估模型的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Training+Entire-Space+Models+for+Target-oriented+Opinion+Words+Extraction)|1|
|[Point Prompt Tuning for Temporally Language Grounding](https://doi.org/10.1145/3477495.3531795)|Yawen Zeng|Tencent Inc., Shenzhen, China|The task of temporally language grounding (TLG) aims to locate a video moment from an untrimmed video that match a given textual query, which has attracted considerable research attention. In recent years, typical retrieval-based TLG methods are inefficient due to pre-segmented candidate moments, while localization-based TLG solutions adopt reinforcement learning resulting in unstable convergence. Therefore, how to perform TLG task efficiently and stably is a non-trivial work. Toward this end, we innovatively contribute a solution, Point Prompt Tuning (PPT), which formulates this task as a prompt-based multi-modal problem and integrates multiple sub-tasks to tuning performance. Specifically, a flexible prompt strategy is contributed to rewrite the query firstly, which contains both query, start point and end point. Thereafter, a multi-modal Transformer is adopted to fully learn the multi-modal context. Meanwhile, we design various sub-tasks to constrain the novel framework, namely matching task and localization task. Finally, the start and end points of matched video moment are straightforward predicted, simply yet stably. Extensive experiments on two real-world datasets have well verified the effectiveness of our proposed solution.|时间语言接地任务(TLG)是从未经修剪的视频中定位匹配给定文本查询的视频片段，已经引起了相当多的研究关注。近年来，典型的基于检索的 TLG 方法由于预分割候选矩而效率低下，而基于定位的 TLG 解决方案则采用强化学习，导致收敛不稳定。因此，如何高效、稳定地完成 TLG 任务是一项非常重要的工作。为此，我们创新性地提供了一个解决方案，即 Point Prompt Tuning (PPT) ，它将此任务描述为一个基于提示的多模态问题，并将多个子任务集成到性能调优中。提出了一种灵活的提示策略，首先对查询进行重写，包括查询、起始点和终结点。此后，采用多模态变压器，以充分了解多模态背景。同时，我们设计了各种子任务来约束新的框架，即匹配任务和定位任务。最后，对匹配视频时刻的起始点和终止点进行了简单而稳定的预测。在两个实际数据集上的大量实验已经很好地验证了我们提出的解决方案的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Point+Prompt+Tuning+for+Temporally+Language+Grounding)|1|
|[What Makes a Good Podcast Summary?](https://doi.org/10.1145/3477495.3531802)|Rezvaneh Rezapour, Sravana Reddy, Rosie Jones, Ian Soboroff|Drexel University, Philadelphia, PA, USA; Spotify, Boston, MA, USA; ASAPP, New York, NY, USA; NIST, Gaithersburg, MD, USA|Abstractive summarization of podcasts is motivated by the growing popularity of podcasts and the needs of their listeners. Podcasting is a markedly different domain from news and other media that are commonly studied in the context of automatic summarization. As such, the qualities of a good podcast summary are yet unknown. Using a collection of podcast summaries produced by different algorithms alongside human judgments of summary quality obtained from the TREC 2020 Podcasts Track, we study the correlations between various automatic evaluation metrics and human judgments, as well as the linguistic aspects of summaries that result in strong evaluations.|播客的抽象摘要是受到日益流行的播客和听众需求的推动。播客是一个明显不同的领域，从新闻和其他媒体，通常研究的背景下的自动汇总。因此，一个好的播客总结的质量还是未知数。使用由不同算法产生的播客摘要集合以及从 TREC 2020播客跟踪获得的总结质量的人类判断，我们研究各种自动评估指标和人类判断之间的相关性，以及导致强烈评估的总结的语言方面。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=What+Makes+a+Good+Podcast+Summary?)|1|
|[BSAL: A Framework of Bi-component Structure and Attribute Learning for Link Prediction](https://doi.org/10.1145/3477495.3531804)|Bisheng Li, Min Zhou, Shengzhong Zhang, Menglin Yang, Defu Lian, Zengfeng Huang|The Chinese University of Hong Kong, Hong Kong, China; University of Science and Technology of China, Hefei, China; Fudan University, Shanghai, China; Huawei Noah's Ark Lab, Shenzhen, China|Given the ubiquitous existence of graph-structured data, learning the representations of nodes for the downstream tasks ranging from node classification, link prediction to graph classification is of crucial importance. Regarding missing link inference of diverse networks, we revisit the link prediction techniques and identify the importance of both the structural and attribute information. However, the available techniques either heavily count on the network topology which is spurious in practice, or cannot integrate graph topology and features properly. To bridge the gap, we propose a bicomponent structural and attribute learning framework (BSAL) that is designed to adaptively leverage information from topology and feature spaces. Specifically, BSAL constructs a semantic topology via the node attributes and then gets the embeddings regarding the semantic view, which provides a flexible and easy-to-implement solution to adaptively incorporate the information carried by the node attributes. Then the semantic embedding together with topology embedding are fused together using attention mechanism for the final prediction. Extensive experiments show the superior performance of our proposal and it significantly outperforms baselines on diverse research benchmarks.|由于图结构数据的普遍存在，学习下游任务的节点表示，从节点分类、链路预测到图分类，都是至关重要的。关于不同网络的缺失链接推断，我们重新审视了链接预测技术，并确定了结构和属性信息的重要性。然而，现有的技术要么严重依赖于在实践中虚假的网络拓扑，要么不能恰当地整合图形拓扑和特性。为了弥补这一差距，我们提出了一个双组件结构和属性学习框架(BSAL) ，该框架旨在自适应地利用拓扑和特征空间中的信息。具体来说，BSAL 通过节点属性构造一个语义拓扑，然后得到关于语义视图的嵌入，这为自适应地合并节点属性所携带的信息提供了一个灵活且易于实现的解决方案。然后利用注意机制将语义嵌入和拓扑嵌入融合在一起进行最终预测。广泛的实验表明，我们的建议的优越性能，它明显优于基线的不同研究基准。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BSAL:+A+Framework+of+Bi-component+Structure+and+Attribute+Learning+for+Link+Prediction)|1|
|[Tensor-based Graph Modularity for Text Data Clustering](https://doi.org/10.1145/3477495.3531834)|Rafika Boutalbi, Mira Ait Saada, Anastasiia Iurshina, Steffen Staab, Mohamed Nadif|University of Stuttgart, Stuttgart, Germany; Université de Paris, Paris, France|Graphs are used in several applications to represent similarities between instances. For text data, we can represent texts by different features such as bag-of-words, static embeddings (Word2vec, GloVe, etc.), and contextual embeddings (BERT, RoBERTa, etc.), leading to multiple similarities (or graphs) based on each representation. The proposal posits that incorporating the local invariance within every graph and the consistency across different graphs leads to a consensus clustering that improves the document clustering. This problem is complex and challenged with the sparsity and the noisy data included in each graph. To this end, we rely on the modularity metric, which effectively evaluates graph clustering in such circumstances. Therefore, we present a novel approach for text clustering based on both a sparse tensor representation and graph modularity. This leads to cluster texts (nodes) while capturing information arising from the different graphs. We iteratively maximize a Tensor-based Graph Modularity criterion. Extensive experiments on benchmark text clustering datasets are performed, showing that the proposed algorithm referred to as Tensor Graph Modularity -TGM- outperforms other baseline methods in terms of clustering task. The source code is available at https://github.com/TGMclustering/TGMclustering.|在几个应用程序中使用图表来表示实例之间的相似性。对于文本数据，我们可以通过不同的特性来表示文本，例如词包、静态嵌入(Word2vec、 GloVe 等)和上下文嵌入(BERT、 RoBERTa 等) ，从而基于每种表示形式产生多种相似性(或图形)。该方案假定，在每个图中引入局部不变性和不同图之间的一致性，可以形成共识聚类，从而改善文档聚类。这个问题是复杂的和挑战的稀疏性和噪声数据包含在每个图。为此，我们依赖于模块化度量，它可以有效地评估在这种情况下的图聚类。因此，我们提出了一种基于稀疏张量表示和图模块化的文本聚类方法。这将导致集群文本(节点) ，同时捕获来自不同图表的信息。我们迭代地最大化一个基于张量的图模块化准则。对基准文本聚类数据集进行了广泛的实验，结果表明该算法在聚类任务方面优于其他基准方法。源代码可在 https://github.com/tgmclustering/tgmclustering 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tensor-based+Graph+Modularity+for+Text+Data+Clustering)|1|
|[GraphAD: A Graph Neural Network for Entity-Wise Multivariate Time-Series Anomaly Detection](https://doi.org/10.1145/3477495.3531848)|Xu Chen, Qiu Qiu, Changshan Li, Kunqing Xie|Peking University, Beijing, China; Alibaba DAMO Academy, Hangzhou, China|In recent years, the emergence and development of third-party platforms have greatly facilitated the growth of the Online to Offline (O2O) business. However, the large amount of transaction data raises new challenges for retailers, especially anomaly detection in operating conditions. Thus, platforms begin to develop intelligent business assistants with embedded anomaly detection methods to reduce the management burden on retailers. Traditional time-series anomaly detection methods capture underlying patterns from the perspectives of time and attributes, ignoring the difference between retailers in this scenario. Besides, similar transaction patterns extracted by the platforms can also provide guidance to individual retailers and enrich their available information without privacy issues. In this paper, we pose an entity-wise multivariate time-series anomaly detection problem that considers the time-series of each unique entity. To address this challenge, we propose GraphAD, a novel multivariate time-series anomaly detection model based on the graph neural network. GraphAD decomposes the Key Performance Indicator (KPI) into stable and volatility components and extracts their patterns in terms of attributes, entities and temporal perspectives via graph neural networks. We also construct a real-world entity-wise multivariate time-series dataset from the business data of Ele.me. The experimental results on this dataset show that GraphAD significantly outperforms existing anomaly detection methods.|近年来，第三方平台的出现和发展极大地促进了 Online To Offline线上到线下业务的发展。然而，大量的交易数据给零售商带来了新的挑战，尤其是在经营异常检测方面。因此，平台开始开发具有嵌入式异常检测方法的智能商务助理，以减轻零售商的管理负担。传统的时间序列异常检测方法从时间和属性的角度捕捉潜在的模式，忽略了在这种情况下零售商之间的差异。此外，平台所提取的类似交易模式亦可为个别零售商提供指引，并在不涉及私隐问题的情况下丰富他们的资料。在本文中，我们提出了一个实体多变量时间序列异常检测问题，考虑了每个独特的实体的时间序列。为了应对这一挑战，我们提出了一种新的基于图形神经网络的多变量时间序列异常检测模型 GraphAD。GraphAD 将关键绩效指标分解成稳定和波动的分量，并通过图形神经网络从属性、实体和时间角度提取它们的模式。我们还利用 Ele.me 的业务数据构造了一个真实世界的实体多变量时间序列数据集。在这个数据集上的实验结果显示，GraphAD 显著优于现有的异常检测方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphAD:+A+Graph+Neural+Network+for+Entity-Wise+Multivariate+Time-Series+Anomaly+Detection)|1|
|[Lightweight Meta-Learning for Low-Resource Abstractive Summarization](https://doi.org/10.1145/3477495.3531908)|Taehun Huh, Youngjoong Ko|Sungkyunkwan University, Suwon-si, Gyeonggi-do, Republic of Korea|Recently, supervised abstractive summarization using high-resource datasets, such as CNN/DailyMail and Xsum, has achieved significant performance improvements. However, most of the existing high-resource dataset is biased towards a specific domain like news, and annotating document-summary pairs for low-resource datasets is too expensive. Furthermore, the need for low-resource abstractive summarization task is emerging but existing methods for the task such as transfer learning still have domain shifting and overfitting problems. To address these problems, we propose a new framework for low-resource abstractive summarization using a meta-learning algorithm that can quickly adapt to a new domain using small data. For adaptive meta-learning, we introduce a lightweight module inserted into the attention mechanism of a pre-trained language model; the module is first meta-learned with high-resource task-related datasets and then is fine-tuned with the low-resource target dataset. We evaluate our model on 11 different datasets. Experimental results show that the proposed method achieves the state-of-the-art on 9 datasets in low-resource abstractive summarization.|最近，使用高资源数据集(如 CNN/DailyMail 和 Xsum)的监督抽象摘要已经取得了显著的性能改进。然而，大多数现有的高资源数据集偏向于新闻这样的特定领域，并且为低资源数据集注释文档-摘要对过于昂贵。此外，对低资源抽象摘要任务的需求正在出现，但现有的任务转移学习等方法仍然存在领域移位和过拟合问题。为了解决这些问题，我们提出了一个新的框架，低资源抽象摘要使用元学习算法，可以快速适应新的领域使用小数据。对于自适应元学习，我们在预训练语言模型的注意机制中引入了一个轻量级模块，该模块首先对高资源任务相关数据集进行元学习，然后对低资源目标数据集进行微调。我们在11个不同的数据集上评估我们的模型。实验结果表明，该方法在低资源抽象摘要的9个数据集上达到了最高水平。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Lightweight+Meta-Learning+for+Low-Resource+Abstractive+Summarization)|1|
|[Task-Oriented Dialogue System as Natural Language Generation](https://doi.org/10.1145/3477495.3531920)|Weizhi Wang, Zhirui Zhang, Junliang Guo, Yinpei Dai, Boxing Chen, Weihua Luo||In this paper, we propose to formulate the task-oriented dialogue system as the purely natural language generation task, so as to fully leverage the large-scale pre-trained models like GPT-2 and simplify complicated delexicalization prepossessing. However, directly applying this method heavily suffers from the dialogue entity inconsistency caused by the removal of delexicalized tokens, as well as the catastrophic forgetting problem of the pre-trained model during fine-tuning, leading to unsatisfactory performance. To alleviate these problems, we design a novel GPT-Adapter-CopyNet network, which incorporates the lightweight adapter and CopyNet modules into GPT-2 to achieve better performance on transfer learning and dialogue entity generation. Experimental results conducted on the DSTC8 Track 1 benchmark and MultiWOZ dataset demonstrate that our proposed approach significantly outperforms baseline models with a remarkable performance on automatic and human evaluations.|本文提出将任务导向的对话系统设计为纯自然语言生成任务，以充分利用 GPT-2等大规模预训练模型，简化复杂的去词化过程。然而，直接应用这种方法，由于去词化标记引起的对话实体不一致，以及预训练模型在微调过程中的灾难性遗忘问题，导致性能不理想。为了解决这些问题，我们设计了一种新型的 GPT-Adapter-CopyNet 网络，它将轻量级适配器和 CopyNet 模块集成到 GPT-2中，以实现更好的传递学习和对话实体生成。在 DSTC8 Track1基准和 MultiWOZ 数据集上进行的实验结果表明，我们提出的方法显著优于基线模型，在自动和人工评估方面具有显著的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Task-Oriented+Dialogue+System+as+Natural+Language+Generation)|1|
|[Where Do Queries Come From?](https://doi.org/10.1145/3477495.3531711)|Marwah Alaofi, Luke Gallagher, Dana McKay, Lauren L. Saling, Mark Sanderson, Falk Scholer, Damiano Spina, Ryen W. White|Microsoft Research, Redmond, WA, USA; RMIT University, Melbourne, VIC, Australia|Where do queries -- the words searchers type into a search box -- come from? The Information Retrieval community understands the performance of queries and search engines extensively, and has recently begun to examine the impact of query variation, showing that different queries for the same information need produce different results. In an information environment where bad actors try to nudge searchers toward misinformation, this is worrisome. The source of query variation -- searcher characteristics, contextual or linguistic prompts, cognitive biases, or even the influence of external parties -- while studied in a piecemeal fashion by other research communities has not been studied by ours. In this paper we draw on a variety of literatures (including information seeking, psychology, and misinformation), and report some small experiments to describe what is known about where queries come from, and demonstrate a clear literature gap around the source of query variations in IR. We chart a way forward for IR to research, document and understand this important question, with a view to creating search engines that provide more consistent, accurate and relevant search results regardless of the searcher's framing of the query.|查询——搜索者在搜索框中输入的单词——从何而来？信息检索广泛了解查询和搜索引擎的性能，最近开始研究查询变化的影响，表明对同一信息的不同查询需要产生不同的结果。在信息环境中，坏人试图推动搜索者获得错误信息，这令人担忧。查询变异的来源——搜索者特征、上下文或语言提示、认知偏差，甚至外部各方的影响——虽然被其他研究团体零碎地研究过，但我们还没有研究过。本文借鉴了大量的文献(包括信息搜索、心理学和错误信息) ，通过一些小实验来描述已知的查询来源，并对信息检索中查询变异的来源进行了清晰的文献分析。我们为 IR 研究、记录和理解这个重要问题绘制了一条前进的道路，以期创建能够提供更加一致、准确和相关的搜索结果的搜索引擎，而不管搜索者的查询框架如何。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Where+Do+Queries+Come+From?)|1|
|[OVQA: A Clinically Generated Visual Question Answering Dataset](https://doi.org/10.1145/3477495.3531724)|Yefan Huang, Xiaoli Wang, Feiyan Liu, Guofeng Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OVQA:+A+Clinically+Generated+Visual+Question+Answering+Dataset)|1|
|[Monant Medical Misinformation Dataset: Mapping Articles to Fact-Checked Claims](https://doi.org/10.1145/3477495.3531726)|Ivan Srba, Branislav Pecher, Matús Tomlein, Róbert Móro, Elena Stefancova, Jakub Simko, Mária Bieliková||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Monant+Medical+Misinformation+Dataset:+Mapping+Articles+to+Fact-Checked+Claims)|1|
|[ViQuAE, a Dataset for Knowledge-based Visual Question Answering about Named Entities](https://doi.org/10.1145/3477495.3531753)|Paul Lerner, Olivier Ferret, Camille Guinaudeau, Hervé Le Borgne, Romaric Besançon, José G. Moreno, Jesús LovónMelgarejo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ViQuAE,+a+Dataset+for+Knowledge-based+Visual+Question+Answering+about+Named+Entities)|1|
|[DIANES: A DEI Audit Toolkit for News Sources](https://doi.org/10.1145/3477495.3531660)|Xiaoxiao Shang, Zhiyuan Peng, Qiming Yuan, Sabiq Khan, Lauren Xie, Yi Fang, Subramaniam Vincent||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DIANES:+A+DEI+Audit+Toolkit+for+News+Sources)|1|
|[NAS-CTR: Efficient Neural Architecture Search for Click-Through Rate Prediction](https://doi.org/10.1145/3477495.3532030)|Guanghui Zhu, Feng Cheng, Defu Lian, Chunfeng Yuan, Yihua Huang|University of Science and Technology of China, Hefei, China; Nanjing University, Nanjing, China|Click-Through Rate (CTR) prediction has been widely used in many machine learning tasks such as online advertising and personalization recommendation. Unfortunately, given a domain-specific dataset, searching effective feature interaction operations and combinations from a huge candidate space requires significant expert experience and computational costs. Recently, Neural Architecture Search (NAS) has achieved great success in discovering high-quality network architectures automatically. However, due to the diversity of feature interaction operations and combinations, the existing NAS-based work that treats the architecture search as a black-box optimization problem over a discrete search space suffers from low efficiency. Therefore, it is essential to explore a more efficient architecture search method. To achieve this goal, we propose NAS-CTR, a differentiable neural architecture search approach for CTR prediction. First, we design a novel and expressive architecture search space and a continuous relaxation scheme to make the search space differentiable. Second, we formulate the architecture search for CTR prediction as a joint optimization problem with discrete constraints on architectures and leverage proximal iteration to solve the constrained optimization problem. Additionally, a straightforward yet effective method is proposed to eliminate the aggregation of skip connections. Extensive experimental results reveal that NAS-CTR can outperform the SOTA human-crafted architectures and other NAS-based methods in both test accuracy and search efficiency.|点进率预测已经广泛应用于许多机器学习任务，例如在线广告和个性化推荐。不幸的是，给定一个特定领域的数据集，从一个巨大的候选空间中搜索有效的特征交互操作和组合需要大量的专家经验和计算成本。近年来，神经网络体系结构搜索(NAS)在自动发现高质量的网络体系结构方面取得了巨大的成功。然而，由于功能交互操作和组合的多样性，现有的基于 NAS 的工作将体系结构搜索视为离散搜索空间上的黑盒子最佳化问题，效率低下。因此，有必要探索一种更有效的体系结构搜索方法。为了实现这一目标，我们提出了 NAS-CTR，一种用于 CTR 预测的可微分神经结构搜索方法。首先，我们设计了一个新颖的、具有表现力的体系结构搜索空间和一个连续松弛方案，使搜索空间具有可微性。其次，我们将 CTR 预测的体系结构搜索描述为一个联合最佳化问题，对体系结构进行离散约束，并利用近端迭代来解决约束最佳化问题。此外，提出了一种简单而有效的方法来消除跳跃连接的聚集。大量的实验结果表明，NAS-CTR 在测试精度和搜索效率方面都优于 SOTA 人工架构和其他基于 NAS 的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NAS-CTR:+Efficient+Neural+Architecture+Search+for+Click-Through+Rate+Prediction)|0|
|[Learning to Enrich Query Representation with Pseudo-Relevance Feedback for Cross-lingual Retrieval](https://doi.org/10.1145/3477495.3532013)|Ramraj Chandradevan, Eugene Yang, Mahsa Yarmohammadi, Eugene Agichtein|Johns Hopkins University, Baltimore, MD, USA; Emory University, Atlanta, GA, USA|Cross-lingual information retrieval (CLIR) aims to provide access to information across languages. Recent pre-trained multilingual language models brought large improvements to the natural language tasks, including cross-lingual adhoc retrieval. However, pseudo-relevance feedback (PRF), a family of techniques for improving ranking using the contents of top initially retrieved items, has not been explored with neural CLIR retrieval models. Two of the challenges are incorporating feedback from long documents, and cross-language knowledge transfer. To address these challenges, we propose a novel neural CLIR architecture, NCLPRF, capable of incorporating PRF feedback from multiple potentially long documents, which enables improvements to query representation in the shared semantic space between query and document languages. The additional information that the feedback documents provide in a target language, can enrich the query representation, bringing it closer to relevant documents in the embedding space. The proposed model performance across three CLIR test collections in Chinese, Russian, and Persian languages, exhibits significant improvements over traditional and SOTA neural CLIR baselines across all three collections.|跨语言信息检索(CLIR)旨在提供跨语言的信息获取途径。最近预先训练的多语言模型给自然语言任务带来了很大的改进，包括跨语言的即席检索。然而，伪相关反馈(PRF)作为一种利用最初检索项目的内容来提高排名的技术，尚未在神经元 CLIR 检索模型中得到应用。其中两个挑战是整合来自长文档的反馈，以及跨语言的知识转移。为了应对这些挑战，我们提出了一个新的神经 CLIR 架构，NCLPRF，能够合并来自多个潜在的长文档的 PRF 反馈，这使得查询语言和文档语言之间的共享语义空间中的查询表示得到改进。反馈文档以目标语言提供的附加信息可以丰富查询表示，使其更接近嵌入空间中的相关文档。在中文，俄文和波斯语的三个 CLIR 测试集合中，所提出的模型性能比所有三个集合中的传统和 SOTA 神经 CLIR 基线都有显著的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Enrich+Query+Representation+with+Pseudo-Relevance+Feedback+for+Cross-lingual+Retrieval)|0|
|[Incorporating Retrieval Information into the Truncation of Ranking Lists for Better Legal Search](https://doi.org/10.1145/3477495.3531998)|Yixiao Ma, Qingyao Ai, Yueyue Wu, Yunqiu Shao, Yiqun Liu, Min Zhang, Shaoping Ma|University of Utah, Salt Lake City, UT, USA; Tsinghua University, Beijing, China|The truncation of ranking lists predicted by retrieval models is vital to ensure users' search experience. Particularly, in specific vertical domains where documents are usually complicated and extensive (e.g., legal cases), the cost of browsing results is much higher than traditional IR tasks (e.g., Web search) and setting a reasonable cut-off position is quite necessary. While it is straightforward to apply existing result list truncation approaches to legal case retrieval, the effectiveness of these methods is limited because they only focus on simple document statistics and usually fail to capture the context information of documents in the ranking list. These existing efforts also treat result list truncation as an isolated task instead of a component in the entire ranking process, limiting the usage of truncation in practical systems. To tackle these limitations, we propose LeCut, a ranking list truncation model for legal case retrieval. LeCut utilizes contextual features of the retrieval task to capture the semantic-level similarity between documents and decides the best cut-off position with attention mechanisms. We further propose a Joint Optimization of Truncation and Reranking (JOTR) framework based on LeCut to improve the performance of truncation and retrieval tasks simultaneously. Comparison against competitive baselines on public benchmark datasets demonstrates the effectiveness of LeCut and JOTR. A case study is conducted to visualize the cut-off positions of LeCut and the process of how JOTR improves both retrieval and truncation tasks.|检索模型预测的排名列表的截断对于保证用户的搜索体验至关重要。特别是，在特定的垂直领域，文档通常是复杂和广泛的(例如，法律案件) ，浏览结果的成本远远高于传统的 IR 任务(例如，网络搜索) ，设置一个合理的截止位置是非常必要的。虽然将现有的结果清单截断方法应用于法律案件检索很简单，但这些方法的有效性有限，因为它们只侧重于简单的文件统计，通常无法捕捉排名清单中文件的上下文信息。这些现有的工作还将结果列表截断视为一个孤立的任务，而不是整个排序过程中的一个组件，从而限制了截断在实际系统中的使用。为了解决这些局限性，我们提出了 LeCut，一种用于法律案例检索的排序列表截断模型。LeCut 利用检索任务的上下文特征来捕获文档之间的语义级相似性，并通过注意机制确定最佳截止位置。进一步提出了一种基于 LeCut 的联合优化截断与重排(JOTR)框架，以同时提高截断与检索任务的性能。与公共基准数据集的竞争基线进行比较，可以证明 LeCut 和 JOTR 的有效性。通过案例研究，可视化 LeCut 的截止位置以及 JOTR 如何改进检索和截断任务的过程。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Incorporating+Retrieval+Information+into+the+Truncation+of+Ranking+Lists+for+Better+Legal+Search)|0|
|[Ada-Ranker: A Data Distribution Adaptive Ranking Paradigm for Sequential Recommendation](https://doi.org/10.1145/3477495.3531931)|Xinyan Fan, Jianxun Lian, Wayne Xin Zhao, Zheng Liu, Chaozhuo Li, Xing Xie|Microsoft Research Asia, Beijing, China; Renmin University of China, Beijing, China|A large-scale recommender system usually consists of recall and ranking modules. The goal of ranking modules (aka rankers) is to elaborately discriminate users' preference on item candidates proposed by recall modules. With the success of deep learning techniques in various domains, we have witnessed the mainstream rankers evolve from traditional models to deep neural models. However, the way that we design and use rankers remains unchanged: offline training the model, freezing the parameters, and deploying it for online serving. Actually, the candidate items are determined by specific user requests, in which underlying distributions (e.g., the proportion of items for different categories, the proportion of popular or new items) are highly different from one another in a production environment. The classical parameter-frozen inference manner cannot adapt to dynamic serving circumstances, making rankers' performance compromised. In this paper, we propose a new training and inference paradigm, termed as Ada-Ranker, to address the challenges of dynamic online serving. Instead of using parameter-frozen models for universal serving, Ada-Ranker can adaptively modulate parameters of a ranker according to the data distribution of the current group of item candidates. We first extract distribution patterns from the item candidates. Then, we modulate the ranker by the patterns to make the ranker adapt to the current data distribution. Finally, we use the revised ranker to score the candidate list. In this way, we empower the ranker with the capacity of adapting from a global model to a local model which better handles the current task. As a first study, we examine our Ada-Ranker paradigm in the sequential recommendation scenario. Experiments on three datasets demonstrate that Ada-Ranker can effectively enhance various base sequential models and also outperform a comprehensive set of competitive baselines.|一个大规模的推荐系统通常包括召回和排名模块。排序模块(又称排序器)的目标是精心区分用户对召回模块提出的候选项的偏好。随着深度学习技术在各个领域的成功，我们目睹了主流的排名从传统模型演变为深度神经模型。然而，我们设计和使用排名的方式保持不变: 离线训练模型，冻结参数，并部署它在线服务。实际上，候选项是由特定的用户请求决定的，在这种情况下，底层分布(例如，不同类别的项目比例，流行项目或新项目的比例)在生产环境中彼此之间差异很大。传统的参数冻结推理方式不能适应动态服务环境，使得排序器的性能受到影响。在本文中，我们提出了一个新的训练和推理范式，称为 Ada-Ranker，以解决动态在线服务的挑战。Ada-Ranker 可以根据当前项目候选者组的数据分布自适应地调整排序器的参数，而不必使用通用服务的参数冻结模型。我们首先从候选项中提取分布模式。然后，根据模式对排序器进行调整，使排序器适应当前的数据分布。最后，我们使用修改后的排名对候选人列表进行评分。通过这种方式，我们赋予排名者从全球模型到更好地处理当前任务的局部模型的适应能力。作为第一个研究，我们在顺序推荐场景中检查我们的 Ada-Ranker 范式。在三个数据集上的实验表明，Ada-Ranker 能够有效地增强各种基本序列模型，并且表现优于一组综合的竞争基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ada-Ranker:+A+Data+Distribution+Adaptive+Ranking+Paradigm+for+Sequential+Recommendation)|0|
|[Retrieval and Recommendation Systems at the Crossroads of Artificial Intelligence, Ethics, and Regulation](https://doi.org/10.1145/3477495.3532683)|Markus Schedl, Emilia Gómez, Elisabeth Lex|Johannes Kepler University Linz & Linz Institue of Technology, Linz, Austria; Graz University of Technology, Graz, Austria; European Commission, Joint Research Centre and Universitat Pompeu Fabra, Seville/Barcelona, Spain|This tutorial aims at providing its audience an interdisciplinary overview about the topics of fairness and non-discrimination, diversity, and transparency of AI systems, tailored to the research fields of information retrieval and recommender systems. By means of this tutorial, we would like to equip the mostly technical audience of SIGIR with the necessary understanding of the ethical implications of their research and development on the one hand, and of recent political and legal regulations that address the aforementioned challenges on the other hand.|本教程旨在为读者提供一个关于人工智能系统的公平性和非歧视性、多样性和透明度等主题的跨学科概述，适用于信息检索和推荐系统的研究领域。通过本教程，我们希望让 SIGIR 的大多数技术读者一方面了解他们的研究和发展的道德影响，另一方面了解解决上述挑战的最新政治和法律法规。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Retrieval+and+Recommendation+Systems+at+the+Crossroads+of+Artificial+Intelligence,+Ethics,+and+Regulation)|0|
|[Adversarial Filtering Modeling on Long-term User Behavior Sequences for Click-Through Rate Prediction](https://doi.org/10.1145/3477495.3531788)|Xiaochen Li, Jian Liang, Xialong Liu, Yu Zhang|Lazada Group, Beijing, China; Alibaba Group, Beijing, China|Rich user behavior information is of great importance for capturing and understanding user interest in click-through rate (CTR) prediction. To improve the richness, collecting long-term behaviors becomes a typical approach in academy and industry but at the cost of increasing online storage and latency. Recently, researchers have proposed several approaches to shorten long-term behavior sequence and then model user interests. These approaches reduce online cost efficiently but do not well handle the noisy information in long-term user behavior, which may deteriorate the performance of CTR prediction significantly. To obtain better cost/performance trade-off, we propose a novel Adversarial Filtering Model (ADFM) to model long-term user behavior. ADFM uses a hierarchical aggregation representation to compress raw behavior sequence and then learns to remove useless behavior information with an adversarial filtering mechanism. The selected user behaviors are fed into interest extraction module for CTR prediction. Experimental results on public datasets and industrial dataset demonstrate that our method achieves significant improvements over state-of-the-art models.|丰富的用户行为信息对于捕捉和理解用户对点进率预测的兴趣非常重要。为了提高丰富性，收集长期行为成为学术界和工业界的一种典型方法，但代价是增加在线存储和延迟。最近，研究人员提出了几种方法来缩短长期行为序列，然后模型用户的兴趣。这些方法有效地降低了在线成本，但不能很好地处理长期用户行为中的噪声信息，这可能会严重影响 CTR 预测的性能。为了获得更好的性价比，我们提出了一种新的对抗过滤模型(ADFM)来模拟长期用户行为。ADFM 使用分层聚合表示来压缩原始行为序列，然后学习使用对抗性过滤机制去除无用的行为信息。将选定的用户行为反馈到兴趣提取模块中进行点击率预测。在公共数据集和工业数据集上的实验结果表明，该方法比现有的模型有明显的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adversarial+Filtering+Modeling+on+Long-term+User+Behavior+Sequences+for+Click-Through+Rate+Prediction)|0|
|[LoL: A Comparative Regularization Loss over Query Reformulation Losses for Pseudo-Relevance Feedback](https://doi.org/10.1145/3477495.3532017)|Yunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, Xueqi Cheng|Institute of Computing Technology, CAS & University of Chinese Academy of Sciences, Beijing, China; Data Intelligence System Research Center, Institute of Computing Technology, CAS & University of Chinese Academy of Sciences, Beijing, China; Data Intelligence System Research Center, Institute of Computing Technology, CAS, Beijing, China; Tsinghua University, Beijing, China|Pseudo-relevance feedback (PRF) has proven to be an effective query reformulation technique to improve retrieval accuracy. It aims to alleviate the mismatch of linguistic expressions between a query and its potential relevant documents. Existing PRF methods independently treat revised queries originating from the same query but using different numbers of feedback documents, resulting in severe query drift. Without comparing the effects of two different revisions from the same query, a PRF model may incorrectly focus on the additional irrelevant information increased in the more feedback, and thus reformulate a query that is less effective than the revision using the less feedback. Ideally, if a PRF model can distinguish between irrelevant and relevant information in the feedback, the more feedback documents there are, the better the revised query will be. To bridge this gap, we propose the Loss-over-Loss (LoL) framework to compare the reformulation losses between different revisions of the same query during training. Concretely, we revise an original query multiple times in parallel using different amounts of feedback and compute their reformulation losses. Then, we introduce an additional regularization loss on these reformulation losses to penalize revisions that use more feedback but gain larger losses. With such comparative regularization, the PRF model is expected to learn to suppress the extra increased irrelevant information by comparing the effects of different revised queries. Further, we present a differentiable query reformulation method to implement this framework. This method revises queries in the vector space and directly optimizes the retrieval performance of query vectors, applicable for both sparse and dense retrieval models. Empirical evaluation demonstrates the effectiveness and robustness of our method for two typical sparse and dense retrieval models.|伪相关反馈(PRF)已被证明是一种有效的查询重构技术，以提高检索的准确性。它旨在缓解查询与潜在相关文档之间的语言表达不匹配问题。现有的 PRF 方法独立处理来自同一查询但使用不同数量的反馈文档的修改查询，导致严重的查询漂移。如果不比较来自同一查询的两个不同修订的效果，PRF 模型可能会错误地关注更多反馈中增加的附加不相关信息，从而使用更少的反馈重新表述比修订更低效的查询。理想情况下，如果 PRF 模型能够区分反馈中的不相关信息和相关信息，那么反馈文档越多，修改后的查询就越好。为了弥补这一差距，我们提出了损失超过损失(LoL)框架来比较同一查询在培训期间不同修订版本之间的重构损失。具体来说，我们使用不同数量的反馈并行多次修改原始查询，并计算它们的重新表述损失。然后，我们引入一个额外的正则化损失对这些重制损失，以惩罚修订使用更多的反馈，但获得更大的损失。通过这种比较正则化，PRF 模型可以通过比较不同修订查询的效果来抑制额外增加的不相关信息。进一步，我们提出了一个可微查询重构方法来实现这个框架。该方法对向量空间中的查询进行修正，直接优化查询向量的检索性能，适用于稀疏和密集检索模型。实验结果表明，该方法对两种典型的稀疏和密集检索模型具有较好的鲁棒性和有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LoL:+A+Comparative+Regularization+Loss+over+Query+Reformulation+Losses+for+Pseudo-Relevance+Feedback)|0|
|[Determinantal Point Process Likelihoods for Sequential Recommendation](https://doi.org/10.1145/3477495.3531965)|Yuli Liu, Christian J. Walder, Lexing Xie|Data61, CSIRO & Australian National University, Canberra, Australia; Australian National University & Data61, CSIRO, Canberra, Australia|Sequential recommendation is a popular task in academic research and close to real-world application scenarios, where the goal is to predict the next action(s) of the user based on his/her previous sequence of actions. In the training process of recommender systems, the loss function plays an essential role in guiding the optimization of recommendation models to generate accurate suggestions for users. However, most existing sequential recommendation tech- niques focus on designing algorithms or neural network architectures, and few efforts have been made to tailor loss functions that fit naturally into the practical application scenario of sequential recommender systems. Ranking-based losses, such as cross-entropy and Bayesian Personalized Ranking (BPR) are widely used in the sequential recommendation area. We argue that such objective functions suffer from two inherent drawbacks: i) the dependencies among elements of a sequence are overlooked in these loss formulations; ii) instead of balancing accuracy (quality) and diversity, only generating accurate results has been over emphasized. We therefore propose two new loss functions based on the Determinantal Point Process (DPP) likelihood, that can be adaptively applied to estimate the subsequent item or items. The DPP-distributed item set captures natural dependencies among temporal actions, and a quality vs. diversity decomposition of the DPP kernel pushes us to go beyond accuracy-oriented loss functions. Experimental results using the proposed loss functions on three real-world datasets show marked improvements over state-of-the-art sequential recommendation methods in both quality and diversity metrics.|顺序推荐是学术研究中的一个热门任务，它接近于真实的应用场景，其目标是根据用户以前的操作顺序预测他/她的下一个操作。在推荐系统的培训过程中，损失函数对于指导推荐模型的优化，为用户提供准确的建议起着至关重要的作用。然而，现有的顺序推荐技术大多侧重于设计算法或神经网络体系结构，很少有人努力去调整自然适合顺序推荐系统实际应用场景的损失函数。基于排序的损失，如交叉熵和贝叶斯个性化排序(BPR)被广泛应用于序列推荐领域。我们认为这样的目标函数有两个固有的缺点: i)在这些损失公式中忽略了序列元素之间的依赖关系; ii)没有平衡准确性(质量)和多样性，只有产生准确的结果被过分强调。因此，我们提出两个新的基于行列式点过程(DPP)可能性的损失函数，可以自适应地应用于估计随后的项目。DPP 分布式项目集捕获时间操作之间的自然依赖关系，DPP 内核的质量与多样性分解促使我们超越面向准确性的损失函数。在三个实际数据集上使用提出的损失函数的实验结果显示，在质量和多样性度量方面，该方法比最先进的顺序推荐方法有明显的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Determinantal+Point+Process+Likelihoods+for+Sequential+Recommendation)|0|
|[Re-weighting Negative Samples for Model-Agnostic Matching](https://doi.org/10.1145/3477495.3532053)|Jiazhen Lou, Hong Wen, Fuyu Lv, Jing Zhang, Tengfei Yuan, Zhao Li|Alibaba Group, Hangzhou, China; Zhejiang University, Hangzhou, China; The University of Sydney, Darlington, NSW, Australia|Recommender Systems (RS), as an efficient tool to discover users' interested items from a very large corpus, has attracted more and more attention from academia and industry. As the initial stage of RS, large-scale matching is fundamental yet challenging. A typical recipe is to learn user and item representations with a two-tower architecture and then calculate the similarity score between both representation vectors, which however still struggles in how to properly deal with negative samples. In this paper, we find that the common practice that randomly sampling negative samples from the entire space and treating them equally is not an optimal choice, since the negative samples from different sub-spaces at different stages have different importance to a matching model. To address this issue, we propose a novel method named Unbiased Model-Agnostic Matching Approach (UMA2). It consists of two basic modules including 1) General Matching Model (GMM), which is model-agnostic and can be implemented as any embedding-based two-tower models; and 2) Negative Samples Debias Network (NSDN), which discriminates negative samples by borrowing the idea of Inverse Propensity Weighting (IPW) and re-weighs the loss in GMM. UMA$^2$ seamlessly integrates these two modules in an end-to-end multi-task learning framework. Extensive experiments on both real-world offline dataset and online A/B test demonstrate its superiority over state-of-the-art methods.|推荐系统(RS)作为一种从庞大的语料库中发现用户感兴趣的项目的有效工具，越来越受到学术界和业界的关注。作为遥感的初始阶段，大规模匹配是一个基础性的挑战。一个典型的方法是使用双塔体系结构学习用户和项目表示，然后计算两个表示向量之间的相似度得分，但是如何正确处理负样本仍然是一个难题。在本文中，我们发现从整个空间中随机抽取负样本并平等对待它们的常见做法并不是最优选择，因为不同阶段不同子空间中的负样本对匹配模型的重要性不同。为了解决这一问题，我们提出了一种新的方法——无偏模型-不可知匹配方法(UMA2)。它包括两个基本模块: 1)通用匹配模型(GMM) ，该模型与模型无关，可以作为任何嵌入式双塔模型实现; 2)负样本偏差网络(NSDN) ，该网络借助逆倾向加权(IPW)的思想对负样本进行判别，并在 GMM 中重新权衡损失。UMA $^ 2 $在端到端多任务学习框架中无缝地集成了这两个模块。通过对现实世界离线数据集和在线 A/B 测试的大量实验，证明了该方法优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Re-weighting+Negative+Samples+for+Model-Agnostic+Matching)|0|
|[Item-Provider Co-learning for Sequential Recommendation](https://doi.org/10.1145/3477495.3531756)|Lei Chen, Jingtao Ding, Min Yang, Chengming Li, Chonggang Song, Lingling Yi|Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Tencent Inc., Shenzhen, China; Sun Yat-sen University, Shenzhen, China|Sequential recommender systems (SRSs) have become a research hotspot recently due to its powerful ability in capturing users' dynamic preferences. The key idea behind SRSs is to model the sequential dependencies over the user-item interactions. However, we argue that users' preferences are not only determined by their view or purchase items but also affected by the item-providers with which users have interacted. For instance, in a short-video scenario, a user may click on a video because he/she is attracted to either the video content or simply the video-providers as the vloggers are his/her idols. Motivated by the above observations, in this paper, we propose IPSRec, a novel Item-Provider co-learning framework for Sequential Recommendation. Specifically, we propose two representation learning methods (single-steam and cross-stream) to learn comprehensive item and user representations based on the user's historical item sequence and provider sequence. Then, contrastive learning is employed to further enhance the user embeddings in a self-supervised manner, which treats the representations of a specific user learned from the item side as well as the item-provider side as the positive pair and treats the representations of different users in the batch as the negative samples. Extensive experiments on three real-world SRS datasets demonstrate that IPSRec achieves substantially better results than the strong competitors. For reproducibility, our code and data are available at https://github.com/siat-nlp/IPSRec.|顺序推荐系统(SRS)由于具有捕获用户动态偏好的强大功能，近年来成为研究的热点。SRS 背后的关键思想是在用户-项目交互之间建立顺序依赖关系模型。然而，我们认为用户的偏好不仅取决于他们的观点或购买项目，而且还受到项目供应商的用户已经互动。例如，在一个短视频场景中，用户可能会点击一个视频，因为他/她要么被视频内容吸引，要么被视频提供商吸引，因为视频博客是他/她的偶像。基于上述观察，本文提出了一种新的项目提供者协同学习的序贯推荐框架 IPSRec。具体来说，我们提出了两种表示学习方法(单蒸汽和跨流)来学习综合项目和用户表示基于用户的历史项目序列和提供者序列。然后，采用对比学习的方法，以自监督的方式进一步增强用户嵌入，将从项目侧和项目提供者侧学习到的特定用户的表征视为正对，将批处理中不同用户的表征视为负样本。对三个实际 SRS 数据集的大量实验表明，IPSRec 比强大的竞争对手获得了更好的结果。为确保重复性，我们的代码和数据可在 https://github.com/siat-nlp/ipsrec 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Item-Provider+Co-learning+for+Sequential+Recommendation)|0|
|[Inconsistent Ranking Assumptions in Medical Search and Their Downstream Consequences](https://doi.org/10.1145/3477495.3531898)|Daniel Cohen, Kevin Du, Bhaskar Mitra, Laura Mercurio, Navid Rekabsaz, Carsten Eickhoff|Johannes Kepler Univ Linz, Linz, Austria; Brown Univ, Providence, RI 02912 USA; Microsoft, Montreal, PQ, Canada; Brown Univ, Alpert Med Sch, Providence, RI 02912 USA|Given a query, neural retrieval models predict point estimates of relevance for each document; however, a significant drawback of relying solely on point estimates is that they contain no indication of the model's confidence in its predictions. Despite this lack of information, downstream methods such as reranking, cutoff prediction, and none-of-the-above classification are still able to learn effective functions to accomplish their respective tasks. Unfortunately, these downstream methods can suffer poor performance when the initial ranking model loses confidence in its score predictions. This becomes increasingly important in high-stakes settings, such as medical searches that can influence health decision making. Recent work has resolved this lack of information by introducing Bayesian uncertainty to capture the possible distribution of a document score. This paper presents the use of this uncertainty information as an indicator of how well downstream methods will function over a ranklist. We highlight a significant bias against certain disease-related queries within the posterior distribution of a neural model, and show that this bias in a model's predictive distribution propagates to downstream methods. Finally, we introduce a multi-distribution uncertainty metric, confidence decay, as a valid way of partially identifying these failure cases in an offline setting without the need of any user feedback.|给定一个查询，神经检索模型预测每个文档的相关性点估计; 然而，仅仅依赖点估计的一个显著缺点是，它们不包含模型对其预测的置信度的指示。尽管缺乏这种信息，下游方法，如重新排序，截止预测，以及没有上述分类仍然能够学习有效的功能，以完成各自的任务。不幸的是，当初始排名模型对其分数预测失去信心时，这些下游方法的性能可能会很差。这在高风险环境中变得越来越重要，例如可以影响健康决策的医学搜索。最近的工作通过引入贝叶斯不确定性来捕获文档分数的可能分布，解决了这种信息缺乏的问题。本文介绍了使用这种不确定性信息作为一个指标，以及下游方法将如何在一个排名表的功能。我们强调，在神经模型的后验概率中，某些与疾病相关的查询存在显著的偏差，并表明模型预测分布的这种偏差会传播到下游方法。最后，我们介绍了一个多分布不确定性度量，置信度衰减，作为一个有效的方法，部分识别这些失败案例在脱机设置，而不需要任何用户反馈。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Inconsistent+Ranking+Assumptions+in+Medical+Search+and+Their+Downstream+Consequences)|0|
|[Exploiting Session Information in BERT-based Session-aware Sequential Recommendation](https://doi.org/10.1145/3477495.3531910)|Jinseok Jamie Seol, Youngrok Ko, Sanggoo Lee|Seoul National University, Seoul, Republic of Korea|In recommendation systems, utilizing the user interaction history as sequential information has resulted in great performance improvement. However, in many online services, user interactions are commonly grouped by sessions that presumably share preferences, which requires a different approach from ordinary sequence representation techniques. To this end, sequence representation models with a hierarchical structure or various viewpoints have been developed but with a rather complex network structure. In this paper, we propose three methods to improve recommendation performance by exploiting session information while minimizing additional parameters in a BERT-based sequential recommendation model: using session tokens, adding session segment embeddings, and a time-aware self-attention. We demonstrate the feasibility of the proposed methods through experiments on widely used recommendation datasets.|在推荐系统中，利用用户交互历史作为序列信息，可以大大提高推荐系统的性能。然而，在许多在线服务中，用户交互通常按照可能共享首选项的会话进行分组，这需要一种不同于普通序列表示技术的方法。为此，开发了具有层次结构或不同视点的序列表示模型，但其网络结构相当复杂。在基于 BERT 的顺序推荐模型中，我们提出了利用会话信息同时最小化附加参数来提高推荐性能的三种方法: 使用会话令牌、增加会话段嵌入和有时间意识的自我注意。通过在广泛使用的推荐数据集上的实验，验证了该方法的可行性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploiting+Session+Information+in+BERT-based+Session-aware+Sequential+Recommendation)|0|
|[Towards Reproducible Machine Learning Research in Information Retrieval](https://doi.org/10.1145/3477495.3532686)|Ana Lucic, Maurits J. R. Bleeker, Maarten de Rijke, Koustuv Sinha, Sami Jullien, Robert Stojnic|McGill University, Montreal, Canada; Facebook AI Research, London, United Kingdom; University of Amsterdam, Amsterdam, Netherlands|While recent progress in the field of machine learning (ML) and information retrieval (IR) has been significant, the reproducibility of these cutting-edge results is often lacking, with many submissions failing to provide the necessary information in order to ensure subsequent reproducibility. Despite the introduction of self-check mechanisms before submission (such as the Reproducibility Checklist, criteria for evaluating reproducibility during reviewing at several major conferences, artifact review and badging framework, and dedicated reproducibility tracks and challenges at major IR conferences, the motivation for executing reproducible research is lacking in the broader information community. We propose this tutorial as a gentle introduction to help ensure reproducible research in IR, with a specific emphasis on ML aspects of IR research.|虽然机器学习(ML)和信息检索学习(IR)领域的最新进展显著，但这些尖端结果的可重复性往往缺乏，许多提交的文件未能提供必要的信息，以确保随后的可重复性。尽管在提交之前引入了自我检查机制(例如重现性检查表，在几个主要会议上评估重现性的标准，工件审查和徽章框架，以及在主要 IR 会议上专门的重现性轨道和挑战，在更广泛的信息社区中缺乏执行可重现性研究的动机。我们建议本教程作为一个温和的介绍，以帮助确保在 IR 的重复性研究，并特别强调机器学习方面的 IR 研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Reproducible+Machine+Learning+Research+in+Information+Retrieval)|0|
|[Structured and Natural Responses Co-generation for Conversational Search](https://doi.org/10.1145/3477495.3532063)|Chenchen Ye, Lizi Liao, Fuli Feng, Wei Ji, TatSeng Chua|Singapore Management University, Singapore, Singapore; National University of Singapore, Singapore, Singapore; University of Science and Technology of China, Heifei, China; Sea-NExT Joint Lab, National University of Singapore, Singapore, Singapore|Generating fluent and informative natural responses while main- taining representative internal states for search optimization is critical for conversational search systems. Existing approaches ei- ther 1) predict structured dialog acts first and then generate natural response; or 2) map conversation context to natural responses di- rectly in an end-to-end manner. Both kinds of approaches have shortcomings. The former suffers from error accumulation while the semantic associations between structured acts and natural re- sponses are confined in single direction. The latter emphasizes generating natural responses but fails to predict structured acts. Therefore, we propose a neural co-generation model that gener- ates the two concurrently. The key lies in a shared latent space shaped by two informed priors. Specifically, we design structured dialog acts and natural response auto-encoding as two auxiliary tasks in an interconnected network architecture. It allows for the concurrent generation and bidirectional semantic associations. The shared latent space also enables asynchronous reinforcement learn- ing for further joint optimization. Experiments show that our model achieves significant performance improvements.|生成流畅和信息丰富的自然反应，同时为搜索引擎优化保留有代表性的内部状态，这对会话搜索系统至关重要。现有的方法包括: 1)预测结构化对话首先发生，然后产生自然反应; 或者2)以端到端的方式将对话上下文直接映射到自然反应。这两种方法都有缺点。结构化行为和自然反应之间的语义联系是单向的，而结构化行为和自然反应之间的语义联系是单向的。后者强调产生自然反应，但无法预测结构化行为。因此，我们提出了一个神经元协同生成模型，并生成两个。关键在于一个共享的潜在空间，由两个知情的前任塑造。具体来说，我们设计了结构化对话行为和自然响应自动编码作为互联网络体系结构中的两个辅助任务。它允许并发生成和双向语义关联。共享潜在空间还支持异步强化学习，以进一步优化联合。实验结果表明，该模型取得了显著的性能改善。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Structured+and+Natural+Responses+Co-generation+for+Conversational+Search)|0|
|[PEVAE: A Hierarchical VAE for Personalized Explainable Recommendation](https://doi.org/10.1145/3477495.3532039)|Zefeng Cai, Zerui Cai|East China Normal University, Shanghai, China|Variational autoencoders (VAEs) have been widely applied in recommendations. One reason is that their amortized inferences are beneficial for overcoming the data sparsity. However, in explainable recommendation that generates natural language explanations, they are still rarely explored. Thus, we aim to extend VAE to explainable recommendation. In this task, we find that VAE can generate acceptable explanations for users with few relevant training samples, however, it tends to generate less personalized explanations for users with relatively sufficient samples than autoencoders (AEs). We conjecture that information shared by different users in VAE disturbs the information for a specific user. To deal with this problem, we present PErsonalized VAE (PEVAE) that generates personalized natural language explanations for explainable recommendation. Moreover, we propose two novel mechanisms to aid our model in generating more personalized explanations, including 1) Self-Adaption Fusion (SAF) manipulates the latent space in a self-adaption manner for controlling the influence of shared information. In this way, our model can enjoy the advantage of overcoming the sparsity of data while generating more personalized explanations for a user with relatively sufficient training samples. 2) DEpendence Maximization (DEM) strengthens dependence between recommendations and explanations by maximizing the mutual information. It makes the explanation more specific to the input user-item pair and thus improves the personalization of the generated explanations. Extensive experiments show PEVAE can generate more personalized explanations and further analyses demonstrate the practical effect of our proposed methods.|变分自动编码器(VAE)已被广泛应用于建议。一个原因是，他们的摊销推断有利于克服数据稀疏。然而，在生成自然语言解释的可解释推荐中，它们仍然很少被探索。因此，我们的目标是将 VAE 扩展到可解释的推荐。在这个任务中，我们发现 VAE 可以在相关训练样本较少的情况下为用户生成可接受的解释，但是，与自动编码器(AE)相比，它往往在样本相对充足的情况下为用户生成较少的个性化解释。我们推测 VAE 中不同用户共享的信息会干扰特定用户的信息。为了解决这个问题，我们提出了个性化的 VAE (PEVAE) ，它可以为解释性推荐生成个性化的自然语言解释。此外，我们提出了两种新的机制来帮助我们的模型产生更多的个性化解释，包括1)自适应融合(SAF)以自适应的方式操纵潜在空间来控制共享信息的影响。通过这种方式，我们的模型可以在克服数据稀疏性的同时，通过相对充足的训练样本为用户生成更加个性化的解释。2)依赖最大化(DEM)通过最大化相互信息来增强推荐与解释之间的依赖性。它使解释更加具体到输入用户项对，从而改进了生成的解释的个性化。大量的实验表明，PEVAE 可以产生更加个性化的解释，进一步的分析表明，我们提出的方法的实际效果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PEVAE:+A+Hierarchical+VAE+for+Personalized+Explainable+Recommendation)|0|
|[Counterfactual Learning To Rank for Utility-Maximizing Query Autocompletion](https://doi.org/10.1145/3477495.3531958)|Adam Block, Rahul Kidambi, Daniel N. Hill, Thorsten Joachims, Inderjit S. Dhillon|Amazon Music, San Fransisco, CA, USA; University of Texas at Austin, Austin, TX, USA; Amazon Search, Berkeley, CA, USA; Massachusetts Institute of Technology, Cambridge, MA, USA|Conventional methods for query autocompletion aim to predict which completed query a user will select from a list. A shortcoming of this approach is that users often do not know which query will provide the best retrieval performance on the current information retrieval system, meaning that any query autocompletion methods trained to mimic user behavior can lead to suboptimal query suggestions. To overcome this limitation, we propose a new approach that explicitly optimizes the query suggestions for downstream retrieval performance. We formulate this as a problem of ranking a set of rankings, where each query suggestion is represented by the downstream item ranking it produces. We then present a learning method that ranks query suggestions by the quality of their item rankings. The algorithm is based on a counterfactual learning approach that is able to leverage feedback on the items (e.g., clicks, purchases) to evaluate query suggestions through an unbiased estimator, thus avoiding the assumption that users write or select optimal queries. We establish theoretical support for the proposed approach and provide learning-theoretic guarantees. We also present empirical results on publicly available datasets, and demonstrate real-world applicability using data from an online shopping store.|查询自动完成的传统方法旨在预测用户将从列表中选择哪个已完成的查询。这种方法的一个缺点是，用户通常不知道哪个查询将在当前的信息检索系统上提供最佳的检索性能，这意味着任何经过训练的模拟用户行为的查询自动完成方法都可能导致次优的查询建议。为了克服这一限制，我们提出了一种新的方法，显式优化查询建议的下游检索性能。我们将这个问题表述为对一组排名进行排序的问题，其中每个查询建议由它产生的下游项目排名表示。然后，我们提出了一种学习方法，根据项目排名的质量对查询建议进行排序。该算法基于一种反事实学习方法，能够利用对项目(如点击、购买)的反馈，通过一个无偏估计器来评估查询建议，从而避免了用户编写或选择最佳查询的假设。我们为提出的方法建立了理论支持，并提供了学习理论保证。我们还提出了公开可用数据集的实证结果，并证明了真实世界的适用性使用数据从网上购物商店。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Counterfactual+Learning+To+Rank+for+Utility-Maximizing+Query+Autocompletion)|0|
|[Automatic Expert Selection for Multi-Scenario and Multi-Task Search](https://doi.org/10.1145/3477495.3531942)|Xinyu Zou, Zhi Hu, Yiming Zhao, Xuchu Ding, Zhongyi Liu, Chenliang Li, Aixin Sun|Nanyang Technological University, Singapore, Singapore; Ant Group, Hangzhou, China; Wuhan University, Wuhan, China|Multi-scenario learning (MSL) enables a service provider to cater for users' fine-grained demands by separating services for different user sectors, e.g., by user's geographical region. Under each scenario there is a need to optimize multiple task-specific targets e.g., click through rate and conversion rate, known as multi-task learning (MTL). Recent solutions for MSL and MTL are mostly based on the multi-gate mixture-of-experts (MMoE) architecture. MMoE structure is typically static and its design requires domain-specific knowledge, making it less effective in handling both MSL and MTL. In this paper, we propose a novel Automatic Expert Selection framework for Multi-scenario and Multi-task search, named AESM2. AESM2 integrates both MSL and MTL into a unified framework with an automatic structure learning. Specifically, AESM2 stacks multi-task layers over multi-scenario layers. This hierarchical design enables us to flexibly establish intrinsic connections between different scenarios, and at the same time also supports high-level feature extraction for different tasks. At each multi-scenario/multi-task layer, a novel expert selection algorithm is proposed to automatically identify scenario-/task-specific and shared experts for each input. Experiments over two real-world large-scale datasets demonstrate the effectiveness of AESM2 over a battery of strong baselines. Online A/B test also shows substantial performance gain on multiple metrics. Currently, AESM2 has been deployed online for serving major traffic.|多场景学习可让服务供应商因应用户的细粒度需求，将不同用户界别的服务(例如按用户地理地区)分开。在每种情况下，都需要优化多个特定任务的目标，例如点击率和转换率，称为多任务学习(MTL)。最近的 MSL 和 MTL 解决方案大多基于多门混合专家(MMoE)体系结构。MMoE 结构通常是静态的，其设计需要特定于领域的知识，因此在处理 MSL 和 MTL 时效率较低。本文提出了一种面向多场景多任务搜索的自动专家选择框架 AESM2。AESM2将 MSL 和 MTL 集成到一个具有自动结构学习的统一框架中。具体来说，AESM2在多场景层上堆叠多任务层。这种分层设计使我们能够灵活地建立不同场景之间的内在联系，同时也支持不同任务的高级特征提取。在每个多场景/多任务层，提出了一种新的专家选择算法来自动识别每个输入的场景/任务特定的和共享的专家。通过两个真实世界的大规模数据集的实验证明了 AESM2在一组强基线上的有效性。在线 A/B 测试还显示了在多个指标上的大量性能增益。目前，AESM2已经部署在线服务主要流量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automatic+Expert+Selection+for+Multi-Scenario+and+Multi-Task+Search)|0|
|[Multi-Agent RL-based Information Selection Model for Sequential Recommendation](https://doi.org/10.1145/3477495.3532022)|Kaiyuan Li, Pengfei Wang, Chenliang Li|Wuhan University, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China|For sequential recommender, the coarse-grained yet sparse sequential signals mined from massive user-item interactions have become the bottleneck to further improve the recommendation performance. To alleviate the spareness problem, exploiting auxiliary semantic features (\eg textual descriptions, visual images and knowledge graph) to enrich contextual information then turns into a mainstream methodology. Though effective, we argue that these different heterogeneous features certainly include much noise which may overwhelm the valuable sequential signals, and therefore easily reach the phenomenon of negative collaboration (ie 1 + 1 > 2). How to design a flexible strategy to select proper auxiliary information and alleviate the negative collaboration towards a better recommendation is still an interesting and open question. Unfortunately, few works have addressed this challenge in sequential recommendation. In this paper, we introduce a Multi-Agent RL-based Information S election Model (named MARIS) to explore an effective collaboration between different kinds of auxiliary information and sequential signals in an automatic way. Specifically, MARIS formalizes the auxiliary feature selection as a cooperative Multi-agent Markov Decision Process. For each auxiliary feature type, MARIS resorts to using an agent to determine whether a specific kind of auxiliary feature should be imported to achieve a positive collaboration. In between, a QMIX network is utilized to cooperate their joint selection actions and produce an episode corresponding an effective combination of different auxiliary features for the whole historical sequence. Considering the lack of supervised selection signals, we further devise a novel reward-guided sampling strategy to leverage exploitation and exploration scheme for episode sampling. By preserving them in a replay buffer, MARIS learns the action-value function and the reward alternatively for optimization. Extensive experiments on four real-world datasets demonstrate that our model obtains significant performance improvement over up-to-date state-of-the-art recommendation models.|对于序列推荐系统来说，从大量用户交互中挖掘出的粗粒度稀疏序列信号已经成为进一步提高推荐性能的瓶颈。为了解决这一问题，利用辅助语义特征(如文本描述、视觉图像和知识图形)丰富上下文信息成为主流方法论。虽然有效，我们认为这些不同的异质性特征肯定包括大量的噪声，这可能压倒有价值的序列信号，因此很容易达到负协作现象(即1 + 1 > 2)。如何设计一种灵活的策略，选择合适的辅助信息，减轻负面协作，以获得更好的推荐，仍然是一个有趣而开放的问题。不幸的是，很少有作品在连续推荐中解决了这个问题。本文提出了一种基于多 Agent RL 的信息 S 选择模型(MARIS) ，用于探索不同辅助信息与序列信号之间的自动有效协作。具体来说，MARIS 将辅助特征选择形式化为一个合作的多代理马可夫决策过程。对于每一种辅助特征类型，MARIS 都使用一个代理来确定是否需要导入一种特定的辅助特征来实现积极的协作。其间，利用 QMIX 网络协同它们的联合选择行动，产生对应于整个历史序列的不同辅助特征的有效组合的情节。考虑到缺乏监督选择信号，我们进一步设计了一种新的奖励引导抽样策略，以利用开发和探索方案的情节抽样。通过将它们保存在一个重播缓冲区中，MARIS 学习动作-价值函数和优化的报酬。在四个真实世界数据集上的大量实验表明，我们的模型比最新的最先进的推荐模型获得了显著的性能改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Agent+RL-based+Information+Selection+Model+for+Sequential+Recommendation)|0|
|[Neural Statistics for Click-Through Rate Prediction](https://doi.org/10.1145/3477495.3531762)|Yanhua Huang, Hangyu Wang, Yiyun Miao, Ruiwen Xu, Lei Zhang, Weinan Zhang|Shanghai Jiao Tong University, Shanghai, China; Xiaohongshu Inc., Shanghai, China|With the success of deep learning, click-through rate (CTR) predictions are transitioning from shallow approaches to deep architectures. Current deep CTR prediction usually follows the Embedding & MLP paradigm, where the model embeds categorical features into latent semantic space. This paper introduces a novel embedding technique called neural statistics that instead learns explicit semantics of categorical features by incorporating feature engineering as an innate prior into the deep architecture in an end-to-end manner. Besides, since the statistical information changes over time, we study how to adapt to the distribution shift in the MLP module efficiently. Offline experiments on two public datasets validate the effectiveness of neural statistics against state-of-the-art models. We also apply it to a large-scale recommender system via online A/B tests, where the user's satisfaction is significantly improved.|随着深度学习的成功，点进率预测(ctrl)正从浅层方法向深层架构过渡。目前的深度 CTR 预测通常遵循嵌入与 MLP 范式，该模型将范畴特征嵌入到潜在语义空间中。本文介绍了一种新的嵌入技术，称为神经统计学，通过将特征工程作为一种先天优势以端到端的方式结合到深层体系结构中，来学习范畴特征的显性语义。此外，由于统计信息随时间变化，我们研究了如何有效地适应 MLP 模块中的分布变化。在两个公共数据集上的离线实验验证了针对最先进模型的神经统计的有效性。我们还通过在线 A/B 测试将其应用于大规模的推荐系统测试，用户的满意度显著提高。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Statistics+for+Click-Through+Rate+Prediction)|0|
|[Towards Results-level Proportionality for Multi-objective Recommender Systems](https://doi.org/10.1145/3477495.3531787)|Ladislav Peska, Patrik Dokoupil|Charles University, Prague, Czech Rep|The main focus of our work is the problem of multiple objectives optimization (MOO) while providing a final list of recommendations to the user. Currently, system designers can tune MOO by setting importance of individual objectives, usually in some kind of weighted average setting. However, this does not have to translate into the presence of such objectives in the final results. In contrast, in our work we would like to allow system designers or end-users to directly quantify the required relative ratios of individual objectives in the resulting recommendations, e.g., the final results should have 60% relevance, 30% diversity and 10% novelty. If individual objectives are transformed to represent quality on the same scale, these result conditioning expressions may greatly contribute towards recommendations tuneability and explainability as well as user's control over recommendations. To achieve this task, we propose an iterative algorithm inspired by the mandates allocation problem in public elections. The algorithm is applicable as long as per-item marginal gains of individual objectives can be calculated. Effectiveness of the algorithm is evaluated on several settings of relevance-novelty-diversity optimization problem. Furthermore, we also outline several options to scale individual objectives to represent similar value for the user.|我们工作的主要重点是多目标优化(MOO)问题，同时向用户提供最终的建议列表。目前，系统设计者可以通过设定个人目标的重要性来调整 MOO，通常是在某种加权平均数设置中。然而，这并不意味着在最终结果中存在这样的目标。相比之下，在我们的工作中，我们希望允许系统设计者或最终用户直接量化结果建议中各个目标所需的相对比例，例如，最终结果应该有60% 的相关性，30% 的多样性和10% 的新颖性。如果将单个目标转换为在同一尺度上表示质量，那么这些结果条件表达式可能极大地有助于建议的可调整性和可解释性，以及用户对建议的控制。为了实现这一任务，我们提出了一个迭代算法的启发任务分配问题在公共选举。只要能够计算出单个目标的单项边际收益，该算法是可行的。该算法的有效性是根据相关性-新颖性-多样性最佳化问题进行评估的。此外，我们还概述了几个选项，以缩放单个目标，表示用户的类似价值。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Results-level+Proportionality+for+Multi-objective+Recommender+Systems)|0|
|[Transform Cold-Start Users into Warm via Fused Behaviors in Large-Scale Recommendation](https://doi.org/10.1145/3477495.3531797)|Pengyang Li, Rong Chen, Quan Liu, Jian Xu, Bo Zheng|Alibaba Group, Hangzhou, China; Zhejiang University, Hangzhou, China|Recommendation for cold-start users who have very limited data is a canonical challenge in recommender systems. Existing deep recommender systems utilize user content features and behaviors to produce personalized recommendations, yet often face significant performance degradation on cold-start users compared to existing ones due to the following challenges: (1) Cold-start users may have a quite different distribution of features from existing users. (2) The few behaviors of cold-start users are hard to be exploited. In this paper, we propose a recommender system called Cold-Transformer to alleviate these problems. Specifically, we design context-based Embedding Adaption to offset the differences in feature distribution. It transforms the embedding of cold-start users into a warm state that is more like existing ones to represent corresponding user preferences. Furthermore, to exploit the few behaviors of cold-start users and characterize the user context, we propose Label Encoding that models Fused Behaviors of positive and negative feedback simultaneously, which are relatively more sufficient. Last, to perform large-scale industrial recommendations, we keep the two-tower architecture that de-couples user and target item. Extensive experiments on public and industrial datasets show that Cold-Transformer significantly outperforms state-of-the-art methods, including those that are deep coupled and less scalable.|对于数据非常有限的冷启动用户的推荐在推荐系统中是一个典型的挑战。现有的深度推荐系统利用用户内容特征和行为来产生个性化的推荐，但是由于以下挑战，冷启动用户的性能往往比现有的推荐系统有显著的下降: (1)冷启动用户可能具有与现有用户完全不同的特征分布。(2)冷启动用户的少数行为很难被利用。在这篇文章中，我们提出了一个叫做冷变压器的推荐系统来缓解这些问题。具体来说，我们设计了基于上下文的嵌入适应，以抵消特征分布的差异。它将冷启动用户的嵌入转换为更像现有用户的暖状态，以表示相应的用户偏好。此外，为了充分利用冷启动用户的少数行为并刻画用户上下文特征，我们提出了标签编码方法，该方法同时对正反馈和负反馈的融合行为进行建模，相对来说比较充分。最后，为了执行大规模的工业建议，我们保留了解耦用户和目标项目的双塔架构。在公共和工业数据集上进行的大量实验表明，冷变压器的性能明显优于最先进的方法，包括那些深度耦合和可伸缩性较差的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Transform+Cold-Start+Users+into+Warm+via+Fused+Behaviors+in+Large-Scale+Recommendation)|0|
|[Coarse-to-Fine Sparse Sequential Recommendation](https://doi.org/10.1145/3477495.3531732)|Jiacheng Li, Tong Zhao, Jin Li, Jim Chan, Christos Faloutsos, George Karypis, SooMin Pantel, Julian J. McAuley|University of California, San Diego, La Jolla, CA, USA; Amazon, Seattle, WA, USA; Carnegie Mellon University, Pittsburgh, PA, USA|Sequential recommendation aims to model dynamic user behavior from historical interactions. Self-attentive methods have proven effective at capturing short-term dynamics and long-term preferences. Despite their success, these approaches still struggle to model sparse data, on which they struggle to learn high-quality item representations. We propose to model user dynamics from shopping intents and interacted items simultaneously. The learned intents are coarse-grained and work as prior knowledge for item recommendation. To this end, we present a coarse-to-fine self-attention framework, namely CaFe, which explicitly learns coarse-grained and fine-grained sequential dynamics. Specifically, CaFe first learns intents from coarse-grained sequences which are dense and hence provide high-quality user intent representations. Then, CaFe fuses intent representations into item encoder outputs to obtain improved item representations. Finally, we infer recommended items based on representations of items and corresponding intents. Experiments on sparse datasets show that CaFe outperforms state-of-the-art self-attentive recommenders by 44.03% [email protected] on average.|顺序推荐旨在从历史交互中建立动态用户行为模型。事实证明，自我关注的方法在捕捉短期动态和长期偏好方面是有效的。尽管这些方法取得了成功，但它们仍然难以建立稀疏数据的模型，难以在稀疏数据上学习高质量的项目表示。我们建议同时从购物意图和交互项目建立用户动态模型。学习意图是粗粒度的，作为项目推荐的先验知识。为此，我们提出了一个由粗到细的自我注意框架，即 CaFe，它显式地学习粗粒度和细粒度的序列动力学。具体来说，CaFe 首先从密集的粗粒度序列中学习意图，因此提供高质量的用户意图表示。然后，CaFe 将意图表示融合到项编码器输出中，以获得改进的项表示。最后，根据项目的表示和相应的意图推断推荐项目。在稀疏数据集上的实验表明，CaFe 的性能平均比最先进的自我关注推荐系统高出44.03% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Coarse-to-Fine+Sparse+Sequential+Recommendation)|0|
|[Conversational Recommendation via Hierarchical Information Modeling](https://doi.org/10.1145/3477495.3531830)|Quan Tu, Shen Gao, Yanran Li, Jianwei Cui, Bin Wang, Rui Yan|Renmin University of China, Beijing, China; Xiaomi AI Lab, Beijing, China; Peking University, Beijing, China|Conversational recommendation system aims to recommend appropriate items to user by directly asking preference on attributes or recommending item list. However, most of existing methods only employ the flat item and attribute relationship, and ignore the hierarchical relationship connected by the similar user which can provide more comprehensive information. And these methods usually use the user accepted attributes to represent the conversational history and ignore the hierarchical information of sequential transition in the historical turns. In this paper, we propose Hierarchical Information-aware Conversational Recommender (HICR) to model the two types of hierarchical information to boost the performance of CRS. Experiments conducted on four benchmark datasets verify the effectiveness of our proposed model.|会话推荐系统旨在通过直接询问用户对属性的偏好或推荐项目列表来向用户推荐合适的项目。然而，现有的方法大多只使用平面项目和属性关系，而忽略了相似用户之间的层次关系，这样可以提供更全面的信息。这些方法通常使用用户接受的属性来表示会话历史，而忽略了历史转折中顺序转换的层次信息。本文提出了基于层次信息感知的会话推荐系统(HICR) ，对两种层次信息进行建模，以提高会话推荐系统的性能。在四个基准数据集上进行的实验验证了该模型的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Conversational+Recommendation+via+Hierarchical+Information+Modeling)|0|
|[CTnoCVR: A Novelty Auxiliary Task Making the Lower-CTR-Higher-CVR Upper](https://doi.org/10.1145/3477495.3531843)|Dandan Zhang, Haotian Wu, Guanqi Zeng, Yao Yang, Weijiang Qiu, Yujie Chen, Haoyuan Hu|Beijing Jiaotong University, Beijing, China; Zhejiang Lab, Hangzhou, China; China Electric Power Research Institute, Beijing, China; Cainiao Network, Hangzhou, China|In recent years, multi-task learning models based on deep learning in recommender systems have attracted increasing attention from researchers in industry and academia. Accurately estimating post-click conversion rate (CVR) is often considered as the primary task of multi-task learning in recommender systems. However, some advertisers may try to get higher click-through rates (CTR) by over-decorating their ads, which may result in excessive exposure to samples with lower CVR. For example, some only eye-catching clickbait have higher CTR, but actually, CVR is very low. As a result, the overall performance of the recommender system will be hurt. In this paper, we introduce a novelty auxiliary task called CTnoCVR, which aims to predict the probability of events with click but no-conversion, in various state-of-the-art multi-task models of recommender systems to promote samples with high CVR but low CTR. Plentiful Experiments on a large-scale dataset gathered from traffic logs of Taobao's recommender system demonstrate that the introduction of CTnoCVR task significantly improves the prediction effect of CVR under various multi-task frameworks. In addition, we conduct the online test and evaluate the effectiveness of our proposed method to make those samples with high CVR and low CTR rank higher.|近年来，推荐系统中基于深度学习的多任务学习模型越来越受到业界和学术界的关注。在推荐系统中，准确估计点击后转换率(CVR)常常被认为是多任务学习的首要任务。然而，一些广告商可能试图通过过度装饰他们的广告来获得更高的点击率(CTR) ，这可能导致过度暴露于低 CVR 的样品。例如，一些只有吸引眼球的点击诱饵有较高的点击率，但实际上，CVR 是非常低的。因此，推荐系统的整体表现将受到影响。本文介绍了一种新颖的辅助任务 CTnoCVR，该任务在推荐系统的多任务模型中预测点击不转换的事件发生概率，以提升高 CVR 低 CTR 的样本。大量的实验表明，在多任务框架下，引入 CTnoCVR 任务可以显著提高 CVR 的预测效果。这些实验都是从淘宝推荐系统的流量日志中收集的大规模数据集中得到的。此外，我们进行了在线测试，并评估了我们提出的方法的有效性，使高 CVR 和低 CTR 排名的样本。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CTnoCVR:+A+Novelty+Auxiliary+Task+Making+the+Lower-CTR-Higher-CVR+Upper)|0|
|[Smooth-AUC: Smoothing the Path Towards Rank-based CTR Prediction](https://doi.org/10.1145/3477495.3531865)|Shuang Tang, Fangyuan Luo, Jun Wu|Beijing Jiaotong University, Beijing, China|Deep neural networks (DNNs) have been a key technique for click-through rate (CTR) estimation, yet existing DNNs-based CTR models neglect the inconsistency between their optimization objectives (e.g., Binary Cross Entropy, BCE) and CTR ranking metrics (e.g., Area Under the ROC Curve, AUC). It is noteworthy that directly optimizing AUC by gradient-descent methods is difficult due to the non-differentiable Heaviside function built-in AUC. To this end, we propose a smooth approximation of AUC, called smooth-AUC (SAUC), towards the rank-based CTR prediction. Specifically, SAUC relaxes the Heaviside function via sigmoid with a temperature coefficient (aiming at controlling the function sharpness) in order to facilitate the gradient-based optimization. Furthermore, SAUC is a plug-and-play objective that can be used in any DNNs-based CTR model. Experimental results on two real-world datasets demonstrate that SAUC consistently improves the recommendation accuracy of current DNNs-based CTR models.|深度神经网络(DNN)一直是点进率评估的关键技术，然而现有的基于 DNN 的 CTR 模型忽略了它们的优化目标(例如，二进制交叉熵，BCE)和 CTR 排名指标(例如，ROC Curve 下面积，AUC)之间的不一致性。值得注意的是，由于 AUC 内置的不可微单位阶跃函数，用梯度下降法直接优化 AUC 是困难的。为此，我们提出了一种平滑近似的 AUC，称为平滑 AUC (SAUC) ，用于基于秩的 CTR 预测。具体来说，SAUC 通过 sigmoid 放松单位阶跃函数(目的是控制函数的清晰度) ，以便于基于梯度的优化温度系数。此外，SAUC 是一个即插即用的目标，可以在任何基于 DNN 的 CTR 模型中使用。在两个实际数据集上的实验结果表明，SAUC 一致地提高了当前基于 DNN 的 CTR 模型的推荐精度。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Smooth-AUC:+Smoothing+the+Path+Towards+Rank-based+CTR+Prediction)|0|
|[Alignment Rationale for Query-Document Relevance](https://doi.org/10.1145/3477495.3531883)|Youngwoo Kim, Razieh Rahimi, James Allan|University of Massachusetts Amherst, Amherst, MA, USA|Deep neural networks are widely used for text pair classification tasks such as as adhoc information retrieval. These deep neural networks are not inherently interpretable and require additional efforts to get rationale behind their decisions. Existing explanation models are not yet capable of inducing alignments between the query terms and the document terms -- which part of the document rationales are responsible for which part of the query? In this paper, we study how the input perturbations can be used to infer or evaluate alignments between the query and document spans, which best explain the black-box ranker's relevance prediction. We use different perturbation strategies and accordingly propose a set of metrics to evaluate the faithfulness of alignment rationales to the model. Our experiments show that the defined metrics based on substitution-based perturbation are more successful in preferring higher-quality alignments, compared to the deletion-based metrics.|深度神经网络广泛用于文本对分类任务，如自组织信息检索。这些深层神经网络本质上是不可解释的，需要额外的努力来获得其决策背后的理由。现有的解释模型还不能在查询术语和文档术语之间引入对齐——文档基本原理的哪一部分负责查询的哪一部分？在本文中，我们研究了如何利用输入扰动来推断或评估查询和文档跨度之间的对齐，这最好地解释了黑盒排名的相关性预测。我们使用不同的摄动策略，并相应地提出了一套度量来评估对齐基本原理的忠实性模型。我们的实验表明，与基于删除的度量相比，基于替换扰动的度量更容易获得高质量的比对。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Alignment+Rationale+for+Query-Document+Relevance)|0|
|[Learning to Rank Knowledge Subgraph Nodes for Entity Retrieval](https://doi.org/10.1145/3477495.3531888)|Parastoo Jafarzadeh, Zahra Amirmahani, Faezeh Ensan|Ferdowsi University of Mashhad, Mashhad, Iran; Ryerson University, Toronto, ON, Canada|The importance of entity retrieval, the task of retrieving a ranked list of related entities from big knowledge bases given a textual query, has been widely acknowledged in the literature. In this paper, we propose a novel entity retrieval method that addresses the important challenge that revolves around the need to effectively represent and model context in which entities relate to each other. Based on our proposed method, a model is firstly trained to retrieve and prune a subgraph of a textual knowledge graph that represents contextual relationships between entities. Secondly, a deep model is introduced to reason over the textual content of nodes, edges, and the given question and score and rank entities in the subgraph. We show experimentally that our approach outperforms state-of-the-art methods on a number of benchmarks for entity retrieval.|实体检索的重要性在文献中得到了广泛的认可。在本文中，我们提出了一种新的实体检索方法，以解决围绕着需要有效地表示和模型实体相互关联的上下文的重要挑战。基于该方法，首先训练一个模型来检索和剪枝表示实体间上下文关系的文本知识图的子图。其次，引入一个深度模型来推理子图中节点、边和给定问题的文本内容以及子图中的得分和排序实体。我们的实验表明，我们的方法在实体检索的许多基准上优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Rank+Knowledge+Subgraph+Nodes+for+Entity+Retrieval)|0|
|[ELECRec: Training Sequential Recommenders as Discriminators](https://doi.org/10.1145/3477495.3531894)|Yongjun Chen, Jia Li, Caiming Xiong|Salesforce Research, Palo Alto, CA, USA|Sequential recommendation is often considered as a generative task, i.e., training a sequential encoder to generate the next item of a user's interests based on her historical interacted items. Despite their prevalence, these methods usually require training with more meaningful samples to be effective, which otherwise will lead to a poorly trained model. In this work, we propose to train the sequential recommenders as discriminators rather than generators. Instead of predicting the next item, our method trains a discriminator to distinguish if a sampled item is a 'real' target item or not. A generator, as an auxiliary model, is trained jointly with the discriminator to sample plausible alternative next items and will be thrown out after training. The trained discriminator is considered as the final SR model and denoted as \modelname. Experiments conducted on four datasets demonstrate the effectiveness and efficiency of the proposed approach.|顺序推荐通常被认为是一个生成任务，例如，训练一个顺序编码器根据用户的历史交互项目生成下一个用户感兴趣的项目。尽管这些方法普遍存在，但通常需要训练更有意义的样本才能有效，否则将导致训练不足的模型。在这项工作中，我们建议训练顺序推荐器作为鉴别器，而不是生成器。我们的方法不是预测下一个项目，而是训练一个鉴别器来区分一个采样的项目是否是“真正的”目标项目。发电机作为辅助模型，与鉴别器联合训练，以抽样合理的替代下一个项目，并将在训练后抛出。训练后的鉴别器被认为是最终的 SR 模型，并表示为模型名。在四个数据集上进行的实验表明了该方法的有效性和高效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ELECRec:+Training+Sequential+Recommenders+as+Discriminators)|0|
|[A2A-API: A Prototype for Biomedical Information Retrieval Research and Benchmarking](https://doi.org/10.1145/3477495.3531667)|Maciej Rybinski, Liam Watts, Sarvnaz Karimi|CSIRO Data61, Sydney, NSW, Australia|Finding relevant literature is crucial for biomedical research and in the practice of evidence-based medicine, making biomedical search an important application area within the field of information retrieval. This is recognised by the broader IR community, and in particular by the organisers of Text Retrieval Conference (TREC) as early as 2003. While TREC provides crucial evaluation resources, to get started in biomedical IR one needs to tackle an important software engineering hurdle of parsing, indexing, and deploying several large document collections. Moreover, many newcomers to the field often face a steep learning curve, where theoretical concepts are tangled up with technical aspects. Finally, many of the existing baselines and systems are difficult to reproduce. We aim to alleviate all three of these bottlenecks with the launch of A2A-API. It is a RESTful API which serves as an easy-to-use and programming-language-independent interface to existing biomedical TREC collections. It builds upon A2A, our system for biomedical information retrieval benchmarking, and extends it with additional functionalities. Apart from providing programmatic access to the features of the original A2A system - focused principally on benchmarking - A2A-API supports biomedical IR researchers in development of systems featuring reranking and query reformulation components. In this demonstration, we illustrate the capabilities of A2A-API with comprehensive use cases.|寻找相关文献对于生物医学研究和循证医学的实践至关重要，这使得生物医学搜索成为信息检索领域的一个重要应用领域。早在2003年，更广泛的信息检索社区，特别是文本检索会议(TREC)的组织者就认识到了这一点。虽然 TREC 提供了关键的评估资源，但要开始学习生物医学 IR，需要解决一个重要的软件工程障碍，即解析、索引和部署几个大型文档集。此外，该领域的许多新手往往面临一个陡峭的学习曲线，其中理论概念与技术方面纠缠在一起。最后，许多现有的基线和系统很难再现。我们的目标是通过推出 A2A-API 来缓解所有这三个瓶颈。它是一个 RESTful API，作为一个易于使用和独立于编程语言的接口，用于现有的生物医学 TREC 集合。它建立在我们的生物医学信息检索基准测试系统 A2A 的基础上，并扩展了其他功能。除了提供对原始 A2A 系统特性的程序访问(主要侧重于基准测试)外，A2A-API 还支持生物医学红外研究人员开发具有重新排序和查询重新制定组件的系统。在本演示中，我们通过全面的用例说明了 A2A-API 的功能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A2A-API:+A+Prototype+for+Biomedical+Information+Retrieval+Research+and+Benchmarking)|0|
|[Learning to Rank Instant Search Results with Multiple Indices: A Case Study in Search Aggregation for Entertainment](https://doi.org/10.1145/3477495.3536334)|Scott Rome, Sardar Hamidian, Richard Walsh, Kevin Foley, Ferhan Ture|Comcast, Sunnyvale, CA, USA; Comcast, Washington, DC, USA; Comcast, Philadelphia, PA, USA|At Xfinity, an instant search system provides a variety of results for a given query from different sources. For each keystroke, new results are rendered on screen to the user, which could contain movies, television series, sporting events, music videos, news clips, person pages, and other result types. Users are also able to use the Xfinity Voice Remote to submit longer queries, some of which are more open-ended. Examples of queries include incomplete words which match multiple results through lexical matching (i.e., "ali"), topical searches ("vampire movies"), and more specific longer searches ("Movies with Adam Sandler"). Since results can be based on lexical matches, semantic matches, item-to-item similarity matches, or a variety of business logic driven sources, a key challenge is how to combine results into a single list. To accomplish this, we propose merging the lists via a Learning to Rank (LTR) neural model which takes into account the search query. This combined list can be personalized via a second LTR neural model with knowledge of the user's search history and metadata of the programs. Because instant search is under-represented in the literature, we present our learnings from research to aid other practitioners.|在 Xfinity，即时搜索系统为来自不同来源的特定查询提供多种结果。对于每次按键，新的结果都会在屏幕上呈现给用户，其中可能包含电影、电视剧、体育赛事、音乐视频、新闻剪辑、人物页面和其他结果类型。用户还可以使用 Xfinity Voice Remote 提交更长的查询，其中一些查询更为开放。查询的例子包括通过词汇匹配(例如“ ali”)匹配多个结果的不完整单词、主题搜索(“吸血鬼电影”)和更具体的长搜索(“与 Adam Sandler 的电影”)。由于结果可以基于词汇匹配、语义匹配、项目间相似性匹配或各种业务逻辑驱动源，因此一个关键的挑战是如何将结果组合成一个单独的列表。为了实现这一点，我们建议通过一个学习排序(LTR)神经模型，考虑到搜索查询合并列表。这个组合列表可以通过第二个具有用户搜索历史和程序元数据知识的 LTR 神经模型进行个性化。因为即时搜索在文献中的代表性不足，我们提出我们从研究中学到的东西来帮助其他从业者。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Rank+Instant+Search+Results+with+Multiple+Indices:+A+Case+Study+in+Search+Aggregation+for+Entertainment)|0|
|[Scalable Exploration for Neural Online Learning to Rank with Perturbed Feedback](https://doi.org/10.1145/3477495.3532057)|Yiling Jia, Hongning Wang|University of Virginia, Charlottesville, VA, USA|Deep neural networks (DNNs) demonstrates significant advantages in improving ranking performance in retrieval tasks. Driven by the recent developments in optimization and generalization of DNNs, learning a neural ranking model online from its interactions with users becomes possible. However, the required exploration for model learning has to be performed in the entire neural network parameter space, which is prohibitively expensive and limits the application of such online solutions in practice. In this work, we propose an efficient exploration strategy for online interactive neural ranker learning based on bootstrapping. Our solution is based on an ensemble of ranking models trained with perturbed user click feedback. The proposed method eliminates explicit confidence set construction and the associated computational overhead, which enables the online neural rankers training to be efficiently executed in practice with theoretical guarantees. Extensive comparisons with an array of state-of-the-art OL2R algorithms on two public learning to rank benchmark datasets demonstrate the effectiveness and computational efficiency of our proposed neural OL2R solution.|深层神经网络(DNN)在提高检索任务的排序性能方面具有显著的优势。在 DNN 优化和泛化的最新发展的驱动下，从与用户的交互中学习在线神经排序模型成为可能。然而，模型学习所需要的探索必须在整个神经网络参数空间中进行，这是非常昂贵的，并且限制了这种在线解决方案在实际中的应用。本文提出了一种基于自举的在线交互式神经排序学习的有效探索策略。我们的解决方案是基于一个排名模型的集合训练与不安的用户点击反馈。该方法消除了显式置信集结构和相关的计算开销，使在线神经排序训练能够在理论保证的情况下在实际应用中有效地执行。通过与一系列最先进的 OL2R 算法在两个公共学习基准数据集上的广泛比较，证明了我们提出的神经 OL2R 解决方案的有效性和计算效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scalable+Exploration+for+Neural+Online+Learning+to+Rank+with+Perturbed+Feedback)|0|
|[Towards Validating Long-Term User Feedbacks in Interactive Recommendation Systems](https://doi.org/10.1145/3477495.3531869)|Hojoon Lee, Dongyoon Hwang, Kyushik Min, Jaegul Choo|KAKAO Enterprise, SeongNam, Republic of Korea; KAIST, SeongNam, Republic of Korea|Interactive Recommender Systems (IRSs) have attracted a lot of attention, due to their ability to model interactive processes between users and recommender systems. Numerous approaches have adopted Reinforcement Learning (RL) algorithms, as these can directly maximize users' cumulative rewards. In IRS, researchers commonly utilize publicly available review datasets to compare and evaluate algorithms. However, user feedback provided in public datasets merely includes instant responses (e.g., a rating), with no inclusion of delayed responses (e.g., the dwell time and the lifetime value). Thus, the question remains whether these review datasets are an appropriate choice to evaluate the long-term effects in IRS. In this work, we revisited experiments on IRS with review datasets and compared RL-based models with a simple reward model that greedily recommends the item with the highest one-step reward. Following extensive analysis, we can reveal three main findings: First, a simple greedy reward model consistently outperforms RL-based models in maximizing cumulative rewards. Second, applying higher weighting to long-term rewards leads to degradation of recommendation performance. Third, user feedbacks have mere long-term effects in the benchmark datasets. Based on our findings, we conclude that a dataset has to be carefully verified and that a simple greedy baseline should be included for a proper evaluation of RL-based IRS approaches. Our code and dataset are available at https://github.com/dojeon-ai/irs_validation.|交互式推荐系统(IRS)由于能够对用户和推荐系统之间的交互过程进行建模而引起了人们的广泛关注。许多方法都采用了强化学习算法，因为这些算法可以直接最大化用户的累积回报。在 IRS 中，研究人员通常利用公开的评论数据集来比较和评估算法。然而，在公共数据集中提供的用户反馈只包括即时响应(例如，评级) ，没有包括延迟响应(例如，停留时间和生命周期值)。因此，问题仍然是这些审查数据集是否是评估 IRS 长期影响的合适选择。在这项工作中，我们重新回顾了 IRS 的实验与评论数据集，并比较了基于 RL 的模型与一个简单的奖励模型，贪婪地推荐项目具有最高的一步奖励。经过广泛的分析，我们可以揭示三个主要的发现: 第一，一个简单的贪婪报酬模型在最大化累积报酬方面始终优于基于 RL 的模型。其次，对长期奖励加权会导致推荐绩效的下降。第三，用户反馈在基准数据集中只有长期效果。基于我们的研究结果，我们得出结论，一个数据集必须被仔细验证，并且一个简单的贪婪基线应该被包括在一个基于 RL 的 IRS 方法的正确评估中。我们的代码和数据集可在 https://github.com/dojeon-ai/irs_validation 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Validating+Long-Term+User+Feedbacks+in+Interactive+Recommendation+Systems)|0|
|[Structure-Aware Semantic-Aligned Network for Universal Cross-Domain Retrieval](https://doi.org/10.1145/3477495.3532061)|Jialin Tian, Xing Xu, Kai Wang, Zuo Cao, Xunliang Cai, Heng Tao Shen|University of Electronic Science and Technology of China & Peng Cheng Laboratory, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China; Meituan, Shanghai, China|The goal of cross-domain retrieval (CDR) is to search for instances of the same category in one domain by using a query from another domain. Existing CDR approaches mainly consider the standard scenario that the cross-domain data for both training and testing come from the same categories and underlying distributions. However, these methods cannot be well extended to the newly emerging task of universal cross-domain retrieval (UCDR), where the testing data belong to the domain and categories not present during training. Compared to CDR, the UCDR task is more challenging due to (1) visually diverse data from multi-source domains, (2) the domain shift between seen and unseen domains, and (3) the semantic shift across seen and unseen categories. To tackle these problems, we propose a novel model termed Structure-Aware Semantic-Aligned Network (SASA) to align the heterogeneous representations of multi-source domains without loss of generalizability for the UCDR task. Specifically, we leverage the advanced Vision Transformer (ViT) as the backbone and devise a distillation-alignment ViT (DAViT) with a novel token-based strategy, which incorporates two complementary distillation and alignment tokens into the ViT architecture. In addition, the distillation token is devised to improve the generalizability of our model by structure information preservation and the alignment token is used to improve discriminativeness with trainable categorical prototypes. Extensive experiments on three large-scale benchmarks, i.e., Sketchy, TU-Berlin, and DomainNet, demonstrate the superiority of our SASA method over the state-of-the-art UCDR and ZS-SBIR methods.|跨域检索(CDR)的目标是通过使用来自另一个域的查询在一个域中搜索相同类别的实例。现有的 CDR 方法主要考虑这样的标准场景: 用于培训和测试的跨域数据来自相同的类别和底层分布。然而，这些方法不能很好地推广到新出现的通用跨域检索(UCDR)任务，其中的测试数据属于领域和类别不存在的训练过程中。与 CDR 相比，UCDR 任务更具挑战性，因为(1)来自多源域的视觉多样化数据，(2)可见和不可见域之间的域转移，以及(3)跨可见和不可见类别的语义转移。为了解决这些问题，我们提出了一种称为结构感知语义对齐网络(SASA)的新模型，该模型可以在不损失 UCDR 任务通用性的前提下对多源域的异构表示进行对齐。具体而言，我们利用先进的视觉变压器(ViT)作为骨干，并设计了一种蒸馏对准 ViT (DAViT) ，其具有基于令牌的新策略，其将两个互补的蒸馏和对准令牌合并到 ViT 体系结构中。此外，通过结构信息的保留，设计了精馏令牌来提高模型的泛化能力，并利用对齐令牌来提高可训练范畴原型的区分能力。在 Sketchy、 TU-Berlin 和 DomainNet 这三个大型基准测试上的大量实验证明了我们的 SASA 方法优于最先进的 UCDR 和 ZS-SBIR 方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Structure-Aware+Semantic-Aligned+Network+for+Universal+Cross-Domain+Retrieval)|0|
|[Enhancing Top-N Item Recommendations by Peer Collaboration](https://doi.org/10.1145/3477495.3531773)|Yang Sun, Fajie Yuan, Min Yang, Alexandros Karatzoglou, Li Shen, Xiaoyan Zhao|Harbin Institute of Technology, Shenzhen, Shenzhen, China; Google Research, London, United Kingdom; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Westlake University, Hangzhou, China; JD Explore Academy, Beijing, China|Deep neural networks (DNN) based recommender models often require numerous parameters to achieve remarkable performance. However, this inevitably brings redundant neurons, a phenomenon referred to as over-parameterization. In this paper, we plan to exploit such redundancy phenomena for recommender systems (RS), and propose a top-N item recommendation framework called PCRec that leverages collaborative training of two recommender models of the same network structure, termed peer collaboration. We first introduce two criteria to identify the importance of parameters of a given recommender model. Then, we rejuvenate the unimportant parameters by copying parameters from its peer network. After such an operation and retraining, the original recommender model is endowed with more representation capacity by possessing more functional model parameters. To show its generality, we instantiate PCRec by using three well-known recommender models. We conduct extensive experiments on two real-world datasets, and show that PCRec yields significantly better performance than its counterpart with the same model (parameter) size.|基于深度神经网络(DNN)的推荐模型往往需要大量的参数才能达到显著的性能。然而，这不可避免地带来了多余的神经元，这种现象被称为过度参数化。在本文中，我们计划在推荐系统中利用这种冗余现象，并提出了一个名为 PCRec 的前 N 项推荐框架，该框架利用了两个相同网络结构的推荐模型的协同训练，称为对等协作。我们首先引入两个标准来确定一个给定的推荐模型参数的重要性。然后，我们通过从其对等网络中复制参数来恢复不重要的参数。经过这样的操作和再训练，原有的推荐模型具有更多的功能模型参数，从而具有更强的表示能力。为了显示其通用性，我们使用三个著名的推荐模型来实例化 PCRec。我们在两个真实世界的数据集上进行了广泛的实验，结果表明，与相同模型(参数)大小的同类数据集相比，PCRec 产生了明显更好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Top-N+Item+Recommendations+by+Peer+Collaboration)|0|
|[Learning-to-Rank at the Speed of Sampling: Plackett-Luce Gradient Estimation with Minimal Computational Complexity](https://doi.org/10.1145/3477495.3531842)|Harrie Oosterhuis|Radboud University, Nijmegen, Netherlands|Plackett-Luce gradient estimation enables the optimization of stochastic ranking models within feasible time constraints through sampling techniques. Unfortunately, the computational complexity of existing methods does not scale well with the length of the rankings, i.e. the ranking cutoff, nor with the item collection size. In this paper, we introduce the novel PL-Rank-3 algorithm that performs unbiased gradient estimation with a computational complexity comparable to the best sorting algorithms. As a result, our novel learning-to-rank method is applicable in any scenario where standard sorting is feasible in reasonable time. Our experimental results indicate large gains in the time required for optimization, without any loss in performance. For the field, our contribution could potentially allow state-of-the-art learning-to-rank methods to be applied to much larger scales than previously feasible.|Plackett-Luce 梯度估计可以通过抽样技术在可行的时间约束下优化随机排序模型。遗憾的是，现有方法的计算复杂度并不能很好地与排名的长度(即排名截止值)和项目集合的大小相适应。在本文中，我们介绍了一种新的 PL-Rank-3算法，该算法执行无偏梯度估计，其计算复杂度与最佳排序算法相当。因此，我们的新学习排序方法适用于任何情况下，标准排序是可行的在合理的时间。我们的实验结果表明，优化所需的时间大大增加，性能没有任何损失。对于这个领域，我们的贡献可能使最先进的学习排名方法应用于比以前可行的更大的范围。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning-to-Rank+at+the+Speed+of+Sampling:+Plackett-Luce+Gradient+Estimation+with+Minimal+Computational+Complexity)|0|
|[Rethinking Correlation-based Item-Item Similarities for Recommender Systems](https://doi.org/10.1145/3477495.3532055)|Katsuhiko Hayashi|Hokkaido University, Sapporo, Japan|This paper studies correlation-based item-item similarity measures for recommendation systems. While current research on recommender systems is directed toward deep learning-based approaches, nearest neighbor methods have been still used extensively in commercial recommender systems due to their simplicity. A crucial step in item-based nearest neighbor methods is to compute similarities between items, which are generally estimated through correlation measures like Pearson. The purpose of this paper is to re-investigate the effectiveness of correlation-based nearest neighbor methods on several benchmark datasets that have been used for recommendation evaluation in recent years. This paper also provides a more effective estimation method for correlation measures than the classical Pearson correlation coefficient and shows that this leads to significant improvements in recommendation performance.|本文研究了基于相关性的推荐系统项目相似性度量。虽然目前对推荐系统的研究主要集中在基于深度学习的方法上，但是最近邻方法由于其简单性在商业推荐系统中仍然得到了广泛的应用。基于项目的最近邻方法的一个关键步骤是计算项目之间的相似性，这通常通过相关度量(如 Pearson)来估计。本文旨在重新研究基于相关性的最近邻方法在近年来用于推荐评价的几个基准数据集上的有效性。本文还提供了一种比经典的皮尔逊相关系数更有效的相关度量估计方法，结果表明这种方法可以显著提高推荐性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rethinking+Correlation-based+Item-Item+Similarities+for+Recommender+Systems)|0|
|[DeSCoVeR: Debiased Semantic Context Prior for Venue Recommendation](https://doi.org/10.1145/3477495.3531877)|Sailaja Rajanala, Arghya Pal, Manish Singh, Raphael C.W. Phan, KokSheik Wong|Indian Institute of Technology Hyderabad, Hyderabad, India; Harvard Medical School, Boston, MA, USA; Monash University Malaysia, Bandar Sunway, Malaysia|We present a novel semantic context prior-based venue recommendation system that uses only the title and the abstract of a paper. Based on the intuition that the text in the title and abstract have both semantic and syntactic components, we demonstrate that a joint training of a semantic feature extractor and syntactic feature extractor collaboratively leverages meaningful information that helps to provide venues for papers. The proposed methodology that we call DeSCoVeR at first elicits these semantic and syntactic features using a Neural Topic Model and text classifier respectively. The model then executes a transfer learning optimization procedure to perform a contextual transfer between the feature distributions of the Neural Topic Model and the text classifier during the training phase. DeSCoVeR also mitigates the document-level label bias using a Causal back-door path criterion and a sentence-level keyword bias removal technique. Experiments on the DBLP dataset show that DeSCoVeR outperforms the state-of-the-art methods.|我们提出了一个新的基于语义上下文先验的场地推荐系统，它只使用文章的标题和摘要。基于标题和摘要中的文本同时具有语义和句法成分的直觉，我们证明了语义特征提取器和句法特征提取器的联合训练协同利用有意义的信息，有助于为论文提供场所。我们提出的方法，我们称为 DeSCoVeR 首先引出这些语义和句法特征使用神经主题模型和文本分类器分别。然后，该模型执行一个迁移学习优化过程，在训练阶段在神经主题模型的特征分布和文本分类器之间进行上下文迁移。DeSCoVeR 还使用因果后门路径标准和句子级关键字偏差消除技术来减轻文档级标签偏差。在 DBLP 数据集上的实验表明，DeSCoVeR 方法的性能优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DeSCoVeR:+Debiased+Semantic+Context+Prior+for+Venue+Recommendation)|0|
|[Revisiting Bundle Recommendation: Datasets, Tasks, Challenges and Opportunities for Intent-aware Product Bundling](https://doi.org/10.1145/3477495.3531904)|Zhu Sun, Jie Yang, Kaidong Feng, Hui Fang, Xinghua Qu, Yew Soon Ong|Yanshan University, Qinhuangdao, China; Shanghai University of Finance and Economics, Shanghai, China; A*STAR Centre for Frontier AI Research and Nanyang Technological University, Singapore, Singapore; Institute of High Performance Computing and Centre for Frontier AI Research, A*STAR, Singapore, Singapore; Bytedance AI Lab, Singapore, Singapore; Delft University of Technology, Delft, Netherlands|Product bundling is a commonly-used marketing strategy in both offline retailers and online e-commerce systems. Current research on bundle recommendation is limited by: (1) noisy datasets, where bundles are defined by heuristics, e.g., products co-purchased in the same session; and (2) specific tasks, holding unrealistic assumptions, e.g., the availability of bundles for recommendation directly. In this paper, we propose to take a step back and consider the process of bundle recommendation from a holistic user experience perspective. We first construct high-quality bundle datasets with rich meta information, particularly bundle intents, through a carefully designed crowd-sourcing task. We then define a series of tasks that together, support all key steps in a typical bundle recommendation process, from bundle detection, completion, ranking, to explanation and auto-naming. Finally, we conduct extensive experiments and in-depth analysis that demonstrate the challenges of bundle recommendation, arising from the need for capturing complex relations among users, products, and bundles, as well as the research opportunities, especially in graph-based neural methods. To sum up, our study delivers new data sources, opens up new research directions, and provides useful guidance for product bundling in real e-commerce platforms. Our datasets are available at GitHub (\urlhttps://github.com/BundleRec/bundle_recommendation ).|绑售是线下零售商和在线电子商务系统中常用的营销策略。目前对捆绑推荐的研究受到以下因素的限制: (1)有噪音的数据集，其中捆绑包是由启发式定义的，例如，在同一会话中共同购买的产品; (2)具体的任务，持有不切实际的假设，例如，捆绑包的可用性直接推荐。在本文中，我们建议退一步，从整体用户体验的角度来考虑捆绑推荐的过程。我们首先通过一个精心设计的众包任务，构建包含丰富元信息的高质量捆绑数据集，特别是捆绑意图。然后，我们定义一系列任务，这些任务一起支持典型的包推荐过程中的所有关键步骤，从包检测、完成、排名到解释和自动命名。最后，我们进行了广泛的实验和深入的分析，展示了捆绑推荐的挑战，由于需要捕获用户、产品和捆绑之间的复杂关系，以及研究机会，特别是在基于图的神经方法。总之，我们的研究提供了新的数据来源，开辟了新的研究方向，并为绑售在真正的电子商务平台上提供了有用的指导。我们的数据集可以在 GitHub 上获得(urlhttps:// GitHub.com/bundlerec/bundle_recommendation )。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+Bundle+Recommendation:+Datasets,+Tasks,+Challenges+and+Opportunities+for+Intent-aware+Product+Bundling)|0|
|[Query Facet Mapping and its Applications in Streaming Services: The Netflix Case Study](https://doi.org/10.1145/3477495.3536330)|Sudeep Das, Ivan Provalov, Vickie Zhang, Weidong Zhang|Netflix Inc., Los Gatos, CA, USA|In an instant search setting such as Netflix Search where results are returned in response to every keystroke, determining how a partial query maps onto broad classes of relevant entities orfacets --- such as videos, talent, and genres --- can facilitate a better understanding of the underlying objective of that query. Such a query-to-facet mapping system has a multitude of applications. It can help improve the quality of search results, drive meaningful result organization, and can be leveraged to establish trust by being transparent with Netflix members when they search for an entity that is not available on the service. By anticipating the relevant facets with each keystroke entry, the system can also better guide the experience within a search session. When aggregated across queries, the facets can reveal interesting patterns of member interest. A key challenge for building such a system is to judiciously balance lexical similarity with behavioral relevance. In this paper, we present a high level overview of a Query Facet Mapping system that we have developed at Netflix, describe its main components, provide evaluation results with real-world data, and outline several potential applications.|在像 Netflix Search 这样的即时搜索设置中，每次按键都会返回结果，确定一个部分查询如何映射到相关实体或方面的广泛类别——比如视频、人才和类型——可以促进对该查询的潜在目标的更好理解。这种查询到面的映射系统有大量的应用程序。它可以帮助提高搜索结果的质量，推动有意义的结果组织，并且可以通过在 Netflix 成员搜索服务中不可用的实体时对其保持透明来建立信任。通过预测每个按键输入的相关方面，系统还可以更好地指导搜索会话中的体验。当跨查询聚合时，方面可以显示成员感兴趣的有趣模式。建立这样一个系统的关键挑战是明智地平衡词汇相似性和行为相关性。本文对我们在 Netflix 上开发的 Query Facet Mapping 系统进行了高层次的概述，描述了它的主要组件，提供了实际数据的评估结果，并概述了几个潜在的应用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Query+Facet+Mapping+and+its+Applications+in+Streaming+Services:+The+Netflix+Case+Study)|0|
|[Implicit Feedback for Dense Passage Retrieval: A Counterfactual Approach](https://doi.org/10.1145/3477495.3531994)|Shengyao Zhuang, Hang Li, Guido Zuccon|The University of Queensland, Brisbane, QLD, Australia|In this paper we study how to effectively exploit implicit feedback in Dense Retrievers (DRs). We consider the specific case in which click data from a historic click log is available as implicit feedback. We then exploit such historic implicit interactions to improve the effectiveness of a DR. A key challenge that we study is the effect that biases in the click signal, such as position bias, have on the DRs. To overcome the problems associated with the presence of such bias, we propose the Counterfactual Rocchio (CoRocchio) algorithm for exploiting implicit feedback in Dense Retrievers. We demonstrate both theoretically and empirically that dense query representations learnt with CoRocchio are unbiased with respect to position bias and lead to higher retrieval effectiveness. We make available the implementations of the proposed methods and the experimental framework, along with all results at https://github.com/ielab/Counterfactual-DR.|本文研究了密集检索器(DRs)中如何有效地利用内隐反馈。我们考虑这样一个特定的情况，在这种情况下，来自历史点击日志的点击数据可以作为隐式反馈使用。然后，我们利用这种历史性的隐性相互作用来提高 DR 的有效性。我们研究的一个关键挑战是点击信号中的偏差(如位置偏差)对 DR 的影响。为了克服与存在这种偏差相关的问题，我们提出反事实 Rocchio (CoRocchio)算法用于利用致密检索器中的隐性反馈。我们从理论和实验两方面证明了 CoRocchio 学习的密集查询表示对位置偏差是无偏的，从而提高了检索效率。我们提供了建议方法和实验框架的实施，以及所有 https://github.com/ielab/counterfactual-dr 的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Implicit+Feedback+for+Dense+Passage+Retrieval:+A+Counterfactual+Approach)|0|
|[Offline Evaluation of Ranked Lists using Parametric Estimation of Propensities](https://doi.org/10.1145/3477495.3532032)|Vishwa Vinay, Manoj Kilaru, David Arbour|Adobe Research, Bangalore, India; Adobe Research, San Jose, CA, USA; University of California, San Diego, CA, USA|Search engines and recommendation systems attempt to continually improve the quality of the experience they afford to their users. Refining the ranker that produces the lists displayed in response to user requests is an important component of this process. A common practice is for the service providers to make changes (e.g. new ranking features, different ranking models) and A/B test them on a fraction of their users to establish the value of the change. An alternative approach estimates the effectiveness of the proposed changes offline, utilising previously collected clickthrough data on the old ranker to posit what the user behaviour on ranked lists produced by the new ranker would have been. A majority of offline evaluation approaches invoke the well studied inverse propensity weighting to adjust for biases inherent in logged data. In this paper, we propose the use of parametric estimates for these propensities. Specifically, by leveraging well known learning-to-rank methods as subroutines, we show how accurate offline evaluation can be achieved when the new rankings to be evaluated differ from the logged ones.|搜索引擎和推荐系统试图不断提高它们为用户提供的体验的质量。优化生成响应用户请求的列表的排名是这个过程的一个重要组成部分。一个常见的做法是，服务提供商进行更改(例如，新的排名功能，不同的排名模型)和 A/B 测试他们的一小部分用户，以建立变化的价值。另一种方法是利用先前收集到的老排名者的点击数据，来估计新排名者生成的排名表上的用户行为的有效性。大多数离线评估方法都会调用经过充分研究的倾向性反向加权来调整测井数据中固有的偏差。在本文中，我们提出了这些倾向的参数估计的使用。具体来说，通过利用众所周知的学习排名方法作为子程序，我们展示了当评估的新排名与记录的排名不同时，如何实现准确的离线评估。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Offline+Evaluation+of+Ranked+Lists+using+Parametric+Estimation+of+Propensities)|0|
|[CAPTOR: A Crowd-Aware Pre-Travel Recommender System for Out-of-Town Users](https://doi.org/10.1145/3477495.3531949)|Haoran Xin, Xinjiang Lu, Nengjun Zhu, Tong Xu, Dejing Dou, Hui Xiong|Shanghai University, Shanghai, China; University of Science and Technology of China, Hefei, China; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Baidu Research, Beijing, China|Pre-travel out-of-town recommendation aims to recommend Point-of-Interests (POIs) to the users who plan to travel out of their hometown in the near future yet have not decided where to go, i.e., their destination regions and POIs both remain unknown. It is a non-trivial task since the searching space is vast, which may lead to distinct travel experiences in different out-of-town regions and eventually confuse decision-making. Besides, users' out-of-town travel behaviors are affected not only by their personalized preferences but heavily by others' travel behaviors. To this end, we propose a Crowd-Aware Pre-Travel Out-of-town Recommendation framework (CAPTOR) consisting of two major modules: spatial-affined conditional random field (SA-CRF) and crowd behavior memory network (CBMN). Specifically, SA-CRF captures the spatial affinity among POIs while preserving the inherent information of POIs. Then, CBMN is proposed to maintain the crowd travel behaviors w.r.t. each region through three affiliated blocks reading and writing the memory adaptively. We devise the elaborated metric space with a dynamic mapping mechanism, where the users and POIs are distinguishable both inherently and geographically. Extensive experiments on two real-world nationwide datasets validate the effectiveness of CAPTOR against the pre-travel out-of-town recommendation task.|旅行前出城推荐的目的是向那些计划在不久的将来离开家乡但还没有决定去哪里旅行的用户推荐他们的兴趣点，也就是说，他们的目的地和兴趣点都是未知的。由于搜索空间巨大，这是一个非常重要的任务，可能会导致在不同的城外地区有不同的旅行体验，并最终混淆决策。此外，用户的出城旅游行为不仅受到个人偏好的影响，还受到他人旅游行为的影响。为此，我们提出了一个基于人群感知的预先出城推荐框架(CAPTOR) ，该框架由两个主要模块组成: 空间仿真条件随机域(SA-CRF)和人群行为记忆网络(cBMN)。特别地，SA-CRF 捕获 POI 之间的空间亲和性，同时保留 POI 的固有信息。然后，提出了通过三个附属块自适应地读写记忆来维持每个区域的人群出行行为。我们使用动态映射机制设计了详细的度量空间，其中用户和 POI 在本质上和地理上都是可以区分的。在两个真实世界的全国性数据集上进行了大量的实验，验证了 CAPTOR 对于出城前的推荐任务的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CAPTOR:+A+Crowd-Aware+Pre-Travel+Recommender+System+for+Out-of-Town+Users)|0|
|[Unify Local and Global Information for Top-N Recommendation](https://doi.org/10.1145/3477495.3532070)|Xiaoming Liu, Shaocong Wu, Zhaohan Zhang, Chao Shen||Knowledge graph (KG), integrating complex information and containing rich semantics, is widely considered as side information to enhance the recommendation systems. However, most of the existing KG-based methods concentrate on encoding the structural information in the graph, without utilizing the collaborative signals in user-item interaction data, which are important for understanding user preferences. Therefore, the representations learned by these models are insufficient for representing semantic information of users and items in the recommendation environment. The combination of both kinds of data provides a good chance to solve this problem, but it faces the following challenges: i) the inner correlations in user-item interaction data are difficult to capture from one side of the user or item; ii) capturing the knowledge associations on the whole KG would introduce noises and variously influence the recommendation results; iii) the semantic gap between both kinds of data is hard to alleviate. To tackle this research gap, we propose a novel duet representation learning framework named KADM to fuse local information (user-item interaction data) and global information (external knowledge graph) for the top-N recommendation, which is composed of two separate sub-models. One learns the local representations by discovering the inner correlations in local information with a knowledge-aware co-attention mechanism, and another learns the global representations by encoding the knowledge associations in global information with a relation-aware attention network. The two sub-models are jointly trained as part of the semantic fusion network to compute the user preferences, which discriminates the contribution of the two sub-models under the special context. We conduct experiments on two real-world datasets, and the evaluations show that KADM significantly outperforms state-of-art methods. Further ablation studies confirm that the duet architecture performs significantly better than either sub-model on the recommendation tasks.|知识图集成了复杂的信息，包含丰富的语义，被广泛认为是增强推荐系统的边信息。然而，现有的基于 KG 的方法大多集中于对图中的结构信息进行编码，而没有利用用户交互数据中的协作信号，这对于理解用户偏好非常重要。因此，这些模型所学到的表示方法不足以表示推荐环境中用户和项目的语义信息。这两种数据的结合为解决这一问题提供了很好的机会，但它面临着以下挑战: 1)用户项目交互数据的内部相关性难以从用户或项目的一侧获取; 2)捕捉整个 KG 的知识关联会引入噪声并对推荐结果产生各种影响; 3)两种数据之间的语义差异难以缓解。为了解决这一问题，本文提出了一种新的二元表示学习框架 KADM，它融合了顶层 N 推荐的局部信息(用户项目交互数据)和全局信息(外部知识图) ，该框架由两个独立的子模型组成。一种是通过知识感知共注意机制发现局部信息的内在相关性来学习局部表征，另一种是通过关系感知注意网络对全局信息中的知识关联进行编码来学习全局表征。将这两个子模型作为语义融合网络的一部分进行联合训练，以计算用户偏好，从而区分两个子模型在特定语境下的贡献。我们在两个真实世界的数据集上进行了实验，结果表明 KADM 的性能明显优于最先进的方法。进一步的消融研究证实，二重奏架构在推荐任务上的表现明显优于任何一个子模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unify+Local+and+Global+Information+for+Top-N+Recommendation)|0|
|[Deployable and Continuable Meta-learning-Based Recommender System with Fast User-Incremental Updates](https://doi.org/10.1145/3477495.3531964)|Renchu Guan, Haoyu Pang, Fausto Giunchiglia, Ximing Li, Xuefeng Yang, Xiaoyue Feng|Tencent, Shenzhen, China; Jilin University, Changchun, China; University of Trento, Trento, Italy|User cold-start is a major challenge in building personalized recommender systems. Due to the lack of sufficient interactions, it is difficult to effectively model new users. One of the main solutions is to obtain an initial model through meta-learning (mainly gradient-based methods) and adapt it to new users with a few steps of gradient descent. Although these methods have achieved remarkable performance, they are still far from being usable in real-world applications due to their high-demand data processing, heavy computational burden, and inability to perform effective user-incremental update. In this paper, we propose a d eployable and c ontinuable m eta-learning-based r ecommendation (DCMR) approach, which can achieve fast user-incremental updating with task replay and first-order gradient descent. Specifically, we introduce a dual-constrained task sampler, distillation-based loss functions, and an adaptive controller in this framework to balance the trade-off between stability and plasticity in updating. In summary, DCMR can be updated while serving new users; in other words, it learns continuously and rapidly from a sequential user stream and is able to make recommendations at any time. The extensive experiments conducted on three benchmark datasets illustrate the superiority of our model.|用户冷启动是构建个性化推荐系统的主要挑战。由于缺乏足够的交互，很难对新用户进行有效的建模。其中一个主要的解决方案是通过元学习(主要是基于梯度的方法)获得一个初始模型，并通过几个步骤使其适应新用户的梯度下降法。虽然这些方法已经取得了显著的性能，但是由于其高需求的数据处理、沉重的计算负担以及不能执行有效的用户增量更新，它们在实际应用中仍然远远不能使用。在本文中，我们提出了一种可部署和可持续的基于元学习的 r 推荐(dCMR)方法，它可以通过任务重播和一阶梯度下降法实现快速的用户增量更新。具体来说，我们引入了一个双约束任务采样器，基于蒸馏的损失函数，以及在这个框架中的一个自适应控制器，以平衡稳定性和可塑性之间的权衡在更新。总之，DCMR 可以在为新用户提供服务的同时进行更新; 换句话说，它可以从连续的用户流中不断快速地学习，并且能够在任何时候提出建议。在三个基准数据集上进行的大量实验表明了该模型的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deployable+and+Continuable+Meta-learning-Based+Recommender+System+with+Fast+User-Incremental+Updates)|0|
|[Bias Mitigation for Toxicity Detection via Sequential Decisions](https://doi.org/10.1145/3477495.3531945)|Lu Cheng, Ahmadreza Mosallanezhad, Yasin N. Silva, Deborah L. Hall, Huan Liu|Arizona State University, Glendale, AZ, USA; Loyola University Chicago, Chicago, IL, USA; Arizona State University, Tempe, AZ, USA|Increased social media use has contributed to the greater prevalence of abusive, rude, and offensive textual comments. Machine learning models have been developed to detect toxic comments online, yet these models tend to show biases against users with marginalized or minority identities (e.g., females and African Americans). Established research in debiasing toxicity classifiers often (1) takes a static or batch approach, assuming that all information is available and then making a one-time decision; and (2) uses a generic strategy to mitigate different biases (e.g., gender and racial biases) that assumes the biases are independent of one another. However, in real scenarios, the input typically arrives as a sequence of comments/words over time instead of all at once. Thus, decisions based on partial information must be made while additional input is arriving. Moreover, social bias is complex by nature. Each type of bias is defined within its unique context, which, consistent with intersectionality theory within the social sciences, might be correlated with the contexts of other forms of bias. In this work, we consider debiasing toxicity detection as a sequential decision-making process where different biases can be interdependent. In particular, we study debiasing toxicity detection with two aims: (1) to examine whether different biases tend to correlate with each other; and (2) to investigate how to jointly mitigate these correlated biases in an interactive manner to minimize the total amount of bias. At the core of our approach is a framework built upon theories of sequential Markov Decision Processes that seeks to maximize the prediction accuracy and minimize the bias measures tailored to individual biases. Evaluations on two benchmark datasets empirically validate the hypothesis that biases tend to be correlated and corroborate the effectiveness of the proposed sequential debiasing strategy.|越来越多的社交媒体使用导致了辱骂、粗鲁和冒犯性的文字评论更加普遍。机器学习模型已经被开发用来检测网上的有毒评论，然而这些模型往往显示出对边缘化或少数族裔身份的用户(例如，女性和非裔美国人)的偏见。已建立的减少毒性分类器的研究通常(1)采用静态或批量方法，假设所有信息都可用，然后做出一次性决策; (2)使用通用策略来减轻假定偏见彼此独立的不同偏见(例如性别和种族偏见)。然而，在真实的场景中，输入通常是以注释/单词序列的形式随着时间的推移而到达，而不是一次性全部到达。因此，当额外的输入到达时，必须根据部分信息做出决策。此外，社会偏见本质上是复杂的。每种类型的偏见都是在其独特的背景下定义的，这与社会科学中的交叉性理论一致，可能与其他形式的偏见的背景相关。在这项工作中，我们认为去偏毒性检测是一个连续的决策过程中，不同的偏见可以相互依赖。具体而言，我们研究去偏毒性检测有两个目的: (1)检查不同的偏倚是否倾向于相互关联; (2)研究如何以交互方式共同减轻这些相关偏倚，以最小化偏倚总量。我们的方法的核心是一个建立在序贯马尔可夫决策过程理论基础上的框架，该框架寻求最大限度地提高预测的准确性，最小化针对个别偏差的偏差测量。对两个基准数据集的评估经验验证了偏差倾向于相关的假设，并证实了所提出的序贯去偏策略的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bias+Mitigation+for+Toxicity+Detection+via+Sequential+Decisions)|0|
|[Regulating Group Exposure for Item Providers in Recommendation](https://doi.org/10.1145/3477495.3531760)|Mirko Marras, Ludovico Boratto, Guilherme Ramos, Gianni Fenu|University of Cagliari, Cagliari, Italy; University of Lisbon, Lisbon, Portugal|Engaging all content providers, including newcomers or minority demographic groups, is crucial for online platforms to keep growing and working. Hence, while building recommendation services, the interests of those providers should be valued. In this paper, we consider providers as grouped based on a common characteristic in settings in which certain provider groups have low representation of items in the catalog and, thus, in the user interactions. Then, we envision a scenario wherein platform owners seek to control the degree of exposure to such groups in the recommendation process. To support this scenario, we rely on disparate exposure measures that characterize the gap between the share of recommendations given to groups and the target level of exposure pursued by the platform owners. We then propose a re-ranking procedure that ensures desired levels of exposure are met. Experiments show that, while supporting certain groups of providers by rendering them with the target exposure, beyond-accuracy objectives experience significant gains with negligible impact in recommendation utility.|吸引所有内容提供商，包括新来者或少数族裔群体，对于在线平台保持增长和运作至关重要。因此，在构建推荐服务时，应该重视这些提供者的利益。在本文中，我们认为提供程序是基于一个共同特征进行分组的，在这种情况下，某些提供程序组在目录中的项表示较低，因此在用户交互中也是如此。然后，我们设想一个场景，其中平台所有者寻求控制在推荐过程中暴露于这些群体的程度。为了支持这一设想，我们依靠不同的曝光度量标准，这些标准体现了给予群体的建议份额与平台所有者追求的曝光度目标水平之间的差距。然后，我们提出了一个重新排序的程序，以确保所需的暴露水平得到满足。实验表明，虽然支持某些群体的供应商，使他们的目标暴露，超过准确性的目标经历了显着的收益，对推荐效用的影响可以忽略不计。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Regulating+Group+Exposure+for+Item+Providers+in+Recommendation)|0|
|[IPR: Interaction-level Preference Ranking for Explicit feedback](https://doi.org/10.1145/3477495.3531777)|ShihYang Liu, HsienHao Chen, ChihMing Chen, MingFeng Tsai, ChuanJu Wang|National Chengchi University, Academia Sinica, Taipei, Taiwan Roc; Academia Sinica, Taipei, Taiwan Roc; National Chengchi University, Taipei, Taiwan Roc|Explicit feedback---user input regarding their interest in an item---is the most helpful information for recommendation as it comes directly from the user and shows their direct interest in the item. Most approaches either treat the recommendation given such feedback as a typical regression problem or regard such data as implicit and then directly adopt approaches for implicit feedback; both methods, however,tend to yield unsatisfactory performance in top-k recommendation. In this paper, we propose interaction-level preference ranking(IPR), a novel pairwise ranking embedding learning approach to better utilize explicit feedback for recommendation. Experiments conducted on three real-world datasets show that IPR yields the best results compared to six strong baselines.|明确的反馈——用户关于他们对某个项目感兴趣的输入——是对推荐最有帮助的信息，因为它直接来自用户，并显示了他们对该项目的直接兴趣。大多数方法要么将给出的这种反馈视为典型的回归问题，要么将这种数据视为隐式的，然后直接采用隐式反馈的方法; 然而，这两种方法在 top-k 推荐中的表现往往都不令人满意。在本文中，我们提出了交互层次偏好排序(IPR) ，这是一种新的嵌入学习的成对排序方法，以更好地利用显式反馈进行推荐。在三个实际数据集上进行的实验表明，与六个强基线相比，IPR 产生的结果最好。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IPR:+Interaction-level+Preference+Ranking+for+Explicit+feedback)|0|
|[MP2: A Momentum Contrast Approach for Recommendation with Pointwise and Pairwise Learning](https://doi.org/10.1145/3477495.3531813)|Menghan Wang, Yuchen Guo, Zhenqi Zhao, Guangzheng Hu, Yuming Shen, Mingming Gong, Philip H. S. Torr|The University of Melbourne, Melbourne, VIC, Australia; University of Oxford, London, United Kingdom; eBay Inc., Shanghai, China; Tencent Inc., Shanghai, China|Binary pointwise labels (aka implicit feedback) are heavily leveraged by deep learning based recommendation algorithms nowadays. In this paper we discuss the limited expressiveness of these labels may fail to accommodate varying degrees of user preference, and thus lead to conflicts during model training, which we call annotation bias. To solve this issue, we find the soft-labeling property of pairwise labels could be utilized to alleviate the bias of pointwise labels. To this end, we propose a momentum contrast framework (\method ) that combines pointwise and pairwise learning for recommendation. \method has a three-tower network structure: one user network and two item networks. The two item networks are used for computing pointwise and pairwise loss respectively. To alleviate the influence of the annotation bias, we perform a momentum update to ensure a consistent item representation. Extensive experiments on real-world datasets demonstrate the superiority of our method against state-of-the-art recommendation algorithms.|二进制点态标签(即隐式反馈)是当今基于深度学习的推荐算法的重要组成部分。在本文中，我们讨论了这些标签的有限表达可能无法适应不同程度的用户偏好，从而导致模型训练过程中的冲突，我们称之为注释偏差。为了解决这个问题，我们发现可以利用成对标签的软标签特性来缓解点态标签的偏差。为此，我们提出了一个动量对比框架(方法) ，结合点态和成对学习的推荐。方法具有三塔网络结构: 一个用户网络和两个项目网络。两项网络分别用于计算逐点损失和成对损失。为了减轻注释偏差的影响，我们进行动量更新以确保项目表示的一致性。在真实世界数据集上的大量实验证明了我们的方法对最先进的推荐算法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MP2:+A+Momentum+Contrast+Approach+for+Recommendation+with+Pointwise+and+Pairwise+Learning)|0|
|[Deep Page-Level Interest Network in Reinforcement Learning for Ads Allocation](https://doi.org/10.1145/3477495.3531847)|Guogang Liao, Xiaowen Shi, Ze Wang, Xiaoxu Wu, Chuheng Zhang, Yongkang Wang, Xingxing Wang, Dong Wang|Meituan, Beijing, China; Tsinghua University, Beijing, China|A mixed list of ads and organic items is usually displayed in feed and how to allocate the limited slots to maximize the overall revenue is a key problem. Meanwhile, user behavior modeling is essential in recommendation and advertising (e.g., CTR prediction and ads allocation). Most previous works only model point-level positive feedback (i.e., click), which neglect the page-level information of feedback and other types of feedback. To this end, we propose Deep Page-level Interest Network (DPIN) to model the page-level user preference and exploit multiple types of feedback. Specifically, we introduce four different types of page-level feedback, and capture user preference for item arrangement under different receptive fields through the multi-channel interaction module. Through extensive offline and online experiments on Meituan food delivery platform, we demonstrate that DPIN can effectively model the page-level user preference and increase the revenue.|一个广告和有机项目的混合列表通常显示在饲料和如何分配有限的插槽，以最大限度地提高总收入是一个关键问题。同时，用户行为建模对于推荐和广告(例如，点击率预测和广告分配)至关重要。大多数以前的作品只是模拟点级别的正反馈(例如，点击) ，而忽略了反馈和其他类型的反馈的页级信息。为此，我们提出了深层页面级兴趣网络(DPIN)来建模页面级用户偏好，并利用多种类型的反馈。具体来说，我们引入了四种不同类型的页面级反馈，并通过多通道交互模块捕捉用户对不同接收域下项目排列的偏好。通过在美团外卖平台上进行的大量线下和线上实验，我们证明了 DPIN 可以有效地模拟页面级别的用户偏好，并增加收入。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Page-Level+Interest+Network+in+Reinforcement+Learning+for+Ads+Allocation)|0|
|[Improving Micro-video Recommendation via Contrastive Multiple Interests](https://doi.org/10.1145/3477495.3531861)|Beibei Li, Beihong Jin, Jiageng Song, Yisong Yu, Yiyuan Zheng, Wei Zhou|Institute of Software, Chinese Academy of Sciences & University of Chinese Academy of Sciences, Beijing, China; MX Media Co., Ltd., Singapore, Singapore|With the rapid increase of micro-video creators and viewers, how to make personalized recommendations from a large number of candidates to viewers begins to attract more and more attention. However, existing micro-video recommendation models rely on expensive multi-modal information and learn an overall interest embedding that cannot reflect the user's multiple interests in micro-videos. Recently, contrastive learning provides a new opportunity for refining the existing recommendation techniques. Therefore, in this paper, we propose to extract contrastive multi-interests and devise a micro-video recommendation model CMI. Specifically, CMI learns multiple interest embeddings for each user from his/her historical interaction sequence, in which the implicit orthogonal micro-video categories are used to decouple multiple user interests. Moreover, it establishes the contrastive multi-interest loss to improve the robustness of interest embeddings and the performance of recommendations. The results of experiments on two micro-video datasets demonstrate that CMI achieves state-of-the-art performance over existing baselines.|随着微视频制作者和观众的迅速增多，如何从大量的候选人中向观众提供个性化的推荐，开始引起越来越多的关注。然而，现有的微视频推荐模型依赖于昂贵的多模态信息，学习的总体兴趣嵌入不能反映用户在微视频中的多重兴趣。近年来，对比学习为完善现有的推荐技术提供了一个新的机会。因此，本文提出提取对比多兴趣并设计一个微视频推荐模型 CMI。具体来说，CMI 从每个用户的历史交互序列中学习多个兴趣嵌入，其中使用隐式正交微视频类别来解耦多个用户兴趣。此外，本文还建立了对比的多利益损失模型，以提高利益嵌入的鲁棒性和建议的执行效率。在两个微视频数据集上的实验结果表明，CMI 在现有的基线上取得了最好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Micro-video+Recommendation+via+Contrastive+Multiple+Interests)|0|
|[Can Users Predict Relative Query Effectiveness?](https://doi.org/10.1145/3477495.3531893)|Oleg Zendel, Melika P. Ebrahim, J. Shane Culpepper, Alistair Moffat, Falk Scholer|The University of Melbourne, Melbourne, VIC, Australia; RMIT University, Melbourne, VIC, Australia|Any given information need can be expressed via a wide range of possible queries. Recent work with such query variations has demonstrated that different queries can fetch notably divergent sets of documents, even when the queries have identical intents and superficial similarity. That is, different users might receive SERPs of quite different effectiveness for the same information need. That observation then raises an interesting question: do users have a sense of how useful any given query will be? Can they anticipate the effectiveness of alternative queries for the same retrieval need? To explore that question we designed and carried out a crowd-sourced user study in which we asked subjects to consider an information need statement expressed as a backstory, and then provide their opinions as to the relative usefulness of a set of queries ostensibly addressing that objective. We solicited opinions using two different interfaces: one that collected absolute ratings of queries, and one that required that the subjects place a set of queries into "order". We found that crowd workers are reasonably consistent in their estimates of how effective queries are likely to be, and also that their estimates correlate positively with actual system performance.|任何给定的信息需求都可以通过各种可能的查询来表示。最近对这种查询变体的研究表明，不同的查询可以获取明显不同的文档集，即使查询具有相同的意图和表面上的相似性。也就是说，对于相同的信息需求，不同的用户可能会收到效果完全不同的 SERP。这种观察提出了一个有趣的问题: 用户是否知道任何给定的查询有多大用处？他们能够预测相同检索需求的替代查询的有效性吗？为了探索这个问题，我们设计并进行了一个众包用户研究，在这个研究中，我们要求受试者考虑一个表达为背景故事的信息需求陈述，然后提供他们对一组表面上针对该目标的查询的相对有用性的意见。我们使用两种不同的界面来征求意见: 一种是收集查询的绝对评分，另一种是要求被试将一组查询按顺序排列。我们发现，人群工作者对查询可能的有效性的估计是相当一致的，而且他们的估计与实际系统性能正相关。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+Users+Predict+Relative+Query+Effectiveness?)|0|
|[Is Non-IID Data a Threat in Federated Online Learning to Rank?](https://doi.org/10.1145/3477495.3531709)|Shuyi Wang, Guido Zuccon|The University of Queensland, Brisbane, QLD, Australia|In this perspective paper we study the effect of non independent and identically distributed (non-IID) data on federated online learning to rank (FOLTR) and chart directions for future work in this new and largely unexplored research area of Information Retrieval. In the FOLTR process, clients participate in a federation to jointly create an effective ranker from the implicit click signal originating in each client, without the need to share data (documents, queries, clicks). A well-known factor that affects the performance of federated learning systems, and that poses serious challenges to these approaches, is that there may be some type of bias in the way data is distributed across clients. While FOLTR systems are on their own rights a type of federated learning system, the presence and effect of non-IID data in FOLTR has not been studied. To this aim, we first enumerate possible data distribution settings that may showcase data bias across clients and thus give rise to the non-IID problem. Then, we study the impact of each setting on the performance of the current state-of-the-art FOLTR approach, the Federated Pairwise Differentiable Gradient Descent (FPDGD), and we highlight which data distributions may pose a problem for FOLTR methods. We also explore how common approaches proposed in the federated learning literature address non-IID issues in FOLTR. This allows us to unveil new research gaps that, we argue, future research in FOLTR should consider.|在这篇前瞻性的论文中，我们研究了非独立和同分布(非 IID)数据对联邦在线学习排名(FOLTR)的影响，以及未来工作的图表方向，这是一个新的、很大程度上尚未探索的信息检索研究领域。在 FOLTR 过程中，客户端参与到一个联合中，从每个客户端发出的隐式点击信号中共同创建一个有效的排名，而不需要共享数据(文档、查询、点击)。影响联邦学习系统性能的一个众所周知的因素是，数据在客户端之间的分布方式可能存在某种偏差，这对这些方法提出了严峻的挑战。虽然 FOLTR 系统本身就是一种联邦学习系统，但是对于 FOLTR 中非 IID 数据的存在和影响还没有进行研究。为此，我们首先列举可能的数据分布设置，这些设置可能显示客户端之间的数据偏差，从而引起非 IID 问题。然后，我们研究了每种设置对当前最先进的 FOLTR 方法——联邦成对可微分梯度下降法(fPDGD)——性能的影响，并强调了哪些数据分布可能会给 FOLTR 方法带来问题。我们还探讨了联合学习文献中提出的常用方法如何解决 FOLTR 中的非 IID 问题。这使我们能够揭示新的研究差距，我们认为，在 FOLTR 的未来研究应该考虑。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Is+Non-IID+Data+a+Threat+in+Federated+Online+Learning+to+Rank?)|0|
|[On Natural Language User Profiles for Transparent and Scrutable Recommendation](https://doi.org/10.1145/3477495.3531873)|Filip Radlinski, Krisztian Balog, Fernando Diaz, Lucas Dixon, Ben Wedin|Google, London, United Kingdom; Google, Paris, France; Google, Cambridge, MA, USA; Google, Montreal, Canada; Google, Stavanger, Norway|Natural interaction with recommendation and personalized search systems has received tremendous attention in recent years. We focus on the challenge of supporting people's understanding and control of these systems and explore a fundamentally new way of thinking about representation of knowledge in recommendation and personalization systems. Specifically, we argue that it may be both desirable and possible for algorithms that use natural language representations of users' preferences to be developed. We make the case that this could provide significantly greater transparency, as well as affordances for practical actionable interrogation of, and control over, recommendations. Moreover, we argue that such an approach, if successfully applied, may enable a major step towards systems that rely less on noisy implicit observations while increasing portability of knowledge of one's interests.|近年来，与推荐系统和个性化检索系统的自然交互受到了极大的关注。我们重点关注支持人们理解和控制这些系统的挑战，并探索一种在推荐和个性化系统中表示知识的全新思维方式。具体来说，我们认为开发使用用户偏好的自然语言表示的算法是可取的，也是可能的。我们认为，这可以提供更大的透明度，以及提供实际可行的审讯和控制，建议。此外，我们认为，这种方法，如果成功地应用，可能使一个重大的步骤，系统的依赖噪音较少的隐含观察，同时增加了一个人的兴趣知识的可移植性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Natural+Language+User+Profiles+for+Transparent+and+Scrutable+Recommendation)|0|
|[Retrieval-Enhanced Machine Learning](https://doi.org/10.1145/3477495.3531722)|Hamed Zamani, Fernando Diaz, Mostafa Dehghani, Donald Metzler, Michael Bendersky|Google Research, Amsterdam, Netherlands; Google Research, Montréal, PQ, Canada; Google Research, Mountain View, CA, USA; University of Massachusetts Amherst, Amherst, MA, USA|Although information access systems have long supportedpeople in accomplishing a wide range of tasks, we propose broadening the scope of users of information access systems to include task-driven machines, such as machine learning models. In this way, the core principles of indexing, representation, retrieval, and ranking can be applied and extended to substantially improve model generalization, scalability, robustness, and interpretability. We describe a generic retrieval-enhanced machine learning (REML) framework, which includes a number of existing models as special cases. REML challenges information retrieval conventions, presenting opportunities for novel advances in core areas, including optimization. The REML research agenda lays a foundation for a new style of information access research and paves a path towards advancing machine learning and artificial intelligence.|尽管信息访问系统长期以来一直支持人们完成各种各样的任务，但我们建议扩大信息访问系统的用户范围，以包括任务驱动的机器，如机器学习模型。通过这种方式，可以应用和扩展索引、表示、检索和排序的核心原则，从而大大提高模型泛化、可伸缩性、健壮性和可解释性。我们描述了一个通用的检索增强机器学习(REML)框架，其中包括一些现有的模型作为特殊情况。REML 挑战了信息检索惯例，为核心领域的新进展提供了机会，包括优化。REML 研究议程为新型的信息获取研究奠定了基础，为机器学习和人工智能的发展铺平了道路。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Retrieval-Enhanced+Machine+Learning)|0|
|[Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval](https://doi.org/10.1145/3477495.3531736)|Dingkun Long, Qiong Gao, Kuan Zou, Guangwei Xu, Pengjun Xie, Ruijie Guo, Jian Xu, Guanjun Jiang, Luxi Xing, Ping Yang|Alibaba Group, Hangzhou, China|Passage retrieval is a fundamental task in information retrieval (IR) research, which has drawn much attention recently. In the English field, the availability of large-scale annotated dataset (e.g, MS MARCO) and the emergence of deep pre-trained language models (e.g, BERT) has resulted in a substantial improvement of existing passage retrieval systems. However, in the Chinese field, especially for specific domains, passage retrieval systems are still immature due to quality-annotated dataset being limited by scale. Therefore, in this paper, we present a novel multi-domain Chinese dataset for passage retrieval (Multi-CPR). The dataset is collected from three different domains, including E-commerce, Entertainment video and Medical. Each dataset contains millions of passages and a certain amount of human annotated query-passage related pairs. We implement various representative passage retrieval methods as baselines. We find that the performance of retrieval models trained on dataset from general domain will inevitably decrease on specific domain. Nevertheless, a passage retrieval system built on in-domain annotated dataset can achieve significant improvement, which indeed demonstrates the necessity of domain labeled data for further optimization. We hope the release of the Multi-CPR dataset could benchmark Chinese passage retrieval task in specific domain and also make advances for future studies.|短文检索是信息检索研究中的一项基础性工作，近年来备受关注。在英语领域，大规模注释数据集(例如 MS MARCO)的可用性和深度预训练语言模型(例如 BERT)的出现使现有的文章检索系统得到了实质性的改进。然而，在中文领域，特别是在特定领域，由于质量注释数据集受到规模的限制，文章检索系统还不成熟。因此，本文提出了一种新的多领域中文文本检索数据集(Multi-CPR)。该数据集收集自三个不同的领域，包括电子商务，娱乐视频和医疗。每个数据集包含数百万个段落和一定数量的人工注释的查询-段落相关对。我们实现了各种具有代表性的文章检索方法作为基线。研究发现，对一般领域数据集训练的检索模型在特定领域的性能不可避免地会下降。然而，建立在域内注释数据集上的文章检索系统可以取得显著的改进，这确实说明了域标记数据进一步优化的必要性。我们希望通过多 CPR 数据集的发布，能够为特定领域的中文文章检索任务提供基准，并为今后的研究提供参考。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-CPR:+A+Multi+Domain+Chinese+Dataset+for+Passage+Retrieval)|0|
|[MIMICS-Duo: Offline & Online Evaluation of Search Clarification](https://doi.org/10.1145/3477495.3531750)|Leila Tavakoli, Johanne R. Trippas, Hamed Zamani, Falk Scholer, Mark Sanderson|University of Melbourne, Melbourne, VIC, Australia; RMIT University, Melbourne, VIC, Australia; University of Massachusetts Amherst, Amherst, MA, USA|Asking clarification questions is an active area of research; however, resources for training and evaluating search clarification methods are not sufficient. To address this issue, we describe MIMICS-Duo, a new freely available dataset of 306 search queries with multiple clarifications (a total of 1,034 query-clarification pairs). MIMICS-Duo contains fine-grained annotations on clarification questions and their candidate answers and enhances the existing MIMICS datasets by enabling multi-dimensional evaluation of search clarification methods, including online and offline evaluation. We conduct extensive analysis to demonstrate the relationship between offline and online search clarification datasets and outline several research directions enabled by MIMICS-Duo. We believe that this resource will help researchers better understand clarification in search.|提出澄清问题是一个活跃的研究领域，然而，培训和评估搜索澄清方法的资源是不够的。为了解决这个问题，我们描述了 MIMICS-Duo，这是一个新的免费数据集，包含306个具有多重澄清的搜索查询(总共1,034个查询-澄清对)。MIMICS-Duo 包含关于澄清问题及其候选答案的细粒度注释，并通过支持搜索澄清方法的多维评估(包括在线和离线评估)来增强现有的 MIMICS 数据集。我们进行了广泛的分析，以证明离线和在线搜索澄清数据集之间的关系，并概述了由 MIMICS-Duo 实现的几个研究方向。我们相信，这一资源将有助于研究人员更好地理解在搜索澄清。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MIMICS-Duo:+Offline+&+Online+Evaluation+of+Search+Clarification)|0|
|[A Common Framework for Exploring Document-at-a-Time and Score-at-a-Time Retrieval Methods](https://doi.org/10.1145/3477495.3531657)|Andrew Trotman, Joel Mackenzie, Pradeesh Parameswaran, Jimmy Lin|The University of Queensland, Brisbane, QLD, Australia; University of Otago, Dunedin, New Zealand; University of Waterloo, Waterloo, Canada|Document-at-a-time (DaaT) and score-at-a-time (SaaT) query evaluation techniques are different approaches to top-k retrieval with inverted indexes. While modern systems are dominated by DaaT, the academic literature has seen decades of debate about the merits of each. Recently, there has been renewed interest in SaaT methods for learned sparse lexical models, where studies have shown that transformers generate "wacky weights" that appear to reduce opportunities for optimizations in DaaT methods. However, researchers currently lack an easy-to-use SaaT system to support further exploration. This is the gap that our work fills. Starting with a modern SaaT system (JASS), we built Python bindings in order to integrate into the DaaT Pyserini IR toolkit (Lucene). The result is a common frontend to both a DaaT and a SaaT system. We demonstrate how recent experiments with a wide range of learned sparse lexical models can be easily reproduced. Our contribution is a framework that enables future research comparing DaaT and SaaT methods in the context of modern neural retrieval models.|一次文档(DaaT)和一次得分(SaaT)查询评估技术是两种不同的方法，用于带有倒排索引的 top-k 检索。虽然现代系统是由 DaaT 主导的，但学术文献已经对每个系统的优点进行了数十年的争论。最近，人们对学习稀疏词汇模型的 SaaT 方法重新产生了兴趣，研究表明，变压器产生的“古怪的权重”似乎减少了 DaaT 方法优化的机会。然而，研究人员目前缺乏一个易于使用的 SaaT 系统来支持进一步的探索。这是我们的工作填补的空白。从一个现代 SaaT 系统(JASS)开始，我们构建了 Python 绑定，以便集成到 DaaT Pyserini IR 工具包(Lucene)中。其结果是 DaaT 和 SaaT 系统的共同前端。我们证明了最近的实验与广泛的学习稀疏词汇模型可以很容易地再现。我们的贡献是一个框架，使未来的研究比较 DaaT 和 SaaT 方法在现代神经检索模型的背景下。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Common+Framework+for+Exploring+Document-at-a-Time+and+Score-at-a-Time+Retrieval+Methods)|0|
|[BiTe-REx: An Explainable Bilingual Text Retrieval System in the Automotive Domain](https://doi.org/10.1145/3477495.3531665)|Viju Sudhi, Sabine Wehnert, Norbert Michael Homner, Sebastian Ernst, Mark Gonter, Andreas Krug, Ernesto William De Luca|Otto von Guericke University, Magdeburg, Germany; Audi AG, Ingolstadt, Germany|To satiate the comprehensive information need of users, retrieval systems surpassing the boundaries of language are inevitable in the present digital space in the wake of an ever-rising multilingualism. This work presents the first-of-its-kind Bilingual Text Retrieval Explanations (BiTe-REx) aimed at users performing competitor or wage analysis in the automotive domain. BiTe-REx supports users to gather a more comprehensive picture of their query by retrieving results regardless of the query language and enables them to make a more informed decision by exposing how the underlying model judges the relevance of documents. With a user study, we demonstrate statistically significant results on the understandability and helpfulness of the explanations provided by the system.|为了满足用户的综合信息需求，随着多种语言的日益普及，超越语言边界的检索系统在当今数字化空间中是不可避免的。这项工作提出了第一种双语文本检索解释(BiTe-REx) ，旨在用户执行竞争对手或工资分析在汽车领域。BiTe-REx 支持用户通过检索结果(不管查询语言如何)收集更全面的查询信息，并通过揭示底层模型如何判断文档的相关性，使用户能够做出更明智的决策。通过用户研究，我们证明了系统提供的解释的可理解性和有用性的统计学显著结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BiTe-REx:+An+Explainable+Bilingual+Text+Retrieval+System+in+the+Automotive+Domain)|0|
|[Are Taylor's Posts Risky? Evaluating Cumulative Revelations in Online Personal Data: A persona-based tool for evaluating awareness of online risks and harms](https://doi.org/10.1145/3477495.3531659)|Leif Azzopardi, Jo Briggs, Melissa Duheric, Callum Nash, Emma Nicol, Wendy Moncur, Burkhard Schafer||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Are+Taylor's+Posts+Risky?+Evaluating+Cumulative+Revelations+in+Online+Personal+Data:+A+persona-based+tool+for+evaluating+awareness+of+online+risks+and+harms)|0|
|[DDEN: A Heterogeneous Learning-to-Rank Approach with Deep Debiasing Experts Network](https://doi.org/10.1145/3477495.3536320)|Wenchao Xiu, Yiran Wang, Taofeng Xue, Kai Zhang, Qin Zhang, Zhonghuo Wu, Yifan Yang, Gong Zhang|Meituan, Shanghai, China|Learning-to-Rank(LTR) is widely used in many Information Retrieval(IR) scenarios, including web search and Location Based Services(LBS) search. However, most existing LTR techniques mainly focus on homogeneous ranking. Taking QAC in Dianping search as an example, heterogeneous documents including suggested queries (SQ) and Point-of-Interests(POI) need to be ranked and presented to enhance user experience. New challenges are faced when conducting heterogeneous ranking, including inconsistent feature space and more serious position bias caused by distinct representation spaces. Therefore, we propose Deep Debiasing Experts Network (DDEN), a novel heterogeneous LTR approach based on Mixture-of-Experts architecture and gating network, to deal with the inconsistent feature space of documents in ranking system. Furthermore, DDEN mitigates the position bias by adopting adversarial-debiasing framework embedded with heterogeneous LTR techniques. We conduct reproducible experiments on industrial datasets from Dianping, one of the largest local life platforms, and deploy DDEN in online application. Results show that DDEN substantially improves ranking performance in offline evaluation and boost the overall click-through rate in online A/B test by 2.1%.|学习到排名(learning-to-Rank，LTR)广泛应用于许多信息检索场景，包括网络搜索和基于位置的服务(Location Based Services，LBS)搜索。然而，大多数现有的 LTR 技术主要集中在同质排序。以点评搜索中的质量控制(QAC)为例，需要对包括建议查询(SQ)和兴趣点(POI)在内的异构文档进行排序和呈现，以提高用户体验。异构排序面临的新挑战包括不一致的特征空间和不同表示空间引起的更严重的位置偏差。为此，本文提出了一种基于专家混合体系结构和门网络的异构 LTR 方法——深度去偏专家网络(Deep Debioning Expert Network，DDEN) ，用于处理排序系统中文档的不一致特征空间。此外，DDEN 通过采用嵌入异构 LTR 技术的对抗性消偏框架来缓解位置偏差。我们在本地最大的生活平台之一 Dianping 的工业数据集上进行可重复的实验，并在在线应用中部署 DDEN。结果显示，DDEN 大大提高了离线评估的排名表现，并使在线 A/B 测试的整体点进率提高了2.1% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DDEN:+A+Heterogeneous+Learning-to-Rank+Approach+with+Deep+Debiasing+Experts+Network)|0|
|[An Intelligent Advertisement Short Video Production System via Multi-Modal Retrieval](https://doi.org/10.1145/3477495.3536323)|Yanheng Wei, Lianghua Huang, Yanhao Zhang, Yun Zheng, Pan Pan|Alibaba Group, Beijing, China|In its most basic form, advertising video production communicates a message about a product or service to the public. In the age of digital marketing, where the most popular way to connect with audiences is through advertising videos. However, advertising video production is a costly and complicated process from creation, material shooting, editing to the final commercial video. Therefore, producing qualified advertising videos is a capital and talent-intensive task, which poses a huge challenge for start-ups or inexperienced ad creators. paper proposes an intelligent advertising video production system driven by multi-modal retrieval, which only requires the input of descriptive copy. This system can automatically generate scripts, then extract key queries, retrieve related short video materials in the video library, and finally synthesize short advertising videos. The whole process minimizes human input, greatly reduces the threshold for advertising video production and greatly improves output and efficiency. It has a modular design to encourage the study of new multi-modal algorithms, which can be evaluated in batch mode. It can also integrate with a user interface, which allows user studies and data collection in an interactive mode, where the back end can be fully algorithmic or a wizard of oz setup. The proposed system has been fully verified and has broad prospects in the production of short videos for commodity advertisements within Alibaba.|在最基本的形式中，广告视频制作向公众传达了关于产品或服务的信息。在数字营销时代，最流行的与观众联系的方式是通过广告视频。然而，广告视频制作是一个昂贵而复杂的过程，从创作、素材拍摄、编辑到最终的商业视频。因此，制作合格的广告视频是一项资本和人才密集型的任务，这对初创企业或缺乏经验的广告创作者来说是一个巨大的挑战。提出了一种基于多模态检索的智能广告视频制作系统，该系统只需要输入描述性文本。该系统可以自动生成脚本，然后提取关键查询，检索视频库中相关的短视频资料，最后合成广告短视频。整个过程最大限度地减少了人工投入，大大降低了广告视频制作的门槛，大大提高了产量和效率。它采用模块化设计，以鼓励对新的多模态算法的研究，这些算法可以在批处理模式下进行评估。它还可以集成一个用户界面，允许用户研究和数据收集在一个交互模式，其中后端可以完全算法或绿野仙踪设置向导。建议的系统已经全面验证，在阿里巴巴制作商品广告短片方面具有广阔前景。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Intelligent+Advertisement+Short+Video+Production+System+via+Multi-Modal+Retrieval)|0|
|[An Industrial Framework for Cold-Start Recommendation in Zero-Shot Scenarios](https://doi.org/10.1145/3477495.3536332)|Zhaoxin Huan, Gongduo Zhang, Xiaolu Zhang, Jun Zhou, Qintong Wu, Lihong Gu, Jinjie Gu, Yong He, Yue Zhu, Linjian Mo|Ant Group, Hangzhou, China|There exists the cold-start problem in the recommendation systems when observed user-item interactions are insufficient. To alleviate this problem, most existing works aim to learn globally shared prior knowledge across all items and be fast adapted to a new item with few interactions. However, such learning techniques are data demanding and work poorly on new items with no interactions. In this applied paper, we present an industrial framework recently deployed on Alipay to address the item cold-start problem in zero-shot scenarios. The proposed framework provides both efficient and high-quality recommendations for cold items with no log data. Specifically, we formulate the cold-start problem as a zero-shot learning problem and build a highly efficient infrastructure to accomplish online zero-shot recommendations used on large-scale platforms. Extensive offline experiments and online A/B testing demonstrate that the proposed framework has superior performance and recommends cold items to preferred users more effectively than other state-of-the-art methods.|当观察到的用户-项目交互不足时，推荐系统存在冷启动问题。为了缓解这一问题，大多数现有的工作旨在学习全球共享的所有项目的先验知识，并迅速适应一个新的项目，几乎没有互动。然而，这种学习技术对数据的要求很高，而且在没有交互的情况下对新项目的处理效果很差。在这篇应用文章中，我们提出了一个最近部署在支付宝上的产业框架，来解决零射击情景下的项目冷启动问题。拟议的框架为没有日志数据的冷藏物品提供了高效率和高质量的建议。具体来说，我们将冷启动问题描述为一个零拍学习问题，并建立一个高效的基础设施来实现在大规模平台上使用的在线零拍推荐。大量的离线实验和在线 A/B 测试表明，所提出的框架具有优越的性能，比其他最先进的方法更有效地向首选用户推荐冷项。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Industrial+Framework+for+Cold-Start+Recommendation+in+Zero-Shot+Scenarios)|0|
|[What the Actual...Examining User Behaviour in Information Retrieval](https://doi.org/10.1145/3477495.3532687)|George Buchanan, Dana McKay|University of Melbourne, Melbourne, VIC, Australia; RMIT University, Melbourne, VIC, Australia|Conducting studies involving actual users is a recurring challenge in information retrieval. In this tutorial we will address the main strategic and tactical choices for engaging with, designing and executing user studies, considering both evaluation and formative investigation. The tension between reproducibility and ensuring natural user behaviour will be a recurring focus, seeking to help individual researchers make an intentional and well-argued choice for their research. The presenters have over fifty years of combined experience working in interactive information retrieval, and information interaction in general.|进行涉及实际使用者的研究是信息检索的一个反复出现的挑战。在本教程中，我们将讨论参与、设计和执行用户研究的主要战略和战术选择，同时考虑评估和形成性调查。可重复性和确保自然使用者行为之间的紧张关系将是一个反复出现的焦点，目的是帮助个别研究人员为其研究做出有意识和有充分理由的选择。主持人在互动信息检索和一般的信息互动方面有超过五十年的工作经验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=What+the+Actual...Examining+User+Behaviour+in+Information+Retrieval)|0|
|[User-centered Non-factoid Answer Retrieval](https://doi.org/10.1145/3477495.3531689)|Marwah Alaofi|RMIT University, Melbourne, VIC, Australia|In this research, we aim to examine the assumptions made about users when searching for non-factoid answers using search engines. That is, the way they approach non-factoid question-answering tasks, the language they use to express their questions, the variability in their queries and their behavior towards the provided answers. The investigation will also examine the extent to which these neglected factors affect retrieval performance and potentially highlight the importance of building more realistic methodologies and test collections that capture the real nature of this task. Through our preliminary work, we have begun to explore the characteristics of non-factoid question-answering queries and investigate query variability and their impact on modern retrieval models. Our preliminary results demonstrate notable differences between non-factoid questions sampled from a large query log and those used in QA datasets. In addition, our results demonstrate a profound effect of query variability on retrieval consistency, indicating a potential impact on retrieval performance that is worth studying. We highlight the importance of understanding user behaviour while searching for non-factoid answers, specifically the way they behave in response to receiving an answer. This should advance our understanding of the support users require across different types of non-factoid questions and inform the design of interaction models that support learning and encourage exploring.|本研究旨在探讨使用搜寻引擎搜寻非事实性答案时，对使用者所作的假设。也就是说，他们处理非事实性问答任务的方式，他们用来表达他们的问题的语言，他们的查询的可变性和他们对提供的答案的行为。调查还将审查这些被忽视的因素在多大程度上影响检索性能，并可能强调建立更现实的方法和测试收集的重要性，以捕捉这一任务的真实性质。通过我们的初步工作，我们已经开始探索非事实问答查询的特点，并调查查询的可变性及其对现代检索模型的影响。我们的初步结果表明，从大型查询日志中抽样的非事实性问题与 QA 数据集中使用的问题之间存在显著差异。此外，我们的研究结果显示了查询变异性对检索一致性的深刻影响，表明了对检索性能的潜在影响，值得研究。我们强调了理解用户行为的重要性，同时寻找非事实性的答案，特别是他们的行为方式，以回应收到的答案。这将提高我们对用户在不同类型的非事实性问题中需要的支持的理解，并为支持学习和鼓励探索的交互模型的设计提供信息。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=User-centered+Non-factoid+Answer+Retrieval)|0|
|[Intelligent Conversational Agents for Ambient Computing](https://doi.org/10.1145/3477495.3532087)|Ruhi Sarikaya|Amazon, Seattle, WA, USA|We are in the midst of an AI revolution. Three primary disruptive changes set off this revolution: 1) increase in compute power, mobile internet, and advances in deep learning. The next decade is expected to be about the proliferation of Internet-of-Things (IoT) devices and sensors, which will generate exponentially larger amounts of data to reason over and pave the way for ambient computing. This will also give rise to new forms of interaction patterns with these systems. Users will have to interact with these systems under increasingly richer context and in real-time. Conversational AI has a critical role to play in this revolution, but only if it delivers on its promise of enabling natural, frictionless, and personalized interactions in any context the user is in, while hiding the complexity of these systems through ambient intelligence. However, current commercial conversational AI systems are trained primarily with a supervised learning paradigm, which is difficult, if not impossible, to scale by manually annotating data for increasingly complex sets of contextual conditions. Inherent ambiguity in natural language further complicates the problem. We need to devise new forms of learning paradigms and frameworks that will scale to this complexity. In this talk, we present some early steps we are taking with Alexa, Amazon's Conversational AI system, to move from supervised learning to self-learning methods, where the AI relies on customer interactions for supervision in our journey to ambient intelligence.|我们正处于人工智能革命的中期。三个主要的颠覆性变化引发了这场革命: 1)计算能力的提高，移动互联网的发展，以及深度学习的进步。下一个十年预计将是物联网设备和传感器的激增，它们将产生指数级数量的数据来进行推理，并为环境计算铺平道路。这也将产生与这些系统交互模式的新形式。用户将不得不在日益丰富的上下文环境下与这些系统进行实时交互。对话式人工智能在这场革命中扮演着关键的角色，但前提是它能够在用户所处的任何环境中实现自然、无摩擦和个性化的交互，同时通过环境智能隐藏这些系统的复杂性。然而，目前的商业会话人工智能系统主要使用监督式学习范式进行训练，这种范式很难(如果不是不可能的话)通过手动为日益复杂的上下文条件集注释数据来扩展。自然语言中固有的歧义使问题进一步复杂化。我们需要设计新的学习范式和框架，以适应这种复杂性。在本次演讲中，我们将介绍亚马逊的对话式人工智能系统 Alexa 的一些早期步骤，该系统将从监督式学习转向自学习方法，在我们的环境智能过程中，人工智能依赖客户互动进行监督。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Intelligent+Conversational+Agents+for+Ambient+Computing)|0|
|[A Robust Computerized Adaptive Testing Approach in Educational Question Retrieval](https://doi.org/10.1145/3477495.3531928)|Yan Zhuang, Qi Liu, Zhenya Huang, Zhi Li, Binbin Jin, Haoyang Bi, Enhong Chen, Shijin Wang|Anhui Province Key Laboratory of Big Data Analysis and Application, University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence, Hefei, China; Anhui Province Key Laboratory of Big Data Analysis and Application, University of Science and Technology of China & Institute of Artificial Intelligence, Hefei Comprehensive National Science Center & State Key Laboratory of Cognitive Intelligence, Hefei, China; Huawei Cloud Computing Technologies Co., Ltd, Hangzhou, China; State Key Laboratory of Cognitive Intelligence & iFLYTEK AI Research (Central China), iFLYTEK Co., Ltd, Hefei, China|Computerized Adaptive Testing (CAT) is a promising testing mode in personalized online education (e.g., GRE), which aims at measuring student's proficiency accurately and reducing test length. The "adaptive" is reflected in its selection algorithm that can retrieve best-suited questions for student based on his/her estimated proficiency at each test step. Although there are many sophisticated selection algorithms for improving CAT's effectiveness, they are restricted and perturbed by the accuracy of current proficiency estimate, thus lacking robustness. To this end, we investigate a general method to enhance the robustness of existing algorithms by leveraging student's "multi-facet" nature during tests. Specifically, we present a generic optimization criterion Robust Adaptive Testing (RAT) for proficiency estimation via fusing multiple estimates at each step, which maintains a multi-facet description of student's potential proficiency. We further provide theoretical analyses of such estimator's desirable statistical properties: asymptotic unbiasedness, efficiency, and consistency. Extensive experiments on perturbed synthetic data and three real-world datasets show that selection algorithms in our RAT framework are robust and yield substantial improvements.|计算机自适应测试(CAT)是个性化网络教育(如 GRE)中一种很有前途的测试模式，其目的是准确测量学生的水平，减少测试时间。“适应性”反映在其选择算法中，该算法可以根据学生在每个测试步骤中的估计熟练程度为学生检索最适合的问题。虽然有许多复杂的选择算法来提高 CAT 的有效性，但它们都受到当前水平估计精度的限制和干扰，因此缺乏鲁棒性。为此，我们研究了一种通用的方法，以增强现有的算法的健壮性，利用学生的“多方面”的性质在测试。具体来说，我们提出了一个通用的优化标准鲁棒自适应测试(RAT)的水平估计融合多个估计在每一个步骤，它保持了一个学生的潜在水平的多方面的描述。进一步从理论上分析了这类估计量的理想统计性质: 渐近无偏性、有效性和一致性。在扰动合成数据和三个实际数据集上的大量实验表明，我们的 RAT 框架中的选择算法是健壮的，并且产生了实质性的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Robust+Computerized+Adaptive+Testing+Approach+in+Educational+Question+Retrieval)|0|
|[Forest-based Deep Recommender](https://doi.org/10.1145/3477495.3531980)|Chao Feng, Defu Lian, Zheng Liu, Xing Xie, Le Wu, Enhong Chen|Microsoft Research Asia, Beijing, China; University of Science and Technology of China, Hefei, China; Hefei university of Technology, Hefei, China|With the development of deep learning techniques, deep recommendation models also achieve remarkable improvements in terms of recommendation accuracy. However, due to the large number of candidate items in practice and the high cost of preference computation, these methods also suffer from low efficiency of recommendation. The recently proposed tree-based deep recommendation models alleviate the problem by directly learning tree structure and representations under the guidance of recommendation objectives. However, such models have two shortcomings. First, the max-heap assumption in the hierarchical tree, in which the preference for a parent node should be the maximum between the preferences for its children, is difficult to satisfy in their binary classification objectives. Second, the learned index only includes a single tree, which is different from the widely-used multiple trees index, providing an opportunity to improve the accuracy of recommendation. To this end, we propose a Deep Forest-based Recommender (DeFoRec for short) for an efficient recommendation. In DeFoRec, all the trees generated during training process are retained to form the forest. When learning node representation of each tree, we have to satisfy the max-heap assumption as much as possible and mimic beam search behavior over the tree in the training stage. This is achieved by DeFoRec to regard the training task as multi-classification over tree nodes at the same level. However, the number of tree nodes grows exponentially with levels, making us to train the preference model by the guidance of sampled-softmax technique. The experiments are conducted on real-world datasets, validating the effectiveness of the proposed preference model learning method and tree learning method.|随着深度学习技术的发展，深度推荐模型在推荐精度方面也取得了显著的提高。然而，由于实际中候选项数量大，偏好计算成本高，这些方法也存在推荐效率低的问题。最近提出的基于树的深度推荐模型通过在推荐目标的指导下直接学习树的结构和表示来解决这个问题。然而，这种模式有两个缺点。首先，层次树中的最大堆假设(父节点的首选项应该是其子节点的首选项之间的最大值)难以满足其二进制分类目标。其次，学习索引只包括一棵树，这与广泛使用的多棵树索引不同，为提高推荐的准确性提供了机会。为此，我们提出了一个基于深度森林的推荐器(简称 DeFoRec)来实现有效的推荐。在 DeFoRec 中，所有在训练过程中生成的树被保留以形成森林。在学习每棵树的节点表示时，必须尽可能满足最大堆假设，并在训练阶段模拟树上的束搜索行为。DeFoRec 将训练任务视为同一层次上的树节点上的多分类，从而实现了这一目标。然而，树节点的数量随着层次的增加呈指数增长，这使得我们在采样-软极大技术的指导下对偏好模型进行训练。在实际数据集上进行了实验，验证了所提出的偏好模型学习方法和树学习方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Forest-based+Deep+Recommender)|0|
|[Ranking Interruptus: When Truncated Rankings Are Better and How to Measure That](https://doi.org/10.1145/3477495.3532051)|Enrique Amigó, Stefano Mizzaro, Damiano Spina|University of Udine, Udine, Italy; UNED NLP & IR Group, Madrid, Spain; RMIT University, Melbourne, VIC, Australia|Most of information retrieval effectiveness evaluation metrics assume that systems appending irrelevant documents at the bottom of the ranking are as effective as (or not worse than) systems that have a stopping criteria to 'truncate' the ranking at the right position to avoid retrieving those irrelevant documents at the end. It can be argued, however, that such truncated rankings are more useful to the end user. It is thus important to understand how to measure retrieval effectiveness in this scenario. In this paper we provide both theoretical and experimental contributions. We first define formal properties to analyze how effectiveness metrics behave when evaluating truncated rankings. Our theoretical analysis shows that de-facto standard metrics do not satisfy desirable properties to evaluate truncated rankings: only Observational Information Effectiveness (OIE) -- a metric based on Shannon's information theory -- satisfies them all. We then perform experiments to compare several metrics on nine TREC datasets. According to our experimental results, the most appropriate metrics for truncated rankings are OIE and a novel extension of Rank-Biased Precision that adds a user effort factor penalizing the retrieval of irrelevant documents.|大多数信息检索有效性评估指标都假定，在排名底部附加不相关文档的系统与那些有停止标准的系统一样有效(或者不比那些有停止标准的系统差) ，后者会在正确的位置“截断”排名，以避免在最后检索到那些不相关的文档。然而，可以说，这种截断的排名对最终用户更有用。因此，了解如何在此场景中度量检索效率非常重要。在本文中，我们提供了理论和实验的贡献。我们首先定义形式属性来分析效率指标在评估截断排名时的表现。我们的理论分析表明，事实上的标准指标不能满足评估截断排名的理想属性: 只有观测信息有效性(OIE)——一个基于香农信息理论的指标——能够满足所有这些指标。然后，我们进行实验来比较九个 TREC 数据集上的几个指标。根据我们的实验结果，最适合截断排名的指标是 OIE 和一个新的扩展排名偏差精度，增加了用户的努力因素惩罚检索不相关的文档。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ranking+Interruptus:+When+Truncated+Rankings+Are+Better+and+How+to+Measure+That)|0|
|[Offline Retrieval Evaluation Without Evaluation Metrics](https://doi.org/10.1145/3477495.3532033)|Fernando Diaz, Andres Ferraro|Mila - Quebec Artificial Intelligence Institute, Montréal, PQ, Canada; Google, Montréal, PQ, Canada|Offline evaluation of information retrieval and recommendation has traditionally focused on distilling the quality of a ranking into a scalar metric such as average precision or normalized discounted cumulative gain. We can use this metric to compare the performance of multiple systems for the same request. Although evaluation metrics provide a convenient summary of system performance, they also collapse subtle differences across users into a single number and can carry assumptions about user behavior and utility not supported across retrieval scenarios. We propose recall-paired preference (RPP), a metric-free evaluation method based on directly computing a preference between ranked lists. RPP simulates multiple user subpopulations per query and compares systems across these pseudo-populations. Our results across multiple search and recommendation tasks demonstrate that RPP substantially improves discriminative power while correlating well with existing metrics and being equally robust to incomplete data.|对信息检索和推荐的离线评估传统上侧重于将排名的质量提炼为一个标量指标，如平均精度或标准化折现累计增益。我们可以使用这个度量来比较同一个请求的多个系统的性能。虽然评估指标提供了一个方便的系统性能总结，但是它们也将用户之间的细微差异折叠成一个数字，并且可以对不支持检索场景的用户行为和实用程序进行假设。我们提出了一种基于直接计算排名表之间偏好的无度量评价方法——召回配对偏好(RPP)。RPP 模拟每个查询的多个用户子种群，并比较这些伪种群中的系统。我们在多个搜索和推荐任务中的结果表明，RPP 大大提高了识别能力，同时与现有指标关联良好，对不完整数据具有同样的鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Offline+Retrieval+Evaluation+Without+Evaluation+Metrics)|0|
|[Pareto-Optimal Fairness-Utility Amortizations in Rankings with a DBN Exposure Model](https://doi.org/10.1145/3477495.3532036)|Till Kletti, JeanMichel Renders, Patrick Loiseau|Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG, Grenoble, France; Naver Labs Europe, Meylan, France|In recent years, it has become clear that rankings delivered in many areas need not only be useful to the users but also respect fairness of exposure for the item producers. We consider the problem of finding ranking policies that achieve a Pareto-optimal tradeoff between these two aspects. Several methods were proposed to solve it; for instance a popular one is to use linear programming with a Birkhoff-von Neumann decomposition. These methods, however, are based on a classical Position Based exposure Model (PBM), which assumes independence between the items (hence the exposure only depends on the rank). In many applications, this assumption is unrealistic and the community increasingly moves towards considering other models that include dependences, such as the Dynamic Bayesian Network (DBN) exposure model. For such models, computing (exact) optimal fair ranking policies remains an open question. In this paper, we answer this question by leveraging a new geometrical method based on the so-called expohedron proposed recently for the PBM (Kletti et al., WSDM'22). We lay out the structure of a new geometrical object (the DBN-expohedron), and propose for it a Carathéodory decomposition algorithm of complexity $O(n^3)$, where n is the number of documents to rank. Such an algorithm enables expressing any feasible expected exposure vector as a distribution over at most n rankings; furthermore we show that we can compute the whole set of Pareto-optimal expected exposure vectors with the same complexity $O(n^3)$. Our work constitutes the first exact algorithm able to efficiently find a Pareto-optimal distribution of rankings. It is applicable to a broad range of fairness notions, including classical notions of meritocratic and demographic fairness. We empirically evaluate our method on the TREC2020 and MSLR datasets and compare it to several baselines in terms of Pareto-optimality and speed.|近年来，很明显，在许多领域提供的排名不仅需要对用户有用，而且还要尊重项目制作者的公平曝光。我们考虑的问题，找到排序的政策，实现了帕累托最优权衡这两个方面。人们提出了几种方法来解决这个问题，例如，一种流行的方法是使用伯克霍夫-冯诺依曼分解的线性规划。然而，这些方法是基于经典的基于位置的曝光模型(PBM) ，该模型假设项目之间的独立性(因此曝光只取决于排名)。在许多应用程序中，这种假设是不现实的，社区越来越倾向于考虑包含依赖关系的其他模型，例如动态贝氏网路暴露模型。对于这样的模型，计算(精确的)最优公平排序策略仍然是一个悬而未决的问题。在本文中，我们回答这个问题，利用一个新的几何方法的基础上，所谓的外三面体最近提出的 PBM (Kletti 等，WSDM’22)。我们给出了一个新的几何对象(DBN-expohedron)的结构，并提出了一个复杂度为 $O (n ^ 3) $的 Carathéodory 分解算法，其中 n 是要排序的文档数。这种算法能够表示任何可行的期望暴露矢量作为一个分布在最多 n 个排名; 此外，我们表明，我们可以计算整个集合的帕累托最优期望暴露矢量具有相同的复杂度 $O (n ^ 3) $。我们的工作构成了第一个精确的算法，能够有效地找到排名的帕累托最优分布。它适用于广泛的公平概念，包括精英统治和人口统计公平的经典概念。我们在 TREC2020和 MSLR 数据集上经验性地评估了我们的方法，并将其与几个基线在帕累托最优性和速度方面进行了比较。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pareto-Optimal+Fairness-Utility+Amortizations+in+Rankings+with+a+DBN+Exposure+Model)|0|
|[Risk-Sensitive Deep Neural Learning to Rank](https://doi.org/10.1145/3477495.3532056)|Pedro Henrique Silva Rodrigues, Daniel Xavier de Sousa, Thierson Couto Rosa, Marcos André Gonçalves|Federal University of Minas Gerais - UFMG, Belo Horizonte, Brazil; Federal Institute of Goiás - IFG, Anápolis, Brazil; Federal University of Goiás - UFG, Goiânia, Brazil|Learning to Rank (L2R) is the core task of many Information Retrieval systems. Recently, a great effort has been put on exploring Deep Neural Networks (DNNs) for L2R, with significant results. However, risk-sensitiveness, an important and recent advance in the L2R arena, that reduces variability and increases trust, has not been incorporated into Deep Neural L2R yet. Risk-sensitive measures are important to assess the risk of an IR system to perform worse than a set of baseline IR systems for several queries. However, the risk-sensitive measures described in the literature have a non-smooth behavior, making them difficult, if not impossible, to be optimized by DNNs. In this work we solve this difficult problem by proposing a family of new loss functions -- \riskloss\ -- that support a smooth risk-sensitive optimization. \riskloss\ introduces two important contributions: (i) the substitution of the traditional NDCG or MAP metrics in risk-sensitive measures with smooth loss functions that evaluate the correlation between the predicted and the true relevance order of documents for a given query and (ii) the use of distinct versions of the same DNN architecture as baselines by means of a multi-dropout technique during the smooth risk-sensitive optimization, avoiding the inconvenience of assessing multiple IR systems as part of DNN training. We empirically demonstrate significant achievements of the proposed \riskloss\ functions when used with recent DNN methods in the context of well-known web-search datasets such as WEB10K, YAHOO, and MQ2007. Our solutions reach improvements of 8% in effectiveness (NDCG) while improving in around 5% the risk-sensitiveness (\grisk\ measure) when applied together with a state-of-the-art Self-Attention DNN-L2R architecture. Furthermore, \riskloss\ is capable of reducing by 28% the losses over the best evaluated baselines and significantly improving over the risk-sensitive state-of-the-art non-DNN method (by up to 13.3%) while keeping (or even increasing) overall effectiveness. All these results ultimately establish a new level for the state-of-the-art on risk-sensitiveness and DNN-L2R research.|学习排名(L2R)是许多信息检索系统的核心任务。近年来，针对 L2R 的深层神经网络(DNN)的研究取得了显著的成果。然而，风险敏感性，一个重要的和最近在 L2R 领域的进展，减少变异性和增加信任，尚未被纳入深层神经 L2R。风险敏感度量对于评估一个 IR 系统在几个查询中的性能低于一组基准 IR 系统的风险非常重要。然而，文献中描述的风险敏感性措施有一个不平滑的行为，使他们难以，如果不是不可能，被 DNN 优化。在这项工作中，我们通过提出一系列新的损失函数——风险损失——来解决这个难题，这些函数支持平稳的风险敏感优化。风险损失引入了两个重要贡献: (i)用平滑损失函数替换风险敏感度量中的传统 NDCG 或 MAP 指标，评估给定查询的文档的预测和真实相关顺序之间的相关性; (ii)通过平滑风险敏感性优化期间的多退出技术使用相同 DNN 架构的不同版本作为基线，避免了评估多个 IR 系统作为 DNN 训练的一部分的不便。当与最近的 DNN 方法在诸如 WEB10K，YAHOO 和 MQ2007等著名的网络搜索数据集的背景下使用时，我们经验性地证明了所提出的风险损失函数的显着成就。我们的解决方案在与最先进的自我注意 DNN-L2R 架构一起应用时，有效性(NDCG)提高了8% ，而风险敏感性(风险测量)提高了约5% 。此外，风险损失能够比最佳评估基线减少28% 的损失，并且比风险敏感的最先进的非 DNN 方法(高达13.3%)显着改善，同时保持(甚至增加)总体有效性。所有这些结果最终为风险敏感性和 DNN-L2R 研究的最新水平奠定了一个新的基础。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Risk-Sensitive+Deep+Neural+Learning+to+Rank)|0|
|[Adaptable Text Matching via Meta-Weight Regulator](https://doi.org/10.1145/3477495.3531932)|Bo Zhang, Chen Zhang, Fang Ma, Dawei Song|Beijing Institute of Technology, Beijing, China|Neural text matching models have been used in a range of applications such as question answering and natural language inference, and have yielded a good performance. However, these neural models are of a limited adaptability, resulting in a decline in performance when encountering test examples from a different dataset or even a different task. The adaptability is particularly important in the few-shot setting: in many cases, there is only a limited amount of labeled data available for a target dataset or task, while we may have access to a richly labeled source dataset or task. However, adapting a model trained on the abundant source data to a few-shot target dataset or task is challenging. To tackle this challenge, we propose a Meta-Weight Regulator (MWR), which is a meta-learning approach that learns to assign weights to the source examples based on their relevance to the target loss. Specifically, MWR first trains the model on the uniformly weighted source examples, and measures the efficacy of the model on the target examples via a loss function. By iteratively performing a (meta) gradient descent, high-order gradients are propagated to the source examples. These gradients are then used to update the weights of source examples, in a way that is relevant to the target performance. As MWR is model-agnostic, it can be applied to any backbone neural model. Extensive experiments are conducted with various backbone text matching models, on four widely used datasets and two tasks. The results demonstrate that our proposed approach significantly outperforms a number of existing adaptation methods and effectively improves the cross-dataset and cross-task adaptability of the neural text matching models in the few-shot setting.|神经文本匹配模型已经在问答、自然语言推理等领域得到了广泛的应用，并取得了良好的效果。然而，这些神经模型的适应性有限，当遇到来自不同数据集甚至不同任务的测试例子时，会导致性能下降。适应性在少镜头设置中尤其重要: 在许多情况下，目标数据集或任务只有有限数量的标记数据可用，而我们可以访问标记丰富的源数据集或任务。然而，将一个基于大量源数据训练的模型应用于少量目标数据集或任务是具有挑战性的。为了应对这一挑战，我们提出了一种元权重调节器(MWR) ，它是一种元学习方法，学习根据源示例与目标损失的相关性为其分配权重。具体来说，MWR 首先在均匀加权的源例子上训练模型，然后通过损失函数来度量模型对目标例子的有效性。通过迭代执行一个(元)梯度下降法，高阶梯度被传播到源示例。然后使用这些渐变来更新源示例的权重，其方式与目标性能相关。由于 MWR 是模型无关的，因此它可以应用于任何骨干神经网络模型。在四个广泛使用的数据集和两个任务上，使用各种骨干文本匹配模型进行了广泛的实验。结果表明，本文提出的方法明显优于现有的一些自适应方法，有效地提高了神经元文本匹配模型在少镜头情况下的跨数据集和跨任务适应性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adaptable+Text+Matching+via+Meta-Weight+Regulator)|0|
|[Re-thinking Knowledge Graph Completion Evaluation from an Information Retrieval Perspective](https://doi.org/10.1145/3477495.3532052)|Ying Zhou, Xuanang Chen, Ben He, Zheng Ye, Le Sun|South-Central University for Nationalities, Wuhan, China; University of Chinese Academy of Sciences & Institute of Software, Chinese Academy of Sciences, Beijing, China; Institute of Software, Chinese Academy of Sciences, Beijing, China|Knowledge graph completion (KGC) aims to infer missing knowledge triples based on known facts in a knowledge graph. Current KGC research mostly follows an entity ranking protocol, wherein the effectiveness is measured by the predicted rank of a masked entity in a test triple. The overall performance is then given by a micro(-average) metric over all individual answer entities. Due to the incomplete nature of the large-scale knowledge bases, such an entity ranking setting is likely affected by unlabelled top-ranked positive examples, raising questions on whether the current evaluation protocol is sufficient to guarantee a fair comparison of KGC systems. To this end, this paper presents a systematic study on whether and how the label sparsity affects the current KGC evaluation with the popular micro metrics. Specifically, inspired by the TREC paradigm for large-scale information retrieval (IR) experimentation, we create a relatively "complete" judgment set based on a sample from the popular FB15k-237 dataset following the TREC pooling method. According to our analysis, it comes as a surprise that switching from the original labels to our "complete" labels results in a drastic change of system ranking of a variety of 13 popular KGC models in terms of micro metrics. Further investigation indicates that the IR-like macro(-average) metrics are more stable and discriminative under different settings, meanwhile, less affected by label sparsity. Thus, for KGC evaluation, we recommend conducting TREC-style pooling to balance between human efforts and label completeness, and reporting also the IR-like macro metrics to reflect the ranking nature of the KGC task.|知识图完成(KGC)是基于知识图中已知事实推断出缺失的知识三元组。目前的 KGC 研究大多遵循一个实体排名协议，其中的有效性是衡量一个被掩盖的实体在一个测试三元组的预测排名。然后，通过对所有单个答案实体的微观(平均)度量给出总体表现。由于大规模知识库的不完整性，这种实体排名设置可能会受到没有标记的排名最高的积极实例的影响，从而引起目前的评价议定书是否足以保证公平比较 KGC 系统的问题。为此，本文利用当前流行的微观指标，对标签稀疏性是否以及如何影响当前 KGC 评价进行了系统的研究。具体来说，受到 TREC 大规模信息检索(IR)实验范例的启发，我们创建了一个相对“完整”的判断集，该判断集基于流行的 FB15k-237数据集的样本，采用 TREC 汇集方法。根据我们的分析，令人惊讶的是，从原始标签切换到我们的“完整”标签导致系统排名的急剧变化的各种13个流行的 KGC 模型在微观指标方面。进一步的研究表明，类 IR 宏(平均)指标在不同的设置下更加稳定和具有区分性，同时受标签稀疏性的影响较小。因此，对于 KGC 评估，我们建议进行 TREC 风格的池来平衡人工努力和标签完整性，并报告类似 IR 的宏指标来反映 KGC 任务的排名性质。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Re-thinking+Knowledge+Graph+Completion+Evaluation+from+an+Information+Retrieval+Perspective)|0|
|[CRET: Cross-Modal Retrieval Transformer for Efficient Text-Video Retrieval](https://doi.org/10.1145/3477495.3531960)|Kaixiang Ji, Jiajia Liu, Weixiang Hong, Liheng Zhong, Jian Wang, Jingdong Chen, Wei Chu|Ant Group, Hangzhou, China|Given a text query, the text-to-video retrieval task aims to find the relevant videos in the database. Recently, model-based (MDB) methods have demonstrated superior accuracy than embedding-based (EDB) methods due to their excellent capacity of modeling local video/text correspondences, especially when equipped with large-scale pre-training schemes like ClipBERT. Generally speaking, MDB methods take a text-video pair as input and harness deep models to predict the mutual similarity, while EDB methods first utilize modality-specific encoders to extract embeddings for text and video, then evaluate the distance based on the extracted embeddings. Notably, MDB methods cannot produce explicit representations for text and video, instead, they have to exhaustively pair the query with every database item to predict their mutual similarities in the inference stage, which results in significant inefficiency in practical applications. In this work, we propose a novel EDB method CRET (Cross-modal REtrieval Transformer), which not only demonstrates promising efficiency in retrieval tasks, but also achieves better accuracy than existing MDB methods. The credits are mainly attributed to our proposed Cross-modal Correspondence Modeling (CCM) module and Gaussian Estimation of Embedding Space (GEES) loss. Specifically, the CCM module is composed by transformer decoders and a set of decoder centers. With the help of the learned decoder centers, the text/video embeddings can be efficiently aligned, without suffering from pairwise model-based inference. Moreover, to balance the information loss and computational overhead when sampling frames from a given video, we present a novel GEES loss, which implicitly conducts dense sampling in the video embedding space, without suffering from heavy computational cost. Extensive experiments show that without pre-training on extra datasets, our proposed CRET outperforms the state-of-the-art MDB methods that were pre-trained on additional datasets, meanwhile still shows promising efficiency in retrieval tasks.|给定一个文本查询，文本到视频检索任务的目的是在数据库中找到相关的视频。近年来，基于模型(MDB)的方法由于其优异的局部视频/文本对应建模能力而显示出优于基于嵌入的方法的准确性，特别是当配备了大规模的预训练方案，如 ClipBERT。一般来说，MDB 方法以文本-视频对为输入，利用深度模型来预测相似度，而 EDB 方法首先利用特定于模态的编码器来提取文本和视频的嵌入，然后根据提取的嵌入来评估距离。值得注意的是，MDB 方法不能生成文本和视频的显式表示，相反，它们必须将查询与每个数据库项穷举地配对，以预测它们在推理阶段的相似性，这导致了实际应用中的显著效率低下。本文提出了一种新的多模态检索转换器(CRET)方法，该方法不仅在检索任务中表现出良好的效率，而且比现有的 MDB 方法具有更高的准确率。这主要归功于我们提出的交叉模态对应建模(CCM)模块和嵌入空间的高斯估计(GEES)损失。具体来说，CCM 模块由变压器解码器和一组解码中心组成。借助于所学习的解码中心，文本/视频嵌入可以有效地对齐，而不会受到基于成对模型的推理的影响。此外，为了平衡从给定视频帧采样时的信息损失和计算开销，我们提出了一种新的 GEES 损失算法，该算法在视频嵌入空间中隐式地进行密集采样，不需要承担大量的计算开销。大量的实验表明，在不对额外数据集进行预训练的情况下，我们提出的 CRET 方法优于对额外数据集进行预训练的最先进的 MDB 方法，同时在检索任务中仍然显示出有希望的效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CRET:+Cross-Modal+Retrieval+Transformer+for+Efficient+Text-Video+Retrieval)|0|
|[Learn from Unlabeled Videos for Near-duplicate Video Retrieval](https://doi.org/10.1145/3477495.3532010)|Xiangteng He, Yulin Pan, Mingqian Tang, Yiliang Lv, Yuxin Peng|Alibaba Group, Hangzhou, China; Peking University, Beijing, China|Near-duplicate video retrieval (NDVR) aims to find the copies or transformations of the query video from a massive video database. It plays an important role in many video related applications, including copyright protection, tracing, filtering and etc. Video representation and similarity search are crucial to any video retrieval system. To derive effective video representation, most video retrieval systems require a large amount of manually annotated data for training, making it costly inefficient. In addition, most retrieval systems are based on frame-level features for video similarity searching, making it expensive both storage wise and search wise. To address the above issues, we propose a video representation learning (VRL) approach to effectively address the above shortcomings. It first effectively learns video representation from unlabeled videos via contrastive learning to avoid the expensive cost of manual annotation. Then, it exploits transformer structure to aggregate frame-level features into clip-level to reduce both storage space and search complexity. It can learn the complementary and discriminative information from the interactions among clip frames, as well as acquire the frame permutation and missing invariant ability to support more flexible retrieval manners. Comprehensive experiments on two challenging near-duplicate video retrieval datasets, namely FIVR-200K and SVD, verify the effectiveness of our proposed VRL approach, which achieves the best performance of video retrieval on accuracy and efficiency.|近重复视频检索(NDVR)的目标是从海量视频数据库中查找查询视频的副本或变换。它在许多视频相关应用中起着重要作用，包括版权保护、跟踪、过滤等。视频表示和最近邻搜索对于任何视频检索系统都至关重要。为了获得有效的视频表示，大多数视频检索系统需要大量的人工注释数据进行训练，这使得系统效率低下。此外，大多数检索系统基于帧级特征进行视频相似性搜索，这使得存储和搜索成本都很高。针对上述问题，本文提出了一种视频表示学习(VRL)方法，有效地解决了上述问题。它首先通过对比学习有效地从未标记的视频中学习视频表示，从而避免了人工标注的昂贵成本。然后，利用变压器结构将帧级特征聚合为剪辑级特征，降低存储空间和搜索复杂度。它可以从剪辑帧之间的交互中学习互补信息和鉴别信息，获得帧排列和缺失不变量能力，支持更灵活的检索方式。通过对 FIVR-200K 和 SVD 两个具有挑战性的近重复视频检索数据集的综合实验，验证了本文提出的 VRL 方法的有效性，在准确性和效率方面达到了最佳的视频检索性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learn+from+Unlabeled+Videos+for+Near-duplicate+Video+Retrieval)|0|
|[Progressive Learning for Image Retrieval with Hybrid-Modality Queries](https://doi.org/10.1145/3477495.3532047)|Yida Zhao, Yuqing Song, Qin Jin|Renmin University of China, Beijing, China|Image retrieval with hybrid-modality queries, also known as composing text and image for image retrieval (CTI-IR), is a retrieval task where the search intention is expressed in a more complex query format, involving both vision and text modalities. For example, a target product image is searched using a reference product image along with text about changing certain attributes of the reference image as the query. It is a more challenging image retrieval task that requires both semantic space learning and cross-modal fusion. Previous approaches that attempt to deal with both aspects achieve unsatisfactory performance. In this paper, we decompose the CTI-IR task into a three-stage learning problem to progressively learn the complex knowledge for image retrieval with hybrid-modality queries. We first leverage the semantic embedding space for open-domain image-text retrieval, and then transfer the learned knowledge to the fashion-domain with fashion-related pre-training tasks. Finally, we enhance the pre-trained model from single-query to hybrid-modality query for the CTI-IR task. Furthermore, as the contribution of individual modality in the hybrid-modality query varies for different retrieval scenarios, we propose a self-supervised adaptive weighting strategy to dynamically determine the importance of image and text in the hybrid-modality query for better retrieval. Extensive experiments show that our proposed model significantly outperforms state-of-the-art methods in the mean of [email protected] by 24.9% and 9.5% on the Fashion-IQ and Shoes benchmark datasets respectively.|基于混合模态查询的图像检索，即组合文本和图像进行图像检索(CTI-IR) ，是一种以更复杂的查询格式表达搜索意图的检索任务，涉及视觉和文本模态。例如，使用参考产品图像以及关于将参考图像的某些属性更改为查询的文本搜索目标产品图像。语义空间学习和跨模态融合是图像检索中一个更具挑战性的任务。以前试图同时处理这两个方面的方法的性能都不令人满意。本文将 CTI-IR 任务分解为三阶段学习问题，逐步学习用于图像检索的复杂知识。我们首先利用语义嵌入空间进行开放领域的图文检索，然后利用与时尚相关的预训练任务将所学知识转移到时尚领域。最后，对 CTI-IR 任务的预训练模型进行了改进，从单查询模型改进为混合模态查询模型。此外，由于个体模态在混合模态查询中的贡献因检索场景的不同而不同，我们提出了一种自监督自适应加权策略，动态确定图像和文本在混合模态查询中的重要性，以便更好地检索。大量的实验表明，在 Fashion-IQ 和 Shoes 基准数据集上，我们提出的模型在平均值(电子邮件受保护)方面明显优于最先进的方法，分别为24.9% 和9.5% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Progressive+Learning+for+Image+Retrieval+with+Hybrid-Modality+Queries)|0|
|[Incorporating Explicit Knowledge in Pre-trained Language Models for Passage Re-ranking](https://doi.org/10.1145/3477495.3531997)|Qian Dong, Yiding Liu, Suqi Cheng, Shuaiqiang Wang, Zhicong Cheng, Shuzi Niu, Dawei Yin|Baidu Inc., Beijing, China; Institute of Software, Chinese Academy of Sciences, Beijing, China|Passage re-ranking is to obtain a permutation over the candidate passage set from retrieval stage. Re-rankers have been boomed by Pre-trained Language Models (PLMs) due to their overwhelming advantages in natural language understanding. However, existing PLM based re-rankers may easily suffer from vocabulary mismatch and lack of domain specific knowledge. To alleviate these problems, explicit knowledge contained in knowledge graph is carefully introduced in our work. Specifically, we employ the existing knowledge graph which is incomplete and noisy, and first apply it in passage re-ranking task. To leverage a reliable knowledge, we propose a novel knowledge graph distillation method and obtain a knowledge meta graph as the bridge between query and passage. To align both kinds of embedding in the latent space, we employ PLM as text encoder and graph neural network over knowledge meta graph as knowledge encoder. Besides, a novel knowledge injector is designed for the dynamic interaction between text and knowledge encoder. Experimental results demonstrate the effectiveness of our method especially in queries requiring in-depth domain knowledge.|文章重新排序是从检索阶段对候选文章集合进行排列。由于预训练语言模型在自然语言理解方面具有压倒性的优势，重新排名的语言模型得到了蓬勃发展。然而，现有的基于 PLM 的重新排名可能很容易受到词汇不匹配和缺乏领域特定知识的影响。为了解决这些问题，我们在工作中仔细介绍了知识图表中的外显知识。具体地说，我们利用现有的不完备且有噪声的知识图，首先将其应用于段落重排任务。为了利用可靠的知识，我们提出了一种新的知识图提取方法，并得到一个知识元图作为查询和文章之间的桥梁。为了在潜空间中对齐这两种嵌入，我们采用 PLM 作为文本编码器，知识元图上的图形神经网络作为知识编码器。此外，还设计了一种新颖的知识注入器，用于文本和知识编码器之间的动态交互。实验结果表明了该方法的有效性，特别是在需要深度领域知识的查询中。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Incorporating+Explicit+Knowledge+in+Pre-trained+Language+Models+for+Passage+Re-ranking)|0|
|[Axiomatically Regularized Pre-training for Ad hoc Search](https://doi.org/10.1145/3477495.3531943)|Jia Chen, Yiqun Liu, Yan Fang, Jiaxin Mao, Hui Fang, Shenghao Yang, Xiaohui Xie, Min Zhang, Shaoping Ma|University of Delaware, Newark, DE, USA; Renmin University of China, Beijing, China; Tsinghua University, Beijing, China|Recently, pre-training methods tailored for IR tasks have achieved great success. However, as the mechanisms behind the performance improvement remain under-investigated, the interpretability and robustness of these pre-trained models still need to be improved. Axiomatic IR aims to identify a set of desirable properties expressed mathematically as formal constraints to guide the design of ranking models. Existing studies have already shown that considering certain axioms may help improve the effectiveness and interpretability of IR models. However, there still lack efforts of incorporating these IR axioms into pre-training methodologies. To shed light on this research question, we propose a novel pre-training method with \underlineA xiomatic \underlineRe gularization for ad hoc \underlineS earch (ARES). In the ARES framework, a number of existing IR axioms are re-organized to generate training samples to be fitted in the pre-training process. These training samples then guide neural rankers to learn the desirable ranking properties. Compared to existing pre-training approaches, ARES is more intuitive and explainable. Experimental results on multiple publicly available benchmark datasets have shown the effectiveness of ARES in both full-resource and low-resource (e.g., zero-shot and few-shot) settings. An intuitive case study also indicates that ARES has learned useful knowledge that existing pre-trained models (e.g., BERT and PROP) fail to possess. This work provides insights into improving the interpretability of pre-trained models and the guidance of incorporating IR axioms or human heuristics into pre-training methods.|近年来，针对 IR 任务的预训练方法取得了很大的成功。然而，由于性能改进背后的机制仍然没有得到充分的研究，这些预先训练的模型的可解释性和鲁棒性仍然需要改进。公理 IR 旨在识别一组用数学方法表示为形式约束的理想属性，以指导排序模型的设计。现有的研究已经表明，考虑某些公理可能有助于提高红外模型的有效性和可解释性。然而，仍然缺乏将这些 IR 公理纳入预训练方法的努力。针对这一问题，本文提出了一种基于下划线公理化下划线正则化的自组织下划线搜索(ARES)预训练方法。在 ARES 框架中，对一些现有的信息检索公理进行了重新组织，以生成培训样本，用于培训前进程。这些训练样本然后指导神经排序学习理想的排序属性。与现有的预训练方法相比，ARES 更加直观和易于解释。在多个公开可用的基准数据集上的实验结果显示了 ARES 在全资源和低资源(例如，零拍摄和少拍摄)环境下的有效性。一个直观的案例研究还表明，ARES 已经学到了有用的知识，现有的预训练模型(例如，BERT 和 PROP)不能拥有。这项工作为提高预训练模型的可解释性提供了见解，并指导将 IR 公理或人类启发式融入预训练方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Axiomatically+Regularized+Pre-training+for+Ad+hoc+Search)|0|
|[On the Role of Relevance in Natural Language Processing Tasks](https://doi.org/10.1145/3477495.3532034)|Artsiom Sauchuk, James Thorne, Alon Y. Halevy, Nicola Tonellotto, Fabrizio Silvestri|Sapienza University of Rome, Roma, Italy; University of Pisa, Pisa, Italy; Meta AI, Menlo Park, CA, USA; Cambridge University, London, United Kingdom|Many recent Natural Language Processing (NLP) task formulations, such as question answering and fact verification, are implemented as a two-stage cascading architecture. In the first stage an IR system retrieves "relevant'' documents containing the knowledge, and in the second stage an NLP system performs reasoning to solve the task. Optimizing the IR system for retrieving relevant documents ensures that the NLP system has sufficient information to operate over. These recent NLP task formulations raise interesting and exciting challenges for IR, where the end-user of an IR system is not a human with an information need, but another system exploiting the documents retrieved by the IR system to perform reasoning and address the user information need. Among these challenges, as we will show, is that noise from the IR system, such as retrieving spurious or irrelevant documents, can negatively impact the accuracy of the downstream reasoning module. Hence, there is the need to balance maximizing relevance while minimizing noise in the IR system. This paper presents experimental results on two NLP tasks implemented as a two-stage cascading architecture. We show how spurious or irrelevant retrieved results from the first stage can induce errors in the second stage. We use these results to ground our discussion of the research challenges that the IR community should address in the context of these knowledge-intensive NLP tasks.|近年来，自然语言处理(NLP)的许多任务公式，如问题回答和事实验证，都是作为一个两阶段级联结构实现的。在第一阶段，IR 系统检索包含知识的“相关”文档，在第二阶段，NLP 系统执行推理来解决任务。优化检索相关文件的红外系统，确保自然语言处理系统有足够的信息进行操作。这些最新的 NLP 任务公式为 IR 提出了有趣和令人兴奋的挑战，其中 IR 系统的最终用户不是一个有信息需求的人，而是另一个利用 IR 系统检索到的文档进行推理并满足用户信息需求的系统。在这些挑战中，正如我们将要展示的，来自 IR 系统的噪音，例如检索虚假或不相关的文档，可能会对下游推理模块的准确性产生负面影响。因此，需要在最大相关性和最小噪声之间取得平衡。本文给出了两个实现为两级级联结构的自然语言处理任务的实验结果。我们展示了如何伪造或不相关的检索结果从第一阶段可以导致错误的第二阶段。我们使用这些结果来基础我们的研究挑战的讨论，IR 社区应该在这些知识密集型的自然语言处理任务的背景下解决。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+the+Role+of+Relevance+in+Natural+Language+Processing+Tasks)|0|
|[Adversarial Graph Perturbations for Recommendations at Scale](https://doi.org/10.1145/3477495.3531763)|Huiyuan Chen, Kaixiong Zhou, KweiHerng Lai, Xia Hu, Fei Wang, Hao Yang|Visa Research, palo alto, CA, USA; Rice University, Houston, TX, USA|Graph Neural Networks (GNNs) provide a class of powerful architectures that are effective for graph-based collaborative filtering. Nevertheless, GNNs are known to be vulnerable to adversarial perturbations. Adversarial training is a simple yet effective way to improve the robustness of neural models. For example, many prior studies inject adversarial perturbations into either node features or hidden layers of GNNs. However, perturbing graph structures has been far less studied in recommendations. To bridge this gap, we propose AdvGraph to model adversarial graph perturbations during the training of GNNs. Our AdvGraph is mainly based on min-max robust optimization, where an universal graph perturbation is obtained through an inner maximization while the outer optimization aims to compute the model parameters of GNNs. However, direct optimizing the inner problem is challenging due to discrete nature of the graph perturbations. To address this issue, an unbiased gradient estimator is further proposed to compute the gradients of discrete variables. Extensive experiments demonstrate that our AdvGraph is able to enhance the generalization performance of GNN-based recommenders.|图形神经网络(GNN)为基于图形的协同过滤提供了一种强大的架构。然而，GNN 是众所周知的脆弱的对抗性扰动。对抗训练是提高神经模型鲁棒性的一种简单而有效的方法。例如，许多先前的研究将对抗扰动注入到 GNN 的节点特征或隐层中。然而，令人不安的图形结构在推荐中却很少被研究。为了弥补这一差距，我们提出了 AdvGraph 来模拟 GNN 训练过程中的对抗图扰动。我们的 AdvGraph 主要是基于最小-最大鲁棒优化，其中通过内部最大化获得通用图摄动，而外部优化的目的是计算 GNN 的模型参数。然而，直接优化的内部问题是具有挑战性的，由于离散性质的图摄动。为了解决这一问题，进一步提出了一种无偏的梯度估计器来计算离散变量的梯度。大量的实验表明，我们的 AdvGraph 能够提高基于 GNN 的推荐器的泛化性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adversarial+Graph+Perturbations+for+Recommendations+at+Scale)|0|
|[Relevance under the Iceberg: Reasonable Prediction for Extreme Multi-label Classification](https://doi.org/10.1145/3477495.3531767)|JyunYu Jiang, WeiCheng Chang, Jiong Zhang, ChoJui Hsieh, HsiangFu Yu|University of California, Los Angeles, Los Angeles, CA, USA; Amazon Search, Palo Alto, CA, USA|In the era of big data, eXtreme Multi-label Classification (XMC) has already become one of the most essential research tasks to deal with enormous label spaces in machine learning applications. Instead of assessing every individual label, most XMC methods rely on label trees or filters to derive short ranked label lists as prediction, thereby reducing computational overhead. Specifically, existing studies obtain ranked label lists with a fixed length for prediction and evaluation. However, these predictions are unreasonable since data points have varied numbers of relevant labels. The greatly small and large list lengths in evaluation, such as [email protected] and [email protected] , can also lead to the ignorance of other relevant labels or the tolerance of many irrelevant labels. In this paper, we aim to provide reasonable prediction for extreme multi-label classification with dynamic numbers of predicted labels. In particular, we propose a novel framework, Model-Agnostic List Truncation with Ordinal Regression (MALTOR), to leverage the ranking properties and truncate long ranked label lists for better accuracy. Extensive experiments conducted on six large-scale real-world benchmark datasets demonstrate that MALTOR significantly outperforms statistical baseline methods and conventional ranked list truncation methods in ad-hoc retrieval with both linear and deep XMC models. The results of an ablation study also shows the effectiveness of each individual component in our proposed MALTOR.|在大数据时代，极限多标签分类(XMC)已经成为处理机器学习应用中大量标签空间的重要研究课题之一。大多数 XMC 方法不是评估每个单独的标签，而是依靠标签树或过滤器来推导出短排名标签列表作为预测，从而减少计算开销。具体来说，现有的研究获得了具有固定长度的排名标签列表，用于预测和评价。然而，这些预测是不合理的，因为数据点有不同数量的相关标签。评估中的列表长度大大小小，如[ email protected ]和[ email protected ] ，也可能导致对其他相关标签的忽视或许多不相关标签的容忍。在本文中，我们的目的是提供合理的预测与预测标签的动态数量极端多标签分类。特别是，我们提出了一个新的框架，模型不可知列表与有序回归截断(MALTOR) ，以利用排名属性和截断长的排名标签列表，以更好的准确性。在六个大规模真实世界基准数据集上进行的大量实验表明，MALTOR 在线性和深度 XMC 模型的特别检索中显著优于统计基线方法和传统的排序列表截断方法。消融研究的结果也显示了我们提出的 MALTOR 中每个单独组件的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Relevance+under+the+Iceberg:+Reasonable+Prediction+for+Extreme+Multi-label+Classification)|0|
|[Gating-adapted Wavelet Multiresolution Analysis for Exposure Sequence Modeling in CTR Prediction](https://doi.org/10.1145/3477495.3531771)|Xiaoxiao Xu, Zhiwei Fang, Qian Yu, Ruoran Huang, Chaosheng Fan, Yong Li, Yang He, Changping Peng, Zhangang Lin, Jingping Shao, Non Non|JD Com, Business Growth BU, Beijing, Peoples R China|The exposure sequence is being actively studied for user interest modeling in Click-Through Rate (CTR) prediction. However, the existing methods for exposure sequence modeling bring extensive computational burden and neglect noise problems, resulting in an excessively latency and the limited performance in online recommenders. In this paper, we propose to address the high latency and noise problems via Gating-adapted wavelet multiresolution analysis (Gama), which can effectively denoise the extremely long exposure sequence and adaptively capture the implied multi-dimension user interest with linear computational complexity. This is the first attempt to integrate non-parametric multiresolution analysis technique into deep neural network to model user exposure sequence. Extensive experiments on large scale benchmark dataset and real production dataset confirm the effectiveness of Gama for exposure sequence modeling, especially in cold-start scenarios. Benefited from its low latency and high effecitveness, Gama has been deployed in our real large-scale industrial recommender, successfully serving over hundreds of millions users.|我们正积极研究暴露次序，以建立用户兴趣模式，预测点进率。然而，现有的曝光序列建模方法存在计算量大、忽视噪声等问题，导致在线推荐系统的延迟过长，性能有限。在这篇文章中，我们提出利用门控自适应小波多解析度分析(Gama)来处理高延迟和噪声问题，它可以有效地去除极长曝光序列的噪声，并以线性计算复杂度自适应地捕捉隐含的多维用户兴趣。这是首次尝试将非参数多解析度分析技术与深层神经网络相结合，建立用户暴露序列模型。在大规模基准数据集和实际生产数据集上的大量实验证实了伽马方法对曝光序列建模的有效性，特别是在冷启动情况下。得益于它的低延迟和高效率，伽马已经部署在我们真正的大规模工业推荐，成功地为数亿用户服务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Gating-adapted+Wavelet+Multiresolution+Analysis+for+Exposure+Sequence+Modeling+in+CTR+Prediction)|0|
|[Animating Images to Transfer CLIP for Video-Text Retrieval](https://doi.org/10.1145/3477495.3531776)|Yu Liu, Huai Chen, Lianghua Huang, Di Chen, Bin Wang, Pan Pan, Lisheng Wang|Shanghai Jiao Tong University, Shanghai, China; DAMO Academy, Alibaba Group, Beijing, China; DAMO Academy, Alibaba Group, Hangzhou, China|Recent works show the possibility of transferring the CLIP (Contrastive Language-Image Pretraining) model for video-text retrieval with promising performance. However, due to the domain gap between static images and videos, CLIP-based video-text retrieval models with interaction-based matching perform far worse than models with representation-based matching. In this paper, we propose a novel image animation strategy to transfer the image-text CLIP model to video-text retrieval effectively. By imitating the video shooting components, we convert widely used image-language corpus to synthesized video-text data for pretraining. To reduce the time complexity of interaction matching, we further propose a coarse to fine framework which consists of dual encoders for fast candidates searching and a cross-modality interaction module for fine-grained re-ranking. The coarse to fine framework with the synthesized video-text pretraining provides significant gains in retrieval accuracy while preserving efficiency. Comprehensive experiments conducted on MSR-VTT, MSVD, and VATEX datasets demonstrate the effectiveness of our approach.|最近的研究表明，将对比语言-图像预训练(CLIP)模型应用于视频文本检索具有良好的性能。然而，由于静态图像和视频之间存在领域差异，基于 CLIP 的基于交互匹配的视频文本检索模型的性能远远不如基于表示匹配的模型。本文提出了一种新的图像动画策略，将图像-文本 CLIP 模型有效地转化为视频-文本检索。通过模拟视频拍摄组件，将广泛使用的图像语言语料库转换为合成的视频文本数据进行预训练。为了降低交互匹配的时间复杂度，我们进一步提出了一个由双编码器组成的快速候选搜索框架和一个交叉模式交互模块组成的细粒度重排序框架。综合视频文本预训练的粗细框架在保持检索效率的同时，提高了检索精度。在 MSR-VTT、 MSVD 和 VATEX 数据集上进行的综合实验证明了该方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Animating+Images+to+Transfer+CLIP+for+Video-Text+Retrieval)|0|
|[Image-Text Retrieval via Contrastive Learning with Auxiliary Generative Features and Support-set Regularization](https://doi.org/10.1145/3477495.3531783)|Lei Zhang, Min Yang, Chengming Li, Ruifeng Xu|Harbin Institute of Technology, Shenzhen, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Sun Yat-sen University, Shenzhen, China; Shenzhen Institutes of Advanced Technology, The Chinese Academy of Sciences, Shenzhen, China|In this paper, we bridge the heterogeneity gap between different modalities and improve image-text retrieval by taking advantage of auxiliary image-to-text and text-to-image generative features with contrastive learning. Concretely, contrastive learning is devised to narrow the distance between the aligned image-text pairs and push apart the distance between the unaligned pairs from both inter- and intra-modality perspectives with the help of cross-modal retrieval features and auxiliary generative features. In addition, we devise a support-set regularization term to further improve contrastive learning by constraining the distance between each image/text and its corresponding cross-modal support-set information contained in the same semantic category. To evaluate the effectiveness of the proposed method, we conduct experiments on three benchmark datasets (i.e., MIRFLICKR-25K, NUS-WIDE, MS COCO). Experimental results show that our model significantly outperforms the strong baselines for cross-modal image-text retrieval. For reproducibility, we submit the code and data publicly at: \urlhttps://github.com/Hambaobao/CRCGS.|本文通过对比学习，利用图像到文本和文本到图像生成的辅助特征，弥补了不同检索方式之间的异质性差距，提高了图像-文本检索的性能。具体来说，对比学习是通过跨模态检索特征和辅助生成特征来缩小图像-文本对之间的距离，并从情态间和情态内的角度分离未对齐图像-文本对之间的距离。此外，我们设计了一个支持集正则化项来进一步改善对比学习，约束图像/文本之间的距离及其相应的跨模态支持集信息包含在同一语义范畴。为了评估该方法的有效性，我们对三个基准数据集(即 MIRFLICKR-25K，NUS-WIDE，MS COCO)进行了实验。实验结果表明，该模型在图像文本检索中的性能明显优于强基线检索。为了重现性，我们公开在 urlhttps:// github.com/hambaobao/crcgs 提交代码和数据。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Image-Text+Retrieval+via+Contrastive+Learning+with+Auxiliary+Generative+Features+and+Support-set+Regularization)|0|
|[Denoising Time Cycle Modeling for Recommendation](https://doi.org/10.1145/3477495.3531785)|Sicong Xie, Qunwei Li, Weidi Xu, Kaiming Shen, Shaohu Chen, Wenliang Zhong|Ant Group, Beijing, China; Ant Group, Hangzhou, China; Ant Group, Shanghai, China|Recently, modeling temporal patterns of user-item interactions have attracted much attention in recommender systems. We argue that existing methods ignore the variety of temporal patterns of user behaviors. We define the subset of user behaviors that are ir- relevant to the target item as noises, which limits the performance of target-related time cycle modeling and affect the recommendation performance. In this paper, we propose Denoising Time Cycle Modeling (DiCycle), a novel approach to denoise user behaviors and select the subset of user behaviors that are highly related to the target item. DiCycle is able to explicitly model diverse time cycle patterns for recommendation. Extensive experiments are conducted on both public benchmarks and a real-world dataset, demonstrating the superior performance of DiCycle over the state-of-the-art recommendation methods.|近年来，推荐系统中用户-项目交互的时间模式建模引起了人们的广泛关注。我们认为现有的方法忽略了用户行为的时间模式的多样性。将与目标项无关的用户行为定义为噪声，限制了目标相关时间周期建模的性能，影响了推荐性能。本文提出了一种新的去噪时间周期建模方法(DiCycle) ，用于去除用户行为的噪声，并选择与目标项高度相关的用户行为子集。DiCycle 能够显式地为推荐建模不同的时间周期模式。在公共基准测试和真实世界数据集上进行了广泛的实验，证明了 DiCycle 优于最先进的推荐方法的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Denoising+Time+Cycle+Modeling+for+Recommendation)|0|
|[Value Penalized Q-Learning for Recommender Systems](https://doi.org/10.1145/3477495.3531796)|Chengqian Gao, Ke Xu, Kuangqi Zhou, Lanqing Li, Xueqian Wang, Bo Yuan, Peilin Zhao||Scaling reinforcement learning (RL) to recommender systems (RS) is promising since maximizing the expected cumulative rewards for RL agents meets the objective of RS, i.e., improving customers' long-term satisfaction. A key approach to this goal is offline RL, which aims to learn policies from logged data rather than expensive online interactions. In this paper, we propose Value Penalized Q-learning (VPQ), a novel uncertainty-based offline RL algorithm that penalizes the unstable Q-values in the regression target using uncertainty-aware weights, achieving the conservative Q-function without the need of estimating the behavior policy, suitable for RS with a large number of items. Experiments on two real-world datasets show the proposed method serves as a gain plug-in for existing RS models.|由于推荐系统能够最大化推荐系统代理商的预期累积回报，从而达到推荐系统的目标，即提高客户的长期满意度，因此，将推荐系统扩展到推荐系统的强化学习是有前景的。实现这一目标的一个关键方法是离线 RL，它旨在从记录的数据中学习策略，而不是昂贵的在线交互。本文提出了一种新的基于不确定性的离线 RL 算法——价值惩罚 Q 学习算法(VPQ) ，该算法利用不确定性感知权值惩罚回归目标中不稳定的 Q 值，不需要估计行为策略就可以实现保守的 Q 函数，适用于大项目的 RS。在两个实际数据集上的实验表明，该方法可以作为现有 RS 模型的增益插件。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Value+Penalized+Q-Learning+for+Recommender+Systems)|0|
|[From Cluster Ranking to Document Ranking](https://doi.org/10.1145/3477495.3531819)|Egor Markovskiy, Fiana Raiber, Shoham Sabach, Oren Kurland|Technion, Haifa, Israel; Yahoo Research, Haifa, Israel|The common approach of using clusters of similar documents for ad hoc document retrieval is to rank the clusters in response to the query; then, the cluster ranking is transformed to document ranking. We present a novel supervised approach to transform cluster ranking to document ranking. The approach allows to simultaneously utilize different clusterings and the resultant cluster rankings; this helps to improve the modeling of the document similarity space. Empirical evaluation shows that using our approach results in performance that substantially transcends the state-of-the-art in cluster-based document retrieval.|使用类似文档集群进行特别文献检索的常见方法是根据查询对集群进行排序，然后将集群排序转换为文档排序。提出了一种新的监督方法将聚类排序转换为文档排序。该方法允许同时使用不同的聚类和由此产生的聚类排名; 这有助于改进文档相似性空间的建模。经验性的评估表明，使用我们的方法所产生的效果远远超过了基于集群的文献检索的最新水平。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Cluster+Ranking+to+Document+Ranking)|0|
|[ILMART: Interpretable Ranking with Constrained LambdaMART](https://doi.org/10.1145/3477495.3531840)|Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, Alberto Veneri|ISTI-CNR, Pisa, Italy; Ca' Foscari University of Venice & ISTI-CNR, Venice, Italy; Ca' Foscari University of Venice, Venice, Italy|Interpretable Learning to Rank (LtR) is an emerging field within the research area of explainable AI, aiming at developing intelligible and accurate predictive models. While most of the previous research efforts focus on creating post-hoc explanations, in this paper we investigate how to train effective and intrinsically-interpretable ranking models. Developing these models is particularly challenging and it also requires finding a trade-off between ranking quality and model complexity. State-of-the-art rankers, made of either large ensembles of trees or several neural layers, exploit in fact an unlimited number of feature interactions making them black boxes. Previous approaches on intrinsically-interpretable ranking models address this issue by avoiding interactions between features thus paying a significant performance drop with respect to full-complexity models. Conversely, ILMART, our novel and interpretable LtR solution based on LambdaMART, is able to train effective and intelligible models by exploiting a limited and controlled number of pairwise feature interactions. Exhaustive and reproducible experiments conducted on three publicly-available LtR datasets show that ILMART outperforms the current state-of-the-art solution for interpretable ranking of a large margin with a gain of nDCG of up to 8%.|可解释排序学习是可解释人工智能研究领域中的一个新兴领域，其目标是建立可理解的、准确的预测模型。以往的研究大多侧重于创建事后解释，本文主要研究如何训练有效且内在可解释的排序模型。开发这些模型尤其具有挑战性，而且还需要在排名质量和模型复杂性之间找到平衡。最先进的排名器，由大型的树木集合或几个神经层组成，实际上利用了无限数量的特征交互，使它们成为黑盒子。以前关于内在可解释的排名模型的方法通过避免特性之间的交互来解决这个问题，因此相对于全复杂模型来说，性能下降很大。相反，我们基于 lambdaMART 的新颖且可解释的 ILMART 有限公司解决方案，能够通过利用有限和可控数量的成对特征交互来培训有效和可理解的模型。在三个公开可用的有限责任公司数据集上进行的详尽和可重复的实验表明，ILMART 在可解释的大幅度排名方面优于目前的最先进的解决方案，nDCG 的增益高达8% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ILMART:+Interpretable+Ranking+with+Constrained+LambdaMART)|0|
|[On Extractive Summarization for Profile-centric Neural Expert Search in Academia](https://doi.org/10.1145/3477495.3531713)|Rennan C. Lima, Rodrygo L. T. Santos|Universidade Federal de Minas Gerais, Belo Horizonte, Brazil|Identifying academic experts is crucial for the progress of science, enabling researchers to connect, form networks, and collaborate on the most pressing research problems. A key challenge for ranking experts in response to a query is how to infer their expertise from the publications they coauthored. Profile-centric approaches represent candidate experts by concatenating all their publications into a text-based profile. Despite offering a complete picture of each candidate's scientific output, such lengthy profiles make it inefficient to leverage state-of-the-art neural architectures for inferring expertise. To overcome this limitation, we investigate the suitability of extractive summarization as a mechanism to reduce candidate profiles for semantic encoding using Transformers. Our thorough experiments with a representative academic search test collection demonstrate the benefits of encoding summarized profiles for an improved expertise inference.|识别学术专家对科学进步至关重要，使研究人员能够在最紧迫的研究问题上进行联系、形成网络和协作。对专家进行排名以回答问题的一个关键挑战是如何从他们合著的出版物中推断出他们的专业知识。以配置文件为中心的方法通过将候选专家的所有出版物连接到一个基于文本的配置文件中来代表他们。尽管提供了每个候选人的科学成果的完整图片，这样冗长的档案使得利用最先进的神经结构来推断专业知识效率低下。为了克服这个限制，我们研究了提取摘要作为使用 Transformers 减少语义编码候选配置文件的机制的适用性。我们对一个有代表性的学术搜索测试集合进行了彻底的实验，证明了对概要进行编码以改进专业知识推理的好处。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Extractive+Summarization+for+Profile-centric+Neural+Expert+Search+in+Academia)|0|
|[Joint Optimization of Ad Ranking and Creative Selection](https://doi.org/10.1145/3477495.3531855)|Kaiyi Lin, Xiang Zhang, Feng Li, Pengjie Wang, Qingqing Long, Hongbo Deng, Jian Xu, Bo Zheng|Alibaba Group, Beijing, China|In e-commerce, ad creatives play an important role in effectively delivering product information to users. The purpose of online creative selection is to learn users' preferences for ad creatives, and to select the most appealing design for users to maximize Click-Through Rate (CTR). However, the existing common practices in the industry usually place the creative selection after the ad ranking stage, and thus the optimal creative fails to reflect the influence on the ad ranking stage. To address these issues, we propose a novel Cascade Architecture of Creative Selection (CACS), which is built before the ranking stage to joint optimization of intra-ad creative selection and inter-ad ranking. To improve the efficiency, we design a classic two-tower structure and allow creative embeddings of the creative selection stage to share with the ranking stage. To boost the effectiveness, on the one hand, we propose a soft label list-wise ranking distillation method to distill the ranking knowledge from the ranking stage to guide CACS learning; and on the other hand, we also design an adaptive dropout network to encourage the model to probabilistically ignore ID features in favor of content features to learn multi-modal representations of the creative. Most of all, the ranking model obtains the optimal creative information of each ad from our CACS, and uses all available features to improve the performance of the ranking model. We have launched our solution in Taobao advertising platform and have obtained significant improvements both in offline and online evaluations.|在电子商务中，广告创意人员在有效地向用户传递产品信息方面发挥着重要作用。在线创意选择的目的是了解用户对广告创意的偏好，并为用户选择最具吸引力的设计，以最大限度地提高点进率。然而，现有的行业惯例通常将创意选择置于广告排名阶段之后，因此最优创意未能反映出对广告排名阶段的影响。为了解决这些问题，我们提出了一种新颖的创意选择级联体系结构(CACS) ，该体系结构建立在排名阶段之前，以联合优化内部广告创意选择和内部广告排名。为了提高效率，我们设计了一个经典的双塔结构，并允许创造性的嵌入创造性的选择阶段与排名阶段共享。为了提高效率，一方面，我们提出了一种软标签列表式的排序精馏方法，从排序阶段提取排序知识来指导 CACS 学习; 另一方面，我们还设计了一个自适应辍学网络来鼓励模型忽略 ID 特征而有利于内容特征学习创造性的多模态表示。最重要的是，排名模型从我们的 CACS 中获得每个广告的最佳创意信息，并利用所有可用的功能来改善排名模型的性能。我们已经在淘宝广告平台上推出了我们的解决方案，并且在线下和在线评估方面都取得了显著的进步。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Joint+Optimization+of+Ad+Ranking+and+Creative+Selection)|0|
|[Long Document Re-ranking with Modular Re-ranker](https://doi.org/10.1145/3477495.3531860)|Luyu Gao, Jamie Callan|Carnegie Mellon University, Pittsburgh, PA, USA|Long document re-ranking has been a challenging problem for neural re-rankers based on deep language models like BERT. Early work breaks the documents into short passage-like chunks. These chunks are independently mapped to scalar scores or latent vectors, which are then pooled into a final relevance score. These encode-and-pool methods however inevitably introduce an information bottleneck: the low dimension representations. In this paper, we propose instead to model full query-to-document interaction, leveraging the attention operation and modular Transformer re-ranker framework. First, document chunks are encoded independently with an encoder module. An interaction module then encodes the query and performs joint attention from the query to all document chunk representations. We demonstrate that the model can use this new degree of freedom to aggregate important information from the entire document. Our experiments show that this design produces effective re-ranking on two classical IR collections Robust04 and ClueWeb09, and a large-scale supervised collection MS-MARCO document ranking.|长文档重新排序一直是基于 BERT 等深层语言模型的神经网络重新排序的一个具有挑战性的问题。早期的工作将文档分解成短小的段落。这些块被独立映射到标量分数或潜在向量，然后将它们汇集到最终的相关分数中。然而，这些编码和池方法不可避免地引入了一个信息瓶颈: 低维表示。在本文中，我们提出了利用注意力操作和模块化的 formerre-rank 框架来建立完全的查询到文档的交互模型。首先，文档块使用编码器模块进行独立编码。然后，交互模块对查询进行编码，并执行从查询到所有文档块表示的联合注意。我们证明该模型可以使用这种新的自由度来聚合整个文档中的重要信息。我们的实验表明，这种设计产生了有效的重新排序的两个经典的红外收集鲁棒04和 ClueWeb09，以及一个大规模的监督收集 MS-MARCO 文件排序。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Long+Document+Re-ranking+with+Modular+Re-ranker)|0|
|[Relation Extraction as Open-book Examination: Retrieval-enhanced Prompt Tuning](https://doi.org/10.1145/3477495.3531746)|Xiang Chen, Lei Li, Ningyu Zhang, Chuanqi Tan, Fei Huang, Luo Si, Huajun Chen|Zhejiang University, Hangzhou, Chile; Alibaba Group, Hangzhou, China; Zhejiang University, Hangzhou, China|Pre-trained language models have contributed significantly to relation extraction by demonstrating remarkable few-shot learning abilities. However, prompt tuning methods for relation extraction may still fail to generalize to those rare or hard patterns. Note that the previous parametric learning paradigm can be viewed as memorization regarding training data as a book and inference as the close-book test. Those long-tailed or hard patterns can hardly be memorized in parameters given few-shot instances. To this end, we regard RE as an open-book examination and propose a new semiparametric paradigm of retrieval-enhanced prompt tuning for relation extraction. We construct an open-book datastore for retrieval regarding prompt-based instance representations and corresponding relation labels as memorized key-value pairs. During inference, the model can infer relations by linearly interpolating the base output of PLM with the non-parametric nearest neighbor distribution over the datastore. In this way, our model not only infers relation through knowledge stored in the weights during training but also assists decision-making by unwinding and querying examples in the open-book datastore. Extensive experiments on benchmark datasets show that our method can achieve state-of-the-art in both standard supervised and few-shot settings|预训练的语言模型通过显示出显著的短镜头学习能力，对关系抽取做出了重要贡献。然而，关系抽取的快速调优方法可能仍然无法推广到那些罕见的或难以实现的模式。请注意，以前的参数学习范式可以被视为记忆的训练数据作为一本书和推论作为关闭书测试。这些长尾或硬模式几乎不能被记忆在参数中，因为实例很少。为此，我们将 RE 视为一个开放式的考试，并提出了一个新的检索半参数范式——关系抽取的增强型提示调优。我们构造了一个开放式数据存储，用于检索基于提示的实例表示和相应的关系标签作为记忆的键值对。在推理过程中，该模型可以通过对 PLM 的基本输出与数据存储上的非参数最近邻分布进行线性插值来推断关系。这样，我们的模型不仅可以通过训练过程中权重中存储的知识来推断关系，而且可以通过展开和查询开卷数据库中的实例来辅助决策。对基准数据集的大量实验表明，该方法可以在标准监督和少镜头设置下达到最先进的水平|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Relation+Extraction+as+Open-book+Examination:+Retrieval-enhanced+Prompt+Tuning)|0|
|[End-to-end Distantly Supervised Information Extraction with Retrieval Augmentation](https://doi.org/10.1145/3477495.3531876)|Yue Zhang, Hongliang Fei, Ping Li|Baidu, Bellevue, WA, USA|Distant supervision (DS) has been a prevalent approach to generating labeled data for information extraction (IE) tasks. However, DS often suffers from noisy label problems, where the labels are extracted from the knowledge base (KB), regardless of the input context. Many efforts have been devoted to designing denoising mechanisms. However, most strategies are only designed for one specific task and cannot be directly adapted to other tasks. We propose a general paradigm (Dasiera) to resolve issues in KB-based DS. Labels from KB can be viewed as universal labels of a target entity or an entity pair. While the given context for an IE task may only contain partial/zero information about the target entities, or the entailed information may be vague. Hence the mismatch between the given context and KB labels, i.e., the given context has insufficient information to infer DS labels, can happen in IE training datasets. To solve the problem, during training, Dasiera leverages a retrieval-augmentation mechanism to complete missing information of the given context, where we seamlessly integrate a neural retriever and a general predictor in an end-to-end framework. During inference, we can keep/remove the retrieval component based on whether we want to predict solely on the given context. We have evaluated Dasiera on two IE tasks under the DS setting: named entity typing and relation extraction. Experimental results show Dasiera's superiority to other baselines in both tasks.|远程监控(DS)已经成为一种普遍的方法来为信息抽取(IE)任务生成标记数据。然而，DS 经常遇到噪声标签问题，这些标签是从知识库(KB)中提取出来的，与输入上下文无关。人们在设计去噪机制方面付出了很多努力。然而，大多数策略只针对一个特定任务设计，不能直接适应其他任务。我们提出了一个通用范例(Dasiera)来解决基于知识库的 DS 中的问题。来自 KB 的标签可以被视为目标实体或实体对的通用标签。IE 任务的给定上下文可能只包含关于目标实体的部分/零信息，或者所涉及的信息可能是模糊的。因此，给定上下文和知识库标签之间的不匹配，即给定上下文没有足够的信息来推断 DS 标签，可能发生在 IE 训练数据集中。为了解决这个问题，在训练期间，Dasiera 利用检索增强机制来完成给定上下文的缺失信息，在这里我们无缝地将神经检索器和通用预测器集成在一个端到端框架中。在推理过程中，我们可以保留/删除检索组件，这取决于我们是否希望仅根据给定的上下文进行预测。我们在 DS 设置下对 Dasiera 的两个 IE 任务进行了评估: 命名实体类型和关系提取。实验结果表明，Dasiera 在这两个任务中都优于其他基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=End-to-end+Distantly+Supervised+Information+Extraction+with+Retrieval+Augmentation)|0|
|[Assessing Scientific Research Papers with Knowledge Graphs](https://doi.org/10.1145/3477495.3531879)|Kexuan Sun, Zhiqiang Qiu, Abel Salinas, Yuzhong Huang, DongHo Lee, Daniel Benjamin, Fred Morstatter, Xiang Ren, Kristina Lerman, Jay Pujara|Nova Southeastern University, Fort Lauderdale, CA, USA; University of Southern California, Los Angeles, CA, USA|In recent decades, the growing scale of scientific research has led to numerous novel findings. Reproducing these findings is the foundation of future research. However, due to the complexity of experiments, manually assessing scientific research is laborious and time-intensive, especially in social and behavioral sciences. Although increasing reproducibility studies have garnered increased attention in the research community, there is still a lack of systematic ways for evaluating scientific research at scale. In this paper, we propose a novel approach towards automatically assessing scientific publications by constructing a knowledge graph (KG) that captures a holistic view of the research contributions. Specifically, during the KG construction, we combine information from two different perspectives: micro-level features that capture knowledge from published articles such as sample sizes, effect sizes, and experimental models, and macro-level features that comprise relationships between entities such as authorship and reference information. We then learn low-dimensional representations using language models and knowledge graph embeddings for entities (nodes in KGs), which are further used for the assessments. A comprehensive set of experiments on two benchmark datasets shows the usefulness of leveraging KGs for scoring scientific research.|近几十年来，科学研究的规模不断扩大，产生了许多新的发现。复制这些发现是未来研究的基础。然而，由于实验的复杂性，人工评估科学研究是费时费力的，尤其是在社会科学和行为科学领域。虽然越来越多的重复性研究已经引起了研究界越来越多的关注，但仍然缺乏系统的方法来评价科学研究的规模。在本文中，我们提出了一种新的方法来自动评估科学出版物，通过构造一个知识图(KG) ，捕获了研究贡献的整体观点。具体而言，在 KG 构建过程中，我们从两个不同的角度组合信息: 从已发表的文章(如样本量，效应量和实验模型)中捕获知识的微观层面特征，以及包含实体(如作者和参考信息)之间关系的宏观层面特征。然后，我们学习低维表示使用语言模型和知识图嵌入的实体(节点在幼稚园) ，这是进一步用于评估。在两个基准数据集上的一组综合实验表明了利用幼儿园评分科学研究的有用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Assessing+Scientific+Research+Papers+with+Knowledge+Graphs)|0|
|[A Content Recommendation Policy for Gaining Subscribers](https://doi.org/10.1145/3477495.3531885)|Konstantinos Theocharidis, Manolis Terrovitis, Spiros Skiadopoulos, Panagiotis Karras|University of the Peloponnese, Tripoli, Greece; Aarhus University, Aarhus, Denmark; Information Management Systems Institute, Athena Research Center, Athens, Greece; University of the Peloponnese & Information Management Systems Institute, Athena Research Center, Tripoli & Athens, Greece|How can we recommend content for a brand agent to use over a series of rounds so as to gain new subscribers to its social network page? The Influence Maximization (IM) problem seeks a set of~k users, and its content-aware variants seek a set of~k post features, that achieve, in both cases, an objective of expected influence in a social network. However, apart from raw influence, it is also relevant to study gain in subscribers, as long-term success rests on the subscribers of a brand page; classic IM may select~k users from the subscriber set, and content-aware IM starts the post's propagation from that subscriber set. In this paper, we propose a novel content recommendation policy to a brand agent for Gaining Subscribers by Messaging (GSM) over many rounds. In each round, the brand agent messages a fixed number of social network users and invites them to visit the brand page aiming to gain their subscription, while its most recently published content consists of features that intensely attract the preferences of the invited users. To solve GSM, we find, in each round, which content features to publish and which users to notify aiming to maximize the cumulative subscription gain over all rounds. We deploy three GSM solvers, named \sR, \sSC, and \sSU, and we experimentally evaluate their performance based on VKontakte (VK) posts by considering different user sets and feature sets. Our experimental results show that \sSU provides the best solution, as it is significantly more efficient than \sSC with a minor loss of efficacy and clearly more efficacious than \sR with competitive efficiency.|我们怎样才能向品牌代理商推荐一系列的内容，从而吸引新的用户访问其社交网络页面？影响力最大化(IM)问题寻找一组 ~ k 用户，其内容感知变体寻找一组 ~ k 帖子特征，在这两种情况下，都实现了社交网络中预期影响力的目标。然而，除了原始的影响力之外，研究订阅者的收益也是相关的，因为长期的成功取决于品牌页面的订阅者; 传统的 IM 可能从订阅者集合中选择 ~ k 用户，而内容感知的 IM 从订阅者集合中开始发布信息。在本文中，我们提出了一个新的内容推荐策略的品牌代理商获得用户的消息(GSM)多轮。在每一轮中，品牌代理人给固定数量的社交网络用户发信息，并邀请他们访问品牌页面以获得订阅，而其最近发布的内容包括强烈吸引受邀用户偏好的功能。为了解决 GSM 问题，我们发现，在每一轮中，哪些内容特性要发布，哪些用户要通知，目的是在所有轮次中最大化累积订阅收益。我们部署了三个 GSM 解决方案，分别命名为 sR、 sSC 和 sSU，通过考虑不同的用户集和特性集，实验性地评估了它们基于 VKontakte (VK)帖子的性能。我们的实验结果表明，sSU 提供了最佳的解决方案，因为它明显地比 sSC 更有效，具有较小的效率损失，并且明显地比具有竞争效率的 sR 更有效。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Content+Recommendation+Policy+for+Gaining+Subscribers)|0|
|[MM-Rec: Visiolinguistic Model Empowered Multimodal News Recommendation](https://doi.org/10.1145/3477495.3531896)|Chuhan Wu, Fangzhao Wu, Tao Qi, Chao Zhang, Yongfeng Huang, Tong Xu|Microsoft Research Asia, Beijing, China; Shandong University, Jinan, China; University of Science and Technology of China, Hefei, China; Tsinghua University, Beijing, China|News representation is critical for news recommendation. Most existing methods learn news representations only from news texts while ignoring the visual information of news. In fact, users may click news not only due to the interest in news titles but also the attraction of news images. Thus, images are useful for representing news and predicting news clicks. Pretrained visiolinguistic models are powerful in multi-modal understanding, which can represent news from both textual and visual contents. In this paper, we propose a multimodal news recommendation method that can incorporate both textual and visual information of news to learn multimodal news representations. We first extract region-of-interests (ROIs) from news images via object detection. We then use a pre-trained visiolinguistic model to encode both news texts and image ROIs and model their inherent relatedness using co-attentional Transformers. In addition, we propose a crossmodal candidate-aware attention network to select relevant historical clicked news for the accurate modeling of user interest in candidate news. Experiments validate that incorporating multimodal news information can effectively improve the performance of news recommendation.|新闻表达对新闻推荐至关重要。现有的大多数方法只从新闻文本中学习新闻表征，而忽视了新闻的视觉信息。事实上，用户之所以会点击新闻，不仅是因为他们对新闻标题感兴趣，还因为新闻图片的吸引力。因此，图像对于表示新闻和预测新闻点击是非常有用的。预先训练的视觉语言模型在多模态理解中具有很强的表现能力，可以从文本和视觉两个方面表现新闻。在本文中，我们提出了一种多通道新闻推荐方法，它可以结合新闻的文本信息和视觉信息来学习多通道新闻表示。我们首先通过目标检测从新闻图像中提取感兴趣区域(ROI)。然后，我们使用一个预先训练的视觉语言学模型来编码新闻文本和图像 ROI，并使用共注意转换器来模拟它们之间的内在联系。此外，我们提出了一个跨模式的候选人感知注意网络来选择相关的历史点击新闻，以准确建模用户对候选人新闻的兴趣。实验证明，融合多模态新闻信息可以有效地提高新闻推荐的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MM-Rec:+Visiolinguistic+Model+Empowered+Multimodal+News+Recommendation)|0|
|[Towards Personalized Bundle Creative Generation with Contrastive Non-Autoregressive Decoding](https://doi.org/10.1145/3477495.3531909)|Penghui Wei, Shaoguo Liu, Xuanhua Yang, Liang Wang, Bo Zheng|Alibaba Group, Beijing, China|Current bundle generation studies focus on generating a combination of items to improve user experience. In real-world applications, there is also a great need to produce bundle creatives that consist of mixture types of objects (e.g., items, slogans and templates) for achieving better promotion effect. We study a new problem named bundle creative generation: for given users, the goal is to generate personalized bundle creatives that the users will be interested in. To take both quality and efficiency into account, we propose a contrastive non-autoregressive model that captures user preferences with ingenious decoding objective. Experiments on large-scale real-world datasets verify that our proposed model shows significant advantages in terms of creative quality and generation speed.|当前的捆绑包生成研究侧重于生成项目的组合，以改善用户体验。在实际应用中，为了达到更好的促销效果，还需要产生由混合类型的对象(例如，项目、标语和模板)组成的捆绑创意。我们研究了一个新的问题——捆绑包创意生成: 对于给定的用户，目标是生成用户感兴趣的个性化捆绑包创意。为了同时考虑质量和效率，我们提出了一个对比的非自回归模型，捕捉用户的喜好与巧妙的解码目标。在大规模真实世界数据集上的实验表明，我们提出的模型在创新质量和生成速度方面具有显著的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Personalized+Bundle+Creative+Generation+with+Contrastive+Non-Autoregressive+Decoding)|0|
|[Another Look at Information Retrieval as Statistical Translation](https://doi.org/10.1145/3477495.3531717)|Yuqi Liu, Chengcheng Hu, Jimmy Lin|University of Waterloo, Waterloo, ON, Canada|Over two decades ago, Berger and Lafferty proposed "information retrieval as statistical translation" (IRST), a simple and elegant method for ad hoc retrieval based on the noisy channel model. At the time, they lacked the large-scale human-annotated datasets necessary to properly train their models. In this paper, we ask the simple question: What if Berger and Lafferty had access to datasets such as the MS MARCO passage ranking dataset that we take for granted today? The answer to this question tells us how much of recent improvements in ranking can be solely attributed to having more data available, as opposed to improvements in models (e.g., pretrained transformers) and optimization techniques (e.g., contrastive loss). In fact, Boytsov and Kolter recently began to answer this question with a replication of Berger and Lafferty's model, and this work can be viewed as another independent replication effort, with generalizations to additional conditions not previously explored, including replacing the sum of translation probabilities with ColBERT's MaxSim operator. We confirm that while neural models (particularly pretrained transformers) have indeed led to great advances in retrieval effectiveness, the IRST model proposed decades ago is quite effective if provided sufficient training data.|二十多年前，伯杰和拉弗蒂提出了“信息检索作为统计翻译”(IRST) ，这是一种基于噪声信道模型的简单而优雅的自组织检索方法。当时，他们缺乏必要的大规模人工注释数据集来适当地训练他们的模型。在本文中，我们提出一个简单的问题: 如果 Berger 和 Lafferty 能够访问数据集，比如我们今天认为理所当然的 MS MARCO 通道排名数据集，那会怎样？这个问题的答案告诉我们，最近排名的改善在多大程度上可以完全归因于有更多的数据可用，而不是模型(例如，预先训练的变压器)和优化技术(例如，对比损失)的改进。事实上，Boytsov 和 Kolter 最近开始通过复制 Berger 和 Lafferty 的模型来回答这个问题，这项工作可以被看作是另一个独立的复制努力，对以前没有探索过的其他条件进行推广，包括用 ColBERT 的 MaxSim 运算符替换翻译概率的总和。我们证实，虽然神经模型(特别是预先训练的变压器)确实导致了检索效率的巨大进步，几十年前提出的 IRST 模型是相当有效的，如果提供足够的训练数据。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Another+Look+at+Information+Retrieval+as+Statistical+Translation)|0|
|[ACORDAR: A Test Collection for Ad Hoc Content-Based (RDF) Dataset Retrieval](https://doi.org/10.1145/3477495.3531729)|Tengteng Lin, Qiaosheng Chen, Gong Cheng, Ahmet Soylu, Basil Ell, Ruoqi Zhao, Qing Shi, Xiaxia Wang, Yu Gu, Evgeny Kharlamov|Bosch Center for Artificial Intelligence & University of Oslo, Renningen, Germany; Nanjing University, Nanjing, China; Bielefeld University & University of Oslo, Bielefeld, Germany; OsloMet -- Oslo Metropolitan University & Norwegian University of Science and Technology, Oslo, Norway; The Ohio State University, Columbus, OH, USA|Ad hoc dataset retrieval is a trending topic in IR research. Methods and systems are evolving from metadata-based to content-based ones which exploit the data itself for improving retrieval accuracy but thus far lack a specialized test collection. In this paper, we build and release the first test collection for ad hoc content-based dataset retrieval, where content-oriented dataset queries and content-based relevance judgments are annotated by human experts who are assisted with a dashboard designed specifically for comprehensively and conveniently browsing both the metadata and data of a dataset. We conduct extensive experiments on the test collection to analyze its difficulty and provide insights into the underlying task.|自组织数据集检索是信息检索领域的一个研究热点。方法和系统正在从基于元数据向基于内容的方法和系统演变，这些方法和系统利用数据本身来提高检索的准确性，但迄今为止还缺乏专门的测试集合。本文构建并发布了第一个基于特定内容的数据集检索测试集合，其中面向内容的数据集查询和基于内容的相关性判断由人类专家进行注释，并辅以一个专门为全面方便地浏览数据集的元数据和数据而设计的仪表板。我们进行了广泛的实验测试收集，以分析其难度，并提供深入了解潜在的任务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ACORDAR:+A+Test+Collection+for+Ad+Hoc+Content-Based+(RDF)+Dataset+Retrieval)|0|
|[RELISON: A Framework for Link Recommendation in Social Networks](https://doi.org/10.1145/3477495.3531730)|Javier SanzCruzado, Pablo Castells|University of Glasgow, Glasgow, United Kingdom; Universidad Autónoma de Madrid, Madrid, Spain|Link recommendation is an important and compelling problem at the intersection of recommender systems and online social networks. Given a user, link recommenders identify people in the platform the user might be interested in interacting with. We present RELISON, an extensible framework for running link recommendation experiments. The library provides a wide range of algorithms, along with tools for evaluating the produced recommendations. RELISON includes algorithms and metrics that consider the potential effect of recommendations on the properties of online social networks. For this reason, the library also implements network structure analysis metrics, community detection algorithms, and network diffusion simulation functionalities. The library code and documentation is available at https://github.com/ir-uam/RELISON.|在推荐系统和在线社交网络的交叉点上，链接推荐是一个重要且引人注目的问题。给定一个用户，链接推荐器会识别出平台中用户可能感兴趣的交互对象。我们提出了 RELISON，一个运行链路推荐实验的可扩展框架。该库提供了广泛的算法，以及用于评估生成的建议的工具。RELISON 包括一些算法和度量标准，这些算法和度量标准考虑了推荐对在线社交网络属性的潜在影响。出于这个原因，该库还实现了网络结构分析度量、社区检测算法和网络扩散模拟功能。图书馆代码及文件可于 https://github.com/ir-uam/relison 索取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RELISON:+A+Framework+for+Link+Recommendation+in+Social+Networks)|0|
|[The Istella22 Dataset: Bridging Traditional and Neural Learning to Rank Evaluation](https://doi.org/10.1145/3477495.3531740)|Domenico Dato, Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto|University of Glasgow, Glasgow, United Kingdom; ISTI-CNR, Pisa, Italy; University of Pisa, Pisa, Italy; Istella, Milano, Italy|Neural approaches that use pre-trained language models are effective at various ranking tasks, such as question answering and ad-hoc document ranking. However, their effectiveness compared to feature-based Learning-to-Rank (LtR) methods has not yet been well-established. A major reason for this is because present LtR benchmarks that contain query-document feature vectors do not contain the raw query and document text needed for neural models. On the other hand, the benchmarks often used for evaluating neural models, e.g., MS MARCO, TREC Robust, etc., provide text but do not provide query-document feature vectors. In this paper, we present Istella22, a new dataset that enables such comparisons by providing both query/document text and strong query-document feature vectors used by an industrial search engine. The dataset consists of a comprehensive corpus of 8.4M web documents, a collection of query-document pairs including 220 hand-crafted features, relevance judgments on a 5-graded scale, and a set of 2,198 textual queries used for testing purposes. Istella22 enables a fair evaluation of traditional learning-to-rank and transfer ranking techniques on the same data. LtR models exploit the feature-based representations of training samples while pre-trained transformer-based neural rankers can be evaluated on the corresponding textual content of queries and documents. Through preliminary experiments on Istella22, we find that neural re-ranking approaches lag behind LtR models in terms of effectiveness. However, LtR models identify the scores from neural models as strong signals.|使用预训练语言模型的神经网络方法可以有效地完成各种排序任务，例如问题回答和即席文档排序。然而，与基于特征的学习到等级(LT)方法相比，它们的有效性还没有得到很好的证实。其中一个主要原因是，现有的包含查询文档特征向量的 LITR 基准测试不包含神经模型所需的原始查询和文档文本。另一方面，常用于评估神经模型的基准，如 MS MARCO、 TREC 鲁棒性等，提供文本但不提供查询文档特征向量。在本文中，我们介绍了新的数据集 Istella22，它通过提供工业搜索引擎使用的查询/文档文本和强查询-文档特征向量来实现这种比较。该数据集包括840万份网络文档的综合语料库、包括220个手工制作的功能的查询-文档对的集合、5级量表的相关性判断，以及用于测试目的的2198个文本查询。Istella22可以对传统的学习排序和转移排序技术在相同数据上进行公平的评估。LTR 模型利用训练样本的基于特征的表示，而预训练的基于变压器的神经排序器可以根据查询和文档的相应文本内容进行评估。通过在 Istella22上的初步实验，我们发现神经重新排序方法在有效性方面落后于 LTR 模型。然而，LTR 模型将神经模型的分数识别为强信号。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Istella22+Dataset:+Bridging+Traditional+and+Neural+Learning+to+Rank+Evaluation)|0|
|[Axiomatic Retrieval Experimentation with ir_axioms](https://doi.org/10.1145/3477495.3531743)|Alexander Bondarenko, Maik Fröbe, Jan Heinrich Reimer, Benno Stein, Michael Völske, Matthias Hagen|Martin-Luther-Universität Halle-Wittenberg, Halle, Germany; Bauhaus-Universität Weimar, Weimar, Germany|Axiomatic approaches to information retrieval have played a key role in determining basic constraints that characterize good retrieval models. Beyond their importance in retrieval theory, axioms have been operationalized to improve an initial ranking, to "guide" retrieval, or to explain some model's rankings. However, recent open-source retrieval frameworks like PyTerrier and Pyserini, which made it easy to experiment with sparse and dense retrieval models, have not included any retrieval axiom support so far. To fill this gap, we propose ir_axioms, an open-source Python framework that integrates retrieval axioms with common retrieval frameworks. We include reference implementations for 25 retrieval axioms, as well as components for preference aggregation, re-ranking, and evaluation. New axioms can easily be defined by implementing an abstract data type or by intuitively combining existing axioms with Python operators or regression. Integration with PyTerrier and ir_datasets makes standard retrieval models, corpora, topics, and relevance judgments---including those used at TREC---immediately accessible for axiomatic experimentation. Our experiments on the TREC Deep Learning tracks showcase some potential research questions that ir_axioms can help to address.|公理化的信息检索检索方法在确定优秀检索模型的基本约束条件方面发挥了关键作用。除了在检索理论中的重要性，公理已经被用来改进初始排名、“指导”检索或解释某些模型的排名。然而，最近的开放源代码检索框架，如 PyTerrier 和 Pyserini，使得对稀疏和密集的检索模型进行试验变得容易，到目前为止还没有包含任何检索公理支持。为了填补这个空白，我们提出 ir _ xioms，这是一个开放源码的 Python 框架，它将检索公理与通用检索框架集成在一起。我们包括用于25个检索公理的参考实现，以及用于偏好聚合、重新排序和评估的组件。通过实现一个抽象数据类型或直观地将现有公理与 Python 运算符或回归相结合，可以很容易地定义新公理。与 PyTerrier 和 ir _ data 集合的集成使得标准检索模型、语料库、主题和相关性判断——包括 TREC 使用的那些——可以立即用于公理化实验。我们在 TREC 深度学习轨道上的实验展示了一些 ir _ axioms 可以帮助解决的潜在研究问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Axiomatic+Retrieval+Experimentation+with+ir_axioms)|0|
|[Knowledge Graph Question Answering Datasets and Their Generalizability: Are They Enough for Future Research?](https://doi.org/10.1145/3477495.3531751)|Longquan Jiang, Ricardo Usbeck|University Hamburg, Hamburg, Germany|Existing approaches on Question Answering over Knowledge Graphs (KGQA) have weak generalizability. That is often due to the standard i.i.d. assumption on the underlying dataset. Recently, three levels of generalization for KGQA were defined, namely i.i.d., compositional, zero-shot. We analyze 25 well-known KGQA datasets for 5 different Knowledge Graphs (KGs). We show that according to this definition many existing and online available KGQA datasets are either not suited to train a generalizable KGQA system or that the datasets are based on discontinued and out-dated KGs. Generating new datasets is a costly process and, thus, is not an alternative to smaller research groups and companies. In this work, we propose a mitigation method for re-splitting available KGQA datasets to enable their applicability to evaluate generalization, without any cost and manual effort. We test our hypothesis on three KGQA datasets, i.e., LC-QuAD, LC-QuAD 2.0 and QALD-9). Experiments on re-splitted KGQA datasets demonstrate its effectiveness towards generalizability. The code and a unified way to access 18 available datasets is online at https://github.com/semantic-systems/KGQA-datasets as well as https://github.com/semantic-systems/KGQA-datasets-generalization.|现有的知识图问答方法具有较弱的泛化能力。这通常是由于基础数据集上的标准 i.id 假设造成的。最近，定义了 KGQA 的三个推广水平，即标识、合成和零拍。我们分析了5个不同的知识图表(KG)的25个著名的 KGQA 数据集。我们表明，根据这个定义，许多现有的和在线可用的 KGQA 数据集要么不适合训练一个可推广的 KGQA 系统，要么数据集基于不连续的和过时的 KG。生成新的数据集是一个昂贵的过程，因此，不能替代较小的研究团体和公司。在这项工作中，我们提出了一个缓解方法，重新分裂可用的 KGQA 数据集，使其适用性评估一般化，没有任何成本和人工的努力。我们在三个 KGQA 数据集上检验我们的假设，即 LC-QuAD，LC-QuAD 2.0和 QALD-9)。通过对 KGQA 数据集的重新分割实验，证明了该算法的有效性。该代码和一个统一的方式访问18个可用的数据集是在线的 https://github.com/semantic-systems/kgqa-datasets 和 https://github.com/semantic-systems/kgqa-datasets-generalization。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Graph+Question+Answering+Datasets+and+Their+Generalizability:+Are+They+Enough+for+Future+Research?)|0|
|[Golden Retriever: A Real-Time Multi-Modal Text-Image Retrieval System with the Ability to Focus](https://doi.org/10.1145/3477495.3531666)|Florian Schneider, Chris Biemann|Universität Hamburg, Hamburg, Germany|In this work, we present the Golden Retriever, a system leveraging state-of-the-art visio-linguistic models (VLMs) for real-time text-image retrieval. The unique feature of our system is that it can focus on words contained in the textual query, i.e., locate and high-light them within retrieved images. An efficient two-stage process implements real-time capability and the ability to focus. Therefore, we first drastically reduce the number of images processed by a VLM. Then, in the second stage, we rank the images and highlight the focussed word using the outputs of a VLM. Further, we introduce a new and efficient algorithm based on the idea of TF-IDF to retrieve images for short textual queries. One of multiple use cases where we employ the Golden Retriever is a language learner scenario, where visual cues for "difficult" words within sentences are provided to improve a user's reading comprehension. However, since the backend is completely decoupled from the frontend, the system can be integrated into any other application where images must be retrieved fast. We demonstrate the Golden Retriever with screenshots of a minimalistic user interface.|在这项工作中，我们介绍了金毛寻回犬，一个利用最先进的视觉语言模型(vlm)进行实时文本图像检索的系统。我们的系统的独特之处在于它可以专注于文本查询中包含的单词，也就是说，在检索到的图像中定位并高亮显示它们。一个有效的两阶段过程实现实时能力和集中能力。因此，我们首先大幅度减少图像处理的 VLM 数量。然后，在第二阶段，我们使用 VLM 的输出对图像进行排序并突出显示聚焦词。在此基础上，提出了一种基于 TF-IDF 思想的短文本查询图像检索算法。我们使用金毛寻回犬的多个用例之一是语言学习者场景，其中提供句子中“难”词的视觉提示，以提高用户的阅读理解。但是，由于后端与前端完全解耦，因此系统可以集成到任何其他必须快速检索图像的应用程序中。我们用一个极简的用户界面截图来演示这个金毛寻回犬。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Golden+Retriever:+A+Real-Time+Multi-Modal+Text-Image+Retrieval+System+with+the+Ability+to+Focus)|0|
|[ZeroMatcher: A Cost-Off Entity Matching System](https://doi.org/10.1145/3477495.3531661)|Congcong Ge, Xiaocan Zeng, Lu Chen, Yunjun Gao|Zhejiang University, Ningbo, China; Zhejiang University, Hangzhou, China|Entity Matching (EM) aims to find data instances from different sources that refer to the same real-world entity. The existing EM techniques can be either costly or tailored for a specific data type. We present ZeroMatcher, a cost-off entity matching system, which supports (i) handling EM tasks with different data types, including relational tables and knowledge graphs; (ii) keeping its EM performance always competitive by enabling the sub-modules to be updated in a lightweight manner, thus reducing development costs; and (iii) performing EM without human annotations to further slash the labor costs. First, ZeroMatcher automatically suggests users a set of appropriate modules for EM according to the data types of the input datasets. Users could specify the modules for the subsequent EM process according to their preferences. Alternatively, users are able to customize the modules of ZeroMatcher. Then, the system proceeds to the EM task, where users can track the entire EM process and monitor the memory usage changes in real-time. When the EM process is completed, ZeroMatcher visualizes the EM results from different aspects to ease the understanding for users. Finally, ZeroMatcher provides EM results evaluation, enabling users to compare the effectiveness among different parameter settings.|实体匹配(Entity Matching，EM)的目标是从引用相同实体的不同数据源中找到数据实例。现有的 EM 技术要么成本高昂，要么针对特定的数据类型进行量身定制。我们提出了 ZeroMatcher，一个成本实体匹配系统，它支持(i)处理不同数据类型的 EM 任务，包括关系表和知识图; (ii)保持其 EM 性能始终具有竞争力，使子模块以轻量级的方式更新，从而降低开发成本; 以及(iii)执行 EM 而不需要人工注释，以进一步降低劳动力成本。首先，ZeroMatcher 根据输入数据集的数据类型自动为用户建议一组适合 EM 的模块。用户可以根据自己的偏好为后续 EM 过程指定模块。或者，用户可以自定义 ZeroMatcher 的模块。然后，系统继续执行 EM 任务，在这个任务中，用户可以跟踪整个 EM 进程并实时监视内存使用的变化。当 EM 过程完成后，ZeroMatcher 从不同方面可视化 EM 结果，以便于用户理解。最后，ZeroMatcher 提供 EM 结果评估，使用户能够比较不同参数设置之间的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ZeroMatcher:+A+Cost-Off+Entity+Matching+System)|0|
|[QFinder: A Framework for Quantity-centric Ranking](https://doi.org/10.1145/3477495.3531672)|Satya Almasian, Milena Bruseva, Michael Gertz|Heidelberg University, Heidelberg, Germany|Quantities shape our understanding of measures and values, and they are an important means to communicate the properties of objects. Often, search queries contain numbers as retrieval units, e.g., "iPhone that costs less than 800 Euros''. Yet, modern search engines lack a proper understanding of numbers and units. In queries and documents, search engines handle them as normal keywords and therefore are ignorant of relative conditions between numbers, such as greater than or less than, or, more generally, the numerical proximity of quantities. In this work, we demonstrate QFinder, our quantity-centric framework for ranking search results for queries with quantity constraints. We also open-source our new ranking method as an Elasticsearch plug-in for future use. Our demo is available at: https://qfinder.ifi.uni-heidelberg.de/|数量塑造了我们对度量和价值的理解，它们是沟通对象属性的重要手段。通常，搜索查询包含数字作为检索单位，例如，“成本低于800欧元的 iPhone”。然而，现代搜索引擎缺乏对数字和单位的正确理解。在查询和文档中，搜索引擎将它们作为普通关键字处理，因此不知道数字之间的相对条件，例如大于或小于，或者更一般地说，数量的数值接近性。在这项工作中，我们展示了 QFinder，我们的数量为中心的框架排序查询搜索结果的数量约束。我们还将我们的新排名方法作为一个 Elasticsearch 插件开源以供将来使用。我们的演示可以在 https://qfinder.ifi.uni-heidelberg.de/下载|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=QFinder:+A+Framework+for+Quantity-centric+Ranking)|0|
|[CHERCHE: A New Tool to Rapidly Implement Pipelines in Information Retrieval](https://doi.org/10.1145/3477495.3531695)|Raphaël Sourty, José G. Moreno, Lynda Tamine, FrançoisPaul Servant|Renault, Boulogne-Billancourt, France; Université Paul Sabatier, IRIT & Renault, Toulouse, France; Université Paul Sabatier, IRIT, Toulouse, France|In this demo paper, we present a new open-source python module for building information retrieval pipelines with transformers namely CHERCHE. Our aim is to propose an easy to plug tool capable to execute, simple but strong, state-of-the-art information retrieval models. To do so, we have integrated classical models based on lexical matching but also recent models based on semantic matching. Indeed, a large number of models available on public hubs can be now tested on information retrieval tasks with only a few lines. CHERCHE is oriented to newcomers into the neural information retrieval field that want to use transformer-based models in small collections without struggling with heavy tools. The code and documentation of CHERCHE is public available at https://github.com/raphaelsty/cherche|在本演示文件中，我们提出了一个新的开源 python 模块，用于建设带有变压器的信息检索管道，即 CHERCHE。我们的目标是提出一个容易插入的工具，能够执行，简单但强大，国家的最先进的信息检索模型。为此，我们集成了基于词汇匹配的经典模型和基于语义匹配的最新模型。事实上，在公共集线器上提供的大量模型，现在只需几行代码就可以在信息检索任务上进行测试。CHERCHE 面向的是神经信息检索领域的新手，他们希望在小型集合中使用基于变压器的模型，而无需使用笨重的工具。CHERCHE 的代码和文档可在 https://github.com/raphaelsty/CHERCHE 查阅|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CHERCHE:+A+New+Tool+to+Rapidly+Implement+Pipelines+in+Information+Retrieval)|0|
|[Arm: Efficient Learning of Neural Retrieval Models with Desired Accuracy by Automatic Knowledge Amalgamation](https://doi.org/10.1145/3477495.3531664)|Linzhu Yu, Dawei Jiang, Ke Chen, Lidan Shou|Zhejiang University, Hangzhou, China|In recent years, there has been increasing interest in adopting published neural retrieval models learned from corpora for text retrieval. Although these models achieve excellent retrieval performance, in terms of popular accuracy metrics, on datasets they have been trained, their performance on new text data might degrade. To obtain the desired retrieval performance on both the data used in training and the latest data collected after training, the simple approach of learning a new model from both datasets is not always feasible since the annotated dataset used in training is often not published along with the learned model. Knowledge amalgamation (KA) is an emerging technique to deal with this problem of inaccessibility of data used in previous training. KA learns a new model (called a student model) from new data by reusing (called amalgamating) a number of trained models (called teacher models) instead of accessing the teachers' original training data. However, in order to efficiently learn an accurate student model, the classical KA approach requires manual selection of an appropriate subset of teacher models for amalgamation. This manual procedure for selecting teacher models prevents the classical KA from being scaled to retrieval tasks for which a large number of candidate teacher models are ready to be reused. This paper presents Arm, an intelligent system for efficiently learning a neural retrieval model with the desired accuracy on incoming data by automatically amalgamating a subset of teacher models (called a teacher model combination or simply combination ) among a large number of teacher models. o filter combinations that fail to produce accurate student models, Arm employs Bayesian optimization to derive an accuracy prediction model based on sampled amalgamation tasks. Then, Arm uses the derived prediction model to exclude unqualified combinations without training the rest combinations. To speed up training, Arm introduces a cost model that picks the teacher model combination with the minimal training cost among all qualified teacher model combinations to produce the final student model. This paper will demonstrate the major workflow of Arm and present the produced student models to users.|近年来，从语料库中学习的已发表的神经检索模型越来越多地被用于文本检索。尽管这些模型在它们已经训练过的数据集上获得了优异的检索性能，但是它们在新文本数据上的性能可能会下降。为了获得对训练中使用的数据和训练后收集的最新数据的期望检索性能，从两个数据集中学习新模型的简单方法并不总是可行的，因为训练中使用的注释数据集通常不与学习模型一起发布。知识融合(KA)是一种新兴的技术，以解决这一问题的数据无法访问以往的训练使用。KA 从新的数据中学习新的模型(称为学生模型) ，方法是重用(称为合并)一些经过训练的模型(称为教师模型) ，而不是访问教师的原始培训数据。然而，为了有效地学习一个准确的学生模型，经典的 KA 方法需要手动选择合适的教师模型子集进行合并。这个选择教师模型的手工程序阻止了经典的 KA 被缩放到检索任务，其中大量的候选教师模型可以被重用。本文提出了一个智能系统 Arm，它通过在大量的教师模型中自动合并一个教师模型子集(称为教师模型组合或简单组合)来有效地学习一个神经检索模型，该模型对输入数据具有期望的准确性。O 过滤器组合不能产生准确的学生模型，Arm 使用贝叶斯优化得到一个基于采样融合任务的精度预测模型。然后，利用导出的预测模型排除不合格组合，而不对剩余组合进行训练。为了加速培训，Arm 引入了一个成本模型，在所有合格的教师模型组合中选择教师模型组合和最小的培训成本，从而产生最终的学生模型。本文将演示 Arm 的主要工作流程，并将生成的学生模型提供给用户。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Arm:+Efficient+Learning+of+Neural+Retrieval+Models+with+Desired+Accuracy+by+Automatic+Knowledge+Amalgamation)|0|
|[An Auto Encoder-based Dimensionality Reduction Technique for Efficient Entity Linking in Business Phone Conversations](https://doi.org/10.1145/3477495.3536322)|Md. Tahmid Rahman Laskar, Cheng Chen, Jonathan Johnston, XueYong Fu, Shashi Bhushan TN, Simon CorstonOliver|Dialpad Canada Inc., Vancouver, BC, Canada|An entity linking system links named entities in a text to their corresponding entries in a knowledge base. In recent years, building an entity linking system that leverages the transformer architecture has gained lots of attention. However, deploying a transformer-based neural entity linking system in industrial production environments in a limited resource setting is a challenging task. In this work, we present an entity linking system that leverages a transformer-based BERT encoder (the BLINK model) to connect the product and organization type entities in business phone conversations to their corresponding Wikipedia entries. We propose a dimensionality reduction technique via utilizing an auto encoder that can effectively compress the dimension of the pre-trained BERT embeddings to 256 from the original size of 1024. This allows our entity linking system to significantly optimize the space requirement when deployed in a resource limited cloud machine while reducing the inference time along with retaining high accuracy.|连接系统的实体将文本中的命名实体链接到知识库中相应的条目。近年来，利用变压器体系结构构建实体连接系统引起了人们的广泛关注。然而，在资源有限的工业生产环境中部署一个基于变压器的神经实体连接系统是一个具有挑战性的任务。在这项工作中，我们提出了一个实体链接系统，该系统利用基于转换器的 BERT 编码器(BLINK 模型)将商务电话会话中的产品和组织类型实体连接到相应的 Wikipedia 条目。我们提出了一种降维技术，通过使用自动编码器，可以有效地将预先训练的 BERT 嵌入的尺寸从原来的1024压缩到256。这使得我们的实体连接系统在部署在资源有限的云计算机中时，能够显著优化空间需求，同时减少推理时间，并保持高精度。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Auto+Encoder-based+Dimensionality+Reduction+Technique+for+Efficient+Entity+Linking+in+Business+Phone+Conversations)|0|
|[Applications and Future of Dense Retrieval in Industry](https://doi.org/10.1145/3477495.3536324)|Yubin Kim|Etsy, Inc., Brooklyn, NY, USA|Large-scale search engines are often designed as tiered systems with at least two layers. The L1 candidate retrieval layer efficiently generates a subset of potentially relevant documents (typically ~1000 documents) from a corpus many orders of magnitude larger in size. L1 systems emphasize efficiency and are designed to maximize recall. The L2 re-ranking layer uses a more computationally expensive, but more accurate model (e.g. learning-to-rank or neural model) to re-rank the candidates generated by L1 in order to maximize precision of the final result list. Traditionally, candidate retrieval was performed with an inverted index data structure, with exact lexical matching. Candidates are ordered by a dot-product-like scoring function f(q,d) where q and d are sparse vectors containing token weights, typically derived from the token's frequency in the document/query and corpus. The inverted index enables sub-linear ranking of the documents. Due to the sparse vector representation of the documents and queries, lexical match retrieval systems have also been called sparse retrieval. To contrast, dense retrieval represents queries and documents by embedding the text into lower dimensional dense vectors. Candidate documents are scored based on the distance between the query and document embedding vectors. Practically, the similarity computations are made efficiently with approximate k-nearest neighbours (ANN) systems. In this panel, we bring together experts in dense retrieval across multiple industry applications, including web search, enterprise and personal search, e-commerce, and out-of-domain retrieval.|大型搜索引擎通常被设计成至少有两层的分层系统。第一语言候选检索层有效地从一个数量级更大的语料库中生成一个可能相关的文档子集(通常是1000个文档)。L1系统强调效率，旨在最大限度地提高召回率。L2重新排序层使用一个计算更加昂贵，但更加精确的模型(例如学习到排序或神经模型)来重新排序 L1产生的候选人，以最大限度地提高最终结果列表的精确度。传统的候选检索采用倒排索引数据结构，并且采用精确的词法匹配。候选者按照类似点乘积的评分函数 f (q，d)排序，其中 q 和 d 是包含令牌权重的稀疏向量，通常来自文档/查询和语料库中令牌的频率。倒排索引允许对文档进行次线性排序。由于文档和查询的稀疏向量表示，词汇匹配检索系统也被称为稀疏检索。相比之下，密集检索通过将文本嵌入低维密集向量来表示查询和文档。候选文档根据查询和文档嵌入向量之间的距离进行评分。在实际应用中，利用近似 k 最近邻(ANN)系统可以有效地进行相似度计算。在这个专题讨论小组中，我们汇集了跨多个行业应用的密集检索方面的专家，包括网络搜索、企业和个人搜索、电子商务和域外检索。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Applications+and+Future+of+Dense+Retrieval+in+Industry)|0|
|[Flipping the Script: Inverse Information Seeking Dialogues for Market Research](https://doi.org/10.1145/3477495.3536326)|Josh Seltzer, Kathy Cheng, Shi Zong, Jimmy Lin|Nexxt Intelligence, Toronto, Canada; University of Waterloo, Waterloo, Canada|Information retrieval has traditionally been framed in terms of searching and extracting information from mostly static resources. Interactive information retrieval (IIR) has widened the scope, with interactive dialogues largely playing the role of clarifying (i.e., making explicit, and/or refining) the information search space. Informed by market research practices, we seek to reframe IIR as a process of eliciting novel information from human interlocutors, with a chatbot-inspired virtual agent playing the role of an interviewer. This reframing flips conventional IIR into what we call an inverse information seeking dialogue, wherein the virtual agent recurrently extracts information from human utterances and poses questions intended to elicit related information. In this work, we introduce and provide a formal definition of an inverse information seeking agent, outline some of its unique challenges, and propose our novel framework to tackle this problem based on techniques from natural language processing (NLP) and IIR.|传统上，信息检索的框架是从大多数静态资源中搜索和提取信息。交互式信息检索(IIR)扩大了搜索范围，交互式对话主要扮演澄清(即明确和/或完善)信息搜索空间的角色。通过市场研究实践，我们试图将 IIR 重新定义为从人类对话者那里获取新信息的过程，由聊天机器人启发的虚拟代理扮演访问者的角色。这种重构将传统的 IIR 翻转为我们所说的反向信息寻求对话，在这种对话中，虚拟代理不断地从人类的话语中提取信息，并提出旨在引出相关信息的问题。本文首先介绍并给出了反向信息搜索代理的形式化定义，概述了它所面临的一些独特挑战，并提出了基于自然语言处理(NLP)和 IIR 技术的反向信息搜索代理框架。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Flipping+the+Script:+Inverse+Information+Seeking+Dialogues+for+Market+Research)|0|
|[Information Ecosystem Threats in Minoritized Communities: Challenges, Open Problems and Research Directions](https://doi.org/10.1145/3477495.3536327)|Shiri DoriHacohen, Scott A. Hale|Meedan & University of Oxford, San Francisco, CA, USA; University of Connecticut & AuCoDe, Storrs, CT, USA|Journalists, fact-checkers, academics, and community media are overwhelmed in their attempts to support communities suffering from gender-, race- and ethnicity-targeted information ecosystem threats, including but not limited to misinformation, hate speech, weaponized controversy and online-to-offline harassment. Yet, for a plethora of reasons, minoritized groups are underserved by current approaches to combat such threats. In this panel, we will present and discuss the challenges and open problems facing such communities and the researchers hoping to serve them. We will also discuss the current state-of-the-art as well as the most promising future directions, both within IR specifically, across Computer Science more broadly, as well as that requiring transdisciplinary and cross-sectoral collaborations. The panel will attract both IR practitioners and researchers and include at least one panelist outside of IR, with unique expertise in this space.|记者、事实核查员、学者和社区媒体在试图支持遭受针对性别、种族和民族的信息生态系统威胁的社区时，不堪重负，这些威胁包括但不限于错误信息、仇恨言论、武器化的争议和线上到线下的骚扰。然而，由于种种原因，少数群体在目前应对这些威胁的方法中得不到充分的服务。在这个小组中，我们将介绍和讨论这些社区面临的挑战和公开的问题，以及研究人员希望为他们服务的问题。我们还将讨论当前最先进的技术，以及最有前途的未来方向，既包括在信息研究领域，也包括更广泛的计算机科学领域，还包括需要跨学科和跨部门合作的领域。该小组将吸引国际关系从业人员和研究人员，并包括至少一个国际关系以外的小组成员，在这个领域具有独特的专业知识。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Information+Ecosystem+Threats+in+Minoritized+Communities:+Challenges,+Open+Problems+and+Research+Directions)|0|
|[Extractive Search for Analysis of Biomedical Texts](https://doi.org/10.1145/3477495.3536328)|Daniel Clothiaux, Ravi Starzl|Bioplx & Carnegie Mellon University, Boulder, CO, USA|Extractive search has been used to create datasets matching queries and syntactic patterns, but less attention has been paid on what to do with those datasets. We present a two-stage system targeted towards biomedical texts. First, it creates custom datasets using a powerful mix of keyword and syntactic matching. We then return lists of related words, provide semantic search, train a large language model, a synthetic data based QA model, a summarization model over those results, and so on. These are then used in downstream biomedical work.|提取搜索已被用于创建匹配查询和语法模式的数据集，但对如何处理这些数据集的关注较少。我们提出了一个针对生物医学文本的两阶段系统。首先，它使用关键字和语法匹配的强大组合创建自定义数据集。然后，我们返回相关词汇的列表，提供语义搜索，训练一个大型语言模型，一个基于综合数据的 QA 模型，这些结果的汇总模型，等等。这些然后用于下游的生物医学工作。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Extractive+Search+for+Analysis+of+Biomedical+Texts)|0|
|[Recent Advances in Retrieval-Augmented Text Generation](https://doi.org/10.1145/3477495.3532682)|Deng Cai, Yan Wang, Lemao Liu, Shuming Shi|Tencent AI Lab, Shenzhen, China; The Chinese University of Hong Kong, Hong Kong, China|Recently retrieval-augmented text generation has achieved state-of-the-art performance in many NLP tasks and has attracted increasing attention of the NLP and IR community, this tutorial thereby aims to present recent advances in retrieval-augmented text generation comprehensively and comparatively. It firstly highlights the generic paradigm of retrieval-augmented text generation, then reviews notable works for different text generation tasks including dialogue generation, machine translation, and other generation tasks, and finally points out some limitations and shortcomings to facilitate future research.|近年来，检索增强文本生成技术在许多自然语言处理任务中都取得了很好的效果，引起了自然语言处理和信息检索领域的广泛关注，本教程旨在全面、比较地介绍检索增强文本生成技术的最新进展。首先介绍了检索增强文本生成的一般范式，然后回顾了不同文本生成任务(包括对话生成、机器翻译和其他生成任务)中值得注意的工作，最后指出了一些局限性和不足，以利于今后的研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Recent+Advances+in+Retrieval-Augmented+Text+Generation)|0|
|[Adaptive Dialogue Management for Conversational Information Elicitation](https://doi.org/10.1145/3477495.3531684)|Harshita Sahijwani|Emory University, Atlanta, GA, USA|Information elicitation conversations, for example, when a medical professional asks about a patient's history or a sales agent tries to understand their client's preferences, often start with a set of routine questions. The interviewer asks a predetermined set of questions conversationally, adapting them to the unique characteristics and context of an individual. Multiple-choice questionnaires are commonly used as a screening tool before the client sees the professional for more efficient information elicitation [5]. However, recent proof-of-concept studies show that users are more likely to report their symptoms to an embodied conversational agent (ECA) than on a pen-and-paper survey [3], and rate ECAs highly on user experience [4]. Chatbots allow the user to give free-form responses and ask clarification questions instead of having to interpret and choose from a list of given options. They can also keep the user engaged by sharing relevant information and offering empathetic acknowledgments when appropriate. However, many of the technical challenges involved in building such a conversational agent remain unsolved.|例如，当医疗专业人员询问病人的病史或销售代理人试图了解他们的客户的偏好时，信息引导谈话通常从一组常规问题开始。面试官会用对话的方式提出一系列预先设定好的问题，并根据个人的独特性格和背景进行调整。多项选择问卷通常被用作在客户见到专业人员之前的筛选工具，以便更有效地获取信息[5]。然而，最近的概念验证研究表明，用户更可能向具体的会话代理(ECA)报告他们的症状，而不是纸笔调查[3] ，并且对用户体验给予 ECA 高度评价[4]。聊天机器人允许用户给出自由形式的回答，并提出澄清问题，而不必从给定的选项列表中进行解释和选择。他们还可以通过分享相关信息和在适当的时候提供感同身受的确认来保持用户的参与度。然而，构建这样一个会话代理所涉及的许多技术挑战仍然没有得到解决。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adaptive+Dialogue+Management+for+Conversational+Information+Elicitation)|0|
|[Pre-Training for Mathematics-Aware Retrieval](https://doi.org/10.1145/3477495.3531680)|Anja Reusch|Technische Universität Dresden, Dresden, Germany|Mathematical formulas are an important tool to concisely communicate ideas in science and education, used to clarify descriptions, calculations or derivations. When searching in scientific literature, mathematical notation, which is often written using the LATEX notation, therefore plays a crucial role that should not be neglected. The task of mathematics-aware information retrieval is to retrieve relevant passages given a query or question, which both can include natural language and mathematical formulas. As in many domains that rely on Natural Language Understanding, transformer-based models are now dominating the field of information retrieval [3]. Apart from their size and the transformerencoder architecture, pre-training is considered to be a key factor for the high performance of these models. It has also been shown that domain-adaptive pre-training improves their performance on down-stream tasks even further [2] especially when the vocabulary overlap between pre-training and in-domain data is low. This is also the case for the domain of mathematical documents.|数学公式是在科学和教育中简明地传达思想的重要工具，用于澄清描述、计算或推导。在搜索科学文献时，数学符号(通常使用 LATEX 符号书写)起着不容忽视的关键作用。具有数学意识的信息检索的任务是检索给出查询或问题的相关段落，这些段落既可以包括自然语言，也可以包括数学公式。正如许多依赖于自然语言理解的领域一样，基于转换器的模型目前在信息检索领域占据主导地位。除了它们的大小和变压器编码器的架构，预训练被认为是这些模型的高性能的一个关键因素。研究还表明，领域自适应预训练可以进一步提高他们在下游任务中的表现，尤其是当预训练和领域内数据的词汇重叠度较低时。数学文献领域也是如此。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pre-Training+for+Mathematics-Aware+Retrieval)|0|
|[Explainable Conversational Question Answering over Heterogeneous Sources](https://doi.org/10.1145/3477495.3531688)|Philipp Christmann|Max Planck Institute for Informatics & Saarland University, Saarbrücken, Germany|State-of-the-art conversational question answering (ConvQA) operates over homogeneous sources of information: either a knowledge base (KB), or a text corpus, or a collection of tables. This inherently limits the answer coverage of ConvQA systems. Therefore, during my PhD, we would like to tap into heterogeneous sources for answering conversational questions. Further, we plan to investigate the explainability of such ConvQA systems, to identify what helps users in understanding the answer derivation process.|最先进的会话问答(ConvQA)操作于同质的信息源: 知识库(KB)、文本语料库或表格集合。这固有地限制了 ConvQA 系统的应答覆盖范围。因此，在我攻读博士学位期间，我们希望利用不同的资源来回答会话中的问题。此外，我们计划调查这样的 ConvQA 系统的可解释性，以确定什么有助于用户理解的答案推导过程。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explainable+Conversational+Question+Answering+over+Heterogeneous+Sources)|0|
|[KA-Recsys: Patient Focused Knowledge Appropriate Health Recommender System](https://doi.org/10.1145/3477495.3531687)|Khushboo Thaker||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=KA-Recsys:+Patient+Focused+Knowledge+Appropriate+Health+Recommender+System)|0|
|[Bilateral Self-unbiased Learning from Biased Implicit Feedback](https://doi.org/10.1145/3477495.3531946)|Jaewoong Lee, Seongmin Park, Joonseok Lee, Jongwuk Lee|Sungkyunkwan Univ, Dept Elect & Comp Engn, Seoul, South Korea; Sungkyunkwan Univ, Dept Artificial Intelligence, Seoul, South Korea|Implicit feedback has been widely used to build commercial recommender systems. Because observed feedback represents users' click logs, there is a semantic gap between true relevance and observed feedback. More importantly, observed feedback is usually biased towards popular items, thereby overestimating the actual relevance of popular items. Although existing studies have developed unbiased learning methods using inverse propensity weighting (IPW) or causal reasoning, they solely focus on eliminating the popularity bias of items. In this paper, we propose a novel unbiased recommender learning model, namely BIlateral SElf-unbiased Recommender (BISER), to eliminate the exposure bias of items caused by recommender models. Specifically, BISER consists of two key components: (i) self-inverse propensity weighting (SIPW) to gradually mitigate the bias of items without incurring high computational costs; and (ii) bilateral unbiased learning (BU) to bridge the gap between two complementary models in model predictions, i.e., user- and item-based autoencoders, alleviating the high variance of SIPW. Extensive experiments show that BISER consistently outperforms state-of-the-art unbiased recommender models over several datasets, including Coat, Yahoo! R3, MovieLens, and CiteULike.|隐式反馈已被广泛用于构建商业推荐系统。因为观察到的反馈代表用户的点击日志，所以在真正的相关性和观察到的反馈之间存在语义差距。更重要的是，观察到的反馈通常偏向于流行项目，从而高估了流行项目的实际相关性。虽然现有的研究已经开发出使用反倾向加权(IPW)或因果推理的无偏学习方法，但他们只关注于消除项目的流行偏差。本文提出了一种新的无偏推荐学习模型，即双边自我无偏推荐模型(BISER) ，以消除由推荐模型引起的项目暴露偏差。具体而言，BISER 由两个关键组成部分组成: (i)自逆倾向加权(SIPW) ，以逐渐减轻项目的偏差，而不会产生高计算成本; (ii)双边无偏学习(BU) ，以弥补模型预测中两个互补模型之间的差距，即用户和基于项目的自动编码器，减轻 SIPW 的高方差。大量的实验表明，BISER 始终优于几个数据集，包括 Coat，Yahoo! R3，MovieLens 和 CiteULike 的最先进的无偏推荐模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bilateral+Self-unbiased+Learning+from+Biased+Implicit+Feedback)|0|
|[Why do Semantically Unrelated Categories Appear in the Same Session?: A Demand-aware Method](https://doi.org/10.1145/3477495.3531806)|Liqi Yang, Linhao Luo, Xiaofeng Zhang, Fengxin Li, Xinni Zhang, Zelin Jiang, Shuai Tang|Harbin Institute of Technology, Shenzhen, Shenzhen, China; China Merchants Securities Co., Ltd, Shenzhen, China|Session-based recommendation has recently attracted more and more research efforts. Most existing approaches are intuitively proposed to discover users' potential preferences or interests from the anonymous session data. This apparently ignores the fact that these sequential behavior data usually reflect session user's potential demand, i.e., a semantic level factor, and therefore how to estimate underlying demands from a session has become a challenging task. To tackle the aforementioned issue, this paper proposes a novel demand-aware graph neural network model. Particularly, a demand modeling component is designed to extract the underlying multiple demands of each session. Then, the demand-aware graph neural network is designed to first construct session demand graphs and then learn the demand-aware item embeddings to make the recommendation. The mutual information loss is further designed to enhance the quality of the learnt embeddings. Extensive experiments have been performed on two real-world datasets and the proposed model achieves the SOTA model performance.|近年来，基于会话的推荐引起了越来越多的研究者的关注。现有的大多数方法都是直观地从匿名会话数据中发现用户的潜在偏好或兴趣。这显然忽略了这样一个事实，即这些顺序行为数据通常反映会话用户的潜在需求，即语义级因素，因此如何估计会话的潜在需求已经成为一个具有挑战性的任务。为了解决上述问题，本文提出了一种新的需求感知图神经网络模型。特别地，需求建模组件被设计用于提取每个会话的底层多个需求。然后，设计需求感知图神经网络，首先构造会话需求图，然后学习需求感知项嵌入，进行推荐。进一步设计了互信息损失，提高了学习嵌入的质量。在两个实际数据集上进行了广泛的实验，所提出的模型达到了 SOTA 模型的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Why+do+Semantically+Unrelated+Categories+Appear+in+the+Same+Session?:+A+Demand-aware+Method)|0|
|[Scalable User Interface Optimization Using Combinatorial Bandits](https://doi.org/10.1145/3477495.3536325)|Ioannis Kangas, Maud Schwoerer, Lucas Bernardi|Booking.com, Amsterdam, Netherlands|The mission of major e-commerce platforms is to enable their customers to find the best products for their needs. In the common case of large inventories, complex User Interfaces (UIs) are required to allow a seamless navigation. However, as UIs often contain many widgets of different relevance, the task of constructing an optimal layout arises in order to improve the customer's experience. This is a challenging task, especially in the typical industrial setup where multiple independent teams conflict by adding and modifying UI widgets. It becomes even more challenging due to the customer preferences evolving over time, bringing the need for adaptive solutions. In a previous work [6], we addressed this task by introducing a UI governance framework powered by Machine Learning (ML) algorithms that automatically and continuously search for the optimal layout. Nevertheless, we highlighted that naive algorithmic choices exhibit several issues when implemented in the industry, such as widget dependency, combinatorial solution space and cold start problem. In this work, we demonstrate how we deal with these issues using Combinatorial Bandits, an extension of Multi-Armed Bandits (MAB) where the agent selects not only one but multiple arms at the same time. We develop two novel approaches to model combinatorial bandits, inspired by the Natural Language Processing (NLP) and the Evolutionary Algorithms (EA) fields and present their ability to enable scalable UI optimization.|主要电子商务平台的使命是使其客户能够找到满足其需要的最佳产品。在大量存货的常见情况下，需要复杂的用户界面(UI)来实现无缝导航。然而，由于 UI 通常包含许多不同相关性的小部件，因此需要构建最佳布局以改善客户体验。这是一项具有挑战性的任务，特别是在典型的工业设置中，多个独立团队通过添加和修改 UI 小部件而发生冲突。由于客户的偏好随着时间的推移而不断变化，因此对适应性解决方案的需求变得更加具有挑战性。在之前的工作[6]中，我们通过引入一个由机器学习(ML)算法支持的 UI 治理框架来解决这个问题，该框架可以自动并持续地搜索最佳布局。然而，我们强调，幼稚的算法选择表现出几个问题，如小部件依赖，组合解决方案空间和冷启动问题。在这项工作中，我们展示了如何处理这些问题使用组合强盗，一个扩展的多臂强盗(MAB) ，其中代理人选择不仅一个，但多个武器在同一时间。受自然语言处理(NLP)和进化算法(EA)领域的启发，我们开发了两种新的组合强盗建模方法，并展示了它们实现可扩展 UI 优化的能力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scalable+User+Interface+Optimization+Using+Combinatorial+Bandits)|0|
|[Users: Can't Work With Them, Can't Work Without Them?](https://doi.org/10.1145/3477495.3532787)|Alistair Moffat|The University of Melbourne, Melbourne, VIC, Australia|If we could design the ideal IR "effectiveness" experiment (as distinct from an IR "efficiency" experiment), what would it look like? It would probably be a lab-based observational study [3] involving multiple search systems masked behind a uniform interface, and with hundreds (or thousands) of users each progressing some "real" search activity they were interested in. And we'd plan to (non-intrusively, somehow) capture per-snippet, per-document, per-SERP, and per-session annotations and satisfaction responses. The collected data could then be compared against a range of measured "task completion quality" indicators, and also against search effectiveness metric scores computed from the elements contained in the SERPs that were served by the systems. That's a tremendously big ask! So we often use offline evaluation techniques instead, employing test collections, static qrels sets, and effectiveness metrics [6]. We abstract the user into a deterministic evaluation script, supposing for pragmatic reasons that we know what query they would issue, and at the same time assuming that we can apply an effectiveness metric to calculate how much usefulness (or satisfaction) they will derive from any given SERP. The great advantage of this approach is that aside from the process of collecting the qrels, it is free of the need for users, meaning that it is repeatable. Indeed, we often do repeat, iterating to set parameters (and to rectify programming errors). Then, once metric scores have been computed, we carry out one or more paired statistical tests and draw conclusions as to relative system effectiveness.|如果我们可以设计一个理想的红外“有效性”实验(不同于红外“有效性”实验) ，它会是什么样子？这可能是一个基于实验室的观察性研究，在一个统一的界面背后隐藏着多个搜索系统，数百(或数千)用户每个人都在进行一些他们感兴趣的“真实”搜索活动。我们计划(非侵入性地，以某种方式)捕获每个片段、每个文档、每个 SERP 和每个会话的注释和满意度响应。然后，收集的数据可以与一系列测量的“任务完成质量”指标进行比较，也可以与搜索效率指标得分进行比较，这些得分是从系统提供的 SERP 中包含的元素计算出来的。这个要求太过分了！因此，我们经常使用离线评估技术，使用测试集合、静态 qrel 集和有效性度量[6]。我们将用户抽象成一个确定性的评估脚本，假设出于实用原因，我们知道他们会发出什么样的查询，同时假设我们可以应用一个有效性度量来计算他们将从任何给定的 SERP 中获得多少有用性(或满意度)。这种方法的最大优点是，除了收集 qrel 的过程之外，它不需要用户，这意味着它是可重复的。实际上，我们经常重复，迭代以设置参数(并纠正编程错误)。然后，一旦计算出度量分数，我们进行一个或多个成对的统计检验，并得出相对系统有效性的结论。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Users:+Can't+Work+With+Them,+Can't+Work+Without+Them?)|0|
|[Interacting with Non-Cooperative User: A New Paradigm for Proactive Dialogue Policy](https://doi.org/10.1145/3477495.3532001)|Wenqiang Lei, Yao Zhang, Feifan Song, Hongru Liang, Jiaxin Mao, Jiancheng Lv, Zhenglu Yang, TatSeng Chua|Sichuan University, Chengdu, China; Renmin University of China, Beijing, China; National University of Singapore, Singapore, Singapore; Nankai University, Tianjin, China; Peking University, Beijing, China|Proactive dialogue system is able to lead the conversation to a goal topic and has advantaged potential in bargain, persuasion, and negotiation. Current corpus-based learning manner limits its practical application in real-world scenarios. To this end, we contribute to advancing the study of the proactive dialogue policy to a more natural and challenging setting, i.e., interacting dynamically with users. Further, we call attention to the non-cooperative user behavior - the user talks about off-path topics when he/she is not satisfied with the previous topics introduced by the agent. We argue that the targets of reaching the goal topic quickly and maintaining a high user satisfaction are not always converged, because the topics close to the goal and the topics user preferred may not be the same. Towards this issue, we propose a new solution named I-Pro that can learn Proactive policy in the Interactive setting. Specifically, we learn the trade-off via a learned goal weight, which consists of four factors (dialogue turn, goal completion difficulty, user satisfaction estimation, and cooperative degree). The experimental results demonstrate I-Pro significantly outperforms baselines in terms of effectiveness and interpretability.|积极主动的对话系统能够将对话引向一个目标话题，并且在讨价还价、说服和谈判方面具有优势。目前基于语料库的学习方式限制了其在现实情景中的实际应用。为此，我们致力于将积极对话政策的研究推向一个更加自然和富有挑战性的环境，即与用户进行动态互动。此外，我们还提请注意非合作用户行为——当用户对代理引入的前一个主题不满意时，他/她会谈论偏离路径的主题。我们认为，快速达到目标主题和保持高用户满意度的目标并不总是趋同的，因为接近目标的主题和用户喜欢的主题可能不一样。针对这个问题，我们提出了一个新的解决方案，称为 I-Pro，可以学习积极的政策在互动的设置。具体来说，我们通过一个学习的目标权重来学习权衡，这个权重包括四个因素(对话转向、目标完成难度、用户满意度估计和合作程度)。实验结果表明，I-Pro 在有效性和可解释性方面明显优于基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interacting+with+Non-Cooperative+User:+A+New+Paradigm+for+Proactive+Dialogue+Policy)|0|
|[ADPL: Adversarial Prompt-based Domain Adaptation for Dialogue Summarization with Knowledge Disentanglement](https://doi.org/10.1145/3477495.3531933)|Lulu Zhao, Fujia Zheng, Weihao Zeng, Keqing He, Ruotong Geng, Huixing Jiang, Wei Wu, Weiran Xu|Meituan Group, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China|Traditional dialogue summarization models rely on a large-scale manually-labeled corpus, lacking generalization ability to new domains, and domain adaptation from a labeled source domain to an unlabeled target domain is important in practical summarization scenarios. However, existing domain adaptation works in dialogue summarization generally require large-scale pre-training using extensive external data. To explore the lightweight fine-tuning methods, in this paper, we propose an efficient Adversarial Disentangled Prompt Learning (ADPL) model for domain adaptation in dialogue summarization. We introduce three kinds of prompts including domain-invariant prompt (DIP), domain-specific prompt (DSP), and task-oriented prompt (TOP). DIP aims to disentangle and transfer the shared knowledge from the source domain and target domain in an adversarial way, which improves the accuracy of prediction about domain-invariant information and enhances the ability for generalization to new domains. DSP is designed to guide our model to focus on domain-specific knowledge using domain-related features. TOP is to capture task-oriented knowledge to generate high-quality summaries. Instead of fine-tuning the whole pre-trained language model (PLM), we only update the prompt networks but keep PLM fixed. Experimental results on the zero-shot setting show that the novel design of prompts can yield more coherent, faithful, and relevant summaries than baselines using the prefix-tuning, and perform at par with fine-tuning while being more efficient. Overall, our work introduces a prompt-based perspective to the zero-shot learning for dialogue summarization task and provides valuable findings and insights for future research.|传统的对话摘要模型依赖于大规模的人工标记语料库，缺乏对新领域的泛化能力，在实际的摘要场景中，从标记的源领域到未标记的目标领域的领域适应性是非常重要的。然而，现有的对话摘要领域适应工作一般需要使用大量的外部数据进行大规模的预训练。为了探索轻量级微调方法，本文提出了一种有效的对话摘要领域自适应对抗性分离提示学习(ADPL)模型。介绍了领域不变提示(DIP)、领域特定提示(DSP)和面向任务提示(TOP)三种提示方式。DIP 旨在对源域和目标域的共享知识进行对抗性的分离和转移，提高了领域不变信息预测的准确性，增强了对新领域的推广能力。DSP 被设计用来指导我们的模型使用领域相关的特征来关注领域特定的知识。TOP 是一种以任务为导向的知识获取方法，用于生成高质量的摘要。我们没有对整个预先训练好的语言模型(PLM)进行微调，而是只更新提示网络，但保持 PLM 不变。零镜头设置的实验结果表明，新颖的提示符设计比基线前缀调整更能产生连贯、忠实和相关的总结，并且在更有效率的同时表现出与微调相当的效果。总的来说，我们的工作为对话总结任务的零拍学习引入了一个基于及时性的视角，并为未来的研究提供了有价值的发现和见解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ADPL:+Adversarial+Prompt-based+Domain+Adaptation+for+Dialogue+Summarization+with+Knowledge+Disentanglement)|0|
|[IR Evaluation and Learning in the Presence of Forbidden Documents](https://doi.org/10.1145/3477495.3532006)|David Carmel, Nachshon Cohen, Amir Ingber, Elad Kravi|Pinecone Systems, Haifa, Israel; Amazon, Haifa, Israel|Many IR collections contain forbidden documents (F-docs), i.e. documents that should not be retrieved to the searcher. In an ideal scenario F-docs are clearly flagged, hence the ranker can filter them out, guaranteeing that no F-doc will be exposed. However, in real-world scenarios, filtering algorithms are prone to errors. Therefore, an IR evaluation system should also measure filtering quality in addition to ranking quality. Typically, filtering is considered as a classification task and is evaluated independently of the ranking quality. However, due to the mutual affinity between the two, it is desirable to evaluate ranking quality while filtering decisions are being made. In this work we propose nDCGf, a novel extension of the nDCGmin metric[14], which measures both ranking and filtering quality of the search results. We show both theoretically and empirically that while nDCGmin is not suitable for the simultaneous ranking and filtering task, nDCGf is a reliable metric in this case. We experiment with three datasets for which ranking and filtering are both required. In the PR dataset our task is to rank product reviews while filtering those marked as spam. Similarly, in the CQA dataset our task is to rank a list of human answers per question while filtering bad answers. We also experiment with the TREC web-track datasets, where F-docs are explicitly labeled, sorting participant runs according to their ranking and filtering quality, demonstrating the stability, sensitivity, and reliability of nDCGf for this task. We propose a learning to rank and filter (LTRF) framework that is specifically designed to optimize nDCGf, by learning a ranking model and optimizing a filtering threshold used for discarding documents with lower scores. We experiment with several loss functions demonstrating their success in learning an effective LTRF model for the simultaneous learning and filtering task.|许多 IR 集合包含禁用的文档(F-docs) ，即不应该被检索到搜索器的文档。在一个理想的场景中，F-doc 被清楚地标记，因此排名者可以过滤掉它们，保证没有 F-doc 被暴露。然而，在真实场景中，过滤算法很容易出错。因此，一个红外评价系统除了评级质量外，还应该衡量过滤质量。通常，过滤被认为是一个分类任务，独立于排序质量进行评估。然而，由于两者之间的相互亲和性，在做出过滤决策的同时评估排名质量是可取的。在这项工作中，我们提出了 nDCGf，nDCGmin 度量的一个新的扩展[14] ，它衡量搜索结果的排名和过滤质量。我们从理论和实验两方面证明了，虽然 nDCGmin 不适合同时进行排序和过滤任务，但在这种情况下，nDCGf 是一个可靠的度量。我们对三个数据集进行了实验，这三个数据集都需要排名和过滤。在公关数据集中，我们的任务是对产品评论进行排序，同时过滤那些被标记为垃圾邮件的评论。类似地，在 CQA 数据集中，我们的任务是对每个问题的人工答案列表进行排序，同时过滤错误答案。我们还对 TREC 网络跟踪数据集进行了实验，其中 F-docs 被明确标记，根据其排名和过滤质量对参与者进行排序，证明了 nDCGf 用于该任务的稳定性，灵敏度和可靠性。我们提出了一个学习排序和过滤(LTRF)框架，专门设计优化 nDCGf，通过学习排序模型和优化过滤阈值用于丢弃分数较低的文档。我们用几个损失函数进行了实验，证明了它们在同时学习和过滤任务中学习一个有效的 LTRF 模型是成功的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IR+Evaluation+and+Learning+in+the+Presence+of+Forbidden+Documents)|0|
|[Human Preferences as Dueling Bandits](https://doi.org/10.1145/3477495.3531991)|Xinyi Yan, Chengxi Luo, Charles L. A. Clarke, Nick Craswell, Ellen M. Voorhees, Pablo Castells|University of Waterloo, Waterloo, ON, Canada; National Institute of Standards and Technology, Gaithersburg, MD, USA; Universidad Autónoma de Madrid, Madrid, Spain; Microsoft, Bellevue, WA, USA|The dramatic improvements in core information retrieval tasks engendered by neural rankers create a need for novel evaluation methods. If every ranker returns highly relevant items in the top ranks, it becomes difficult to recognize meaningful differences between them and to build reusable test collections. Several recent papers explore pairwise preference judgments as an alternative to traditional graded relevance assessments. Rather than viewing items one at a time, assessors view items side-by-side and indicate the one that provides the better response to a query, allowing fine-grained distinctions. If we employ preference judgments to identify the probably best items for each query, we can measure rankers by their ability to place these items as high as possible. We frame the problem of finding best items as a dueling bandits problem. While many papers explore dueling bandits for online ranker evaluation via interleaving, they have not been considered as a framework for offline evaluation via human preference judgments. We review the literature for possible solutions. For human preference judgments, any usable algorithm must tolerate ties, since two items may appear nearly equal to assessors, and it must minimize the number of judgments required for any specific pair, since each such comparison requires an independent assessor. Since the theoretical guarantees provided by most algorithms depend on assumptions that are not satisfied by human preference judgments, we simulate selected algorithms on representative test cases to provide insight into their practical utility. Based on these simulations, one algorithm stands out for its potential. Our simulations suggest modifications to further improve its performance. Using the modified algorithm, we collect over 10,000 preference judgments for pools derived from submissions to the TREC 2021 Deep Learning Track, confirming its suitability. We test the idea of best-item evaluation and suggest ideas for further theoretical and practical progress.|由神经排序引起的核心信息检索任务的显著改进创造了对新的评估方法的需求。如果每个排名都返回顶级排名中高度相关的项目，那么就很难识别它们之间有意义的差异，也很难构建可重用的测试集合。最近的几篇论文探讨了成对偏好判断作为传统分级相关性评估的替代方法。评估员不是一次查看一个项，而是并排查看项，并指出哪个项能够对查询提供更好的响应，从而允许细粒度的区分。如果我们使用偏好判断来确定每个查询可能最好的项目，我们可以通过他们将这些项目放在尽可能高的位置的能力来衡量排名。我们把寻找最佳物品的问题框定为一个决斗土匪问题。尽管许多论文通过交错的方式探索了在线排名评价中的决斗强盗，但是它们并没有被认为是一个通过人类偏好判断进行离线评价的框架。我们回顾了可能的解决方案的文献。对于人类偏好判断，任何可用的算法都必须容忍关系，因为两个项目可能看起来几乎等于评估者，它必须最小化任何特定对所需的判断数量，因为每个这样的比较需要一个独立的评估者。由于大多数算法提供的理论保证依赖于人类偏好判断不能满足的假设，我们在代表性测试案例上模拟选定的算法，以深入了解它们的实际效用。基于这些模拟，一种算法因其潜力而脱颖而出。我们的模拟结果表明，改进可以进一步提高它的性能。使用修改后的算法，我们收集了超过10,000个来自 TREC 2021深度学习跟踪提交的池的偏好判断，确认了它的适用性。我们检验了最佳项目评价的思想，并为进一步的理论和实践进展提出了建议。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Human+Preferences+as+Dueling+Bandits)|0|
|[IAOTP: An Interactive End-to-End Solution for Aspect-Opinion Term Pairs Extraction](https://doi.org/10.1145/3477495.3532085)|Ambreen Nazir, Yuan Rao|Xi'an Jiaotong University, Xi'an, China|Recently, the aspect-opinion term pairs (AOTP) extraction task has gained substantial importance in the domain of aspect-based sentiment analysis. It intends to extract the potential pair of each aspect term with its corresponding opinion term present in a user review. Some existing studies heavily relied on the annotated aspect terms and/or opinion terms, or adopted external knowledge/resources to figure out the task. Therefore, in this study, we propose a novel end-to-end solution, called an Interactive AOTP (IAOTP) model, for exploring AOTP. The IAOTP model first tracks the boundary of each token in given aspect-specific and opinion-specific representations through a span-based operation. Next, it generates the candidate AOTP by formulating the dyadic relations between tokens through the Biaffine transformation. Then, it computes the positioning information to capture the significant distance relationship that each candidate pair holds. And finally, it jointly models collaborative interactions and prediction of AOTP through a 2D self-attention. Besides the IAOTP model, this study also proposes an independent aspect/opinion encoding model (a RS model) that formulates relational semantics to obtain aspect-specific and opinion-specific representations that can effectively perform the extraction of aspect and opinion terms. Detailed experiments conducted on the publicly available benchmark datasets for AOTP, aspect terms, and opinion terms extraction tasks, clearly demonstrate the significantly improved performance of our models relative to other competitive state-of-the-art baselines.|近年来，在基于方面的情绪分析领域，方面-意见词对(AOTP)提取任务得到了广泛的重视。它打算提取每个方面术语与其相应的意见术语在用户评论中出现的潜在对。一些现有的研究严重依赖于注释的方面术语和/或意见术语，或者采用外部知识/资源来完成任务。因此，在这项研究中，我们提出了一个新的端到端解决方案，称为交互式 AOTP (IAOTP)模型，用于探索 AOTP。IAOTP 模型首先通过基于跨度的操作跟踪给定方面特定表示和意见特定表示中每个令牌的边界。其次，通过双仿射变换建立令牌之间的并元关系，生成候选 AOTP。然后，通过计算定位信息来捕获每个候选对所持有的重要距离关系。最后，通过二维自我注意共同建立 AOTP 协作交互和预测模型。除了 IAOTP 模型外，本研究还提出了一个独立的方面/意见编码模型(RS 模型) ，该模型通过建立关系语义来获得方面特定和意见特定的表示，从而有效地提取方面和意见术语。在 AOTP，方面术语和意见术语提取任务的公开可用基准数据集上进行的详细实验清楚地表明，相对于其他竞争性的最先进的基线，我们的模型的性能显着改善。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IAOTP:+An+Interactive+End-to-End+Solution+for+Aspect-Opinion+Term+Pairs+Extraction)|0|
|[Exploring Heterogeneous Data Lake based on Unified Canonical Graphs](https://doi.org/10.1145/3477495.3531759)|Qin Yuan, Ye Yuan, Zhenyu Wen, He Wang, Chen Chen, Guoren Wang|Zhejiang University of Technology, Hangzhou, China; Beijing Institute of Technology, Beijing, China|A data lake is a repository for massive raw and heterogeneous data, which includes multiple data models with different data schemas and query interfaces. Keyword search can extract valuable information for users without the knowledge of underlying schemas and query languages. However, conventional keyword searches are restricted to a certain data model and cannot easily adapt to a data lake. In this paper, we study a novel keyword search. To achieve high accuracy and efficiency, we introduce canonical graphs and then integrate semantically related vertices based on vertex representations. A matching entity based keyword search algorithm is presented to find answers across multiple data sources. Finally, extensive experimental study shows the effectiveness and efficiency of our solution.|数据湖是大量原始和异构数据的存储库，其中包括具有不同数据模式和查询接口的多个数据模型。关键字搜索可以在不了解基础模式和查询语言的情况下为用户提取有价值的信息。然而，传统的关键字搜索仅限于特定的数据模型，不能很容易地适应数据湖。本文研究了一种新的关键词搜索方法。为了实现高精度和高效率，我们引入了规范图，然后基于顶点表示对语义相关的顶点进行集成。提出了一种基于匹配实体的关键字搜索算法，用于在多个数据源之间寻找答案。最后，广泛的实验研究表明了我们的解决方案的有效性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+Heterogeneous+Data+Lake+based+on+Unified+Canonical+Graphs)|0|
|[Distilling Knowledge on Text Graph for Social Media Attribute Inference](https://doi.org/10.1145/3477495.3531968)|Quan Li, Xiaoting Li, Lingwei Chen, Dinghao Wu|The Pennsylvania State University, State College, PA, USA; Wright State University, Dayton, OH, USA; Visa Research, Palo Alto, CA, USA|The popularization of social media generates a large amount of user-oriented data, where text data especially attracts researchers and speculators to infer user attributes (e.g., age, gender) for fulfilling their intents. Generally, this line of work casts attribute inference as a text classification problem, and starts to leverage graph neural networks for higher-level text representations. However, these text graphs are constructed on words, suffering from high memory consumption and ineffectiveness on few labeled texts. To address this challenge, we design a text-graph-based few-shot learning model for social media attribute inferences. Our model builds a text graph with texts as nodes and edges learned from current text representations via manifold learning and message passing. To further use unlabeled texts to improve few-shot performance, a knowledge distillation is devised to optimize the problem. This offers a trade-off between expressiveness and complexity. Experiments on social media datasets demonstrate the state-of-the-art performance of our model on attribute inferences with considerably fewer labeled texts.|社交媒体的普及产生了大量以用户为导向的数据，其中文本数据尤其吸引研究人员和投机者推断用户属性(如年龄、性别)以实现其意图。一般来说，这一行的工作将属性推理作为一个文本分类问题，并开始利用图神经网络进行更高级别的文本表示。然而，这些文本图形是建立在单词上的，由于高记忆消耗和对少数带标签的文本无效。为了解决这个问题，我们设计了一个基于文本图形的社会媒体属性推理的少镜头学习模型。我们的模型通过流形学习和消息传递从当前的文本表示中学习文本作为节点和边来构建一个文本图。为了进一步利用未标记文本来提高短镜头性能，设计了一种知识提取方法来优化问题。这在表达性和复杂性之间提供了一种权衡。在社会媒体数据集上的实验证明了我们的模型在标记文本相当少的情况下对属性推理的最新性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distilling+Knowledge+on+Text+Graph+for+Social+Media+Attribute+Inference)|0|
|[A Simple Meta-learning Paradigm for Zero-shot Intent Classification with Mixture Attention Mechanism](https://doi.org/10.1145/3477495.3531803)|Han Liu, Siyang Zhao, Xiaotong Zhang, Feng Zhang, Junjie Sun, Hong Yu, Xianchao Zhang|Dalian University of Technology, Dalian, China; Peking University, Beijing, China|Zero-shot intent classification is a vital and challenging task in dialogue systems, which aims to deal with numerous fast-emerging unacquainted intents without annotated training data. To obtain more satisfactory performance, the crucial points lie in two aspects: extracting better utterance features and strengthening the model generalization ability. In this paper, we propose a simple yet effective meta-learning paradigm for zero-shot intent classification. To learn better semantic representations for utterances, we introduce a new mixture attention mechanism, which encodes the pertinent word occurrence patterns by leveraging the distributional signature attention and multi-layer perceptron attention simultaneously. To strengthen the transfer ability of the model from seen classes to unseen classes, we reformulate zero-shot intent classification with a meta-learning strategy, which trains the model by simulating multiple zero-shot classification tasks on seen categories, and promotes the model generalization ability with a meta-adapting procedure on mimic unseen categories. Extensive experiments on two real-world dialogue datasets in different languages show that our model outperforms other strong baselines on both standard and generalized zero-shot intent classification tasks.|零射击意图分类是对话系统中的一项重要而具有挑战性的任务，其目的是在没有注释训练数据的情况下处理大量快速出现的不熟悉意图。要获得更满意的表现，关键在于提取更好的话语特征和增强模型泛化能力两个方面。在本文中，我们提出了一个简单而有效的元学习范式的零射击意图分类。为了更好地学习话语的语义表征，我们引入了一种新的混合注意机制，该机制同时利用分布特征注意和多层感知器注意对相关词语出现模式进行编码。为了增强模型从可见类到不可见类的迁移能力，我们采用元学习策略重新构建了零射击意图分类模型，通过模拟可见类的多个零射击分类任务来训练模型，并通过模拟不可见类的元自适应过程来提高模型的泛化能力。在两个不同语言的真实世界对话数据集上的大量实验表明，我们的模型在标准和广义零射击意图分类任务上都优于其他强基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Simple+Meta-learning+Paradigm+for+Zero-shot+Intent+Classification+with+Mixture+Attention+Mechanism)|0|
|[Analyzing the Support Level for Tips Extracted from Product Reviews](https://doi.org/10.1145/3477495.3531805)|Miriam Farber, David Carmel, Lital Kuchy, Avihai Mejer|Amazon, Haifa, Israel; Faebook, Haifa, Israel|Useful tips extracted from product reviews assist customers to take a more informed purchase decision, as well as making a better, easier, and safer usage of the product. In this work we argue that extracted tips should be examined based on the amount of support and opposition they receive from all product reviews. A classifier, developed for this purpose, determines the degree to which a tip is supported or contradicted by a single review sentence. These support-levels are then aggregated over all review sentences, providing a global support score, and a global contradiction score, reflecting the support-level of all reviews to the given tip, thus improving the customer confidence in the tip validity. By analyzing a large set of tips extracted from product reviews, we propose a novel taxonomy for categorizing tips as highly-supported, highly-contradicted, controversial (supported and contradicted), and anecdotal (neither supported nor contradicted).|从产品评论中提取的有用的技巧可以帮助客户做出更明智的购买决定，以及更好、更容易和更安全地使用产品。在这项工作中，我们认为提取的技巧应该基于他们从所有产品评审中得到的支持和反对的数量进行检查。为此目的开发的分类器决定了一个复习句支持或反驳一个提示的程度。然后将这些支持级别聚合到所有评论句中，提供全局支持评分和全局矛盾评分，反映所有评论对给定提示的支持级别，从而提高客户对提示有效性的信心。通过分析从产品评论中提取的大量技巧，我们提出了一种新的分类方法，将技巧分为高支持、高矛盾、有争议(支持和矛盾)和轶事(既不支持也不矛盾)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Analyzing+the+Support+Level+for+Tips+Extracted+from+Product+Reviews)|0|
|[UserBERT: Pre-training User Model with Contrastive Self-supervision](https://doi.org/10.1145/3477495.3531810)|Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang|Microsoft Research Asia, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China|User modeling is critical for personalization. Existing methods usually train user models from task-specific labeled data, which may be insufficient. In fact, there are usually abundant unlabeled user behavior data that encode rich universal user information, and pre-training user models on them can empower user modeling in many downstream tasks. In this paper, we propose a user model pre-training method named UserBERT to learn universal user models on unlabeled user behavior data with two contrastive self-supervision tasks. The first one is masked behavior prediction and discrimination, aiming to model the contexts of user behaviors. The second one is behavior sequence matching, aiming to capture user interest stable in different periods. Besides, we propose a medium-hard negative sampling framework to select informative negative samples for better contrastive pre-training. Extensive experiments validate the effectiveness of UserBERT in user model pre-training.|用户建模对个性化至关重要。现有的方法通常根据特定于任务的标记数据训练用户模型，这可能是不够的。事实上，通常存在大量未标记的用户行为数据，这些数据编码了丰富的通用用户信息，并且对这些数据进行预训练的用户模型可以在许多下游任务中增强用户建模能力。本文提出了一种用户模型预训练方法 UserBERT，通过两个对比的自我监督任务来学习未标记用户行为数据的通用用户模型。第一种是隐蔽行为预测和识别，旨在对用户行为的上下文进行建模。第二种是行为序列匹配，旨在捕获不同时期用户兴趣的稳定性。此外，我们提出了一个中等硬度的负面抽样框架来选择信息量大的负面样本，以便更好地进行对比预训练。大量实验验证了 UserBERT 在用户模型预训练中的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=UserBERT:+Pre-training+User+Model+with+Contrastive+Self-supervision)|0|
|[Modern Baselines for SPARQL Semantic Parsing](https://doi.org/10.1145/3477495.3531841)|Debayan Banerjee, Pranav Ajit Nair, Jivat Neet Kaur, Ricardo Usbeck, Chris Biemann|Universität Hamburg, Hamburg, Germany; Microsoft Research, Bengaluru, India; Indian Institute of Technology (BHU), Varanasi, India|In this work, we focus on the task of generating SPARQL queries from natural language questions, which can then be executed on Knowledge Graphs (KGs). We assume that gold entity and relations have been provided, and the remaining task is to arrange them in the right order along with SPARQL vocabulary, and input tokens to produce the correct SPARQL query. Pre-trained Language Models (PLMs) have not been explored in depth on this task so far, so we experiment with BART, T5 and PGNs (Pointer Generator Networks) with BERT embeddings, looking for new baselines in the PLM era for this task, on DBpedia and Wikidata KGs. We show that T5 requires special input tokenisation, but produces state of the art performance on LC-QuAD 1.0 and LC-QuAD 2.0 datasets, and outperforms task-specific models from previous works. Moreover, the methods enable semantic parsing for questions where a part of the input needs to be copied to the output query, thus enabling a new paradigm in KG semantic parsing.|在这项工作中，我们将重点放在从自然语言问题生成 SPARQL 查询的任务上，然后可以在知识图(Knowledge Graphs，KG)上执行这些查询。我们假设已经提供了 gold 实体和关系，剩下的任务是将它们与 SPARQL 词汇表一起按正确的顺序排列，并输入令牌以生成正确的 SPARQL 查询。预先训练的语言模型(PLM)到目前为止还没有被深入探索，所以我们在 BART，T5和 PGNs (指针生成器网络)中嵌入 BERT 进行试验，在 DBpedia 和 Wikidata KG 上为这项任务寻找 PLM 时代的新基线。我们展示了 T5需要特殊的输入标记，但是在 LC-QuAD 1.0和 LC-QuAD 2.0数据集上产生了最先进的性能，并且优于以前作品中的任务特定模型。此外，这些方法还支持对需要将部分输入复制到输出查询的问题进行语义解析，从而为 KG 语义解析提供了一个新的范例。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modern+Baselines+for+SPARQL+Semantic+Parsing)|0|
|[Posterior Probability Matters: Doubly-Adaptive Calibration for Neural Predictions in Online Advertising](https://doi.org/10.1145/3477495.3531911)|Penghui Wei, Weimin Zhang, Ruijie Hou, Jinquan Liu, Shaoguo Liu, Liang Wang, Bo Zheng|Alibaba Group, Beijing, China|Predicting user response probabilities is vital for ad ranking and bidding. We hope that predictive models can produce accurate probabilistic predictions that reflect true likelihoods. Calibration techniques aims to post-process model predictions to posterior probabilities. Field-level calibration -- which performs calibration w.r.t. to a specific field value -- is fine-grained and more practical. In this paper we propose a doubly-adaptive approach AdaCalib. It learns an isotonic function family to calibrate model predictions with the guidance of posterior statistics, and field-adaptive mechanisms are designed to ensure that the posterior is appropriate for the field value to be calibrated. Experiments verify that AdaCalib achieves significant improvement on calibration performance. It has been deployed online and beats previous approach.|预测用户响应概率对广告排名和投标至关重要。我们希望预测模型能够产生反映真实可能性的准确概率预测。校正技术旨在对模型预测进行后期处理，以便后验概率。现场级校准——对特定的现场值进行校准。现场级校准更加细粒度和实用。在本文中，我们提出了一个双重自适应的方法 AdaCalib。它学习了一个等调函数族，以后验统计学为指导校准模型预测，并设计了场自适应机制，以确保后验适合于被校准的场值。实验证明，AdaCalib 在校准性能方面取得了显著的改善。它已经在网上部署，打破了以前的做法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Posterior+Probability+Matters:+Doubly-Adaptive+Calibration+for+Neural+Predictions+in+Online+Advertising)|0|
|[Table Enrichment System for Machine Learning](https://doi.org/10.1145/3477495.3531678)|Yuyang Dong, Masafumi Oyamada|NEC Corporation, Kawasaki, Japan|Data scientists are constantly facing the problem of how to improve prediction accuracy with insufficient tabular data. We propose a table enrichment system that enriches a query table by adding external attributes (columns) from data lakes and improves the accuracy of machine learning predictive models. Our system has four stages, join row search, task-related table selection, row and column alignment, and feature selection and evaluation, to efficiently create an enriched table for a given query table and a specified machine learning task. We demonstrate our system with a web UI to show the use cases of table enrichment.|如何在表格数据不足的情况下提高预测的准确性，是数据科学家不断面临的问题。我们提出了一个表增加系统，通过增加数据湖的外部属性(列)来丰富查询表，提高机器学习预测模型的准确性。我们的系统分为四个阶段: 连接行搜索、任务相关的表选择、行和列对齐以及特征选择和评估，以有效地为给定的查询表和指定的机器学习任务创建一个丰富的表。我们用一个 web UI 来演示我们的系统，以显示表充实的用例。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Table+Enrichment+System+for+Machine+Learning)|0|
|[LawNet-Viz: A Web-based System to Visually Explore Networks of Law Article References](https://doi.org/10.1145/3477495.3531668)|Lucio La Cava, Andrea Simeri, Andrea Tagarelli|University of Calabria, Rende (CS), Italy|We present LawNet-Viz, a web-based tool for the modeling, analysis and visualization of law reference networks extracted from a statute law corpus. LawNet-Viz is designed to support legal research tasks and help legal professionals as well as laymen visually exploring the article connections built upon the explicit law references detected in the article contents. To demonstrate LawNet-Viz, we show its application to the Italian Civil Code (ICC), which exploits a recent BERT-based model fine-tuned on the ICC. LawNet-Viz is a system prototype that is planned for product development.|我们介绍了 LawNet-Viz，这是一个基于网络的工具，用于对从成文法语料库中提取的法律参考网络进行建模、分析和可视化。LawNet-Viz 旨在支持法律研究任务，帮助法律专业人士以及外行人士在文章内容中发现的明确的法律参考文献的基础上，直观地探索文章之间的联系。为了演示 LawNet-Viz，我们展示了它在意大利民法典(ICC)中的应用，该法典利用了最近在 ICC 上微调的基于 BERT 的模型。LawNet-Viz 是计划用于产品开发的系统原型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LawNet-Viz:+A+Web-based+System+to+Visually+Explore+Networks+of+Law+Article+References)|0|
|[Quote Erat Demonstrandum: A Web Interface for Exploring the Quotebank Corpus](https://doi.org/10.1145/3477495.3531696)|Vuk Vukovic, Akhil Arora, HuanCheng Chang, Andreas Spitz, Robert West|University of Konstanz, Konstanz, Germany; EPFL, Lausanne, Switzerland|The use of attributed quotes is the most direct and least filtered pathway of information propagation in news. Consequently, quotes play a central role in the conception, reception, and analysis of news stories. Since quotes provide a more direct window into a speaker's mind than regular reporting, they are a valuable resource for journalists and researchers alike. While substantial research efforts have been devoted to methods for the automated extraction of quotes from news and their attribution to speakers, few comprehensive corpora of attributed quotes from contemporary sources are available to the public. Here, we present an adaptive web interface for searching Quotebank, a massive collection of quotes from the news, which we make available at https://quotebank.dlab.tools.|引用属性是新闻信息传播中最直接、过滤最少的途径。因此，引文在新闻故事的概念、接受和分析中起着核心作用。由于引语比常规报道提供了一个更直接的窗口来了解演讲者的思想，因此它们对记者和研究人员都是一种宝贵的资源。虽然大量的研究工作致力于从新闻中自动摘录引语及其对发言者的归属的方法，但公众很少能够获得来自当代来源的归属引语的综合语料库。在这里，我们提供了一个自适应的网络界面，用于搜索“报价银行”(Quotebank) ，这是一个来自新闻的大量引用集合，我们可以在 https://Quotebank.dlab.tools 上找到它。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Quote+Erat+Demonstrandum:+A+Web+Interface+for+Exploring+the+Quotebank+Corpus)|0|
|[Unsupervised Product Offering Title Quality Scores](https://doi.org/10.1145/3477495.3536333)|Henry S. Vieira|LuizaLabs, São Paulo, Brazil|The title of a product offering is the consolidation of a product's characteristics in textual format for user consumption. The low quality of the textual content of a product's title can negatively influence the entire shopping experience. The negative experience can start with the impossibility of discovering a desired product, going from problems in identifying a product and its characteristics up to the purchase of an unwanted item. A solution to this problem is to establish an indicator that automatically describes the quality of the product title. With this assessment, it is possible to notify sellers who have registered products with poor quality titles and encourage revisions or suggest improvements. The focus of this work is to show how it is possible to assign a score that indicates the descriptive quality of product offers in an e-commerce marketplace environment using unsupervised methods.|产品提供的标题是以文本格式整合产品的特征以供用户使用。产品标题文字内容的低质量会对整个购物体验产生负面影响。消极的体验可以从发现想要的产品的不可能性开始，从识别产品及其特征的问题到购买不想要的产品。解决这个问题的一个办法是建立一个自动描述产品名称质量的指标。通过这种评估，可以通知注册产品质量低劣的卖方，并鼓励修改或提出改进建议。这项工作的重点是展示如何可以指定一个分数，表明在电子商务市场环境中的产品提供的描述性质量使用无监督的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unsupervised+Product+Offering+Title+Quality+Scores)|0|
|[Few-shot Information Extraction is Here: Pre-train, Prompt and Entail](https://doi.org/10.1145/3477495.3532786)|Eneko Agirre|University of the Basque Country UPV/EHU, Donostia, Spain|Deep Learning has made tremendous progress in Natural Language Processing (NLP), where large pre-trained language models (PLM) fine-tuned on the target task have become the predominant tool. More recently, in a process called prompting, NLP tasks are rephrased as natural language text, allowing us to better exploit linguistic knowledge learned by PLMs and resulting in significant improvements. Still, PLMs have limited inference ability. In the Textual Entailment task, systems need to output whether the truth of a certain textual hypothesis follows from the given premise text. Manually annotated entailment datasets covering multiple inference phenomena have been used to infuse inference capabilities to PLMs. This talk will review these recent developments, and will present an approach that combines prompts and PLMs fine-tuned for textual entailment that yields state-of-the-art results on Information Extraction (IE) using only a small fraction of the annotations. The approach has additional benefits, like the ability to learn from different schemas and inference datasets. These developments enable a new paradigm for IE where the expert can define the domain-specific schema using natural language and directly run those specifications, annotating a handful of examples in the process. A user interface based on this new paradigm will also be presented. Beyond IE, inference capabilities could be extended, acquired and applied from other tasks, opening a new research avenue where entailment and downstream task performance improve in tandem.|深度学习在自然语言处理(NLP)领域取得了巨大的进步，其中针对目标任务进行微调的大型预训练语言模型(PLM)已成为主要的工具。最近，在一个叫做“提示”的过程中，NLP 任务被重新定义为自然语言文本，使我们能够更好地利用 PLM 学到的语言知识，从而带来显著的改进。不过，PLM 的推理能力有限。在文字蕴涵任务中，系统需要输出某个文本假设的真实性是否来自给定的前提文本。包含多种推理现象的人工注释的蕴涵数据集已经被用来为 PLM 注入推理能力。这个演讲将回顾这些最近的发展，并将提出一个方法，结合提示和 PLM 微调的文字蕴涵，产生最先进的结果在信息抽取(IE)使用只有一小部分的注释。这种方法还有其他好处，比如可以从不同的模式和推断数据集中学习。这些开发为 IE 提供了一个新的范例，在这个范例中，专家可以使用自然语言定义特定于领域的模式，并直接运行这些规范，在过程中注释一些示例。本文还将介绍一个基于这种新范例的用户界面。在 IE 之外，推理能力可以从其他任务中得到扩展、获取和应用，开辟了一条新的研究途径，其中蕴含和下游任务绩效同步提高。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Few-shot+Information+Extraction+is+Here:+Pre-train,+Prompt+and+Entail)|0|
|[Improving Implicit Alternating Least Squares with Ring-based Regularization](https://doi.org/10.1145/3477495.3531995)|Rui Fan, Jin Chen, Jin Zhang, Defu Lian, Enhong Chen|University of Electronic Science and Technology of China, Chengdu, China; University of Science and Technology of China, Hefei, China|Due to the widespread presence of implicit feedback, recommendation based on them has been a long-standing research problem in academia and industry. However, it suffers from the extremely-sparse problem, since each user only interacts with a few items. One well-known and good-performing method is to treat each user's all uninteracted items as negative with low confidence. The method intrinsically imposes an implicit regularization to penalize large deviation of each user's preferences for uninteracted items from a constant. However, these methods have to assume a constant-rating prior to uninteracted items, which may be questionable. In this paper, we propose a novel ring-based regularization to penalize significant differences of each user's preferences between each item and some other items. The ring structure, described by an item graph, determines which other items are selected for each item in the regularization. The regularization not only averts the introduction of the prior ratings but also implicitly penalizes the remarkable preference differences for all items according to theoretical analysis. However, optimizing the recommenders with the regularization still suffers from computational challenges, so we develop a scalable alternating least square algorithm by carefully designing gradient computation. Therefore, as long as connecting each item with a sublinear/constant number of other items in the item graph, the overall learning algorithm could be comparably efficient to the existing algorithms. The proposed regularization is extensively evaluated with several public recommendation datasets, where the results show that the regularization could lead to considerable improvements in recommendation performance.|由于内隐反馈的广泛存在，基于内隐反馈的推荐已经成为学术界和业界长期研究的课题。但是，由于每个用户只与少数几个项目交互，因此它遇到了极其稀疏的问题。一个众所周知的好方法是将每个用户的所有未交互的项目视为负面的，并且信心不足。这种方法本质上强加了一种隐式正则化，以惩罚每个用户对常量中未交互项偏好的巨大偏差。但是，这些方法必须在未交互的项目之前假设一个常量评级，这可能是值得怀疑的。在本文中，我们提出了一个新的基于环的正则化，以惩罚每个用户的偏好之间的显着差异，每个项目和其他一些项目。由项目图描述的环结构确定为正则化中的每个项目选择哪些其他项目。根据理论分析，规则化不仅避免了先验评分的引入，而且隐含地惩罚了对所有项目的显著偏好差异。然而，正则化推荐算法的优化仍然面临着计算上的挑战，因此我们通过精心设计梯度计算，发展了一种可扩展的交替最小二乘算法。因此，只要将每个项目与项目图中其他项目的次线性/常数连接起来，整个学习算法就可以比现有算法更有效。利用若干公共推荐数据集对拟议的规范化进行了广泛评估，结果表明，规范化可大大改善推荐性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Implicit+Alternating+Least+Squares+with+Ring-based+Regularization)|0|
|[Target-aware Abstractive Related Work Generation with Contrastive Learning](https://doi.org/10.1145/3477495.3532065)|Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Rui Yan, Xin Gao, Xiangliang Zhang|Renmin University of China, Beijing, China; University of Notre Dame & KAUST, South Bend, China; KAUST, Jeddah, Saudi Arabia; Peking University, Beijing, China; KAUST, Jeddah, China|The related work section is an important component of a scientific paper, which highlights the contribution of the target paper in the context of the reference papers. Authors can save their time and effort by using the automatically generated related work section as a draft to complete the final related work. Most of the existing related work section generation methods rely on extracting off-the-shelf sentences to make a comparative discussion about the target work and the reference papers. However, such sentences need to be written in advance and are hard to obtain in practice. Hence, in this paper, we propose an abstractive target-aware related work generator (TAG), which can generate related work sections consisting of new sentences. Concretely, we first propose a target-aware graph encoder, which models the relationships between reference papers and the target paper with target-centered attention mechanisms. In the decoding process, we propose a hierarchical decoder that attends to the nodes of different levels in the graph with keyphrases as semantic indicators. Finally, to generate a more informative related work, we propose multi-level contrastive optimization objectives, which aim to maximize the mutual information between the generated related work with the references and minimize that with non-references. Extensive experiments on two public scholar datasets show that the proposed model brings substantial improvements over several strong baselines in terms of automatic and tailored human evaluations.|相关工作部分是科学论文的一个重要组成部分，它突出了目标论文在参考文件中的贡献。作者可以通过使用自动生成的相关工作部分作为草稿来完成最终的相关工作，从而节省时间和精力。现有的相关作品章节生成方法大多依赖于抽取现成的句子，对目标作品和参考文献进行比较讨论。然而，这样的句子需要事先写好，在实践中很难获得。因此，本文提出了一种抽象的目标感知相关工作生成器(TAG) ，它可以生成由新句子组成的相关工作部分。具体地说，我们首先提出了一种目标感知图形编码器，它使用以目标为中心的注意机制来模拟参考文献和目标文献之间的关系。在解码过程中，我们提出了一种以关键词作为语义指标的层次化解码器，它可以处理图中不同层次的节点。最后，为了生成信息量更大的相关作品，我们提出了多层次对比优化目标，目的是最大化生成的相关作品与参考文献之间的相互信息，最小化非参考文献之间的相互信息。对两个公共学者数据集的大量实验表明，该模型在自动化和量身定制的人类评估方面比几个强大的基线带来了实质性的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Target-aware+Abstractive+Related+Work+Generation+with+Contrastive+Learning)|0|
|[Information Need Awareness: An EEG Study](https://doi.org/10.1145/3477495.3531999)|Dominika Michalkova, Mario ParraRodriguez, Yashar Moshfeghi|University of Strathclyde, Glasgow, United Kingdom|A fundamental goal of Information Retrieval (IR) is to satisfy search­ers' information need (IN). Advances in neuroimaging technologies have allowed for interdisciplinary research to investigate the brain activity associated with the realisation of IN. While these studies have been informative, they were not able to capture the cognitive processes underlying the realisation of IN and the interplay between them with a high temporal resolution. This paper aims to investigate this research question by inferring the variability of brain activity based on the contrast of a state of IN with the two other (no-IN) scenarios. To do so, we employed Electroencephalography (EEG) and constructed an Event-Related Potential (ERP) analysis of the brain signals captured while the participants were experiencing the realisation of IN. In particular, the brain signals of 24 healthy participants were captured while performing a Question-Answering (Q/A) Task. Our results show a link between the early stages of processing, corresponding to awareness and the late activity, meaning memory control mechanisms. Our findings also show that participants exhibited early N1-P2 complex indexing awareness processes and indicate, thus, that the realisation of IN is manifested in the brain before it reaches the user's consciousness. This research contributes novel insights into a better understanding of IN and informs the design of IR systems to better satisfy it.|信息检索的一个基本目标是满足用户的信息需求。神经影像技术的进步使得科际整合能够研究与智能网络实现相关的大脑活动。虽然这些研究提供了很多信息，但是他们并没有能够高时间解析度地捕捉到智力认知的认知过程以及它们之间的相互作用。本文旨在通过比较智力状态与其他两种情境(无智力状态)的差异来推断大脑活动的变异性，从而探讨这一研究问题。为了做到这一点，我们使用了脑电图(EEG) ，并构建了一个事件相关电位(ERP)分析，分析参与者在体验智力活动时捕捉到的大脑信号。特别是，24名健康参与者的大脑信号在进行问答(Q/A)任务时被捕获。我们的研究结果显示，早期的加工阶段，相应的意识和晚期的活动，意味着记忆控制机制之间的联系。我们的研究结果还表明，参与者表现出早期的 N1-P2复杂的索引意识过程，并表明，因此，实现 IN 是表现在大脑之前，达到用户的意识。这项研究为更好地理解智能网提供了新的见解，并为设计更好地满足智能网的红外系统提供了参考。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Information+Need+Awareness:+An+EEG+Study)|0|
|[Unifying Cross-lingual Summarization and Machine Translation with Compression Rate](https://doi.org/10.1145/3477495.3532071)|Yu Bai, Heyan Huang, Kai Fan, Yang Gao, Yiming Zhu, Jiaao Zhan, Zewen Chi, Boxing Chen||Cross-lingual Summarization (CLS), converting a document into a cross-lingual summary, is highly related to Machine Translation (MT) task. However, MT resources are still underutilized for the CLS task. In this paper, we propose a novel task, Cross-lingual Summarization with Compression rate (CSC), to benefit cross-lingual summarization through large-scale MT corpus. Through introducing compression rate, we regard MT task as a special CLS task with the compression rate of 100%. Hence they can be trained as a unified task, sharing knowledge more effectively. Moreover, to bridge these two tasks smoothly, we propose a simple yet effective data augmentation method to produce document-summary pairs with different compression rates. The proposed method not only improves the performance of CLS task, but also provides controllability to generate summaries in desired lengths. Experiments demonstrate that our method outperforms various strong baselines.|跨语言摘要(CLS)是将文档转换为跨语言摘要的一种技术，它与机器翻译(MT)任务密切相关。然而，用于 CLS 任务的 MT 资源仍未得到充分利用。在本文中，我们提出了一个新的任务，跨语言压缩率摘要(CSC) ，以利于跨语言摘要通过大规模的机器翻译语料库。通过引入压缩率，将机器翻译任务视为一种特殊的 CLS 任务，压缩率为100% 。因此，他们可以被训练成一个统一的任务，更有效地分享知识。此外，为了平滑地连接这两个任务，我们提出了一种简单而有效的数据增强方法来产生不同压缩率的文档-摘要对。该方法不仅提高了 CLS 任务的性能，而且提供了生成所需长度摘要的可控性。实验结果表明，该方法的性能优于各种强基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unifying+Cross-lingual+Summarization+and+Machine+Translation+with+Compression+Rate)|0|
|[What Makes the Story Forward?: Inferring Commonsense Explanations as Prompts for Future Event Generation](https://doi.org/10.1145/3477495.3532080)|Li Lin, Yixin Cao, Lifu Huang, Shuang Li, Xuming Hu, Lijie Wen, Jianmin Wang|Singapore Management University, Singapore, Singapore; Virginia Tech, Blacksburg, VA, USA; Tsinghua University, Beijing, China|Prediction over event sequences is critical for many real-world applications in Information Retrieval and Natural Language Processing. Future Event Generation (FEG) is a challenging task in event sequence prediction because it requires not only fluent text generation but also commonsense reasoning to maintain the logical coherence of the entire event story. In this paper, we propose a novel explainable FEG framework, Coep. It highlights and integrates two types of event knowledge, sequential knowledge of direct event-event relations and inferential knowledge that reflects the intermediate character psychology between events, such as intents, causes, reactions, which intrinsically pushes the story forward. To alleviate the knowledge forgetting issue, we design two modules, IM and GM, for each type of knowledge, which are combined via prompt tuning. First, IM focuses on understanding inferential knowledge to generate commonsense explanations and provide a soft prompt vector for GM. We also design a contrastive discriminator for better generalization ability. Second, GM generates future events by modeling direct sequential knowledge with the guidance of IM. Automatic and human evaluation demonstrate that our approach can generate more coherent, specific, and logical future events.|对事件序列的预测对于信息检索和自然语言处理中的许多实际应用来说是至关重要的。未来事件生成(FEG)是事件序列预测中的一个具有挑战性的任务，因为它不仅需要流畅的文本生成，而且需要常识推理来保持整个事件故事的逻辑一致性。在本文中，我们提出了一个新的可解释的 FEG 框架，Coep。它突出并整合了两种类型的事件知识，即直接事件-事件关系的序贯知识和反映事件之间的中间人物心理的推理知识，这些中间人物心理在本质上推动了故事的发展。为了解决知识遗忘问题，我们针对每种类型的知识设计了两个模块，即 IM 模块和 GM 模块，并通过快速调整将它们结合起来。首先，IM 侧重于理解推理知识，产生常识性的解释，并为 GM 提供一个软提示向量。为了提高泛化能力，我们还设计了一种对比鉴别器。其次，GM 在 IM 的指导下，通过建模直接序列知识生成未来事件。自动和人工评估表明，我们的方法可以生成更加连贯、具体和合乎逻辑的未来事件。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=What+Makes+the+Story+Forward?:+Inferring+Commonsense+Explanations+as+Prompts+for+Future+Event+Generation)|0|
|[A Dual-Expert Framework for Event Argument Extraction](https://doi.org/10.1145/3477495.3531923)|Rui Li, Wenlin Zhao, Cheng Yang, Sen Su|Beijing University of Posts and Telecommunications, Beijing, China|Event argument extraction (EAE) is an important information extraction task, which aims to identify the arguments of an event described in a given text and classify the roles played by them. A key characteristic in realistic EAE data is that the instance numbers of different roles follow an obvious long-tail distribution. However, the training and evaluation paradigms of existing EAE models either prone to neglect the performance on "tail roles'', or change the role instance distribution for model training to an unrealistic uniform distribution. Though some generic methods can alleviate the class imbalance in long-tail datasets, they usually sacrifice the performance of "head classes'' as a trade-off. To address the above issues, we propose to train our model on realistic long-tail EAE datasets, and evaluate the average performance over all roles. Inspired by the Mixture of Experts (MOE), we propose a Routing-Balanced Dual Expert Framework (RBDEF), which divides all roles into "head" and "tail" two scopes and assigns the classifications of head and tail roles to two separate experts. In inference, each encoded instance will be allocated to one of the two experts by a routing mechanism. To reduce routing errors caused by the imbalance of role instances, we design a Balanced Routing Mechanism (BRM), which transfers several head roles to the tail expert to balance the load of routing, and employs a tri-filter routing strategy to reduce the misallocation of the tail expert's instances. To enable an effective learning of tail roles with scarce instances, we devise Target-Specialized Meta Learning (TSML) to train the tail expert. Different from other meta learning algorithms that only search a generic parameter initialization equally applying to infinite tasks, TSML can adaptively adjust its search path to obtain a specialized initialization for the tail expert, thereby expanding the benefits to the learning of tail roles. In experiments, RBDEF significantly outperforms the state-of-the-art EAE models and advanced methods for long-tail data.|事件参数提取(EAE)是一项重要的信息抽取任务，其目的是识别给定文本中描述的事件的参数，并对它们所扮演的角色进行分类。实际 EAE 数据的一个关键特征是不同角色的实例数量遵循明显的长尾分布。然而，现有 EAE 模型的训练和评估范式要么忽视了“尾部角色”的表现，要么将模型训练的角色实例分布改变为不现实的统一分布。虽然一些通用的方法可以缓解长尾数据集中的类不平衡，但它们通常牺牲“头类”的性能作为一种权衡。为了解决上述问题，我们建议在实际的长尾 EAE 数据集上训练我们的模型，并评估所有角色的平均性能。受专家混合模型(MOE)的启发，本文提出了一种路由平衡双专家框架(RBDEF) ，该框架将所有角色划分为“头部”和“尾部”两个范围，并将“头部”和“尾部”角色的分类分配给两个独立的专家。在推理中，每个编码实例将通过路由机制分配给两个专家中的一个。为了减少由于角色实例不平衡而引起的路由错误，设计了一种平衡路由机制(BRM) ，将多个主要角色转移给尾部专家以平衡路由负载，并采用三重过滤路由策略来减少尾部专家实例的错误分配。为了能够有效地学习尾部角色与稀少的实例，我们设计了目标专门化元学习(TSML) ，以培训尾部专家。不同于其他元学习算法，只搜索一个通用的参数初始化同样适用于无限的任务，TSML 可以自适应地调整其搜索路径，以获得专门的初始化尾部专家，从而扩大的好处，尾部角色的学习。在实验中，RBDEF 的性能明显优于最先进的 EAE 模型和先进的长尾数据处理方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Dual-Expert+Framework+for+Event+Argument+Extraction)|0|
|[CorED: Incorporating Type-level and Instance-level Correlations for Fine-grained Event Detection](https://doi.org/10.1145/3477495.3531956)|Jiawei Sheng, Rui Sun, Shu Guo, Shiyao Cui, Jiangxia Cao, Lihong Wang, Tingwen Liu, Hongbo Xu|National Computer Network Emergency Response Technical Team/Coordination Center of China, Beijing, China; Beihang University, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences & UCAS, Beijing, China|Event detection (ED) is a pivotal task for information retrieval, which aims at identifying event triggers and classifying them into pre-defined event types. In real-world applications, events are usually annotated with numerous fine-grained types, which often arises long-tail type nature and co-occurrence event nature. Existing studies explore the event correlations without full utilization, which may limit the capability of event detection. This paper simultaneously incorporates both the type-level and instance-level event correlations, and proposes a novel framework, termed as CorED. Specifically, we devise an adaptive graph-based type encoder to capture instance-level correlations, learning type representations not only from their training data but also from their relevant types, thus leading to more informative type representations especially for the low-resource types. Besides, we devise an instance interactive decoder to capture instance-level correlations, which predicts event instance types conditioned on the contextual typed event instances, leveraging co-occurrence events as remarkable evidence in prediction. We conduct experiments on two public benchmarks, MAVEN and ACE-2005 dataset. Empirical results demonstrate the unity of both type-level and instance-level correlations, and the model achieves effectiveness performance on both benchmarks.|事件检测是信息检索的关键任务，其目的是识别事件触发器，并将其分类为预先定义的事件类型。在实际应用程序中，事件通常用许多细粒度类型进行注释，这些类型通常具有长尾类型性质和共现事件性质。现有的研究在未充分利用事件相关性的情况下，可能会限制事件检测的能力。本文同时结合了类型级和实例级的事件相关性，提出了一种新的框架，称为 CoreED。具体来说，我们设计了一种基于自适应图形的类型编码器来捕获实例级相关性，不仅从它们的训练数据中学习类型表示，而且从它们的相关类型中学习类型表示，从而导致更多的信息类型表示，特别是对于低资源类型。此外，我们设计了一个实例交互式解码器来捕获实例级的相关性，该解码器利用共现事件作为预测中的显著证据来预测以上下文类型的事件实例为条件的事件实例类型。我们在 MAVEN 和 ACE-2005两个公共基准数据集上进行了实验。实证结果表明，该模型实现了类型级和实例级相关性的统一，达到了两个基准的有效性表现。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CorED:+Incorporating+Type-level+and+Instance-level+Correlations+for+Fine-grained+Event+Detection)|0|
|[QUASER: Question Answering with Scalable Extractive Rationalization](https://doi.org/10.1145/3477495.3532049)|Asish Ghoshal, Srinivasan Iyer, Bhargavi Paranjape, Kushal Lakhotia, Scott Wentau Yih, Yashar Mehdad|University of Washington, Seattle, WA, USA; Meta AI, Seattle, WA, USA|Designing natural language processing (NLP) models that produce predictions by first extracting a set of relevant input sentences, i.e., rationales, is gaining importance for improving model interpretability and producing supporting evidence for users. Current unsupervised approaches are designed to extract rationales that maximize prediction accuracy, which is invariably obtained by exploiting spurious correlations in datasets, and leads to unconvincing rationales. In this paper, we introduce unsupervised generative models to extract dual-purpose rationales, which must not only be able to support a subsequent answer prediction, but also support a reproduction of the input query. We show that such models can produce more meaningful rationales, that are less influenced by dataset artifacts, and as a result, also achieve the state-of-the-art on rationale extraction metrics on four datasets from the ERASER benchmark, significantly improving upon previous unsupervised methods. Our multi-task model is scalable and enables using state-of-the-art pretrained language models to design explainable question answering systems.|设计自然语言处理(NLP)模型，通过首先提取一组相关的输入句，即基本原理，来产生预测，这对于提高模型的可解释性和为用户提供支持性证据越来越重要。目前的无监督方法旨在提取最大限度地提高预测准确性的理由，这些理由总是通过利用数据集中的虚假相关性来获得，并导致不令人信服的理由。在本文中，我们引入了无监督生成模型来提取双重用途的基本原理，它不仅要能够支持后续的答案预测，而且还要能够支持输入查询的重现。我们表明，这样的模型可以产生更有意义的基本原理，这些基本原理受数据集伪影的影响较小，因此，在 ERASER 基准的四个数据集上也实现了最先进的基本原理提取指标，显着改善了以前的无监督方法。我们的多任务模型是可扩展的，并且能够使用最先进的预先训练的语言模型来设计可解释的问答系统。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=QUASER:+Question+Answering+with+Scalable+Extractive+Rationalization)|0|
|[PTAU: Prompt Tuning for Attributing Unanswerable Questions](https://doi.org/10.1145/3477495.3532048)|Jinzhi Liao, Xiang Zhao, Jianming Zheng, Xinyi Li, Fei Cai, Jiuyang Tang|National University of Defense Technology, Changsha, China|Current question answering systems are insufficient when confronting real-life scenarios, as they can hardly be aware of whether a question is answerable given its context. Hence, there is a recent pursuit of unanswerability of a question and its attribution. Attribution of unanswerability requires the system to choose an appropriate cause for an unanswerable question. As the task is sophisticated for even human beings, it is expensive to acquire labeled data, which makes it a low-data regime problem. Moreover, the causes themselves are semantically abstract and complex, and the process of attribution is heavily question- and context-dependent. Thus, a capable model has to carefully appreciate the causes, and then, judiciously contrast the question with its context, in order to cast it into the right cause. In response to the challenges, we present PTAU, which refers to and implements a high-level human reading strategy such that one reads with anticipation. In specific, PTAU leverages the recent prompt-tuning paradigm, and is further enhanced with two innovatively conceived modules: 1) a cause-oriented template module that constructs continuous templates towards certain attributing class in high dimensional vector space; and 2) a semantics-aware label module that exploits label semantics through contrastive learning to render the classes distinguishable. Extensive experiments demonstrate that the proposed design better enlightens not only the attribution model, but also current question answering models, leading to superior performance.|当前的问答系统在面对现实情景时是不够的，因为它们很难意识到一个问题是否可以根据其上下文进行回答。因此，最近有一个追求一个问题及其归属无法回答。无法回答的归因要求系统为无法回答的问题选择合适的原因。由于这项任务甚至对人类来说都是复杂的，因此获取标记数据的成本很高，这使得它成为一个低数据量的系统问题。此外，原因本身具有语义抽象性和复杂性，归因过程严重依赖于问题和上下文。因此，一个有能力的模型必须仔细地鉴别原因，然后，明智地将问题与其上下文进行对比，以便将其投入正确的原因。为了应对这些挑战，我们提出了 PTAU，它提出并实施了一种高水平的人类阅读策略，使人们在阅读时带有预期性。具体而言，PTAU 利用了最近的提示调优范式，并进一步增强了两个创新构想的模块: 1)面向原因的模板模块，在高维向量空间中构建针对特定属性类的连续模板; 2)语义感知的标签模块，通过对比学习利用标签语义来使类可区分。大量实验表明，该设计不仅对归因模型有较好的启发作用，而且对现有的问答模型也有较好的启发作用，具有较好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PTAU:+Prompt+Tuning+for+Attributing+Unanswerable+Questions)|0|
|[DGQAN: Dual Graph Question-Answer Attention Networks for Answer Selection](https://doi.org/10.1145/3477495.3532084)|Haitian Yang, Xuan Zhao, Yan Wang, Min Li, Wei Chen, Weiqing Huang|York University, Toronto, Canada; Shanghai University of Finance and Economics, Shanghai, China; Institute of Information Engineering, Chinese Academy of Sciences & School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China|Community question answering (CQA) becomes increasingly prevalent in recent years, providing platforms for users with various backgrounds to obtain information and share knowledge. However, the redundancy and lengthiness issues of crowd-sourced answers limit the performance of answer selection, thus leading to difficulties in reading or even misunderstandings for community users. To solve these problems, we propose the dual graph question-answer attention networks (DGQAN) for answer selection task. Aims to fully understand the internal structure of the question and the corresponding answer, firstly, we construct a dual-CQA concept graph with graph convolution networks using the original question and answer text. Specifically, our CQA concept graph exploits the correlation information between question-answer pairs to construct two sub-graphs (QSubject-Answer and QBody-Answer), respectively. Further, a novel dual attention mechanism is incorporated to model both the internal and external semantic relations among questions and answers. More importantly, we conduct experiment to investigate the impact of each layer in the BERT model. The experimental results show that DGQAN model achieves state-of-the-art performance on three datasets (SemEval-2015, 2016, and 2017), outperforming all the baseline models.|近年来，社区问答越来越普遍，为不同背景的用户提供了获取信息和分享知识的平台。然而，众包答案的冗余和冗长问题限制了答案选择的性能，从而导致阅读困难，甚至对社区用户产生误解。为了解决这些问题，我们提出了双图问答注意网络(DGQAN)的答案选择任务。为了充分理解问题和相应答案的内部结构，首先利用原始问答文本构建了一个具有图卷积网络的双 CQA 概念图。具体来说，我们的 CQA 概念图利用问题-答案对之间的相关信息分别构造了两个子图(问题-答案和问题-答案)。此外，还引入了一种新的双重注意机制来建立问答之间的内部和外部语义关系模型。更重要的是，我们通过实验研究了 BERT 模型中各层的影响。实验结果表明，DGQAN 模型在三个数据集(SemEval-2015,2016和2017)上实现了最先进的性能，优于所有基线模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DGQAN:+Dual+Graph+Question-Answer+Attention+Networks+for+Answer+Selection)|0|
|[Towards Event-level Causal Relation Identification](https://doi.org/10.1145/3477495.3531758)|Chuang Fan, Daoxing Liu, Libo Qin, Yue Zhang, Ruifeng Xu|WestLake University, Hangzhou, China; Harbin Institute of Technology, Harbin, China; Harbin Institute of Technology (Shenzhen), Shenzhen, China|Existing methods usually identify causal relations between events at the mention-level, which takes each event mention pair as a separate input. As a result, they either suffer from conflicts among causal relations predicted separately or require a set of additional constraints to resolve such conflicts. We propose to study this task in a more realistic setting, where event-level causality identification can be made. The advantage is two folds: 1) with modeling different mentions of an event as a single unit, no more conflicts among predicted results, without any extra constraints; 2) with the use of diverse knowledge sources (e.g., co-occurrence and coreference relations), a rich graph-based event structure can be induced from the document for supporting event-level causal inference. Graph convolutional network is used to encode such structural information, which aims to capture the local and non-local dependencies among nodes. Results show that our model achieves the best performance under both mention- and event-level settings, outperforming a number of strong baselines by at least 2.8% on F1 score.|现有的方法通常在提及级别上确定事件之间的因果关系，它将每个事件提及对作为一个单独的输入。因此，它们要么受到单独预测的因果关系之间的冲突的影响，要么需要一套额外的约束来解决这些冲突。我们建议在一个更现实的背景下研究这个任务，在这里可以进行事件级的因果关系识别。其优点有两个方面: 1)将事件的不同提及建模为一个单元，预测结果之间没有冲突，没有任何额外的约束; 2)利用多样化的知识来源(例如，共现关系和共参照关系) ，可以从文档中诱导出丰富的基于图的事件结构，以支持事件级因果推理。图卷积网络用于对这些结构信息进行编码，目的是捕获节点之间的局部和非局部依赖关系。结果表明，我们的模型在提及和事件级别设置下都达到了最佳性能，在 F1得分上比一些强基线至少高出2.8% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Event-level+Causal+Relation+Identification)|0|
|[Hierarchical Task-aware Multi-Head Attention Network](https://doi.org/10.1145/3477495.3531781)|Jing Du, Lina Yao, Xianzhi Wang, Bin Guo, Zhiwen Yu|Northwestern Polytechnical University, Xi'an, Shaanxi, China; The University of New South Wales, Sydney, NSW, Australia; University of Technology Sydney, Sydney, NSW, Australia|Neural Multi-task Learning is gaining popularity as a way to learn multiple tasks jointly within a single model. While related research continues to break new ground, two major limitations still remain, including (i) poor generalization to scenarios where tasks are loosely correlated; and (ii) under-investigation on global commonality and local characteristics of tasks. Our aim is to bridge these gaps by presenting a neural multi-task learning model coined Hierarchical Task-aware Multi-headed Attention Network (HTMN). HTMN explicitly distinguishes task-specific features from task-shared features to reduce the impact caused by weak correlation between tasks. The proposed method highlights two parts: Multi-level Task-aware Experts Network that identifies task-shared global features and task-specific local features, and Hierarchical Multi-Head Attention Network that hybridizes global and local features to profile more robust and adaptive representations for each task. Afterwards, each task tower receives its hybrid task-adaptive representation to perform task-specific predictions. Extensive experiments on two real datasets show that HTMN consistently outperforms the compared methods on a variety of prediction tasks.|神经多任务学习作为一种在单个模型中联合学习多个任务的方法越来越受到人们的欢迎。尽管相关研究不断取得新进展，但仍然存在两个主要的局限性，包括(i)对任务松散相关的情景概括不足; 以及(ii)对任务的全局共性和局部特征调查不足。我们的目标是通过提出一个神经多任务学习模型来弥补这些差距，该模型被称为分层任务感知多头注意网络(HTMN)。HTMN 明确区分任务特定特性和任务共享特性，以减少任务之间相关性较弱所造成的影响。提出的方法突出了两个部分: 多级任务感知专家网络，识别任务共享的全局特征和任务特定的局部特征，和分层多头注意网络，混合全局和局部特征，以配置更健壮的和自适应的表示为每个任务。然后，每个任务塔接收它的混合任务自适应表示来执行任务特定的预测。在两个实际数据集上进行的大量实验表明，HTMN 在各种预测任务中的表现均优于比较方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Task-aware+Multi-Head+Attention+Network)|0|
|[Enhancing Event-Level Sentiment Analysis with Structured Arguments](https://doi.org/10.1145/3477495.3531784)|Qi Zhang, Jie Zhou, Qin Chen, Qingchun Bai, Liang He|East China Normal University, Shanghai, China; Shanghai Open University, Shanghai, China; Fudan University, Shanghai, China|Previous studies about event-level sentiment analysis (SA) usually model the event as a topic, a category or target terms, while the structured arguments (e.g., subject, object, time and location) that have potential effects on the sentiment are not well studied. In this paper, we redefine the task as structured event-level SA and propose an End-to-End Event-level Sentiment Analysis (E3SA) approach to solve this issue. Specifically, we explicitly extract and model the event structure information for enhancing event-level SA. Extensive experiments demonstrate the great advantages of our proposed approach over the state-of-the-art methods. Noting the lack of the dataset, we also release a large-scale real-world dataset with event arguments and sentiment labelling for promoting more researches.|以往关于事件层面情绪分析(SA)的研究通常将事件建模为一个主题、一个类别或目标术语，而对情绪有潜在影响的结构化论证(如主语、客体、时间和地点)则没有得到很好的研究。本文将任务重新定义为结构化事件级情绪分析，并提出了一种端到端事件级情绪分析(E3SA)方法来解决这一问题。具体来说，我们显式地提取和建模事件结构信息，以增强事件级 SA。大量的实验证明了我们提出的方法相对于最先进的方法的巨大优势。注意到数据集的缺乏，我们也发布了一个大规模的现实世界的数据集与事件论点和情绪标签，以促进更多的研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Event-Level+Sentiment+Analysis+with+Structured+Arguments)|0|
|[Translation-Based Implicit Annotation Projection for Zero-Shot Cross-Lingual Event Argument Extraction](https://doi.org/10.1145/3477495.3531808)|Chenwei Lou, Jun Gao, Changlong Yu, Wei Wang, Huan Zhao, Weiwei Tu, Ruifeng Xu|Tsinghua University, Beijing, China; 4Paradigm Inc, Beijing, China; Harbin Institute of Technology (Shenzhen), Shenzhen, China; The Hong Kong University of Science and Technology, Hong Kong, Hong Kong|Zero-shot cross-lingual event argument extraction (EAE) is a challenging yet practical problem in Information Extraction. Most previous works heavily rely on external structured linguistic features, which are not easily accessible in real-world scenarios. This paper investigates a translation-based method to implicitly project annotations from the source language to the target language. With the use of translation-based parallel corpora, no additional linguistic features are required during training and inference. As a result, the proposed approach is more cost effective than previous works on zero-shot cross-lingual EAE. Moreover, our implicit annotation projection approach introduces less noises and hence is more effective and robust than explicit ones. Experimental results show that our model achieves the best performance, outperforming a number of competitive baselines. The thorough analysis further demonstrates the effectiveness of our model compared to explicit annotation projection approaches.|零镜头跨语言事件参数提取(EAE)是一个具有挑战性的实际信息抽取问题。以往的大多数作品都严重依赖于外部结构化语言特征，而这些特征在现实世界中并不容易获得。本文研究了一种基于翻译的方法来隐式地将注释从源语言投射到目标语言。使用基于翻译的平行语料库，在训练和推理过程中不需要额外的语言特征。结果表明，本文提出的方法比以往针对零镜头跨语言 EAE 的研究更具有成本效益。此外，我们的隐式注释投影方法引入较少的噪声，因此比显式更有效和鲁棒性。实验结果表明，我们的模型达到了最佳的性能，超过了一些竞争基线。深入的分析进一步证明了我们的模型与显式注释投影方法相比的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Translation-Based+Implicit+Annotation+Projection+for+Zero-Shot+Cross-Lingual+Event+Argument+Extraction)|0|
|[Understanding Long Programming Languages with Structure-Aware Sparse Attention](https://doi.org/10.1145/3477495.3531811)|Tingting Liu, Chengyu Wang, Cen Chen, Ming Gao, Aoying Zhou|East China Normal University, Shanghai, China; Alibaba Group, Hangzhou, China|Programming-based Pre-trained Language Models (PPLMs) such as CodeBERT have achieved great success in many downstream code-related tasks. Since the memory and computational complexity of self-attention in the Transformer grow quadratically with the sequence length, PPLMs typically limit the code length to 512. However, codes in real-world applications are generally long, such as code searches, which cannot be processed efficiently by existing PPLMs. To solve this problem, in this paper, we present SASA, a Structure-Aware Sparse Attention mechanism, which reduces the complexity and improves performance for long code understanding tasks. The key components in SASA are top-k sparse attention and Abstract Syntax Tree (AST)-based structure-aware attention. With top-k sparse attention, the most crucial attention relation can be obtained with a lower computational cost. As the code structure represents the logic of the code statements, which is a complement to the code sequence characteristics, we further introduce AST structures into attention. Extensive experiments on CodeXGLUE tasks show that SASA achieves better performance than the competing baselines.|基于编程的预训练语言模型(PPLM) ，如 CodeBERT，在许多下游代码相关的任务中取得了巨大的成功。由于变压器中自注意的内存和计算复杂度随序列长度的二次增长而增长，PPLM 通常将码长限制在512。然而，实际应用中的代码通常都很长，比如代码搜索，现有的 PPLM 无法有效地处理这些代码。为了解决这一问题，本文提出了一种基于结构感知的稀疏注意机制 SASA，该机制降低了长代码理解任务的复杂度，提高了性能。SASA 的关键组成部分是 top-k 稀疏注意和基于抽象语法树(AST)的结构感知注意。使用 top-k 稀疏注意，可以以较低的计算代价得到最关键的注意关系。由于代码结构代表了代码语句的逻辑，是对代码序列特性的补充，因此我们进一步引入了 AST 结构。在 CodeXGLUE 任务上的大量实验表明，SASA 比竞争基线获得了更好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+Long+Programming+Languages+with+Structure-Aware+Sparse+Attention)|0|
|[Dialogue Topic Segmentation via Parallel Extraction Network with Neighbor Smoothing](https://doi.org/10.1145/3477495.3531817)|Jinxiong Xia, Cao Liu, Jiansong Chen, Yuchen Li, Fan Yang, Xunliang Cai, Guanglu Wan, Houfeng Wang|Meituan, Beijing, China; Peking University, Beijing, China|Dialogue topic segmentation is a challenging task in which dialogues are split into segments with pre-defined topics. Existing works on topic segmentation adopt a two-stage paradigm, including text segmentation and segment labeling. However, such methods tend to focus on the local context in segmentation, and the inter-segment dependency is not well captured. Besides, the ambiguity and labeling noise in dialogue segment bounds bring further challenges to existing models. In this work, we propose the Parallel Extraction Network with Neighbor Smoothing (PEN-NS) to address the above issues. Specifically, we propose the parallel extraction network to perform segment extractions, optimizing the bipartite matching cost of segments to capture inter-segment dependency. Furthermore, we propose neighbor smoothing to handle the segment-bound noise and ambiguity. Experiments on a dialogue-based and a document-based topic segmentation dataset show that PEN-NS outperforms state-the-of-art models significantly.|对话主题分割是一个具有挑战性的任务，其中对话分割成具有预定义的主题片段。现有的主题切分研究采用两阶段模式，包括文本切分和段标注。然而，这些方法在分割过程中往往只关注局部上下文，而且不能很好地捕获分段间的依赖关系。此外，对话段边界的模糊性和标注噪声也给现有的模型带来了进一步的挑战。针对上述问题，本文提出了一种基于邻域平滑的并行抽取网络(PEN-NS)。具体来说，我们提出并行提取网络来执行分段提取，优化分段的二部匹配代价来捕获分段间的依赖关系。此外，我们还提出了邻域平滑法来处理分段定界噪声和模糊度。在基于对话和基于文档的主题分割数据集上进行的实验表明，PEN-NS 模型的性能明显优于目前最先进的模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dialogue+Topic+Segmentation+via+Parallel+Extraction+Network+with+Neighbor+Smoothing)|0|
|[Expression Syntax Information Bottleneck for Math Word Problems](https://doi.org/10.1145/3477495.3531824)|Jing Xiong, Chengming Li, Min Yang, Xiping Hu, Bin Hu|Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Lanzhou University, Lanzhou, China; Sun Yat-sen University, Shenzhen, China|Math Word Problems (MWP) aims to automatically solve mathematical questions given in texts. Previous studies tend to design complex models to capture additional information in the original text so as to enable the model to gain more comprehensive features. In this paper, we turn our attention in the opposite direction, and work on how to discard redundant features containing spurious correlations for MWP. To this end, we design an Expression Syntax Information Bottleneck method for MWP (called ESIB) based on variational information bottleneck, which extracts essential features of the expression syntax tree while filtering latent-specific redundancy containing syntax-irrelevant features. The key idea of ESIB is to encourage multiple models to predict the same expression syntax tree for different problem representations of the same problem by mutual learning so as to capture consistent information of expression syntax tree and discard latent-specific redundancy. To improve the generalization ability of the model and generate more diverse expressions, we design a self-distillation loss to encourage the model to rely more on the expression syntax information in the latent space. Experimental results on two large-scale benchmarks show that our model not only achieves state-of-the-art results but also generates more diverse solutions.|数学词汇问题(MWP)旨在自动解决课文中给出的数学问题。以往的研究倾向于设计复杂的模型来捕捉原始文本中的附加信息，从而使模型获得更全面的特征。本文从相反的角度出发，研究了如何去除含有虚假相关的冗余特征。为此，我们设计了一种基于变分信息瓶颈的 MWP 表达式语法信息瓶颈方法(称为 ESIB) ，该方法在过滤包含语法无关特征的潜在特定冗余的同时，提取表达式语法树的基本特征。ESIB 的核心思想是鼓励多个模型通过相互学习对同一问题的不同问题表示预测相同的表达式语法树，从而获取表达式语法树的一致性信息，去除潜在的特定冗余。为了提高模型的泛化能力，生成更多不同的表达式，我们设计了一个自蒸馏损失，以鼓励模型更多地依赖潜在空间中的表达式语法信息。在两个大规模基准上的实验结果表明，该模型不仅取得了最佳的结果，而且产生了更加多样化的解决方案。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Expression+Syntax+Information+Bottleneck+for+Math+Word+Problems)|0|
|[Masking and Generation: An Unsupervised Method for Sarcasm Detection](https://doi.org/10.1145/3477495.3531825)|Rui Wang, Qianlong Wang, Bin Liang, Yi Chen, Zhiyuan Wen, Bing Qin, Ruifeng Xu|Harbin Institute of Technology & Peng Cheng Laboratory, Shenzhen, China; Harbin Institute of Technology, Shenzhen, China; Harbin Institute of Technology, Harbin, China|Existing approaches for sarcasm detection are mainly based on supervised learning, in which the promising performance largely depends on a considerable amount of labeled data or extra information. In the real world scenario, however, the abundant labeled data or extra information requires high labor cost, not to mention that sufficient annotated data is unavailable in many low-resource conditions. To alleviate this dilemma, we investigate sarcasm detection from an unsupervised perspective, in which we explore a masking and generation paradigm in the context to extract the context incongruities for learning sarcastic expression. Further, to improve the feature representations of the sentences, we use unsupervised contrastive learning to improve the sentence representation based on the standard dropout. Experimental results on six perceived sarcasm detection benchmark datasets show that our approach outperforms baselines. Simultaneously, our unsupervised method obtains comparative performance with supervised methods for the intended sarcasm dataset.|现有的挖苦检测方法主要基于监督式学习，其有效性很大程度上取决于大量的标记数据或额外信息。然而，在现实世界的场景中，大量的标记数据或额外的信息需要很高的人工成本，更不用说在许多资源不足的情况下没有足够的注释数据了。为了缓解这一困境，我们从无监督的角度研究了讽刺语的检测问题，探索了语境中的掩蔽和生成范式，以提取语境中的不一致性，从而学习讽刺语的表达。此外，为了改善句子的特征表示，我们使用无监督对比学习来改善基于标准辍学的句子表示。实验结果表明，该方法的性能优于基准测试。同时，我们的无监督方法获得了比较性能的监督方法为预期讽刺数据集。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Masking+and+Generation:+An+Unsupervised+Method+for+Sarcasm+Detection)|0|
|[Learned Token Pruning in Contextualized Late Interaction over BERT (ColBERT)](https://doi.org/10.1145/3477495.3531835)|Carlos Lassance, Maroua Maachou, Joohee Park, Stéphane Clinchant|Naver Labs Europe, Meylan, France; Naver, Seoul, Republic of Korea|BERT-based rankers have been shown very effective as rerankers in information retrieval tasks. In order to extend these models to full-ranking scenarios, the ColBERT model has been recently proposed, which adopts a late interaction mechanism. This mechanism allows for the representation of documents to be precomputed in advance. However, the late-interaction mechanism leads to large index size, as one needs to save a representation for each token of every document. In this work, we focus on token pruning techniques in order to mitigate this problem. We test four methods, ranging from simpler ones to the use of a single layer of attention mechanism to select the tokens to keep at indexing time. Our experiments show that for the MS MARCO-passages collection, indexes can be pruned up to 70% of their original size, without a significant drop in performance. We also evaluate on the MS MARCO-documents collection and the BEIR benchmark, which reveals some challenges for the proposed mechanism.|以 BERT 为基础的排名已经被证明在信息检索任务中非常有效。为了将这些模型扩展到完全排序的场景，最近提出了 ColBERT 模型，该模型采用了一种后交互机制。这种机制允许预先计算文档的表示形式。但是，后期交互机制会导致索引大小增加，因为需要为每个文档的每个标记保存表示形式。在这项工作中，我们重点关注令牌剪枝技术，以减轻这个问题。我们测试了四种方法，从简单的方法到使用单层注意机制在索引时选择要保留的令牌。我们的实验表明，对于 MS MARCO 段收集，索引可以修剪高达原始大小的70% ，性能没有明显下降。我们还对 MS MARCO 文档集和 BEIR 基准进行了评估，揭示了该机制面临的一些挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learned+Token+Pruning+in+Contextualized+Late+Interaction+over+BERT+(ColBERT))|0|
|[GraFN: Semi-Supervised Node Classification on Graph with Few Labels via Non-Parametric Distribution Assignment](https://doi.org/10.1145/3477495.3531838)|Junseok Lee, Yunhak Oh, Yeonjun In, Namkyeong Lee, Dongmin Hyun, Chanyoung Park|POSTECH, Pohang, Republic of Korea; KAIST, Daejeon, Republic of Korea|Despite the success of Graph Neural Networks (GNNs) on various applications, GNNs encounter significant performance degradation when the amount of supervision signals, i.e., number of labeled nodes, is limited, which is expected as GNNs are trained solely based on the supervision obtained from the labeled nodes. On the other hand, recent self-supervised learning paradigm aims to train GNNs by solving pretext tasks that do not require any labeled nodes, and it has shown to even outperform GNNs trained with few labeled nodes. However, a major drawback of self-supervised methods is that they fall short of learning class discriminative node representations since no labeled information is utilized during training. To this end, we propose a novel semi-supervised method for graphs, GraFN, that leverages few labeled nodes to ensure nodes that belong to the same class to be grouped together, thereby achieving the best of both worlds of semi-supervised and self-supervised methods. Specifically, GraFN randomly samples support nodes from labeled nodes and anchor nodes from the entire graph. Then, it minimizes the difference between two predicted class distributions that are non-parametrically assigned by anchor-supports similarity from two differently augmented graphs. We experimentally show that GraFN surpasses both the semi-supervised and self-supervised methods in terms of node classification on real-world graphs.|尽管图形神经网络(GNN)在各种应用中取得了成功，但是当监督信号(即标记节点的数量)受到限制时，GNN 会遇到显著的性能下降，这是预期的，因为 GNN 仅仅基于从标记节点获得的监督进行训练。另一方面，最近的自我监督学习范式旨在通过解决不需要任何标记节点的托辞任务来训练 GNN，并且已经证明它的表现甚至优于用少量标记节点训练的 GNN。然而，自监督方法的一个主要缺点是，由于在训练过程中没有使用标记信息，因此它们不能很好地表示学习类的判别节点。为此，我们提出了一种新的图的半监督方法，GraFN，它利用少量的标记节点来确保属于同一类的节点被分组在一起，从而实现了半监督和自监督方法的最佳结合。具体来说，GraFN 随机采样支持来自标记节点的节点和来自整个图的锚节点。然后，从两个不同的增广图中最小化由锚支持相似性非参数赋值的两个预测类分布之间的差异。实验结果表明，GraFN 在实际图的节点分类方面优于半监督和自监督方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraFN:+Semi-Supervised+Node+Classification+on+Graph+with+Few+Labels+via+Non-Parametric+Distribution+Assignment)|0|
|[Which Discriminator for Cooperative Text Generation?](https://doi.org/10.1145/3477495.3531858)|Antoine Chaffin, Thomas Scialom, Sylvain Lamprier, Jacopo Staiano, Benjamin Piwowarski, Ewa Kijak, Vincent Claveau|ISIR - Sorbonne Université, reciTAL, Paris, France; ISIR - Sorbonne Université, Paris, France; Université Rennes, IRISA, Rennes, France; CNRS, ISIR - Sorbonne Université, Paris, France; reciTAL, Paris, France; CNRS, IRISA, Rennes, France; IRISA, IMATAG, Rennes, France|Language models generate texts by successively predicting probability distributions for next tokens given past ones. A growing field of interest tries to leverage external information in the decoding process so that the generated texts have desired properties, such as being more natural, non toxic, faithful, or having a specific writing style. A solution is to use a classifier at each generation step, resulting in a cooperative environment where the classifier guides the decoding of the language model distribution towards relevant texts for the task at hand. In this paper, we examine three families of (transformer-based) discriminators for this specific task of cooperative decoding: bidirectional, left-to-right and generative ones. We evaluate the pros and cons of these different types of discriminators for cooperative generation, exploring respective accuracy on classification tasks along with their impact on the resulting sample quality and computational performances. We also provide the code of a batched implementation of the powerful cooperative decoding strategy used for our experiments, the Monte Carlo Tree Search, working with each discriminator for Natural Language Generation.|语言模型通过依次预测给定过去标记的下一个标记的概率分布来生成文本。越来越多的研究领域试图在解码过程中利用外部信息，使生成的文本具有期望的特性，如更自然、无毒、忠实或具有特定的写作风格。一个解决方案是在每个生成步骤中使用一个分类器，从而形成一个合作环境，在这个环境中，分类器将语言模型分布的解码引导到手头任务的相关文本中。在这篇论文中，我们针对这个特定的合作解码任务，研究了三类(基于变压器的)鉴别器: 双向的，从左到右的和生成的。我们评估了这些不同类型的鉴别器在合作生成中的优缺点，探讨了它们在分类任务中各自的准确性及其对所得样本质量和计算性能的影响。我们还提供了一个批处理实现的强大的合作解码策略，用于我们的实验，蒙特卡罗树搜索，与自然语言生成的每个鉴别器工作的代码。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Which+Discriminator+for+Cooperative+Text+Generation?)|0|
|[Topological Analysis of Contradictions in Text](https://doi.org/10.1145/3477495.3531881)|Xiangcheng Wu, Xi Niu, Ruhani Rahman|University of North Carolina at Charlotte, Charlotte, NC, USA|Automatically finding contradictions from text is a fundamental yet under-studied problem in natural language understanding and information retrieval. Recently, topology, a branch of mathematics concerned with the properties of geometric shapes, has been shown useful to understand semantics of text. This study presents a topological approach to enhancing deep learning models in detecting contradictions in text. In addition, in order to better understand contradictions, we propose a classification with six types of contradictions. Following that, the topologically enhanced models are evaluated with different contradictions types, as well as different text genres. Overall we have demonstrated the usefulness of topological features in finding contradictions, especially the more latent and more complex contradictions in text.|自动从文本中找出矛盾是自然语言理解和信息检索中一个基本但尚未得到充分研究的问题。近年来，拓扑学作为一门研究 Unicode几何图形列表性质的数学分支，在理解文本语义方面发挥了重要作用。本研究提出了一种拓扑方法来增强深度学习模型在文本矛盾检测中的应用。此外，为了更好地理解矛盾，我们提出了六种类型的矛盾分类。然后，利用不同的矛盾类型和不同的文本类型对拓扑增强模型进行评估。总的来说，我们已经证明了拓扑特征在发现矛盾，特别是在文本中更多的潜在和更复杂的矛盾方面的有用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Topological+Analysis+of+Contradictions+in+Text)|0|
|[Dual Pseudo Supervision for Semi-Supervised Text Classification with a Reliable Teacher](https://doi.org/10.1145/3477495.3531887)|Shujie Li, Min Yang, Chengming Li, Ruifeng Xu|Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Harbin Institute of Technology & Peng Cheng Lab, Shenzhen, China; University of Science and Technology of China, Hefei, China; Sun Yat-sen University, Shenzhen, China|In this paper, we study the semi-supervised text classification (SSTC) by exploring both labeled and extra unlabeled data. One of the most popular SSTC techniques is pseudo-labeling which assigns pseudo labels for unlabeled data via a teacher classifier trained on labeled data. These pseudo labeled data is then applied to train a student classifier. However, when the pseudo labels are inaccurate, the student classifier will learn from inaccurate data and get even worse performance than the teacher. To mitigate this issue, we propose a simple yet efficient pseudo-labeling framework called Dual Pseudo Supervision (DPS), which exploits the feedback signal from the student to guide the teacher to generate better pseudo labels. In particular, we alternately update the student based on the pseudo labeled data annotated by the teacher and optimize the teacher based on the student's performance via meta learning. In addition, we also design a consistency regularization term to further improve the stability of the teacher. With the above two strategies, the learned reliable teacher can provide more accurate pseudo-labels to the student and thus improve the overall performance of text classification. We conduct extensive experiments on three benchmark datasets (i.e., AG News, Yelp and Yahoo) to verify the effectiveness of our DPS method. Experimental results show that our approach achieves substantially better performance than the strong competitors. For reproducibility, we will release our code and data of this paper publicly at https://github.com/GRIT621/DPS.|本文通过对已标记和未标记数据的分析，研究了半监督文本分类算法(SSTC)。最流行的 SSTC 技术之一是伪标记技术，它通过对标记数据进行训练的教师分类器为未标记的数据分配伪标记。然后应用这些伪标记数据训练学生分类器。然而，当伪标签不准确时，学生分类器会从不准确的数据中学习，得到比教师更差的性能。为了解决这个问题，我们提出了一个简单而有效的伪标签框架，称为双伪监督(DPS) ，它利用学生的反馈信号来指导教师生成更好的伪标签。特别是，我们交替更新学生的基础上伪标记数据的教师注释和优化教师的基础上学生的表现通过元学习。此外，我们还设计了一个一致性正则项，以进一步提高教师的稳定性。通过以上两种策略，学习可靠的教师可以为学生提供更准确的伪标签，从而提高文本分类的整体性能。我们对三个基准数据集(即 AG News、 Yelp 和 Yahoo)进行了广泛的实验，以验证我们的 DPS 方法的有效性。实验结果表明，我们的方法实现了大大优于强竞争对手的性能。为确保重复性，我们会在 https://github.com/grit621/dps 公开发布本文件的代码和数据。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dual+Pseudo+Supervision+for+Semi-Supervised+Text+Classification+with+a+Reliable+Teacher)|0|
|[An Efficient Fusion Mechanism for Multimodal Low-resource Setting](https://doi.org/10.1145/3477495.3531900)|Dushyant Singh Chauhan, Asif Ekbal, Pushpak Bhattacharyya|Indian Institute of Technology Patna, Patna, India|The effective fusion of multiple modalities (i.e., text, acoustic, and visual) is a non-trivial task, as these modalities often carry specific and diverse information and do not contribute equally. The fusion of different modalities could even be more challenging under the low-resource setting, where we have fewer samples for training. This paper proposes a multi-representative fusion mechanism that generates diverse fusions with multiple modalities and then chooses the best fusion among them. To achieve this, we first apply convolution filters on multimodal inputs to generate different and diverse representations of modalities. We then fuse pairwise modalities with multiple representations to get the multiple fusions. Finally, we propose an attention mechanism that only selects the most appropriate fusion, which eventually helps resolve the noise problem by ignoring the noisy fusions. We evaluate our proposed approach on three low-resource multimodal sentiment analysis datasets, i.e., YouTube, MOUD, and ICT-MMMO. Experimental results show the effectiveness of our proposed approach with the accuracies of 59.3%, 83.0%, and 84.1% for the YouTube, MOUD, and ICT-MMMO datasets, respectively.|多种模式(即文本、声学和视觉)的有效融合是一项非常重要的任务，因为这些模式往往携带特定的、多样化的信息，并且不能平等地作出贡献。在资源匮乏的情况下，不同模式的融合可能更具挑战性，因为我们的培训样本较少。提出了一种多代表性融合机制，该机制通过多种融合方式生成多种融合，然后从中选择最佳融合方式。为了实现这一点，我们首先应用卷积滤波器的多模式输入，以产生不同的和不同的表示形式。然后，我们融合成对模式与多重表示，以获得多重融合。最后，我们提出了一种注意机制，只选择最适当的融合，这最终有助于解决噪声问题，而忽略了噪声融合。我们在三个低资源多模态情绪分析数据集(即 YouTube、 MOUD 和 ICT-MMMO)上评估了我们提出的方法。实验结果表明，该方法对 YouTube、 MOUD 和 ICT-MMMO 数据集的准确率分别为59.3% 、83.0% 和84.1% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Efficient+Fusion+Mechanism+for+Multimodal+Low-resource+Setting)|0|
|[PST: Measuring Skill Proficiency in Programming Exercise Process via Programming Skill Tracing](https://doi.org/10.1145/3477495.3531903)|Ruixin Li, Yu Yin, Le Dai, Shuanghong Shen, Xin Lin, Yu Su, Enhong Chen|Institute of Advanced Technology, University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence, Hefei, Anhui, China; Hefei Normal University & Hefei Comprehensive National Science Center, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence, Hefei, Anhui, China; School of Data Science, University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence, Hefei, Anhui, China|Programming has become an important skill for individuals nowadays. For the demand to improve personal programming skill, tracking programming skill proficiency is getting more and more important. However, few researchers pay attention to measuring the programming skill of learners. Most of existing studies on learner capability portrait only made use of the exercise results, while the rich behavioral information contained in programming exercise process remains unused. Therefore, we propose a model that measures skill proficiency in programming exercise process named Programming Skill Tracing (PST). We designed Code Information Graph (CIG) to represent the feature of learners' solution code, and Code Tracing Graph (CTG) to measure the changes between the adjacent submissions. Furthermore, we divided programming skill into programming knowledge and coding ability to get more fine-grained assessment. Finally, we conducted various experiments to verify the effectiveness and interpretability of our PST model.|编程已经成为当今个人的一项重要技能。对于提高个人编程技能的需求，跟踪编程技能熟练程度变得越来越重要。然而，很少有研究者注意测量学习者的编程技能。现有的关于学习者能力描述的研究大多只是利用了习题结果，而编程习题过程中所包含的丰富的行为信息却没有得到充分的利用。因此，我们提出了一个测量编程技能熟练程度的模型，命名为编程技能跟踪(PST)。我们设计了代码信息图(CIG)来表示学习者解决方案代码的特征，代码跟踪图(CTG)来度量相邻提交的代码之间的变化。此外，将编程技能分为编程知识和编码能力两部分，以获得更细粒度的评价。最后，我们进行了各种实验来验证我们的 PST 模型的有效性和可解释性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PST:+Measuring+Skill+Proficiency+in+Programming+Exercise+Process+via+Programming+Skill+Tracing)|0|
|[MuchSUM: Multi-channel Graph Neural Network for Extractive Summarization](https://doi.org/10.1145/3477495.3531906)|Qianren Mao, Hongdong Zhu, Junnan Liu, Cheng Ji, Hao Peng, Jianxin Li, Lihong Wang, Zheng Wang|Beihang University, Beijing, China; CNCERT, Beijing, China; University of Leeds, Leeds, West Yorkshire, United Kingdom|Recent studies of extractive text summarization have leveraged BERT for document encoding with breakthrough performance. However, when using a pre-trained BERT-based encoder, existing approaches for selecting representative sentences for text summarization are inadequate since the encoder is not explicitly trained for representing sentences. Simply providing the BERT-initialized sentences to cross-sentential graph-based neural networks (GNNs) to encode semantic features of the sentences is not ideal because doing so fail to integrate other summary-worthy features like sentence importance and positions. This paper presents MuchSUM, a better approach for extractive text summarization. MuchSUM is a multi-channel graph convolutional network designed to explicitly incorporate multiple salient summary-worthy features. Specifically, we introduce three specific graph channels to encode the node textual features, node centrality features, and node position features, respectively, under bipartite word-sentence heterogeneous graphs. Then, a cross-channel convolution operation is designed to distill the common graph representations shared by different channels. Finally, the sentence representations of each channel are fused for extractive summarization. We also investigate three weighted graphs in each channel to infuse edge features for graph-based summarization modeling. Experimental results demonstrate our model can achieve considerable performance compared with some BERT-initialized graph-based extractive summarization systems.|提取文本摘要的最新研究已经利用 BERT 技术实现了突破性的文档编码。然而，当使用预先训练的 BERT 编码器时，现有的选择文本摘要代表性句子的方法是不够的，因为编码器没有明确地训练代表性句子。简单地将 BERT 初始化的句子提供给基于跨句图的神经网络(GNN)来编码句子的语义特征是不理想的，因为这样做不能整合其他有总结价值的特征，如句子的重要性和位置。提出了一种更好的文本摘要提取方法 MuchSUM。MuchSUM 是一个多通道图卷积网络，旨在明确合并多个突出的值得总结的功能。具体来说，我们引入了三个特定的图通道，分别对二分词-句子异质图下的结点文本特征、结点中心特征和结点位置特征进行编码。然后，设计一个跨信道卷积运算来提取不同信道共享的公共图表示。最后，对每个通道的句子表示进行融合，进行提取摘要。我们还研究了每个通道中的三个加权图，为基于图的摘要建模注入边缘特征。实验结果表明，与一些 BERT 初始化的基于图的抽取摘要系统相比，该模型具有较好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MuchSUM:+Multi-channel+Graph+Neural+Network+for+Extractive+Summarization)|0|
|[Multi-label Masked Language Modeling on Zero-shot Code-switched Sentiment Analysis](https://doi.org/10.1145/3477495.3531914)|Zhi Li, Xing Gao, Ji Zhang, Yin Zhang|Alibaba Group, Hangzhou, China; Zhejiang University, Hangzhou, China|In multilingual communities, code-switching is a common phenomenon and code-switched tasks have become a crucial area of research in natural language processing (NLP) applications. Existing approaches mainly focus on supervised learning. However, it is expensive to annotate a sufficient amount of code-switched data. In this paper, we consider zero-shot setting and improve model performance on code-switched tasks via monolingual language datasets, unlabeled code-switched datasets, and semantic dictionaries. Inspired by the mechanism of code-switching itself, we propose multi-label masked language modeling and predict both the masked word and its synonyms in other languages. Experimental results show that compared with baselines, our method can further improve the pretrained multilingual model's performance on code-switched sentiment analysis datasets.|在多语言社区中，语码转换是一种常见的现象，语码转换任务已经成为自然语言处理(NLP)应用研究的一个重要领域。现有方法主要侧重于监督式学习。然而，对足够数量的代码切换数据进行注释是昂贵的。本文通过单语种语言数据集、未标记的语码转换数据集和语义词典，考虑了零拍设置，提高了语码转换任务的模型性能。受语码转换本身机制的启发，我们提出了多标签隐藏语言模型，并对其他语言中的隐藏词及其同义词进行了预测。实验结果表明，与基线方法相比，该方法可以进一步提高预训练多语言模型在编码切换情感分析数据集上的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-label+Masked+Language+Modeling+on+Zero-shot+Code-switched+Sentiment+Analysis)|0|
|[Extractive Elementary Discourse Units for Improving Abstractive Summarization](https://doi.org/10.1145/3477495.3531916)|Ye Xiong, Teeradaj Racharak, Minh Le Nguyen|Japan Advanced Institute of Science and Technology, Nomi,Ishikawa, Japan|Abstractive summarization focuses on generating concise and fluent text from an original document while maintaining the original intent and containing the new words that do not appear in the original document. Recent studies point out that rewriting extractive summaries help improve the performance with a more concise and comprehensible output summary, which uses a sentence as a textual unit. However, a single document sentence normally cannot supply sufficient information. In this paper, we apply elementary discourse unit (EDU) as textual unit of content selection. In order to utilize EDU for generating a high quality summary, we propose a novel summarization model that first designs an EDU selector to choose salient content. Then, the generator model rewrites the selected EDUs as the final summary. To determine the relevancy of each EDU on the entire document, we choose to apply group tag embedding, which can establish the connection between summary sentences and relevant EDUs, so that our generator does not only focus on selected EDUs, but also ingest the entire original document. Extensive experiments on the CNN/Daily Mail dataset have demonstrated the effectiveness of our model.|抽象摘要的重点是从原始文档中生成简洁流畅的文本，同时保持原始意图并包含原始文档中没有出现的新词。最近的研究指出，重写提取摘要有助于提高性能与更简明易懂的输出摘要，使用一个句子作为文本单位。然而，一个单独的文档句子通常不能提供足够的信息。本文采用基本语篇单位(EDU)作为内容选择的语篇单位。为了利用 EDU 生成高质量的摘要，我们提出了一种新的摘要模型，首先设计一个 EDU 选择器来选择显著的内容。然后，生成器模型重写所选的 EDU 作为最终摘要。为了确定每个 EDU 对整个文档的相关性，我们选择使用组标签嵌入，它可以建立摘要句子和相关 EDU 之间的连接，因此我们的生成器不仅关注选定的 EDU，而且摄取整个原始文档。在 CNN/Daily Mail 数据集上的大量实验已经证明了我们模型的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Extractive+Elementary+Discourse+Units+for+Improving+Abstractive+Summarization)|0|
|[LightSGCN: Powering Signed Graph Convolution Network for Link Sign Prediction with Simplified Architecture Design](https://doi.org/10.1145/3477495.3531917)|Haoxin Liu|Tsinghua University, Beijing, China|With both positive and negative links, signed graphs exist widely in the real world. Recently, signed graph neural networks (GNNs) have shown superior performance in the most common signed graph analysis task, i.e., link sign prediction. Existing signed GNNs follow the classic nonlinear-propagation paradigm in unsigned GNNs. However, several recent studies on unsigned GNNs have shown that such a paradigm increases training difficulty and even reduces performance in various unsigned graph analysis tasks. Meanwhile, most of the public real-world signed graph datasets do not provide node features. These motivate us to consider whether the existing complex model architecture is suitable. In this work, we aim to simplify the architecture of signed GNNs to make it more concise and appropriate for link sign prediction. We propose a simplified signed graph convolution network model called LightSGCN. Specifically, LightSGCN utilizes linear propagation based on the balance theory, a widely adopted social theory. Then, the linear combination of hidden representations at each layer is used as the final representations. Moreover, we also propose a tailored prediction function. These finally yield a simple yet effective LightSGCN model, which is more interpretable, easier to implement, and more efficient to train. Experimental results on four real-world signed graphs demonstrate that such a linear method outperforms the state-of-the-art signed GNNs methods with significant improvement in the link sign prediction task and achieves more than 100X speedup over the most similar and simplest baseline.|符号图有正负两个链接，在现实世界中广泛存在。最近，签名图神经网络(GNN)在最常见的签名图分析任务，即链路符号预测中表现出了优越的性能。现有的签名 GNN 遵循无签名 GNN 中经典的非线性传播范式。然而，最近几项关于无符号 GNN 的研究表明，这种模式增加了训练难度，甚至降低了各种无符号图分析任务的性能。同时，现实世界中的大多数公共签名图数据集都没有提供节点特征。这促使我们考虑现有的复杂模型体系结构是否合适。在这项工作中，我们的目标是简化签名 GNN 的体系结构，使其更简洁，适合链路符号预测。我们提出了一个简化的符号图卷积网络模型，称为 LightSGCN。具体来说，LightSGCN 利用基于平衡理论的线性传播，这是一种被广泛采用的社会理论。然后，每一层的隐藏表示的线性组合被用作最终的表示。此外，我们还提出了一个量身定制的预测函数。这些最终产生了一个简单而有效的 LightSGCN 模型，该模型更易于解释、更易于实现、更易于训练。实验结果表明，这种线性方法的性能优于最先进的签名 GNN 方法，在链路符号预测任务方面有显著改进，在最相似和最简单的基线上实现了超过100倍的加速。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LightSGCN:+Powering+Signed+Graph+Convolution+Network+for+Link+Sign+Prediction+with+Simplified+Architecture+Design)|0|
|[ir_metadata: An Extensible Metadata Schema for IR Experiments](https://doi.org/10.1145/3477495.3531738)|Timo Breuer, Jüri Keller, Philipp Schaer||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ir_metadata:+An+Extensible+Metadata+Schema+for+IR+Experiments)|0|
|[CODEC: Complex Document and Entity Collection](https://doi.org/10.1145/3477495.3531712)|Iain Mackie, Paul Owoicho, Carlos Gemmell, Sophie Fischer, Sean MacAvaney, Jeffrey Dalton||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CODEC:+Complex+Document+and+Entity+Collection)|0|
|[Would You Ask it that Way?: Measuring and Improving Question Naturalness for Knowledge Graph Question Answering](https://doi.org/10.1145/3477495.3531739)|Trond Linjordet, Krisztian Balog||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Would+You+Ask+it+that+Way?:+Measuring+and+Improving+Question+Naturalness+for+Knowledge+Graph+Question+Answering)|0|
|[Biographical Semi-Supervised Relation Extraction Dataset](https://doi.org/10.1145/3477495.3531742)|Alistair Plum, Tharindu Ranasinghe, Spencer Jones, Constantin Orasan, Ruslan Mitkov||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Biographical+Semi-Supervised+Relation+Extraction+Dataset)|0|
|[CAVES: A Dataset to facilitate Explainable Classification and Summarization of Concerns towards COVID Vaccines](https://doi.org/10.1145/3477495.3531745)|Soham Poddar, Azlaan Mustafa Samad, Rajdeep Mukherjee, Niloy Ganguly, Saptarshi Ghosh||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CAVES:+A+Dataset+to+facilitate+Explainable+Classification+and+Summarization+of+Concerns+towards+COVID+Vaccines)|0|
|[SparCAssist: A Model Risk Assessment Assistant Based on Sparse Generated Counterfactuals](https://doi.org/10.1145/3477495.3531677)|Zijian Zhang, Vinay Setty, Avishek Anand||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SparCAssist:+A+Model+Risk+Assessment+Assistant+Based+on+Sparse+Generated+Counterfactuals)|0|
|[TARexp: A Python Framework for Technology-Assisted Review Experiments](https://doi.org/10.1145/3477495.3531663)|Eugene Yang, David D. Lewis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TARexp:+A+Python+Framework+for+Technology-Assisted+Review+Experiments)|0|
|[SpaceQA: Answering Questions about the Design of Space Missions and Space Craft Concepts](https://doi.org/10.1145/3477495.3531697)|Andrés GarcíaSilva, Cristian Berrio, José Manuél GómezPérez, José Antonio Martínez Heras, Alessandro Donati, Ilaria Roma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SpaceQA:+Answering+Questions+about+the+Design+of+Space+Missions+and+Space+Craft+Concepts)|0|
|[A Python Interface to PISA!](https://doi.org/10.1145/3477495.3531656)|Sean MacAvaney, Craig Macdonald||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Python+Interface+to+PISA!)|0|
|[Organizing Portuguese Legal Documents through Topic Discovery](https://doi.org/10.1145/3477495.3536329)|Daniela Vianna, Edleno Silva de Moura||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Organizing+Portuguese+Legal+Documents+through+Topic+Discovery)|0|
|[A Low-Cost, Controllable and Interpretable Task-Oriented Chatbot: With Real-World After-Sale Services as Example](https://doi.org/10.1145/3477495.3536331)|Xiangyu Xi, Chenxu Lv, Yuncheng Hua, Wei Ye, Chaobo Sun, Shuaipeng Liu, Fan Yang, Guanglu Wan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Low-Cost,+Controllable+and+Interpretable+Task-Oriented+Chatbot:+With+Real-World+After-Sale+Services+as+Example)|0|
|[Beyond Opinion Mining: Summarizing Opinions of Customer Reviews](https://doi.org/10.1145/3477495.3532676)|Reinald Kim Amplayo, Arthur Brazinskas, Yoshi Suhara, Xiaolan Wang, Bing Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Beyond+Opinion+Mining:+Summarizing+Opinions+of+Customer+Reviews)|0|
|[Fairness-Aware Question Answering for Intelligent Assistants](https://doi.org/10.1145/3477495.3531682)|Sachin Pathiyan Cherumanal||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fairness-Aware+Question+Answering+for+Intelligent+Assistants)|0|
|[Continuous Result Delta Evaluation of IR Systems](https://doi.org/10.1145/3477495.3531686)|Gabriela González Sáez||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Continuous+Result+Delta+Evaluation+of+IR+Systems)|0|
