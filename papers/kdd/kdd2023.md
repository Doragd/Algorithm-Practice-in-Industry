# KDD2023 Paper List

|论文|作者|组织|摘要|翻译|代码|引用数|
|---|---|---|---|---|---|---|
|[How Transitive Are Real-World Group Interactions? - Measurement and Reproduction](https://doi.org/10.1145/3580305.3599382)|Sunwoo Kim, Fanchen Bu, Minyoung Choe, Jaemin Yoo, Kijung Shin|KAIST; Carnegie Mellon University|Many real-world interactions (e.g., researcher collaborations and email communication) occur among multiple entities. These group interactions are naturally modeled as hypergraphs. In graphs, transitivity is helpful to understand the connections between node pairs sharing a neighbor, and it has extensive applications in various domains. Hypergraphs, an extension of graphs, are designed to represent group relations. However, to the best of our knowledge, there has been no examination regarding the transitivity of real-world group interactions. In this work, we investigate the transitivity of group interactions in real-world hypergraphs. We first suggest intuitive axioms as necessary characteristics of hypergraph transitivity measures. Then, we propose a principled hypergraph transitivity measure HyperTrans, which satisfies all the proposed axioms, with a fast computation algorithm Fast-HyperTrans. After that, we analyze the transitivity patterns in real-world hypergraphs distinguished from those in random hypergraphs. Lastly, we propose a scalable hypergraph generator THera. It reproduces the observed transitivity patterns by leveraging community structures, which are pervasive in real-world hypergraphs. Our code and datasets are available at https://github.com/kswoo97/hypertrans.|许多真实世界的交互(例如，研究人员协作和电子邮件通信)发生在多个实体之间。这些群体的相互作用自然地被建模为超图。在图中，传递性有助于理解共享邻居的节点对之间的联系，在各个领域有着广泛的应用。超图是图的一个扩展，被设计用来表示群关系。然而，据我们所知，还没有关于现实世界群体交互传递性的研究。在这项工作中，我们研究了现实世界超图中群相互作用的传递性。我们首先提出直观公理作为超图传递度量的必要特征。然后，我们提出了一个原则性的超图传递度度量 HyperTrans，它满足所提出的所有公理，并使用快速计算算法 Fast-HyperTrans。在此基础上，分析了现实超图与随机超图的传递模式。最后，我们提出了一个可扩展的超图生成器 THera。它通过利用在现实世界超图中普遍存在的社区结构来重现观察到的传递性模式。我们的代码和数据集 https://github.com/kswoo97/hypertrans 可用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+Transitive+Are+Real-World+Group+Interactions?+-+Measurement+and+Reproduction)|1|
|[Temporal Dynamics-Aware Adversarial Attacks on Discrete-Time Dynamic Graph Models](https://doi.org/10.1145/3580305.3599517)|Kartik Sharma, Rakshit Trivedi, Rohit Sridhar, Srijan Kumar||Real-world graphs such as social networks, communication networks, and rating networks are constantly evolving over time. Many deep learning architectures have been developed to learn effective node representations using both graph structure and dynamics. While being crucial for practical applications, the robustness of these representation learners for dynamic graphs in the presence of adversarial attacks is highly understudied. In this work, we design a novel adversarial attack on discrete-time dynamic graph models where we desire to perturb the input graph sequence in a manner that preserves the temporal dynamics of the graph while dropping the performance of representation learners. To this end, we motivate a novel Temporal Dynamics-Aware Perturbation (TDAP) constraint, which ensures that perturbations introduced at each time step are restricted to only a small fraction of the number of changes in the graph since the previous time step. We present a theoretically-motivated Projected Gradient Descent approach for dynamic graphs to find effective perturbations under the TDAP constraint. Experiments on two tasks - dynamic link prediction and node classification, show that our approach is up to 4x more effective than the baseline methods for attacking these models. We extend our approach to a more practical online setting where graphs become available in real-time and show up to 5x superior performance over baselines We also show that our approach successfully evades state-of-the-art neural approaches for anomaly detection, thereby promoting the need to study robustness as a part of representation-learning approaches for dynamic graphs.|真实世界的图表，如社交网络，通信网络和评级网络是不断演变的时间。为了学习有效的节点表示，人们开发了许多深度学习体系结构，包括图结构和动态学习。在实际应用中，这些表示学习者对于动态图的鲁棒性是至关重要的，但是在存在对抗性攻击的情况下，这些表示学习者的鲁棒性却被低估了。本文针对离散时间动态图模型设计了一种新的对抗性攻击方法，该方法通过扰动输入图序列来保持图的时间动态性，同时降低表示学习者的学习效率。为此，我们激发了一种新的时间动态感知扰动(TDAP)约束，它确保在每个时间步骤引入的扰动仅限于自上一个时间步骤以来图中变化数量的一小部分。我们提出了一个理论驱动的动态图的投影梯度下降法方法，以找到在 TDAP 约束下的有效扰动。在动态链路预测和节点分类两个任务上的实验表明，我们的方法比基线方法攻击这些模型的效率高达4倍。我们将我们的方法扩展到一个更实用的在线环境，图表可以实时获得，并且比基线表现出5倍的优越性能。我们还表明，我们的方法成功地避开了最先进的神经异常检测方法，从而促进了研究鲁棒性作为动态图表示学习方法的一部分的需要。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Temporal+Dynamics-Aware+Adversarial+Attacks+on+Discrete-Time+Dynamic+Graph+Models)|1|
|[Contrastive Cross-scale Graph Knowledge Synergy](https://doi.org/10.1145/3580305.3599286)|Yifei Zhang, Yankai Chen, Zixing Song, Irwin King||Graph representation learning via Contrastive Learning (GCL) has drawn considerable attention recently. Efforts are mainly focused on gathering more global information via contrasting on a single high-level graph view, which, however, underestimates the inherent complex and hierarchical properties in many real-world networks, leading to sub-optimal embeddings. To incorporate these properties of a complex graph, we propose Cross-Scale Contrastive Graph Knowledge Synergy (CGKS), a generic feature learning framework, to advance graph contrastive learning with enhanced generalization ability and the awareness of latent anatomies. Specifically, to maintain the hierarchical information, we create a so-call graph pyramid (GP) consisting of coarse-grained graph views. Each graph view is obtained via the careful design topology-aware graph coarsening layer that extends the Laplacian Eigenmaps with negative sampling. To promote cross-scale information sharing and knowledge interactions among GP, we propose a novel joint optimization formula that contains a pairwise contrastive loss between any two coarse-grained graph views. This synergy loss not only promotes knowledge sharing that yields informative representations, but also stabilizes the training process. Experiments on various downstream tasks demonstrate the substantial improvements of the proposed method over its counterparts.|基于对比学习(GCL)的图表示学习近年来引起了人们的广泛关注。主要的工作集中在通过对比单个高级图形视图来收集更多的全局信息，然而，这低估了许多现实世界网络固有的复杂性和层次性，导致次优嵌入。为了整合复杂图形的这些特性，我们提出了跨尺度对比图知识协同(CGKS) ，一个通用的特征学习框架，以提高图形的对比学习与增强的泛化能力和潜在的解剖意识。具体地说，为了维护层次信息，我们创建了一个由粗粒度图视图组成的所谓图金字塔(GP)。每个图视图都是通过精心设计的拓扑感知图粗化层获得的，粗化层扩展了负采样的拉普拉斯特征映射。为了促进 GP 之间的跨尺度信息共享和知识交互，提出了一种新的联合优化公式，该公式包含任意两个粗粒度图视图之间的成对对比损失。这种协同损失不仅促进知识共享，产生信息表示，但也稳定了培训过程。在各种下游任务上的实验表明，该方法比其他方法有了实质性的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contrastive+Cross-scale+Graph+Knowledge+Synergy)|1|
|[FedMultimodal: A Benchmark for Multimodal Federated Learning](https://doi.org/10.1145/3580305.3599825)|Tiantian Feng, Digbalay Bose, Tuo Zhang, Rajat Hebbar, Anil Ramakrishna, Rahul Gupta, Mi Zhang, Salman Avestimehr, Shrikanth Narayanan|University of Southern California; Amazon Alexa AI; The Ohio State University|Over the past few years, Federated Learning (FL) has become an emerging machine learning technique to tackle data privacy challenges through collaborative training. In the Federated Learning algorithm, the clients submit a locally trained model, and the server aggregates these parameters until convergence. Despite significant efforts that have been made to FL in fields like computer vision, audio, and natural language processing, the FL applications utilizing multimodal data streams remain largely unexplored. It is known that multimodal learning has broad real-world applications in emotion recognition, healthcare, multimedia, and social media, while user privacy persists as a critical concern. Specifically, there are no existing FL benchmarks targeting multimodal applications or related tasks. In order to facilitate the research in multimodal FL, we introduce FedMultimodal, the first FL benchmark for multimodal learning covering five representative multimodal applications from ten commonly used datasets with a total of eight unique modalities. FedMultimodal offers a systematic FL pipeline, enabling end-to-end modeling framework ranging from data partition and feature extraction to FL benchmark algorithms and model evaluation. Unlike existing FL benchmarks, FedMultimodal provides a standardized approach to assess the robustness of FL against three common data corruptions in real-life multimodal applications: missing modalities, missing labels, and erroneous labels. We hope that FedMultimodal can accelerate numerous future research directions, including designing multimodal FL algorithms toward extreme data heterogeneity, robustness multimodal FL, and efficient multimodal FL. The datasets and benchmark results can be accessed at: https://github.com/usc-sail/fed-multimodal.|在过去的几年中，联邦学习(FL)已经成为一种新兴的机器学习技术，通过协作培训来解决数据隐私方面的挑战。在联邦学习算法中，客户端提交一个本地训练的模型，服务器聚集这些参数直到收敛。尽管在计算机视觉、音频和自然语言处理等领域已经为 FL 做出了重大努力，但是利用多模态数据流的 FL 应用在很大程度上仍然是未知的。众所周知，多模态学习在情感识别、医疗保健、多媒体和社交媒体等领域有着广泛的现实应用，而用户隐私问题一直是人们关注的焦点。具体来说，目前还没有针对多模式应用程序或相关任务的 FL 基准测试。为了促进多模式 FL 的研究，我们引入了 FedMultimode，这是第一个多模式学习的 FL 基准，涵盖了来自10个常用数据集的5个代表性的多模式应用，总共有8种独特的模式。FedMultimode 提供了一个系统的 FL 流水线，支持从数据分区和特征提取到 FL 基准算法和模型评估的端到端建模框架。与现有的 FL 基准不同，FedMultimode 提供了一种标准化的方法来评估 FL 对现实多模式应用程序中三种常见数据损坏的稳健性: 缺失模式，缺失标签和错误标签。我们希望 FedMultimode 能够加速未来众多的研究方向，包括设计面向极端数据异构性的多模态 FL 算法、鲁棒的多模态 FL 算法和高效的多模态 FL 算法。数据集和基准测试结果可在以下 https://github.com/usc-sail/fed-multimodal 查阅:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedMultimodal:+A+Benchmark+for+Multimodal+Federated+Learning)|1|
|[Querywise Fair Learning to Rank through Multi-Objective Optimization](https://doi.org/10.1145/3580305.3599482)|Debabrata Mahapatra, Chaosheng Dong, Michinari Momma||In Learning-to-Rank (LTR) problems, the task of delivering relevant search results and allocating fair exposure to items of a protected group can conflict. Previous works in Fair LTR have attempted to resolve this by combining the objectives of relevant ranking and fair ranking into a single linear combination, but this approach is limited by the nonconvexity of the objective functions and can result in suboptimal relevance in ranking outputs. To address this, we propose a solution using Multi-Objective Optimization (MOO) algorithms. We extend these algorithms to querywise MOO to reduce the exposure disparity, not only on average but also at the query level. Interestingly, for moderate fairness requirements, it improves the relevance of ranking instead of deteriorating. We attribute this improvement to the benefits of multi-task learning and study the effect of fair ranking on the relevant ranking task. Moreover, we significantly improve the computational efficiency compared to previous methods by using the Gumbel max trick to sample the Plackett-Luce distribution. We evaluate our proposed methods on three real-world datasets and show their improvement in relevance ranking over state-of-the-art solutions.|在学习排名(LTR)问题中，传递相关搜索结果和公平分配受保护群体的项目曝光量的任务可能会发生冲突。公平长期目标研究的前期工作试图通过将相关排名和公平排名的目标合并为一个单一的线性组合来解决这个问题，但是这种方法受到目标函数的非凸性的限制，并且可能导致产出排名的次优相关性。为了解决这个问题，我们提出了一个使用多目标优化(MOO)算法的解决方案。我们将这些算法扩展到查询 MOO，以减少暴露差异，不仅是平均暴露差异，而且是在查询级别。有趣的是，对于适度的公平需求，它提高了排名的相关性，而不是恶化。我们将这种改进归因于多任务学习的好处，并研究了公平排序对相关排序任务的影响。此外，通过对 Plackett-Luce 分布进行采样，我们显著提高了计算效率。我们在三个真实世界的数据集上评估了我们提出的方法，并且显示了它们在相关性排序方面的改进，而不是最先进的解决方案。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Querywise+Fair+Learning+to+Rank+through+Multi-Objective+Optimization)|0|
|[E-commerce Search via Content Collaborative Graph Neural Network](https://doi.org/10.1145/3580305.3599320)|Guipeng Xv, Chen Lin, Wanxian Guan, Jinping Gou, Xubin Li, Hongbo Deng, Jian Xu, Bo Zheng||Recently, many E-commerce search models are based on Graph Neural Networks (GNNs). Despite their promising performances, they are (1) lacking proper semantic representation of product contents; (2) less efficient for industry-scale graphs; and (3) less accurate on long-tail queries and cold-start products. To address these problems simultaneously, this paper proposes CC-GNN, a novel Content Collaborative Graph Neural Network. Firstly, CC-GNN enables content phrases to participate explicitly in graph propagation to capture the proper meaning of phrases and semantic drifts. Secondly, CC-GNN presents several efforts towards a more scalable graph learning framework, including efficient graph construction, MetaPath-guided Message Passing, and Difficulty-aware Representation Perturbation for graph contrastive learning. Furthermore, CC-GNN adopts Counterfactual Data Supplement at both supervised and contrastive learning to resolve the long-tail/cold-start problems. Extensive experiments on a real E-commerce dataset of 100-million-scale nodes show that CC-GNN produces significant improvements over existing methods (i.e., more than 10% improvements in terms of several key evaluation metrics for overall, long-tail queries and cold-start products) while reducing computational complexity. The proposed components of CC-GNN can be applied to other models for search and recommendation tasks. Experiments on a public dataset show that applying the proposed components can improve the performance of different recommendation models.|近年来，许多电子商务搜索模型都是基于图神经网络的。尽管它们具有良好的性能，但是它们(1)缺乏对产品内容的正确语义表示; (2)对于工业规模的图表来说，它们的效率较低; (3)对于长尾查询和冷启动产品来说，它们的准确性较低。为了同时解决这些问题，本文提出了一种新的内容协同图神经网络 CC-GNN。首先，CC-GNN 允许内容短语明确地参与图的传播，以获取短语的正确意义和语义漂移。其次，CC-GNN 提出了一个更具可扩展性的图形学习框架，包括高效的图形构造、 MetaPath 引导的消息传递和图形对比学习的难度感知表示扰动。此外，CC-GNN 在监督学习和对比学习中都采用了反事实数据补充的方法来解决长尾/冷启动问题。对1亿个规模节点的真实电子商务数据集进行的广泛实验表明，CC-GNN 比现有方法产生了显着的改进(即，对于总体、长尾查询和冷启动产品的几个关键评估指标提高了10% 以上) ，同时降低了计算复杂性。提出的 CC-GNN 组件可以应用于其他模型的搜索和推荐任务。在一个公共数据集上的实验表明，应用所提出的组件可以提高不同推荐模型的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=E-commerce+Search+via+Content+Collaborative+Graph+Neural+Network)|0|
|[Cognitive Evolutionary Search to Select Feature Interactions for Click-Through Rate Prediction](https://doi.org/10.1145/3580305.3599277)|Runlong Yu, Xiang Xu, Yuyang Ye, Qi Liu, Enhong Chen||Click-Through Rate (CTR) prediction of intelligent marketing systems is of great importance, in which feature interaction selection plays a key role. Most approaches model interactions of features by the same pre-defined operation under expert guidance, among which improper interactions may bring unnecessary noise and complicate the training process. To that end, in this paper, we aim to adaptively evolve the model to select proper operations to interact on feature pairs under task guidance. Inspired by natural evolution, we propose a general Cognitive EvoLutionary Search (CELS) framework, where cognitive ability refers to the malleability of organisms to orientate to the environment. Specifically, we conceptualize interactions as genomes, models as organisms, and tasks as natural environments. Mirroring how genetic malleability develops environmental adaptability, we thus diagnose the fitness of models to simulate the survival rates of organisms for natural selection, thereby an evolution path can be planned and visualized, offering an intuitive interpretation of the mechanisms underlying interaction modeling and selection. Based on the CELS framework, we develop four instantiations including individual-based search and population-based search. We demonstrate how individual mutation and population crossover enable CELS to evolve into diverse models suitable for various tasks and data, providing ready-to-use models. Extensive experiments on real-world datasets demonstrate that CELS significantly outperforms state-of-the-art approaches.|智能营销系统的点进率预测非常重要，其中特征交互选择起着关键作用。大多数方法在专家指导下通过相同的预定义操作对特征间的交互进行建模，其中不正确的交互会带来不必要的噪声，使训练过程复杂化。为此，本文旨在通过自适应演化模型来选择合适的操作，以便在任务指导下对特征对进行交互。受自然进化的启发，我们提出了一个通用的认知进化搜索(CELS)框架，其中认知能力是指生物体定位于环境的可塑性。具体来说，我们将相互作用概念化为基因组，将模型概念化为生物体，将任务概念化为自然环境。反映遗传可塑性如何发展环境适应性，因此我们诊断模型的适应性来模拟自然选择的生物体的生存率，从而可以规划和可视化进化路径，提供对相互作用建模和选择机制的直观解释。在 CELS 框架的基础上，我们开发了四个实例，包括基于个体的搜索和基于种群的搜索。我们展示了个体突变和种群交叉如何使 CELS 演变成适合各种任务和数据的不同模型，提供了现成的使用模型。对真实世界数据集的大量实验表明，CELS 显著优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cognitive+Evolutionary+Search+to+Select+Feature+Interactions+for+Click-Through+Rate+Prediction)|0|
|[Binary Embedding-based Retrieval at Tencent](https://doi.org/10.1145/3580305.3599782)|Yukang Gan, Yixiao Ge, Chang Zhou, Shupeng Su, Zhouchuan Xu, Xuyuan Xu, Quanchao Hui, Xiang Chen, Yexin Wang, Ying Shan|Tencent|Large-scale embedding-based retrieval (EBR) is the cornerstone of search-related industrial applications. Given a user query, the system of EBR aims to identify relevant information from a large corpus of documents that may be tens or hundreds of billions in size. The storage and computation turn out to be expensive and inefficient with massive documents and high concurrent queries, making it difficult to further scale up. To tackle the challenge, we propose a binary embedding-based retrieval (BEBR) engine equipped with a recurrent binarization algorithm that enables customized bits per dimension. Specifically, we compress the full-precision query and document embeddings, formulated as float vectors in general, into a composition of multiple binary vectors using a lightweight transformation model with residual multilayer perception (MLP) blocks. We can therefore tailor the number of bits for different applications to trade off accuracy loss and cost savings. Importantly, we enable task-agnostic efficient training of the binarization model using a new embedding-to-embedding strategy. We also exploit the compatible training of binary embeddings so that the BEBR engine can support indexing among multiple embedding versions within a unified system. To further realize efficient search, we propose Symmetric Distance Calculation (SDC) to achieve lower response time than Hamming codes. We successfully employed the introduced BEBR to Tencent products, including Sogou, Tencent Video, QQ World, etc. The binarization algorithm can be seamlessly generalized to various tasks with multiple modalities. Extensive experiments on offline benchmarks and online A/B tests demonstrate the efficiency and effectiveness of our method, significantly saving 30%~50% index costs with almost no loss of accuracy at the system level.|大规模嵌入式检索(EBR)是搜索相关工业应用的基石。给定一个用户查询，EBR 系统的目标是从大量文档中识别相关信息，这些文档的规模可能达到数百亿或数千亿。由于大量文档和高并发查询，存储和计算成本高、效率低，难以进一步扩展。为了解决这一问题，我们提出了一种基于二进制嵌入的检索引擎(BEBR) ，该引擎配备了一个循环二进制算法，可以实现每维定制位。具体地说，我们使用带有剩余多层感知(MLP)块的轻量级变换模型，将通常表示为浮点向量的全精度查询和文档嵌入压缩为多个二进制向量的组合。因此，我们可以为不同的应用程序量身定制位数，以权衡精度损失和成本节约。重要的是，我们使任务无关的二值化模型的有效训练使用一种新的嵌入到嵌入策略。我们还利用二进制嵌入的兼容性训练，使 BEBR 引擎能够在一个统一的系统中支持多个嵌入版本之间的索引。为了进一步实现有效的搜索，我们提出了对称距离计算(SDC)来实现比汉明码更低的响应时间。我们成功地将引进的 BEBR 引入腾讯产品，包括搜狗、腾讯视频、 QQ 世界等。二值化算法可以无缝地推广到具有多种模式的各种任务。对离线基准测试和在线 A/B 测试的大量实验证明了该方法的有效性和有效性，显著节省了30% ~ 50% 的指标成本，在系统级几乎没有准确性的损失。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Binary+Embedding-based+Retrieval+at+Tencent)|0|
|[Optimizing Airbnb Search Journey with Multi-task Learning](https://doi.org/10.1145/3580305.3599881)|Chun How Tan, Austin Chan, Malay Haldar, Jie Tang, Xin Liu, Mustafa Abdool, Huiji Gao, Liwei He, Sanjeev Katariya|Airbnb Inc.|At Airbnb, an online marketplace for stays and experiences, guests often spend weeks exploring and comparing multiple items before making a final reservation request. Each reservation request may then potentially be rejected or cancelled by the host prior to check-in. The long and exploratory nature of the search journey, as well as the need to balance both guest and host preferences, present unique challenges for Airbnb search ranking. In this paper, we present Journey Ranker, a new multi-task deep learning model architecture that addresses these challenges. Journey Ranker leverages intermediate guest actions as milestones, both positive and negative, to better progress the guest towards a successful booking. It also uses contextual information such as guest state and search query to balance guest and host preferences. Its modular and extensible design, consisting of four modules with clear separation of concerns, allows for easy application to use cases beyond the Airbnb search ranking context. We conducted offline and online testing of the Journey Ranker and successfully deployed it in production to four different Airbnb products with significant business metrics improvements.|Airbnb 是一家提供住宿和体验服务的在线市场，在提出最终预订请求之前，客人通常要花费数周时间来探索和比较多个项目。然后，主机可能会在签入之前拒绝或取消每个预订请求。漫长而探索性的搜索旅程，以及平衡客人和主人偏好的需要，为 Airbnb 的搜索排名提出了独特的挑战。在本文中，我们提出了一个新的多任务深度学习模型体系结构 Journey Ranker，以解决这些挑战。Journey Ranker 利用中间的客人行为作为里程碑，包括积极的和消极的，以更好地推动客人成功预订。它还使用上下文信息(如来宾状态和搜索查询)来平衡来宾和主机的首选项。它的模块化和可扩展的设计，由四个模块组成，具有明确的关注点分离，可以很容易地应用到 Airbnb 搜索排名上下文之外的用例。我们对 Journey Ranker 进行了离线和在线测试，并成功地将其部署到四个不同的 Airbnb 产品上，并对业务指标进行了重大改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+Airbnb+Search+Journey+with+Multi-task+Learning)|0|
|[User-Regulation Deconfounded Conversational Recommender System with Bandit Feedback](https://doi.org/10.1145/3580305.3599539)|Yu Xia, Junda Wu, Tong Yu, Sungchul Kim, Ryan A. Rossi, Shuai Li||Recent conversational recommender systems (CRSs) have achieved considerable success on addressing the cold-start problem. While they utilize conversational key-terms to efficiently elicit user preferences, most of them, however, neglect that key-terms can also introduce biases. Systems learning key-term-level user preferences may make a biased item recommendation based on an overrated key-term instead of the item itself. As key-term conversation is a crucial part of CRSs, it is important to properly handle such bias resulting from the item-key-term relationship. While many debiasing methods have been proposed for traditional recommender systems, most of them focus on items or item groups re-ranking or re-weighting strategies such as calibration and propensity score, which are not designed to model the relation between item and key-term user preference. There is also no effective way for traditional debiasing methods to measure potentially useful biases through conversational key-terms to enhance the recommendation performance. In this paper, we develop a deconfounded CRS, which enables the user to provide both item and key-term feedback in each round such that we can promisingly capture more accurate relation between key-term-level and item-level user preference to alleviate the bias. To better model the relations and understand such bias, we view CRSs from a causal perspective and introduce a novel structural causal model (SCM) that identifies the confounding effect of key-term-level user preference. Inspired by our causal view, we devise an online backdoor adjustment approximation to alleviate the confounding effect when making item recommendations. Consider that not all biases are harmful, we utilize the useful bias and propose DecUCB, which leverages conversational key-term feedback to regulate the influence of backdoor adjustment adaptively in a personalized fashion. Extensive experiments on real-world datasets demonstrate the advantages of our proposed method in both recommendation performance and bias mitigation.|最近的会话推荐系统(CRS)在解决冷启动问题上取得了相当大的成功。虽然他们利用会话关键词有效地引出用户偏好，但是大多数人忽略了关键词也可能引入偏见。系统学习关键词级别的用户偏好可能会根据被高估的关键词而不是项目本身做出有偏见的项目推荐。由于关键词会话是 CRS 的重要组成部分，因此正确处理由项目-关键词关系引起的这种偏见是非常重要的。针对传统的推荐系统，人们提出了许多消除偏差的方法，但大多数方法都集中于项目或项目组的重新排序或重新加权策略，如校准和倾向评分，这些方法并不是为了建立项目和关键词用户偏好之间的关系模型。传统的去偏方法也没有有效的方法来衡量潜在的有用的偏见，通过会话关键词，以提高推荐性能。本文提出了一种解构 CRS，使用户能够在每一轮中同时提供项目反馈和关键词反馈，从而有望更准确地捕捉到关键词水平和项目水平用户偏好之间的关系，以减轻偏差。为了更好地建立关系模型并理解这种偏差，我们从因果关系的角度来看待 CRS，并引入一种新的结构因果模型(SCM)来识别关键词水平用户偏好的混杂效应。受我们的因果观点的启发，我们设计了一个在线后门调整近似，以减轻混淆的影响时，作出项目的建议。考虑到并非所有的偏差都是有害的，我们利用有用的偏差，提出了 DecUCB，它利用会话关键词反馈，以个性化的方式自适应地调节后门调整的影响。在实际数据集上的大量实验证明了该方法在推荐性能和减少偏差方面的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=User-Regulation+Deconfounded+Conversational+Recommender+System+with+Bandit+Feedback)|0|
|[Contrastive Learning for User Sequence Representation in Personalized Product Search](https://doi.org/10.1145/3580305.3599287)|Shitong Dai, Jiongnan Liu, Zhicheng Dou, Haonan Wang, Lin Liu, Bo Long, JiRong Wen|Renmin University of China; Engineering Research Center of Next-Generation Intelligent Search and Recommendation, Ministry of Education; JD.com, Inc.|Providing personalization in product search has attracted increasing attention in both industry and research communities. Most existing personalized product search methods model users' individual search interests based on their historical search logs to generate personalized search results. However, the search logs may be sparse or noisy in the real scenario, which is difficult for existing methods to learn accurate and robust user representations. To address this issue, we propose a contrastive learning framework CoPPS that aims to learn high-quality user representations for personalized product search. Specifically, we design three data augmentation and contrastive learning strategies to construct self-supervision signals from the original search behaviours. The contrastive learning tasks utilize an external knowledge graph and exploit the correlations within and between user sequences, thereby facilitating the discovery of more meaningful search patterns and ultimately enhancing the quality of personalized search. Experimental results on the public Amazon datasets verify the effectiveness of our approach.|在产品搜索中提供个性化服务已经引起了工业界和研究界越来越多的关注。大多数现有的个性化产品搜索方法都是根据用户的历史搜索记录来模拟用户的个人搜索兴趣，从而生成个性化检索的结果。然而，在实际场景中，搜索日志可能是稀疏的或有噪声的，这对于现有的方法来说很难学习准确的和鲁棒的用户表示。为了解决这个问题，我们提出了一个对比学习框架 CoPPS，旨在学习个性化产品搜索的高质量用户表示。具体来说，我们设计了三种数据增强和对比学习策略，从原始的搜索行为中构建自我监督信号。对比学习任务利用外部知识图表，并利用用户序列内部和之间的相关性，从而促进发现更有意义的搜索模式，并最终提高个性化检索的质量。在公共 Amazon 数据集上的实验结果验证了我们方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contrastive+Learning+for+User+Sequence+Representation+in+Personalized+Product+Search)|0|
|[LATTE: A Framework for Learning Item-Features to Make a Domain-Expert for Effective Conversational Recommendation](https://doi.org/10.1145/3580305.3599401)|Taeho Kim, Juwon Yu, WonYong Shin, Hyunyoung Lee, JiHui Im, SangWook Kim||For high-quality conversational recommender systems (CRS), it is important to recommend the suitable items by capturing the items' features mentioned in the dialog and to explain the appropriate ones among the various features of the recommended item. We argue that the CRS model should be a domain-expert who is (1) knowledgeable about the relationships between items and their various features and (2) able to explain the recommended item with its features relevant to dialog context. To this end, we propose a novel framework, named as LATTE, to pre-train each core module in CRS (i.e., the recommendation and the conversation module) through abundant external data. For the recommendation module, we pre-train the recommendation module to comprehensively understand the relationships between items and their various features by leveraging both multi-reviews and a knowledge graph. For pre-training the conversation module, we create the synthetic dialogs, which contain responses providing the explanation relevant to the dialog context by using all the items' features and dialog templates. Through extensive experiments on two public CRS datasets, we demonstrate that LATTE exhibits (1) the effectiveness of each module in LATTE, (2) the superiority over 7 state-of-the art methods, and (3) the interpretations based on visualization.|对于高质量的会话推荐系统(CRS)来说，通过捕捉对话框中提到的项目特征来推荐合适的项目，并在推荐项目的各种特征中解释合适的项目是非常重要的。我们认为 CRS 模型应该是一个领域专家，他(1)了解项目之间的关系及其各种特征，(2)能够解释推荐的项目及其与对话框上下文相关的特征。为此，我们提出了一个新的框架，命名为 LatTE，通过大量的外部数据对 CRS 中的每个核心模块(即推荐模块和会话模块)进行预训练。对于推荐模块，我们预先训练推荐模块，通过利用多重评论和知识图全面理解项目之间的关系及其各种特性。对于会话模块的预训练，我们创建合成对话框，其中包含通过使用所有项目的特性和对话框模板提供与对话框上下文相关的解释的响应。通过对两个公共 CRS 数据集的广泛实验，我们证明 LatTE 展示了(1) LatTE 中每个模块的有效性，(2)超过7种最先进的方法的优越性，以及(3)基于可视化的解释。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LATTE:+A+Framework+for+Learning+Item-Features+to+Make+a+Domain-Expert+for+Effective+Conversational+Recommendation)|0|
|[An Empirical Study of Selection Bias in Pinterest Ads Retrieval](https://doi.org/10.1145/3580305.3599771)|Yuan Wang, Peifeng Yin, Zhiqiang Tao, Hari Venkatesan, Jin Lai, Yi Fang, PJ Xiao||Data selection bias has been a long-lasting challenge in the machine learning domain, especially in multi-stage recommendation systems, where the distribution of labeled items for model training is very different from that of the actual candidates during inference time. This distribution shift is even more prominent in the context of online advertising where the user base is diverse and the platform contains a wide range of contents. In this paper, we first investigate the data selection bias in the upper funnel (Ads Retrieval) of Pinterest's multi-cascade ads ranking system. We then conduct comprehensive experiments to assess the performance of various state-of-the-art methods, including transfer learning, adversarial learning, and unsupervised domain adaptation. Moreover, we further introduce some modifications into the unsupervised domain adaptation and evaluate the performance of different variants of this modified method. Our online A/B experiments show that the modified version of unsupervised domain adaptation (MUDA) could provide the largest improvements to the performance of Pinterest's advertisement ranking system compared with other methods and the one used in current production.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Empirical+Study+of+Selection+Bias+in+Pinterest+Ads+Retrieval)|0|
|[PIER: Permutation-Level Interest-Based End-to-End Re-ranking Framework in E-commerce](https://doi.org/10.1145/3580305.3599886)|Xiaowen Shi, Fan Yang, Ze Wang, Xiaoxu Wu, Muzhi Guan, Guogang Liao, Yongkang Wang, Xingxing Wang, Dong Wang|Meituan|Re-ranking draws increased attention on both academics and industries, which rearranges the ranking list by modeling the mutual influence among items to better meet users' demands. Many existing re-ranking methods directly take the initial ranking list as input, and generate the optimal permutation through a well-designed context-wise model, which brings the evaluation-before-reranking problem. Meanwhile, evaluating all candidate permutations brings unacceptable computational costs in practice. Thus, to better balance efficiency and effectiveness, online systems usually use a two-stage architecture which uses some heuristic methods such as beam-search to generate a suitable amount of candidate permutations firstly, which are then fed into the evaluation model to get the optimal permutation. However, existing methods in both stages can be improved through the following aspects. As for generation stage, heuristic methods only use point-wise prediction scores and lack an effective judgment. As for evaluation stage, most existing context-wise evaluation models only consider the item context and lack more fine-grained feature context modeling. This paper presents a novel end-to-end re-ranking framework named PIER to tackle the above challenges which still follows the two-stage architecture and contains two mainly modules named FPSM and OCPM. We apply SimHash in FPSM to select top-K candidates from the full permutation based on user's permutation-level interest in an efficient way. Then we design a novel omnidirectional attention mechanism in OCPM to capture the context information in the permutation. Finally, we jointly train these two modules end-to-end by introducing a comparative learning loss. Offline experiment results demonstrate that PIER outperforms baseline models on both public and industrial datasets, and we have successfully deployed PIER on Meituan food delivery platform.|重新排名吸引了越来越多的学术界和行业的关注，它们通过建立项目之间的相互影响模型来重新排列排名列表，以更好地满足用户的需求。许多现有的重新排序方法直接以初始排序列表为输入，通过设计良好的上下文智能模型生成最优排序，从而产生重新排序前的评价问题。同时，评估所有候选排列在实践中带来不可接受的计算成本。因此，为了更好地平衡效率和有效性，在线系统通常采用两阶段的体系结构，使用一些启发式的方法，如束搜索，生成适当数量的候选排列，然后反馈到评估模型，以获得最优的排列。然而，这两个阶段的现有方法可以通过以下几个方面进行改进。对于生成阶段，启发式方法只使用逐点预测得分，缺乏有效的判断。在评价阶段，现有的基于上下文的评价模型大多只考虑项目上下文，缺乏更细粒度的特征上下文建模。本文提出了一种新的端到端重新排序框架 PIER，以解决上述挑战，该框架仍然遵循两阶段的体系结构，包含两个主要模块: FPSM 和 OCPM。将模拟哈希算法应用于基于用户兴趣排列的 FSM 中，有效地从完全排列中选择出最优 K 候选算法。然后在 OCPM 中设计了一种新的全方位注意机制来捕获排列中的上下文信息。最后，通过引入比较学习损失，对这两个模块进行了端到端的联合训练。离线实验结果显示 PIER 在公共和工业数据集上都优于基线模型，我们已经成功地在美团食品配送平台上部署 PIER。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PIER:+Permutation-Level+Interest-Based+End-to-End+Re-ranking+Framework+in+E-commerce)|0|
|[Exploiting Intent Evolution in E-commercial Query Recommendation](https://doi.org/10.1145/3580305.3599821)|Yu Wang, Zhengyang Wang, Hengrui Zhang, Qingyu Yin, Xianfeng Tang, Yinghan Wang, Danqing Zhang, Limeng Cui, Monica Cheng, Bing Yin, Suhang Wang, Philip S. Yu||Aiming at a better understanding of the search goals in the user search sessions, recent query recommender systems explicitly model the reformulations of queries, which hopes to estimate the intents behind these reformulations and thus benefit the next-query recommendation. However, in real-world e-commercial search scenarios, user intents are much more complicated and may evolve dynamically. Existing methods merely consider trivial reformulation intents from semantic aspects and fail to model dynamic reformulation intent flows in search sessions, leading to sub-optimal capacities to recommend desired queries. To deal with these limitations, we first explicitly define six types of query reformulation intents according to the desired products of two consecutive queries. We then apply two self-attentive encoders on top of two pre-trained large language models to learn the transition dynamics from semantic query and intent reformulation sequences, respectively. We develop an intent-aware query decoder to utilize the predicted intents for suggesting the next queries. We instantiate such a framework with an Intent-aware Variational AutoEncoder (IVAE) under deployment at Amazon. We conduct comprehensive experiments on two real-world e-commercial datasets from Amazon and one public dataset from BestBuy. Specifically, IVAE improves the Recall@15 by 25.44% and 60.47% on two Amazon datasets and 13.91% on BestBuy, respectively.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploiting+Intent+Evolution+in+E-commercial+Query+Recommendation)|0|
|[QUERT: Continual Pre-training of Language Model for Query Understanding in Travel Domain Search](https://doi.org/10.1145/3580305.3599891)|Jian Xie, Yidan Liang, Jingping Liu, Yanghua Xiao, Baohua Wu, Shenghua Ni|Alibaba Group; Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University; School of Information Science and Engineering, East China University of Science and Technology|In light of the success of the pre-trained language models (PLMs), continual pre-training of generic PLMs has been the paradigm of domain adaption. In this paper, we propose QUERT, A Continual Pre-trained Language Model for QUERy Understanding in Travel Domain Search. QUERT is jointly trained on four tailored pre-training tasks to the characteristics of query in travel domain search: Geography-aware Mask Prediction, Geohash Code Prediction, User Click Behavior Learning, and Phrase and Token Order Prediction. Performance improvement of downstream tasks and ablation experiment demonstrate the effectiveness of our proposed pre-training tasks. To be specific, the average performance of downstream tasks increases by 2.02% and 30.93% in supervised and unsupervised settings, respectively. To check on the improvement of QUERT to online business, we deploy QUERT and perform A/B testing on Fliggy APP. The feedback results show that QUERT increases the Unique Click-Through Rate and Page Click-Through Rate by 0.89% and 1.03% when applying QUERT as the encoder. Our code and downstream task data will be released for future research.|鉴于预训练语言模型(PLM)的成功，通用 PLM 的连续预训练已经成为领域适应的范例。本文提出了一种连续预训练语言模型 QUERT，用于旅游领域搜索中的查询理解。QUERT 针对旅游领域搜索中查询的特点，共同接受了四项量身定制的预先培训任务: 地理感知掩码预测、 Geohash 代码预测、用户点击行为学习以及短语和令牌顺序预测。下游任务的性能改进和烧蚀实验验证了我们提出的预训练任务的有效性。具体来说，在监督和非监督环境下，下游任务的平均性能分别提高了2.02% 和30.93% 。为了检查 QUERT 对在线业务的改进，我们部署 QUERT 并在 Fliggy APP 上进行 A/B 测试。反馈结果显示，当应用 QUERT 作为编码器时，QUERT 增加了0.89% 和1.03% 的唯一点进率和页面点进率。我们的代码和下游任务数据将被公布，以供未来研究使用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=QUERT:+Continual+Pre-training+of+Language+Model+for+Query+Understanding+in+Travel+Domain+Search)|0|
|[A Collaborative Transfer Learning Framework for Cross-domain Recommendation](https://doi.org/10.1145/3580305.3599758)|Wei Zhang, Pengye Zhang, Bo Zhang, Xingxing Wang, Dong Wang|Meituan|In the recommendation systems, there are multiple business domains to meet the diverse interests and needs of users, and the click-through rate(CTR) of each domain can be quite different, which leads to the demand for CTR prediction modeling for different business domains. The industry solution is to use domain-specific models or transfer learning techniques for each domain. The disadvantage of the former is that the data from other domains is not utilized by a single domain model, while the latter leverage all the data from different domains, but the fine-tuned model of transfer learning may trap the model in a local optimum of the source domain, making it difficult to fit the target domain. Meanwhile, significant differences in data quantity and feature schemas between different domains, known as domain shift, may lead to negative transfer in the process of transferring. To overcome these challenges, we propose the Collaborative Cross-Domain Transfer Learning Framework (CCTL). CCTL evaluates the information gain of the source domain on the target domain using a symmetric companion network and adjusts the information transfer weight of each source domain sample using the information flow network. This approach enables full utilization of other domain data while avoiding negative migration. Additionally, a representation enhancement network is used as an auxiliary task to preserve domain-specific features. Comprehensive experiments on both public and real-world industrial datasets, CCTL achieved SOTA score on offline metrics. At the same time, the CCTL algorithm has been deployed in Meituan, bringing 4.37% CTR and 5.43% GMV lift, which is significant to the business.|在推荐系统中，有多个业务领域可以满足用户的不同兴趣和需求，而每个领域的点进率可能有很大差异，因此需要为不同的业务领域建立点击率预测模型。行业解决方案是对每个领域使用特定于领域的模型或转移学习技术。前者的缺点是其他领域的数据不能被单一的领域模型所利用，而后者则利用来自不同领域的所有数据，但是经过微调的迁移学习模型可能使模型陷入源领域的局部最优，从而难以适应目标领域。同时，不同领域间数据量和特征模式的显著差异，称为领域移位，可能导致传递过程中的负迁移。为了克服这些挑战，我们提出了协作跨域转移学习框架(CCTL)。CCTL 使用对称伴侣网络对源域在目标域上的信息增益进行评估，并使用信息流网络调整每个源域样本的信息传输权重。这种方法可以充分利用其他域数据，同时避免负迁移。此外，表示增强网络用作辅助任务，以保持特定领域的特征。CCTL 在公共和现实世界的工业数据集上进行了全面的实验，在离线指标上取得了 SOTA 评分。与此同时，CCTL 算法已经在美团中部署，带来了4.37% 的点击率和5.43% 的 GMV 提升，这对业务具有重要意义。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Collaborative+Transfer+Learning+Framework+for+Cross-domain+Recommendation)|0|
|[Towards Disentangling Relevance and Bias in Unbiased Learning to Rank](https://doi.org/10.1145/3580305.3599914)|Yunan Zhang, Le Yan, Zhen Qin, Honglei Zhuang, Jiaming Shen, Xuanhui Wang, Michael Bendersky, Marc Najork|Google Research; Google; University of Illinois at Urbana-Champaign|Unbiased learning to rank (ULTR) studies the problem of mitigating various biases from implicit user feedback data such as clicks, and has been receiving considerable attention recently. A popular ULTR approach for real-world applications uses a two-tower architecture, where click modeling is factorized into a relevance tower with regular input features, and a bias tower with bias-relevant inputs such as the position of a document. A successful factorization will allow the relevance tower to be exempt from biases. In this work, we identify a critical issue that existing ULTR methods ignored - the bias tower can be confounded with the relevance tower via the underlying true relevance. In particular, the positions were determined by the logging policy, i.e., the previous production model, which would possess relevance information. We give both theoretical analysis and empirical results to show the negative effects on relevance tower due to such a correlation. We then propose three methods to mitigate the negative confounding effects by better disentangling relevance and bias. Empirical results on both controlled public datasets and a large-scale industry dataset show the effectiveness of the proposed approaches.|无偏学习排序(ULTR)研究的是如何减轻隐性用户反馈数据(如点击)中的各种偏差，近年来受到了广泛的关注。一种流行的 ULTR 方法用于现实世界的应用程序使用一个双塔架构，其中点击建模被分解为一个具有常规输入特征的相关塔，以及一个具有偏倚相关输入(如文档的位置)的偏倚塔。一个成功的因子分解将使相关塔免于偏见。在这项工作中，我们确定了一个关键问题，现有的 ULTR 方法忽略-偏倚塔可以混淆与相关塔通过潜在的真实相关性。具体来说，位置是由测井策略决定的，即先前的生产模型，它将拥有相关信息。我们给出了理论分析和实证结果来说明这种相关性对关联塔的负面影响。然后，我们提出了三种方法，通过更好地分离相关性和偏倚来减轻负面混杂效应。对受控公共数据集和大规模行业数据集的实证结果表明了该方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Disentangling+Relevance+and+Bias+in+Unbiased+Learning+to+Rank)|0|
|[M5: Multi-Modal Multi-Interest Multi-Scenario Matching for Over-the-Top Recommendation](https://doi.org/10.1145/3580305.3599863)|Pengyu Zhao, Xin Gao, Chunxu Xu, Liang Chen||Matching preferred shows to the subscribers is extremely important in the Over-the-Top (OTT) platforms. The existing methods did not adequately consider the characteristics of the OTT services, i.e., rich meta information, diverse user interests, and mixed recommendation scenarios, leading to sub-optimal performance. This paper introduces the Multi-Modal Multi-Interest Multi-Scenario Matching (M5) for the OTT recommendation to fully exploit these attributes. A multi-modal embedding layer is first introduced to transform the show IDs into both ID embeddings initialized randomly and content graph (CG) embeddings derived from the node representations pre-trained on a metagraph. To segregate the semantics between ID and CG embeddings, M5 exploits the mirrored two-tower modeling in the subsequent layers for efficiency and effectiveness. Specifically, a multi-interest extraction layer is proposed separately on ID and CG behaviors to model users' coarse-grained and fine-grained interests through behavioral categorization, subsidiary decoration, masked-language-modeling augmented self-attention modeling and subsidiary-intensity interest calibration. Facing the inherent diverse scenarios, M5 distinguishes the scenario differences at both feature and model levels, which crosses features with the scenario indicators and employs Split Mixture-of-Experts to generate the ID, and CG user embeddings. Finally, a weighted candidate matching layer is established to calculate the ID- and CG-oriented user-item preferences and then merge into a hybrid score with dynamic weighting. The extensive online and offline experiments over two real-world OTT platforms Hulu and Disney+ reveal that M5 significantly outperforms the previous state-of-the-art and online matching algorithms over various scenarios, indicating the effectiveness and robustness of the proposed method. M5 has been fully deployed on the main traffic of the most popular "For You'' sets of both platforms, continuously enhancing the user experience for hundreds of millions of subscribers every day and steadily increasing business revenue.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=M5:+Multi-Modal+Multi-Interest+Multi-Scenario+Matching+for+Over-the-Top+Recommendation)|0|
|[Accelerating Personalized PageRank Vector Computation](https://doi.org/10.1145/3580305.3599251)|Zhen Chen, Xingzhi Guo, Baojian Zhou, Deqing Yang, Steven Skiena|State University of New York at Stony Brook; Fudan University|Personalized PageRank Vectors are widely used as fundamental graph-learning tools for detecting anomalous spammers, learning graph embeddings, and training graph neural networks. The well-known local FwdPush algorithm approximates PPVs and has a sublinear rate of $O\big(\frac{1}{\alpha\epsilon}\big)$. A recent study found that when high precision is required, FwdPush is similar to the power iteration method, and its run time is pessimistically bounded by $O\big(\frac{m}{\alpha} \log\frac{1}{\epsilon}\big)$. This paper looks closely at calculating PPVs for both directed and undirected graphs. By leveraging the linear invariant property, we show that FwdPush is a variant of Gauss-Seidel and propose a Successive Over-Relaxation based method, FwdPushSOR to speed it up by slightly modifying FwdPush. Additionally, we prove FwdPush has local linear convergence rate $O\big(\tfrac{\text{vol}(S)}{\alpha} \log\tfrac{1}{\epsilon}\big)$ enjoying advantages of two existing bounds. We also design a new local heuristic push method that reduces the number of operations by 10-50 percent compared to FwdPush. For undirected graphs, we propose two momentum-based acceleration methods that can be expressed as one-line updates and speed up non-acceleration methods by$\mathcal{O}\big(\tfrac{1}{\sqrt{\alpha}}\big)$. Our experiments on six real-world graph datasets confirm the efficiency of FwdPushSOR and the acceleration methods for directed and undirected graphs, respectively.|个性化 PageRank 向量广泛用作基本的图形学习工具，用于检测异常垃圾邮件发送者、学习图形嵌入和训练图形神经网络。著名的局部 FwdPush 算法近似于 PPV，其次线性速率为 $O big (frac {1}{ alpha epsilon } big) $。最近的一项研究发现，当需要高精度时，FwdPush 类似于幂迭代法，其运行时间悲观地受到 $O big (frac { m }{ alpha } log frac {1}{ epsilon } big) $的限制。本文主要研究有向图和无向图的 PPV 的计算。通过利用线性不变性，我们证明了 FwdPush 是 Gauss-Seidel 的一个变体，并提出了一个基于逐次超松驰法的方法，FwdPushSOR，通过稍微修改 FwdPush 来加速它。另外，我们证明了 FwdPush 具有局部线性收敛速度 $O big (tfrac { text { vol }(S)}{ alpha } log tfrac {1}{ epsilon } big) $具有两个现有界的优点。我们还设计了一种新的局部启发式推送方法，与 FwdPush 相比减少了10-50% 的操作次数。对于无向图，我们提出了两种基于动量的加速方法，它们可以表示为一行更新，并且可以通过 $mathcal { O } big (tfrac {1}{ sqrt { alpha }} big) $来加速非加速方法。我们在六个实际图形数据集上的实验分别证实了 FwdPushSOR 和有向图和无向图加速方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Accelerating+Personalized+PageRank+Vector+Computation)|0|
|[Text Is All You Need: Learning Language Representations for Sequential Recommendation](https://doi.org/10.1145/3580305.3599519)|Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, Julian J. McAuley|Amazon; University of California, San Diego|Sequential recommendation aims to model dynamic user behavior from historical interactions. Existing methods rely on either explicit item IDs or general textual features for sequence modeling to understand user preferences. While promising, these approaches still struggle to model cold-start items or transfer knowledge to new datasets. In this paper, we propose to model user preferences and item features as language representations that can be generalized to new items and datasets. To this end, we present a novel framework, named Recformer, which effectively learns language representations for sequential recommendation. Specifically, we propose to formulate an item as a "sentence" (word sequence) by flattening item key-value attributes described by text so that an item sequence for a user becomes a sequence of sentences. For recommendation, Recformer is trained to understand the "sentence" sequence and retrieve the next "sentence". To encode item sequences, we design a bi-directional Transformer similar to the model Longformer but with different embedding layers for sequential recommendation. For effective representation learning, we propose novel pretraining and finetuning methods which combine language understanding and recommendation tasks. Therefore, Recformer can effectively recommend the next item based on language representations. Extensive experiments conducted on six datasets demonstrate the effectiveness of Recformer for sequential recommendation, especially in low-resource and cold-start settings.|顺序推荐旨在从历史交互中建立动态用户行为模型。现有的方法依赖于显式的项 ID 或一般的文本特性来进行序列建模，以理解用户的首选项。尽管这些方法很有前途，但它们仍然难以对冷启动项目进行建模或将知识转移到新的数据集中。本文提出将用户偏好和项目特征建模为语言表示，并将其推广到新的项目和数据集。为此，我们提出了一个新的框架，称为 Recformer，它有效地学习语言表示顺序推荐。具体来说，我们建议通过将文本所描述的项目键值属性扁平化，将项目表述为“句子”(单词序列) ，从而使用户的项目序列成为一个句子序列。为了便于推荐，Recformer 接受了理解“句子”序列和检索下一个“句子”的训练。为了对项目序列进行编码，我们设计了一个类似于 Longform 模型但具有不同嵌入层的双向变压器用于顺序推荐。为了有效地进行表征学习，我们提出了一种新的预训练和微调方法，将语言理解和推荐任务结合起来。因此，Recformer 可以根据语言表示有效地推荐下一个项目。在六个数据集上进行的大量实验证明了 Recformer 对于顺序推荐的有效性，特别是在低资源和冷启动环境下。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Text+Is+All+You+Need:+Learning+Language+Representations+for+Sequential+Recommendation)|0|
|[MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction](https://doi.org/10.1145/3580305.3599422)|Jianghao Lin, Yanru Qu, Wei Guo, Xinyi Dai, Ruiming Tang, Yong Yu, Weinan Zhang|Shanghai Jiao Tong University; Huawei NoahÊäØ Ark Lab|With the widespread application of personalized online services, click-through rate (CTR) prediction has received more and more attention and research. The most prominent features of CTR prediction are its multi-field categorical data format, and vast and daily-growing data volume. The large capacity of neural models helps digest such massive amounts of data under the supervised learning paradigm, yet they fail to utilize the substantial data to its full potential, since the 1-bit click signal is not sufficient to guide the model to learn capable representations of features and instances. The self-supervised learning paradigm provides a more promising pretrain-finetune solution to better exploit the large amount of user click logs, and learn more generalized and effective representations. However, self-supervised learning for CTR prediction is still an open question, since current works on this line are only preliminary and rudimentary. To this end, we propose a Model-agnostic pretraining (MAP) framework that applies feature corruption and recovery on multi-field categorical data, and more specifically, we derive two practical algorithms: masked feature prediction (MFP) and replaced feature detection (RFD). MFP digs into feature interactions within each instance through masking and predicting a small portion of input features, and introduces noise contrastive estimation (NCE) to handle large feature spaces. RFD further turns MFP into a binary classification mode through replacing and detecting changes in input features, making it even simpler and more effective for CTR pretraining. Our extensive experiments on two real-world large-scale datasets (i.e., Avazu, Criteo) demonstrate the advantages of these two methods on several strong backbones (e.g., DCNv2, DeepFM), and achieve new state-of-the-art performance in terms of both effectiveness and efficiency for CTR prediction.|随着个性化网上服务的广泛应用，点进率预测越来越受到重视和研究。CTR 预测最突出的特点是它的多领域分类数据格式，以及海量和日益增长的数据量。神经模型的巨大容量有助于在监督式学习范式下消化如此大量的数据，但它们未能充分利用大量的数据，因为1位点击信号不足以指导模型学习特征和实例的能力表示。自监督学习范式为更好地利用大量的用户点击日志，学习更广泛和有效的表示提供了一种更有前途的预训练-微调解决方案。然而，自我监督学习的 CTR 预测仍然是一个悬而未决的问题，因为目前在这方面的工作只是初步和基础。为此，我们提出了一个模型无关预训练(model-agnotic pretraining，MAP)框架，该框架将特征损坏和恢复应用于多领域分类数据，更具体地说，我们推导出两种实用算法: 掩盖特征预测(mFP)和替换特征提取(RFD)。MFP 通过屏蔽和预测一小部分输入特征，深入挖掘每个实例中的特征交互，并引入噪声对比估计(NCE)来处理较大的特征空间。RFD 通过替换和检测输入特征的变化，进一步将 MFP 转化为二进制分类模式，使 CTR 预训练更加简单有效。我们在两个真实世界的大规模数据集(例如，Avazu，Criteo)上的广泛实验证明了这两种方法在几个强骨干(例如，dCNv2，DeepFM)上的优势，并在有效性和效率方面实现了新的最先进的 CTR 预测性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MAP:+A+Model-agnostic+Pretraining+Framework+for+Click-through+Rate+Prediction)|0|
|[Learning to Relate to Previous Turns in Conversational Search](https://doi.org/10.1145/3580305.3599411)|Fengran Mo, JianYun Nie, Kaiyu Huang, Kelong Mao, Yutao Zhu, Peng Li, Yang Liu|Renmin University of China; Tsinghua University; University of Montreal|Conversational search allows a user to interact with a search system in multiple turns. A query is strongly dependent on the conversation context. An effective way to improve retrieval effectiveness is to expand the current query with historical queries. However, not all the previous queries are related to, and useful for expanding the current query. In this paper, we propose a new method to select relevant historical queries that are useful for the current query. To cope with the lack of labeled training data, we use a pseudo-labeling approach to annotate useful historical queries based on their impact on the retrieval results. The pseudo-labeled data are used to train a selection model. We further propose a multi-task learning framework to jointly train the selector and the retriever during fine-tuning, allowing us to mitigate the possible inconsistency between the pseudo labels and the changed retriever. Extensive experiments on four conversational search datasets demonstrate the effectiveness and broad applicability of our method compared with several strong baselines.|会话搜索允许用户多次与搜索系统交互。查询强烈依赖于会话上下文。提高检索效率的一个有效方法是使用历史查询扩展当前查询。但是，并非所有以前的查询都与之相关，并且对于展开当前查询非常有用。本文提出了一种新的方法来选择对当前查询有用的相关历史查询。为了解决缺乏标记训练数据的问题，我们使用伪标记方法根据有用的历史查询对检索结果的影响来注释它们。利用伪标记数据训练选择模型。我们进一步提出了一个多任务学习框架，在微调过程中联合训练选择器和检索器，使我们能够减轻伪标签和更改后的检索器之间可能的不一致性。通过对四个会话搜索数据集的大量实验，证明了该方法的有效性和广泛的适用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Relate+to+Previous+Turns+in+Conversational+Search)|0|
|[PSLOG: Pretraining with Search Logs for Document Ranking](https://doi.org/10.1145/3580305.3599477)|Zhan Su, Zhicheng Dou, Yujia Zhou, Ziyuan Zhao, JiRong Wen||Recently, pretrained models have achieved remarkable performance not only in natural language processing but also in information retrieval (IR). Previous studies show that IR-oriented pretraining tasks can achieve better performance than only finetuning pretrained language models in IR datasets. Besides, the massive search log data obtained from mainstream search engines can be used in IR pretraining, for it contains users' implicit judgments of document relevance under a concrete query. However, existing methods mainly use direct query-document click signals to pretrain models. The potential supervision signals from search logs are far from being well explored. In this paper, we propose to comprehensively leverage four query-document relevance relations, including co-interaction and multi-hop relations, to pretrain ranking models in IR. Specifically, we focus on the user's click behavior and construct an Interaction Graph to represent the global relevance relations between queries and documents from all search logs. With the graph, we can consider the co-interaction and multi-hop q-d relationships through their neighbor nodes. Based on the relations extracted from the interaction graph, we propose four strategies to generate contrastive positive and negative q-d pairs and use these data to pretrain ranking models. Experimental results on both industrial and academic datasets demonstrate the effectiveness of our method.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PSLOG:+Pretraining+with+Search+Logs+for+Document+Ranking)|0|
|[Improving Conversational Recommendation Systems via Counterfactual Data Simulation](https://doi.org/10.1145/3580305.3599387)|Xiaolei Wang, Kun Zhou, Xinyu Tang, Wayne Xin Zhao, Fan Pan, Zhao Cao, JiRong Wen|Renmin University of China; Huawei Poisson Lab|Conversational recommender systems (CRSs) aim to provide recommendation services via natural language conversations. Although a number of approaches have been proposed for developing capable CRSs, they typically rely on sufficient training data for training. Since it is difficult to annotate recommendation-oriented dialogue datasets, existing CRS approaches often suffer from the issue of insufficient training due to the scarcity of training data. To address this issue, in this paper, we propose a CounterFactual data simulation approach for CRS, named CFCRS, to alleviate the issue of data scarcity in CRSs. Our approach is developed based on the framework of counterfactual data augmentation, which gradually incorporates the rewriting to the user preference from a real dialogue without interfering with the entire conversation flow. To develop our approach, we characterize user preference and organize the conversation flow by the entities involved in the dialogue, and design a multi-stage recommendation dialogue simulator based on a conversation flow language model. Under the guidance of the learned user preference and dialogue schema, the flow language model can produce reasonable, coherent conversation flows, which can be further realized into complete dialogues. Based on the simulator, we perform the intervention at the representations of the interacted entities of target users, and design an adversarial training method with a curriculum schedule that can gradually optimize the data augmentation strategy. Extensive experiments show that our approach can consistently boost the performance of several competitive CRSs, and outperform other data augmentation methods, especially when the training data is limited. Our code is publicly available at https://github.com/RUCAIBox/CFCRS.|会话推荐系统(CRS)旨在通过自然语言对话提供推荐服务。虽然已经提出了一些开发有能力的 CRS 的方法，但它们通常依赖于足够的培训数据进行培训。由于很难对面向建议的对话数据集进行注释，现有的 CRS 方法往往因缺乏培训数据而面临培训不足的问题。为了解决这一问题，本文提出了一种 CRS 的 CounterFact 数据模拟方法 CFCRS，以缓解 CRS 中的数据稀缺问题。我们的方法是在反事实数据增强框架的基础上发展起来的，该框架在不干扰整个会话流程的情况下，逐渐将真实对话中的用户偏好重写纳入其中。为了开发这种方法，我们描述了用户偏好的特征，并根据对话所涉及的实体组织了对话流程，设计了一个基于对话流程语言模型的多阶段推荐对话模拟器。在用户偏好和对话模式的指导下，流语言模型可以产生合理、连贯的会话流，进一步实现完整的对话。在该模拟器的基础上，对目标用户交互实体的表示进行干预，设计了一种基于课程表的对抗性训练方法，可以逐步优化数据增强策略。大量实验表明，该方法可以持续提高多个竞争性 CRS 的性能，并且优于其他数据增强方法，特别是在训练数据有限的情况下。我们的代码可以在 https://github.com/rucaibox/cfcrs 上公开获取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Conversational+Recommendation+Systems+via+Counterfactual+Data+Simulation)|0|
|[Efficient and Joint Hyperparameter and Architecture Search for Collaborative Filtering](https://doi.org/10.1145/3580305.3599322)|Yan Wen, Chen Gao, Lingling Yi, Liwei Qiu, Yaqing Wang, Yong Li|Tencent Inc.; Tsinghua University; Baidu Inc.|Automated Machine Learning (AutoML) techniques have recently been introduced to design Collaborative Filtering (CF) models in a data-specific manner. However, existing works either search architectures or hyperparameters while ignoring the fact they are intrinsically related and should be considered together. This motivates us to consider a joint hyperparameter and architecture search method to design CF models. However, this is not easy because of the large search space and high evaluation cost. To solve these challenges, we reduce the space by screening out usefulness yperparameter choices through a comprehensive understanding of individual hyperparameters. Next, we propose a two-stage search algorithm to find proper configurations from the reduced space. In the first stage, we leverage knowledge from subsampled datasets to reduce evaluation costs; in the second stage, we efficiently fine-tune top candidate models on the whole dataset. Extensive experiments on real-world datasets show better performance can be achieved compared with both hand-designed and previous searched models. Besides, ablation and case studies demonstrate the effectiveness of our search framework.|自动机器学习(AutoML)技术最近被引入到设计特定数据的协同过滤模型(CF)中。然而，现有的工作要么搜索体系结构或超参数，而忽略了这些内在联系的事实，应该一起考虑。这促使我们考虑联合使用超参数和体系结构搜索方法来设计 CF 模型。然而，这并不容易，因为大搜索空间和高评价成本。为了解决这些挑战，我们通过全面理解各个超参数筛选出有用的超参数选择来减少空间。接下来，我们提出了一个两阶段的搜索算法，以找到适当的配置从缩减的空间。在第一阶段，我们利用次采样数据集的知识来降低评估成本; 在第二阶段，我们有效地微调整整个数据集上的顶级候选模型。在真实世界数据集上的大量实验表明，与手工设计和以前的搜索模型相比，该算法可以获得更好的性能。此外，消融和案例研究证明了我们的搜索框架的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+and+Joint+Hyperparameter+and+Architecture+Search+for+Collaborative+Filtering)|0|
|[Efficient Single-Source SimRank Query by Path Aggregation](https://doi.org/10.1145/3580305.3599328)|Mingxi Zhang, Yanghua Xiao, Wei Wang|Fudan University; University of Shanghai for Science and Technology|Single-source SimRank query calculates the similarity between a query node and every node in a graph, which traverses the paths starting from the query node for similarity computation. However, the scale of the paths increases exponentially as path length increases, which decreases the computation efficiency. Sampling-based algorithms reduce computational cost by path sampling, but they need to sample sufficient paths to ensure the accuracy, and the performance might be affected by the large scale of paths. In this paper, we propose VecSim for efficient single-source SimRank query by path aggregation. VecSim first aggregates the paths starting from query node with common arrived nodes step by step to obtain the hitting probabilities, and then aggregates the paths starting from the arrived nodes reversely to obtain the first-meeting probabilities in a similar way, in which only several vectors are maintained. The extra-meeting probabilities are excluded from each step, and an efficient sampling-based algorithm is designed, which estimates the extra-meeting probabilities by sampling paths within a specified length. For further speeding up query processing, we propose a threshold-sieved algorithm, which prunes the entries with small values that contribute little to the final similarity scores by setting a threshold. Extensive experiments are done on four small and four large graphs, which demonstrate that VecSim outperforms the competitors in terms of time and space costs on a comparable accuracy. In particular, VecSim achieves an empirical error of 10 -4 level in under 0.1 second over all of these graphs.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Single-Source+SimRank+Query+by+Path+Aggregation)|0|
|[Adaptive Disentangled Transformer for Sequential Recommendation](https://doi.org/10.1145/3580305.3599253)|Yipeng Zhang, Xin Wang, Hong Chen, Wenwu Zhu|Tsinghua University, Tsinghua University|Sequential recommendation aims at mining time-aware user interests through modeling sequential behaviors. Transformer, as an effective architecture designed to process sequential input data, has shown its superiority in capturing sequential relations for recommendation. Nevertheless, existing Transformer architectures lack explicit regularization for layer-wise disentanglement, which fails to take advantage of disentangled representation in recommendation and leads to suboptimal performance. In this paper, we study the problem of layer-wise disentanglement for Transformer architectures and propose the Adaptive Disentangled Transformer (ADT) framework, which is able to adaptively determine the optimal degree of disentanglement of attention heads within different layers. Concretely, we propose to encourage disentanglement by requiring the independence constraint via mutual information estimation over attention heads and employing auxiliary objectives to prevent the information from collapsing into useless noise. We further propose a progressive scheduler to adaptively adjust the weights controlling the degree of disentanglement via an evolutionary process. Extensive experiments on various real-world datasets demonstrate the effectiveness of our proposed ADT framework.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adaptive+Disentangled+Transformer+for+Sequential+Recommendation)|0|
|[CADENCE: Offline Category Constrained and Diverse Query Generation for E-commerce Autosuggest](https://doi.org/10.1145/3580305.3599787)|Abhinav Anand, Surender Kumar, Nandeesh Kumar, Samir Shah||Query AutoComplete (QAC) or AutoSuggest is the first place of user interaction with an e-commerce search engine. It is critical for the QAC system to suggest relevant and well-formed queries for multiple possible user intents. Suggesting only the historical user queries fails in the case of infrequent or new prefixes. Much of the recent works generate synthetic candidates using models trained on user queries and thus have these issues: a) cold start problem as new products in the catalogue fail to get visibility due to lack of representation in user queries b) poor quality of generated candidates due to concept drift and c) low diversity/coverage of attributes such as brand, color & other facets in generated candidates. In this paper, we propose an offline neural query generation framework - CADENCE - to address these challenges by a) using both user queries and noisy product titles to train two separate neural language models using self-attention memory networks, b) adding category constraints during the training and query generation process to prevent concept drift c) implementing customized dynamic beam search to generate more diverse candidates for a given prefix. Besides solving for cold start and rare/unseen prefix coverage, CADENCE also increases the coverage of the existing query prefixes through a higher number of relevant and diverse query suggestions. We generated ~700K new offline queries, which have resulted in significant improvement in recall, reduction in product cold start, and increased coverage of attributes. Online A/B tests also show a significant impact on QAC usage, downstream search click-through rates, and product conversion.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CADENCE:+Offline+Category+Constrained+and+Diverse+Query+Generation+for+E-commerce+Autosuggest)|0|
|[PEPNet: Parameter and Embedding Personalized Network for Infusing with Personalized Prior Information](https://doi.org/10.1145/3580305.3599884)|Jianxin Chang, Chenbin Zhang, Yiqun Hui, Dewei Leng, Yanan Niu, Yang Song, Kun Gai|Kuaishou Technology; Unaffiliated|With the increase of content pages and display styles in online services such as online-shopping and video-watching websites, industrial-scale recommender systems face challenges in multi-domain and multi-task recommendations. The core of multi-task and multi-domain recommendation is to accurately capture user interests in different domains given different user behaviors. In this paper, we propose a plug-and-play \textit{\textbf{P}arameter and \textbf{E}mbedding \textbf{P}ersonalized \textbf{Net}work (\textbf{PEPNet})} for multi-task recommendation in the multi-domain setting. PEPNet takes features with strong biases as input and dynamically scales the bottom-layer embeddings and the top-layer DNN hidden units in the model through a gate mechanism. By mapping personalized priors to scaling weights ranging from 0 to 2, PEPNet introduces both parameter personalization and embedding personalization. Embedding Personalized Network (EPNet) selects and aligns embeddings with different semantics under multiple domains. Parameter Personalized Network (PPNet) influences DNN parameters to balance interdependent targets in multiple tasks. We have made a series of special engineering optimizations combining the Kuaishou training framework and the online deployment environment. We have deployed the model in Kuaishou apps, serving over 300 million daily users. Both online and offline experiments have demonstrated substantial improvements in multiple metrics. In particular, we have seen a more than 1\% online increase in three major scenarios.|随着在线购物和视频观看网站等在线服务内容页面和显示方式的增加，工业规模的推荐系统面临着多领域、多任务推荐的挑战。多任务、多领域推荐的核心是根据不同的用户行为准确捕获不同领域的用户兴趣。本文针对多领域环境下的多任务推荐问题，提出了一种即插即用的文本参数{ textbf { P }参数和 textbf { E }嵌入式 textbf { P }个性化 textbf { Net } work (textbf { PEPNet })}。PEPNet 以具有强偏差的特征作为输入，通过门机制动态扩展模型中的底层嵌入和顶层 DNN 隐藏单元。通过将个性化前期映射到0到2之间的权重，PEPNet 引入了参数个性化和嵌入个性化。嵌入式个性化网络(EPNet)在多个域下选择和对齐具有不同语义的嵌入式。参数个性化网络(PPNet)影响 DNN 参数以平衡多任务中相互依赖的目标。我们结合快手培训框架和在线部署环境，进行了一系列特殊的工程优化。我们在 Kuaishou 的应用程序中采用了这种模式，每天为超过3亿用户提供服务。这两个在线和离线的实验都显示了在多个指标方面的重大改进。特别是，我们已经看到在三种主要情况下在线增长超过1% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PEPNet:+Parameter+and+Embedding+Personalized+Network+for+Infusing+with+Personalized+Prior+Information)|0|
|[Controllable Multi-Objective Re-ranking with Policy Hypernetworks](https://doi.org/10.1145/3580305.3599796)|Sirui Chen, Yuan Wang, Zijing Wen, Zhiyu Li, Changshuo Zhang, Xiao Zhang, Quan Lin, Cheng Zhu, Jun Xu||Multi-stage ranking pipelines have become widely used strategies in modern recommender systems, where the final stage aims to return a ranked list of items that balances a number of requirements such as user preference, diversity, novelty etc. Linear scalarization is arguably the most widely used technique to merge multiple requirements into one optimization objective, by summing up the requirements with certain preference weights. Existing final-stage ranking methods often adopt a static model where the preference weights are determined during offline training and kept unchanged during online serving. Whenever a modification of the preference weights is needed, the model has to be re-trained, which is time and resources inefficient. Meanwhile, the most appropriate weights may vary greatly for different groups of targeting users or at different time periods (e.g., during holiday promotions). In this paper, we propose a framework called controllable multi-objective re-ranking (CMR) which incorporates a hypernetwork to generate parameters for a re-ranking model according to different preference weights. In this way, CMR is enabled to adapt the preference weights according to the environment changes in an online manner, without retraining the models. Moreover, we classify practical business-oriented tasks into four main categories and seamlessly incorporate them in a new proposed re-ranking model based on an Actor-Evaluator framework, which serves as a reliable real-world testbed for CMR. Offline experiments based on the dataset collected from Taobao App showed that CMR improved several popular re-ranking models by using them as underlying models. Online A/B tests also demonstrated the effectiveness and trustworthiness of CMR.|多阶段排名管道已经成为现代推荐系统中广泛使用的策略，最后阶段的目标是返回一个项目的排名列表，平衡用户偏好、多样性、新颖性等要求。线性标量可以说是最广泛使用的技术合并多个需求到一个优化目标，通过总结需求与一定的偏好权重。现有的最后阶段排序方法通常采用静态模型，在离线训练时确定偏好权重，在线服务时保持不变。当需要修改偏好权重时，模型必须重新训练，这是时间和资源效率低下的。与此同时，最合适的权重可能会因不同的目标用户群体或在不同的时间段(例如，在假日促销期间)而有很大差异。本文提出了一种可控的多目标重排序(CMR)框架，该框架结合了一个超网络，根据不同的偏好权重为重排序模型生成参数。通过这种方式，CMR 能够根据环境变化在线调整偏好权重，而不需要重新训练模型。此外，我们将面向业务的实际任务分为四个主要类别，并将它们无缝地纳入一个新提出的重新排序模型，该模型基于一个演员-评估者框架，作为一个可靠的现实世界的 CMR 测试平台。基于从淘宝应用收集的数据集的离线实验表明，CMR 通过使用它们作为基础模型改进了几个流行的重新排名模型。在线 A/B 测试也证明了 CMR 的有效性和可信性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Controllable+Multi-Objective+Re-ranking+with+Policy+Hypernetworks)|0|
|[CT4Rec: Simple yet Effective Consistency Training for Sequential Recommendation](https://doi.org/10.1145/3580305.3599798)|Liu Chong, Xiaoyang Liu, Rongqin Zheng, Lixin Zhang, Xiaobo Liang, Juntao Li, Lijun Wu, Min Zhang, Leyu Lin||Sequential recommendation methods are increasingly important in cutting-edge recommender systems. Through leveraging historical records, the systems can capture user interests and perform recommendations accordingly. State-of-the-art sequential recommendation models proposed very recently combine contrastive learning techniques for obtaining high-quality user representations. Though effective and performing well, the models based on contrastive learning require careful selection of data augmentation methods and pretext tasks, efficient negative sampling strategies, and massive hyper-parameters validation. In this paper, we propose an ultra-simple alternative for obtaining better user representations and improving sequential recommendation performance. Specifically, we present a simple yet effective Consistency T braining method for sequential Recommendation (CT4Rec) in which only two extra training objectives are utilized without any structural modifications and data augmentation. Experiments on three benchmark datasets and one large newly crawled industrial corpus demonstrate that our proposed method outperforms SOTA models by a large margin and with much less training time than these based on contrastive learning. Online evaluation on real-world content recommendation system also achieves 2.717% improvement on the click-through rate and 3.679% increase on the average click number per capita. Further exploration reveals that such a simple method has great potential for CTR prediction. Our code is available at https://github.com/ct4rec/CT4Rec.git.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CT4Rec:+Simple+yet+Effective+Consistency+Training+for+Sequential+Recommendation)|0|
|[S2phere: Semi-Supervised Pre-training for Web Search over Heterogeneous Learning to Rank Data](https://doi.org/10.1145/3580305.3599935)|Yuchen Li, Haoyi Xiong, Linghe Kong, Qingzhong Wang, Shuaiqiang Wang, Guihai Chen, Dawei Yin|Shanghai Jiao Tong University, Shanghai, China; Baidu Inc., Beijing, China|While Learning to Rank (LTR) models on top of transformers have been widely adopted to achieve decent performance, it is still challenging to train the model with sufficient data as only an extremely small number of query-webpage pairs could be annotated versus trillions of webpages available online and billions of web search queries everyday. In the meanwhile, industry research communities have released a number of open-source LTR datasets with well annotations but incorporating different designs of LTR features/labels (i.e., heterogeneous domains). In this work, inspired by the recent progress in pre-training transformers for performance advantages, we study the problem of pre-training LTR models using both labeled and unlabeled samples, especially we focus on the use of well-annotated samples in heterogeneous open-source LTR datasets to boost the performance of pre-training. Hereby, we propose S 2 phere-Semi-Supervised Pre-training with Heterogeneous LTR data strategies for LTR models using both unlabeled and labeled query-webpage pairs across heterogeneous LTR datasets. S 2 phere consists of a three-step approach: (1) Semi-supervised Feature Extraction Pre-training via Perturbed Contrastive Loss, (2) Cross-domain Ranker Pre-training over Heterogeneous LTR Datasets and (3) End-to-end LTR Fine-tuning via Modular Network Composition. Specifically, given an LTR model composed of a backbone (the feature extractor), a neck (the module to reason the orders) and a head (the predictor of ranking scores), S 2 phere uses unlabeled/labeled data from the search engine to pre-train the backbone in Step (1) via semi-supervised learning; then Step (2) incorporates multiple open-source heterogeneous LTR datasets to improve pre-training of the neck module as shared parameters of cross-domain learning; and finally, S2phere in Step (3) composes the backbone and neck with a randomly-initialized head into a whole LTR model and fine-tunes the model using search engine data with various learning strategies. Extensive experiments have been done with both offline experiments and online A/B Test on top of Baidu search engine. The comparisons against numbers of baseline algorithms confirmed the advantages of S 2 phere in producing high-performance LTR models for web-scale search.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=S2phere:+Semi-Supervised+Pre-training+for+Web+Search+over+Heterogeneous+Learning+to+Rank+Data)|0|
|[Multi-Label Learning to Rank through Multi-Objective Optimization](https://doi.org/10.1145/3580305.3599870)|Debabrata Mahapatra, Chaosheng Dong, Yetian Chen, Michinari Momma||Learning to Rank (LTR) technique is ubiquitous in the Information Retrieval system nowadays, especially in the Search Ranking application. The query-item relevance labels typically used to train the ranking model are often noisy measurements of human behavior, e.g., product rating for product search. The coarse measurements make the ground truth ranking non-unique with respect to a single relevance criterion. To resolve ambiguity, it is desirable to train a model using many relevance criteria, giving rise to Multi-Label LTR (MLLTR). Moreover, it formulates multiple goals that may be conflicting yet important to optimize for simultaneously, e.g., in product search, a ranking model can be trained based on product quality and purchase likelihood to increase revenue. In this research, we leverage the Multi-Objective Optimization (MOO) aspect of the MLLTR problem and employ recently developed MOO algorithms to solve it. Specifically, we propose a general framework where the information from labels can be combined in a variety of ways to meaningfully characterize the trade-off among the goals. Our framework allows for any gradient based MOO algorithm to be used for solving the MLLTR problem. We test the proposed framework on two publicly available LTR datasets and one e-commerce dataset to show its efficacy.|学习排名(LTR)技术在当今的信息检索系统中无处不在，特别是在搜索排名应用程序中。通常用于训练排名模型的查询条目相关标签通常是对人类行为的嘈杂测量，例如，产品搜索的产品评级。粗测量使得地面真实度排序相对于单一的相关准则是非唯一的。为了解决模糊问题，需要使用多个相关准则来训练模型，从而产生多标签 LTR (MLLTR)。此外，它制定了多个目标，可能是冲突的，但重要的优化同时进行，例如，在产品搜索，排名模型可以训练基于产品质量和购买可能性，以增加收入。在这项研究中，我们利用多目标优化(MOO)方面的 MLLTR 问题，并采用最近开发的 MOO 算法来解决它。具体而言，我们提出了一个总体框架，在这个框架中，可以通过各种方式组合来自标签的信息，以有意义地描述目标之间的权衡。我们的框架允许使用任何基于梯度的 MOO 算法来解决 MLLTR 问题。我们在两个公开的 LTR 数据集和一个电子商务数据集上测试了该框架，以验证其有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Label+Learning+to+Rank+through+Multi-Objective+Optimization)|0|
|[Entity-aware Multi-task Learning for Query Understanding at Walmart](https://doi.org/10.1145/3580305.3599816)|Zhiyuan Peng, Vachik Dave, Nicole McNabb, Rahul Sharnagat, Alessandro Magnani, Ciya Liao, Yi Fang, Sravanthi Rajanala||Query Understanding (QU) is a fundamental process in E-commerce search engines by extracting the shopping intents of customers. It usually includes a set of different tasks such as named entity recognization and query classification. Traditional approaches often tackle each task separately by its own network, which leads to excessive workload for development and maintenance as well as increased latency and resource usage in large-scale E-commerce platforms. To tackle these challenges, this paper presents a multi-task learning approach to query understanding at Walmart. We experimented with several state-of-the-art multi-task learning architectures including MTDNN, MMoE, and PLE. Furthermore, we propose a novel large-scale entity-aware multi-task learning model (EAMT) 1 by retrieving entities from engagement data as query context to augment the query representation. To the best of our knowledge, there exists no prior work on multi-task learning for E-commerce query understanding. Comprehensive offline experiments are conducted on industry-scale datasets (up to 965M queries) to illustrate the effectiveness of our approach. The results from online experiments show substantial gains in key accuracy and latency metrics. https://github.com/zhiyuanpeng/KDD2023-EAMT|查询理解(QU)是电子商务搜索引擎中通过提取顾客购物意图的一个基本过程。它通常包括一组不同的任务，如命名实体识别和查询分类。传统的方法往往通过自己的网络分别处理每一项任务，这导致大规模电子商务平台的开发和维护工作量过大，延迟和资源使用增加。为了应对这些挑战，本文提出了一个多任务学习方法来查询理解沃尔玛。我们试验了几种最先进的多任务学习架构，包括 MTDNN、 MMoE 和 PLE。在此基础上，提出了一种新的大规模实体感知多任务学习模型(EAMT)1，该模型通过从参与数据中检索实体作为查询上下文来增强查询表示。据我们所知，目前还没有关于电子商务查询理解的多任务学习的研究。在工业规模的数据集上进行了全面的离线实验(多达965M 个查询) ，以说明我们的方法的有效性。在线实验的结果显示，在关键准确性和延迟指标方面取得了实质性的进展。Https://github.com/zhiyuanpeng/kdd2023-eamt|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Entity-aware+Multi-task+Learning+for+Query+Understanding+at+Walmart)|0|
|[Improving Training Stability for Multitask Ranking Models in Recommender Systems](https://doi.org/10.1145/3580305.3599846)|Jiaxi Tang, Yoel Drori, Daryl Chang, Maheswaran Sathiamoorthy, Justin Gilmer, Li Wei, Xinyang Yi, Lichan Hong, Ed H. Chi|Google Deepmind; Google Research; Google Inc|Recommender systems play an important role in many content platforms. While most recommendation research is dedicated to designing better models to improve user experience, we found that research on stabilizing the training for such models is severely under-explored. As recommendation models become larger and more sophisticated, they are more susceptible to training instability issues, \emph{i.e.}, loss divergence, which can make the model unusable, waste significant resources and block model developments. In this paper, we share our findings and best practices we learned for improving the training stability of a real-world multitask ranking model for YouTube recommendations. We show some properties of the model that lead to unstable training and conjecture on the causes. Furthermore, based on our observations of training dynamics near the point of training instability, we hypothesize why existing solutions would fail, and propose a new algorithm to mitigate the limitations of existing solutions. Our experiments on YouTube production dataset show the proposed algorithm can significantly improve training stability while not compromising convergence, comparing with several commonly used baseline methods.|推荐系统在许多内容平台中发挥着重要作用。虽然大多数推荐研究致力于设计更好的模型来改善用户体验，但是我们发现，关于稳定此类模型的训练的研究严重不足。随着推荐模型的不断扩大和复杂化，它们更容易受到训练不稳定性问题的影响，如损失发散等，这些问题会导致模型无法使用，浪费大量资源和阻塞模型的发展。在本文中，我们分享了我们的发现和最佳实践，我们学到了提高训练的稳定性的一个真实世界的多任务排名模型 YouTube 的建议。我们给出了模型的一些性质，这些性质导致了训练的不稳定性和对原因的猜测。此外，基于我们对训练不稳定点附近的训练动力学的观察，我们假设为什么现有的解决方案会失败，并提出了一个新的算法来减轻现有解决方案的局限性。我们在 YouTube 生产数据集上的实验表明，与几种常用的基线方法相比，该算法能够在不影响收敛性的前提下显著提高训练的稳定性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Training+Stability+for+Multitask+Ranking+Models+in+Recommender+Systems)|0|
|[PASS: Personalized Advertiser-aware Sponsored Search](https://doi.org/10.1145/3580305.3599882)|Zhoujin Tian, Chaozhuo Li, Zhiqiang Zuo, Zengxuan Wen, Lichao Sun, Xinyue Hu, Wen Zhang, Haizhen Huang, Senzhang Wang, Weiwei Deng, Xing Xie, Qi Zhang||The nucleus of online sponsored search systems lies in measuring the relevance between the search intents of users and the advertising purposes of advertisers. Existing conventional doublet-based (query-keyword) relevance models solely rely on short queries and keywords to uncover such intents, which ignore the diverse and personalized preferences of participants (i.e., users and advertisers), resulting in undesirable advertising performance. In this paper, we investigate the novel problem of Personalized A dvertiser-aware Sponsored Search (PASS). Our motivation lies in incorporating the portraits of users and advertisers into relevance models to facilitate the modeling of intrinsic search intents and advertising purposes, leading to a quadruple-based (i.e., user-query-keyword-advertiser) task. Various types of historical behaviors are explored in the format of hypergraphs to provide abundant signals on identifying the preferences of participants. A novel heterogeneous textual hypergraph transformer is further proposed to deeply fuse the textual semantics and the high-order hypergraph topology. Our proposal is extensively evaluated over real industry datasets, and experimental results demonstrate its superiority.|在线赞助搜索系统的核心在于衡量用户的搜索意图与广告商的广告目的之间的相关性。现有的传统的基于对偶(查询-关键词)的相关性模型仅仅依赖于短查询和关键词来揭示这些意图，这忽略了参与者(即用户和广告商)的多样化和个性化偏好，导致不良的广告表现。在本文中，我们研究了个性化广告主意识支持搜索(PASS)的新问题。我们的动机在于将用户和广告商的肖像融入相关性模型，以促进内在搜索意图和广告目的的建模，从而导致一个基于四重(即，用户-查询-关键字-广告商)的任务。以超图的形式探讨了不同类型的历史行为，为识别参与者的偏好提供了丰富的信号。进一步提出了一种新的异构文本超图转换器，将文本语义和高阶超图拓扑深度融合。我们的方案在实际工业数据集上进行了广泛的评估，实验结果证明了其优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PASS:+Personalized+Advertiser-aware+Sponsored+Search)|0|
|[Towards Fairness in Personalized Ads Using Impression Variance Aware Reinforcement Learning](https://doi.org/10.1145/3580305.3599916)|Aditya Srinivas Timmaraju, Mehdi Mashayekhi, Mingliang Chen, Qi Zeng, Quintin Fettes, Wesley Cheung, Yihan Xiao, Manojkumar Rangasamy Kannadasan, Pushkar Tripathi, Sean Gahagan, Miranda Bogen, Rob Roudani|Meta|Variances in ad impression outcomes across demographic groups are increasingly considered to be potentially indicative of algorithmic bias in personalized ads systems. While there are many definitions of fairness that could be applicable in the context of personalized systems, we present a framework which we call the Variance Reduction System (VRS) for achieving more equitable outcomes in Meta's ads systems. VRS seeks to achieve a distribution of impressions with respect to selected protected class (PC) attributes that more closely aligns the demographics of an ad's eligible audience (a function of advertiser targeting criteria) with the audience who sees that ad, in a privacy-preserving manner. We first define metrics to quantify fairness gaps in terms of ad impression variances with respect to PC attributes including gender and estimated race. We then present the VRS for re-ranking ads in an impression variance-aware manner. We evaluate VRS via extensive simulations over different parameter choices and study the effect of the VRS on the chosen fairness metric. We finally present online A/B testing results from applying VRS to Meta's ads systems, concluding with a discussion of future work. We have deployed the VRS to all users in the US for housing ads, resulting in significant improvement in our fairness metric. VRS is the first large-scale deployed framework for pursuing fairness for multiple PC attributes in online advertising.|不同人群的广告印象结果的差异越来越被认为是个性化广告系统中算法偏差的潜在指示。虽然有许多公平的定义，可以适用于个性化系统的背景下，我们提出了一个框架，我们称之为方差减少系统(VRS) ，以实现更公平的结果在元数据的广告系统。VRS 试图通过选定的受保护类别(PC)属性来实现印象的分布，从而以保护隐私的方式将广告合格受众的人口统计数据(广告客户定位标准的功能)与看到该广告的受众的人口统计数据更紧密地联系起来。我们首先定义指标来量化广告印象差异的公平性差距方面的个人电脑属性，包括性别和估计的种族。然后，我们提出了一个印象方差感知的方式重新排名广告的 VRS。我们通过对不同参数选择的大量仿真来评估 VRS，并研究 VRS 对选择的公平性度量的影响。最后给出了 VRS 应用于 Meta 广告系统的在线 A/B 测试结果，并对今后的工作进行了讨论。我们已在美国所有用户的住房广告中部署了 VRS，从而显著改善了我们的公平性指标。VRS 是第一个大规模部署的框架，以追求公平的多个个人电脑属性在网上广告。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Fairness+in+Personalized+Ads+Using+Impression+Variance+Aware+Reinforcement+Learning)|0|
|[PlanRanker: Towards Personalized Ranking of Train Transfer Plans](https://doi.org/10.1145/3580305.3599887)|Jia Xu, Wanjie Tao, Zulong Chen, Jin Huang, Huihui Liu, Hong Wen, Shenghua Ni, Qun Dai, Yu Gu||Train transfer plan ranking has become the core business of online travel platforms (OTPs), due to the flourish development of high- speed rail technology and convenience of booking trains online. Currently, mainstream OTPs adopt rule-based or simple preference- based strategies to rank train transfer plans. However, the insuf- ficient emphasis on the costs of plans and the negligence of con- sidering reference transfer plans make these existing strategies less effective in solving the personalized ranking problem of train transfer plans. To this end, a novel personalized deep network (Plan- Ranker) is presented in this paper to better address the problem. In PlanRanker, a personalized learning component is first proposed to capture both of the query semantics and the target transfer plan- relevant personalized interests of a user over the user's behavior log data. Then, we present a cost learning component, where both of the price cost and the time cost of a target transfer plan are emphasized and learned. Finally, a reference transfer plan learning component is designed to enable the whole framework of PlanRanker to learn from reference transfer plans which are pieced together by plat- form users and thus reflect the wisdom of crowd. PlanRanker is now successfully deployed at Alibaba Fliggy, one of the largest OTPs in China, serving millions of users every day for train ticket reservation. Offline experiments on two production datasets and a country-scale online A/B test at Fliggy both demonstrate the superiority of the proposed PlanRanker over baselines.|列车换乘计划排序已成为在线旅游平台(OTP)的核心业务，由于高速铁路技术的蓬勃发展和在线预订列车的便利性。目前，主流 OTP 采用基于规则或简单偏好的策略对列车换乘计划进行排序。然而，由于对列车换乘计划成本的重视不够，而忽视了对参考换乘计划的考虑，使得现有的换乘策略在解决列车换乘计划个性化排序问题时效率较低。为此，本文提出了一种新的个性化深度网络(Plan-Ranker)来更好地解决这一问题。在 PlanRanker 中，首先提出了一个个性化学习组件，用于在用户行为日志数据上捕获用户的查询语义和与目标传输计划相关的个性化兴趣。然后，我们提出了一个成本学习组件，其中的价格成本和时间成本的目标转移计划都强调和学习。最后，设计了一个参考转移计划学习组件，使得 PlanRanker 的整个框架能够从平台用户拼凑起来的参考转移计划中学习，从而反映出群体的智慧。PlanRanker 现已成功部署在中国最大的在线旅行社之一的阿里巴巴，每天为数百万用户提供火车票预订服务。在两个生产数据集上的离线实验和在 Fliggy 进行的国家级在线 A/B 测试都证明了提议的 PlanRanker 比基线更优越。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PlanRanker:+Towards+Personalized+Ranking+of+Train+Transfer+Plans)|0|
|[Multi-factor Sequential Re-ranking with Perception-Aware Diversification](https://doi.org/10.1145/3580305.3599869)|Yue Xu, Hao Chen, Zefan Wang, Jianwen Yin, Qijie Shen, Dimin Wang, Feiran Huang, Lixiang Lai, Tao Zhuang, Junfeng Ge, Xia Hu|Alibaba Group; Jinan University; Rice University; The Hong Kong Polytechnic University|Feed recommendation systems, which recommend a sequence of items for users to browse and interact with, have gained significant popularity in practical applications. In feed products, users tend to browse a large number of items in succession, so the previously viewed items have a significant impact on users' behavior towards the following items. Therefore, traditional methods that mainly focus on improving the accuracy of recommended items are suboptimal for feed recommendations because they may recommend highly similar items. For feed recommendation, it is crucial to consider both the accuracy and diversity of the recommended item sequences in order to satisfy users' evolving interest when consecutively viewing items. To this end, this work proposes a general re-ranking framework named Multi-factor Sequential Re-ranking with Perception-Aware Diversification (MPAD) to jointly optimize accuracy and diversity for feed recommendation in a sequential manner. Specifically, MPAD first extracts users' different scales of interests from their behavior sequences through graph clustering-based aggregations. Then, MPAD proposes two sub-models to respectively evaluate the accuracy and diversity of a given item by capturing users' evolving interest due to the ever-changing context and users' personal perception of diversity from an item sequence perspective. This is consistent with the browsing nature of the feed scenario. Finally, MPAD generates the return list by sequentially selecting optimal items from the candidate set to maximize the joint benefits of accuracy and diversity of the entire list. MPAD has been implemented in Taobao's homepage feed to serve the main traffic and provide services to recommend billions of items to hundreds of millions of users every day.|提要推荐系统为用户推荐了一系列可供浏览和交互的条目，在实际应用中得到了广泛的应用。在提要产品中，用户倾向于连续浏览大量条目，因此以前查看的条目对用户对下列条目的行为有显著影响。因此，主要侧重于提高推荐项目准确性的传统方法对于饲料推荐是次优的，因为它们可能推荐高度相似的项目。为了满足用户在连续查看条目时不断变化的兴趣，对推荐条目序列的准确性和多样性进行考虑是至关重要的。为此，本文提出了一种基于感知多样化的多因素序贯推荐(MPAD)的通用推荐框架，该框架以序贯方式对推荐的准确性和多样性进行联合优化。具体来说，MPAD 首先通过基于图聚类的聚合从用户的行为序列中提取出用户不同尺度的兴趣。然后，MPAD 提出了两个子模型，分别从项目序列的角度通过捕获不断变化的用户兴趣和用户个人对多样性的感知来评价项目的准确性和多样性。这与提要场景的浏览特性一致。最后，MPAD 通过从候选集中依次选择最优项目来生成返回列表，以最大限度地提高整个列表的准确性和多样性。MPAD 已经在淘宝网的主页 feed 中实现，为主流流量提供服务，每天向数亿用户推荐数十亿个项目。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-factor+Sequential+Re-ranking+with+Perception-Aware+Diversification)|0|
|[TWIN: TWo-stage Interest Network for Lifelong User Behavior Modeling in CTR Prediction at Kuaishou](https://doi.org/10.1145/3580305.3599922)|Jianxin Chang, Chenbin Zhang, Zhiyi Fu, Xiaoxue Zang, Lin Guan, Jing Lu, Yiqun Hui, Dewei Leng, Yanan Niu, Yang Song, Kun Gai|Kuaishou Technology; Unaffiliated|Life-long user behavior modeling, i.e., extracting a user's hidden interests from rich historical behaviors in months or even years, plays a central role in modern CTR prediction systems. Conventional algorithms mostly follow two cascading stages: a simple General Search Unit (GSU) for fast and coarse search over tens of thousands of long-term behaviors and an Exact Search Unit (ESU) for effective Target Attention (TA) over the small number of finalists from GSU. Although efficient, existing algorithms mostly suffer from a crucial limitation: the \textit{inconsistent} target-behavior relevance metrics between GSU and ESU. As a result, their GSU usually misses highly relevant behaviors but retrieves ones considered irrelevant by ESU. In such case, the TA in ESU, no matter how attention is allocated, mostly deviates from the real user interests and thus degrades the overall CTR prediction accuracy. To address such inconsistency, we propose \textbf{TWo-stage Interest Network (TWIN)}, where our Consistency-Preserved GSU (CP-GSU) adopts the identical target-behavior relevance metric as the TA in ESU, making the two stages twins. Specifically, to break TA's computational bottleneck and extend it from ESU to GSU, or namely from behavior length $10^2$ to length $10^4-10^5$, we build a novel attention mechanism by behavior feature splitting. For the video inherent features of a behavior, we calculate their linear projection by efficient pre-computing \& caching strategies. And for the user-item cross features, we compress each into a one-dimentional bias term in the attention score calculation to save the computational cost. The consistency between two stages, together with the effective TA-based relevance metric in CP-GSU, contributes to significant performance gain in CTR prediction.|终身用户行为建模，即在数月甚至数年内从丰富的历史行为中提取用户隐藏的兴趣，在现代 CTR 预测系统中起着核心作用。传统的算法大多遵循两个级联阶段: 一个简单的通用搜索单元(GSU)用于快速和粗略搜索成千上万的长期行为和一个精确搜索单元(ESU)用于有效的目标注意(TA)在少数决赛选手从 GSU。虽然有效，但现有的算法大多受到一个关键的限制: 文本{不一致}目标行为相关度量 GSU 和 ESU 之间。因此，他们的 GSU 通常会错过高度相关的行为，但检索被 ESU 认为无关的行为。在这种情况下，ESU 中的 TA，无论如何分配注意力，大多偏离了真实用户的兴趣，从而降低了整体 CTR 预测的准确性。为了解决这种不一致性，我们提出 textbf { TWo-stage Interest Network (TWIN)} ，其中我们的保持一致性的 GSU (CP-GSU)采用与 ESU 中的 TA 相同的目标行为相关度量，使两个阶段成为孪生的。具体来说，为了打破 TA 的计算瓶颈，将其从 ESU 扩展到 GSU，或者说从行为长度 $10 ^ 2 $扩展到长度 $10 ^ 4-10 ^ 5 $，我们通过行为特征分裂构建了一种新的注意机制。对于视频行为的固有特征，我们通过有效的预计算和缓存策略来计算它们的线性投影。对于用户-项目交叉特征，在注意得分计算中将每个特征压缩为一维偏差项，以节省计算成本。两个阶段之间的一致性，加上 CP-GSU 中有效的基于 TA 的相关度量，有助于提高 CTR 预测的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TWIN:+TWo-stage+Interest+Network+for+Lifelong+User+Behavior+Modeling+in+CTR+Prediction+at+Kuaishou)|0|
|[On Manipulating Signals of User-Item Graph: A Jacobi Polynomial-based Graph Collaborative Filtering](https://doi.org/10.1145/3580305.3599450)|Jiayan Guo, Lun Du, Xu Chen, Xiaojun Ma, Qiang Fu, Shi Han, Dongmei Zhang, Yan Zhang|Peking University; Microsoft|Collaborative filtering (CF) is an important research direction in recommender systems that aims to make recommendations given the information on user-item interactions. Graph CF has attracted more and more attention in recent years due to its effectiveness in leveraging high-order information in the user-item bipartite graph for better recommendations. Specifically, recent studies show the success of graph neural networks (GNN) for CF is attributed to its low-pass filtering effects. However, current researches lack a study of how different signal components contributes to recommendations, and how to design strategies to properly use them well. To this end, from the view of spectral transformation, we analyze the important factors that a graph filter should consider to achieve better performance. Based on the discoveries, we design JGCF, an efficient and effective method for CF based on Jacobi polynomial bases and frequency decomposition strategies. Extensive experiments on four widely used public datasets show the effectiveness and efficiency of the proposed methods, which brings at most 27.06% performance gain on Alibaba-iFashion. Besides, the experimental results also show that JGCF is better at handling sparse datasets, which shows potential in making recommendations for cold-start users.|协同过滤(CF)是推荐系统的一个重要研究方向，其目的是根据用户项目交互的信息提供推荐。近年来，Graph CF 由于能够有效地利用用户-项目双向图中的高阶信息来获得更好的建议而引起了越来越多的关注。具体来说，最近的研究表明，图神经网络(GNN)对 CF 的成功归功于其低通滤波效果。然而，目前的研究缺乏研究不同的信号成分如何有助于推荐，以及如何设计策略，以适当地使用它们。为此，本文从谱变换的角度出发，分析了图形滤波器要获得更好的性能所应考虑的重要因素。基于这些发现，我们设计了一种基于 Jacobi 多项式基和频率分解策略的高效率和有效的协同过滤方法。在四个广泛使用的公共数据集上进行的大量实验表明了该方法的有效性和效率，在阿里巴巴-iFashion 平台上最多获得27.06% 的性能增益。此外，实验结果还表明，JGCF 在处理稀疏数据集方面有较好的表现，可以为冷启动用户提供建议。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Manipulating+Signals+of+User-Item+Graph:+A+Jacobi+Polynomial-based+Graph+Collaborative+Filtering)|0|
|[Off-Policy Evaluation of Ranking Policies under Diverse User Behavior](https://doi.org/10.1145/3580305.3599447)|Haruka Kiyohara, Masatoshi Uehara, Yusuke Narita, Nobuyuki Shimizu, Yasuo Yamamoto, Yuta Saito|Yale University; Cornell University; Yahoo Japan Corporation; Hanjuku-Kaso Co., Ltd.|Ranking interfaces are everywhere in online platforms. There is thus an ever growing interest in their Off-Policy Evaluation (OPE), aiming towards an accurate performance evaluation of ranking policies using logged data. A de-facto approach for OPE is Inverse Propensity Scoring (IPS), which provides an unbiased and consistent value estimate. However, it becomes extremely inaccurate in the ranking setup due to its high variance under large action spaces. To deal with this problem, previous studies assume either independent or cascade user behavior, resulting in some ranking versions of IPS. While these estimators are somewhat effective in reducing the variance, all existing estimators apply a single universal assumption to every user, causing excessive bias and variance. Therefore, this work explores a far more general formulation where user behavior is diverse and can vary depending on the user context. We show that the resulting estimator, which we call Adaptive IPS (AIPS), can be unbiased under any complex user behavior. Moreover, AIPS achieves the minimum variance among all unbiased estimators based on IPS. We further develop a procedure to identify the appropriate user behavior model to minimize the mean squared error (MSE) of AIPS in a data-driven fashion. Extensive experiments demonstrate that the empirical accuracy improvement can be significant, enabling effective OPE of ranking systems even under diverse user behavior.|在线平台中，排序界面无处不在。因此，人们对非策略评估(OPE)越来越感兴趣，其目标是使用日志数据对策略进行准确的性能评估。OPE 的一个事实上的方法是反倾向评分(IPS) ，它提供了一个无偏和一致的价值估计。然而，它变得非常不准确的排名设置，由于其高方差下的大行动空间。为了解决这个问题，以前的研究假设独立或级联用户行为，导致一些排名版本的 IPS。虽然这些估计量在减少方差方面有一定的效果，但是所有现有的估计量都对每个用户适用一个统一的假设，从而导致过度的偏差和方差。因此，这项工作探索了一个更一般的公式，其中用户行为是多样的，可以根据用户上下文而变化。我们证明了所得到的估计量，我们称之为自适应 IPS (AIPS) ，在任何复杂的用户行为下都是无偏的。此外，AIPS 在所有基于 IPS 的无偏估计量之间实现了最小方差。我们进一步开发了一个程序，以确定适当的用户行为模型，从而以数据驱动的方式最大限度地减少 AIPS 的均方差。大量的实验表明，经验的准确性改善可以是显着的，使有效的排名系统的 OPE 即使在不同的用户行为。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Off-Policy+Evaluation+of+Ranking+Policies+under+Diverse+User+Behavior)|0|
|[Impatient Bandits: Optimizing Recommendations for the Long-Term Without Delay](https://doi.org/10.1145/3580305.3599386)|Thomas M. McDonald, Lucas Maystre, Mounia Lalmas, Daniel Russo, Kamil Ciosek|; University of Manchester; Photon Spot (United States)|Recommender systems are a ubiquitous feature of online platforms. Increasingly, they are explicitly tasked with increasing users' long-term satisfaction. In this context, we study a content exploration task, which we formalize as a multi-armed bandit problem with delayed rewards. We observe that there is an apparent trade-off in choosing the learning signal: Waiting for the full reward to become available might take several weeks, hurting the rate at which learning happens, whereas measuring short-term proxy rewards reflects the actual long-term goal only imperfectly. We address this challenge in two steps. First, we develop a predictive model of delayed rewards that incorporates all information obtained to date. Full observations as well as partial (short or medium-term) outcomes are combined through a Bayesian filter to obtain a probabilistic belief. Second, we devise a bandit algorithm that takes advantage of this new predictive model. The algorithm quickly learns to identify content aligned with long-term success by carefully balancing exploration and exploitation. We apply our approach to a podcast recommendation problem, where we seek to identify shows that users engage with repeatedly over two months. We empirically validate that our approach results in substantially better performance compared to approaches that either optimize for short-term proxies, or wait for the long-term outcome to be fully realized.|推荐系统是在线平台的一个普遍特征。他们越来越明确地被赋予了提高用户长期满意度的任务。在这个背景下，我们研究了一个内容探索任务，我们将其形式化为一个延迟奖励的多臂老虎机问题。我们观察到，在选择学习信号时有一个明显的权衡: 等待完整的奖励变得可用可能需要几个星期，损害了学习发生的速度，而衡量短期代理奖励只能不完美地反映实际的长期目标。我们用两个步骤来应对这个挑战。首先，我们开发了一个延迟奖励的预测模型，其中包含了迄今为止获得的所有信息。通过贝叶斯滤波器将全部观测结果以及部分(短期或中期)结果组合起来，得到概率信念。其次，我们设计了一个土匪算法，利用这个新的预测模型的优势。该算法通过仔细平衡探索和开发，很快学会识别与长期成功相一致的内容。我们将我们的方法应用于播客推荐问题，我们试图确定用户在两个月内反复参与的节目。我们经验性地验证了我们的方法与那些为短期代理优化或者等待长期结果完全实现的方法相比，能够获得更好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Impatient+Bandits:+Optimizing+Recommendations+for+the+Long-Term+Without+Delay)|0|
|[Unbiased Delayed Feedback Label Correction for Conversion Rate Prediction](https://doi.org/10.1145/3580305.3599536)|Yifan Wang, Peijie Sun, Min Zhang, Qinglin Jia, Jingjie Li, Shaoping Ma|Noah’s Ark Lab, Huawei; Tsinghua University|Conversion rate prediction is critical to many online applications such as digital display advertising. To capture dynamic data distribution, industrial systems often require retraining models on recent data daily or weekly. However, the delay of conversion behavior usually leads to incorrect labeling, which is called delayed feedback problem. Existing work may fail to introduce the correct information about false negative samples due to data sparsity and dynamic data distribution. To directly introduce the correct feedback label information, we propose an Unbiased delayed feedback Label Correction framework (ULC), which uses an auxiliary model to correct labels for observed negative feedback samples. Firstly, we theoretically prove that the label-corrected loss is an unbiased estimate of the oracle loss using true labels. Then, as there are no ready training data for label correction, counterfactual labeling is used to construct artificial training data. Furthermore, since counterfactual labeling utilizes only partial training data, we design an embedding-based alternative training method to enhance performance. Comparative experiments on both public and private datasets and detailed analyses show that our proposed approach effectively alleviates the delayed feedback problem and consistently outperforms the previous state-of-the-art methods.|转化率预测是许多在线应用程序，如数字显示广告的关键。为了捕获动态数据分布，工业系统通常需要每天或每周对最近的数据进行再训练。然而，转换行为的延迟通常会导致不正确的标记，这就是所谓的延迟反馈问题。由于数据稀疏和数据分布的动态性，现有的工作可能无法引入正确的假阴性样本信息。为了直接引入正确的反馈标签信息，我们提出了一种无偏的延迟反馈标签校正框架(ULC) ，它使用一个辅助模型对观测到的负反馈样本进行标签校正。首先，我们从理论上证明了标签校正损失是使用真实标签对甲骨文损失进行的无偏估计。然后，由于没有现成的训练数据用于标签校正，采用反事实标注来构造人工训练数据。此外，由于反事实标注只利用部分训练数据，我们设计了一个基于嵌入的替代训练方法来提高性能。对公共和私人数据集的比较实验和详细的分析表明，我们提出的方法有效地缓解了延迟反馈问题，并始终优于以前的最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unbiased+Delayed+Feedback+Label+Correction+for+Conversion+Rate+Prediction)|0|
|[PrefRec: Recommender Systems with Human Preferences for Reinforcing Long-term User Engagement](https://doi.org/10.1145/3580305.3599473)|Wanqi Xue, Qingpeng Cai, Zhenghai Xue, Shuo Sun, Shuchang Liu, Dong Zheng, Peng Jiang, Kun Gai, Bo An||Current advances in recommender systems have been remarkably successful in optimizing immediate engagement. However, long-term user engagement, a more desirable performance metric, remains difficult to improve. Meanwhile, recent reinforcement learning (RL) algorithms have shown their effectiveness in a variety of long-term goal optimization tasks. For this reason, RL is widely considered as a promising framework for optimizing long-term user engagement in recommendation. Though promising, the application of RL heavily relies on well-designed rewards, but designing rewards related to long-term user engagement is quite difficult. To mitigate the problem, we propose a novel paradigm, recommender systems with human preferences (or Preference-based Recommender systems), which allows RL recommender systems to learn from preferences about users' historical behaviors rather than explicitly defined rewards. Such preferences are easily accessible through techniques such as crowdsourcing, as they do not require any expert knowledge. With PrefRec, we can fully exploit the advantages of RL in optimizing long-term goals, while avoiding complex reward engineering. PrefRec uses the preferences to automatically train a reward function in an end-to-end manner. The reward function is then used to generate learning signals to train the recommendation policy. Furthermore, we design an effective optimization method for PrefRec, which uses an additional value function, expectile regression and reward model pre-training to improve the performance. We conduct experiments on a variety of long-term user engagement optimization tasks. The results show that PrefRec significantly outperforms previous state-of-the-art methods in all the tasks.|目前在推荐系统方面取得的进展在优化即时接触方面取得了显著的成功。然而，长期用户参与(一个更理想的性能指标)仍然难以改进。与此同时，最近的强化学习算法已经证明了它们在各种长期目标优化任务中的有效性。由于这个原因，RL 被广泛认为是一个优化长期用户参与推荐的有前途的框架。尽管 RL 的应用前景看好，但它严重依赖于精心设计的奖励，但设计与长期用户参与相关的奖励却相当困难。为了缓解这一问题，我们提出了一种新的范式，即具有人类偏好的推荐系统(或基于偏好的推荐系统) ，它允许 RL 推荐系统从用户的历史行为偏好中学习，而不是明确定义奖励。这些偏好通过诸如众包等技术很容易获得，因为它们不需要任何专业知识。使用 PrefRec，我们可以充分发挥 RL 在优化长期目标方面的优势，同时避免复杂的报酬工程。PrefRec 使用首选项以端到端的方式自动训练奖励函数。然后利用奖励函数生成学习信号对推荐策略进行训练。在此基础上，设计了一种有效的 PrefRec 优化方法，该方法利用附加值函数、期望回归和奖励模型预训练来提高系统的性能。我们对各种长期的用户参与优化任务进行了实验。结果表明，PrefRec 在所有任务中都明显优于以前的最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PrefRec:+Recommender+Systems+with+Human+Preferences+for+Reinforcing+Long-term+User+Engagement)|0|
|[Sequence As Genes: An User Behavior Modeling Framework for Fraud Transaction Detection in E-commerce](https://doi.org/10.1145/3580305.3599905)|Ziming Wang, Qianru Wu, Baolin Zheng, Junjie Wang, Kaiyu Huang, Yanjie Shi||With the explosive growth of e-commerce, detecting fraudulent transactions in real-world scenarios is becoming increasingly important for e-commerce platforms. Recently, several supervised approaches have been proposed to use user behavior sequences, which record the user's track on platforms and contain rich information for fraud transaction detection. Nevertheless, these methods always suffer from the scarcity of labeled data in real-world scenarios. The recent remarkable pre-training methods in Natural Language Processing (NLP) and Computer Vision (CV) domains offered glimmers of light. However, user behavior sequences differ intrinsically from text, images, and videos. In this paper, we propose a novel and general user behavior pre-training framework, named Sequence As GEnes (SAGE), which provides a new perspective for user behavior modeling. Following the inspiration of treating sequences as genes, we carefully designed the user behavior data organization paradigm and pre-training scheme. Specifically, we propose an efficient data organization paradigm inspired by the nature of DNA expression, which decouples the length of behavior sequences and the corresponding time spans. Also inspired by the natural mechanisms in genetics, we propose two pre-training tasks, namely sequential mutation and sequential recombination, to improve the robustness and consistency of user behavior representations in complicated real-world scenes. Extensive experiments on four differentiated fraud transaction detection real scenarios demonstrate the effectiveness of our proposed framework.|随着电子商务的爆炸式增长，在现实世界中检测欺诈交易对于电子商务平台变得越来越重要。最近，人们提出了几种监督方法来利用用户行为序列来记录用户在平台上的行为轨迹，并包含丰富的欺诈交易检测信息。然而，这些方法总是受到缺乏标记的数据在现实世界的情况下。最近在自然语言处理(NLP)和计算机视觉(CV)领域显著的预训练方法提供了一线曙光。然而，用户行为序列与文本、图像和视频本质上是不同的。本文提出了一种新的通用的用户行为预训练框架，即序列作为基因(SAGE) ，为用户行为建模提供了一个新的视角。在将序列视为基因的启示下，我们精心设计了用户行为数据组织范式和预训练方案。具体来说，我们提出了一种受 DNA 表达本质启发的高效数据组织范式，它解耦了行为序列的长度和相应的时间跨度。受遗传学中自然机制的启发，我们提出了两个预训练任务，即序列变异和序列重组，以提高用户行为表征在复杂现实场景中的鲁棒性和一致性。在四种不同的欺诈交易检测实际场景中进行的大量实验证明了我们提出的框架的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sequence+As+Genes:+An+User+Behavior+Modeling+Framework+for+Fraud+Transaction+Detection+in+E-commerce)|0|
|[TransAct: Transformer-based Realtime User Action Model for Recommendation at Pinterest](https://doi.org/10.1145/3580305.3599918)|Xue Xia, Pong Eksombatchai, Nikil Pancha, Dhruvil Deven Badani, PoWei Wang, Neng Gu, Saurabh Vishwas Joshi, Nazanin Farahpour, Zhiyuan Zhang, Andrew Zhai|Pinterest|Sequential models that encode user activity for next action prediction have become a popular design choice for building web-scale personalized recommendation systems. Traditional methods of sequential recommendation either utilize end-to-end learning on realtime user actions, or learn user representations separately in an offline batch-generated manner. This paper (1) presents Pinterest's ranking architecture for Homefeed, our personalized recommendation product and the largest engagement surface; (2) proposes TransAct, a sequential model that extracts users' short-term preferences from their realtime activities; (3) describes our hybrid approach to ranking, which combines end-to-end sequential modeling via TransAct with batch-generated user embeddings. The hybrid approach allows us to combine the advantages of responsiveness from learning directly on realtime user activity with the cost-effectiveness of batch user representations learned over a longer time period. We describe the results of ablation studies, the challenges we faced during productionization, and the outcome of an online A/B experiment, which validates the effectiveness of our hybrid ranking model. We further demonstrate the effectiveness of TransAct on other surfaces such as contextual recommendations and search. Our model has been deployed to production in Homefeed, Related Pins, Notifications, and Search at Pinterest.|为下一步行动预测编码用户活动的序列模型已成为建立网络规模个性化推荐系统的流行设计选择。传统的顺序推荐方法要么利用实时用户操作的端到端学习，要么以离线批量生成的方式单独学习用户表示。本文(1)介绍了 Pinterest 针对 Homefeed 的排名体系结构，这是我们的个性化推荐产品，也是最大的参与表面; (2)提出了 TransAct，一个从用户的实时活动中提取用户短期偏好的顺序模型; (3)描述了我们的混合排名方法，它结合了通过 TransAct 的端到端顺序建模和批量生成的用户嵌入。这种混合方法使我们能够将直接学习实时用户活动的响应性优势与长期学习的批量用户表示的成本效益结合起来。我们描述了烧蚀研究的结果，我们在生产过程中面临的挑战，以及一个在线 A/B 实验的结果，它验证了我们的混合排序模型的有效性。我们进一步展示了 TransAct 在上下文推荐和搜索等其他表面上的有效性。我们的模型已经部署到 Homefeed 的生产环境中，相关的 Pins，通知，和在 Pinterest 上的搜索。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TransAct:+Transformer-based+Realtime+User+Action+Model+for+Recommendation+at+Pinterest)|0|
|[A Personalized Automated Bidding Framework for Fairness-aware Online Advertising](https://doi.org/10.1145/3580305.3599765)|Haoqi Zhang, Lvyin Niu, Zhenzhe Zheng, Zhilin Zhang, Shan Gu, Fan Wu, Chuan Yu, Jian Xu, Guihai Chen, Bo Zheng||Powered by machine learning techniques, online advertising platforms have launched various automated bidding strategy services to facilitate intelligent decision-making for advertisers. However, advertisers experience heterogeneous advertising environments, and thus the unified bidding strategies widely used in both academia and industry suffer from severe unfairness issues, resulting in significant ad performance disparity among advertisers. In this work, to resolve the unfairness issue and improve the overall system performance, we propose a personalized automated bidding framework, namely PerBid, shifting the classical automated bidding strategy with a unified agent to multiple context-aware agents corresponding to different advertiser clusters. Specifically, we first design an ad campaign profiling network to model dynamic advertising environments. By clustering the advertisers with similar profiles and generating context-aware automated bidding agents for each cluster, we can match advertisers with personalized automated bidding strategies. Experiments conducted on the real-world dataset and online A/B test on Alibaba display advertising platform demonstrate the effectiveness of PerBid in improving overall ad performance and guaranteeing fairness among heterogeneous advertisers.|在机器学习技术的推动下，在线广告平台推出了各种自动化投标策略服务，以促进广告商的智能决策。然而，广告主所处的广告环境是异质的，因此学术界和业界广泛采用的统一投标策略存在着严重的不公平问题，造成了广告主之间广告绩效的显著差异。为了解决不公平问题，提高系统的整体性能，本文提出了一个个性化的自动投标框架 PerBid，将传统的统一代理的自动投标策略转化为对应于不同广告客户集群的多上下文感知代理的自动投标策略。具体来说，我们首先设计一个广告活动分析网络来模拟动态的广告环境。通过对具有相似特征的广告主进行聚类，并为每个聚类生成上下文感知的自动投标代理，我们可以将广告主与个性化的自动投标策略进行匹配。在阿里巴巴展示广告平台上对现实世界的数据集和在线 A/B 测试进行的实验表明，perBid 在提高整体广告效果和保证不同广告商之间的公平性方面是有效的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Personalized+Automated+Bidding+Framework+for+Fairness-aware+Online+Advertising)|0|
|[Privacy Matters: Vertical Federated Linear Contextual Bandits for Privacy Protected Recommendation](https://doi.org/10.1145/3580305.3599475)|Zeyu Cao, Zhipeng Liang, Bingzhe Wu, Shu Zhang, Hangyu Li, Ouyang Wen, Yu Rong, Peilin Zhao||Recent awareness of privacy protection and compliance requirement resulted in a controversial view of recommendation system due to personal data usage. Therefore, privacy-protected recommendation emerges as a novel research direction. In this paper, we first formulate this problem as a vertical federated learning problem, i.e., features are vertically distributed over different departments. We study a contextual bandit learning problem for recommendation in the vertical federated setting. To this end, we carefully design a customized encryption scheme named orthogonal matrix-based mask mechanism (O3M). O3M mechanism, a tailored component for contextual bandits by carefully exploiting their shared structure, can ensure privacy protection while avoiding expensive conventional cryptographic techniques. We further apply the mechanism to two commonly-used bandit algorithms, LinUCB and LinTS, and instantiate two practical protocols for online recommendation. The proposed protocols can perfectly recover the service quality of centralized bandit algorithms while achieving a satisfactory runtime efficiency, which is theoretically proved and analysed in this paper. By conducting extensive experiments on both synthetic and real-world datasets, we show the superiority of the proposed method in terms of privacy protection and recommendation performance.|由于个人数据的使用，最近对隐私保护和遵守要求的认识导致对推荐系统的看法存在争议。因此，隐私保护推荐成为一个新的研究方向。在本文中，我们首先将这个问题表述为一个垂直联合学习问题，即特征在不同部门之间垂直分布。本文研究了垂直联邦环境下的上下文强盗学习推荐问题。为此，我们精心设计了一个定制的加密方案——基于正交矩阵的掩码机制(O3M)。O3M 机制是一种为上下文盗版者量身定制的组件，通过仔细利用它们的共享结构，可以在避免昂贵的传统密码技术的同时确保隐私保护。我们进一步将该机制应用于两个常用的盗贼算法 LinUCB 和 LinTS，并实例化了两个实用的在线推荐协议。本文从理论上证明和分析了所提出的协议能够很好地恢复集中式抢劫算法的服务质量，同时获得满意的运行效率。通过对合成数据集和真实数据集的大量实验，我们证明了该方法在隐私保护和推荐性能方面的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Privacy+Matters:+Vertical+Federated+Linear+Contextual+Bandits+for+Privacy+Protected+Recommendation)|0|
|[Approximation Algorithms for Size-Constrained Non-Monotone Submodular Maximization in Deterministic Linear Time](https://doi.org/10.1145/3580305.3599259)|Yixin Chen, Alan Kuhnle||In this work, we study the problem of finding the maximum value of a non-negative submodular function subject to a limit on the number of items selected, a ubiquitous problem that appears in many applications, such as data summarization and nonlinear regression. We provide the first deterministic, linear-time approximation algorithms for this problem that do not assume the objective is monotone. We present three deterministic, linear-time algorithms: a single-pass streaming algorithm with a ratio of 23.313 + ε, which is the first linear-time streaming algorithm; a simpler deterministic linear-time algorithm with a ratio of 11.657; and a (4 + O(ε))-approximation algorithm. Finally, we present a deterministic algorithm that obtains ratio of e + ε in O_ε (n log(n)) time, close to the best known expected ratio of e - 0.121 in polynomial time.|在这项工作中，我们研究的问题是找到一个非负子模函数的最大值受到选择的项目数量的限制，这是一个普遍存在的问题，在许多应用中出现，如数据汇总和非线性回归。我们提供了这个问题的第一个确定性，线性时间近似算法，不假设目标是单调的。我们提出了三个确定性的线性时间算法: 一个比率为23.313 + ε 的单通道流式算法，这是第一个线性时间流式算法; 一个比率为11.657的简单的确定性线性时间算法; 和一个(4 + o (ε))-近似演算法。最后，我们给出了一个确定性算法，在 O _ ε (n log (n))时间内得到 e + ε 的比率，接近于多项式时间内 e-0.121的最佳期望比率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Approximation+Algorithms+for+Size-Constrained+Non-Monotone+Submodular+Maximization+in+Deterministic+Linear+Time)|0|
|[Constraint-aware and Ranking-distilled Token Pruning for Efficient Transformer Inference](https://doi.org/10.1145/3580305.3599284)|Junyan Li, Li Lyna Zhang, Jiahang Xu, Yujing Wang, Shaoguang Yan, Yunqing Xia, Yuqing Yang, Ting Cao, Hao Sun, Weiwei Deng, Qi Zhang, Mao Yang|Microsoft Research; Microsoft; Zhejiang University|Deploying pre-trained transformer models like BERT on downstream tasks in resource-constrained scenarios is challenging due to their high inference cost, which grows rapidly with input sequence length. In this work, we propose a constraint-aware and ranking-distilled token pruning method ToP, which selectively removes unnecessary tokens as input sequence passes through layers, allowing the model to improve online inference speed while preserving accuracy. ToP overcomes the limitation of inaccurate token importance ranking in the conventional self-attention mechanism through a ranking-distilled token distillation technique, which distills effective token rankings from the final layer of unpruned models to early layers of pruned models. Then, ToP introduces a coarse-to-fine pruning approach that automatically selects the optimal subset of transformer layers and optimizes token pruning decisions within these layers through improved $L_0$ regularization. Extensive experiments on GLUE benchmark and SQuAD tasks demonstrate that ToP outperforms state-of-the-art token pruning and model compression methods with improved accuracy and speedups. ToP reduces the average FLOPs of BERT by 8.1x while achieving competitive accuracy on GLUE, and provides a real latency speedup of up to 7.4x on an Intel CPU.|在资源受限的情况下，在下游任务中部署像 BERT 这样的预先训练的变压器模型是具有挑战性的，因为它们的推理成本很高，并且随着输入序列长度的增长而迅速增长。本文提出了一种基于约束和排序的令牌剪枝方法 TOP，该方法在输入序列通过层的同时选择性地去除不必要的令牌，使模型在保持精度的同时提高了在线推理速度。TOP 通过排序-提取令牌精馏技术克服了传统自注意机制中不准确的令牌重要性排序的局限性，该技术将有效的令牌排序从未修剪模型的最后一层提取到修剪模型的早期层。然后，TOP 引入了一种从粗到精的剪枝方法，该方法自动选择变压器层的最优子集，并通过改进的 $L _ 0 $正则化来优化这些变压器层内的令牌剪枝决策。在 GLUE 基准测试和 SQuAD 任务上的大量实验表明，ToP 优于最先进的令牌剪枝和模型压缩方法，具有更高的准确性和加速性。在 GLUE 上，TOP 减少了 BERT 的平均 FLOP 8.1 x，同时实现了具有竞争力的准确性，并且在 Intel CPU 上提供了高达7.4 x 的实际延迟加速。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Constraint-aware+and+Ranking-distilled+Token+Pruning+for+Efficient+Transformer+Inference)|0|
|[Learning Balanced Tree Indexes for Large-Scale Vector Retrieval](https://doi.org/10.1145/3580305.3599406)|Wuchao Li, Chao Feng, Defu Lian, Yuxin Xie, Haifeng Liu, Yong Ge, Enhong Chen|University of Science and Technology of China; Guangdong OPPO Mobile Telecommunications Corp., Ltd; University of Arizona|Vector retrieval focuses on finding the k-nearest neighbors from a bunch of data points, and is widely used in a diverse set of areas such as information retrieval and recommender system. The current state-of-the-art methods represented by HNSW usually generate indexes with a big memory footprint, restricting the scale of data they can handle, except resorting to a hybrid index with external storage. The space-partitioning learned indexes, which only occupy a small memory, have made great breakthroughs in recent years. However, these methods rely on a large amount of labeled data for supervised learning, so model complexity affects the generalization. To this end, we propose a lightweight learnable hierarchical space partitioning index based on a balanced K-ary tree, called BAlanced Tree Learner (BATL), where the same bucket of data points are represented by a path from the root to the corresponding leaf. Instead of mapping each query into a bucket, BATL classifies it into a sequence of branches (i.e. a path), which drastically reduces the number of classes and potentially improves generalization. BATL updates the classifier and the balanced tree in an alternating way. When updating the classifier, we innovatively leverage the sequence-to-sequence learning paradigm for learning to route each query into the ground-truth leaf on the balanced tree. Retrieval is then boiled down into a sequence (i.e. path) generation task, which can be simply achieved by beam search on the encoder-decoder. When updating a balanced tree, we apply the classifier for navigating each data point into the tree nodes layer by layer under the balance constraints. We finally evaluate BATL with several large-scale vector datasets, where the experimental results show the superiority of the proposed method to the SOTA baselines in the tradeoff among latency, accuracy, and memory cost.|矢量检索主要从一系列数据点中寻找 k 个最近的邻居，广泛应用于信息检索和推荐系统等不同的领域。以 HNSW 为代表的当前最先进的方法通常生成内存占用量很大的索引，这限制了它们可以处理的数据规模，只能求助于带有外部存储的混合索引。空间划分学习指标只占用很小的内存，近年来取得了很大的突破。然而，这些方法依赖于大量的标记数据作为监督式学习，所以模型的复杂性会影响泛化。为此，我们提出了一个轻量级的可学习的层次化空间分割索引，它基于一个平衡的 k 元树，称为平衡树学习器(balancedtree Learner，BATL) ，其中相同的一桶数据点由一条从根到相应叶子的路径表示。BATL 没有将每个查询映射到一个 bucket 中，而是将其分类到一个分支序列(即路径)中，这极大地减少了类的数量，并有可能提高泛化能力。BATL 以交替的方式更新分类器和平衡树。在更新分类器时，我们创新地利用序列到序列学习范式来学习将每个查询路由到平衡树上的地面真相叶子。然后，检索被归结为一个序列(即路径)生成任务，这可以简单地通过在编码器-解码器上进行波束搜索来实现。在更新平衡树时，我们应用分类器在平衡约束下将每个数据点逐层导航到树节点。最后利用几个大规模矢量数据集对 BATL 进行了评估，实验结果表明该方法在时延、准确性和内存消耗等方面优于 SOTA 基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Balanced+Tree+Indexes+for+Large-Scale+Vector+Retrieval)|0|
|[Generative Flow Network for Listwise Recommendation](https://doi.org/10.1145/3580305.3599364)|Shuchang Liu, Qingpeng Cai, Zhankui He, Bowen Sun, Julian J. McAuley, Dong Zheng, Peng Jiang, Kun Gai|Peking University; Kuaishou Technology; University of California, San Diego; Unaffiliated|Personalized recommender systems fulfill the daily demands of customers and boost online businesses. The goal is to learn a policy that can generate a list of items that matches the user's demand or interest. While most existing methods learn a pointwise scoring model that predicts the ranking score of each individual item, recent research shows that the listwise approach can further improve the recommendation quality by modeling the intra-list correlations of items that are exposed together. This has motivated the recent list reranking and generative recommendation approaches that optimize the overall utility of the entire list. However, it is challenging to explore the combinatorial space of list actions and existing methods that use cross-entropy loss may suffer from low diversity issues. In this work, we aim to learn a policy that can generate sufficiently diverse item lists for users while maintaining high recommendation quality. The proposed solution, GFN4Rec, is a generative method that takes the insight of the flow network to ensure the alignment between list generation probability and its reward. The key advantages of our solution are the log scale reward matching loss that intrinsically improves the generation diversity and the autoregressive item selection model that captures the item mutual influences while capturing future reward of the list. As validation of our method's effectiveness and its superior diversity during active exploration, we conduct experiments on simulated online environments as well as an offline evaluation framework for two real-world datasets.|个性化推荐系统满足了客户的日常需求，促进了在线业务的发展。目标是学习一种策略，该策略可以生成符合用户需求或兴趣的项目列表。虽然大多数现有的方法学习点式评分模型，预测每个单独的项目的排名得分，最近的研究表明，列表式方法可以进一步提高推荐质量的建模内列表相关性的项目是暴露在一起。这推动了最近的名单重新排名和生成性建议方法，优化了整个名单的总体效用。然而，探索列表行为的组合空间是一个挑战，现有的使用交叉熵损失的方法可能会遇到低多样性问题。在这项工作中，我们的目标是学习一个政策，可以产生足够多样化的项目清单的用户，同时保持高质量的推荐。所提出的解决方案 GFN4Rec 是一种生成方法，它利用流网络的洞察力来确保列表生成概率与其报酬之间的一致性。该解决方案的主要优点是对数规模的奖励匹配损失，本质上改善了生成多样性和自回归项选择模型，捕捉项目的相互影响，同时捕捉未来的奖励列表。为了验证我们的方法在主动勘探过程中的有效性和优越的多样性，我们在模拟的在线环境中进行了实验，并对两个真实世界的数据集进行了离线评估框架。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+Flow+Network+for+Listwise+Recommendation)|0|
|[Hyper-USS: Answering Subset Query Over Multi-Attribute Data Stream](https://doi.org/10.1145/3580305.3599383)|Ruijie Miao, Yiyao Zhang, Guanyu Qu, Kaicheng Yang, Tong Yang, Bin Cui||Sketching algorithms are considered as promising solutions for answering approximate query on massive data stream. In real scenarios, a large number of problems can be abstracted as subset query over multiple attributes. Existing sketches are designed for query on single attributes, and therefore are inefficient for query on multiple attributes. In this work, we propose Hyper-USS, an innovative sketching algorithm that supports subset query over multiple attributes accurately and efficiently. To the best of our knowledge, this work is the first sketching algorithm designed to answer approximate query over multi-attribute data stream. We utilize the key technique, Joint Variance Optimization, to guarantee high estimation accuracy on all attributes. Experiment results show that, compared with the state-of-the-art (SOTA) sketches that support subset query on single attributes, Hyper-USS improves the accuracy by 16.67X and the throughput by 8.54X. The code is open-sourced at Github.|草图算法被认为是解决海量数据流中近似查询问题的有效方法。在实际场景中，大量的问题可以抽象为对多个属性的子集查询。现有的草图是为单个属性的查询而设计的，因此对于多个属性的查询效率很低。在这项工作中，我们提出了一个新颖的草图算法 Hyper-USS，它能够准确而有效地支持多属性的子集查询。据我们所知，本文是第一个针对多属性数据流的近似查询而设计的草图算法。我们利用关键技术，联合方差优化，以保证高估计精度的所有属性。实验结果表明，与支持单属性子集查询的 SOTA 草图相比，Hyper-USS 算法的精度提高了16.67 X，吞吐量提高了8.54 X。这些代码在 Github 是开源的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hyper-USS:+Answering+Subset+Query+Over+Multi-Attribute+Data+Stream)|0|
|[Reconsidering Learning Objectives in Unbiased Recommendation: A Distribution Shift Perspective](https://doi.org/10.1145/3580305.3599487)|Teng Xiao, Zhengyu Chen, Suhang Wang||This work studies the problem of learning unbiased algorithms from biased feedback for recommendation. We address this problem from a novel distribution shift perspective. Recent works in unbiased recommendation have advanced the state-of-the-art with various techniques such as re-weighting, multi-task learning, and meta-learning. Despite their empirical successes, most of them lack theoretical guarantees, forming non-negligible gaps between theories and recent algorithms. In this paper, we propose a theoretical understanding of why existing unbiased learning objectives work for unbiased recommendation. We establish a close connection between unbiased recommendation and distribution shift, which shows that existing unbiased learning objectives implicitly align biased training and unbiased test distributions. Built upon this connection, we develop two generalization bounds for existing unbiased learning methods and analyze their learning behavior. Besides, as a result of the distribution shift, we further propose a principled framework, Adversarial Self-Training (AST), for unbiased recommendation. Extensive experiments on real-world and semi-synthetic datasets demonstrate the effectiveness of AST.|本文研究了从有偏反馈中学习无偏推荐算法的问题。我们从一个新的分布转移的角度来解决这个问题。最近的作品在无偏见的推荐已经推进了国家的最先进的各种技术，如重新加权，多任务学习，元学习。尽管他们在经验上取得了成功，但大多数缺乏理论保证，在理论和最近的算法之间形成了不容忽视的差距。在本文中，我们提出了一个理论上的理解，为什么现有的无偏学习目标工作的无偏推荐。我们建立了无偏见的推荐和分布转移之间的密切联系，这表明现有的无偏见的学习目标隐含地调整有偏见的培训和无偏见的测试分布。在此基础上，我们对现有的无偏学习方法进行了两种推广，并分析了它们的学习行为。此外，由于分布转移的结果，我们进一步提出了一个原则性的框架，对抗性自我训练(AST) ，为无偏见的建议。在真实世界和半合成数据集上的大量实验证明了 AST 的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reconsidering+Learning+Objectives+in+Unbiased+Recommendation:+A+Distribution+Shift+Perspective)|0|
|[VQNE: Variational Quantum Network Embedding with Application to Network Alignment](https://doi.org/10.1145/3580305.3599542)|Xinyu Ye, Ge Yan, Junchi Yan|Shanghai Jiao Tong University|Learning of network embedding with vector-based node representation has attracted wide attention over the decade. It differs from the general setting of graph node embedding whereby the node attributes are also considered and yet may incur privacy issues. In this paper, we depart from the classic CPU/GPU architecture to consider the well-established network alignment problem based on network embedding, and develop a quantum machine learning approach with a low qubit cost for its near-future applicability on Noisy Intermediate-Scale Quantum (NISQ) devices. Specifically, our model adopts the discrete-time quantum walk (QW) and conducts the QW on the tailored merged network to extract structure information from the two aligning networks without the need for quantum state preparation which otherwise requires high quantum gate cost. Then the quantum states from QW are fed to a quantum embedding ansatz (i.e., parameterized circuit) to learn the latent representation of each node. The key part of our approach is to connect these two quantum modules to achieve a pure quantum paradigm without involving classical modules. To our best knowledge, there has not been any classic-quantum hybrid approach to network embedding, let alone a pure quantum paradigm being free from the bottleneck of communication between classic devices and quantum devices, which is still an open problem. Experimental results on two real-world datasets show the effectiveness of our quantum embedding approach in comparison with classical embedding approaches. Our model is readily and efficiently implemented in Python with a full-amplitude simulation of the QW and the quantum circuit. Therefore, our model can be readily deployed on an existing NISQ device with all the circuits provided, and only 13 qubits are needed in the experiments, which is rarely attained in existing quantum graph learning works.|基于矢量节点表示的网络嵌入学习近年来受到广泛关注。它不同于图节点嵌入的一般设置，即也考虑节点属性，但可能会引起隐私问题。本文从传统的 CPU/GPU 体系结构出发，考虑了基于网络嵌入的网络对准问题，提出了一种低量子比特代价的量子机器学习方法，并将其应用于噪声中尺度量子(NISQ)器件。具体地说，我们的模型采用离散时间量子行走(QW) ，在量子化的合并网络上进行量子行走，从两个对齐的网络中提取结构信息，而不需要进行量子态准备，否则需要很高的量子门开销。然后将量子态反馈给量子嵌入回路(即参数化电路) ，学习每个节点的潜在表示。我们的方法的关键部分是连接这两个量子模块，以实现一个纯量子范式，而不涉及经典模块。据我们所知，目前还没有任何经典的量子混合方法来实现网络嵌入，更不用说一个纯量子范式能够摆脱经典设备和量子设备之间通信的瓶颈，这仍然是一个悬而未决的问题。在两个实际数据集上的实验结果表明了量子嵌入方法与经典嵌入方法相比的有效性。我们的模型是容易和有效地实现在 Python 与全振幅模拟量子波和量子电路。因此，我们的模型可以很容易地部署在现有的 NISQ 器件上，所提供的所有电路，只需要13个量子位的实验，这是很少达到现有的量子图学习工作。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=VQNE:+Variational+Quantum+Network+Embedding+with+Application+to+Network+Alignment)|0|
|[Debiasing Recommendation by Learning Identifiable Latent Confounders](https://doi.org/10.1145/3580305.3599296)|Qing Zhang, Xiaoying Zhang, Yang Liu, Hongning Wang, Min Gao, Jiheng Zhang, Ruocheng Guo|University of Virginia; ByteDance Research; Chongqing University; Hong Kong University of Science and Technology|Recommendation systems aim to predict users' feedback on items not exposed to them. Confounding bias arises due to the presence of unmeasured variables (e.g., the socio-economic status of a user) that can affect both a user's exposure and feedback. Existing methods either (1) make untenable assumptions about these unmeasured variables or (2) directly infer latent confounders from users' exposure. However, they cannot guarantee the identification of counterfactual feedback, which can lead to biased predictions. In this work, we propose a novel method, i.e., identifiable deconfounder (iDCF), which leverages a set of proxy variables (e.g., observed user features) to resolve the aforementioned non-identification issue. The proposed iDCF is a general deconfounded recommendation framework that applies proximal causal inference to infer the unmeasured confounders and identify the counterfactual feedback with theoretical guarantees. Extensive experiments on various real-world and synthetic datasets verify the proposed method's effectiveness and robustness.|推荐系统旨在预测用户对未接触到的项目的反馈。由于存在不可测量的变量(例如，用户的社会经济地位) ，可以影响用户的曝光和反馈，混淆偏见就会产生。现有的方法要么(1)对这些未测量的变量做出不可靠的假设，要么(2)直接从用户的暴露中推断出潜在的混杂因素。然而，他们不能保证识别反事实反馈，这可能导致偏见的预测。在这项工作中，我们提出了一种新的方法，即可识别的解构者(iDCF) ，它利用一组代理变量(例如，观察到的用户特征)来解决上述非识别问题。提出的 iDCF 是一个通用的解构推荐框架，它应用近因推理来推断不可测量的混杂因素，并用理论保证来识别反事实反馈。在各种真实世界和合成数据集上的大量实验验证了该方法的有效性和鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Debiasing+Recommendation+by+Learning+Identifiable+Latent+Confounders)|0|
|[Hierarchical Invariant Learning for Domain Generalization Recommendation](https://doi.org/10.1145/3580305.3599377)|Zeyu Zhang, Heyang Gao, Hao Yang, Xu Chen||Most cross-domain recommenders require samples on target domains or source-target overlaps to carry out domain adaptation. However, in many real-world situations, target domains are lack of such knowledge. Few works discuss this problem, whose essence is domain generalization recommendation. In this paper, we figure out domain generalization recommendation with a clear symbolized definition and propose corresponding models. Moreover, we illustrate its strong connection with zero-shot recommendation, pretrained recommendation and cold-start recommendation, distinguishing it from content-based recommendation. By analyzing its properties, we propose HIRL^+ and a series of heuristic methods to solve this problem. We propose hierarchical invariant learning to expel the specific patterns in both domain-level and environment-level, and find the common patterns in generalization space. To make the division of environments flexible, fine-grained and balanced, we put forward a learnable environment assignment method. To improve the robustness against distribution shifts inside domain generalization, we present an adversarial environment refinement method. In addition, we conduct experiments on real-word datasets to verify the effectiveness of our models, and carry out further studies on the domain distance and domain diversity. To benefit the research community and promote this direction, we discuss the future of this field.|大多数跨域推荐需要在目标域或源-目标重叠上采样来执行域适配。然而，在许多现实情况下，目标领域缺乏这样的知识。很少有文献讨论这个问题，其实质是领域推广推荐。在本文中，我们给出了一个明确的符号化定义的域泛化推荐，并提出了相应的模型。此外，我们还说明了它与零拍推荐、预训练推荐和冷启动推荐之间的紧密联系，区别于基于内容的推荐。通过分析其性质，提出了 HIRL ^ + 及一系列启发式方法来解决这一问题。我们提出了分层不变式学习来去除领域层和环境层的特定模式，并在泛化空间中找到共同的模式。为了实现环境划分的灵活性、细粒度和均衡性，提出了一种可学习的环境分配方法。为了提高对域内分布移位的鲁棒性，我们提出了一种对抗性环境细化方法。此外，我们还对实际数据集进行了实验，验证了模型的有效性，并对领域距离和领域多样性进行了进一步的研究。为了有利于研究界和促进这一方向，我们讨论了这一领域的未来。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Invariant+Learning+for+Domain+Generalization+Recommendation)|0|
|[Narrow the Input Mismatch in Deep Graph Neural Network Distillation](https://doi.org/10.1145/3580305.3599442)|Qiqi Zhou, Yanyan Shen, Lei Chen|Shanghai Jiao Tong University; Hong Kong University of Science and Technology|Graph neural networks (GNNs) have been widely studied for modeling graph-structured data. Thanks to the over-parameterization and large receptive field of deep GNNs, "deep" is a promising direction to develop GNNs further and has shown some superior performances. However, the over-stacked structures of deep architectures incur high inference cost in deployment. To compress deep GNNs, we can use knowledge distillation (KD) to make shallow student GNNs mimic teacher GNNs. Existing KD methods in graph domain focus on constructing diverse supervision on embedding or prediction produced by student GNNs, but overlook the gap of the receptive field (i.e., input information) between student and teacher, which brings difficulties to KD. We call this gap "input mismatch". To alleviate this problem, we propose a lightweight stochastic extended module to provide an estimation for missing input information for student GNNs. The estimator models the distribution of missing information. Specifically, we model the missing information as an independent distribution from graph level and a conditional distribution from node level (given the condition of observable input). These two estimates are optimized using a Bayesian methodology and combined into a balanced estimate as additional input to student GNNs. To the best of our knowledge, we are the first to address the "input mismatch" problem in deep GNNs distillation. Experiments on extensive benchmarks demonstrate that our method outperforms existing KD methods for GNNs in distillation performance, which confirms that the estimations are reasonable and effective.|图形神经网络(GNN)在图形结构数据建模中得到了广泛的研究。由于深层 GNN 的过度参数化和较大的接收场，“深层”是 GNN 进一步发展的一个有希望的方向，并已显示出一些优越的性能。但是，深层体系结构的过堆叠结构在部署过程中会产生很高的推理成本。为了压缩深层 GNN，我们可以使用知识提取(KD)来使浅层学生 GNN 模拟教师 GNN。现有的图形领域的 KD 方法侧重于构建对学生 GNN 嵌入或预测的多样化监控，但忽视了学生与教师之间接受域(即输入信息)的差距，给 KD 带来了困难。我们称这种差距为“输入不匹配”。为了解决这个问题，我们提出了一个轻量级的随机扩展模块来估计学生 GNN 丢失的输入信息。估计器模拟缺失信息的分布。具体来说，我们将缺失信息建模为独立于图层的分布和从节点层的条件分布(给定可观测输入的条件)。这两个估计是优化使用贝叶斯方法，并结合成一个平衡的估计作为额外的投入学生 GNN。据我们所知，我们是第一个解决深层 GNN 精馏中的“输入不匹配”问题的。在大量基准上的实验表明，该方法在精馏性能方面优于现有的 KD 方法，证明了该方法的合理性和有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Narrow+the+Input+Mismatch+in+Deep+Graph+Neural+Network+Distillation)|0|
|[Robust Positive-Unlabeled Learning via Noise Negative Sample Self-correction](https://doi.org/10.1145/3580305.3599491)|Zhangchi Zhu, Lu Wang, Pu Zhao, Chao Du, Wei Zhang, Hang Dong, Bo Qiao, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang|Microsoft Research; East China Normal University; Microsoft 365|Learning from positive and unlabeled data is known as positive-unlabeled (PU) learning in literature and has attracted much attention in recent years. One common approach in PU learning is to sample a set of pseudo-negatives from the unlabeled data using ad-hoc thresholds so that conventional supervised methods can be applied with both positive and negative samples. Owing to the label uncertainty among the unlabeled data, errors of misclassifying unlabeled positive samples as negative samples inevitably appear and may even accumulate during the training processes. Those errors often lead to performance degradation and model instability. To mitigate the impact of label uncertainty and improve the robustness of learning with positive and unlabeled data, we propose a new robust PU learning method with a training strategy motivated by the nature of human learning: easy cases should be learned first. Similar intuition has been utilized in curriculum learning to only use easier cases in the early stage of training before introducing more complex cases. Specifically, we utilize a novel ``hardness'' measure to distinguish unlabeled samples with a high chance of being negative from unlabeled samples with large label noise. An iterative training strategy is then implemented to fine-tune the selection of negative samples during the training process in an iterative manner to include more ``easy'' samples in the early stage of training. Extensive experimental validations over a wide range of learning tasks show that this approach can effectively improve the accuracy and stability of learning with positive and unlabeled data. Our code is available at https://github.com/woriazzc/Robust-PU|从阳性和未标记数据中学习被称为阳性-未标记(PU)学习，近年来引起了人们的广泛关注。PU 学习中常用的一种方法是使用自组织阈值从未标记的数据中抽取一组伪阴性样本，这样传统的监督方法就可以同时应用于正样本和负样本。由于未标记数据之间存在标记不确定性，训练过程中不可避免地会出现将未标记阳性样本错误分类为阴性样本的错误，甚至可能累积。这些错误经常导致性能下降和模型不稳定。为了减轻标签不确定性的影响，提高正数和未标签数据学习的鲁棒性，我们提出了一种新的鲁棒性 PU 学习方法，其训练策略受人类学习的本质驱动: 应该首先学习简单的情况。在课程学习中也使用了类似的直觉，即在培训的早期阶段只使用较容易的案例，然后再引入更复杂的案例。具体来说，我们利用一种新的“硬度”测量方法来区分未标记样品与具有较大标记噪声的未标记样品。然后采用迭代训练策略，以迭代的方式对训练过程中的负样本选择进行微调，以便在训练的早期阶段包含更多的“简单”样本。通过对大量学习任务的大量实验验证表明，该方法能够有效地提高正数和未标记数据学习的准确性和稳定性。我们的代码可以在 https://github.com/woriazzc/robust-pu 找到|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Positive-Unlabeled+Learning+via+Noise+Negative+Sample+Self-correction)|0|
|[RankFormer: Listwise Learning-to-Rank Using Listwide Labels](https://doi.org/10.1145/3580305.3599892)|Maarten Buyl, Paul Missault, PierreAntoine Sondag|Amazon|Web applications where users are presented with a limited selection of items have long employed ranking models to put the most relevant results first. Any feedback received from users is typically assumed to reflect a relative judgement on the utility of items, e.g. a user clicking on an item only implies it is better than items not clicked in the same ranked list. Hence, the objectives optimized in Learning-to-Rank (LTR) tend to be pairwise or listwise. Yet, by only viewing feedback as relative, we neglect the user's absolute feedback on the list's overall quality, e.g. when no items in the selection are clicked. We thus reconsider the standard LTR paradigm and argue the benefits of learning from this listwide signal. To this end, we propose the RankFormer as an architecture that, with a Transformer at its core, can jointly optimize a novel listwide assessment objective and a traditional listwise LTR objective. We simulate implicit feedback on public datasets and observe that the RankFormer succeeds in benefitting from listwide signals. Additionally, we conduct experiments in e-commerce on Amazon Search data and find the RankFormer to be superior to all baselines offline. An online experiment shows that knowledge distillation can be used to find immediate practical use for the RankFormer.|在 Web 应用程序中，用户只能看到有限的条目，这种情况长期以来一直采用排名模型，将最相关的结果放在第一位。从用户收到的任何反馈通常被认为反映了对项目效用的相对判断，例如，用户点击一个项目只意味着它比没有在同一排名列表中点击的项目要好。因此，学习排名(Learning-to-Rank，LTR)中优化的目标往往是成对的或列表的。然而，由于只把反馈看作是相对的，我们忽略了用户对列表总体质量的绝对反馈，例如，当选择中没有项被点击的时候。因此，我们重新考虑标准的 LTR 范式，并讨论从这个列表范围的信号中学习的好处。为此，我们提出 RankForm 作为一种体系结构，其核心是一个 Transformer，可以联合优化一个新的列表范围评估目标和一个传统的列表式 LTR 目标。我们模拟公共数据集上的隐式反馈，并观察到 RankForm 成功地从列表宽信号中受益。此外，我们在亚马逊搜索数据上进行电子商务实验，发现排名前优于所有离线基线。一个在线实验表明，知识提取可以用来找到直接的实际应用的秩次前。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RankFormer:+Listwise+Learning-to-Rank+Using+Listwide+Labels)|0|
|[Graph-Based Model-Agnostic Data Subsampling for Recommendation Systems](https://doi.org/10.1145/3580305.3599834)|Xiaohui Chen, Jiankai Sun, Taiqing Wang, Ruocheng Guo, LiPing Liu, Aonan Zhang|ByteDance Inc.; Apple Inc.; ByteDance Research; Tufts University|Data subsampling is widely used to speed up the training of large-scale recommendation systems. Most subsampling methods are model-based and often require a pre-trained pilot model to measure data importance via e.g. sample hardness. However, when the pilot model is misspecified, model-based subsampling methods deteriorate. Since model misspecification is persistent in real recommendation systems, we instead propose model-agnostic data subsampling methods by only exploring input data structure represented by graphs. Specifically, we study the topology of the user-item graph to estimate the importance of each user-item interaction (an edge in the user-item graph) via graph conductance, followed by a propagation step on the network to smooth out the estimated importance value. Since our proposed method is model-agnostic, we can marry the merits of both model-agnostic and model-based subsampling methods. Empirically, we show that combing the two consistently improves over any single method on the used datasets. Experimental results on KuaiRec and MIND datasets demonstrate that our proposed methods achieve superior results compared to baseline approaches.|数据子采样被广泛用于加速大规模推荐系统的训练。大多数次抽样方法是基于模型的，通常需要一个预先训练的试点模型来通过例如样本硬度来测量数据的重要性。然而，当导频模型被错误指定时，基于模型的子抽样方法就会变质。由于模型错误说明在实际推荐系统中一直存在，因此我们提出了模型无关的数据子抽样方法，只是探讨了用图表示的输入数据结构。具体来说，我们研究了用户项目图的拓扑结构，通过图电导来估计每个用户项目交互(用户项目图中的一条边)的重要性，然后通过网络上的传播步骤来平滑估计的重要性值。由于我们提出的方法是模型不可知的，我们可以结合模型不可知和基于模型的子抽样方法的优点。经验上，我们表明，结合使用这两种方法比使用的数据集上的任何单一方法都要好。在 KuaiRec 和 MIND 数据集上的实验结果表明，与基线方法相比，我们提出的方法取得了更好的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph-Based+Model-Agnostic+Data+Subsampling+for+Recommendation+Systems)|0|
|[BOSS: A Bilateral Occupational-Suitability-Aware Recommender System for Online Recruitment](https://doi.org/10.1145/3580305.3599783)|Xiao Hu, Yuan Cheng, Zhi Zheng, Yue Wang, Xinxin Chi, Hengshu Zhu||With the rapid development of online recruitment platforms, a variety of emerging recommendation services have been witnessed for benefiting both job seekers and recruiters. While many researchers have studied the problem of reciprocal recommendation in two- sided markets (e.g., marriage market and real estate market), there is still a lack of in-depth understanding of the bilateral occupational preferences of different participants in the online recruitment market. To this end, in this paper, we propose a Bilateral Occupational-Suitability-aware recommender System (BOSS) for online recruitment, in consideration of the reciprocal, bilateral, and sequential properties of realistic recruitment scenarios simultaneously. To be specific, in BOSS, we first propose a multi-group-based mixture-of-experts (MoE) module to independently learn the preference representations of job seekers and recruiters. Then, with a specially-designed multi-task learning module, BOSS can progressively model the action sequence of recruitment process through a bilateral probabilistic manner. As a result, the reciprocal recommendations can be efficiently implemented by leveraging the product of different action probabilities of job seekers and recruiters. Finally, we have conducted extensive experiments on 5 real-world large-scale datasets as well as the online environment. Both online A/B test and offline experimental results clearly validate that our recommender system BOSS can outperform other state-of-the-art baselines with a significant margin.|随着在线招聘平台的迅速发展，各种新出现的推荐服务使求职者和招聘人员都受益。虽然许多研究者对双边市场(如婚姻市场和房地产市场)中的互惠推荐问题进行了研究，但对于网络招聘市场中不同参与者的双边职业偏好仍缺乏深入的理解。为此，在本文中，我们提出了一个双边职业适合性意识在线招聘推荐系统(BOSS) ，同时考虑到现实招聘场景的互惠性、双边性和顺序性。具体来说，在 BOSS 系统中，我们首先提出了一个基于多群体的专家混合模型(MoE)来独立学习求职者和招聘者的偏好表示。然后，通过专门设计的多任务学习模块，BOSS 可以通过双边概率的方式逐步建模招聘过程的行为序列。因此，通过利用求职者和招聘人员不同行动概率的结果，可以有效地执行互惠建议。最后，我们在5个真实世界的大规模数据集上以及在线环境下进行了广泛的实验。线上和线下的实验结果都清楚地证明了我们的推荐系统老板能够以显著的优势超越其他最先进的基准线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BOSS:+A+Bilateral+Occupational-Suitability-Aware+Recommender+System+for+Online+Recruitment)|0|
|[Real Time Index and Search Across Large Quantities of GNN Experts for Low Latency Online Learning](https://doi.org/10.1145/3580305.3599893)|Johan Kok Zhi Kang, Sien Yi Tan, Bingsheng He, Zhen Zhang|GrabTaxi Holdings; National University of Singapore|Online learning is a powerful technique that allows models to adjust to concept drift in dynamically changing graphs. This approach is crucial for large mobility-based companies like Grab, where batch-learning methods fail to keep up with the large amount of training data. Our work focuses on scaling graph neural network mixture of expert (MoE) models for real-time traffic speed prediction on road networks, while meeting high accuracy and low latency requirements. Conventional spatio-temporal and incremental MoE frameworks struggle with poor inference accuracy and linear time complexity when scaling experts, for the latter, leading to prohibitively high latency in model updates. To address this issue, we introduce the Indexed Router, a novel method that categorizes experts into a structured hierarchy called the indexed tree. This approach reduces the time to scale and search N number of experts from O(N) to O(log N), making it ideal for online learning under tight service level agreements. Our experiments show that these time savings do not compromise inference accuracy, and our Indexed Router outperforms state-of-the-art spatio-temporal and incremental MoE models in terms of traffic speed prediction accuracy on real-life GPS traces from Grab's database and publicly available records. In summary, the Indexed Router enables MoE models to scale across large numbers of experts with low latency, while accurately identifying the relevant experts for inference.|在线学习是一种强大的技术，它允许模型根据动态变化的图形中的概念漂移进行调整。这种方法对于大型的基于移动性的公司来说是至关重要的，比如 Grab 公司，批量学习方法无法跟上大量的培训数据。我们的工作集中在缩放图神经网络混合专家(MoE)模型的实时交通速度预测道路网络，同时满足高精度和低延迟的要求。传统的时空和增量式 MoE 框架在缩放专家(对于后者而言)面临推断精度和线性时间复杂性较差的问题，导致模型更新延迟过高。为了解决这个问题，我们引入了索引路由器(Indexed Router) ，这是一种将专家分类到一个称为索引树(Indexed tree)的结构化层次结构中的新方法。这种方法减少了从 O (N)到 O (log N)的缩放和搜索 N 个专家的时间，使其成为严格服务水平协议下在线学习的理想选择。我们的实验表明，这些节省的时间不会影响推断的准确性，我们的 Indexed 路由器在时空和增量 MoE 模型方面优于最先进的交通速度预测精度，这些预测精度来自 Grab 数据库和公开可用记录的实际 GPS 跟踪。总之，索引路由器使 MoE 模型能够以较低的延迟跨越大量的专家，同时准确地确定相关的专家进行推理。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Real+Time+Index+and+Search+Across+Large+Quantities+of+GNN+Experts+for+Low+Latency+Online+Learning)|0|
|[A Preference-aware Meta-optimization Framework for Personalized Vehicle Energy Consumption Estimation](https://doi.org/10.1145/3580305.3599767)|Siqi Lai, Weijia Zhang, Hao Liu|The Hong Kong University of Science and Technology (Guangzhou)|Vehicle Energy Consumption (VEC) estimation aims to predict the total energy required for a given trip before it starts, which is of great importance to trip planning and transportation sustainability. Existing approaches mainly focus on extracting statistically significant factors from typical trips to improve the VEC estimation. However, the energy consumption of each vehicle may diverge widely due to the personalized driving behavior under varying travel contexts. To this end, this paper proposes a preference-aware meta-optimization framework Meta-Pec for personalized vehicle energy consumption estimation. Specifically, we first propose a spatiotemporal behavior learning module to capture the latent driver preference hidden in historical trips. Moreover, based on the memorization of driver preference, we devise a selection-based driving behavior prediction module to infer driver-specific driving patterns on a given route, which provides additional basis and supervision signals for VEC estimation. Besides, a driver-specific meta-optimization scheme is proposed to enable fast model adaption by learning and sharing transferable knowledge globally. Extensive experiments on two real-world datasets show the superiority of our proposed framework against ten numerical and data-driven machine learning baselines. The source code is available at https://github.com/usail-hkust/Meta-Pec.|车辆能耗(VEC)估算的目的是在出行前预测出行所需的总能量，这对出行规划和交通可持续性有重要意义。现有的方法主要集中在从典型行程中提取统计学显著因子，以改善 VEC 估计。然而，在不同的出行环境下，由于个性化驾驶行为的影响，每辆车的能源消耗可能会有很大的差异。为此，本文提出了一个基于偏好感知的元优化框架 Meta-Pec，用于个性化车辆能耗估算。具体来说，我们首先提出了一个时空行为学习模块来捕捉隐藏在历史行程中的潜在驱动偏好。此外，基于驾驶员偏好的记忆，我们设计了一个基于选择的驾驶行为预测模块，以推断特定路线上驾驶员的驾驶模式，为 VEC 估计提供额外的依据和监控信号。此外，提出了一种特定于驱动程序的元优化方案，通过全局学习和共享可转移知识来实现模型的快速自适应。在两个实际数据集上的大量实验表明，我们提出的框架对于十个数字和数据驱动的机器学习基线具有优越性。源代码可在 https://github.com/usail-hkust/meta-pec 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Preference-aware+Meta-optimization+Framework+for+Personalized+Vehicle+Energy+Consumption+Estimation)|0|
|[MUSER: A MUlti-Step Evidence Retrieval Enhancement Framework for Fake News Detection](https://doi.org/10.1145/3580305.3599873)|Hao Liao, Jiahao Peng, Zhanyi Huang, Wei Zhang, Guanghua Li, Kai Shu, Xing Xie|Illinois Institute of Technology; Shenzhen University; Microsoft Research Asia|The ease of spreading false information online enables individuals with malicious intent to manipulate public opinion and destabilize social stability. Recently, fake news detection based on evidence retrieval has gained popularity in an effort to identify fake news reliably and reduce its impact. Evidence retrieval-based methods can improve the reliability of fake news detection by computing the textual consistency between the evidence and the claim in the news. In this paper, we propose a framework for fake news detection based on MUlti-Step Evidence Retrieval enhancement (MUSER), which simulates the steps of human beings in the process of reading news, summarizing, consulting materials, and inferring whether the news is true or fake. Our model can explicitly model dependencies among multiple pieces of evidence, and perform multi-step associations for the evidence required for news verification through multi-step retrieval. In addition, our model is able to automatically collect existing evidence through paragraph retrieval and key evidence selection, which can save the tedious process of manual evidence collection. We conducted extensive experiments on real-world datasets in different languages, and the results demonstrate that our proposed model outperforms state-of-the-art baseline methods for detecting fake news by at least 3% in F1-Macro and 4% in F1-Micro. Furthermore, it provides interpretable evidence for end users.|在网上传播虚假信息的便利使得有恶意的个人能够操纵公众舆论，破坏社会稳定。近年来，基于证据检索的假新闻检测技术在可靠识别假新闻、减少假新闻影响等方面得到了广泛的应用。基于证据检索的方法通过计算新闻中证据与索赔之间的文本一致性，提高了假新闻检测的可靠性。本文提出了一种基于多步证据检索增强(MUSER)的假新闻检测框架，该框架模拟了人类在阅读新闻、总结新闻、查阅资料、推断新闻是真是假的过程中的步骤。该模型可以显式地对多个证据之间的依赖关系进行建模，并通过多步检索对新闻验证所需的证据进行多步关联。此外，该模型通过段落检索和关键证据选择，能够自动收集现有证据，节省了繁琐的人工证据收集过程。我们在不同语言的真实世界数据集上进行了广泛的实验，结果表明，我们提出的模型比最先进的基线方法在 F1-Macro 中检测假新闻的性能至少高出3% ，在 F1-Micro 中高出4% 。此外，它还为最终用户提供了可解释的证据。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MUSER:+A+MUlti-Step+Evidence+Retrieval+Enhancement+Framework+for+Fake+News+Detection)|0|
|[PrivateRec: Differentially Private Model Training and Online Serving for Federated News Recommendation](https://doi.org/10.1145/3580305.3599889)|Ruixuan Liu, Yang Cao, Yanlin Wang, Lingjuan Lyu, Yun Chen, Hong Chen||Federated recommendation can potentially alleviate the privacy concerns in collecting sensitive and personal data for training personalized recommendation systems. However, it suffers from a low recommendation quality when a local serving is inapplicable due to the local resource limitation and the data privacy of querying clients is required in online serving. Furthermore, a theoretically private solution in both the training and serving of federated recommendation is essential but still lacking. Naively applying differential privacy (DP) to the two stages in federated recommendation would fail to achieve a satisfactory trade-off between privacy and utility due to the high-dimensional characteristics of model gradients and hidden representations. In this work, we propose a federated news recommendation method for achieving better utility in model training and online serving under a DP guarantee. We first clarify the DP definition over behavior data for each round in the pipeline of federated recommendation systems. Next, we propose a privacy-preserving online serving mechanism under this definition based on the idea of decomposing user embeddings with public basic vectors and perturbing the lower-dimensional combination coefficients. We apply a random behavior padding mechanism to reduce the required noise intensity for better utility. Besides, we design a federated recommendation model training method, which can generate effective and public basic vectors for serving while providing DP for training participants. We avoid the dimension-dependent noise for large models via label permutation and differentially private attention modules. Experiments on real-world news recommendation datasets validate that our method achieves superior utility under a DP guarantee in both training and serving of federated news recommendations.|联合推荐可以减轻收集敏感数据和个人数据以培训个性化推荐系统时对隐私的担忧。然而，当本地服务由于本地资源的限制而不适用，并且在线服务需要查询客户端的数据隐私时，该算法的推荐质量较低。此外，在联合推荐的培训和服务中，理论上的私有解决方案是必不可少的，但仍然缺乏。由于模型梯度和隐藏表示的高维特性，在联邦推荐的两个阶段天真地应用差分隐私(DP)将无法在隐私和实用性之间达成令人满意的平衡。在本研究中，我们提出一个联邦新闻推荐的方法，以达到更好的效用模型训练和在线服务下的 DP 保证。我们首先阐明联邦推荐系统管道中每一轮行为数据的 DP 定义。其次，基于公共基本向量分解用户嵌入和扰动低维组合系数的思想，在此定义下提出了一种保护隐私的在线服务机制。我们应用一个随机行为填充机制，以降低所需的噪声强度，以获得更好的效用。此外，我们还设计了一种联邦推荐模型训练方法，该方法可以生成有效的、公开的基本向量，为训练参与者提供 DP 服务。对于大型模型，我们通过标签置换和差分私有注意模块来避免与维数相关的噪声。在实际的新闻推荐数据集上进行的实验验证了该方法在联邦新闻推荐的培训和服务方面，在 DP 保证下取得了较好的效用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PrivateRec:+Differentially+Private+Model+Training+and+Online+Serving+for+Federated+News+Recommendation)|0|
|[Hierarchical Projection Enhanced Multi-behavior Recommendation](https://doi.org/10.1145/3580305.3599838)|Chang Meng, Hengyu Zhang, Wei Guo, Huifeng Guo, Haotian Liu, Yingxue Zhang, Hongkun Zheng, Ruiming Tang, Xiu Li, Rui Zhang||Various types of user behaviors are recorded in most real-world recommendation scenarios. To fully utilize the multi-behavior information, the exploration of multiplex interaction among them is essential. Many multi-task learning based multi-behavior methods are proposed recently to use multiple types of supervision signals and perform information transfer among them. Despite the great successes, these methods fail to design prediction tasks comprehensively, leading to insufficient utilization of multi-behavior correlative information. Besides, these methods are either based on the weighting of expert information extracted from the coupled input or modeling of information transfer between multiple behavior levels through task-specific extractors, which are usually accompanied by negative transfer phenomenon 1 . To address the above problems, we propose a multi-behavior recommendation framework, called Hierarchical Projection Enhanced Multi-behavior Recommendation (HPMR). The key module, Projection-based Transfer Network (PTN), uses the projection mechanism to "explicitly" model the correlations of upstream and downstream behaviors, refines the upstream behavior representations, and fully uses the refined representations to enhance the learning of downstream tasks. Offline experiments on public and industrial datasets and online A/B test further verify the effectiveness of HPMR in modeling the associations from upstream to downstream and alleviating the negative transfer. The source code and datasets are available at https://github.com/MC-CV/HPMR.|在大多数真实世界的推荐场景中记录了各种类型的用户行为。为了充分利用多行为信息，探索它们之间的多元互动是必不可少的。近年来出现了许多基于多任务学习的多行为方法，它们利用多种监控信号进行信息传递。这些方法虽然取得了很大的成功，但是未能全面地设计预测任务，导致多行为相关信息利用不足。此外，这些方法要么是基于从耦合输入中提取的专家信息的权重，要么是通过任务特定的提取器建立多个行为水平之间的信息传递模型，通常伴随着负迁移现象1。为了解决上述问题，我们提出了一个多行为推荐框架，称为层次投影增强多行为推荐(HPMR)。关键模块，基于投影的传输网络(PTN) ，使用投影机制来“显式”建模上游或下游行为的相关性，提炼上游行为表示，并充分利用提炼表示来增强下游任务的学习。在公共和工业数据集上的离线实验和在线 A/B 检验进一步验证了 HPMR 模型在建立上下游关联模型和减轻负迁移方面的有效性。源代码和数据集可在 https://github.com/mc-cv/hpmr 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Projection+Enhanced+Multi-behavior+Recommendation)|0|
|[End-to-End Query Term Weighting](https://doi.org/10.1145/3580305.3599815)|Karan Samel, Cheng Li, Weize Kong, Tao Chen, Mingyang Zhang, Shaleen Kumar Gupta, Swaraj Khadanga, Wensong Xu, Xingyu Wang, Kashyap Kolipaka, Michael Bendersky, Marc Najork||Bag-of-words based lexical retrieval systems are still the most commonly used methods for real-world search applications. Recently deep learning methods have shown promising results to improve this retrieval performance but are expensive to run in an online fashion, non-trivial to integrate into existing production systems, and might not generalize well in out-of-domain retrieval scenarios. Instead, we build on top of lexical retrievers by proposing a Term Weighting BERT (TW-BERT) model. TW-BERT learns to predict the weight for individual n-gram (e.g., uni-grams and bi-grams) query input terms. These inferred weights and terms can be used directly by a retrieval system to perform a query search. To optimize these term weights, TW-BERT incorporates the scoring function used by the search engine, such as BM25, to score query-document pairs. Given sample query-document pairs we can compute a ranking loss over these matching scores, optimizing the learned query term weights in an end-to-end fashion. Aligning TW-BERT with search engine scorers minimizes the changes needed to integrate it into existing production applications, whereas existing deep learning based search methods would require further infrastructure optimization and hardware requirements. The learned weights can be easily utilized by standard lexical retrievers and by other retrieval techniques such as query expansion. We show that TW-BERT improves retrieval performance over strong term weighting baselines within MSMARCO and in out-of-domain retrieval on TREC datasets.|基于词包的词汇检索系统仍然是实际搜索应用中最常用的方法。近年来，深度学习方法在提高检索性能方面取得了一些有希望的成果，但是在在线方式下运行成本较高，与现有生产系统的集成也不容易，而且在域外检索场景中可能无法得到很好的推广。相反，我们通过提出一个词汇加权 BERT (TW-BERT)模型，在词汇检索器的基础上构建。TW-BERT 学习预测单个 n-gram (例如，un- gram 和 bi-gram)查询输入项的权重。检索系统可以直接使用这些推断的权重和术语来执行查询搜索。为了优化这些术语权重，TW-BERT 结合了搜索引擎(如 BM25)使用的评分函数，以对查询文档对进行评分。给定样本查询-文档对，我们可以计算这些匹配得分的排名损失，以端到端的方式优化学习的查询术语权重。将 TW-BERT 与搜索引擎评分器结合起来，可以最大限度地减少将其集成到现有生产应用程序所需的更改，而现有的基于深度学习的搜索方法将需要进一步的基础设施优化和硬件要求。标准词法检索器和查询扩展等其他检索技术可以很容易地利用学习权重。我们表明，TW-BERT 提高检索性能的强项权重基线内 MSMARCO 和域外的 TREC 数据集检索。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=End-to-End+Query+Term+Weighting)|0|
|[UnifieR: A Unified Retriever for Large-Scale Retrieval](https://doi.org/10.1145/3580305.3599927)|Tao Shen, Xiubo Geng, Chongyang Tao, Can Xu, Guodong Long, Kai Zhang, Daxin Jiang||Large-scale retrieval is to recall relevant documents from a huge collection given a query. It relies on representation learning to embed documents and queries into a common semantic encoding space. According to the encoding space, recent retrieval methods based on pre-trained language models (PLM) can be coarsely categorized into either dense-vector or lexicon-based paradigms. These two paradigms unveil the PLMs' representation capability in different granularities, i.e., global sequence-level compression and local word-level contexts, respectively. Inspired by their complementary global-local contextualization and distinct representing views, we propose a new learning framework, UnifieR, which unifies dense-vector and lexicon-based retrieval in one model with a dual-representing capability. Experiments on passage retrieval benchmarks verify its effectiveness in both paradigms. A uni-retrieval scheme is further presented with even better retrieval quality. We lastly evaluate the model on BEIR benchmark to verify its transferability.|大规模检索是从给定查询的大量集合中回收相关文档。它依靠表示学习将文档和查询嵌入到一个公共的语义编码空间中。根据编码空间的不同，现有的基于预训练语言模型(PLM)的检索方法可以粗略地分为基于密集向量的检索方法和基于词典的检索方法。这两种范式分别揭示了 PLM 在不同粒度上的表示能力，即全局序列级压缩和局部词级上下文。受到它们互补的全局-局部上下文化和不同表示视图的启发，我们提出了一种新的学习框架 UnifieR，它将密集向量检索和基于词典的检索结合在一个具有双重表示能力的模型中。文章检索基准的实验结果验证了该方法在两种范式下的有效性。进一步提出了一种单一检索方案，检索效果更好。最后通过对 BEIR 基准测试模型的评估，验证了该模型的可推广性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=UnifieR:+A+Unified+Retriever+for+Large-Scale+Retrieval)|0|
|[Counterfactual Video Recommendation for Duration Debiasing](https://doi.org/10.1145/3580305.3599797)|Shisong Tang, Qing Li, Dingmin Wang, Ci Gao, Wentao Xiao, Dan Zhao, Yong Jiang, Qian Ma, Aoyang Zhang||Duration bias widely exists in video recommendations, where models tend to recommend short videos for the higher ratio of finish playing and thus possibly fail to capture users' true interests. In this paper, we eliminate the duration bias from both data and model. First, based on the extensive data analysis, we observe that play completion rate of videos with the same duration presents a bimodal distribution. Hence, we propose to perform threshold division to construct binary labels as training labels for alleviating the drawback of finish playing labels overly biased towards short videos. Algorithmically, we resort to causal inference, which enables us to inspect causal relationships of video recommendations with a causal graph. We identify that duration has two kinds of effect on prediction: direct and indirect. Duration bias lies in the direct effect, while the indirect effect benefits prediction. To this end, we design a model-agnostic Counterfactual Video Recommendation for Duration Debiasing (CVRDD) framework, which incorporates multi-task learning to estimate different causal effect during training. In the inference phase, we perform counterfactual inference to remove the direct effect of duration for unbiased prediction. We conduct experiments on two industrial datasets, and in addition to achieving highly promising results on traditional top-k recommendation metrics, CVRDD also improves the user watch time.|持续时间偏差广泛存在于视频推荐中，其中模型倾向于推荐较高比例的完成播放的短视频，因此可能无法捕捉用户的真实兴趣。本文从数据和模型两方面消除了持续时间偏差。首先，基于广泛的数据分析，我们观察到在相同持续时间的视频中，播放完成率呈现出一个双峰分布。因此，我们建议使用阈值分割来构造二进制标签作为训练标签，以减轻完成播放标签过于偏向短视频的缺点。在算法上，我们求助于因果推理，它使我们能够检查因果图视频推荐的因果关系。我们发现持续时间对预测有两种影响: 直接影响和间接影响。持续时间偏差在于直接效应，而间接效应有利于预测。为此，我们设计了一个模型无关的持续时间消偏反事实视频推荐(CVRDD)框架，该框架结合多任务学习来估计训练过程中不同的因果效应。在推理阶段，我们通过反事实推理消除持续时间对无偏预测的直接影响。我们在两个工业数据集上进行了实验，除了在传统的 top-k 推荐指标上取得了非常有前景的结果之外，CVRDD 还提高了用户的观看时间。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Counterfactual+Video+Recommendation+for+Duration+Debiasing)|0|
|[Semantic-Enhanced Differentiable Search Index Inspired by Learning Strategies](https://doi.org/10.1145/3580305.3599903)|Yubao Tang, Ruqing Zhang, Jiafeng Guo, Jiangui Chen, Zuowei Zhu, Shuaiqiang Wang, Dawei Yin, Xueqi Cheng|ICT, CAS; Baidu Inc.|Recently, a new paradigm called Differentiable Search Index (DSI) has been proposed for document retrieval, wherein a sequence-to-sequence model is learned to directly map queries to relevant document identifiers. The key idea behind DSI is to fully parameterize traditional ``index-retrieve'' pipelines within a single neural model, by encoding all documents in the corpus into the model parameters. In essence, DSI needs to resolve two major questions: (1) how to assign an identifier to each document, and (2) how to learn the associations between a document and its identifier. In this work, we propose a Semantic-Enhanced DSI model (SE-DSI) motivated by Learning Strategies in the area of Cognitive Psychology. Our approach advances original DSI in two ways: (1) For the document identifier, we take inspiration from Elaboration Strategies in human learning. Specifically, we assign each document an Elaborative Description based on the query generation technique, which is more meaningful than a string of integers in the original DSI; and (2) For the associations between a document and its identifier, we take inspiration from Rehearsal Strategies in human learning. Specifically, we select fine-grained semantic features from a document as Rehearsal Contents to improve document memorization. Both the offline and online experiments show improved retrieval performance over prevailing baselines.|最近，一个新的范例被称为微分搜索索引(DSI)已被提出的文献检索，其中一个序列到序列模型学习直接映射查询到相关的文档标识符。DSI 的关键思想是将传统的“索引-检索”流水线完全参数化在一个单一的神经模型中，将语料库中的所有文档编码到模型参数中。本质上，DSI 需要解决两个主要问题: (1)如何为每个文档分配标识符，(2)如何学习文档与其标识符之间的关联。在本研究中，我们在认知心理学领域提出了一个以学习策略为动机的语义增强型 DSI 模型(SE-DSI)。我们的方法从两个方面对原始 DSI 进行了改进: (1)对于文档标识符，我们从人类学习中的精化策略中得到了启发。具体来说，我们基于查询生成技术为每个文档分配一个精细描述，这比原始 DSI 中的一串整数更有意义; (2)对于文档及其标识符之间的关联，我们从人类学习中的排练策略中获得灵感。具体来说，我们从文档中选择细粒度的语义特征作为预演内容，以提高文档的记忆能力。离线和在线实验都表明，相对于主流基线，检索性能有所提高。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semantic-Enhanced+Differentiable+Search+Index+Inspired+by+Learning+Strategies)|0|
|[Doctor Specific Tag Recommendation for Online Medical Record Management](https://doi.org/10.1145/3580305.3599810)|Yejing Wang, Shen Ge, Xiangyu Zhao, Xian Wu, Tong Xu, Chen Ma, Zhi Zheng||With the rapid growth of online medical platforms, more and more doctors are willing to manage and communicate with patients via online services. Considering the large volume and various patient conditions, identifying and classifying patients' medical records has become a crucial problem. To efficiently index these records, a common practice is to annotate them with semantically meaningful tags. However, manual labeling tags by doctors is impractical due to the possibility of thousands of tag candidates, which necessitates a tag recommender system. Due to the long tail distribution of tags and the dominance of low-activity doctors, as well as the unique uploaded medical records, this task is rather challenging. This paper proposes an efficient doctor specific tag recommendation framework for improved medical record management without side information. Specifically, we first utilize effective language models to learn the text representation. Then, we construct a doctor embedding learning module to enhance the recommendation quality by integrating implicit information within text representations and considering latent tag correlations to make more accurate predictions. Extensive experiment results demonstrate the effectiveness of our framework from the viewpoints of all doctors (20% improvement) or low-activity doctors (10% improvement).|随着在线医疗平台的快速发展，越来越多的医生愿意通过在线服务与患者进行管理和沟通。鉴于病案数量庞大，病人情况多种多样，对病案进行识别和分类已成为一个关键问题。为了有效地对这些记录进行索引，通常的做法是使用有语义意义的标记对它们进行注释。然而，由医生手动标签是不切实际的，因为可能有数千个标签候选人，这就需要一个标签推荐系统。由于标签的长尾分布和低活跃度医生的主导地位，以及独特的上传医疗记录，这项任务是相当具有挑战性的。本文提出了一个有效的医生特定标签推荐框架，改进了病案管理中的无侧信息。具体来说，我们首先利用有效的语言模型来学习文本表征。然后，通过整合文本表征中的隐含信息，考虑潜在的标签相关性，构建医生嵌入式学习模块，提高推荐质量。广泛的实验结果表明，我们的框架从所有医生的观点(20% 的改善)或低活动医生(10% 的改善)的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Doctor+Specific+Tag+Recommendation+for+Online+Medical+Record+Management)|0|
|[On-device Integrated Re-ranking with Heterogeneous Behavior Modeling](https://doi.org/10.1145/3580305.3599878)|Yunjia Xi, Weiwen Liu, Yang Wang, Ruiming Tang, Weinan Zhang, Yue Zhu, Rui Zhang, Yong Yu||As an emerging field driven by industrial applications, integrated re-ranking combines lists from upstream sources into a single list, and presents it to the user. The quality of integrated re-ranking is especially sensitive to real-time user behaviors and preferences. However, existing methods are all built on the cloud-to-edge framework, where mixed lists are generated by the cloud model and then sent to the devices. Despite its effectiveness, such a framework fails to capture users' real-time preferences due to the network bandwidth and latency. Hence, we propose to place the integrated re-ranking model on devices, allowing for the full exploitation of real-time behaviors. To achieve this, we need to address two key issues: first, how to extract users' preferences for different sources from heterogeneous and imbalanced user behaviors; second, how to explore the correlation between the extracted personalized preferences and the candidate items. In this work, we present the first on-Device Integrated Re-ranking framework, DIR, to avoid delays in processing real-time user behaviors. DIR includes a multi-sequence behavior modeling module to extract the user's source-level preferences, and a preference-adaptive re-ranking module to incorporate personalized source-level preferences into the re-ranking of candidate items. Besides, we design exposure loss and utility loss to jointly optimize exposure fairness and overall utility. Extensive experiments on three datasets show that DIR significantly outperforms the state-of-the-art baselines in utility-based and fairness-based metrics.|作为一个以工业应用为驱动力的新兴领域，集成重排将来自上游源的列表合并为一个单独的列表，并将其呈现给用户。集成重新排序的质量对实时用户行为和偏好特别敏感。但是，现有的方法都构建在云到边缘的框架上，在这个框架中，云模型生成混合列表，然后发送给设备。尽管这种框架很有效，但由于网络带宽和延迟，它无法捕获用户的实时偏好。因此，我们建议将集成的重新排序模型放置在设备上，以便充分利用实时行为。为了实现这一目标，我们需要解决两个关键问题: 第一，如何从异构和不平衡的用户行为中提取用户对不同来源的偏好; 第二，如何探索提取的个性化偏好与候选项之间的相关性。在这项工作中，我们提出了第一个设备集成重新排序框架 DIR，以避免处理实时用户行为的延迟。DIR 包括用于提取用户源级偏好的多序列行为建模模块和用于将个性化源级偏好合并到候选项重新排序中的偏好自适应重新排序模块。此外，我们还设计了曝光损失和效用损失，以共同优化曝光公平性和整体效用。对三个数据集的大量实验表明，DIR 在基于效用和基于公平的指标方面明显优于最先进的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On-device+Integrated+Re-ranking+with+Heterogeneous+Behavior+Modeling)|0|
|[Empowering Long-tail Item Recommendation through Cross Decoupling Network (CDN)](https://doi.org/10.1145/3580305.3599814)|Yin Zhang, Ruoxi Wang, Derek Zhiyuan Cheng, Tiansheng Yao, Xinyang Yi, Lichan Hong, James Caverlee, Ed H. Chi|Texas AM University; Google Research, Brain Team|Recommenders provide personalized content recommendations to users. They often suffer from highly skewed long-tail item distributions, with a small fraction of the items receiving most of the user feedback. This hurts model quality especially for the slices without much supervision. Existing work in both academia and industry mainly focuses on re-balancing strategies (e.g., up-sampling and up-weighting), leveraging content features, and transfer learning. However, there still lacks of a deeper understanding of how the long-tail distribution influences the recommendation performance. In this work, we theoretically demonstrate that the prediction of user preference is biased under the long-tail distributions. This bias comes from the discrepancy of both the prior and conditional probabilities between training data and test data. Most existing methods mainly attempt to reduce the bias from the prior perspective, which ignores the discrepancy in the conditional probability. This leads to a severe forgetting issue and results in suboptimal performance. To address the problem, we design a novel Cross Decoupling Network (CDN) to reduce the differences in both prior and conditional probabilities. Specifically, CDN (i) decouples the learning process of memorization and generalization on the item side through a mixture-of-expert structure; (ii) decouples the user samples from different distributions through a regularized bilateral branch network. Finally, a novel adapter is introduced to aggregate the decoupled vectors, and softly shift the training attention to tail items. Extensive experimental results show that CDN significantly outperforms state-of-the-art approaches on popular benchmark datasets, leading to an improvement in HR@50 (hit ratio) of 8.7\% for overall recommendation and 12.4\% for tail items.|推荐程序向用户提供个性化内容推荐。他们经常受到高度扭曲的长尾条目分布的影响，其中一小部分条目接受了大部分用户反馈。这会损害模型的质量，特别是对于没有很多监督的切片。学术界和业界现有的工作主要集中在重新平衡策略(例如，上调样本和上调权重)、利用内容特性和转移学习。然而，对于长尾分布是如何影响推荐性能的，目前还缺乏更深入的理解。本文从理论上证明了在长尾分布下，用户偏好的预测是有偏差的。这种偏差来自于训练数据和测试数据之间先验概率和条件概率的差异。大多数现有的方法主要试图从先验的角度减少偏差，而忽略了条件概率的差异。这会导致严重的遗忘问题，并导致次优性能。为了解决这一问题，我们设计了一种新的交叉解耦网络(CDN) ，以减少先验概率和条件概率的差异。具体来说，CDN (i)通过混合专家结构解耦项目侧记忆和概括的学习过程; (ii)通过正则化的双边分支网络解耦来自不同分布的用户样本。最后，引入一种新的适配器对解耦后的向量进行聚合，并将训练注意力柔和地转移到尾项上。广泛的实验结果表明，CDN 在流行的基准数据集上显着优于最先进的方法，导致总体推荐的 HR@50(命中率)改善为8.7% ，尾部项目的改善为12.4% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Empowering+Long-tail+Item+Recommendation+through+Cross+Decoupling+Network+(CDN))|0|
|[PDAS: A Practical Distributed ADMM System for Large-Scale Linear Programming Problems at Alipay](https://doi.org/10.1145/3580305.3599883)|Jun Zhou, Yang Bao, Daohong Jian, Hua Wu|Zhejiang University; Ant Group|Linear programming (LP) is arguably the most common optimization problem encountered in practical settings. Important examples include machine learning systems optimization, resource allocation, and other decision-making scenarios. However, even with state-of-the-art (SOTA) solvers, it is extremely challenging to solve large-scale problems arising in industry settings, which could have up to billions of decision variables and require solutions within a time limit to meet business demands. This paper proposes PDAS, a Practical Distributed ADMM System to solve such problems with a variant of the Alternating Direction Method of Multipliers (ADMM) algorithm. PDAS offers user-friendly interfaces and provides near-linear speedup thanks to its high scalability and excellent performance. It also comes with a failover mechanism to ensure the stability of the iterative process. The convergence, feasibility, and optimality of PDAS have been verified on two real-world data-sets, resulting in a 10 -4 average relative deviation from Gurobi. Although SOTA solvers do have advantages if only considering the solving time when tested on five small and medium-sized public data-sets, PDAS is more promising after including the modeling time. Moreover, when used to solve large-scale LP problems with up to 10 9 decision variables and 10 4 constraints in three real-world scenarios, PDAS achieves at least 2x speedups, well beyond the capabilities of SOTA.|线性规划(LP)可以说是在实际环境中遇到的最常见的最佳化问题。重要的例子包括机器学习系统优化、资源分配和其他决策场景。然而，即使使用最先进的(SOTA)解决方案，解决产业环境中出现的大规模问题也是极具挑战性的，这些问题可能有数十亿个决策变量，需要在一定时限内找到解决方案以满足业务需求。本文提出了一种实用的分布式 ADMM 系统 PDAS，采用改进的交替方向乘法器(ADMM)算法来解决这类问题。PDAS 提供用户友好的界面，并提供近线性的加速，由于其高可伸缩性和优异的性能。它还提供了故障转移机制，以确保迭代过程的稳定性。PDAS 的收敛性、可行性和最优性已在两个现实世界的数据集上得到验证，结果与 Gurobi 的平均相对偏差为10-4。虽然 SOTA 求解器在五个中小型公共数据集上测试时只考虑求解时间的优势，但在考虑建模时间的情况下，PDAS 更有前途。此外，当在三个实际场景中用于解决具有多达10个9个决策变量和10个4个约束的大规模 LP 问题时，PDAS 至少可以实现2倍的加速，远远超过 SOTA 的能力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PDAS:+A+Practical+Distributed+ADMM+System+for+Large-Scale+Linear+Programming+Problems+at+Alipay)|0|
|[Practical Design of Performant Recommender Systems using Large-scale Linear Programming-based Global Inference](https://doi.org/10.1145/3580305.3599183)|Aman Gupta, S. Sathiya Keerthi, Ayan Acharya, Miao Cheng, Borja Ocejo Elizondo, Rohan Ramanath, Rahul Mazumder, Kinjal Basu, J. Kenneth Tay, Rupesh Gupta|Banyan Biomarkers, USA.; George Washington School of Medicine, USA.; Children's Hospital, Harvard Medical School, USA.; University of California, USA.; Metro Orthopedics & Sports Therapy, USA.; Stanford Center on Longevity, USA.; Safe Kids Worldwide, Inc., USA.; Alzheimer's Drug Discovery Foundation, 57 West 57th Street, Suite 904, New York, NY 10019, USA.; University of Virginia School of Medicine, USA.; Burke Rehabilitation Hospital, USA.; Andrews Institute for Orthopaedics and Sports Medicine, USA.; Boston University Medical Center, USA.; CrowdOptic, Inc., USA.; The Hastings Center, USA.; Icahn School of Medicine at Mount Sinai, USA.; Alzheimer's Association, USA.; University of Colorado at Denver, USA.; Norton Healthcare, University of Kentucky, USA.; Baylor College of Medicine, USA.; National Collegiate Athletic Association, USA.; Novant Health Sports Medicine, USA.|Sports-related concussions and repetitive subconcussive exposure are increasingly recognized as potential dangers to paediatric populations, but much remains unknown about the short-term and long-term consequences of these events, including potential cognitive impairment and risk of later-life dementia. This Expert Consensus Document is the result of a 1-day meeting convened by Safe Kids Worldwide, the Alzheimer's Drug Discovery Foundation, and the Andrews Institute for Orthopaedics and Sports Medicine. The goal is to highlight knowledge gaps and areas of critically needed research in the areas of concussion science, dementia, genetics, diagnostic and prognostic biomarkers, neuroimaging, sports injury surveillance, and information sharing. For each of these areas, we propose clear and achievable paths to improve the understanding, treatment and prevention of youth sports-related concussions. In 2009, around 250,000 nonfatal traumatic brain injuries (TBIs) were recorded among individuals aged <19 years in the USA.1 The Centers for Disease Control and Prevention estimate that young people aged 5–18 years sustain 65% of all sports-related concussions.2 Despite recent advances in diagnostic brain imaging and in our understanding of the physics of concussion, long-term cognitive outcomes remain poorly understood. As the physical, cognitive and emotional consequences of concussion gain wider public attention, our incomplete knowledge of how to prevent, diagnose and treat such injuries endangers the health of our children in general and the health of their brains in particular. This Expert Consensus Document is the result of a 1-day meeting of experts in the fields of paediatric and adult TBI, Alzheimer disease (AD) research, genetics, epidemiology, bioethics and sports medicine (Box 1), which was convened in November 2013 by Safe Kids Worldwide, the Alzheimer's Drug Discovery Foundation and the Andrews Institute for Orthopaedics and Sports Medicine. Our primary goal is to highlight critical gaps in our knowledge of child and adolescent concussion. We emphasize areas where research is needed, such as development of diagnostic and predictive biomarkers, elucidation of genetic risk factors, and prediction of short-term and long-term outcomes. In our conclusions, we suggest paths toward improving our understanding of the long-term consequences of sports-related paediatric concussion. The term 'concussion' is often used interchangeably with the term 'mild TBI' (mTBI), a potentially misleading practice considering the possible extent of brain damage and potential for chronic neuropsychological dysfunction following concussion. We should stress, however, that most concussions resolve without sequelae. The American Congress of Rehabilitative Medicine defines mTBI as a Glasgow Coma Scale3 score of 13–15, with loss of consciousness for <30 min and post-traumatic amnesia lasting <24 h.4 Concussion describes a heterogeneous mixture of injury phenotypes that depends on many factors, including the magnitude, location and direction of head impact. Despite a lack of macroscopic structural findings, concussive brain injury involves primary neuronal injury caused by linear and rotational shear forces that disrupt axonal and membrane function (diffuse axonal injury,5 ionic flux and glutamate excitotoxicity), followed by secondary pathophysiological effects including mitochondrial oxidative stress, disruption of cerebral blood flow, compromised blood–brain barrier (BBB) integrity, synaptic dysfunction, and neuroinflammation.6, 7 Lasting neuropsychological post-concussion symptoms (post-concussion syndrome) comprise mood disorders (for example, depression), difficulty concentrating, and memory problems (Box 2).8 Both physical and physiological components of concussive injury can damage the developing brain, putting youths engaged in impact sports at particular risk. The necks and torsos of young athletes are weaker than those of older individuals and, consequently, less force is required to cause brain injury. The developing brain might also be particularly vulnerable to axonal damage caused by the shearing forces of head trauma, which, in youth American football, can exceed linear acceleration forces of 100 g.9 However, the average forces sustained in youth sports will generally be smaller than at higher levels of sport. Proper synaptic development is critical to cognitive and behavioural health.10, 11, 12, 13, 14, 15 Processes such as neurogenesis, competitive synaptic elimination ('pruning'), myelination, and axonal and dendritic arborization continue from prenatal development throughout the lifespan.14 The frontal and temporal lobes are the last areas to mature, and humans experience pruning in these regions into their early 20s,16 so damage to these still-developing areas may have pathophysiological effects on the brain that increase the potential for neuropsychological problems later in life.17 Axonal myelination continues through adolescence into the early 20s, and is susceptible to disruption by injury.10, 18, 19, 20, 21, 22 Early results from the Professional Fighters Brain Health Study, a 5-year longitudinal study of boxers and mixed martial arts fighters, who experienced repetitive subconcussive injuries as well as concussions, indicate that earlier age of first exposure to competitive boxing correlates with greater loss of caudate volume and greater axonal damage in the frontal lobe.23, 24 The young brain also has features that contribute to its resilience. Increased neuroplasticity in this age group has been shown to contribute to better outcomes after focal injuries.25 In addition, developing animals display a shorter window of glucose metabolic impairment in response to repeat TBI than do adult animals.26 Overall, the developing brain shows both vulnerability and resilience after TBI. These interwoven factors are likely to account for differences in the effects of concussion and repeat mTBI on young versus adult brains. A conservative approach to concussion risk and greater efforts to investigate these developmental differences should be given high priority. Most people—both young and old—recover fully from concussions. In children, factors potentially influencing recovery include age and history of concussions.27, 28 In one study, approximately 90% of young adult male athletes experienced symptomatic recovery within 21 days.29 However, in an emergency department study of patients aged 11–22 years (including all causes of concussion, not just sports-related), 15% of the sample still exhibited post-concussion symptoms, including headache, dizziness, 'mental fogginess' and depression, 90 days after injury.30 Several studies suggest that high school American football players are slower to recover from concussion than are college31, 32 and professional players.33 No direct comparisons with adolescents below high school age have yet been published, although a recent study that included a pre-adolescent age group (11–12 years) suggested that post-concussion recovery duration may not exhibit a linear relationship with age,30 as adolescents in this sample took longer to recover than did the pre-adolescent children. These findings, taken together, imply a unique risk of lengthier recovery in the male adolescent age group. Further studies of younger children and females would add greatly to our ability to assess and mitigate risk across the full paediatric and adolescent age span. Youths who sustained one or more concussions within 1 year prior to a new concussion reported more-prolonged symptoms,30 suggesting a possible 'window of vulnerability', and placing previously injured youths at higher risk of protracted recovery. Adolescents aged 11–18 years were nearly 80% more likely to develop post-concussion syndrome after presenting in emergency rooms than were children aged 5–10 years; similarly, presentation with headache doubled the risk of post-concussion syndrome in both children and adolescents.34 Among children treated in an emergency room after mTBI, those aged >6 years reported higher rates of persistent symptoms 3 months post injury than did those aged <6 years.35 Of course, the ability to acquire accurate information about concussion symptoms in children <6 years of age may be limited by a lack of self-awareness of symptoms and the necessary verbal skills to effectively communicate those symptoms. Also, direct comparison of injury severity is not possible from these reports; in fact, the physical heterogeneity of various injuries, taken together with the individual's innate capacity to recover from concussion, makes such comparisons highly challenging. 'Smart helmets' are being used in some speciality research centres to standardize the physical force and angular acceleration that accompanies head hits, and the utility of these helmets to measure and predict impacts that may result in concussion is currently under investigation.36, 37 Young people recovering from concussion can experience important challenges, including altered social and academic development,38, 39, 40 lower scores on general intelligence tests, and decreased school performance (measured by grade-point average).39 Lower levels of parental education and child academic achievement both correlate with poorer concussion recovery.41 Personality traits also play a part; for example, pre-injury anxiety is a risk factor for prolonged recovery periods after sports-related concussion.42 Young athletes of both sexes are at risk of concussion, but girls report higher concussion rates than boys, particularly in high school and college soccer, basketball, and baseball or softball.28, 43, 44, 45 The factors that account for these differences remain uncertain, but might include quality of protective gear, recognition and reporting of concussion symptoms, and neck length and neck muscle strength.46 Differences in recovery trajectories between males and females are also poorly understood. However, one recent study suggested that progesterone levels in females influence post-concussion recovery.47 Hormonal changes during puberty that contribute to migraine headaches might also contribute to sex differences in concussion recovery. Migraine headaches are up to fourfold more common in females than in males after puberty,48, 49 and some evidence suggests that migraineurs recover more slowly after concussion.50, 51 Research is warranted to further delineate sex differences in concussion risk and recovery. In general, adult concussive brain injury is much better understood than its counterpart in children and adolescents. Several points are important to note. First, concussion has multiple, non-harmonized definitions. Second, concussion diagnosis is an imperfect art. Last, in the absence of rapid and inexpensive objective diagnostic measures, concussion remains a clinical diagnosis that is subject to variability—including different thresholds for diagnosis across various subspecialities and across individual physicians, neuropsychologists and athletic trainers—and under-reporting by coaches, parents and young athletes. Without validated diagnostics, concussion will remain a nebulous and under-reported entity, and the accuracy of incidence estimates will continue to be tainted by the differential application of inexact criteria. Repetitive subconcussive trauma can result in structural and functional brain changes.52 White matter abnormalities detected by diffusion tensor imaging (DTI) have been reported in professional soccer players even in the absence of any obvious history of concussions. Compared with swimmers, male professional soccer players showed DTI signal changes suggestive of decreased white matter integrity in several brain regions, which might indicate loss of axonal myelination, similar to changes seen in individuals with mTBI.53 Collegiate ice hockey players exhibited similar white matter changes over the course of a season.54, 55, 56, 57 In addition, repetitive subconcussive head impacts in collegiate American football players have been linked, in a dose-dependent manner, to deficits in BBB integrity, potential loss of white matter integrity, and cognitive dysfunction.58 These findings probably reflect some level of risk for youths who sustain repetitive subconcussive head impacts, although little research has been devoted specifically to this topic. A metric to track head impacts—that is, a 'hit count'—has been proposed,59 and could serve as one factor to determine cumulative risk exposure. One challenge of this approach is to accurately define the parameters of a 'hit', but improved biosensors show some promise in this regard. Similar to a 'pitch count' in baseball, this concept has also recently been proposed for boxers.24 No evidence is currently available to show a causal link between repetitive subconcussive head impacts in youth and dementia later in life, and such metrics could prove invaluable if validated by future studies correlating head impacts with subsequent neuropsychological dysfunction. In adults, TBI, including concussion,60, 61, 62 might increase an individual's risk of developing neurodegenerative disease,63, 64 including AD and chronic traumatic encephalopathy (CTE), a disease associated exclusively with repetitive head trauma.65, 66 TBI may also increase the risk of developing Parkinson disease (PD),67 although the relationship between mTBI and PD risk remains uncertain.68 In paediatric populations, particularly young athletes, the effects of single or repetitive concussions on the risk of later-life neurodegeneration and dementia are unknown. CTE was first described symptomatically in the late 1920s as 'punch-drunk' dementia in boxers,69 was later described as 'dementia pugilistica',70 and was first described pathologically in 1973.71 Since the identification of CTE in a former professional American football player in 2005,72 and additional intensive pathological studies, this condition has gained widespread public attention, and has now been identified in brains of former ice hockey, baseball, rugby and soccer players,73 wrestlers,74 and military veterans.75, 76 The prevalence and incidence of CTE in amateur and professional athletes is still unknown, adding to difficulties in discussing its epidemiology and population risks for athletes. Although CTE is primarily considered to be a neurodegenerative disease that sometimes results from a career of either collegiate or professional contact sports, cases of CTE have been reported in high school athletes.77 This finding suggests that long sporting careers are not required for CTE development, and that youth athletes represent an at-risk population. Emerging evidence suggests that clinical CTE symptoms can be grouped into two common presentations: cognitive and mood–behavioural.78, 79 Subjective memory complaints such as anterograde amnesia are common, as are mood disorders including anxiety or depression,79 and reduced executive function, which can result in disinhibition and impaired decision-making skills.80 These clinical symptoms define disease severity.81 The neurodegenerative pathophysiology of CTE is complex, and the neurological sequelae are poorly understood. In severe cases, the cerebral cortex and medial temporal lobes seem most profoundly affected,81, 82 with pathology characterized by neurofibrillary tangles composed of phosphorylated tau79 and, in some cases, TAR DNA-binding protein 43 pathology.83 CTE is also associated with marked atrophy, notably in the frontal cortex and medial temporal lobe, as well as in the mammillary bodies, thalamus and hypothalamus.79 Confirmed clinical diagnosis of CTE remains autopsy-based.84 Given the uncertainty over whether the tauopathy described in CTE is causative of the clinical phenotype, and the fact that most professional and collegiate athletes do not develop CTE, it is vital to understand whether early exposure to concussion is associated with other forms of neurodegeneration and cognitive dysfunction, including chronic neurocognitive impairment (CNI). Important clinical distinctions exist between CTE and CNI,28, 51 some of which make direct comparisons difficult. CTE is an emerging clinical and pathological condition that involves progressive deterioration of neurological and cognitive function in multiple domains, and is diagnosed primarily at autopsy. Conversely, the CNI phenotype is not necessarily progressive, and is characterized by functional decline from group averages or baseline functioning established before TBI. CNI can be diagnosed clinically through neuropsychological testing. No causal link between CNI and head trauma has yet been confirmed, but a dose-dependent risk has consistently been found in professional athletes.28 In addition, almost half of the studies conducted in amateur athletes have found an elevated risk of CNI.28 Whether similar risk associations are present in younger populations remains to be determined. One hypothesis is that CNI represents a prodromal—but not inevitable—step toward CTE, analogous to the relationship between mild cognitive impairment (MCI) and AD.85, 86 Alternatively, CNI may represent static impairment without degeneration. Our current lack of understanding of the basic biological underpinnings of CNI and CTE underscores the need for more research. Increased knowledge of the biology of both conditions, as well as early detection of CNI in athletes (in particular, youth athletes), may drive interventions to stem the development of further cognitive impairment, and could also aid validation of putative biomarkers. Assessment of CNI via tau imaging may help determine the likelihood of progression to CTE. The field of concussion genetics, especially in paediatric populations, is still in its infancy. Although repetitive head impacts seem necessary for the development of CTE, other factors, including genetics, are likely to have an important role, as most concussed athletes do not develop CTE.87 The genetic risk factors for CTE probably overlap with those that influence susceptibility to and recovery from concussion, and genetic risk factors for AD are providing important clues to the identity of these factors. The ε4 allele of apolipoprotein E (APOE ε4), the most important genetic risk factor for AD identified to date,88 critically affects the CNS injury response,89 in particular, amyloid-β (Aβ) clearance from the brain. The three alleles of APOE confer varying degrees of AD risk: APOE ε2 reduces the risk, APOE ε3, the most common allele, represents baseline risk with which other variants are compared, and APOE ε4 increases the risk.90, 91 Studies suggest an interaction between APOE ε4 and sex, such that APOE ε4-related risk of AD is more prominent in women than in men.92, 93 The APOE genotype acts synergistically with TBI in increasing the risk of AD,94 although its hypothesized risk association with CTE as an outcome of repetitive mTBI requires more study.95 No consensus has yet been reached on the effects of APOE isotype on the outcome of paediatric TBI, but data from adults suggest that APOE ε4 negatively influences concussion outcomes. Several studies indicate that possession of at least one APOE ε4 allele is associated with poorer cognition and lasting neuropsychological impairment after concussion in professional American football players,96 boxers95 and other adults,97, 98, 99, 100 although other studies found no such association.101, 102 Some evidence points to polymorphisms in both the APOE gene and its promoter as contributory factors to concussion risk in college athletes.103, 104 Another study did not identify a role for APOE ε4 in concussion risk,105 although this allele might increase the risk of dementia following midlife or late-life mTBI.106 Drawing conclusions from these conflicting studies is difficult, owing to small sample sizes and differing methodologies. In children, little is known about the relationship between APOE ε4 and neuropsychological outcomes after concussion, and APOE ε4 testing is not routine in paediatric TBI studies. In 2012, Kurowski reviewed the few existing studies and combined the results of three studies107, 108, 109 that used the Glasgow Outcome Scale.110 In the combined sample (252 children), the risk of poor clinical outcomes after 6–12 months was over twofold higher in APOE ε4 carriers than in noncarriers (19% versus 9%). However, these studies included a broad developmental range of children with heterogeneous injuries, and did not account for a possible interaction between age and genotype. In addition, the interaction between APOE and sex has not been studied in the context of concussion. Improved prospective studies are warranted to clarify these connections. Incorporation of genetics into paediatric concussion research is fraught with complicated challenges, including acquisition of parental consent and informed consent for a child, perceived stigmatization of clinical study participants, the actionability of the genetic knowledge obtained, and potential concerns regarding insurability (particularly long-term care insurance). Studies of adults who learn of their APOE ε4+ status demonstrate that many are willing to make lifestyle modifications, including increased exercise and improved medication management,111 as well as increased purchases of health and long-term care insurance.112, 113 Education about new genetic knowledge and corresponding disease risk is essential, as demonstrated by the substantial discordance between an individual's personal feelings about the implications of the acquired knowledge and the actual consequences of increased dementia risk.114 The effects of APOE genetic knowledge on children, their families and decision-making processes regarding participation in impact sports remain unclear. The influence of APOE genotype on concussion risk and recovery in this age group also needs further elucidation. If future studies find that, for any particular level of impact, children with APOE ε4+ status are at greater risk of concussion or poor recovery than are their APOE ε4− peers, consideration should be given to genetic testing of school-age athletes before participation in impact sports. Careful studies of high school and younger athletes are required to fully understand the nuances of genetic influences. Future research into youth concussion outcomes, including cognitive outcomes and risk of dementia, should include APOE genotyping wherever possible. New APOE studies should standardize research methodologies and reporting measures, including the collection of 'common data elements', to ensure valid comparison across studies.110, 115 The APOE genotype is not necessarily a non-modifiable risk factor for concussion recovery: therapies being developed for AD include drugs that modify the interaction between the ApoE4 protein and Aβ, which might also be applicable to paediatric concussion.116, 117 The Val66Met polymorphism in the gene encoding brain-derived neurotrophic factor has been linked to better outcomes after mTBI,118 but worse outcomes after focal penetrating brain injury.119 Polymorphisms in genes involved in dopaminergic signalling may also help to account for the wide range of TBI outcomes.120 In addition, the Rep1 polymorphism in the promoter region of the α-synuclein gene might increase the risk of PD after head injury.121 To advance our understanding of concussion risk and management, large, prospective, population-based genome-wide association studies (GWAS) and whole-genome sequencing studies should be conducted to identify other genetic variants—possibly of low frequency or low penetrance—that modify the risk of prolonged recovery, poor cognitive outcomes or dementia.122 Such studies will require large-scale data sharing, and must address issues of ethics, privacy, and potential implications for insurability and employability. Despite progress in identifying possible cerebrospinal fluid (CSF) and blood-based biomarkers that might be applied to adult TBI management, no clinically validated biomarkers are available for either the adult or the paediatric population. Paediatric concussions present with even greater clinical variability than do adult concussions; therefore, biomarkers have special potential for improving concussion diagnosis in children. Of note, most TBI biomarkers have been studied in the context of moderate to severe TBI, leaving us with obvious gaps in our knowledge of mTBI biomarkers, especially in children. Biomarker development has been critical to the advancement of AD therapeutics. CSF-based biomarkers are already being employed to identify at-risk patients and to improve the design of both epidemiological studies and clinical trials.123 New PET radioligands, such as amyloid-labelling agents (three of which are now FDA-approved), can be used both diagnostically and to improve neuropathology-based patient stratification for clinical trials. Several tau imaging agents are also in human trials, and their utility in tauopathies, including CTE, is rapidly being established. As with fluid-based biomarkers, there are currently no neuroimaging biomarkers sensitive or specific enough to diagnose concussion or CTE in either adults or children. No TBI diagnostic or therapeutic agents have yet been approved by the FDA, and validation of concussion biomarkers could accelerate the development of such agents. Efforts must be made, however, to ensure the cost-effectiveness and wide availability of clinical biomarker testing. Also, given the risks associated with lumbar puncture, ethical concerns regarding sampling of CSF from concussed youths for biomarker research should be addressed. Promising findings in adult fluid-based biomarker research must be explored in paediatric populations. Putative concussion biomarkers have emerged sporadically in the scientific literature over the past few decades, the most prominent being S100 calcium-binding protein B (S100B), a nonspecific marker of astrocyte activation. The presence of S100B in serum may indicate loss of BBB integrity. Elevated serum and CSF levels of S100B have been observed in adult boxers after matches, and correlate positively with the number and severity of head impacts.124, 125 Increased serum S100B levels have also been observed in concussed professional ice hockey players,126 with levels measured 1 h post-concussion predicting symptomatic recovery time. However, S100B levels were also raised after controlled play where no concussions occurred, indicating that this marker is not injury-specific.126 Indeed, S100B serum levels are elevated in adult trauma patients without head injury.127, 128, 129 Other research suggests that initial post-concussion S100B levels are poor predictors of recovery.130 As with all biomarkers, the role of S100B in TBI management in children is even less clear,131 with some arguing that this marker has little diagnostic or prognostic utility in paediatric populations.132 In a study of children with TBI aged ≤15 years, those <5 years or >9 years of age had higher serum levels of S100B than did those aged 5–9 years.133 S100B may, therefore, be an inadequate marker to distinguish between symptomatic and asymptomatic children with concussion,133 and the utility of S100B in diagnostics and outcome prognosis is questionable.134, 135, 136 Neuron-specific enolase (NSE) is a marker of neuronal injury, but its usefulness as a serum or CSF biomarker remains uncertain.133, 134, 135, 136, 137 Elevated serum NSE levels have been observed after head impacts in boxers,124 but were also seen in ice hockey players after a match where no concussions occurred.126 Serum NSE levels failed to predict recovery time after concussion,126 and might not correlate with injury severity in children.133 In children aged ≤15 years, serum NSE levels correlate inversely with age.133 Once released into the blood, NSE has slow elimination kinetics, making it difficult to distinguish primary from secondary neuronal injuries on the basis of NSE levels.138, 139 Neurofilament light chain and glial fibrillary acidic protein (GFAP) are CSF neuron-specific and glial-specific damage markers, respectively, and are both elevated in CSF in adult boxers after fights.125, 137, 140 Little is known about either marker in the context of paediatric concussion, but a preliminary study in children and young adults suggested that serum GFAP levels within 72 h after concussion correlate with symptom burden up to 1 month post injury.141 The neuron-specific protein UCH-L1 (ubiquitin carboxyl-terminal hydrolase isozyme L1) was first linked to neurodegenerative pathology through its involvement in PD,142 and its presence in serum was later identified as a biomarker for severe TBI.143, 144, 145 Serum levels of UCH-L1 may have diagnostic utility in concussion,146 but recent evidence suggests a lack of correlation between elevated serum levels and subconcussive hits.147 The clinical utility of UCH-L1 in paediatric populations warrants further study. Perhaps the most promising advances in adult fluid-based TBI biomarkers concern tau protein. Serum or CSF tau levels are thought to indicate axonal damage, as tau normally resides in axons, where it stabilizes microtubules. Serum tau is proteolytically cleaved,148 and in patients with AD, levels of cleaved tau in CSF might correlate with cognitive function.149 Tau levels in CSF and blood are elevated in boxers after a match, and CSF tau levels correlate with the quality and quantity of head impacts.125, 150 Recent evidence suggests that tau levels are elevated in the blood of ice hockey players after concussion, and may be useful in predicting recovery time.126 Questions remain, however, with several studies reporting little or no value of serum cleaved tau for predicting post-concussion syndrome or long-term outcomes.130, 151 The potential of tau as a biomarker in children remains unclear, with no studies conducted to date. In fact, the reliability of serum tau as a biomarker has not yet been established for any indication. The likelihood is that no single biomarker will suffice to diagnose paediatric concussion or predict outcomes. In addition, few studies have examined the interactions between genetic make-up and putative biomarkers. As our understanding of the relationships of biomarkers to injury severity and to each other increases, development of biomarker panels, perhaps incorporating inflammatory and oxidative markers,152 should be considered. Future studies should attempt to further define these relationships and establish the clinical value of biomarker panels, factoring in commercial cost and practical feasibility. Recent advances in metabolomics, lipidomics and proteomics—in particular, the search for metabolomic and lipidomic markers for AD—might inform future research into biomarkers for concussion and subconcussive injuries. Several recent studies propose altered metabolite and lipid profiles associated with MCI and AD.153, 154, 155, 156 Data from animal models suggest that lipid and metabolite changes accompany both acute and chronic post-concussion periods, and could be useful for predicting recovery trajectory,157, 158 but these findings have yet to be validated in humans. Expanding the biomarker search beyond blood and CSF to saliva and urine159 might improve the ability to obtain measurements rapidly and noninvasively, particularly from children. Sampling of CSF from children, particularly when rapid assessment is desirable, is largely impractical. Mondello et al. proposed a set of useful criteria for evaluating TBI biomarkers that should allow more-streamlined development and validation.137 Any validated biomarker panel must, inevitably, be a component of a larger, multimodal diagnostic suite that may include structural and functional imaging and neuropsychological testing. When designing future biomarker studies, the potential for FDA approval should be considered, in order to expedite approval for clinical use. Although concussion remains a clinical diagnosis, neuroimaging techniques are improving our understanding of the structural and functional consequences in adults. Neuroimaging in paediatric populations may be limited by several factors; for example, measurements of longitudinal changes after concussion are complicated by the background of a dynamic, immature brain. No imaging techniques have been validated as diagnostic tools for concussion, and the correlation between imaging findings and clinically measurable cognitive or behavioural functions is variable. Tools such as volumetric imaging, DTI and functional MRI (fMRI)—in particular, arterial spin labelling—are currently being explored.160, 161 Fractional anisotropy (FA), as measured by DTI, allows inference of the structural integrity of white matter tracts, which are commonly disrupted after TBI. The clinical implications of FA change remain controversial, as both increased and decreased FA has been observed in concussion studies.162, 163, 164, 165, 166 These discrepancies may be due, in part, to the considerable spatial heterogeneity in the brain areas examined,167 as well as differences in the post-injury interval. FA may still have prognostic value, with evidence suggesting that the direction and magnitude of change correlates with clinical outcomes;166, 168 however, this idea awaits validation in both paediatric and adult populations. FA might lack the necessary sensitivity to fully appreciate changes in white matter tract integrity following brain injury, and measures of diffusivity may be more appropriate.169 The DTI field would benefit greatly from the development of normative data sets against which to gauge observed changes. Pre-game versus post-game and season-long studies of young athletes could employ serial DTI imaging to establish normative data for a particular individual, but the utility of the data when pooled is unclear. The scarcity of normative paediatric data severely limits the clinical usefulness of neuroimaging techniques, including DTI. Studies of 'return-to-baseline' neuroimaging after paediatric concussion are also needed, as they could greatly improve prediction of recovery. Although automation has increased reproducibility, DTI measurements remain sensitive to the hardware and software specifics, acquisition parameters and analysis software, which limit reproducibility, standardization and comparison between centres and across studies. Efforts to standardize DTI across imaging centres are underway.170 MRI has been particularly successful in mapping the brain's 'connectome'—the collection of structural and functional neural connectivity networks and their respective focal nodes—and for studying how concussion affects these networks. Focal or diffuse TBI can disrupt the brain's functional connectivity, resulting in dysfunction of multiple networks including the default mode and salience networks, which have been implicated in memory, emotion and mood.171 Network dysfunction might have a stronger influence on recovery than does lesion location,171, 172, 173 but the long-term implications for brain development and cognitive function remain unclear.26, 174 Further studies of network connectivity dysfunction in children after concussion will be critical to improve injury prognostication and management. Radiotracers for PET imaging have the potential to advance the diagnosis and treatment of concussion and CTE, but their use in paediatric populations is purely investigational at present. Three FDA-approved radiolabelled imaging agents are currently available for detecting brain amyloid in patients with suspected AD.175 In adults, some cases of concussion are associated with acute Aβ pathology. PET scanning could enable paediatric patients to be monitored for the presence and persistence of acute post-concussion amyloid, and to determine whether scan positivity and negativity predict different outcomes.176, 177 Other PET imaging agents with potential utility in paediatric populations include new tracers that bind neurofibrillary tangles composed of tau. Early imaging results with 18F-T807, 18F-T808 and 18F-THK5105 are proving to be useful in confirming the presence of tauopathy in various clinical situations, including AD.178, 179, 180 In a recent AD study, the magnitude of tau tracer signal correlated positively with the stage of disease and severity of cognitive impairment.180 A third tau PET tracer, 11C-PBB3, has been tested in healthy individuals and patients with AD, and may be able to detect non-AD conformations of tau.181 In addition, a recent report contains the first description of tauopathy imaging in a living person with suspected sports-associated CTE.177 Given the extent of chronic tau pathology in concussion, repetitive subconcussive injury and CTE, tau tracers may be useful as diagnostic and prognostic biomarkers (for example, to distinguish CNI from CTE). Studies with these tracers in adults with CTE are underway, but their use in paediatric populations will depend on future research to determine whether tau pathology is present in young patients after TBI or concussion. A PET tracer for the microglial cholesterol transporter protein might be useful for imaging of neuroinflammation associated with TBI.182 New PET ligands to image brain microglia, which are being developed with potential utility in neurodegenerative diseases, may also prove useful in concussion and CTE management. Exploration of these PET ligands in paediatric populations with concussion and TBI would be informative, but risk–benefit analyses must be performed before embarking on studies involving radiotracers in this age group. The ultimate utility of any PET imaging agent will depend on its diagnostic and prognostic value as part of a multimodal panel of biomarkers and neuroimaging techniques. Noninvasive techniques such as transcranial magnetic stimulation (TMS) have also uncovered changes in synaptic plasticity following TBI and concussion,183 particularly in asymptomatic individuals.184, 185, 186 Several small TMS studies of young athletes in their early 20s with a history of concussion suggest imbalances in γ-aminobutyric acid and/or glutamate neurotransmission in the motor cortex that are associated with deficits in synaptic long-term potentiation and depression.184, 185, 187, 188 TMS has also revealed that concussion-related impairments in synaptic plasticity can impair aspects of motor learning,188 and that these deficits are detectable decades after an individual's last concussion.189 Another crucial noninvasive tool for detecting neurochemical dysfunction associated with concussion is proton magnetic resonance spectroscopy (MRS). Reports specifically addressing the use of spectroscopy following sports-related concussion suggest various abnormalities consistent with neurochemical alterations.190 In younger (high school) athletes, increased glutamate and glutamine levels were detected by MRS at post-season versus pre-season evaluation, even in players who had not experienced clinically significant concussion during the season.191 Such findings suggest that even subconcussive head impacts can result in the activation of glutamate pathways, implying cellular injury or neuronal death, despite the absence of symptoms. Levels of creatinine and myoinositol (an organic osmolyte located in astrocytes192, 193) were also significantly altered in a subset of the participants in the aforementioned study. In a rare longitudinal study utilizing MRS,194 individuals who sustained a single sports-related concussion exhibited significantly reduced levels of N-acetylaspartate (NAA, a marker of neuronal and axonal health, integrity and functioning195) in the brain 3 days after injury. Levels were increased at 15 days post injury, and reverted to control values at 30 days post injury. By contrast, participants who sustained a second concussion 10–13 days after their initial concussion displayed a prolonged reduction in NAA levels, which had not normalized even 45 days post injury. These results suggest that repeated injury within a short time frame increases the likelihood of protracted or incomplete recovery. In addition to the acute and subacute alterations detected by MRS, other studies of the long-term effects of concussion have disclosed increased myoinositol (associated with glial proliferation) and decreased choline (associated with membrane turnover195) levels in the medial temporal lobe in otherwise healthy former athletes who sustained their last concussion more than three decades prior to testing.196 Another recent study examined a cohort of symptomatic retired National Football League players, using an advanced MRS method called correlated spectroscopy (COSY), which can measure additional metabolites.197 The authors identified increased choline and glutamate–glutamine levels (indicative of diffuse axonal injury and excitotoxicity, respectively), consistent with previous mTBI MRS studies, as well as additional cerebral metabolites that were indicative of neuroinflammatory changes. These metabolic changes may provide insight into mechanisms of injury, such as excitotoxicity and/or inflammation, which could underlie the reported structural changes. Overall, the available data support the use of MRS as a research tool to identify altered neurophysiology and monitor recovery in adult athletes, even following resolution of post-concussive symptoms. At present, MRS-detected biochemical alterations may enhance our understanding of the underlying pathophysiology, but do not yet provide specific diagnostic information. Larger cross-sectional, prospective and longitudinal studies are needed to determine the sensitivity and prognostic value of MRS within the field of sports-related concussion.190 Because the interpretation of MRS in the immature brain requires certain developmental considerations, appropriate comparison samples will be needed for future work in children. MRS techniques with greater spectral resolution, including COSY, might provide additional biochemical specificity.197 Other advances in spatial resolution, such as 3D chemical shift imaging, may also provide greater specificity by allowing the investigation of metabolic alterations throughout the brain rather than in specific regions of interest. Finally, MRS could have a role in measurement of treatment effects, such as those induced by transcranial direct current stimulation198 and TMS.199 The mechanisms and surveillance infrastructure for sports-related injury measurement, reporting, tracking and data sharing are insufficient for current needs and objectives. Concussion research and clinical efforts are hindered by a lack of concussion data across sports and playing levels. A 2014 Institute of Medicine report identified only three national sports injury surveillance systems: the National Electronic Injury Surveillance System—All Injury Program (NEISS-AIP), the National Collegiate Athletic Association Injury Surveillance System (NCAA ISS), and the High School Reporting Injury Online (RIO™).1 These systems can be supplemented with clinical data (for example, from emergency departments, hospitalized inpatients and sports clinics), but these data are biased toward more-severe injuries and patients of higher socioeconomic status. Indeed, schools in rural areas or communities with lower socioeconomic status often have limited access to sports medicine care professionals and facilities. Several emerging programmes may improve surveillance. Regional efforts such as Clinical Outcomes Research Education for Athletic Trainers (CORE-AT) and national efforts such as the National Athletic Trainers' Association National Athletic Treatment, Injury and Outcomes Network (NATA NATION™) attempt to integrate injury tracking with treatment and outcomes data at the high school and collegiate levels. However, none of these systems specifically capture injuries to younger athletes, those participating in non-school sponsored sports, or those at schools without athletic trainers. Sports injury databases also rarely account for demographic factors including socioeconomic status, race or ethnicity, and health-care coverage. Currently, no effective mechanisms exist to consistently and inexpensively link various surveillance data sets, or to follow up individual athletes across sports, tracking systems or the age continuum. There is a considerable need for a system that tracks individual athletes through their playing careers and beyond. Each individual should be tracked for several decades to establish if, when and how a given burden of TBI evolves into CTE, and to assess all the possible negative health outcomes associated with concussion. Such a system would also provide more-accurate descriptions of concussion history and exposure to risk factors, and could capture both short-term and long-term outcomes, including measures of physical and mental health, academic and career success, quality of life and social connectivity, and evolving socioeconomic status. Such efforts are challenged by a variety of issues, including a lack of mandatory reporting of concussion at any level. Mandatory concussion reporting, funding for surveillance efforts, and provision of training to data reporters (for example, coaches and athletic trainers) would greatly improve epidemiological research. However, mandatory reporting will not provide meaningful results without validated, consensus definitions for concussions, and development of a universal data repository and a global unique identifier (GUID) system. Data sets from standardized surveillance efforts could then be linked, thereby improving data sharing for research and clinical care. Coupling of surveillance data with standardized collection, storage and curation infrastructures for biobanking of tissue and fluid samples could dramatically improve injury and outcomes research.200 These efforts might be catalyzed by funding from public–private partnerships, and made actionable by setting realistic short-term and long-term goals to create a multi-year plan. However, in the USA at least, such efforts are currently hampered by misunderstanding of Health Insurance Portability and Accountability Act (HIPAA) regulations and general concerns for athlete confidentiality. Wider use of computerized neurocognitive testing (CNT) for athletes could improve concussion surveillance, as well as diagnosis and management. However, several important challenges must be overcome before CNT becomes routine. These challenges include a lack of standardized administration protocols, the potential for technological errors arising from different computer hardware, limits in the types of cognitive functions assessed, and a lack of qualified test administrators and data interpreters.201 Despite these shortcomings, however, CNT is already used by approximately 40% of US high schools that employ athletic trainers.202 Though not affordable for all schools, CNT could enhance ground-level data collection and aid risk-exposure estimation and post-concussion recovery tracking, as well as increasing the quality of data reported to sports injury surveillance networks. CNT may be also useful in evaluating and tracking post-concussion cognitive improvement or decline, and could have utility in predicting outcomes.203, 204 Whether CNT data collected in the school setting will reach the validation and reproducibility standards achieved by CNT conducted by a clinical research team remains to be seen. Importantly, CNT needs standardization and guidelines for determining 'return to play' and 'return to learn' for athletes who show recovery in one domain but are still symptomatic in others. More research is required on the utility of CNT, both in the clinic and for concussion surveillance and management of youth athletes. In several critical areas, incomplete knowledge hampers meaningful advances in the field of paediatric concussion. At the molecular and cellular levels, research that focuses on axonal damage after concussion and repetitive subconcussive injury is urgently needed to elucidate changes in axonal trafficking and repair, and to better define the role of transient Aβ accumulation as a potential driver of downstream and/or future pathology. Concussion researchers may need to identify more-suitable animal models to study molecular pathology, including tau and its contribution to post-concussion and CTE pathologies, as the structure and organization of the brain differs dramatically in rodents and humans. Without a clearer understanding of how TBI changes the young, still-developing brain, and what pathological events happen in the weeks, months and years following injury, we are left to speculate about the underlying biological bases of such changes. Head impact data collection and risk assessment in youth sports might be improved through use of sensor technologies that record linear and rotational forces. Such commercially available devices, if validated, could determine levels of cumulative head impact forces during games and across seasons of play, and the findings could be linked to neuroimaging data and functional outcome assessments. Combined with 'hit-count' metrics, sensor data may improve knowledge of short-term and long-term neuropsychological outcomes of repetitive subconcussive impacts. Our knowledge of CTE might be improved by understanding baseline rates in the general population, in injured athletes, among uninjured athletes matched by sport and playing positions, and in 'control' athletes in low-risk sports. Improved knowledge of risk exposures could lead to prevention efforts, including practice and competition rule changes. A decades-long, prospective, longitudinal study, following youth athletes through their sporting careers and beyond, would provide more-definitive knowledge of cumulative head impacts and risks of long-term neuropsychological dysfunction and dementia. Such a study is underway in NCAA alumni, who were first studied in 2003 and were re-assessed in 2013.29, 205 Studies in other populations, especially if NIH-funded, would probably begin with a 5-year study that could be renewed in further 5-year increments. Public–private partnerships are likely to be required to secure enough funding to involve multiple study centres. The NCAA has provided partial sponsorship for the 10-year re-assessment of over 100 athletes, but further funding from the NIH, the US Department of Defense (DoD), and private philanthropic sources will be required to extend the range of assessment from neuropsychology, through MRI, to molecular imaging for amyloid, tau and/or inflammation. Ideally, the longitudinal study design should combine epidemiological and interventional trial methodologies and utilize multiple control groups, including non-contact athletes and uninjured impact sport athletes. A longitudinal study would also shed light on the role of cognitive reserve. A precedent for such studies has been established by the late-life dementia research community, using NIH funds and public–private partnerships involving pharmaceutical companies and foundations. For such studies to be successful, additional surveillance systems and data repositories must first be established. Efforts would be accelerated if athletes participating in impact sports had universal access to athletic trainers, who could act as reliable data reporters while promoting safety and providing basic care. In addition, any longitudinal studies must include postmortem analyses to better understand the influence of childhood and young-adult concussions on the development of neurodegenerative pathology and dementia in later life. 'Return-to-play' guidelines are currently hampered by a lack of rigorous epidemiological evidence, and could be greatly improved by long-term safety data from longitudinal studies.206 Longitudinal research could also include studies to determine whether those athletes who fail to follow guidelines experience any negative health effects, such as lingering symptoms or altered risk of incurring a second concussion. The infrastructure for a long-term prospective study might be created through the formation of a research consortium modelled after the Alzheimer's Disease Neuroimaging Initiative (ADNI). ADNI has set standards for data collection, dissemination agreements, testing methodologies, and biomarker collection and analysis. A version of ADNI currently underway with participation of the DoD (ADNI-DoD) is focused on blast-related TBI research in military populations.207 In May 2014, in addition to the NCAA Concussion Study, the NCAA and the DoD announced the launch of the largest prospective sports-related concussion study to date, which will monitor approximately 37,000 NCAA athletes over 3 years. One can envision how this study's infrastructure may eventually be extended to study younger athletes over an extended longitudinal range. Many gaps remain in our knowledge of the biology of TBI, which limit our ability to develop effective drugs. These gaps must be filled if we are to tackle the underlying disease pathology and move beyond treating the symptoms. However, much can be accomplished while research into fundamental TBI biology continues. Drug repurposing involves testing of existing FDA-approved drugs for new indications, and can reduce expense and shorten the path for drug approval. Current repurposing trials include methylphenidate for pain and mental fatigue,208 the dopamine receptor agonist bromocriptine for working memory,209 and the antidepressant sertraline for mood and anxiety, the most frequent neuropsychological complications that influence long-term outcomes after concussion.210 Larger randomized clinical trials should be conducted before these drugs can be introduced into clinical practice for these new indications. In addition, the recent failure of the PROTECT phase III trial of progesterone to improve outcomes after acute TBI211 may serve as a reminder of the need for more research to better understand the fundamental biology underlying TBI. Although many drug repurposing efforts are designed primarily to address concussion symptoms, the drugs may also influence injury pathology and progression. Research on established drugs can also lead to new drug discovery efforts and, potentially, new preventive or management therapeutics. New drugs are urgently needed for TBI and concussions that do not resolve. Drug discovery efforts in the areas of neuroprotection and anti-inflammation are especially relevant because of their potential cross-applicability to neurodegenerative diseases such as AD. Similarly, drugs currently in development for other neurodegenerative diseases might be repositioned for testing in patients with TBI or nonresolving concussion symptoms. As is often the case in medical research, recent advances in concussion research raise as many questions as they answer. Evidence exists for long-term neuropsychological dysfunction and later-life dementia after concussions or repetitive subconcussive head impacts, and more work is needed to better understand the implications and outcomes of youth participation in impact sports. As outlined in this Expert Consensus Document, there is a path forward, but achieving the goals outlined here will require public and private sector cooperation. While recommendations can be improved with increased knowledge, the available evidence can still inform individual decision-making when considering youth sport participation, as well as practice policies and competition rules. With an ageing population and a looming epidemic of dementia, we must learn more about potential early-life risk factors, including sports-related concussion. The choices made by parents, coaches, school boards and children will be better informed when the critical gaps in scientific knowledge of concussion are filled. Download references|与运动相关的脑震荡和重复性亚震荡暴露越来越被认为是儿科人群的潜在危险，但是对于这些事件的短期和长期后果，包括潜在的认知障碍和晚年痴呆的风险，仍然知之甚少。这份专家共识文件是由全球安全儿童、阿尔茨海默氏症药物发现基金会和安德鲁斯矫形外科和运动医学研究所召集的为期一天的会议的结果。目标是强调在脑震荡科学、痴呆症、遗传学、诊断和预后生物标志物、神经影像学、运动损伤监测和信息共享等领域的知识差距和亟需研究的领域。针对这些领域，我们提出了明确和可实现的途径，以提高对青少年体育相关脑震荡的理解、治疗和预防。2009年，美国年龄 < 19岁的个体中记录了约250,000例非致命性创伤性脑损伤(TBI)。1疾病控制和预防中心估计，5-18岁的年轻人维持着所有运动相关脑震荡的65% 。2尽管最近在诊断性脑成像方面取得了进展，并且在我们对脑震荡物理学的理解方面，长期的认知结果仍然知之甚少。由于脑震荡的身体、认知和情感后果引起了公众的广泛关注，我们对如何预防、诊断和治疗这种伤害的不完整知识危及我们儿童的总体健康，特别是他们的大脑健康。这份专家共识文件是儿科和成人创伤性脑损伤、阿兹海默病(AD)研究、遗传学、流行病学、生物伦理学和运动医学领域专家为期一天的会议的结果(专栏1) ，该会议于2013年11月由全球安全儿童、阿尔茨海默氏症药物发现基金会和安德鲁斯矫形外科和运动医学研究所召集。我们的主要目标是强调我们在儿童和青少年脑震荡知识方面的重大差距。我们强调需要进行研究的领域，如开发诊断和预测性生物标志物，阐明遗传风险因素，以及预测短期和长期结果。在我们的结论中，我们提出了提高我们对与运动相关的儿童脑震荡的长期后果的理解的途径。术语“脑震荡”经常与术语“轻度 TBI”(mTBI)交替使用，考虑到脑震荡后可能的脑损伤程度和慢性神经心理功能障碍的潜在可能性，这是一种潜在的误导性做法。然而，我们应该强调的是，大多数脑震荡不会产生后遗症。美国康复医学会将 mTBI 定义为格拉斯哥昏迷量表3评分为13-15分，意识丧失 < 30分钟，创伤后遗忘持续时间 < 24小时。脑震荡描述了损伤表型的异质混合物，取决于许多因素，包括头部撞击的大小，位置和方向。尽管缺乏宏观结构发现，脑震荡损伤涉及由线性和旋转剪切力破坏轴突和膜功能(弥漫性轴突损伤，5离子通量和谷氨酸兴奋毒性)引起的原发性神经元损伤，随后是继发性病理生理效应，包括线粒体氧化应激，脑血流中断，血脑屏障(BBB)完整性受损，突触功能障碍和神经炎症。持续的神经心理学脑震荡后症状(脑震盪症候群)包括情绪障碍(例如抑郁症) ，难以集中和记忆问题(方框2)。年轻运动员的脖子和躯干比老年人的脖子和躯干更弱，因此，造成脑损伤所需的力量更少。发育中的大脑也可能特别容易受到由头部创伤的剪切力引起的轴突损伤，这在美国青年足球中可以超过100g 的线性加速力。然而，青年运动中持续的平均力量通常会小于较高水平的运动。正确的突触发育对认知和行为健康至关重要。神经发生、竞争性突触消除(“修剪”)、髓鞘形成、轴突和树突树枝化等过程在产前发育的整个生命周期中持续进行。额叶和颞叶是最后成熟的区域，人类在20岁出头的时候经历了这些区域的修剪[16] ，因此这些仍在发育的区域的损伤可能对大脑产生病理生理效应，增加了以后生活中出现神经心理问题的可能性。轴突髓鞘形成在青春期持续到20岁出头，易受损伤的影响。职业拳击手大脑健康研究的早期结果表明，第一次接触拳击比赛的年龄越早，尾状核体积损失越大，额叶轴突损伤越严重。这项研究对拳击手和追踪研究综合格斗拳击手进行了5年的研究，他们都经历过重复性的脑震荡和脑震荡。23,24年轻的大脑也有一些有助于恢复的特征。已经显示，这个年龄组的神经可塑性增加有助于局灶性损伤后更好的结果[25]。此外，发育中的动物对重复 TBI 的葡萄糖代谢障碍的窗口比成年动物更短[26]。总的来说，发育中的大脑在 TBI 后显示出脆弱性和恢复力。这些相互交织的因素可能解释了脑震荡和重复 mTBI 对年轻人和成年人大脑影响的差异。应高度重视对脑震荡风险采取保守的方法，并加大努力调查这些发育差异。大多数人ーー无论老少ーー从脑震荡中完全恢复过来。在儿童中，可能影响康复的因素包括年龄和脑震荡史。27,28在一项研究中，大约90% 的年轻成年男运动员在21天内经历了症状恢复。然而，在一项针对11-22岁患者(包括所有脑震荡原因，而不仅仅是运动相关)的急诊科研究中，15% 的样本在受伤后90天仍然表现出脑震荡后症状，包括头痛，头晕，“精神模糊”和抑郁。一些研究表明，美国高中橄榄球运动员从脑震荡中恢复的速度比大学运动员和职业运动员要慢。尽管最近一项包括青春期前年龄组(11-12岁)的研究表明，脑震荡后恢复持续时间可能与年龄没有线性关系，但与高中以下青少年的直接比较尚未发表[30] ，因为这个样本中的青少年恢复时间比青春期前的儿童更长。这些发现加在一起，意味着男性青春期年龄组的恢复时间较长的独特风险。对年幼儿童和女性的进一步研究将大大提高我们评估和减轻整个儿科和青少年年龄段风险的能力。在新的脑震荡发生前1年内遭受一次或多次脑震荡的青少年报告出现更长时间的症状，30表明可能存在“脆弱性窗口”，并将先前受伤的青少年置于更高的长期恢复风险中。11-18岁的青少年在急诊室出现脑震荡后发生脑震盪症候群的可能性比5-10岁的儿童高出近80% ，同样，伴有头痛的儿童和青少年出现脑震盪症候群的风险增加了一倍。在 mtBI 后在急诊室接受治疗的儿童中，6岁以上的儿童在受伤后3个月报告持续症状的发生率高于6岁以下的儿童。当然，获得 < 6岁儿童脑震荡症状的准确信息的能力可能受到缺乏症状自我意识和有效沟通这些症状的必要语言技能的限制。此外，从这些报告中不可能直接比较损伤的严重程度; 事实上，各种损伤的身体异质性，加上个体从脑震荡中恢复的先天能力，使得这种比较具有高度挑战性。一些专业研究中心正在使用“智能头盔”来标准化头部撞击产生的体力和角加速度，目前正在研究这些头盔用于测量和预测可能导致脑震荡的影响。36,37从脑震荡中恢复的年轻人可能会经历重大挑战，包括社会和学术发展的改变，38,39,40在一般智力测试中得分较低，以及学校表现下降(以年级平均分衡量)。39较低的父母教育水平和儿童学业成绩都与较差的脑震荡恢复相关。人格特质也起到了一定的作用，例如，伤前焦虑是运动性脑震荡后长时间恢复的一个危险因素。42年轻的男女运动员都有脑震荡的危险，但是女孩的脑震荡发生率高于男孩，特别是在高中和大学的足球、篮球、棒球或垒球比赛中。28,43,44,45解释这些差异的因素仍然不确定，但可能包括保护装备的质量，脑震荡症状的识别和报告，以及颈部长度和颈部肌肉力量。46男女之间在恢复轨迹方面的差异也知之甚少。然而，最近的一项研究表明，女性黄体酮水平影响脑震荡后的恢复。47青春期激素变化导致偏头痛，也可能导致脑震荡后恢复的性别差异。在青春期后，女性偏头痛的发病率是男性的四倍[48,49] ，一些证据表明，偏头痛患者在脑震荡后恢复较慢[50,51]。有必要进一步研究脑震荡风险和恢复的性别差异。一般来说，成人脑震荡比儿童和青少年脑震荡更容易理解。有几点值得注意。首先，脑震荡有多种非协调的定义。其次，脑震荡诊断是一门不完善的艺术。最后，在缺乏快速和廉价的客观诊断措施的情况下，脑震荡仍然是一种临床诊断，受到变异性的影响，包括不同亚专业和个体医生、神经心理学家和运动训练员的诊断阈值不同，以及教练、家长和年轻运动员报告不足。如果没有经过验证的诊断，脑震荡将仍然是一个模糊和报告不足的实体，发病率估计的准确性将继续受到不确切标准的差别应用的影响。重复性次级脑震荡可导致大脑结构和功能的改变。52弥散张量成像(DTI)检测到的白质异常在职业足球运动员中已有报道，即使没有任何明显的脑震荡史。与游泳运动员相比，男性职业足球运动员表现出 DTI 信号改变，提示几个大脑区域的白质完整性降低，这可能表明轴突髓鞘形成的丧失，类似于 mTBI 患者的改变。53名大学冰球运动员在一个赛季中表现出类似的白质变化。54,55,56,57此外，美国大学生橄榄球运动员重复性亚震荡性头部撞击已经以剂量依赖性方式与 BBB 完整性缺陷，白质完整性潜在丧失和认知功能障碍有关。58这些研究结果可能反映了持续遭受重复性次生脑震荡撞击的青少年的某种程度的风险，尽管很少有专门针对这一主题的研究。一个跟踪头部影响的指标ーー即“命中次数”ーー已经提出，59可以作为确定累积风险敞口的一个因素。这种方法的一个挑战是准确定义“命中”的参数，但改进的生物传感器在这方面显示出一些希望。与棒球中的“投球次数”类似，这个概念最近也被提出用于拳击运动员。24目前没有证据表明青少年重复性脑震荡冲击与晚年痴呆之间的因果关系，如果未来的研究将头部冲击与随后的神经心理功能障碍相关联，这些指标可能被证明是无价的。在成年人中，包括脑震荡在内的脑外伤可能会增加个体发生神经退行性疾病的风险，包括 AD 和 CTE (CTE) ，这是一种仅与重复性头部创伤相关的疾病[65,66]。尽管 mTBI 和 PD 风险之间的关系仍然不确定，但 TBI 也可能增加发生帕金森氏症的风险[67]。在儿科人群，特别是年轻运动员中，单次或重复性脑震荡对晚年神经退行性疾病和痴呆风险的影响是未知的。CTE 在20世纪20年代后期首次被症状性描述为拳击运动员的“拳击醉”痴呆，69后来被描述为“痴呆拳击”[70] ，并在1973年首次被病理学描述[71]。自2005年在一名前职业美式足球运动员身上发现 CTE 以来,这种病症已经引起了公众的广泛关注，目前已经在前冰球、棒球、橄榄球和足球运动员、73名摔跤运动员、74名退伍军人的大脑中发现。75,76业余和职业运动员慢性创伤性脑病的患病率和发病率仍然是未知的，这增加了讨论其流行病学和运动员的人口风险的困难。虽然慢性创伤性脑病主要被认为是一种神经退行性疾病，有时是由大学或专业接触性运动的职业生涯造成的，但在高中运动员中也有慢性创伤性脑病的报道。这一发现表明，慢性创伤性脑病的发展并不需要长期的运动生涯，青年运动员代表着高危人群。新出现的证据表明，临床的慢性创伤性脑病症状可以分为认知和情绪行为两种常见表现[78,79]。主观记忆症状如顺行性遗忘症是常见的，包括焦虑或抑郁在内的情绪障碍也是常见的[79] ，并且执行功能降低，这可能导致去抑制和决策技能受损[80]。这些临床症状定义了疾病的严重程度[81]。慢性创伤性脑病的神经退行性病理生理学是复杂的，对神经系统后遗症的了解很少。在严重的情况下，大脑皮层和内侧颞叶似乎受到最深刻的影响，81,82与病理学拥有属性由磷酸化 tau79组成的神经原纤维缠结，在某些情况下，TAR DNA 结合蛋白43病理学。CTE 也与明显的萎缩有关，特别是在额叶皮层和内侧颞叶，以及在乳头体，丘脑和下丘脑。79确诊的 CTE 临床诊断仍以尸检为基础。鉴于慢性脑震荡中描述的重复病变是否引起临床表型的不确定性，以及大多数专业和大学运动员不发展慢性脑震荡的事实，了解早期暴露于脑震荡是否与其他形式的神经退行性疾病和认知功能障碍(包括慢性神经认知障碍(CNI))相关至关重要。CTE 和 CNI 之间存在重要的临床区别，其中一些使得直接比较困难。CTE 是一种新出现的临床和病理状况，涉及多个领域的神经和认知功能的进行性恶化，主要在尸检中诊断。相反，CNI 表型并不一定是进行性的，而是拥有属性功能从组平均值或基线功能下降到创伤性脑损伤之前的水平。CNI 可以通过神经心理测试进行临床诊断。CNI 与头部创伤之间的因果关系尚未得到证实，但在专业运动员中一直发现剂量依赖性风险。此外，在业余运动员中进行的几乎一半的研究发现 CNI 的风险升高。年轻人群中是否存在类似的风险关联仍有待确定。一个假设是 CNI 代表了慢性创伤性脑病的前驱症状，但并非不可避免，类似于轻微认知障碍和 AD 之间的关系。另外，CNI 可能代表静态损伤而不退化。我们目前对 CNI 和 CTE 的基本生物学基础缺乏了解，这强调了进一步研究的必要性。对这两种情况的生物学知识的增加以及运动员(特别是青年运动员) CNI 的早期检测可能会推动干预措施以阻止进一步认知障碍的发展，并且还可能有助于验证推定的生物标志物。通过 tau 成像评估 CNI 可能有助于确定进展为 CTE 的可能性。脑震荡遗传学领域，特别是在儿科人群中，仍然处于起步阶段。尽管重复的头部撞击似乎对于 CTE 的发展是必要的，但是包括遗传学在内的其他因素可能具有重要作用，因为大多数脑震荡运动员不发展 CTE.87 CTE 的遗传危险因素可能与影响脑震荡易感性和恢复的因素重叠，AD 的遗传危险因素为这些因素的身份提供了重要的线索。E型载脂蛋白质的 ε4等位基因(APOEε4)是迄今为止发现的 AD 最重要的遗传危险因素，它严重影响中枢神经系统的损伤反应，特别是从大脑中清除淀粉样蛋白 -β (Aβ)。APOE 的三个等位基因赋予不同程度的 AD 风险: APOEε2降低风险，APOEε3是最常见的等位基因，代表与其他变体进行比较的基线风险，APOEε4增加风险。90,91研究表明 APOEε4与性别之间存在相互作用，因此 APOEε4相关的 AD 风险在女性中比在男性中更为突出。92,93 APOE 基因型与 TBI 协同作用增加 AD 的风险[94] ，尽管其与 CTE 作为重复 mTBI 的结果的假设风险相关性需要更多的研究。关于 APOE 同种型对儿童 TBI 结果的影响尚未达成共识，但来自成年人的数据表明 APOEε4对脑震荡结果有负面影响。一些研究表明，拥有至少一个 APOEε4等位基因与美国职业橄榄球运动员，96名拳击运动员95和其他成年人97,98,99,100的脑震荡后认知较差和持续的神经心理障碍有关，尽管其他研究没有发现这种关联。101,102一些证据表明 APOE 基因及其启动子的多态性是大学生运动员脑震荡危险的促成因素。另一项研究没有确定 APOEε4在脑震荡风险中的作用[105] ，尽管这个等位基因可能增加中年或晚年 mTBI 后痴呆的风险。106由于样本量小，方法不同，很难从这些相互矛盾的研究中得出结论。在儿童中，对于 APOEε4与脑震荡后神经心理学结果之间的关系知之甚少，而且 APOEε4测试在儿科 TBI 研究中并不常规。2012年，Kurowski 回顾了少数现有的研究，并结合了使用格拉斯哥结果量表的三项研究的结果[107,108,109]。在合并样本(252名儿童)中，6-12个月后不良临床结果的风险在 APOEε4携带者中高于非携带者(19% 比9%)。然而，这些研究包括了广泛的异质性损伤儿童的发育范围，并没有考虑到年龄和基因型之间可能的相互作用。此外，APOE 与性别之间的相互作用尚未在脑震荡的背景下进行研究。改进的前瞻性研究有助于澄清这些联系。将遗传学纳入儿科脑震荡研究充满了复杂的挑战，包括获得父母同意和儿童的知情同意，临床研究参与者的感知耻辱，获得的遗传知识的可行性以及关于可保性(特别是长期护理保险)的潜在担忧。对了解 APOEε4 + 状态的成年人的研究表明，许多人愿意改变生活方式，包括增加运动和改善药物管理[111] ，以及增加购买健康和长期护理保险[112,113]。关于新的遗传知识和相应的疾病风险的教育是必不可少的，正如个人对获得的知识的影响的个人感觉与痴呆风险增加的实际后果之间的实质性不一致所证明的那样.114 APOE 遗传知识对儿童，其家庭和参与影响性体育的决策过程的影响尚不清楚。APOE 基因型对该年龄组脑震荡风险和恢复的影响也需要进一步阐明。如果未来的研究发现，对于任何特定水平的影响，具有 APOEε4 + 状态的儿童比其 APOEε4同龄人具有更大的脑震荡或恢复不良的风险，则应考虑在参加影响性运动之前对学龄运动员进行基因检测。要充分理解基因影响的细微差别，就需要对高中和年轻运动员进行仔细研究。未来对青少年脑震荡结果(包括认知结果和痴呆风险)的研究应尽可能包括 APOE 基因分型。新的 APOE 研究应标准化研究方法和报告措施，包括收集“共同数据元素”，以确保有效的比较研究。110,115 APOE 基因型不一定是脑震荡恢复的不可改变的危险因素: 正在开发的 AD 治疗包括改变 ApoE4蛋白和 Aβ 之间相互作用的药物，这也可能适用于儿科脑震荡。编码脑源性神经营养因子的基因中的 Val66Met 多态性与 mtBI 后更好的结果有关，但与局灶性穿透性脑损伤后更差的结果有关。参与多巴胺能信号传导的基因多态性也可能有助于解释广泛的 TBI 结果。120此外，α-synuclein 基因启动子区的 Rep1多态性可能增加头部损伤后帕金森病的风险。为了提高我们对脑震荡风险和管理的理解，应该进行大型的前瞻性基于人群的全基因组关联研究(GWAS)和全基因组测序研究，以确定其他遗传变异(可能是低频率或低外显率) ，这些变异可以改变长期恢复，认知结果差或痴呆的风险。122这样的研究将需要大规模的数据共享，并且必须解决道德、隐私以及对可保性和可雇佣性的潜在影响等问题。尽管在确定可能应用于成人创伤性脑损伤治疗的可能的脑嵴液(CSF)和血液生物标志物方面取得了进展，但成人或儿科人群都没有经过临床验证的生物标志物。与成人脑震荡相比，儿童脑震荡的临床变异性更大; 因此，生物标志物在改善儿童脑震荡诊断方面具有特殊的潜力。值得注意的是，大多数 TBI 生物标志物已经在中度至重度 TBI 的背景下进行了研究，这使我们在 mTBI 生物标志物的知识方面存在明显的差距，特别是在儿童中。生物标志物的发展对 AD 治疗的进步至关重要。基于脑脊液的生物标志物已经被用于识别高危患者，并改善流行病学研究和临床试验的设计。123新的 PET 放射性配体，如淀粉样蛋白标记剂(其中三种现在是 FDA 批准的) ，可以用于诊断和改善基于神经病理学的患者临床试验分层。一些 tau 成像剂也在人体试验中，它们在包括 CTE 在内的 tau 病中的应用正在迅速建立。与基于液体的生物标志物一样，目前还没有足够敏感或特异的神经影像生物标志物来诊断成人或儿童的脑震荡或 CTE。目前 FDA 尚未批准任何创伤性脑损伤的诊断或治疗药物，而脑震荡生物标志物的验证可以加速这类药物的开发。然而，必须努力确保临床生物标志物检测的成本效益和广泛可用性。此外，考虑到与腰椎穿刺相关的风险，对脑震荡青少年脑脊液取样用于生物标志物研究的伦理问题应该得到解决。在成人体液为基础的生物标志物研究中有希望的发现必须在儿科人群中探索。过去数十年，推定脑震荡的生物标志物在科学文献中零星出现，其中最突出的是星形胶质细胞活化的非特异性标志物 S100钙结合蛋白 B (S100B)。血清中 S100B 的存在可能提示血脑屏障完整性的丧失。在成年拳击手比赛后观察到血清和脑脊液 S100B 水平升高，并且与头部撞击的数量和严重程度呈正相关。在脑震荡的职业冰球运动员中也观察到血清 S100B 水平升高，126在脑震荡后1小时测量的水平预测症状恢复时间。然而，S100B 的水平也提高后，控制发挥，没有发生脑震荡，表明这一标志物是不伤害特异性。事实上，没有头部损伤的成年创伤患者血清 S100B 水平升高。127,128,129其他研究表明，脑震荡后最初的 S100B 水平对于恢复不能很好地预测。与所有生物标志物一样，S100B 在儿童 TBI 管理中的作用甚至更不清楚[131] ，一些人认为这种标志物在儿科人群中几乎没有诊断或预后效用。132在一项关于≤15岁 TBI 患儿的研究中，5岁以下或9岁以上儿童的血清 S100B 水平高于5-9岁儿童。因此，S100B 可能不足以区分有症状和无症状的脑震荡儿童[133] ，S100B 在诊断和预后预后方面的效用是值得怀疑的。134,135,136神经元特异性烯醇化酶(NSE)是神经元损伤的标志物，但其作为血清或脑脊液生物标志物的用途仍不确定。拳击手头部撞击后观察到血清 NSE 水平升高[133,134,135,136,137] ，但在没有发生脑震荡的比赛后，冰球运动员也观察到 NSE 水平升高。血清 NSE 水平无法预测脑震荡后的恢复时间，可能与儿童损伤严重程度无关。133在≤15岁的儿童中，血清 NSE 水平与年龄呈负相关。一旦释放到血液中，NSE 具有缓慢的消除动力学，使得难以根据 NSE 水平区分原发性和继发性神经元损伤。神经丝轻链和胶质纤维酸性蛋白(GFAP)分别是 CSF 神经元特异性和胶质特异性损伤标志物，并且在成年拳击手打斗后 CSF 均升高。125,137,140在儿科脑震荡的情况下，对任何一种标志物都知之甚少，但对儿童和年轻成年人的初步研究表明，脑震荡后72小时内的血清 GFAP 水平与损伤后1个月的症状负担相关。神经元特异性蛋白 UCH-L1(泛素羧基末端水解酶同工酶 L1)首先通过参与 PD 与神经退行性病理学相关[142] ，其在血清中的存在后来被确定为严重 TBI 的生物标志物。血清 UCH-L1水平可能对脑震荡有诊断价值[146] ，但最近的证据表明血清水平升高与脑震荡次数之间缺乏相关性。UCH-L1在儿科人群中的临床应用值得进一步研究。也许最有希望的进展成人液基 TBI 生物标志物涉及 tau 蛋白。血清或脑脊液 tau 蛋白水平被认为表明轴突损伤，因为 tau 蛋白通常存在于轴突中，稳定微管。在 AD 患者中，脑脊液中切割的 tau 蛋白水解水平可能与认知功能相关。拳击手在比赛后脑脊液和血液中的 Tau 水平升高，脑脊液 Tau 水平与头部撞击的质量和数量相关。125,150最近的证据表明，脑震荡后冰球运动员血液中的 tau 水平升高，可能有助于预测恢复时间。然而，问题依然存在，一些研究报道血清切割 tau 对预测脑震盪症候群或长期结果的价值很小或没有价值。130,151 tau 作为儿童生物标志物的潜力尚不清楚，至今没有进行研究。事实上，血清 tau 作为一种生物标志物的可靠性尚未被确定为任何适应症。这种可能性是没有单一的生物标志物将足以诊断儿童脑震荡或预测结果。此外，很少有研究调查遗传组成和推定的生物标志物之间的相互作用。随着我们对生物标志物与损伤严重程度及其相互关系的理解的增加，生物标志物小组的发展，可能包括炎症和氧化标志物，152应该被考虑。未来的研究应试图进一步确定这些关系，建立生物标志物小组的临床价值，考虑到商业成本和实际可行性。代谢组学、脂质组学和蛋白质组学的最新进展ーー特别是寻找 AD 的代谢组学和脂质组学标志物ーー可能为今后研究脑震荡和脑震荡下损伤的生物标志物提供参考。最近的一些研究提出了与 MCI 和 AD 相关的代谢物和脂质谱的改变.153,154,155,156来自动物模型的数据表明，脂质和代谢物变化伴随着急性和慢性脑震荡后期，并且可能有助于预测恢复轨迹，157,158但是这些发现尚未在人类中得到验证。将生物标志物的搜索范围从血液和脑脊液扩展到唾液和尿液159，可能会提高快速和非侵入性测量的能力，特别是从儿童身上。从儿童抽取脑脊液样本，特别是在需要快速评估的情况下，在很大程度上是不切实际的。Mondello 等人提出了一套评估 TBI 生物标志物的有用标准，这些标准应该允许更精简的开发和验证.137任何经过验证的生物标志物小组必然是更大的多模式诊断套件的组成部分，其中可能包括结构和功能成像以及神经心理学测试。在设计未来的生物标志物研究时，应考虑 FDA 批准的可能性，以加快批准临床使用。虽然脑震荡仍然是一种临床诊断，但神经影像学技术正在提高我们对成人脑结构和功能后果的认识。儿科人群的神经影像学可能受到几个因素的限制，例如，脑震荡后纵向变化的测量由于动态的、未成熟的大脑的背景而变得复杂。没有成像技术被证实为脑震荡的诊断工具，成像结果与临床可测量的认知或行为功能之间的相关性是可变的。目前正在研究容积成像、 DTI 和功能磁共振成像(fMRI)等工具，特别是动脉自旋标记。通过 DTI 测量的分数各向异性(FA)可以推断白质束的结构完整性，TBI 后白质束通常被破坏。FA 变化的临床意义仍然存在争议，因为在脑震荡研究中观察到 FA 增加和减少[162,163,164,165,166]。这些差异可能部分是由于所检查的脑区域的相当大的空间异质性[167]以及损伤后间隔的差异。FA 可能仍然具有预后价值，有证据表明变化的方向和幅度与临床结果相关; 然而，这个想法等待在儿科和成人人群中验证。FA 可能缺乏必要的敏感性来充分评估脑损伤后白质束完整性的变化，扩散率的测量可能更合适。169 DTI 领域将大大受益于规范数据集的开发，以衡量观察到的变化。年轻运动员的赛前、赛后和赛季研究可以采用连续 DTI 成像技术为特定个体建立规范的数据，但数据汇总后的效用尚不清楚。儿科标准数据的缺乏严重限制了包括 DTI 在内的神经影像技术的临床应用。儿童脑震荡后的“回归基线”神经影像学研究也是必要的，因为它们可以极大地改善恢复的预测。尽管自动化提高了重复性，但 DTI 测量仍然对硬件和软件特异性，采集参数和分析软件敏感，这限制了重复性，标准化和中心之间以及跨研究之间的比较。标准化 DTI 成像中心的努力正在进行中。170 MRI 在绘制大脑的“连接体”(结构和功能神经连接网络及其各自的焦点节点的集合)以及研究脑震荡如何影响这些网络方面特别成功。局灶性或弥漫性 TBI 可以破坏大脑的功能连接，导致多个网络的功能障碍，包括默认模式和显着网络，这与记忆，情绪和情绪有关[171]。网络功能障碍对恢复的影响可能比病变部位更强[171,172,173] ，但对大脑发育和认知功能的长期影响尚不清楚[26,174]。脑震荡后儿童网络连接功能障碍的进一步研究对于改善损伤预后和管理至关重要。用于 PET 成像的放射性示踪剂有可能推进脑震荡和 CTE 的诊断和治疗，但目前它们在儿科人群中的应用纯粹是研究性的。三种 FDA 批准的放射性标记成像剂目前可用于检测疑似 AD 患者的脑淀粉样蛋白。175在成年人中，一些脑震荡病例与急性 Aβ 病理有关。PET 扫描可以使儿科患者监测急性脑震荡后淀粉样蛋白的存在和持续性，并确定扫描阳性和阴性是否预测不同的结果.176,177在儿科人群中具有潜在用途的其他 PET 成像剂包括结合由 tau 组成的神经原纤维缠结的新示踪剂。用18F-T807,18F-T808和18F-THK5105进行的早期成像结果证明对于确认包括 AD 在内的各种临床情况下存在共病是有用的。178,179,180在最近的一项 AD 研究中，tau 示踪信号的大小与疾病的分期和认知障碍的严重程度呈正相关。第三种 tau PET 示踪剂11C-PBB3已经在健康个体和 AD 患者中进行了测试，并且可能能够检测 tau 的非 AD 构象。181此外，最近的一份报告首次描述了疑似与运动相关的慢性创伤性脑病(CTE)在活人中的重病影像学表现。鉴于脑震荡，重复性亚震荡损伤和 CTE 中慢性 tau 病理学的程度，tau 示踪剂可用作诊断和预后生物标志物(例如，区分 CNI 和 CTE)。目前正在对 CTE 成人进行这些示踪剂的研究，但它们在儿科人群中的应用将取决于未来的研究，以确定 TBI 或脑震荡后年轻患者是否存在 tau 病理学。小胶质细胞胆固醇转运蛋白的 PET 示踪剂可能有助于成像与创伤性脑损伤相关的神经炎症。182正在开发的新型 PET 配体可以成像脑小胶质细胞，对神经退行性疾病具有潜在的应用价值，也可能证明对脑震荡和慢性创伤性脑病的治疗有用。在脑震荡和 TBI 的儿科人群中探索这些 PET 配体将是有益的，但是在开始进行涉及该年龄组放射性示踪剂的研究之前必须进行风险-效益分析。任何 PET 成像剂的最终效用将取决于其作为多模式生物标志物和神经影像技术小组的一部分的诊断和预后价值。非侵入性技术如经颅磁力刺激(tMS)也发现了创伤性脑损伤和脑震荡后突触可塑性的变化，特别是在无症状的个体中。对20多岁有脑震荡史的年轻运动员进行的几项小型 TMS 研究表明，运动皮层中 γ-氨基丁酸和/或谷氨酸神经传导的不平衡与突触长时程增强作用和抑郁症的缺陷有关。184,185,187,188经颅磁刺激还显示，脑震荡相关的突触可塑性损伤可以损害运动学习的各个方面，这些缺陷在个体最后一次脑震荡几十年后仍然可以检测到。另一个检测与脑震荡相关的神经化学功能障碍的关键非侵入性工具是质子磁共振谱(MRS)。专门针对运动相关脑震荡后使用光谱学的报告表明，与神经化学改变一致的各种异常。在年轻(高中)运动员中，MRS 在赛季后与赛季前评估中检测到谷氨酸和谷氨酰胺水平增加，即使在赛季期间没有经历临床显着脑震荡的运动员中也是如此。这些发现表明，即使是次震荡性头部撞击也可能导致谷氨酸途径的激活，意味着细胞损伤或神经元死亡，尽管没有症状。在上述研究中，一部分参与者的肌酐和肌醇水平(位于星形胶质细胞中的有机渗透液192,193)也发生了显著变化。在一项使用 MRS 的罕见追踪研究中，194名持续单次运动相关脑震荡的个体在受伤后3天在大脑中表现出显着降低的 N- 乙酰天冬氨酸(NAA，神经元和轴突健康，完整性和功能的标志物195)水平。损伤后15天水平升高，损伤后30天恢复到对照值。相比之下，在第一次脑震荡后10-13天再次受到脑震荡的参与者表现出 NAA 水平的长时间下降，即使在受伤后45天也没有恢复正常。这些结果表明，在短时间内反复受伤增加了延长或不完全恢复的可能性。除了 MRS 检测到的急性和亚急性改变之外，其他关于脑震荡长期影响的研究已经揭示了在其他健康的前运动员中，内侧颞叶中肌醇(与胶质增殖相关)增加和胆碱(与膜转换相关195)水平降低在测试之前持续最后一次脑震荡超过三十年。196最近的另一项研究使用一种叫做相关光谱学(COSY)的先进的 MRS 方法，检测了一组有症状的退役国家橄榄球联盟球员，这种方法可以测量额外的代谢物。作者发现胆碱和谷氨酸-谷氨酰胺水平升高(分别表明弥漫性轴突损伤和兴奋性毒性) ，与之前的 mtBI MRS 研究一致，以及额外的大脑代谢物表明神经炎症的变化。这些新陈代谢的变化可能提供了损伤机制的洞察力，如兴奋性毒性和/或炎症，这可能是所报道的结构变化的基础。总的来说，现有的数据支持使用 MRS 作为一种研究工具，以确定改变的神经生理学和监测恢复成年运动员，即使在解决后脑震荡症状。目前，MRS 检测到的生化改变可以增强我们对潜在病理生理学的理解，但尚不能提供具体的诊断信息。需要更大的横断面，前瞻性和纵向研究来确定 MRS 在运动相关脑震荡领域内的敏感性和预后价值.190由于未成熟大脑中 MRS 的解释需要某些发育方面的考虑，因此将来在儿童中的工作将需要适当的比较样本。具有更高光谱分辨率的 MRS 技术，包括 COSY，可能提供额外的生化特异性。空间分辨率的其他进展，如3D 化学位移成像，也可以通过允许调查整个大脑的代谢改变而不是在特定的感兴趣的区域，提供更大的特异性。最后，MRS 可以在测量治疗效果方面发挥作用，例如经颅直流电刺激198和 TMS.199。体育相关伤害测量，报告，跟踪和数据共享的机制和监测基础设施不足以满足目前的需求和目标。脑震荡的研究和临床工作受到缺乏运动和运动水平的脑震荡数据的阻碍。2014年美国医学研究所的一份报告只确定了三个国家运动伤害监测系统: 国家电子伤害监测系统ーー所有伤害项目(NEISS-AIP)、全美大学体育协会伤害监测系统(NCAA ISS)和高中伤害在线报告系统(rIOTM)。1这些系统可以补充临床数据(例如，来自急诊科、住院病人和体育诊所) ，但这些数据偏向于更严重的伤害和社会经济地位更高的病人。事实上，农村地区或社会经济地位较低的社区的学校往往很难获得运动医疗专业人员和设施。一些新出现的项目可能会改善监督。区域性的努力，如运动训练员临床结果研究教育(CORE-AT)和全国性的努力，如全国运动训练员协会全国运动治疗，伤害和结果网络(NATA NATIONTM)试图将伤害跟踪与高中和大学水平的治疗和结果数据结合起来。然而，这些系统中没有一个专门针对年轻运动员、那些参加非学校赞助体育项目的运动员或那些在没有运动教练的学校的运动员。运动损伤数据库也很少考虑人口统计因素，包括社会经济地位、种族或民族以及医疗保健覆盖率。目前，还没有有效的机制来连贯和廉价地将各种监测数据集联系起来，或者跨越体育、跟踪系统或年龄连续体跟踪个别运动员。现在相当需要一个系统来追踪个人运动员的运动生涯和其他方面。应该对每个人进行数十年的跟踪，以确定 TBI 的负担是否、何时以及如何演变为 CTE，并评估与脑震荡相关的所有可能的负面健康结果。这种系统还可以更准确地描述脑震荡病史和风险因素，并可以捕捉短期和长期的结果，包括身体和心理健康、学业和职业成功、生活质量和社会联系以及不断变化的社会经济地位。这种努力受到各种问题的挑战，包括缺乏任何级别的脑震荡强制性报告。强制性脑震荡报告、为监测工作提供资金以及为数据记者(例如教练和运动员培训员)提供培训将极大地改善流行病学研究。然而，如果没有经过验证的、对脑震荡的共识定义，以及通用数据库和全球唯一标识符(GUID)系统的开发，强制性报告将无法提供有意义的结果。然后可以将标准化监测工作的数据集联系起来，从而改善研究和临床护理的数据共享。将监测数据与组织和液体样本生物库的标准化收集、储存和管理基础设施耦合起来，可以大大改善损伤和结果研究。200这些努力可以通过公私伙伴关系的资金来催化，并通过制定现实的短期和长期目标来实现，以创建一个多年计划。然而，至少在美国，这些努力目前受到对健康保险便利和责任法案(HIPAA)规定的误解和对运动员保密的普遍关注的阻碍。运动员更广泛地使用计算机神经认知测试(CNT)可以改善脑震荡的监测，以及诊断和管理。然而，在 CNT 成为常规手术之前，必须克服几个重要的挑战。这些挑战包括缺乏标准化的管理协议，不同计算机硬件引起的技术错误的可能性，评估的认知功能类型的限制，以及缺乏合格的测试管理员和数据解释员.201尽管存在这些缺陷，但是，CNT 已经被大约40% 的美国高中雇用运动教练员.202虽然不是所有学校都负担得起，但是 CNT 可以加强地面数据收集，帮助风险暴露估计和脑震荡后恢复跟踪，以及提高向运动损伤监测网络报告的数据质量。CNT 也可能有助于评估和跟踪脑震荡后认知改善或下降，并可能有助于预测结果.203,204在学校环境中收集的 CNT 数据是否将达到由临床研究小组进行的 CNT 所达到的验证和重复性标准仍有待观察。重要的是，CNT 需要标准化和指导方针，以确定“返回运动”和“返回学习”的运动员在一个领域表现出恢复，但在其他领域仍然有症状。在临床和青少年运动员脑震荡监测和管理方面，需要对 CNT 的应用进行更多的研究。在一些关键领域，不完整的知识阻碍了儿科脑震荡领域有意义的进展。在分子和细胞水平上，迫切需要重点研究脑震荡和重复性亚震荡损伤后的轴突损伤，以阐明轴突运输和修复的变化，并更好地定义瞬时 Aβ 积累作为下游和/或未来病理学的潜在驱动因素的作用。脑震荡研究人员可能需要确定更合适的动物模型来研究分子病理学，包括 tau 蛋白及其对脑震荡后和慢性创伤脑炎病理学的贡献，因为啮齿动物和人类的大脑结构和组织大不相同。如果不能更清楚地了解创伤性脑损伤如何改变年轻、仍在发育中的大脑，以及在损伤后的数周、数月和数年内会发生什么样的病理事件，我们就只能推测这种改变的潜在生物学基础。通过使用记录线性和旋转力的传感器技术，可以改进青年体育运动中头部影响数据的收集和风险评估。这种商业上可用的设备，如果经过验证，可以确定在比赛期间和整个比赛季节中头部累积冲击力的水平，并且研究结果可以与神经影像学数据和功能结果评估联系起来。结合“击中计数”指标，传感器数据可以提高对重复性次生震荡影响的短期和长期神经心理学结果的认识。我们对慢性创伤性脑病的认识可以通过了解一般人群、受伤运动员、运动和运动位置匹配的未受伤运动员以及低风险运动中的“控制”运动员的基线率来改善。提高对风险暴露的认识可导致预防努力，包括改变做法和竞争规则。一项长达数十年的前瞻性追踪研究，追踪青年运动员的运动生涯及以后的发展，将提供有关累积性头部撞击以及长期神经心理功能障碍和痴呆风险的更确切知识。这样的研究正在 NCAA 校友中进行，他们于2003年首次接受研究，并于2013年重新评估。其他人群的研究，特别是如果 NIH 资助的话，可能会从5年的研究开始，可以进一步延长5年的增量。可能需要建立公私伙伴关系，以获得足够的资金，使多个研究中心参与进来。NCAA 已经为100多名运动员的10年重新评估提供了部分赞助，但需要来自 NIH，美国国防部(DoD)和私人慈善来源的进一步资助，以扩大评估范围，从神经心理学，通过 MRI，淀粉样蛋白，tau 和/或炎症的分子成像。理想情况下，追踪研究设计应结合流行病学和介入试验方法，并利用多个对照组，包括非接触运动员和未受伤的撞击运动员。追踪研究还将阐明认知储备的作用。老年痴呆症研究团体利用国家卫生研究院的资金以及涉及制药公司和基金会的公私伙伴关系，开创了这类研究的先例。为了使这类研究取得成功，必须首先建立更多的监测系统和数据库。如果参加影响力体育运动的运动员能够普遍获得运动员训练员的帮助，这些训练员能够在促进安全和提供基本护理的同时充当可靠的数据报告员，那么将加快努力。此外，任何纵向研究都必须包括死后分析，以便更好地了解儿童和青少年脑震荡对今后生活中神经退行性病理和痴呆发展的影响。由于缺乏严格的流行病学证据，“重返赛场”的指导方针目前受到阻碍，纵向研究的长期安全数据可能会大大改善这一点。纵向研究还可以包括确定那些未能遵循指导方针的运动员是否会经历任何负面健康影响的研究，例如持续的症状或改变发生第二次脑震荡的风险。长期前瞻性研究的基础设施可以通过建立一个以阿尔茨海默氏病神经影像学倡议(ADNI)为模型的研究联盟来创建。ADNI 为数据收集、传播协议、测试方法和生物标志物收集和分析制定了标准。目前正在国防部参与的一个版本的 ADNI (ADNI-DoD)专注于军事人群中与爆炸相关的 TBI 研究。2072014年5月，除了 NCAA 脑震荡研究，NCAA 和国防部宣布启动迄今为止最大的前瞻性运动相关脑震荡研究，该研究将在3年内监测大约37,000名 NCAA 运动员。我们可以想象，这项研究的基础设施可能最终扩展到研究年轻运动员在一个延长的纵向范围。我们对创伤性脑损伤的生物学知识仍然存在许多差距，这限制了我们开发有效药物的能力。如果我们要解决潜在的疾病病理，并超越治疗症状，就必须填补这些空白。然而，当基础创伤性脑损伤生物学的研究继续进行时，许多工作可以完成。药物再利用包括测试现有 FDA 批准的新适应症药物，可以减少费用和缩短药物批准的路径。目前的再利用试验包括哌醋甲酯治疗疼痛和精神疲劳，多巴胺受体激动剂溴隐亭治疗工作记忆，舍曲林治疗情绪和焦虑，这是最常见的影响脑震荡后长期结果的神经心理并发症。此外，黄体酮的 PROTECT III 期临床试验最近未能改善急性 TBI211后的结局，这可能提醒人们需要更多的研究来更好地理解 TBI 的基础生物学。虽然许多药物重新利用的努力主要是为了解决脑震荡症状，药物也可能影响损伤病理学和进展。对现有药物的研究也可能导致新的药物发现努力，并可能导致新的预防或管理治疗。急需新的药物治疗创伤性脑损伤和无法消除的脑震荡。在神经保护和抗炎领域的药物发现努力是特别相关的，因为它们潜在的交叉适用于神经退行性疾病，如 AD。同样，目前正在开发的治疗其他神经退行性疾病的药物可能会被重新定位，用于 TBI 或无脑震荡症状患者的检测。正如医学研究中经常出现的情况一样，脑震荡研究的最新进展提出的问题和回答的问题一样多。有证据表明脑震荡或重复性次生脑震荡后长期神经心理功能障碍和晚年痴呆，需要更多的工作来更好地理解青年参与影响性运动的含义和结果。正如本专家共识文件所概述的那样，有一条前进的道路，但实现这里概述的目标将需要公共和私营部门的合作。虽然可以通过增加知识来改进建议，但现有证据仍然可以在考虑青年参与体育运动以及实践政策和竞赛规则时为个人决策提供信息。随着人口老龄化和痴呆症的流行，我们必须更多地了解潜在的早期生活风险因素，包括与运动有关的脑震荡。家长、教练、学校董事会和孩子们做出的选择将在脑震荡科学知识的关键差距得到填补时得到更好的信息。下载参考资料|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Practical+Design+of+Performant+Recommender+Systems+using+Large-scale+Linear+Programming-based+Global+Inference)|0|
|[Rank-heterogeneous Preference Models for School Choice](https://doi.org/10.1145/3580305.3599484)|Amel Awadelkarim, Arjun Seshadri, Itai Ashlagi, Irene Lo, Johan Ugander|Amazon; Stanford University|School choice mechanism designers use discrete choice models to understand and predict families' preferences. The most widely-used choice model, the multinomial logit (MNL), is linear in school and/or household attributes. While the model is simple and interpretable, it assumes the ranked preference lists arise from a choice process that is uniform throughout the ranking, from top to bottom. In this work, we introduce two strategies for rank-heterogeneous choice modeling tailored for school choice. First, we adapt a context-dependent random utility model (CDM), considering down-rank choices as occurring in the context of earlier up-rank choices. Second, we consider stratifying the choice modeling by rank, regularizing rank-adjacent models towards one another when appropriate. Using data on household preferences from the San Francisco Unified School District (SFUSD) across multiple years, we show that the contextual models considerably improve our out-of-sample evaluation metrics across all rank positions over the non-contextual models in the literature. Meanwhile, stratifying the model by rank can yield more accurate first-choice predictions while down-rank predictions are relatively unimproved. These models provide performance upgrades that school choice researchers can adopt to improve predictions and counterfactual analyses.|学校选择机制的设计者使用离散选择模型来理解和预测家庭的偏好。最广泛使用的选择模型，多项式 logit (MNL) ，在学校和/或家庭属性中是线性的。虽然这个模型是简单和可解释的，但是它假设排名的偏好列表来自于一个从上到下在整个排名过程中是统一的选择过程。本文介绍了两种适用于学校选择的秩异质选择模型的建模策略。首先，我们采用了一个上下文相关的随机效用模型(CDM) ，考虑了在早期上层选择的情况下发生的下层选择。其次，我们考虑根据等级对选择模型进行分层，在适当的时候将相邻等级的模型相互调整。使用来自旧金山联合校区多年的家庭偏好数据，我们发现相对于文献中的非上下文模型，上下文模型大大提高了我们在所有排名位置的外部评估指标。同时，按等级对模型进行分层可以得到更准确的第一选择预测，而低等级预测相对来说没有改进。这些模型提供了学校选择研究人员可以用来改进预测和反事实分析的绩效提升。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rank-heterogeneous+Preference+Models+for+School+Choice)|0|
|[Connecting the Dots - Density-Connectivity Distance unifies DBSCAN, k-Center and Spectral Clustering](https://doi.org/10.1145/3580305.3599283)|Anna Beer, Andrew Draganov, Ellen Hohma, Philipp Jahn, Christian M. M. Frey, Ira Assent||Despite the popularity of density-based clustering, its procedural definition makes it difficult to analyze compared to clustering methods that minimize a loss function. In this paper, we reformulate DBSCAN through a clean objective function by introducing the density-connectivity distance (dc-dist), which captures the essence of density-based clusters by endowing the minimax distance with the concept of density. This novel ultrametric allows us to show that DBSCAN, k-center, and spectral clustering are equivalent in the space given by the dc-dist, despite these algorithms being perceived as fundamentally different in their respective literatures. We also verify that finding the pairwise dc-dists gives DBSCAN clusterings across all epsilon-values, simplifying the problem of parameterizing density-based clustering. We conclude by thoroughly analyzing density-connectivity and its properties -- a task that has been elusive thus far in the literature due to the lack of formal tools. Our code recreates every experiment below: https://github.com/Andrew-Draganov/dc_dist|尽管基于密度的聚类方法很流行，但是它的过程定义使得它很难与损失函数最小化的聚类方法进行比较。本文通过引入密度连通度距离(dc-dist) ，通过一个简洁的目标函数对 DBSCAN 进行了重新构造，该目标函数赋予极大极小距离密度的概念，从而抓住了基于密度的聚类的本质。这种新颖的超测量法使我们能够展示 DBSCAN、 k-center 和 SVD 在 dc-dist 给出的空间中是等价的，尽管这些算法在各自的文献中被认为是根本不同的。我们还验证了找到成对的 dc-dists 可以给出跨所有 ε 值的 DBSCAN 聚类，从而简化了基于密度的聚类的参数化问题。最后，我们通过深入分析密度连通性及其性质得出结论——由于缺乏形式工具，这项任务在文献中迄今为止一直难以实现。我们的代码重现了下面的每一个实验:  https://github.com/andrew-draganov/dc_dist|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Connecting+the+Dots+-+Density-Connectivity+Distance+unifies+DBSCAN,+k-Center+and+Spectral+Clustering)|0|
|[Shilling Black-box Review-based Recommender Systems through Fake Review Generation](https://doi.org/10.1145/3580305.3599502)|HungYun Chiang, YiSyuan Chen, YunZhu Song, HongHan Shuai, Jason S. Chang|National Yang Ming Chiao Tung University; National Tsing Hua University|Review-Based Recommender Systems (RBRS) have attracted increasing research interest due to their ability to alleviate well-known cold-start problems. RBRS utilizes reviews to construct the user and items representations. However, in this paper, we argue that such a reliance on reviews may instead expose systems to the risk of being shilled. To explore this possibility, in this paper, we propose the first generation-based model for shilling attacks against RBRSs. Specifically, we learn a fake review generator through reinforcement learning, which maliciously promotes items by forcing prediction shifts after adding generated reviews to the system. By introducing the auxiliary rewards to increase text fluency and diversity with the aid of pre-trained language models and aspect predictors, the generated reviews can be effective for shilling with high fidelity. Experimental results demonstrate that the proposed framework can successfully attack three different kinds of RBRSs on the Amazon corpus with three domains and Yelp corpus. Furthermore, human studies also show that the generated reviews are fluent and informative. Finally, equipped with Attack Review Generators (ARGs), RBRSs with adversarial training are much more robust to malicious reviews.|基于评论的推荐系统(RBRS)由于其缓解众所周知的冷启动问题的能力而引起了越来越多的研究兴趣。RBRS 利用评论来构建用户和项目表示。然而，在本文中，我们认为，这种对审查的依赖反而可能使系统面临被托儿的风险。为了探索这种可能性，本文提出了第一代基于先令攻击的 RBRS 模型。具体来说，我们通过强化学习学习一个虚假的评论生成器，它在向系统添加生成的评论之后，通过强制预测变化来恶意推销项目。通过引入辅助奖励，以提高文本流畅性和多样性的帮助下，预先训练的语言模型和方面预测，生成的评论可以有效的先令与高保真度。实验结果表明，该框架能够成功地利用三个域和 Yelp 语料库对亚马逊语料库中的三种不同类型的 RBRS 进行攻击。此外，人类研究也表明，生成的评论是流畅和信息。最后，配备了攻击评论生成器(ARGs) ，具有对抗性训练的 RBRS 对恶意评论更加有力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Shilling+Black-box+Review-based+Recommender+Systems+through+Fake+Review+Generation)|0|
|[Below the Surface: Summarizing Event Sequences with Generalized Sequential Patterns](https://doi.org/10.1145/3580305.3599264)|Joscha Cüppers, Jilles Vreeken||We study the problem of succinctly summarizing a database of event sequences in terms of generalized sequential patterns. That is, we are interested in patterns that are not exclusively defined over observed surface-level events, as is usual, but rather may additionally include generalized events that can match a set of events. To avoid spurious and redundant results we define the problem in terms of the Minimum Description Length principle, by which we are after that set of patterns and generalizations that together best compress the data without loss. The resulting optimization problem does not lend itself for exact search, which is why we propose the heuristic Flock algorithm to efficiently find high-quality models in practice. Extensive experiments on synthetic and real-world data show that Flock results in compact and easily interpretable models that accurately recover the ground truth, including rare instances of generalized patterns. Additionally Flock recovers how generalized events within patterns depend on each other, and overall provides clearer insight into the data-generating process than using state of the art algorithms that only consider surface-level patterns.|我们研究了用广义序列模式简洁地汇总事件序列数据库的问题。也就是说，我们感兴趣的模式并不像通常那样专门定义在观察到的表面事件上，而是可能另外包括能够匹配一组事件的广义事件。为了避免虚假和冗余的结果，我们根据最小描述长度原则来定义问题，根据这个原则，我们追求的是一组模式和泛化，这些模式和泛化一起最好地压缩数据而不会丢失。由此产生的最佳化问题无法进行精确的搜索，这就是为什么我们提出启发式的 Flock 算法，以便在实践中有效地找到高质量的模型。对合成和真实世界数据的大量实验表明，Flock 产生了紧凑且易于解释的模型，准确地恢复了地面真相，包括罕见的广义模式实例。此外，Flock 还回顾了模式中的广义事件是如何相互依赖的，并且总体上提供了对数据生成过程的更清晰的洞察，而不是使用只考虑表面层模式的最新算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Below+the+Surface:+Summarizing+Event+Sequences+with+Generalized+Sequential+Patterns)|0|
|[Generalized Matrix Local Low Rank Representation by Random Projection and Submatrix Propagation](https://doi.org/10.1145/3580305.3599361)|Pengtao Dang, Haiqi Zhu, Tingbo Guo, Changlin Wan, Tong Zhao, Paul Salama, Yijie Wang, Sha Cao, Chi Zhang|; Indiana University, Bloomington; Indiana University; Indiana University, School of Medicine; Amazon; Purdue University; Genentech|Detecting distinct submatrices of low rank property is a highly desirable matrix representation learning technique for the ease of data interpretation, called the matrix local low rank representation (MLLRR). Based on different mathematical assumptions of the local pattern, the MLLRR problem could be categorized into two sub-problems, namely local constant variation (LCV) and local linear low rank (LLR). Existing solutions on MLLRR only focused on the LCV problem, which misses a substantial amount of true and interesting patterns. In this work, we develop a novel matrix computational framework called RPSP (Random Probing based submatrix Propagation) that provides an effective solution for both of the LCV and LLR problems. RPSP detects local low rank patterns that grow from small submatrices of low rank property, which are determined by a random projection approach. RPSP is supported by theories of random projection. Experiments on synthetic data demonstrate that RPSP outperforms all state-of-the-art methods, with the capacity to robustly and correctly identify the low rank matrices under both LCV and LLR settings. On real-world datasets, RPSP also demonstrates its effectiveness in identifying interpretable local low rank matrices.|矩阵局部低秩表示(MLLRR)是一种非常理想的矩阵表示学习技术，它可以检测出具有低秩性质的不同子矩阵。根据对局部模式的不同数学假设，MLLRR 问题可以分为局部常变(LCV)和局部线性低秩(LLR)两个子问题。MLLRR 上的现有解决方案只关注 LCV 问题，而这个问题忽略了大量真实而有趣的模式。在这项工作中，我们开发了一个新的矩阵计算框架称为 RPSP (随机探测为基础的子矩阵传播) ，提供了一个有效的解决方案，这两个 LCV 和 LLR 问题。RPSP 检测由低秩性质的小子矩阵生成的局部低秩模式，这些小子矩阵由随机投影方法确定。RPSP 得到了随机投影理论的支持。对合成数据的实验表明，RPSP 算法优于所有的最新方法，在 LCV 和 LLR 设置下都具有鲁棒性和正确识别低秩矩阵的能力。在实际数据集上，RPSP 也证明了其识别可解释的局部低秩矩阵的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generalized+Matrix+Local+Low+Rank+Representation+by+Random+Projection+and+Submatrix+Propagation)|0|
|[TWIN: Personalized Clinical Trial Digital Twin Generation](https://doi.org/10.1145/3580305.3599534)|Trisha Das, Zifeng Wang, Jimeng Sun||Clinical trial digital twins are virtual patients that reflect personal characteristics in a high degree of granularity and can be used to simulate various patient outcomes under different conditions. With the growth of clinical trial databases captured by Electronic Data Capture (EDC) systems, there is a growing interest in using machine learning models to generate digital twins. This can benefit the drug development process by reducing the sample size required for participant recruitment, improving patient outcome predictive modeling, and mitigating privacy risks when sharing synthetic clinical trial data. However, prior research has mainly focused on generating Electronic Healthcare Records (EHRs), which often assume large training data and do not account for personalized synthetic patient record generation. In this paper, we propose a sample-efficient method TWIN for generating personalized clinical trial digital twins. TWIN can produce digital twins of patient-level clinical trial records with high fidelity to the targeting participant's record and preserves the temporal relations across visits and events. We compare our method with various baselines for generating real-world patient-level clinical trial data. The results show that TWIN generates synthetic trial data with high fidelity to facilitate patient outcome predictions in low-data scenarios and strong privacy protection against real patients from the trials.|临床试验数字双胞胎是反映个人特征的高度粒度虚拟患者，可用于模拟不同条件下的各种患者结果。随着电子数据采集(EDC)系统所采集的临床试验数据库的增长，利用机器学习模型生成数字双胞胎的兴趣日益增长。这可以通过减少参与者招募所需的样本量，改善患者结果预测建模，以及在共享合成临床试验数据时减少隐私风险，从而有利于药物开发过程。然而，以前的研究主要集中在生成电子医疗记录(EHRs) ，这往往假设大量的训练数据，并没有考虑到个性化的合成病人记录的生成。在本文中，我们提出了一个样本效率的方法 TWIN 生成个性化的临床试验数字双胞胎。TWIN 可以生成患者级临床试验记录的数字双胞胎，对目标参与者的记录具有高保真度，并保存访问和事件之间的时间关系。我们比较我们的方法与各种基线生成真实世界的患者水平的临床试验数据。结果表明，TWIN 生成的合成试验数据具有高保真度，以便于在低数据情景下预测患者的结果，并对来自试验的真实患者提供强烈的隐私保护。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TWIN:+Personalized+Clinical+Trial+Digital+Twin+Generation)|0|
|[Accelerating Dynamic Network Embedding with Billions of Parameter Updates to Milliseconds](https://doi.org/10.1145/3580305.3599250)|Haoran Deng, Yang Yang, Jiahe Li, Haoyang Cai, Shiliang Pu, Weihao Jiang|Hikvision Research Institute; Carnegie Mellon University; Zhejiang University|Network embedding, a graph representation learning method illustrating network topology by mapping nodes into lower-dimension vectors, is challenging to accommodate the ever-changing dynamic graphs in practice. Existing research is mainly based on node-by-node embedding modifications, which falls into the dilemma of efficient calculation and accuracy. Observing that the embedding dimensions are usually much smaller than the number of nodes, we break this dilemma with a novel dynamic network embedding paradigm that rotates and scales the axes of embedding space instead of a node-by-node update. Specifically, we propose the Dynamic Adjacency Matrix Factorization (DAMF) algorithm, which achieves an efficient and accurate dynamic network embedding by rotating and scaling the coordinate system where the network embedding resides with no more than the number of edge modifications changes of node embeddings. Moreover, a dynamic Personalized PageRank is applied to the obtained network embeddings to enhance node embeddings and capture higher-order neighbor information dynamically. Experiments of node classification, link prediction, and graph reconstruction on different-sized dynamic graphs suggest that DAMF advances dynamic network embedding. Further, we unprecedentedly expand dynamic network embedding experiments to billion-edge graphs, where DAMF updates billion-level parameters in less than 10ms.|网络嵌入是一种通过将节点映射为低维向量来表示网络拓扑的图形表示学习方法，在实际应用中很难适应不断变化的动态图形。现有的研究主要是基于逐个节点的嵌入修改，这种方法陷入了计算效率和精度的两难境地。针对嵌入维数通常远小于节点数的问题，提出了一种新的动态网络嵌入方法，该方法不需要逐个节点更新，而是通过对嵌入空间的轴线进行旋转和缩放来解决这一问题。具体来说，我们提出了动态邻接矩阵分解(dAMF)算法，该算法通过旋转和缩放网络嵌入所在的坐标系，在不超过节点嵌入的边修改变化量的情况下，实现了一个高效、准确的动态网络嵌入。此外，将动态个性化 PageRank 应用于所获得的网络嵌入，以增强节点的嵌入，并动态捕获高阶邻居信息。对不同大小的动态图进行节点分类、链路预测和图重构的实验表明，DAMF 推进了动态网络嵌入。进一步，我们前所未有地将动态网络嵌入实验扩展到十亿边图，其中 DAMF 在不到10ms 的时间内更新十亿级参数。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Accelerating+Dynamic+Network+Embedding+with+Billions+of+Parameter+Updates+to+Milliseconds)|0|
|[MetricPrompt: Prompting Model as a Relevance Metric for Few-shot Text Classification](https://doi.org/10.1145/3580305.3599430)|Hongyuan Dong, Weinan Zhang, Wanxiang Che|Harbin Institute of Technology|Prompting methods have shown impressive performance in a variety of text mining tasks and applications, especially few-shot ones. Despite the promising prospects, the performance of prompting model largely depends on the design of prompt template and verbalizer. In this work, we propose MetricPrompt, which eases verbalizer design difficulty by reformulating few-shot text classification task into text pair relevance estimation task. MetricPrompt adopts prompting model as the relevance metric, further bridging the gap between Pre-trained Language Model's (PLM) pre-training objective and text classification task, making possible PLM's smooth adaption. Taking a training sample and a query one simultaneously, MetricPrompt captures cross-sample relevance information for accurate relevance estimation. We conduct experiments on three widely used text classification datasets across four few-shot settings. Results show that MetricPrompt outperforms manual verbalizer and other automatic verbalizer design methods across all few-shot settings, achieving new state-of-the-art (SOTA) performance.|提示方法已经在各种文本挖掘任务和应用程序中显示出了令人印象深刻的性能，特别是那些很少使用的方法。尽管激励模式前景广阔，但其性能在很大程度上取决于激励模板和语言表达器的设计。在这项工作中，我们提出了 MetricPrompt，它通过将少镜头文本分类任务重构为文本对相关性估计任务，从而减轻了语言表达器的设计难度。MetricPrompt 采用提示模型作为相关度量，进一步缩小了预训练语言模型(Pre-training Language Model，PLM)的预训练目标与文本分类任务之间的差距，使得 PLM 的顺利适应成为可能。同时采用训练样本和查询样本，MetricPrompt 捕获跨样本的相关性信息以进行准确的相关性估计。我们在三个广泛使用的文本分类数据集上通过四个少镜头设置进行实验。结果表明，MetricPrompt 在所有短镜头设置中都优于手动语音表达器和其他自动语音表达器设计方法，实现了新的最新(SOTA)性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MetricPrompt:+Prompting+Model+as+a+Relevance+Metric+for+Few-shot+Text+Classification)|0|
|[Delving into Global Dialogue Structures: Structure Planning Augmented Response Selection for Multi-turn Conversations](https://doi.org/10.1145/3580305.3599304)|Tingchen Fu, Xueliang Zhao, Rui Yan||Retrieval-based dialogue systems are a crucial component of natural language processing, employing information retrieval techniques to select responses from a predefined pool of candidates. The advent of pre-trained language models (PLMs) has significantly advanced the field, with a prevailing paradigm that involves post-training PLMs on specific dialogue corpora, followed by fine-tuning for the response selection (RS) task. This post-training process aims to capture dialogue-specific features, as most PLMs are originally trained on plain text. However, prior approaches predominantly rely on self-supervised tasks or session-level graph neural networks during post-training, focusing on capturing underlying patterns of coherent dialogues without explicitly refining the global pattern across the entire dialogue corpus. Consequently, the learned knowledge for organizing coherent dialogues remains isolated, heavily reliant on specific contexts. Additionally, interpreting or visualizing the implicit knowledge acquired through self-supervised tasks proves challenging. In this study, we address these limitations by explicitly refining the knowledge required for response selection and structuring it into a coherent global flow, known as "dialogue structure." This structure captures the inter-dependency of utterances and topic shifts, thereby enhancing the response selection task. To achieve this, we propose a novel structure model comprising a state recognizer and a structure planner. This model effectively captures the flow within the utterance history and plans the trajectory of future utterances. Importantly, the structure model operates orthogonally to the retrieval model, enabling seamless integration with existing retrieval models and facilitating collaborative training. Extensive experiments conducted on three benchmark datasets demonstrate the superior performance of our method over a wide range of competitive baselines, establishing a new state-of-the-art in the field.|基于检索的对话系统是自然语言处理的重要组成部分，它使用信息检索技术从预先确定的候选人中选择答案。预训练语言模型的出现极大地推动了该领域的发展，其主要范式包括在特定对话语料库中对预训练语言模型进行后训练，然后对响应选择(RS)任务进行微调。这个培训后的过程旨在捕捉对话的特定功能，因为大多数 PLM 最初都是纯文本培训。然而，先前的方法主要依赖于自我监督的任务或会话级图形神经网络在训练后，侧重于捕获连贯对话的潜在模式，而不明确完善整个对话语料库的全局模式。因此，组织连贯对话的所学知识仍然是孤立的，严重依赖于特定的背景。此外，解释或可视化通过自我监督任务获得的内隐知识证明是具有挑战性的。在本研究中，我们通过明确提炼响应选择所需的知识，并将其组织成一个连贯的全球流，即所谓的“对话结构”，来解决这些局限性这种结构捕获了话语和话题转移的相互依赖性，从而增强了回答选择任务。为了实现这一目标，我们提出了一种新的结构模型，包括状态识别器和结构规划器。该模型有效地捕捉了话语历史中的流动，并规划了未来话语的发展轨迹。重要的是，结构模型与检索模型正交操作，能够与现有检索模型无缝集成，并促进协作培训。在三个基准数据集上进行的大量实验表明，我们的方法在广泛的竞争基线上具有优越的性能，建立了该领域的一个新的最先进的状态。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Delving+into+Global+Dialogue+Structures:+Structure+Planning+Augmented+Response+Selection+for+Multi-turn+Conversations)|0|
|[Partial-label Learning with Mixed Closed-set and Open-set Out-of-candidate Examples](https://doi.org/10.1145/3580305.3599460)|Shuo He, Lei Feng, Guowu Yang|University of Electronic Science and Technology of China; Nanyang Technological University|Partial-label learning (PLL) relies on a key assumption that the true label of each training example must be in the candidate label set. This restrictive assumption may be violated in complex real-world scenarios, and thus the true label of some collected examples could be unexpectedly outside the assigned candidate label set. In this paper, we term the examples whose true label is outside the candidate label set OOC (out-of-candidate) examples, and pioneer a new PLL study to learn with OOC examples. We consider two types of OOC examples in reality, i.e., the closed-set/open-set OOC examples whose true label is inside/outside the known label space. To solve this new PLL problem, we first calculate the wooden cross-entropy loss from candidate and non-candidate labels respectively, and dynamically differentiate the two types of OOC examples based on specially designed criteria. Then, for closed-set OOC examples, we conduct reversed label disambiguation in the non-candidate label set; for open-set OOC examples, we leverage them for training by utilizing an effective regularization strategy that dynamically assigns random candidate labels from the candidate label set. In this way, the two types of OOC examples can be differentiated and further leveraged for model training. Extensive experiments demonstrate that our proposed method outperforms state-of-the-art PLL methods.|部分标签学习(PLL)依赖于一个关键的假设，即每个训练样本的真实标签必须在候选标签集中。在复杂的现实场景中，这种限制性假设可能会被违反，因此，一些收集的示例的真实标签可能意外地位于分配的候选标签集之外。在本文中，我们将真实标签在候选标签集外的例子称为候选标签集外的例子，并且开创了一种新的 PLL 研究方法来学习候选标签集外的例子。我们在实际中考虑两种类型的 OOC 示例，即闭集/开集 OOC 示例，它们的真实标签位于已知标签空间的内部或外部。为了解决这个新的锁相环问题，我们首先分别计算候选标签和非候选标签的木质交叉熵损失，并根据特定的准则动态区分两种类型的 OOC 实例。然后，对于闭集 OOC 例子，我们在非候选标签集中进行反向标签消歧; 对于开集 OOC 例子，我们利用它们进行训练，利用一种有效的正则化策略，从候选标签集中动态分配随机候选标签。通过这种方式，两种类型的 OOC 示例可以区分并进一步用于模型培训。大量的实验表明，我们提出的方法优于最先进的锁相环方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Partial-label+Learning+with+Mixed+Closed-set+and+Open-set+Out-of-candidate+Examples)|0|
|[COMET: Learning Cardinality Constrained Mixture of Experts with Trees and Local Search](https://doi.org/10.1145/3580305.3599278)|Shibal Ibrahim, Wenyu Chen, Hussein Hazimeh, Natalia Ponomareva, Zhe Zhao, Rahul Mazumder|Google Research; Massachusetts Institute of Technology; Google DeepMind|The sparse Mixture-of-Experts (Sparse-MoE) framework efficiently scales up model capacity in various domains, such as natural language processing and vision. Sparse-MoEs select a subset of the "experts" (thus, only a portion of the overall network) for each input sample using a sparse, trainable gate. Existing sparse gates are prone to convergence and performance issues when training with first-order optimization methods. In this paper, we introduce two improvements to current MoE approaches. First, we propose a new sparse gate: COMET, which relies on a novel tree-based mechanism. COMET is differentiable, can exploit sparsity to speed up computation, and outperforms state-of-the-art gates. Second, due to the challenging combinatorial nature of sparse expert selection, first-order methods are typically prone to low-quality solutions. To deal with this challenge, we propose a novel, permutation-based local search method that can complement first-order methods in training any sparse gate, e.g., Hash routing, Top-k, DSelect-k, and COMET. We show that local search can help networks escape bad initializations or solutions. We performed large-scale experiments on various domains, including recommender systems, vision, and natural language processing. On standard vision and recommender systems benchmarks, COMET+ (COMET with local search) achieves up to 13% improvement in ROC AUC over popular gates, e.g., Hash routing and Top-k, and up to 9% over prior differentiable gates e.g., DSelect-k. When Top-k and Hash gates are combined with local search, we see up to $100\times$ reduction in the budget needed for hyperparameter tuning. Moreover, for language modeling, our approach improves over the state-of-the-art MoEBERT model for distilling BERT on 5/7 GLUE benchmarks as well as SQuAD dataset.|稀疏混合专家(Sparse-MoE)框架有效地扩展了各种领域的模型容量，例如自然语言处理和视觉。稀疏-MoEs 使用稀疏的、可训练的门为每个输入样本选择一个“专家”子集(因此，只是整个网络的一部分)。现有的稀疏门在用一阶优化方法进行训练时容易出现收敛和性能问题。在本文中，我们介绍了两个改进的目前的教育方法。首先，我们提出了一种新的稀疏门: COMET，它依赖于一种新的基于树的机制。COMET 是可微的，可以利用稀疏性来加速计算，并且性能优于最先进的门。其次，由于稀疏专家选择具有挑战性的组合性质，一阶方法通常倾向于低质量的解决方案。为了应对这一挑战，我们提出了一种新颖的基于置换的局部搜索方法，可以补充一阶方法训练任何稀疏门，例如，散列路由，Top-k，DSelect-k 和 COMET。我们展示了本地搜索可以帮助网络逃避糟糕的初始化或解决方案。我们在不同的领域进行了大规模的实验，包括推荐系统、视觉和自然语言处理。在标准愿景和推荐系统基准上，COMET + (本地搜索的 COMET)在 ROC AUC 比流行的门(如散列路由和 Top-k)提高了13% ，比以前的可微分门(如 DSelect-k)提高了9% 。当 Top-k 和 Hash 门与本地搜索相结合时，我们看到超参数调优所需的预算减少了100倍。此外，对于语言建模，我们的方法改进了最先进的 MoEBERT 模型，用于提取5/7 GLUE 基准测试和 SQuAD 数据集上的 BERT。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=COMET:+Learning+Cardinality+Constrained+Mixture+of+Experts+with+Trees+and+Local+Search)|0|
|[Exploiting Relation-aware Attribute Representation Learning in Knowledge Graph Embedding for Numerical Reasoning](https://doi.org/10.1145/3580305.3599338)|Gayeong Kim, Sookyung Kim, Ko Keun Kim, Suchan Park, Heesoo Jung, Hogun Park||Numerical reasoning is an essential task for supporting machine learning applications, such as recommendation and information retrieval. The reasoning task aims to compare two items and infer new facts (e.g., is taller than) by leveraging existing relational information and numerical attributes (e.g., the height of an entity) in knowledge graphs. However, most existing methods rely on leveraging attribute encoders or additional loss functions to predict numerical relations. Therefore, the prediction performance is often not robust in cases when attributes are sparsely observed. In this paper, we propose a Relation-AAware attribute representation learning-based Knowledge Graph Embedding method for numerical reasoning tasks, which we call RAKGE. RAKGE incorporates a newly proposed attribute representation learning mechanism, which can leverage the association between relations and their corresponding numerical attributes. In addition, we introduce a robust self-supervised learning method to generate unseen positive and negative examples, thereby making our approach more reliable when numerical attributes are sparsely available. In the evaluation of three real-world datasets, our proposed model outperformed state-of-the-art methods, achieving an improvement of up to 65.1% in Hits@1 and up to 52.6% in MRR compared to the best competitor. Our implementation code is available at https://github.com/learndatalab/RAKGE.|数值推理是支持机器学习应用(如推荐和信息检索)的基本任务。推理任务旨在通过利用知识图表中现有的关系信息和数值属性(例如实体的高度)来比较两个项目并推断出新的事实(例如高于)。然而，大多数现有的方法依赖于利用属性编码器或额外的损失函数来预测数值关系。因此，当属性被稀疏地观察到时，预测性能往往是不稳健的。针对数值推理任务，提出了一种基于关系 AAware 属性表示学习的知识图嵌入方法—— RAKGE。RAKGE 引入了一种新的属性表示学习机制，该机制可以利用关系与其相应的数值属性之间的关联。此外，我们还引入了一种鲁棒的自监督学习方法来生成看不见的正例和负例，从而使我们的方法在数值属性稀疏可用的情况下更加可靠。在对三个真实世界数据集的评估中，我们提出的模型优于最先进的方法，与最好的竞争对手相比，Hits@1的改善高达65.1% ，MRR 的改善高达52.6% 。我们的实施守则可于 https://github.com/learndatalab/rakge 索取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploiting+Relation-aware+Attribute+Representation+Learning+in+Knowledge+Graph+Embedding+for+Numerical+Reasoning)|0|
|[Efficient Distributed Approximate k-Nearest Neighbor Graph Construction by Multiway Random Division Forest](https://doi.org/10.1145/3580305.3599327)|SangHong Kim, HaMyung Park||k-nearest neighbor graphs, shortly k-NN graphs, are widely used in many data mining applications like recommendation, information retrieval, and similarity search. Approximate k-NN graph construction has been getting a lot of attention, and most researches focus on developing algorithms that operate efficiently and quickly on a single machine. A few pioneering studies propose distributed algorithms to increase the size of data that can be processed to billions. However, we notice that the distributed algorithms don't perform well enough due to the problems of graph fragmentation and massive data exchange. In this paper, we propose MRDF (Multiway Random Division Forest), a scalable distributed algorithm that constructs highly accurate k-NN graph from numerous high-dimensional vectors quickly. MRDF resolves the problems that the existing distributed algorithms suffer from, through coarse-grained partitioning based on tree path annotation. Experimental results on real-world datasets show that MRDF outperforms the state-of-the-art distributed algorithms with up to 7.6 times faster speed and up to 56%p better accuracy than the second best results.|K- 最近邻图，即短 k- 近邻图，广泛应用于许多数据挖掘应用程序，如推荐、信息检索和最近邻搜索。近似 k-NN 图的构造一直受到人们的广泛关注，目前的研究主要集中在单机高效快速的算法开发上。一些开创性的研究提出了分布式算法，以增加数据的大小，可以处理数十亿。然而，我们注意到由于图的碎片化和大量数据交换的问题，分布式算法的性能不够好。在这篇文章中，我们提出了多路随机分割森林，这是一个可扩展的分散式演算法，它可以从大量的高维向量中快速构造出高精度的 k-NN 图。MRDF 通过基于树路径标注的粗粒度划分解决了现有分布式算法存在的问题。在实际数据集上的实验结果表明，MRDF 算法的速度比最先进的分布式算法快7.6倍，准确率比次优算法高56% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Distributed+Approximate+k-Nearest+Neighbor+Graph+Construction+by+Multiway+Random+Division+Forest)|0|
|[MM-DAG: Multi-task DAG Learning for Multi-modal Data - with Application for Traffic Congestion Analysis](https://doi.org/10.1145/3580305.3599436)|Tian Lan, Ziyue Li, Zhishuai Li, Lei Bai, Man Li, Fugee Tsung, Wolfgang Ketter, Rui Zhao, Chen Zhang|Shanghai AI Laboratory; Tsinghua University; SenseTime Research; The Hong Kong University of Science and Technology (Guangzhou); University of Cologne; The Hong Kong University of Science and Technology|This paper proposes to learn Multi-task, Multi-modal Direct Acyclic Graphs (MM-DAGs), which are commonly observed in complex systems, e.g., traffic, manufacturing, and weather systems, whose variables are multi-modal with scalars, vectors, and functions. This paper takes the traffic congestion analysis as a concrete case, where a traffic intersection is usually regarded as a DAG. In a road network of multiple intersections, different intersections can only have some overlapping and distinct variables observed. For example, a signalized intersection has traffic light-related variables, whereas unsignalized ones do not. This encourages the multi-task design: with each DAG as a task, the MM-DAG tries to learn the multiple DAGs jointly so that their consensus and consistency are maximized. To this end, we innovatively propose a multi-modal regression for linear causal relationship description of different variables. Then we develop a novel Causality Difference (CD) measure and its differentiable approximator. Compared with existing SOTA measures, CD can penalize the causal structural difference among DAGs with distinct nodes and can better consider the uncertainty of causal orders. We rigidly prove our design's topological interpretation and consistency properties. We conduct thorough simulations and one case study to show the effectiveness of our MM-DAG. The code is available under https://github.com/Lantian72/MM-DAG|本文提出学习多任务、多模态直接无环图(MM-DAGs) ，这是在交通、制造、天气等复杂系统中常见的图形，其变量是多模态的，包括标量、向量和函数。本文以交通堵塞分析作为一个具体案例，其中交通十字路口通常被视为一个 DAG。在一个多交叉口的道路网络中，不同的交叉口只能观察到一些重叠的、不同的变量。例如，信号交叉口有与交通灯相关的变量，而无信号交叉口没有。这鼓励了多任务设计: 将每个 DAG 作为一个任务，MM-DAG 试图联合学习多个 DAG，以便最大化它们的一致性和一致性。为此，我们创新性地提出了一种多模态回归方法来描述不同变量之间的线性因果关系。然后我们发展了一个新的因果差分(CD)测度及其可微逼近器。与现有的 SOTA 方法相比，CD 方法能够更好地考虑因果顺序的不确定性，并且能够惩罚具有不同节点的 DAGs 之间的因果结构差异。我们严格证明了我们的设计的拓扑解释和一致性性质。我们进行了彻底的模拟和一个案例研究，以显示我们的 MM-DAG 的有效性。代码可在 https://github.com/lantian72/mm-dag 下查阅|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MM-DAG:+Multi-task+DAG+Learning+for+Multi-modal+Data+-+with+Application+for+Traffic+Congestion+Analysis)|0|
|[Who Should Be Given Incentives? Counterfactual Optimal Treatment Regimes Learning for Recommendation](https://doi.org/10.1145/3580305.3599550)|Haoxuan Li, Chunyuan Zheng, Peng Wu, Kun Kuang, Yue Liu, Peng Cui||Effective personalized incentives can improve user experience and increase platform revenue, resulting in a win-win situation between users and e-commerce companies. Previous studies have used uplift modeling methods to estimate the conditional average treatment effects of users' incentives, and then placed the incentives by maximizing the sum of estimated treatment effects under a limited budget. However, some users will always buy whether incentives are given or not, and they will actively collect and use incentives if provided, named "Always Buyers". Identifying and predicting these "Always Buyers" and reducing incentive delivery to them can lead to a more rational incentive allocation. In this paper, we first divide users into five strata from an individual counterfactual perspective, and reveal the failure of previous uplift modeling methods to identify and predict the "Always Buyers". Then, we propose principled counterfactual identification and estimation methods and prove their unbiasedness. We further propose a counterfactual entire-space multi-task learning approach to accurately perform personalized incentive policy learning with a limited budget. We also theoretically derive a lower bound on the reward of the learned policy. Extensive experiments are conducted on three real-world datasets with two common incentive scenarios, and the results demonstrate the effectiveness of the proposed approaches.|有效的个性化激励可以改善用户体验，增加平台收入，从而实现用户与电子商务公司之间的双赢。以往的研究采用提升模型方法来估计使用者激励的条件平均治疗效果，然后在有限的预算下通过最大化估计治疗效果的总和来放置激励。然而，一些用户总是会购买无论是否给予奖励，他们将积极收集和使用奖励，如果提供，命名为“总是买家”。识别和预测这些“永远的买家”，减少对他们的激励，可以导致更合理的激励分配。本文首先从个体反事实的角度将用户划分为五个层次，揭示了以往的提升建模方法在识别和预测“总是买家”方面的失败。然后，提出了有原则的反事实识别和估计方法，并证明了它们的无偏性。我们进一步提出了一个反事实的全空间多任务学习方法，以准确地执行有限预算的个性化激励政策学习。我们还从理论上导出了学习策略的报酬下界。在三个具有两种常见激励情景的真实世界数据集上进行了广泛的实验，实验结果表明了所提方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Who+Should+Be+Given+Incentives?+Counterfactual+Optimal+Treatment+Regimes+Learning+for+Recommendation)|0|
|[UCEpic: Unifying Aspect Planning and Lexical Constraints for Generating Explanations in Recommendation](https://doi.org/10.1145/3580305.3599535)|Jiacheng Li, Zhankui He, Jingbo Shang, Julian J. McAuley|University of California, San Diego|Personalized natural language generation for explainable recommendations plays a key role in justifying why a recommendation might match a user's interests. Existing models usually control the generation process by aspect planning. While promising, these aspect-planning methods struggle to generate specific information correctly, which prevents generated explanations from being convincing. In this paper, we claim that introducing lexical constraints can alleviate the above issues. We propose a model, UCEpic, that generates high-quality personalized explanations for recommendation results by unifying aspect planning and lexical constraints in an insertion-based generation manner. Methodologically, to ensure text generation quality and robustness to various lexical constraints, we pre-train a non-personalized text generator via our proposed robust insertion process. Then, to obtain personalized explanations under this framework of insertion-based generation, we design a method of incorporating aspect planning and personalized references into the insertion process. Hence, UCEpic unifies aspect planning and lexical constraints into one framework and generates explanations for recommendations under different settings. Compared to previous recommendation explanation generators controlled by only aspects, UCEpic incorporates specific information from keyphrases and then largely improves the diversity and informativeness of generated explanations for recommendations on datasets such as RateBeer and Yelp.|为可解释的推荐生成个性化的自然语言在证明为什么推荐可能符合用户的兴趣方面起着关键作用。现有模型通常通过方面规划来控制生成过程。尽管这些方面规划方法很有前途，但它们难以正确地生成特定的信息，这使得生成的解释无法令人信服。在本文中，我们认为引入词汇约束可以缓解上述问题。我们提出了一个模型，UCEic，通过统一的方面规划和词法约束在一个基于插入的生成方式，为推荐结果生成高质量的个性化解释。在方法上，为了保证文本生成的质量和对各种词汇约束的鲁棒性，我们通过提出的鲁棒插入过程预训练了一个非个性化的文本生成器。然后，为了在基于插入的生成框架下获得个性化的解释，我们设计了一种将方面规划和个性化参考引用融入到插入过程中的方法。因此，UCEic 将方面规划和词法约束统一到一个框架中，并在不同的设置下生成对建议的解释。与以前只受方面控制的推荐解释生成器相比，UCEic 整合了来自关键词的特定信息，然后在很大程度上提高了 RateBeer 和 Yelp 等数据集上生成的推荐解释的多样性和信息性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=UCEpic:+Unifying+Aspect+Planning+and+Lexical+Constraints+for+Generating+Explanations+in+Recommendation)|0|
|[Learning-Based Ad Auction Design with Externalities: The Framework and A Matching-Based Approach](https://doi.org/10.1145/3580305.3599403)|Ningyuan Li, Yunxuan Ma, Yang Zhao, Zhijian Duan, Yurong Chen, Zhilin Zhang, Jian Xu, Bo Zheng, Xiaotie Deng||Learning-based ad auctions have increasingly been adopted in online advertising. However, existing approaches neglect externalities, such as the interaction between ads and organic items. In this paper, we propose a general framework, namely Score-Weighted VCG, for designing learning-based ad auctions that account for externalities. The framework decomposes the optimal auction design into two parts: designing a monotone score function and an allocation algorithm, which facilitates data-driven implementation. Theoretical results demonstrate that this framework produces the optimal incentive-compatible and individually rational ad auction under various externality-aware CTR models while being data-efficient and robust. Moreover, we present an approach to implement the proposed framework with a matching-based allocation algorithm. Experiment results on both real-world and synthetic data illustrate the effectiveness of the proposed approach.|基于学习的广告拍卖越来越多地被用于在线广告。然而，现有的方法忽视了外部性，如广告与有机项目之间的相互作用。在本文中，我们提出了一个通用的框架，即得分加权 VCG，设计基于学习的广告拍卖，考虑到外部性。该框架将最优拍卖设计分解为两部分: 设计单调得分函数和分配算法，便于数据驱动实现。理论结果表明，该框架在不同的外部性感知的 CTR 模型下产生了最优的激励相容和个体理性的广告拍卖，同时具有数据效率和鲁棒性。此外，我们还提出了一种基于匹配的分配算法实现该框架的方法。对实际数据和合成数据的实验结果表明了该方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning-Based+Ad+Auction+Design+with+Externalities:+The+Framework+and+A+Matching-Based+Approach)|0|
|[Communication Efficient Distributed Newton Method with Fast Convergence Rates](https://doi.org/10.1145/3580305.3599280)|Chengchang Liu, Lesi Chen, Luo Luo, John C. S. Lui|Chinese University of Hong Kong; Fudan University; The Chinese University of Hong Kong|We propose a communication and computation efficient second-order method for distributed optimization. For each iteration, our method only requires $\mathcal{O}(d)$ communication complexity, where $d$ is the problem dimension. We also provide theoretical analysis to show the proposed method has the similar convergence rate as the classical second-order optimization algorithms. Concretely, our method can find~$\big(\epsilon, \sqrt{dL\epsilon}\,\big)$-second-order stationary points for nonconvex problem by $\mathcal{O}\big(\sqrt{dL}\,\epsilon^{-3/2}\big)$ iterations, where $L$ is the Lipschitz constant of Hessian. Moreover, it enjoys a local superlinear convergence under the strongly-convex assumption. Experiments on both convex and nonconvex problems show that our proposed method performs significantly better than baselines.|提出了一种分布式优化的通信和计算有效的二阶方法。对于每个迭代，我们的方法只需要 $mathcal { O }(d) $通信复杂性，其中 $d $是问题维度。理论分析表明，该方法与经典的二阶优化算法具有相似的收敛速度。具体地说，我们的方法可以通过数学上的{ O } big (sqrt { dL } ，epsilon ^ {-3/2} big)迭代找到非凸问题的 ~ $big (epsilon，sqrt { dL epsilon } ，big) $- 二阶驻点，其中 $L $是 Hessian 的 Lipschitz 常数。在强凸假设下，该算法具有局部超线性收敛性。对凸问题和非凸问题的实验表明，该方法的性能明显优于基线方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Communication+Efficient+Distributed+Newton+Method+with+Fast+Convergence+Rates)|0|
|[Meta Multi-agent Exercise Recommendation: A Game Application Perspective](https://doi.org/10.1145/3580305.3599429)|Fei Liu, Xuegang Hu, Shuochen Liu, Chenyang Bu, Le Wu||Exercise recommendation is a fundamental and important task in the E-learning system, facilitating students' personalized learning. Most existing exercise recommendation algorithms design a scoring criterion (e.g., weakest mastery, lowest historical correctness) in conjunction with experience, and then recommend the recommended knowledge concepts (KCs). These algorithms rely entirely on the scoring criteria by treating exercise recommendations as a centralized system. However, it is a complex problem for the centralized system to choose a limited number of exercises in a period of time to consolidate and learn the KCs efficiently. Moreover, different groups of students (e.g., different countries, schools, or classes) have different solutions for the same group of KCs according to their own situations, in the spirit of competency-based instructing. Therefore, we propose Meta Multi-Agent Exercise Recommendation (MMER). Specifically, we design the multi-agent exercise recommendation module, in which the KCs involved in exercises are considered agents with competition and cooperation among them. And the meta-training stage is designed to learn a robust recommendation module for new student groups. Extensive experiments on real-world datasets validate the satisfactory performance of the proposed model. Furthermore, the effectiveness of the multi-agent and meta-training part is demonstrated for the model in recommendation applications.|作业推荐是网络学习系统中一项基础性的重要任务，可以促进学生的个性化学习。大多数现有的练习推荐算法结合经验设计一个评分标准(例如，最弱的掌握能力，最低的历史正确率) ，然后推荐推荐的知识概念(KCs)。这些算法完全依赖于评分标准，将练习推荐视为一个集中的系统。然而，中央系统在一段时间内选择有限数量的练习来有效地巩固和学习知识中心是一个复杂的问题。此外，不同类别的学生(例如不同国家、学校或班级)会因应个别情况，本着能力为本的教学精神，为同一类别的知识型中心提供不同的解决方案。因此，我们提出了元多 Agent 练习推荐(MMER)。具体来说，我们设计了多智能体演练推荐模块，将演练中涉及的知识中心视为具有竞争和合作的智能体。元培训阶段的设计目的是为新生群体学习一个健壮的推荐模块。在实际数据集上的大量实验验证了该模型的良好性能。此外，在推荐应用中验证了该模型的多智能体和元训练部分的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Meta+Multi-agent+Exercise+Recommendation:+A+Game+Application+Perspective)|0|
|[Criteria Tell You More than Ratings: Criteria Preference-Aware Light Graph Convolution for Effective Multi-Criteria Recommendation](https://doi.org/10.1145/3580305.3599292)|JinDuk Park, Siqing Li, Xin Cao, WonYong Shin|The University of New South Wales; Yonsei University|The multi-criteria (MC) recommender system, which leverages MC rating information in a wide range of e-commerce areas, is ubiquitous nowadays. Surprisingly, although graph neural networks (GNNs) have been widely applied to develop various recommender systems due to GNN's high expressive capability in learning graph representations, it has been still unexplored how to design MC recommender systems with GNNs. In light of this, we make the first attempt towards designing a GNN-aided MC recommender system. Specifically, rather than straightforwardly adopting existing GNN-based recommendation methods, we devise a novel criteria preference-aware light graph convolution CPA-LGC method, which is capable of precisely capturing the criteria preference of users as well as the collaborative signal in complex high-order connectivities. To this end, we first construct an MC expansion graph that transforms user--item MC ratings into an expanded bipartite graph to potentially learn from the collaborative signal in MC ratings. Next, to strengthen the capability of criteria preference awareness, CPA-LGC incorporates newly characterized embeddings, including user-specific criteria-preference embeddings and item-specific criterion embeddings, into our graph convolution model. Through comprehensive evaluations using four real-world datasets, we demonstrate (a) the superiority over benchmark MC recommendation methods and benchmark recommendation methods using GNNs with tremendous gains, (b) the effectiveness of core components in CPA-LGC, and (c) the computational efficiency.|多准则推荐系统在电子商贸领域广泛应用，充分利用多准则评级信息。令人惊讶的是，尽管图神经网络(GNN)由于其在学习图表示方面的高度表达能力而被广泛应用于开发各种推荐系统，但是如何利用 GNN 设计 MC 推荐系统仍然是一个未知数。有鉴于此，我们首次尝试设计一个 GNN 辅助的 MC 推荐系统。具体而言，我们不直接采用现有的基于 GNN 的推荐方法，而是设计了一种新的标准偏好感知光图卷积 CPA-LGC 方法，该方法能够精确地捕获用户的标准偏好以及复杂高阶连接中的协作信号。为此，我们首先构建一个 MC 扩展图，将用户-项目 MC 评分转换为一个扩展的二分图，以便潜在地学习 MC 评分中的协作信号。接下来，为了加强标准偏好意识的能力，CPA-LGC 将新的特征嵌入，包括用户特定的标准偏好嵌入和项目特定的标准嵌入，纳入我们的图卷积模型。通过使用四个实际数据集的综合评估，我们证明了(a)使用 GNN 的基准 MC 推荐方法和基准推荐方法的优越性，(b) CPA-LGC 中核心组件的有效性，以及(c)计算效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Criteria+Tell+You+More+than+Ratings:+Criteria+Preference-Aware+Light+Graph+Convolution+for+Effective+Multi-Criteria+Recommendation)|0|
|[Locality Sensitive Hashing for Optimizing Subgraph Query Processing in Parallel Computing Systems](https://doi.org/10.1145/3580305.3599419)|Peng Peng, Shengyi Ji, Zhen Tian, Hongbo Jiang, Weiguo Zheng, Xuecang Zhang||This paper explores parallel computing systems for efficient subgraph query processing in large graphs. We investigate how to take advantage of the inherent parallelism of parallel computing systems for both intraquery and interquery optimization during subgraph query processing. Rather than relying on widely-used hash-based methods, we utilize and extend locality sensitive hashing methods. For intraquery optimization, we use the structures of both the data graph and subgraph query to design a query-constraint locality sensitive hashing method named QCMH, which can be used to merge multiple tasks during a single subgraph query processing. For interquery optimization, we propose a query locality sensitive hashing method named QMH, which can be used to detect common subgraphs among different subgraph queries, thereby merging multiple subgraph queries. Our proposed methods can reduce the redundant computation among multiple tasks duringa single subgraph query processing or multiple queries. Extensive experimental studies on large real and synthetic graphs show that our proposed methods can improve query performance compared to state-of-the-art methods by 10% to 50%.|本文探讨了大型图中子图查询处理的并行计算系统。我们研究在子图查询处理过程中如何利用并行计算系统固有的并行性进行查询内和查询间优化。我们不依赖于广泛使用的基于哈希的方法，而是利用和扩展了区域敏感的哈希方法。对于查询内优化，我们利用数据图和子图查询的结构设计了一种查询约束局部敏感的哈希方法 QCMH，该方法可以在单个子图查询处理过程中合并多个任务。针对查询间优化问题，提出了一种查询位置敏感的哈希方法 QMH，该方法可以检测不同子图查询之间的公共子图，从而实现多个子图查询的合并。该方法可以减少单个子图查询处理或多个查询过程中多个任务之间的冗余计算。对大型实图和合成图的大量实验研究表明，与最先进的方法相比，我们提出的方法可以提高10% 到50% 的查询性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Locality+Sensitive+Hashing+for+Optimizing+Subgraph+Query+Processing+in+Parallel+Computing+Systems)|0|
|[Deep Pipeline Embeddings for AutoML](https://doi.org/10.1145/3580305.3599303)|Sebastian PinedaArango, Josif Grabocka|University of Freiburg|Automated Machine Learning (AutoML) is a promising direction for democratizing AI by automatically deploying Machine Learning systems with minimal human expertise. The core technical challenge behind AutoML is optimizing the pipelines of Machine Learning systems (e.g. the choice of preprocessing, augmentations, models, optimizers, etc.). Existing Pipeline Optimization techniques fail to explore deep interactions between pipeline stages/components. As a remedy, this paper proposes a novel neural architecture that captures the deep interaction between the components of a Machine Learning pipeline. We propose embedding pipelines into a latent representation through a novel per-component encoder mechanism. To search for optimal pipelines, such pipeline embeddings are used within deep-kernel Gaussian Process surrogates inside a Bayesian Optimization setup. Furthermore, we meta-learn the parameters of the pipeline embedding network using existing evaluations of pipelines on diverse collections of related datasets (a.k.a. meta-datasets). Through extensive experiments on three large-scale meta-datasets, we demonstrate that pipeline embeddings yield state-of-the-art results in Pipeline Optimization.|自动机器学习(AutoML)是通过自动部署具有最少人类专业知识的机器学习系统来实现人工智能大众化的一个有前途的方向。AutoML 背后的核心技术挑战是优化机器学习系统的管道(例如，预处理、扩展、模型、优化器等的选择)。现有的流水线优化技术无法探索流水线阶段/组件之间的深层交互。作为补救措施，本文提出了一种新颖的神经网络结构，该结构能够捕捉机器学习流水线各组件之间的深层交互。我们提出了一种新的每组件编码机制，将管道嵌入到潜在表示中。为了寻找最佳管道，这种管道嵌入在贝叶斯优化设置内的深核高斯过程代理中使用。此外，我们使用现有的对不同相关数据集(也称为元数据集)上的管道的评估来元学习管道嵌入网络的参数。通过在三个大规模元数据集上的大量实验，我们证明了流水线嵌入在流水线优化中产生了最先进的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Pipeline+Embeddings+for+AutoML)|0|
|[FedAPEN: Personalized Cross-silo Federated Learning with Adaptability to Statistical Heterogeneity](https://doi.org/10.1145/3580305.3599344)|Zhen Qin, Shuiguang Deng, Mingyu Zhao, Xueqiang Yan||In cross-silo federated learning (FL), the data among clients are usually statistically heterogeneous (aka not independent and identically distributed, non-IID) due to diversified data sources, lowering the accuracy of FL. Although many personalized FL (PFL) approaches have been proposed to address this issue, they are only suitable for data with specific degrees of statistical heterogeneity. In the real world, the heterogeneity of data among clients is often immeasurable due to privacy concern, making the targeted selection of PFL approaches difficult. Besides, in cross-silo FL, clients are usually from different organizations, tending to hold architecturally different private models. In this work, we propose a novel FL framework, FedAPEN, which combines mutual learning and ensemble learning to take the advantages of private and shared global models while allowing heterogeneous models. Within FedAPEN, we propose two mechanisms to coordinate and promote model ensemble such that FedAPEN achieves excellent accuracy on various data distributions without prior knowledge of data heterogeneity, and thus, obtains the adaptability to data heterogeneity. We conduct extensive experiments on four real-world datasets, including: 1) Fashion MNIST, CIFAR-10, and CIFAR-100, each with ten different types and degrees of label distribution skew; and 2) eICU with feature distribution skew. The experiments demonstrate that FedAPEN almost obtains superior accuracy on data with varying types and degrees of heterogeneity compared with baselines.|在跨竖井联邦学习(FL)中，由于数据来源的多样性，客户之间的数据通常具有统计异构性(即非独立、同分布、非 IID) ，从而降低了 FL 的准确性。虽然许多个性化的 FL (PFL)方法已被提出来解决这个问题，他们只适用于具有特定程度的统计异质性的数据。在现实世界中，由于对隐私的关注，客户之间的数据异构性往往是不可测量的，这使得有针对性地选择 PFL 方法变得困难。此外，在跨竖井 FL 中，客户通常来自不同的组织，倾向于持有架构上不同的私有模型。在这项工作中，我们提出了一个新的 FL 框架，FedapEN，它结合了相互学习和集成学习，利用私有和共享的全球模型的优势，同时允许异构模型。在 FedAPEN 中，我们提出了两种协调和促进模型集成的机制，使得 FedAPEN 在不知道数据异构性的情况下，在各种数据分布上获得优异的准确性，从而获得对数据异构性的适应性。我们在四个真实世界的数据集上进行了广泛的实验，包括: 1)时尚 MNIST，CIFAR-10和 CIFAR-100，每个都有十种不同的类型和标签分布倾斜程度; 2)特征分布倾斜的 eICU。实验表明，与基线数据相比，FedAPEN 在不同类型和不同程度的异质性数据上几乎获得了更高的准确度。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedAPEN:+Personalized+Cross-silo+Federated+Learning+with+Adaptability+to+Statistical+Heterogeneity)|0|
|[All in One: Multi-Task Prompting for Graph Neural Networks](https://doi.org/10.1145/3580305.3599256)|Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, Jihong Guan|Southeast University; The Hong Kong University of Science and Technology (Guangzhou); Tongji University; The Chinese University of Hong Kong|Recently, ''pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a ''negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.|近年来，“预训练和微调”已经成为许多图形任务的标准工作流，因为它需要一般的图形知识来解决每个应用程序缺乏图形注释的问题。然而，具有节点级、边级和图级的图形任务种类繁多，使得预训练的借口往往与这些多任务不相容。这种差距甚至可能导致特定应用程序的“负转移”，从而导致较差的结果。自然语言处理中的快速学习在利用先验知识完成各种自然语言处理任务方面表现出了显著的效果，受此启发，我们研究了图形的提示主题，以填补预先训练的模型和各种图形任务之间的空白。本文提出了一种新的图模型多任务提示方法。具体来说，我们首先将图形提示符和语言提示符的格式与提示符标记、标记结构和插入模式统一起来。通过这种方式，可以将自然语言处理中的提示思想无缝地引入到图区域中。然后，为了进一步缩小各种图形任务与最先进的预训练策略之间的差距，我们进一步研究了各种图形应用的任务空间，并将下游问题重新表述为图形级任务。在此基础上，引入元学习，有效地学习图形的多任务提示的初始化，使得提示框架对于不同的任务具有更高的可靠性和通用性。我们进行了广泛的实验，实验结果证明了我们方法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=All+in+One:+Multi-Task+Prompting+for+Graph+Neural+Networks)|0|
|[GMOCAT: A Graph-Enhanced Multi-Objective Method for Computerized Adaptive Testing](https://doi.org/10.1145/3580305.3599367)|Hangyu Wang, Ting Long, Liang Yin, Weinan Zhang, Wei Xia, Qichen Hong, Dingyin Xia, Ruiming Tang, Yong Yu||Computerized Adaptive Testing(CAT) refers to an online system that adaptively selects the best-suited question for students with various abilities based on their historical response records. Most CAT methods only focus on the quality objective of predicting the student ability accurately, but neglect concept diversity or question exposure control, which are important considerations in ensuring the performance and validity of CAT. Besides, the students' response records contain valuable relational information between questions and knowledge concepts. The previous methods ignore this relational information, resulting in the selection of sub-optimal test questions. To address these challenges, we propose a Graph-Enhanced Multi-Objective method for CAT (GMOCAT). Firstly, three objectives, namely quality, diversity and novelty, are introduced into the Scalarized Multi-Objective Reinforcement Learning framework of CAT, which respectively correspond to improving the prediction accuracy, increasing the concept diversity and reducing the question exposure. We use an Actor-Critic Recommender to select questions and optimize three objectives simultaneously by the scalarization function. Secondly, we utilize the graph neural network to learn relation-aware embeddings of questions and concepts. These embeddings are able to aggregate neighborhood information in the relation graphs between questions and concepts. We conduct experiments on three real-world educational datasets, and show that GMOCAT not only outperforms the state-of-the-art methods in the ability prediction, but also achieve superior performance in improving the concept diversity and alleviating the question exposure. Our code is available at https://github.com/justarter/GMOCAT.|计算机自适应测试(CAT)是指根据学生的历史回答记录，自适应地为不同能力的学生选择最适合的问题的在线系统。大多数计算机辅助测试(CAT)方法只着眼于准确预测学生能力的质量目标，而忽视了概念多样性或问题暴露控制，这是保证计算机辅助测试(CAT)成绩和有效性的重要因素。此外，学生的回答记录还包含了问题与知识概念之间有价值的关系信息。以前的方法忽略了这些关系信息，导致了次优测试题的选择。为了应对这些挑战，我们提出了一种基于图增强的多目标 CAT (GMOCAT)方法。首先，在计算机辅助测试的标量化多目标强化学习框架中引入质量、多样性和新颖性三个目标，分别对应于提高预测精度、增加概念多样性和减少问题暴露。我们使用一个行为者-批评者推荐系统来选择问题，并通过标量化函数同时优化三个目标。其次，利用图神经网络学习问题和概念的关系感知嵌入。这些嵌入能够在问题和概念之间的关系图中聚合邻域信息。我们在三个实际教育数据集上进行了实验，结果表明，GMOCAT 不仅在能力预测方面优于最先进的方法，而且在提高概念多样性和减少问题暴露方面也取得了较好的效果。我们的代码可以在 https://github.com/justarter/gmocat 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GMOCAT:+A+Graph-Enhanced+Multi-Objective+Method+for+Computerized+Adaptive+Testing)|0|
|[Theoretical Convergence Guaranteed Resource-Adaptive Federated Learning with Mixed Heterogeneity](https://doi.org/10.1145/3580305.3599521)|Yangyang Wang, Xiao Zhang, Mingyi Li, Tian Lan, Huashan Chen, Hui Xiong, Xiuzhen Cheng, Dongxiao Yu||In this paper, we propose an adaptive learning paradigm for resource-constrained cross-device federated learning, in which heterogeneous local submodels with varying resources can be jointly trained to produce a global model. Different from existing studies, the submodel structures of different clients are formed by arbitrarily assigned neurons according to their local resources. Along this line, we first design a general resource-adaptive federated learning algorithm, namely RA-Fed, and rigorously prove its convergence with asymptotically optimal rate O(1/√Γ*TQ) under loose assumptions. Furthermore, to address both submodels heterogeneity and data heterogeneity challenges under non-uniform training, we come up with a new server aggregation mechanism RAM-Fed with the same theoretically proved convergence rate. Moreover, we shed light on several key factors impacting convergence, such as minimum coverage rate, data heterogeneity level, submodel induced noises. Finally, we conduct extensive experiments on two types of tasks with three widely used datasets under different experimental settings. Compared with the state-of-the-arts, our methods improve the accuracy up to 10% on average. Particularly, when submodels jointly train with 50% parameters, RAM-Fed achieves comparable accuracy to FedAvg trained with the full model.|在本文中，我们提出了一个资源受限的跨设备联邦学习的在线机机器学习范式，在这个范式中，具有不同资源的异构本地子模型可以被联合训练以产生一个全局模型。与已有的研究不同，不同客户端的子模型结构是由任意分配的神经元根据其局部资源形成的。在此基础上，我们首先设计了一个通用的资源自适应联邦学习算法，即 RA-Fed，并在松散假设下严格证明了它与渐近最优速率 O (1/√ Γ * TQ)的收敛性。此外，为了解决非均匀训练下子模型异构性和数据异构性的问题，我们提出了一种新的服务器聚合机制 RAM-FED，其收敛速度在理论上得到了相同的证明。此外，本文还分析了影响收敛性的几个关键因素，如最小覆盖率、数据异构程度、子模型引起的噪声。最后，在不同的实验环境下，我们利用三个广泛使用的数据集对两类任务进行了广泛的实验。与最先进的方法相比，我们的方法平均提高了10% 的准确度。特别是，当子模型与50% 的参数联合训练时，RAM-Fed 达到了与 FedAvg 训练相当的精度与完整模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Theoretical+Convergence+Guaranteed+Resource-Adaptive+Federated+Learning+with+Mixed+Heterogeneity)|0|
|[Efficient Bi-Level Optimization for Recommendation Denoising](https://doi.org/10.1145/3580305.3599324)|Zongwei Wang, Min Gao, Wentao Li, Junliang Yu, Linxin Guo, Hongzhi Yin||The acquisition of explicit user feedback (e.g., ratings) in real-world recommender systems is often hindered by the need for active user involvement. To mitigate this issue, implicit feedback (e.g., clicks) generated during user browsing is exploited as a viable substitute. However, implicit feedback possesses a high degree of noise, which significantly undermines recommendation quality. While many methods have been proposed to address this issue by assigning varying weights to implicit feedback, two shortcomings persist: (1) the weight calculation in these methods is iteration-independent, without considering the influence of weights in previous iterations, and (2) the weight calculation often relies on prior knowledge, which may not always be readily available or universally applicable. To overcome these two limitations, we model recommendation denoising as a bi-level optimization problem. The inner optimization aims to derive an effective model for the recommendation, as well as guiding the weight determination, thereby eliminating the need for prior knowledge. The outer optimization leverages gradients of the inner optimization and adjusts the weights in a manner considering the impact of previous weights. To efficiently solve this bi-level optimization problem, we employ a weight generator to avoid the storage of weights and a one-step gradient-matching-based loss to significantly reduce computational time. The experimental results on three benchmark datasets demonstrate that our proposed approach outperforms both state-of-the-art general and denoising recommendation models. The code is available at https://github.com/CoderWZW/BOD.|在现实世界的推荐系统中，获得明确的用户反馈(例如评分)常常受到需要积极的用户参与的阻碍。为了缓解这个问题，在用户浏览期间产生的隐式反馈(例如点击)被用作一个可行的替代品。然而，隐式反馈具有高度的噪声，严重影响了推荐质量。虽然已经提出了许多方法来解决这个问题，通过赋予不同的权重隐式反馈，两个缺点仍然存在: (1)这些方法的权重计算是迭代无关的，没有考虑权重的影响在以前的迭代，和(2)权重计算往往依赖于先验知识，这可能并不总是容易获得或普遍适用。为了克服这两个限制，我们将建议去噪建模为双层最佳化问题。内部优化的目的是为推荐建立一个有效的模型，并指导权重的确定，从而消除了对先验知识的需要。外部优化利用了内部优化的梯度，并在考虑以前权重影响的情况下调整权重。为了有效地解决这种双层最佳化问题，我们使用了权重生成器来避免权重的存储，并使用了一步梯度匹配丢失来显著地减少计算时间。在三个基准数据集上的实验结果表明，我们提出的方法优于最先进的一般和去噪推荐模型。密码可在 https://github.com/coderwzw/bod 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Bi-Level+Optimization+for+Recommendation+Denoising)|0|
|[Meta Graph Learning for Long-tail Recommendation](https://doi.org/10.1145/3580305.3599428)|Chunyu Wei, Jian Liang, Di Liu, Zehui Dai, Mang Li, Fei Wang||Highly skewed long-tail item distribution commonly hurts model performance on tail items in recommendation systems, especially for graph-based recommendation models. We propose a novel idea to learn relations among items as an auxiliary graph to enhance the graph-based representation learning and make recommendations collectively in a coupled framework. This raises two challenges, 1) the long-tail downstream information may also bias the auxiliary graph learning, and 2) the learned auxiliary graph may cause negative transfer to the original user-item bipartite graph. We innovatively propose a novel Meta Graph Learning framework for long-tail recommendation (MGL) for solving both challenges. The meta-learning strategy is introduced to the learning of an edge generator, which is first tuned to reconstruct a debiased item co-occurrence matrix, and then virtually evaluated on generating item relations for recommendation. Moreover, we propose a popularity-aware contrastive learning strategy to prevent negative transfer by aligning the confident head item representations with those of the learned auxiliary graph. Experiments on public datasets demonstrate that our proposed model significantly outperforms strong baselines for tail items without compromising the overall performance.|在推荐系统中，高度倾斜的长尾条目分布通常会损害模型在尾条目上的性能，特别是对于基于图的推荐模型。我们提出了一个新的概念来学习项目之间的关系作为一个辅助图，以提高基于图的表示学习和建议集体在一个耦合的框架。这就提出了两个挑战: 1)长尾下游信息也可能使辅助图学习产生偏差; 2)学习的辅助图可能导致原始用户项二部图的负迁移。我们创新地提出了一个新的元图学习框架，用于长尾推荐(MGL) ，以解决这两个挑战。将元学习策略引入到边缘生成器的学习中，首先对边缘生成器进行调整以重建去偏项目共现矩阵，然后对边缘生成器生成推荐项目关系进行虚拟评估。此外，我们还提出了一种基于知名度的对比学习策略，通过将自信头项表示与学习辅助图的表示对齐来防止负迁移。在公共数据集上的实验表明，我们提出的模型在不影响整体性能的情况下，显著优于尾部项目的强基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Meta+Graph+Learning+for+Long-tail+Recommendation)|0|
|[Personalized Federated Learning with Parameter Propagation](https://doi.org/10.1145/3580305.3599464)|Jun Wu, Wenxuan Bao, Elizabeth A. Ainsworth, Jingrui He||With decentralized data collected from diverse clients, a personalized federated learning paradigm has been proposed for training machine learning models without exchanging raw data from local clients. We dive into personalized federated learning from the perspective of privacy-preserving transfer learning, and identify the limitations of previous personalized federated learning algorithms. First, previous works suffer from negative knowledge transferability for some clients, when focusing more on the overall performance of all clients. Second, high communication costs are required to explicitly learn statistical task relatedness among clients. Third, it is computationally expensive to generalize the learned knowledge from experienced clients to new clients. To solve these problems, in this paper, we propose a novel federated parameter propagation (FEDORA) framework for personalized federated learning. Specifically, we reformulate the standard personalized federated learning as a privacy-preserving transfer learning problem, with the goal of improving the generalization performance for every client. The crucial idea behind FEDORA is to learn how to transfer and whether to transfer simultaneously, including (1) adaptive parameter propagation: one client is enforced to adaptively propagate its parameters to others based on their task relatedness (e.g., explicitly measured by distribution similarity), and (2) selective regularization: each client would regularize its local personalized model with received parameters, only when those parameters are positively correlated with the generalization performance of its local model. The experiments on a variety of federated learning benchmarks demonstrate the effectiveness of the proposed FEDORA framework over state-of-the-art personalized federated learning baselines.|通过从不同的客户端收集分散的数据，提出了一种个性化的联邦学习范式，用于训练机器学习模型，而不需要交换来自本地客户端的原始数据。本文从保护隐私的传递学习的角度对个性化联邦学习进行了深入研究，指出了以往个性化联邦学习算法的局限性。首先，以前的作品在更多地关注所有客户的整体表现时，对一些客户来说存在负面的知识可转移性。第二，显式学习客户之间的统计任务相关性需要较高的沟通成本。第三，将从有经验的客户那里学到的知识推广到新的客户那里是计算代价高昂的。为了解决这些问题，本文提出了一种新的个性化联邦学习的联邦参数传播(FEDORA)框架。具体地说，我们将标准的个性化联邦学习重新定义为一个保护隐私的迁移学习问题，目标是提高每个客户端的泛化性能。FEDORA 背后的关键思想是学习如何传输和是否同时传输，包括(1)自适应参数传播: 一个客户端被强制根据其任务相关性(例如，通过分布相似性明确测量)自适应传播其参数到其他客户端，和(2)选择性正则化: 每个客户端将正则化其本地个性化模型与接收参数，只有当这些参数与其本地模型的泛化性能正相关。在各种联邦学习基准上的实验证明了所提出的 FEDORA 框架在最先进的个性化联邦学习基准上的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalized+Federated+Learning+with+Parameter+Propagation)|0|
|[Serverless Federated AUPRC Optimization for Multi-Party Collaborative Imbalanced Data Mining](https://doi.org/10.1145/3580305.3599499)|Xidong Wu, Zhengmian Hu, Jian Pei, Heng Huang|University of Pittsburgh; Duke University|Multi-party collaborative training, such as distributed learning and federated learning, is used to address the big data challenges. However, traditional multi-party collaborative training algorithms were mainly designed for balanced data mining tasks and are intended to optimize accuracy (\emph{e.g.}, cross-entropy). The data distribution in many real-world applications is skewed and classifiers, which are trained to improve accuracy, perform poorly when applied to imbalanced data tasks since models could be significantly biased toward the primary class. Therefore, the Area Under Precision-Recall Curve (AUPRC) was introduced as an effective metric. Although single-machine AUPRC maximization methods have been designed, multi-party collaborative algorithm has never been studied. The change from the single-machine to the multi-party setting poses critical challenges. To address the above challenge, we study the serverless multi-party collaborative AUPRC maximization problem since serverless multi-party collaborative training can cut down the communications cost by avoiding the server node bottleneck, and reformulate it as a conditional stochastic optimization problem in a serverless multi-party collaborative learning setting and propose a new ServerLess biAsed sTochastic gradiEnt (SLATE) algorithm to directly optimize the AUPRC. After that, we use the variance reduction technique and propose ServerLess biAsed sTochastic gradiEnt with Momentum-based variance reduction (SLATE-M) algorithm to improve the convergence rate, which matches the best theoretical convergence result reached by the single-machine online method. To the best of our knowledge, this is the first work to solve the multi-party collaborative AUPRC maximization problem.|多方协作培训，如分布式学习和联合学习，被用来解决大数据的挑战。然而，传统的多方协同训练算法主要是针对平衡的数据挖掘任务而设计的，其目的是优化精度(例如: 交叉熵)。许多实际应用中的数据分布是倾斜的，分类器经过训练以提高准确性，但在应用于不平衡的数据任务时表现不佳，因为模型可能明显偏向于主类。因此，引入精确回忆曲线下面积(AUPRC)作为一个有效的度量指标。虽然单机 AUPRC 最大化方法已经设计出来，但是多方协作算法还没有得到研究。从单一机器设置到多方设置的变化提出了关键的挑战。为了解决上述问题，我们研究了无服务器多方协作的 AUPRC 最大化问题，因为无服务器多方协作培训可以通过避免服务器节点瓶颈来降低通信成本，并将其重新表述为无服务器多方协作环境中的条件随机最佳化问题，提出了一种新的无服务器偏置 sTo侯机梯度(slATE)算法来直接优化 AUPRC，该算法可以用于解决无服务器多方协作的合作学习。在此基础上，利用方差减少技术，提出了基于动量方差减少(SLATE-M)的 ServerLess 偏向随机梯度算法，提高了算法的收敛速度，达到了单机在线算法的最佳理论收敛效果。据我们所知，这是第一个解决多方协作 AUPRC 最大化问题的工作。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Serverless+Federated+AUPRC+Optimization+for+Multi-Party+Collaborative+Imbalanced+Data+Mining)|0|
|[MSSRNet: Manipulating Sequential Style Representation for Unsupervised Text Style Transfer](https://doi.org/10.1145/3580305.3599438)|Yazheng Yang, Zhou Zhao, Qi Liu|Zhejiang University; The University of Hong Kong|Unsupervised text style transfer task aims to rewrite a text into target style while preserving its main content. Traditional methods rely on the use of a fixed-sized vector to regulate text style, which is difficult to accurately convey the style strength for each individual token. In fact, each token of a text contains different style intensity and makes different contribution to the overall style. Our proposed method addresses this issue by assigning individual style vector to each token in a text, allowing for fine-grained control and manipulation of the style strength. Additionally, an adversarial training framework integrated with teacher-student learning is introduced to enhance training stability and reduce the complexity of high-dimensional optimization. The results of our experiments demonstrate the efficacy of our method in terms of clearly improved style transfer accuracy and content preservation in both two-style transfer and multi-style transfer settings.|无监督文本样式转换任务的目标是在保留文本主要内容的同时将文本重写成目标样式。传统的方法依赖于使用固定大小的向量来调整文本样式，这很难准确地表达每个单独标记的样式强度。事实上，文本的每一个标记都包含着不同的风格强度，并对整体风格做出不同的贡献。我们提出的方法通过为文本中的每个标记分配单独的样式向量来解决这个问题，从而允许对样式强度进行细粒度控制和操作。此外，为了提高训练的稳定性，降低高维优化的复杂性，提出了一种结合师生学习的对抗性训练框架。实验结果表明，该方法在两种类型和多种类型的转移设置下，均能明显提高文体转移的准确性和内容保存率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MSSRNet:+Manipulating+Sequential+Style+Representation+for+Unsupervised+Text+Style+Transfer)|0|
|[Knowledge Graph Self-Supervised Rationalization for Recommendation](https://doi.org/10.1145/3580305.3599400)|Yuhao Yang, Chao Huang, Lianghao Xia, Chunzhen Huang|The University of Hong Kong; Tencent|In this paper, we introduce a new self-supervised rationalization method, called KGRec, for knowledge-aware recommender systems. To effectively identify informative knowledge connections, we propose an attentive knowledge rationalization mechanism that generates rational scores for knowledge triplets. With these scores, KGRec integrates generative and contrastive self-supervised tasks for recommendation through rational masking. To highlight rationales in the knowledge graph, we design a novel generative task in the form of masking-reconstructing. By masking important knowledge with high rational scores, KGRec is trained to rebuild and highlight useful knowledge connections that serve as rationales. To further rationalize the effect of collaborative interactions on knowledge graph learning, we introduce a contrastive learning task that aligns signals from knowledge and user-item interaction views. To ensure noise-resistant contrasting, potential noisy edges in both graphs judged by the rational scores are masked. Extensive experiments on three real-world datasets demonstrate that KGRec outperforms state-of-the-art methods. We also provide the implementation codes for our approach at https://github.com/HKUDS/KGRec.|本文针对知识感知推荐系统，提出了一种新的自监督合理化方法 KGRec。为了有效地识别信息知识连接，我们提出了一种注意的知识合理化机制，为知识三元组生成合理的分数。根据这些分数，KGRec 通过合理的掩蔽将生成性和对比性自我监督任务集成到推荐系统中。为了突出知识图中的基本原理，我们设计了一个新的生成任务，即掩蔽-重构。通过用高理性分数掩盖重要的知识，KGRec 被训练重建和突出作为基本原理的有用的知识联系。为了进一步合理化协作交互对知识图学习的影响，我们引入了一个对比学习任务，该任务从知识和用户项目交互视图中调整信号。为了确保抗噪声的对比，在两个图的潜在噪声边缘判断有理分数被掩盖。在三个真实世界数据集上的大量实验表明，KGRec 的性能优于最先进的方法。我们亦会在 https://github.com/hkuds/kgrec 为我们的方法提供实施守则。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Graph+Self-Supervised+Rationalization+for+Recommendation)|0|
|[FedCP: Separating Feature Information for Personalized Federated Learning via Conditional Policy](https://doi.org/10.1145/3580305.3599345)|Jianqing Zhang, Yang Hua, Hao Wang, Tao Song, Zhengui Xue, Ruhui Ma, Haibing Guan|Louisiana State University; Shanghai Jiao Tong University; Queen’s University Belfast|Recently, personalized federated learning (pFL) has attracted increasing attention in privacy protection, collaborative learning, and tackling statistical heterogeneity among clients, e.g., hospitals, mobile smartphones, etc. Most existing pFL methods focus on exploiting the global information and personalized information in the client-level model parameters while neglecting that data is the source of these two kinds of information. To address this, we propose the Federated Conditional Policy (FedCP) method, which generates a conditional policy for each sample to separate the global information and personalized information in its features and then processes them by a global head and a personalized head, respectively. FedCP is more fine-grained to consider personalization in a sample-specific manner than existing pFL methods. Extensive experiments in computer vision and natural language processing domains show that FedCP outperforms eleven state-of-the-art methods by up to 6.69%. Furthermore, FedCP maintains its superiority when some clients accidentally drop out, which frequently happens in mobile settings. Our code is public at https://github.com/TsingZ0/FedCP.|近年来，个性化联邦学习(pFL)在保护个人隐私、合作学习以及处理客户之间的统计异质性等方面受到越来越多的关注，例如医院、移动智能手机等。现有的 pFL 方法大多侧重于利用客户端模型参数中的全局信息和个性化信息，而忽视了数据是这两类信息的来源。为了解决这个问题，我们提出了联邦条件策略(FedCP)方法，该方法为每个样本生成一个条件策略来分离其特征中的全局信息和个性化信息，然后分别通过一个全局头和一个个性化头来处理它们。与现有的 pFL 方法相比，FedCP 更加细粒度地以特定于样本的方式考虑个性化。在计算机视觉和自然语言处理领域的大量实验表明，FedCP 比11种最先进的方法的性能提高了6.69% 。此外，当一些客户端意外退出时，FedCP 仍然保持其优势，这种情况在移动设置中经常发生。我们的代码在 https://github.com/tsingz0/fedcp 是公开的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedCP:+Separating+Feature+Information+for+Personalized+Federated+Learning+via+Conditional+Policy)|0|
|[CFGL-LCR: A Counterfactual Graph Learning Framework for Legal Case Retrieval](https://doi.org/10.1145/3580305.3599273)|Kun Zhang, Chong Chen, Yuanzhuo Wang, Qi Tian, Long Bai||Legal case retrieval, which aims to find relevant cases based on a short case description, serves as an important part of modern legal systems. Despite the success of existing retrieval methods based on Pretrained Language Models, there are still two issues in legal case retrieval that have not been well considered before. First, existing methods underestimate the semantics associations among legal elements, e.g., law articles and crimes, which played an essential role in legal case retrieval. These methods only adopt the pre-training language model to encode the whole legal case, instead of distinguishing different legal elements in the legal case. They randomly split a legal case into different segments, which may break the completeness of each legal element. Second, due to the difficulty in annotating the relevant labels of similar cases, legal case retrieval inevitably faces the problem of lacking training data. In this paper, we propose a counterfactual graph learning framework for legal case retrieval. Concretely, to overcome the above challenges, we transform the legal case document into a graph and model the semantics of the legal elements through a graph neural network. To alleviate the low resource and learn the causal relationship between the semantics of legal elements and relevance, a counterfactual data generator is designed to augment counterfactual data and enhance legal case representation. Extensive experiments based on two publicly available legal benchmarks demonstrate that our CFGL-LCR can significantly outperform previous state-of-the-art methods in legal case retrieval.|法律案例检索是现代法律体系的重要组成部分，其目的是在简短的案例描述的基础上发现相关案例。尽管现有的基于预训练语言模型的检索方法取得了成功，但在法律案例检索中仍然存在两个以前没有得到很好考虑的问题。首先，现有的方法低估了法律要素之间的语义联系，如法律条文和犯罪，这些语义联系在法律案例检索中发挥了重要作用。这些方法只采用预训练语言模式对整个法律案件进行编码，而没有区分法律案件中的不同法律要素。他们随机地将一个法律案件分成不同的部分，这可能会打破每个法律要素的完整性。其次，由于类似案件相关标注的困难，法律案件检索不可避免地面临缺乏训练数据的问题。本文提出了一个用于法律案例检索的反事实图学习框架。具体来说，为了克服上述挑战，我们将法律案例文档转化为一个图形，并通过一个图神经网络对法律要素的语义进行建模。为了解决法律要素语义与相关性之间的因果关系，减轻资源不足的问题，设计了一个反事实数据生成器，以增强反事实数据，提高法律案件的表述能力。基于两个公开可用的法律基准的大量实验表明，我们的 CFGL-LCR 在法律案件检索方面可以显著优于以前的最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CFGL-LCR:+A+Counterfactual+Graph+Learning+Framework+for+Legal+Case+Retrieval)|0|
|[DM-PFL: Hitchhiking Generic Federated Learning for Efficient Shift-Robust Personalization](https://doi.org/10.1145/3580305.3599311)|Wenhao Zhang, Zimu Zhou, Yansheng Wang, Yongxin Tong||Personalized federated learning collaboratively trains client-specific models, which holds potential for various mobile and IoT applications with heterogeneous data. However, existing solutions are vulnerable to distribution shifts between training and test data, and involve high training workloads on local devices. These two shortcomings hinder the practical usage of personalized federated learning on real-world mobile applications. To overcome these drawbacks, we explore efficient shift-robust personalization for federated learning. The principle is to hitchhike the global model to improve the shift-robustness of personalized models with minimal extra training overhead. To this end, we present DM-PFL, a novel framework that utilizes a dual masking mechanism to train both global and personalized models with weight-level parameter sharing and end-to-end sparse training. Evaluations on various datasets show that our methods not only improve the test accuracy in presence of test-time distribution shifts but also save the communication and computation costs compared to state-of-the-art personalized federated learning schemes.|个性化联邦学习协同培训特定于客户端的模型，这为各种具有异构数据的移动和物联网应用提供了潜力。然而，现有的解决方案很容易受到培训和测试数据之间分布转移的影响，并且需要在本地设备上进行高负荷的培训。这两个缺点阻碍了个性化联邦学习在实际移动应用中的实际应用。为了克服这些缺点，我们探索了联邦学习中有效的移位鲁棒个性化方法。其原理是搭便车全局模型，以最小的额外训练开销提高个性化模型的移位鲁棒性。为此，我们提出了 DM-PFL，一个新的框架，利用双掩蔽机制训练全局和个性化的模型与权重水平参数共享和端到端稀疏训练。对各种数据集的评估表明，与现有的个性化联邦学习方案相比，该方法不仅提高了测试时间分布偏移情况下的测试精度，而且节省了通信和计算成本。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DM-PFL:+Hitchhiking+Generic+Federated+Learning+for+Efficient+Shift-Robust+Personalization)|0|
|[Efficient Approximation Algorithms for Spanning Centrality](https://doi.org/10.1145/3580305.3599323)|Shiqi Zhang, Renchi Yang, Jing Tang, Xiaokui Xiao, Bo Tang|Southern University of Science and Technology; The Hong Kong University of Science and Technology (Guangzhou); Hong Kong Baptist University; National University of Singapore|Given a graph $\mathcal{G}$, the spanning centrality (SC) of an edge $e$ measures the importance of $e$ for $\mathcal{G}$ to be connected. In practice, SC has seen extensive applications in computational biology, electrical networks, and combinatorial optimization. However, it is highly challenging to compute the SC of all edges (AESC) on large graphs. Existing techniques fail to deal with such graphs, as they either suffer from expensive matrix operations or require sampling numerous long random walks. To circumvent these issues, this paper proposes TGT and its enhanced version TGT+, two algorithms for AESC computation that offers rigorous theoretical approximation guarantees. In particular, TGT remedies the deficiencies of previous solutions by conducting deterministic graph traversals with carefully-crafted truncated lengths. TGT+ further advances TGT in terms of both empirical efficiency and asymptotic performance while retaining result quality, based on the combination of TGT with random walks and several additional heuristic optimizations. We experimentally evaluate TGT+ against recent competitors for AESC using a variety of real datasets. The experimental outcomes authenticate that TGT+ outperforms the state of the arts often by over one order of magnitude speedup without degrading the accuracy.|给定一个图 $mathal { G } $，边 $e $的生成中心性(SC)度量 $e $对于要连接的 $mathal { G } $的重要性。在实践中，SC 已经在计算生物学、电力网络和组合优化等领域得到了广泛的应用。然而，计算大图上所有边的 SC (AESC)是一个非常具有挑战性的问题。现有的技术无法处理这样的图，因为它们要么需要进行昂贵的矩阵运算，要么需要对大量的长随机游动进行采样。为了解决这些问题，本文提出了 TGT 及其改进版本 TGT + ，这两种 AESC 计算算法提供了严格的理论近似保证。特别是，TGT 通过使用精心设计的截断长度进行确定性图遍历，弥补了以前解决方案的缺陷。TGT + 基于 TGT 与随机游动的结合以及几个附加的启发式优化，在保持结果质量的同时，进一步提高了 TGT 的经验有效性和渐近性能。我们使用各种实际数据集对 AESC 的最近竞争对手进行了 TGT + 的实验评估。实验结果表明，在不降低准确性的情况下，TGT + 的性能通常比现有技术水平高出一个数量级。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Approximation+Algorithms+for+Spanning+Centrality)|0|
|[Improving Search Clarification with Structured Information Extracted from Search Results](https://doi.org/10.1145/3580305.3599389)|Ziliang Zhao, Zhicheng Dou, Yu Guo, Zhao Cao, Xiaohua Cheng||Search clarification in conversational search systems exhibits a clarification pane composed of several candidate aspect items and a clarifying question. To generate a pane, existing studies usually rely on unstructured document texts. However, important structured information in search results is not effectively considered, making the generated panes inaccurate in some cases. In this paper, we emphasize the importance of structured information in search results for improving search clarification. We propose enhancing unstructured documents with two kinds of structured information: one is "In-List'' relation obtained from HTML list structures, which helps extract groups of high-quality items with abundant parallel information. Another is "Is-A'' relation extracted from knowledge bases, which is helpful to generate good questions with explicit prompts. To avoid introducing excessive noises, we design a relation selection process to filter out ineffective relations. We further design a BART-based model for generating clarification panes. The experimental results show that the structured information is good supplement for generating high-quality clarification panes.|会话搜索系统中的搜索澄清显示一个由几个候选方面项和一个澄清问题组成的澄清窗格。要生成窗格，现有的研究通常依赖于非结构化文档文本。但是，搜索结果中的重要结构化信息没有得到有效考虑，使得生成的窗格在某些情况下不准确。在本文中，我们强调结构化信息在搜索结果中对提高搜索清晰度的重要性。我们提出了利用两种结构化信息来增强非结构化文档: 一种是从 HTML 列表结构中获取的“ In-List”关系，它有助于提取具有大量并行信息的高质量项目组。另一种是从知识库中提取的“是-A”关系，它有助于在明确的提示下产生好的问题。为了避免引入过多的噪声，我们设计了一个关系选择过程来滤除无效关系。我们进一步设计了一个基于 BART 的生成澄清窗格的模型。实验结果表明，结构化信息对生成高质量的澄清面板有很好的补充作用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Search+Clarification+with+Structured+Information+Extracted+from+Search+Results)|0|
|[Dense Representation Learning and Retrieval for Tabular Data Prediction](https://doi.org/10.1145/3580305.3599305)|Lei Zheng, Ning Li, Xianyu Chen, Quan Gan, Weinan Zhang||Data science is concerned with mining data patterns from a database, which is assembled by tabular data. As the routine of machine learning, most of the previous work mining the tabular data's pattern based on a single instance. However, they neglect the similar tabular data instances that could help make the label prediction of the target data instance. Recently, some retrieval-based methods for tabular data label prediction have been proposed, which, however, treat the data as sparse vectors to perform the retrieval, which fails to make use of the semantic information of the tabular data. To address such a problem, in this paper, we propose a novel framework of dense retrieval on tabular data (DERT) to support flexible data representation learning and effective label prediction on tabular data. DERT consists of two major components: (i) the encoder that makes the tabular data as embeddings, which could be trained by flexible neural networks and auxiliary loss functions; (ii) the retrieval and prediction component, which makes use of similar rows in the table to make label prediction of the target row. We test DERT on two tasks based on five real-world datasets and experimental results show that DERT achieves consistent improvements over the state-of-the-art and various baselines.|数据科学涉及从数据库中挖掘数据模式，数据库由表格数据组装而成。作为机器学习的例程，以往的大部分工作都是基于单个实例挖掘表格数据的模式。但是，它们忽略了可以帮助对目标数据实例进行标签预测的类似表格数据实例。最近，人们提出了一些基于检索的表格数据标签预测方法，但这些方法将数据作为稀疏向量进行检索，未能充分利用表格数据的语义信息。为了解决这一问题，本文提出了一种新的表格数据密集检索框架(DERT) ，以支持灵活的表格数据表示学习和有效的标签预测。DERT 包括两个主要部分: (i)将表格数据作为嵌入的编码器，可以通过灵活的神经网络和辅助损失函数进行训练; (ii)检索和预测部分，利用表格中的相似行对目标行进行标签预测。我们基于五个真实世界的数据集对两个任务进行了 DERT 测试，实验结果表明 DERT 在最先进的和不同的基线上取得了一致的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dense+Representation+Learning+and+Retrieval+for+Tabular+Data+Prediction)|0|
|[A Sublinear Time Algorithm for Opinion Optimization in Directed Social Networks via Edge Recommendation](https://doi.org/10.1145/3580305.3599247)|Xiaotian Zhou, Liwang Zhu, Wei Li, Zhongzhi Zhang||In this paper, we study the opinion maximization problem for the leader-follower DeGroot model of opinion dynamics in a social network modelled by a directed graph with n nodes, where a small number of nodes are competing leader nodes with binary opposing opinions 0 or 1, and the rest are follower nodes. We address the problem of maximizing the overall opinion by adding k ⇐ n new edges, where each edge is incident to a 1-leader and a follower. We prove that the objective function is monotone and submodular, and then propose a deterministic greedy algorithm with an approximation ratio (1-1 over e) and O(n 3 ) running time. We then develop a fast sampling algorithm based on l-truncated absorbing random walks and sample-materialization techniques, which has sublinear time complexity O(kn 1/2 log 3/2 n/ε 3 ) for any error parameter ε > 0. We provide extensive experiments on real networks to evaluate the performance of our algorithms. The results show that for undirected graphs our fast sampling algorithm outperforms the state-of-the-art method in terms of efficiency and effectiveness. While for directed graphs our fast sampling algorithm is as effective as our deterministic greedy algorithm, both of which are much better than the baseline strategies. Moreover, our fast algorithm is scalable to large directed graphs with over 41 million nodes.|本文研究了一类社会网络中的领导者-跟随者 DeGroot 意见动力学模型的意见最大化问题，该模型由 n 个节点的有向图建模，其中少数节点为竞争的领导者节点，其余为跟随者节点。我们通过添加 kn 个新边来解决整体意见最大化的问题，其中每个边都与一个1-领导者和一个跟随者相关。证明了目标函数是单调的、子模的，提出了一种具有逼近比(1-1/e)和运行时间为 O (n3)的确定性贪婪算法。然后提出了一种基于 l 截断吸收随机游动和样本物化技术的快速采样算法，该算法对于任意误差参数 ε > 0，具有次线性时间复杂度 O (kn1/2 log 3/2 n/ε3)。我们提供广泛的实验在真实的网络，以评估我们的算法的性能。结果表明，对于无向图，我们的快速抽样算法在效率和有效性方面优于现有的方法。而对于有向图，我们的快速采样算法和确定性贪婪算法一样有效，两者都比基线策略好得多。此外，我们的快速算法可扩展到超过4100万个节点的大型有向图。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Sublinear+Time+Algorithm+for+Opinion+Optimization+in+Directed+Social+Networks+via+Edge+Recommendation)|0|
|[Path-Specific Counterfactual Fairness for Recommender Systems](https://doi.org/10.1145/3580305.3599462)|Yaochen Zhu, Jing Ma, Liang Wu, Qi Guo, Liangjie Hong, Jundong Li|University of Virginia; LinkedIn Inc.|Recommender systems (RSs) have become an indispensable part of online platforms. With the growing concerns of algorithmic fairness, RSs are not only expected to deliver high-quality personalized content, but are also demanded not to discriminate against users based on their demographic information. However, existing RSs could capture undesirable correlations between sensitive features and observed user behaviors, leading to biased recommendations. Most fair RSs tackle this problem by completely blocking the influences of sensitive features on recommendations. But since sensitive features may also affect user interests in a fair manner (e.g., race on culture-based preferences), indiscriminately eliminating all the influences of sensitive features inevitably degenerate the recommendations quality and necessary diversities. To address this challenge, we propose a path-specific fair RS (PSF-RS) for recommendations. Specifically, we summarize all fair and unfair correlations between sensitive features and observed ratings into two latent proxy mediators, where the concept of path-specific bias (PS-Bias) is defined based on path-specific counterfactual inference. Inspired by Pearl's minimal change principle, we address the PS-Bias by minimally transforming the biased factual world into a hypothetically fair world, where a fair RS model can be learned accordingly by solving a constrained optimization problem. For the technical part, we propose a feasible implementation of PSF-RS, i.e., PSF-VAE, with weakly-supervised variational inference, which robustly infers the latent mediators such that unfairness can be mitigated while necessary recommendation diversities can be maximally preserved simultaneously. Experiments conducted on semi-simulated and real-world datasets demonstrate the effectiveness of PSF-RS.|推荐系统已经成为在线平台不可或缺的一部分。随着对算法公平性的日益关注，RSS 不仅被期望提供高质量的个性化内容，而且被要求不因用户的人口统计信息而歧视用户。然而，现有的 RSS 可能捕获敏感特性和观察到的用户行为之间不希望看到的相关性，从而导致有偏见的推荐。大多数公平的 RSS 通过完全屏蔽敏感特性对推荐的影响来解决这个问题。但是，由于敏感特性也可能以公平的方式影响用户的兴趣(例如，基于文化的偏好的种族) ，不加区分地消除敏感特性的所有影响必然会降低推荐的质量和必要的多样性。为了应对这一挑战，我们提出了一个路径特定公平 RS (PSF-RS)的建议。具体而言，我们将敏感特征和观察评分之间的所有公平和不公平的相关性总结为两个潜在的代理中介，其中路径特异性偏倚(PS-Bias)的概念是基于路径特异性反事实推断定义的。受珀尔的最小改变原则的启发，我们通过最小化地将有偏见的现实世界转化为一个假设的公平世界，在这个假设的公平世界中，可以通过解决一个受限制的最佳化问题来相应地学习一个公平的遥感模型，从而解决偏差问题。在技术部分，我们提出了一种可行的 PSF-RS 实现方法，即 PSF-VAE，该方法利用弱监督变分推理强有力地推导出潜在的中介因子，从而在最大限度地保留必要的推荐多样性的同时减少不公平性。在半模拟和真实数据集上进行的实验证明了 PSF-RS 算法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Path-Specific+Counterfactual+Fairness+for+Recommender+Systems)|0|
|[Capturing Conversion Rate Fluctuation during Sales Promotions: A Novel Historical Data Reuse Approach](https://doi.org/10.1145/3580305.3599788)|Zhangming Chan, Yu Zhang, Shuguang Han, Yong Bai, XiangRong Sheng, Siyuan Lou, Jiacen Hu, Baolin Liu, Yuning Jiang, Jian Xu, Bo Zheng|Alibaba Group; University of Science and Technology Beijing; Nanjing University|Conversion rate (CVR) prediction is one of the core components in online recommender systems, and various approaches have been proposed to obtain accurate and well-calibrated CVR estimation. However, we observe that a well-trained CVR prediction model often performs sub-optimally during sales promotions. This can be largely ascribed to the problem of the data distribution shift, in which the conventional methods no longer work. To this end, we seek to develop alternative modeling techniques for CVR prediction. Observing similar purchase patterns across different promotions, we propose reusing the historical promotion data to capture the promotional conversion patterns. Herein, we propose a novel \textbf{H}istorical \textbf{D}ata \textbf{R}euse (\textbf{HDR}) approach that first retrieves historically similar promotion data and then fine-tunes the CVR prediction model with the acquired data for better adaptation to the promotion mode. HDR consists of three components: an automated data retrieval module that seeks similar data from historical promotions, a distribution shift correction module that re-weights the retrieved data for better aligning with the target promotion, and a TransBlock module that quickly fine-tunes the original model for better adaptation to the promotion mode. Experiments conducted with real-world data demonstrate the effectiveness of HDR, as it improves both ranking and calibration metrics to a large extent. HDR has also been deployed on the display advertising system in Alibaba, bringing a lift of $9\%$ RPM and $16\%$ CVR during Double 11 Sales in 2022.|转化率(CVR)预测是在线推荐系统的核心组成部分之一，为了获得准确、标定良好的 CVR 估计，人们提出了各种方法。然而，我们观察到，训练有素的 CVR 预测模型在促销期间往往表现不佳。这在很大程度上归因于数据分布偏移的问题，在这个问题中，传统的方法不再起作用。为此，我们寻求发展可替代的 CVR 预测建模技术。通过观察不同促销活动中相似的购买模式，我们建议重用历史促销数据来捕获促销转换模式。在这里，我们提出了一种新的 textbf { H }历史 textbf { D } ata textbf { R } euse (textbf { HDR })方法，首先检索历史上相似的促销数据，然后用所获得的数据对 CVR 预测模型进行微调，以更好地适应促销模式。人类发展报告由三个组成部分组成: 一个自动数据检索模块，从历史促销活动中寻找类似数据; 一个分配转移校正模块，重新加权检索的数据，以便更好地与目标促销活动保持一致; 一个 TransBlock 模块，快速微调原始模型，以便更好地适应促销模式。利用实际数据进行的实验证明了 HDR 的有效性，因为它在很大程度上改善了排序和校准指标。HDR 也已经部署在阿里巴巴的显示广告系统上，在2022年双11销售期间，带来了9% 的每分钟转速和16% 的 CVR。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Capturing+Conversion+Rate+Fluctuation+during+Sales+Promotions:+A+Novel+Historical+Data+Reuse+Approach)|0|
|[SAMD: An Industrial Framework for Heterogeneous Multi-Scenario Recommendation](https://doi.org/10.1145/3580305.3599955)|Zhaoxin Huan, Ang Li, Xiaolu Zhang, Xu Min, Jieyu Yang, Yong He, Jun Zhou||Industrial recommender systems usually need to serve multiple scenarios at the same time. In practice, there are various heterogeneous scenarios, since users frequently engage in scenarios with varying intentions and the items within each scenario typically belong to diverse categories. Existing works of multi-scenario recommendation mainly focus on modeling homogeneous scenarios which have similar data distributions. They equally transfer knowledge to each scenario without considering the diversity of heterogeneous scenarios. In this paper, we argue that the heterogeneity in multi-scenario recommendations is a key problem that needs to be solved. To this end, we propose an industrial framework named Scenario-Aware Model-Agnostic Meta Distillation (SAMD) for the multi-scenario recommendation. SAMD aims to provide scenario-aware and model-agnostic knowledge sharing across heterogeneous scenarios by modeling scenarios' relationship and conducting heterogeneous knowledge distillation. Specifically, SAMD first measures the comprehensive representation of each scenario and then proposes a novel meta distillation paradigm to conduct scenario-aware knowledge sharing. The meta network first establishes the potential scenarios' relationships and generates the strategies of knowledge sharing for each scenario. Then the heterogeneous knowledge distillation utilizes scenario-aware strategies to share knowledge across heterogeneous scenarios through intermediate features distillation without the restriction of the model architecture. In this way, SAMD shares knowledge across heterogeneous scenarios in a scenario-aware and model-agnostic manner, which addresses the problem of heterogeneity. Compared with other state-of-the-art methods, extensive offline experiments, and online A/B testing demonstrate the superior performance of the proposed SAMD framework, especially in heterogeneous scenarios.|工业推荐系统通常需要同时服务于多个场景。在实践中，存在各种不同的场景，因为用户经常参与具有不同意图的场景，并且每个场景中的项目通常属于不同的类别。现有的多场景推荐主要针对具有相似数据分布的同类场景进行建模。它们同样地将知识转移到每个场景，而不考虑异构场景的多样性。在本文中，我们认为多情景推荐中的异构性是一个需要解决的关键问题。为此，我们提出了一个名为场景感知模型-不可知元精馏(SAMD)的工业框架，用于多场景推荐。SAMD 旨在通过对异构场景关系建模和异构知识提取，实现异构场景间的场景感知和模型无关知识共享。具体来说，SAMD 首先度量每个场景的全面表示，然后提出一种新的元精馏范式来进行场景感知的知识共享。元网络首先建立潜在场景之间的关系，并为每个场景生成知识共享策略。然后在不受模型体系结构限制的前提下，通过中间特征提取，利用场景感知策略实现异构场景间的知识共享。通过这种方式，SAMD 以场景感知和模型无关的方式在异构场景之间共享知识，这解决了异构性问题。与其他最先进的方法相比，大量的离线实验和在线 A/B 测试证明了所提出的 SAMD 框架的优越性能，特别是在异构场景中。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SAMD:+An+Industrial+Framework+for+Heterogeneous+Multi-Scenario+Recommendation)|0|
|[Learning Discrete Document Representations in Web Search](https://doi.org/10.1145/3580305.3599854)|Rong Huang, Danfeng Zhang, Weixue Lu, Han Li, Meng Wang, Daiting Shi, Jun Fan, Zhicong Cheng, Simiu Gu, Dawei Yin||Product quantization (PQ) has been usually applied to dense retrieval (DR) of documents thanks to its competitive time, memory efficiency and compatibility with other approximate nearest search (ANN) methods. Originally, PQ was learned to minimize the reconstruction loss, i.e., the distortions between the original dense embeddings and the reconstructed embeddings after quantization. Unfortunately, such an objective is inconsistent with the goal of selecting ground-truth documents for the input query, which may cause a severe loss of retrieval quality. Recent research has primarily concentrated on jointly training the biencoders and PQ to ensure consistency for improved performance. However, it is still difficult to design an approach that can cope with challenges like discrete representation collapse, mining informative negatives, and deploying effective embedding-based retrieval (EBR) systems in a real search engine. In this paper, we propose a Two-stage Multi-task Joint training technique (TMJ) to learn discrete document representations, which is simple and effective for real-world practical applications. In the first stage, the PQ centroid embeddings are regularized by the dense retrieval loss, which ensures the distinguishability of the quantized vectors and preserves the retrieval quality of dense embeddings. In the second stage, a PQ-oriented sample mining strategy is introduced to explore more informative negatives and further improve the performance. Offline evaluations are performed on a public benchmark (MS MARCO) and two private real-world web search datasets, where our method notably outperforms the SOTA PQ methods both in Recall and Mean Reciprocal Ranking (MRR). Besides, online experiments are conducted to validate that our technique can significantly provide high-quality vector quantization. Moreover, our joint training framework has been successfully applied to a billion-scale web search system.|产品量化(PQ)由于其具有竞争时间、存储效率以及与其他近似最近搜索(ANN)方法的兼容性等优点，被广泛应用于文档的密集检索(DR)。最初，PQ 学会了尽量减少重建损失，即原始的密集嵌入和量化后的重建嵌入之间的扭曲。遗憾的是，这样的目标不符合为输入查询选择地面真实文档的目标，这可能导致检索质量的严重损失。最近的研究主要集中在联合培训双向编码器和 PQ，以确保一致性，提高性能。然而，设计一种能够应对离散表示崩溃、挖掘信息否定性以及在实际搜索引擎中部署有效的嵌入式检索(EBR)系统等挑战的方法仍然十分困难。本文提出了一种两阶段多任务联合训练技术(TMJ)来学习离散文档表示，该方法简单有效，适用于实际应用。在第一阶段，PQ 质心嵌入通过密集检索损失进行正则化，保证了量化向量的可区分性，同时保持了密集嵌入的检索质量。在第二阶段，引入了一个面向 PQ 的样本挖掘策略，以探索更多的负信息，并进一步提高性能。离线评估是在一个公共基准(MS MARCO)和两个私有的真实世界网络搜索数据集上进行的，其中我们的方法明显优于召回和平均互惠排名(MRR)中的 SOTA PQ 方法。此外，还进行了在线实验，以验证我们的技术能够显著提供高质量的向量量化。此外，我们的联合培训框架已经成功地应用于一个十亿规模的网络搜索系统。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Discrete+Document+Representations+in+Web+Search)|0|
|[AdSEE: Investigating the Impact of Image Style Editing on Advertisement Attractiveness](https://doi.org/10.1145/3580305.3599770)|Liyao Jiang, Chenglin Li, Haolan Chen, Xiaodong Gao, Xinwang Zhong, Yang Qiu, Shani Ye, Di Niu|Platform and Content Group, Tencent; University of Alberta|Online advertisements are important elements in e-commerce sites, social media platforms, and search engines. With the increasing popularity of mobile browsing, many online ads are displayed with visual information in the form of a cover image in addition to text descriptions to grab the attention of users. Various recent studies have focused on predicting the click rates of online advertisements aware of visual features or composing optimal advertisement elements to enhance visibility. In this paper, we propose Advertisement Style Editing and Attractiveness Enhancement (AdSEE), which explores whether semantic editing to ads images can affect or alter the popularity of online advertisements. We introduce StyleGAN-based facial semantic editing and inversion to ads images and train a click rate predictor attributing GAN-based face latent representations in addition to traditional visual and textual features to click rates. Through a large collected dataset named QQ-AD, containing 20,527 online ads, we perform extensive offline tests to study how different semantic directions and their edit coefficients may impact click rates. We further design a Genetic Advertisement Editor to efficiently search for the optimal edit directions and intensity given an input ad cover image to enhance its projected click rates. Online A/B tests performed over a period of 5 days have verified the increased click-through rates of AdSEE-edited samples as compared to a control group of original ads, verifying the relation between image styles and ad popularity. We open source the code for AdSEE research at https://github.com/LiyaoJiang1998/adsee.|在线广告是电子商务网站、社交媒体平台和搜索引擎的重要组成部分。随着手机浏览的日益普及，许多在线广告除了文字描述外，还以封面图像的形式显示视觉信息，以吸引用户的注意力。最近的各种研究侧重于预测了解视觉特征的在线广告的点击率或构成最佳的广告元素以提高可见度。本文提出了广告风格编辑与吸引力增强(AdSee)的概念，探讨了对广告图像进行语义编辑是否会影响或改变网络广告的受欢迎程度。我们将基于 StyleGAN 的人脸语义编辑和反转技术引入到广告图像中，除了传统的视觉和文本特征外，我们还训练了一个基于 GAN 的人脸潜在表征的点击率预测器来提高点击率。通过一个名为 QQ-AD 的大型数据集，包含20,527个在线广告，我们进行了大量的离线测试来研究不同的语义方向及其编辑系数如何影响点击率。我们进一步设计了一个遗传广告编辑器，以有效地搜索最佳的编辑方向和强度给予输入广告封面图像，以提高其预计点击率。为期5天的在线 A/B 测试已经证实，与对照组的原始广告相比，广告编辑样本的点击率有所提高，验证了图像风格与广告流行度之间的关系。我们在 https://github.com/liyaojiang1998/AdSEE 上开源了 AdSEE 研究的代码。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AdSEE:+Investigating+the+Impact+of+Image+Style+Editing+on+Advertisement+Attractiveness)|0|
|[Adaptive Graph Contrastive Learning for Recommendation](https://doi.org/10.1145/3580305.3599768)|Yangqin Jiang, Chao Huang, Lianghao Huang|University of Hong Kong|Recently, graph neural networks (GNNs) have been successfully applied to recommender systems as an effective collaborative filtering (CF) approach. The key idea of GNN-based recommender system is to recursively perform the message passing along the user-item interaction edge for refining the encoded embeddings, relying on sufficient and high-quality training data. Since user behavior data in practical recommendation scenarios is often noisy and exhibits skewed distribution, some recommendation approaches, e.g., SGL and SimGCL, leverage self-supervised learning to improve user representations against the above issues. Despite their effectiveness, however, they conduct self-supervised learning through creating contrastvie views, depending on the exploration of data augmentations with the problem of tedious trial-and-error selection of augmentation methods. In this paper, we propose a novel Adaptive Graph Contrastive Learning (AdaptiveGCL) framework which conducts graph contrastive learning with two adaptive contrastive view generators to better empower CF paradigm. Specifically, we use two trainable view generators, which are a graph generative model and a graph denoising model respectively, to create contrastive views. Two generators are able to create adaptive contrastive views, addressing the problem of model collapse and achieving adaptive contrastive learning. With two adaptive contrasive views, more additionally high-quality training signals will be introduced into the CF paradigm and help to alleviate the data sparsity and noise issues. Extensive experiments on three benchmark datasets demonstrate the superiority of our model over various state-of-the-art recommendation methods. Further visual analysis intuitively explains why our AdaptiveGCL outperforms existing contrastive learning approaches based on selected data augmentation methods.|最近，图形神经网络(GNN)已成功应用于推荐系统，作为一种有效的协同过滤(CF)方法。基于 GNN 的推荐系统的关键思想是依靠充分和高质量的训练数据，递归地执行沿用户项目交互边缘传递的消息，以完善编码的嵌入。由于实际推荐场景中的用户行为数据通常是有噪音的，并且呈现出偏态分布，因此一些推荐方法，如 SGL 和 SimGCL，利用自监督学习来改善用户对上述问题的表示。然而，尽管他们的有效性，他们进行自我监督学习通过创建对比观点，依赖于探索数据增强与繁琐的试错选择增强方法的问题。本文提出了一种新的自适应图形对比学习(AdaptiveGCL)框架，该框架使用两个自适应对比视图生成器进行图形对比学习，以更好地支持 CF 范式。具体来说，我们使用两个可训练的视图生成器，分别是一个图形生成模型和一个图形去噪模型，来创建对比视图。两个生成器能够创建自适应对比视图，解决模型崩溃问题，实现自适应对比学习。通过两个自适应对立视图，在 CF 范式中引入更多高质量的训练信号，有助于缓解数据稀疏和噪声问题。在三个基准数据集上的大量实验证明了我们的模型优于各种最先进的推荐方法。进一步的可视化分析直观地解释了为什么我们的 AdaptiveGCL 优于基于所选数据增强方法的现有对比学习方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adaptive+Graph+Contrastive+Learning+for+Recommendation)|0|
|[PGLBox: Multi-GPU Graph Learning Framework for Web-Scale Recommendation](https://doi.org/10.1145/3580305.3599885)|Xuewu Jiao, Weibin Li, Xinxuan Wu, Wei Hu, Miao Li, Jiang Bian, Siming Dai, Xinsheng Luo, Mingqing Hu, Zhengjie Huang, Danlei Feng, Junchao Yang, Shikun Feng, Haoyi Xiong, Dianhai Yu, Shuanglong Li, Jingzhou He, Yanjun Ma, Lin Liu||While having been used widely for large-scale recommendation and online advertising, the Graph Neural Network (GNN) has demonstrated its representation learning capacity to extract embeddings of nodes and edges through passing, transforming, and aggregating information over the graph. In this work, we propose PGLBox 1 - a multi-GPU graph learning framework based on PaddlePaddle [24], incorporating with optimized storage, computation, and communication strategies, to train deep GNNs based on web-scale graphs for the recommendation. Specifically, PGLBox adopts a hierarchical storage system with three layers to facilitate I/O, where graphs and embeddings are stored in the HBMs and SSDs, respectively, with MEMs as the cache. To fully utilize multi-GPUs and I/O bandwidth, PGLBox proposes an asynchronous pipeline with three stages - it first samples the subgraphs from the input graph, then pulls & updates embeddings and trains GNNs on the subgraph with parameters updating queued at the end of the pipeline. Thanks to the capacity of PGLBox in handling web-scale graphs, it becomes feasible to unify the view of GNN-based recommendation tasks for multiple advertising verticals and fuse all these graphs into a unified yet huge one. We evaluate PGLBox using a bucket of realistic GNN training tasks for the recommendation, and compare the performance of PGLBox on top of a multi-GPU server (Tesla A100×8) and the legacy training system based on a 40-node MPI cluster at Baidu. The overall comparisons show that PGLBox could save up to 55% monetary cost for training GNN models, and achieve up to 14× training speedup with the same accuracy as the legacy trainer. The open-source implementation of PGLBox is available at https://github.com/PaddlePaddle/PGL/tree/main/apps/PGLBox.|在广泛应用于大规模推荐和在线广告的同时，图形神经网络(GNN)通过在图上传递、转换和聚合信息，展示了其表示学习能力，能够提取节点和边的嵌入。在这项工作中，我们提出了 PGLBox 1-一个基于 PaddlePaddle [24]的多 GPU 图形学习框架，结合优化的存储，计算和通信策略，训练基于网络尺度图的深度 GNN 用于推荐。具体来说，PGLBox 采用了一个具有三层的层次化存储系统来实现 I/O，其中图形和嵌入分别存储在 HBM 和 SSD 中，并以 MEM 作为缓存。为了充分利用多 GPU 和 I/O 带宽，PGLBox 提出了一种异步流水线模型，该模型分为三个阶段: 首先从输入图中采样子图，然后提取和更新嵌入，并在子图上训练 GNN，在流水线末端排队更新参数。由于 PGLBox 在处理网络图形方面的能力，将基于 GNN 的多个垂直广告推荐任务视图统一起来，并将所有这些图形融合成一个统一而庞大的图形变得可行。我们使用一桶现实的 GNN 培训任务来评估 PGLBox 的推荐，并比较基于多 GPU 服务器(Tesla A100 × 8)的 PGLBox 和基于百度40节点 MPI 集群的遗留培训系统的性能。总体比较表明，PGLBox 可以节省高达55% 的训练 GNN 模型的资金成本，并实现高达14倍的训练加速与遗留教练机同样的准确性。PGLBox 的开源实现可在 https://github.com/paddlepaddle/pgl/tree/main/apps/PGLBox 获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PGLBox:+Multi-GPU+Graph+Learning+Framework+for+Web-Scale+Recommendation)|0|
|[IGB: Addressing The Gaps In Labeling, Features, Heterogeneity, and Size of Public Graph Datasets for Deep Learning Research](https://doi.org/10.1145/3580305.3599843)|Arpandeep Khatua, Vikram Sharma Mailthody, Bhagyashree Taleka, Tengfei Ma, Xiang Song, WenMei Hwu|AWS AI; NVIDIA; IBM Research; UIUC|Graph neural networks (GNNs) have shown high potential for a variety of real-world, challenging applications, but one of the major obstacles in GNN research is the lack of large-scale flexible datasets. Most existing public datasets for GNNs are relatively small, which limits the ability of GNNs to generalize to unseen data. The few existing large-scale graph datasets provide very limited labeled data. This makes it difficult to determine if the GNN model's low accuracy for unseen data is inherently due to insufficient training data or if the model failed to generalize. Additionally, datasets used to train GNNs need to offer flexibility to enable a thorough study of the impact of various factors while training GNN models. In this work, we introduce the Illinois Graph Benchmark (IGB), a research dataset tool that the developers can use to train, scrutinize and systematically evaluate GNN models with high fidelity. IGB includes both homogeneous and heterogeneous graphs of enormous sizes, with more than 40% of their nodes labeled. Compared to the largest graph datasets publicly available, the IGB provides over 162X more labeled data for deep learning practitioners and developers to create and evaluate models with higher accuracy. The IGB dataset is designed to be flexible, enabling the study of various GNN architectures, embedding generation techniques, and analyzing system performance issues. IGB is open-sourced, supports DGL and PyG frameworks, and comes with releases of the raw text that we believe foster emerging language models and GNN research projects. An early public version of IGB is available at https://github.com/IllinoisGraphBenchmark/IGB-Datasets.|图形神经网络(GNN)在现实世界中具有很大的应用潜力，但是缺乏大规模的灵活数据集是 GNN 研究的主要障碍之一。大多数现有的 GNN 公共数据集相对较小，这限制了 GNN 推广到未见数据的能力。少数现有的大规模图形数据集提供非常有限的标记数据。这使得很难确定 GNN 模型对于不可见数据的低精度是否本质上是由于训练数据不足或者模型没有推广。此外，用于训练 GNN 的数据集需要提供灵活性，以便在训练 GNN 模型时能够对各种因素的影响进行彻底的研究。在这项工作中，我们介绍了伊利诺伊图基准(IGB) ，一个研究数据集工具，开发人员可以用来训练，审查和系统地评估 GNN 模型的高保真度。IGB 包括大型的同质和异质图，其中超过40% 的节点被标记。与公开发布的最大的图形数据集相比，IGB 为深度学习从业者和开发者提供了超过162倍的标记数据，以创建和评估更高精度的模型。IGB 数据集的设计是灵活的，能够研究各种 GNN 体系结构、嵌入生成技术和分析系统性能问题。IGB 是开源的，支持 DGL 和 PyG 框架，并且附带了原始文本的发布，我们相信这些原始文本可以促进新兴语言模型和 GNN 研究项目的发展。IGB 的早期公开版本可在 https://github.com/illinoisgraphbenchmark/IGB-datasets 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IGB:+Addressing+The+Gaps+In+Labeling,+Features,+Heterogeneity,+and+Size+of+Public+Graph+Datasets+for+Deep+Learning+Research)|0|
|[AdaTT: Adaptive Task-to-Task Fusion Network for Multitask Learning in Recommendations](https://doi.org/10.1145/3580305.3599769)|Danwei Li, Zhengyu Zhang, Siyang Yuan, Mingze Gao, Weilin Zhang, Chaofei Yang, Xi Liu, Jiyan Yang|Meta AI; Meta Platforms, Inc.|Multi-task learning (MTL) aims at enhancing the performance and efficiency of machine learning models by training them on multiple tasks simultaneously. However, MTL research faces two challenges: 1) modeling the relationships between tasks to effectively share knowledge between them, and 2) jointly learning task-specific and shared knowledge. In this paper, we present a novel model Adaptive Task-to-Task Fusion Network (AdaTT) to address both challenges. AdaTT is a deep fusion network built with task specific and optional shared fusion units at multiple levels. By leveraging a residual mechanism and gating mechanism for task-to-task fusion, these units adaptively learn shared knowledge and task specific knowledge. To evaluate the performance of AdaTT, we conduct experiments on a public benchmark and an industrial recommendation dataset using various task groups. Results demonstrate AdaTT can significantly outperform existing state-of-the-art baselines.|多任务学习(MTL)旨在通过同时对多任务进行训练来提高机器学习模型的性能和效率。然而，MTL 研究面临着两个挑战: 1)建立任务之间的关系以有效地分享它们之间的知识，2)联合学习任务特定的和共享的知识。在本文中，我们提出了一个新的模型自适应任务到任务融合网络(AdaTT) ，以解决这两个挑战。AdaTT 是一个深度融合网络，由多个级别的任务特定的和可选的共享融合单元构成。通过利用剩余机制和门控机制进行任务-任务融合，这些单元自适应地学习共享知识和任务特定知识。为了评估 AdaTT 的性能，我们使用不同的任务组在一个公共基准和一个工业推荐数据集上进行了实验。结果表明，AdaTT 可以显著优于现有的最先进的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AdaTT:+Adaptive+Task-to-Task+Fusion+Network+for+Multitask+Learning+in+Recommendations)|0|
|[Stationary Algorithmic Balancing For Dynamic Email Re-Ranking Problem](https://doi.org/10.1145/3580305.3599909)|Jiayi Liu, Jennifer Neville|Purdue University|Email platforms need to generate personalized rankings of emails that satisfy user preferences, which may vary over time. We approach this as a recommendation problem based on three criteria: closeness (how relevant the sender and topic are to the user), timeliness (how recent the email is), and conciseness (how brief the email is). We propose MOSR (Multi-Objective Stationary Recommender), a novel online algorithm that uses an adaptive control model to dynamically balance these criteria and adapt to preference changes. We evaluate MOSR on the Enron Email Dataset, a large collection of real emails, and compare it with other baselines. The results show that MOSR achieves better performance, especially under non-stationary preferences, where users value different criteria more or less over time. We also test MOSR's robustness on a smaller down-sampled dataset that exhibits high variance in email characteristics, and show that it maintains stable rankings across different samples. Our work offers novel insights into how to design email re-ranking systems that account for multiple objectives impacting user satisfaction.|电子邮件平台需要生成个性化的电子邮件排名，以满足用户的喜好，这可能随着时间的推移而变化。我们基于三个标准来处理这个推荐问题: 亲密性(发送者和主题与用户的相关程度)、及时性(邮件发送时间有多近)和简洁性(邮件有多简短)。我们提出了一种新的在线算法——多目标平稳推荐(MOSR) ，它使用自适应控制模型来动态平衡这些标准，并适应偏好的变化。我们评估 MOSR 的安然电子邮件数据集，一大批真实的电子邮件，并比较它与其他基线。结果表明，在非平稳偏好条件下，特别是在用户随着时间的推移或多或少评价不同标准的情况下，MOSR 可以获得更好的性能。我们还测试了 MOSR 的稳健性较小的下采样数据集，表现出高的电子邮件特征方差，并表明它保持稳定的排名在不同的样本。我们的工作提供了新颖的见解，如何设计电子邮件重新排序系统的帐户多个目标影响用户的满意度。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Stationary+Algorithmic+Balancing+For+Dynamic+Email+Re-Ranking+Problem)|0|
|[Tree based Progressive Regression Model for Watch-Time Prediction in Short-video Recommendation](https://doi.org/10.1145/3580305.3599919)|Xiao Lin, Xiaokai Chen, Linfeng Song, Jingwei Liu, Biao Li, Peng Jiang|Kuaishou Technology|An accurate prediction of watch time has been of vital importance to enhance user engagement in video recommender systems. To achieve this, there are four properties that a watch time prediction framework should satisfy: first, despite its continuous value, watch time is also an ordinal variable and the relative ordering between its values reflects the differences in user preferences. Therefore the ordinal relations should be reflected in watch time predictions. Second, the conditional dependence between the video-watching behaviors should be captured in the model. For instance, one has to watch half of the video before he/she finishes watching the whole video. Third, modeling watch time with a point estimation ignores the fact that models might give results with high uncertainty and this could cause bad cases in recommender systems. Therefore the framework should be aware of prediction uncertainty. Forth, the real-life recommender systems suffer from severe bias amplifications thus an estimation without bias amplification is expected. Therefore we propose TPM for watch time prediction. Specifically, the ordinal ranks of watch time are introduced into TPM and the problem is decomposed into a series of conditional dependent classification tasks which are organized into a tree structure. The expectation of watch time can be generated by traversing the tree and the variance of watch time predictions is explicitly introduced into the objective function as a measurement for uncertainty. Moreover, we illustrate that backdoor adjustment can be seamlessly incorporated into TPM, which alleviates bias amplifications. Extensive offline evaluations have been conducted in public datasets and TPM have been deployed in a real-world video app Kuaishou with over 300 million DAUs. The results indicate that TPM outperforms state-of-the-art approaches and indeed improves video consumption significantly.|准确预测观看时间对于提高用户在视频推荐系统中的参与度至关重要。为了实现这一点，手表时间预测框架应该满足四个特性: 第一，尽管手表时间是连续的，但它也是一个有序变量，其值之间的相对排序反映了用户偏好的差异。因此，序数关系应反映在手表时间预测中。其次，在模型中要捕捉视频观看行为之间的条件依赖关系。例如，一个人必须看完一半的视频才能看完整个视频。第三，使用点估计对手表时间进行建模忽略了这样一个事实，即模型可能会给出高度不确定性的结果，这可能会导致推荐系统出现问题。因此，框架应该意识到预测的不确定性。第四，现实生活中的推荐系统遭受严重的偏差放大，因此估计没有偏差放大的预期。因此，我们提出 TPM 来预测手表时间。在 TPM 中引入了观察时间序列，并将问题分解为一系列条件相关的分类任务，这些任务被组织成一个树形结构。通过遍历该树可以产生观察时间的期望值，并且在目标函数中明确地引入观察时间预测的方差作为不确定性的度量。此外，我们说明后门调整可以无缝地纳入 TPM，从而减轻偏差放大。在公共数据集中已经进行了广泛的离线评估，TPM 已经部署在一个现实世界的视频应用快手中，有超过3亿 DAU。结果表明，TPM 优于最先进的方法，确实显著提高了视频消费。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tree+based+Progressive+Regression+Model+for+Watch-Time+Prediction+in+Short-video+Recommendation)|0|
|[HUGE: Huge Unsupervised Graph Embeddings with TPUs](https://doi.org/10.1145/3580305.3599840)|Brandon A. Mayer, Anton Tsitsulin, Hendrik Fichtenberger, Jonathan Halcrow, Bryan Perozzi|Google Research|Graphs are a representation of structured data that captures the relationships between sets of objects. With the ubiquity of available network data, there is increasing industrial and academic need to quickly analyze graphs with billions of nodes and trillions of edges. A common first step for network understanding is Graph Embedding, the process of creating a continuous representation of nodes in a graph. A continuous representation is often more amenable, especially at scale, for solving downstream machine learning tasks such as classification, link prediction, and clustering. A high-performance graph embedding architecture leveraging Tensor Processing Units (TPUs) with configurable amounts of high-bandwidth memory is presented that simplifies the graph embedding problem and can scale to graphs with billions of nodes and trillions of edges. We verify the embedding space quality on real and synthetic large-scale datasets.|图表是结构化数据的表示，它捕捉对象集之间的关系。随着可用网络数据的普及，工业界和学术界越来越需要快速分析具有数十亿个节点和数万亿条边的图形。网络理解的一个常见的第一步是图形嵌入，即在图形中创建连续的节点表示的过程。连续表示通常更适合于解决下游机器学习任务，如分类、链接预测和聚类，尤其是在规模上。提出了一种利用张量处理单元(TPU)和可配置的高带宽存储器构成的高性能图嵌入体系结构，简化了图嵌入问题，并且可以扩展到具有数十亿个节点和数万亿条边的图。在实际和合成的大规模数据集上验证了嵌入空间的质量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HUGE:+Huge+Unsupervised+Graph+Embeddings+with+TPUs)|0|
|[Revisiting Personalized Federated Learning: Robustness Against Backdoor Attacks](https://doi.org/10.1145/3580305.3599898)|Zeyu Qin, Liuyi Yao, Daoyuan Chen, Yaliang Li, Bolin Ding, Minhao Cheng|Alibaba Group; Hong Kong University of Science and Technology|In this work, besides improving prediction accuracy, we study whether personalization could bring robustness benefits to backdoor attacks. We conduct the first study of backdoor attacks in the pFL framework, testing 4 widely used backdoor attacks against 6 pFL methods on benchmark datasets FEMNIST and CIFAR-10, a total of 600 experiments. The study shows that pFL methods with partial model-sharing can significantly boost robustness against backdoor attacks. In contrast, pFL methods with full model-sharing do not show robustness. To analyze the reasons for varying robustness performances, we provide comprehensive ablation studies on different pFL methods. Based on our findings, we further propose a lightweight defense method, Simple-Tuning, which empirically improves defense performance against backdoor attacks. We believe that our work could provide both guidance for pFL application in terms of its robustness and offer valuable insights to design more robust FL methods in the future.|在这项工作中，除了提高预测的准确性，我们研究个性化是否可以带来健壮性的好处后门攻击。我们在 pFL 框架中进行了后门攻击的第一次研究，在基准数据集 FEMNIST 和 CIFAR-10上测试了4个广泛使用的后门攻击与6个 pFL 方法，共计600个实验。研究表明，部分模型共享的 pFL 方法可以显著提高对后门攻击的鲁棒性。相比之下，完全模型共享的 pFL 方法不具有鲁棒性。为了分析鲁棒性能变化的原因，我们对不同的 pFL 方法进行了全面的消融研究。在此基础上，我们进一步提出了一种轻量级的防御方法——简单调整(Simple-Tuning) ，该方法可以实验性地提高对后门攻击的防御性能。我们相信，我们的工作可以为 pFL 的应用提供指导，在其健壮性方面，并提供有价值的见解，以设计更健壮的 FL 方法在未来。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+Personalized+Federated+Learning:+Robustness+Against+Backdoor+Attacks)|0|
|[Joint Optimization of Ranking and Calibration with Contextualized Hybrid Model](https://doi.org/10.1145/3580305.3599851)|XiangRong Sheng, Jingyue Gao, Yueyao Cheng, Siran Yang, Shuguang Han, Hongbo Deng, Yuning Jiang, Jian Xu, Bo Zheng|Alibaba Group|Despite the development of ranking optimization techniques, the pointwise model remains the dominating approach for click-through rate (CTR) prediction. It can be attributed to the calibration ability of the pointwise model since the prediction can be viewed as the click probability. In practice, a CTR prediction model is also commonly assessed with the ranking ability, for which prediction models based on ranking losses (e.g., pairwise or listwise loss) usually achieve better performances than the pointwise loss. Previous studies have experimented with a direct combination of the two losses to obtain the benefit from both losses and observed an improved performance. However, previous studies break the meaning of output logit as the click-through rate, which may lead to sub-optimal solutions. To address this issue, we propose an approach that can Jointly optimize the Ranking and Calibration abilities (JRC for short). JRC improves the ranking ability by contrasting the logit value for the sample with different labels and constrains the predicted probability to be a function of the logit subtraction. We further show that JRC consolidates the interpretation of logits, where the logits model the joint distribution. With such an interpretation, we prove that JRC approximately optimizes the contextualized hybrid discriminative-generative objective. Experiments on public and industrial datasets and online A/B testing show that our approach improves both ranking and calibration abilities. Since May 2022, JRC has been deployed on the display advertising platform of Alibaba and has obtained significant performance improvements.|尽管排序优化技术不断发展，逐点模型仍然是点进率预测的主要方法。这可以归因于点态模型的校准能力，因为预测可以被视为点击概率。在实践中，CTR 预测模型通常也是用排序能力来评估的，其中基于排序损失的预测模型(例如，成对损失或列表损失)通常比逐点损失的预测模型获得更好的性能。以前的研究已经试验了两种损失的直接组合，以获得两种损失的收益，并观察到改善的性能。然而，以前的研究打破了 logit 作为点进率的意义，这可能导致次优解。为了解决这个问题，我们提出了一种方法，可以联合优化排名和校准能力(简称 JRC)。JRC 通过对比不同标签样本的 logit 值来提高排序能力，并将预测概率约束为 logit 减法的函数。我们进一步表明，JRC 巩固了 logit 的解释，其中 logit 模型的联合分布。通过这样的解释，我们证明了 JRC 近似地优化了上下文混合判别生成目标。在公共和工业数据集上的实验和在线 A/B 测试表明，该方法提高了排序和校准能力。自2022年5月起，JRC 已被部署在阿里巴巴的展示广告平台上，并取得显著的性能改善。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Joint+Optimization+of+Ranking+and+Calibration+with+Contextualized+Hybrid+Model)|0|
|[Workplace Recommendation with Temporal Network Objectives](https://doi.org/10.1145/3580305.3599932)|Kiran Tomlinson, Jennifer Neville, Longqi Yang, Mengting Wan, Cao Lu||Workplace communication software such as Microsoft Teams, Slack, and Google Workspace have become integral to workplace collaboration, especially due to the rise of remote work. By making it easier to access relevant or useful information, recommender systems for these platforms have the potential to improve efficient cross-team information flow through a company's communication network. While there has been some recent work on recommendation approaches that optimize network objectives, these have focused on static graphs. In this work, we focus on optimizing information flow, which is highly temporal and presents a number of novel algorithmic challenges. To overcome these, we develop tractable measures of temporal information flow and design efficient online recommendation algorithms that jointly optimize for relevance and cross-team information flow. We demonstrate the potential for impact of these approaches on a rich multi-modal dataset capturing one month of communication between 180k Microsoft employees through email, chats and posts on Microsoft Teams, and file sharing on SharePoint. We design an offline model-based evaluation pipeline to estimate the effects of recommendations on the temporal communication network. We show that our recommendation algorithms can significantly improve cross-team information flow with only a small decrease in traditional relevance metrics.|像微软团队、 Slack 和 Google Workspace 这样的工作场所通信软件已经成为工作场所协作的一部分，特别是由于远程工作的兴起。通过使访问相关或有用信息变得更加容易，这些平台的推荐系统有可能通过公司的通信网络改善有效的跨团队信息流。虽然最近有一些关于优化网络目标的推荐方法的工作，但这些工作主要集中在静态图表上。在这项工作中，我们的重点是优化信息流，这是高度时间性和提出了一些新的算法挑战。为了克服这些问题，我们开发了时间信息流的易处理措施，并设计了有效的在线推荐算法，这些算法可以针对相关性和跨团队信息流进行联合优化。我们展示了这些方法对一个丰富的多模式数据集的潜在影响，这个数据集捕获了180k 微软员工之间通过电子邮件、聊天和微软团队的帖子以及 SharePoint 上的文件共享进行的一个月的通信。我们设计了一个基于离线模型的评估流水线来估计推荐对时间通信网络的影响。我们表明，我们的推荐算法可以显着改善跨团队的信息流，只有一个小的减少，在传统的相关度量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Workplace+Recommendation+with+Temporal+Network+Objectives)|0|
|[Experimentation Platforms Meet Reinforcement Learning: Bayesian Sequential Decision-Making for Continuous Monitoring](https://doi.org/10.1145/3580305.3599818)|Runzhe Wan, Yu Liu, James McQueen, Doug Hains, Rui Song|Amazon|With the growing needs of online A/B testing to support the innovation in industry, the opportunity cost of running an experiment becomes non-negligible. Therefore, there is an increasing demand for an efficient continuous monitoring service that allows early stopping when appropriate. Classic statistical methods focus on hypothesis testing and are mostly developed for traditional high-stake problems such as clinical trials, while experiments at online service companies typically have very different features and focuses. Motivated by the real needs, in this paper, we introduce a novel framework that we developed in Amazon to maximize customer experience and control opportunity cost. We formulate the problem as a Bayesian optimal sequential decision making problem that has a unified utility function. We discuss extensively practical design choices and considerations. We further introduce how to solve the optimal decision rule via Reinforcement Learning and scale the solution. We show the effectiveness of this novel approach compared with existing methods via a large-scale meta-analysis on experiments in Amazon.|随着支持行业创新的在线 A/B 测试需求的不断增长，运行一个实验的机会成本变得不可忽视。因此，人们越来越需要一种有效的连续监测服务，以便能够在适当的时候提早停止。经典的统计方法侧重于假设检验，主要针对传统的高风险问题，如临床试验，而在线服务公司的实验通常具有非常不同的特点和重点。本文从实际需求出发，介绍了我们在亚马逊开发的一个新的框架，以最大限度地提高客户体验和控制机会成本。将该问题表示为一个具有统一效用函数的贝叶斯最优序贯决策问题。我们广泛讨论实用的设计选择和考虑因素。我们进一步介绍了如何通过强化学习和规模求解最优决策规则。我们通过对亚马逊上的实验进行大规模的荟萃分析，证明了这种新方法与现有方法相比的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Experimentation+Platforms+Meet+Reinforcement+Learning:+Bayesian+Sequential+Decision-Making+for+Continuous+Monitoring)|0|
|[BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction](https://doi.org/10.1145/3580305.3599780)|Dong Wang, Kavé Salamatian, Yunqing Xia, Weiwei Deng, Qi Zhang||Although deep pre-trained language models have shown promising benefit in a large set of industrial scenarios, including Click-Through-Rate (CTR) prediction, how to integrate pre-trained language models that handle only textual signals into a prediction pipeline with non-textual features is challenging. Up to now two directions have been explored to integrate multi-modal inputs in fine-tuning of pre-trained language models. One consists of fusing the outcome of language models and non-textual features through an aggregation layer, resulting into ensemble framework, where the cross-information between textual and non-textual inputs are only learned in the aggregation layer. The second one consists of splitting non-textual features into fine-grained fragments and transforming the fragments to new tokens combined with textual ones, so that they can be fed directly to transformer layers in language models. However, this approach increases the complexity of the learning and inference because of the numerous additional tokens. To address these limitations, we propose in this work a novel framework BERT4CTR, with the Uni-Attention mechanism that can benefit from the interactions between non-textual and textual features while maintaining low time-costs in training and inference through a dimensionality reduction. Comprehensive experiments on both public and commercial data demonstrate that BERT4CTR can outperform significantly the state-of-the-art frameworks to handle multi-modal inputs and be applicable to CTR prediction.|尽管深度预训练的语言模型已经在大量的工业场景中显示出有希望的效益，包括点击率(Click-Through-Rate，CTR)预测，但是如何将仅处理文本信号的预训练语言模型集成到具有非文本特征的预测流水线中是具有挑战性的。到目前为止，已经探索了两个方向来整合多模态输入来微调预先训练好的语言模型。一种是通过聚合层融合语言模型和非文本特征的结果，形成集成框架，其中文本和非文本输入之间的交叉信息只能在聚合层中学习。第二种方法是将非文本特征分割成细粒度的片段，并将片段转换成与文本特征相结合的新标记，以便能够直接提供给语言模型中的转换层。然而，这种方法增加了学习和推理的复杂性，因为有许多额外的标记。为了解决这些局限性，我们在这项工作中提出了一个新的框架 BERT4CTR，它具有统一注意机制，可以从非文本和文本特征之间的交互中受益，同时通过降维保持低时间成本的训练和推理。对公共和商业数据的综合实验表明，BERT4CTR 在处理多模态输入方面的性能明显优于最先进的框架，适用于 CTR 预测。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BERT4CTR:+An+Efficient+Framework+to+Combine+Pre-trained+Language+Model+with+Non-textual+Features+for+CTR+Prediction)|0|
|[Macular: A Multi-Task Adversarial Framework for Cross-Lingual Natural Language Understanding](https://doi.org/10.1145/3580305.3599864)|Haoyu Wang, Yaqing Wang, Feijie Wu, Hongfei Xue, Jing Gao||Cross-lingual natural language understanding~(NLU) aims to train NLU models on a source language and apply the models to NLU tasks in target languages, and is a fundamental task for many cross-language applications. Most of the existing cross-lingual NLU models assume the existence of parallel corpora so that words and sentences in source and target languages could be aligned. However, the construction of such parallel corpora is expensive and sometimes infeasible. Motivated by this challenge, recent works propose data augmentation or adversarial training methods to reduce the reliance on external parallel corpora. In this paper, we propose an orthogonal and novel perspective to tackle this challenging cross-lingual NLU task (i.e., when parallel corpora are unavailable). We propose to conduct multi-task learning across different tasks for mutual performance improvement on both source and target languages. The proposed multi-task learning framework is complementary to existing studies and could be integrated with existing methods to further improve their performance on challenging cross-lingual NLU tasks. Towards this end, we propose a multi-task adversarial framework for cross-lingual NLU, namely Macular. The proposed Macular includes a multi-task module and a task-specific module to infer both the common knowledge across tasks and unique task characteristics. More specifically, in the multi-task module, we incorporate a task adversarial loss into training to ensure the derivation of task-shared knowledge only by the representations. In the task-specific fine-tuning module, we extract task-specific knowledge which is not captured by the multi-task module. A task-level consistency loss is added to the training loss so that consistent predictions across a target task and an auxiliary task (i.e., the task that is the most similar to the target task) are achieved. A language adversarial loss is also incorporated so that knowledge can be transferred from source languages to target ones. To validate the effectiveness of the proposed Macular, we conduct extensive experiments on four public datasets including paraphrase identification, natural language understanding, question answering matching, and query advertisement matching. The experimental results show that the proposed Macular can outperform state-of-the-art cross-lingual NLU approaches.|跨语言自然语言理解 ~ (NLU)的目的是在源语言上训练自然语言模型，并将模型应用于目标语言的自然语言任务，是许多跨语言应用的基本任务。现有的大多数跨语言自然语言学模型都假设存在并行语料库，这样源语言和目标语言中的词语和句子就可以对齐。然而，构建这样的平行语料库是昂贵的，有时是不可行的。基于这一挑战，最近的研究提出了数据增强或对抗性训练方法，以减少对外部平行语料库的依赖。在本文中，我们提出了一个正交和新颖的视角来解决这个具有挑战性的跨语言 NLU 任务(即，当平行语料库不可用)。我们建议在不同的任务间进行多任务学习，以提高源语言和目标语言的相互性能。拟议的多任务学习框架是对现有研究的补充，并可与现有方法相结合，以进一步改善其在具有挑战性的跨语言自然语言大学任务方面的表现。为此，我们提出了一个跨语言 NLU 的多任务对抗框架，即黄斑。该模型包括一个多任务模块和一个任务特定模块，用于推断任务间的共同知识和独特的任务特征。更具体地说，在多任务模型中，我们将任务对抗性损失加入到训练中，以确保任务共享知识的获得仅通过表征。在任务特定的微调模块中，我们提取了多任务模块不能获取的任务特定的知识。任务级一致性损失被添加到训练损失中，以便实现对目标任务和辅助任务(即，与目标任务最相似的任务)的一致性预测。还包括一种语言对抗性损失，以便知识能够从源语言转移到目标语言。为了验证所提出的黄斑模型的有效性，我们在四个公共数据集上进行了广泛的实验，包括释义识别、自然语言理解、问答匹配和查询广告匹配。实验结果表明，提出的黄斑可以优于最先进的跨语言 NLU 方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Macular:+A+Multi-Task+Adversarial+Framework+for+Cross-Lingual+Natural+Language+Understanding)|0|
|[ECGGAN: A Framework for Effective and Interpretable Electrocardiogram Anomaly Detection](https://doi.org/10.1145/3580305.3599812)|Huazhang Wang, Zhaojing Luo, James W. L. Yip, Chuyang Ye, Meihui Zhang||Heart is the most important organ of the human body, and Electrocardiogram (ECG) is an essential tool for clinical monitoring of heart health and detecting cardiovascular diseases. Automatic detection of ECG anomalies is of great significance and clinical value in healthcare. However, performing automatic anomaly detection for the ECG data is challenging because we not only need to accurately detect the anomalies but also need to provide clinically meaningful interpretation of the results. Existing works on automatic ECG anomaly detection either rely on hand-crafted designs of feature extraction algorithms which are typically too simple to deliver good performance, or deep learning for automatically extracting features, which is not interpretable. In this paper, we propose ECGGAN, a novel reconstruction-based ECG anomaly detection framework. The key idea of ECGGAN is to make full use of the characteristics of ECG with the periodic metadata, namely beat, to learn the universal pattern in ECG from representative normal data. We establish a reconstruction model, taking leads as constraints to capture the unique characteristics of different leads in ECG data, and achieve accurate anomaly detection at ECG-level by combining multiple leads. Experimental results on two real-world datasets and their mixed-set confirm that our method achieves superior performance than baselines in terms of precision, recall, F1-score, and AUC. In addition, ECGGAN can provide clinically meaningful interpretation of results by revealing the extent to which abnormal sites deviate from the normal pattern.|心脏是人体最重要的器官，心电图是临床监测心脏健康和检测心血管疾病的重要工具。心电图异常的自动检测在医疗保健中具有重要的意义和临床价值。然而，对心电图数据进行自动异常检测是一个挑战，因为我们不仅需要准确地检测异常，而且还需要对结果提供具有临床意义的解释。现有的自动心电图异常检测要么依赖于手工设计的特征提取算法，这些算法通常过于简单，无法提供良好的性能，要么依赖于深度学习来自动提取特征，而这些特征是无法解释的。在这篇文章中，我们提出了一个新的基于重建的心电图异常检测框架 ECgGAN。ECGGAN 的核心思想是充分利用心电信号的周期性元数据特点，即拍频，从典型的正常数据中学习心电信号的通用模式。我们建立了一个重建模型，以导联为约束，捕捉心电数据中不同导联的独特特征，并通过组合多导联在心电水平上获得准确的异常检测。在两个实际数据集及其混合集上的实验结果表明，该方法在精确度、召回率、 F1得分和 AUC 方面均优于基线方法。此外，ECGGAN 可以通过揭示异常部位偏离正常模式的程度，对结果提供临床意义上的解释。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ECGGAN:+A+Framework+for+Effective+and+Interpretable+Electrocardiogram+Anomaly+Detection)|0|
|[Fresh Content Needs More Attention: Multi-funnel Fresh Content Recommendation](https://doi.org/10.1145/3580305.3599826)|Jianling Wang, Haokai Lu, Sai Zhang, Bart N. Locanthi, Haoting Wang, Dylan Greaves, Benjamin Lipshitz, Sriraj Badam, Ed H. Chi, Cristos J. Goodrow, SuLin Wu, Lexi Baugher, Minmin Chen|Google|Recommendation system serves as a conduit connecting users to an incredibly large, diverse and ever growing collection of contents. In practice, missing information on fresh (and tail) contents needs to be filled in order for them to be exposed and discovered by their audience. We here share our success stories in building a dedicated fresh content recommendation stack on a large commercial platform. To nominate fresh contents, we built a multi-funnel nomination system that combines (i) a two-tower model with strong generalization power for coverage, and (ii) a sequence model with near real-time update on user feedback for relevance. The multi-funnel setup effectively balances between coverage and relevance. An in-depth study uncovers the relationship between user activity level and their proximity toward fresh contents, which further motivates a contextual multi-funnel setup. Nominated fresh candidates are then scored and ranked by systems considering prediction uncertainty to further bootstrap content with less exposure. We evaluate the benefits of the dedicated fresh content recommendation stack, and the multi-funnel nomination system in particular, through user corpus co-diverted live experiments. We conduct multiple rounds of live experiments on a commercial platform serving billion of users demonstrating efficacy of our proposed methods.|推荐系统作为一个管道，将用户连接到一个极其庞大、多样化和不断增长的内容集合。在实践中，需要填补关于新鲜(和尾部)内容的缺失信息，以便它们被观众暴露和发现。我们在这里分享我们在一个大型商业平台上建立一个专门的新内容推荐堆栈的成功故事。为了提名新的内容，我们建立了一个多漏斗提名系统，该系统结合了(i)一个具有很强覆盖泛化能力的双塔模型和(ii)一个具有近实时更新用户反馈相关性的序列模型。多漏斗设置有效地平衡了覆盖率和相关性。深入的研究揭示了用户活动水平与其接近新鲜内容之间的关系，进一步激发了上下文多漏斗设置。提名的新鲜候选人，然后得分和排名的系统考虑预测不确定性，以进一步引导内容，较少的曝光。我们通过用户语料库共转向的现场实验，评估了专用新鲜内容推荐堆栈，特别是多漏斗提名系统的优点。我们在一个为数十亿用户服务的商业平台上进行多轮实验，证明我们提出的方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fresh+Content+Needs+More+Attention:+Multi-funnel+Fresh+Content+Recommendation)|0|
|[Contrastive Learning of Stress-specific Word Embedding for Social Media based Stress Detection](https://doi.org/10.1145/3580305.3599795)|Xin Wang, Huijun Zhang, Lei Cao, Kaisheng Zeng, Qi Li, Ningyun Li, Ling Feng||Detecting stress via user's social media posts has attracted increasing research interests in recent years. The majority of the methods leverage word embeddings to represent each of the posted words as a vector, and then perform classification on a sequence of word vectors. To enhance the performance of distinguishing words/phrases related to stressors and stressful emotions from others, in this study, we present a stress-specific word embedding learning framework upon the pre-trained language model BERT. Specifically, we formulate three self-supervised contrastive learning tasks with a joint learning objective. (1) The stressor discrimination task, which is designed to allow the framework to be sensitive to words/phrases about stressors. (2) The stressor cluster discrimination task, which is designed to allow the framework to distinguish stressors into different categories. (3) The stressful emotion discrimination task, which is designed to allow the framework to grasp words/phrases about stressful emotions. Our performance study shows that the learned stress-specific word embedding can significantly benefit social media based stress detection tasks, especially in the more practical scenarios with insufficient labeled data. Besides, we build two user-level social media based stress detection datasets that can help train machine learning models to facilitate human well-being.|近年来，通过用户的社交媒体帖子来探测压力已经引起了越来越多的研究兴趣。大多数方法利用单词嵌入将每个贴出的单词表示为一个向量，然后对一系列单词向量执行分类。为了提高学生区分与压力源和压力情绪有关的词汇和短语的能力，本研究在预训语言模型 BERT 的基础上，提出了一个压力特异性词汇嵌入学习框架。具体来说，我们设计了三个具有联合学习目标的自监督对比学习任务。(1)压力源识别任务，该任务的设计使框架对压力源相关词语敏感。(2)应激源集群辨别任务，其目的是使框架区分不同类别的应激源。(3)压力情绪识别任务，该任务的设计是为了使学生能够掌握压力情绪的词汇和短语。我们的绩效研究表明，学习压力特异性词语嵌入可以显著有利于基于社交媒体的压力检测任务，特别是在标记数据不足的更实际的情况下。此外，我们建立了两个基于用户级社交媒体的压力检测数据集，可以帮助训练机器学习模型，以促进人类福祉。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contrastive+Learning+of+Stress-specific+Word+Embedding+for+Social+Media+based+Stress+Detection)|0|
|[RLTP: Reinforcement Learning to Pace for Delayed Impression Modeling in Preloaded Ads](https://doi.org/10.1145/3580305.3599900)|Penghui Wei, Yongqiang Chen, Shaoguo Liu, Liang Wang, Bo Zheng|Alibaba Group|To increase brand awareness, many advertisers conclude contracts with advertising platforms to purchase traffic and then deliver advertisements to target audiences. In a whole delivery period, advertisers usually desire a certain impression count for the ads, and they also expect that the delivery performance is as good as possible (e.g., obtaining high click-through rate). Advertising platforms employ pacing algorithms to satisfy the demands via adjusting the selection probabilities to traffic requests in real-time. However, the delivery procedure is also affected by the strategies from publishers, which cannot be controlled by advertising platforms. Preloading is a widely used strategy for many types of ads (e.g., video ads) to make sure that the response time for displaying after a traffic request is legitimate, which results in delayed impression phenomenon. Traditional pacing algorithms cannot handle the preloading nature well because they rely on immediate feedback signals, and may fail to guarantee the demands from advertisers. In this paper, we focus on a new research problem of impression pacing for preloaded ads, and propose a Reinforcement Learning To Pace framework RLTP. It learns a pacing agent that sequentially produces selection probabilities in the whole delivery period. To jointly optimize the two objectives of impression count and delivery performance, RLTP employs tailored reward estimator to satisfy the guaranteed impression count, penalize the over-delivery and maximize the traffic value. Experiments on large-scale industrial datasets verify that RLTP outperforms baseline pacing algorithms by a large margin. We have deployed the RLTP framework online to our advertising platform, and results show that it achieves significant uplift to core metrics including delivery completion rate and click-through rate.|为了提高品牌知名度，许多广告商与广告平台签订合同，购买流量，然后向目标受众投放广告。在整个投放期间，广告商通常希望广告能给人留下一定的印象，而且他们也希望投放的效果尽可能好(例如，获得较高的点进率)。广告平台采用节奏算法，通过实时调整流量请求的选择概率来满足需求。然而，传递过程也受到出版商策略的影响，而出版商策略又不受广告平台的控制。预加载是一种广泛使用的策略，许多类型的广告(如视频广告) ，以确保响应时间显示后的流量请求是合法的，这导致了延迟印象现象。传统的节奏算法不能很好地处理预载性质，因为它们依赖于即时反馈信号，可能无法保证来自广告商的需求。在这篇文章中，我们关注一个新的研究问题——预装广告的印象节奏，并提出了一个强化学习到节奏的框架 RLTP。它学习一种起搏剂，该起搏剂在整个交付期间依次产生选择概率。为了共同优化印象计数和传递性能这两个目标，RLTP 使用定制的报酬估计器来满足保证的印象计数，惩罚超额传递和最大化流量价值。在大规模工业数据集上的实验证明，RLTP 算法的性能优于基线起搏算法。我们已经在我们的广告平台上部署了 RLTP 框架，结果显示它实现了包括交付完成率和点进率在内的核心指标的显著提升。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RLTP:+Reinforcement+Learning+to+Pace+for+Delayed+Impression+Modeling+in+Preloaded+Ads)|0|
|[Multi-channel Integrated Recommendation with Exposure Constraints](https://doi.org/10.1145/3580305.3599868)|Yue Xu, Qijie Shen, Jianwen Yin, Zengde Deng, Dimin Wang, Hao Chen, Lixiang Lai, Tao Zhuang, Junfeng Ge|Alibaba Group.; Cainiao Network.; The Hong Kong Polytechnic University.|Integrated recommendation, which aims at jointly recommending heterogeneous items from different channels in a main feed, has been widely applied to various online platforms. Though attractive, integrated recommendation requires the ranking methods to migrate from conventional user-item models to the new user-channel-item paradigm in order to better capture users' preferences on both item and channel levels. Moreover, practical feed recommendation systems usually impose exposure constraints on different channels to ensure user experience. This leads to greater difficulty in the joint ranking of heterogeneous items. In this paper, we investigate the integrated recommendation task with exposure constraints in practical recommender systems. Our contribution is forth-fold. First, we formulate this task as a binary online linear programming problem and propose a two-layer framework named Multi-channel Integrated Recommendation with Exposure Constraints (MIREC) to obtain the optimal solution. Second, we propose an efficient online allocation algorithm to determine the optimal exposure assignment of different channels from a global view of all user requests over the entire time horizon. We prove that this algorithm reaches the optimal point under a regret bound of $ \mathcal{O}(\sqrt{T}) $ with linear complexity. Third, we propose a series of collaborative models to determine the optimal layout of heterogeneous items at each user request. The joint modeling of user interests, cross-channel correlation, and page context in our models aligns more with the browsing nature of feed products than existing models. Finally, we conduct extensive experiments on both offline datasets and online A/B tests to verify the effectiveness of MIREC. The proposed framework has now been implemented on the homepage of Taobao to serve the main traffic.|综合推荐是指在一个主要的推送平台上联合推荐来自不同渠道的异构项目，已广泛应用于各种在线平台。虽然综合推荐具有吸引力，但是它需要排名方法从传统的用户项目模型迁移到新的用户渠道项目范式，以便更好地捕捉用户在项目和渠道级别上的偏好。此外，实际的饲料推荐系统通常对不同的渠道施加暴露约束，以确保用户体验。这导致了异构项目联合排序的更大困难。本文研究了实际推荐系统中具有曝光约束的集成推荐任务。我们的贡献是四倍。首先，我们将这个任务表述为一个二进制在线线性规划问题，并提出一个名为多通道暴露约束综合推荐(MIREC)的两层架构来获得最优解。其次，我们提出了一个有效的在线分配算法，从全局的角度来确定不同信道在整个时间范围内的最佳曝光分配。证明了该算法在线性复杂度为 $数学{ O }(sqrt { T }) $的遗憾界下达到最优点。第三，我们提出了一系列的协作模型，以确定在每个用户请求的异构项目的最佳布局。在我们的模型中，用户兴趣、跨通道相关性和页面上下文的联合建模比现有模型更符合饲料产品的浏览特性。最后，我们对离线数据集和在线 A/B 测试进行了广泛的实验，以验证 MIREC 的有效性。这个建议框架现已在淘宝网的主页上实施，以服务于主要流量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-channel+Integrated+Recommendation+with+Exposure+Constraints)|0|
|[Interactive Generalized Additive Model and Its Applications in Electric Load Forecasting](https://doi.org/10.1145/3580305.3599848)|Linxiao Yang, Rui Ren, Xinyue Gu, Liang Sun|DAMO Academy, Alibaba Group|Electric load forecasting is an indispensable component of electric power system planning and management. Inaccurate load forecasting may lead to the threat of outages or a waste of energy. Accurate electric load forecasting is challenging when there is limited data or even no data, such as load forecasting in holiday, or under extreme weather conditions. As high-stakes decision-making usually follows after load forecasting, model interpretability is crucial for the adoption of forecasting models. In this paper, we propose an interactive GAM which is not only interpretable but also can incorporate specific domain knowledge in electric power industry for improved performance. This boosting-based GAM leverages piecewise linear functions and can be learned through our efficient algorithm. In both public benchmark and electricity datasets, our interactive GAM outperforms current state-of-the-art methods and demonstrates good generalization ability in the cases of extreme weather events. We launched a user-friendly web-based tool based on interactive GAM and already incorporated it into our eForecaster product, a unified AI platform for electricity forecasting.|电力负荷预测是电力系统规划和管理中不可缺少的组成部分。不准确的负荷预测可能导致停电或能源浪费的威胁。准确的电力负荷预测是具有挑战性的时候，有限的数据，甚至没有数据，如负荷预测在假日，或在极端天气条件下。由于高风险决策通常在负荷预测之后进行，因此模型的可解释性对于采用预测模型至关重要。本文提出了一种交互式 GAM 模型，它不仅具有可解释性，而且能够融合电力行业的特定领域知识，以提高性能。这种基于升压的 GAM 利用分段线性函数，可以通过我们的高效算法学习。在公共基准和电力数据集中，我们的交互式 GAM 优于目前最先进的方法，并且在极端天气事件中展示了良好的推广能力。我们推出了一个基于交互式 GAM 的用户友好的网络工具，并已经将其纳入我们的电子预报产品，一个用于电力预报的统一人工智能平台。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interactive+Generalized+Additive+Model+and+Its+Applications+in+Electric+Load+Forecasting)|0|
|[UA-FedRec: Untargeted Attack on Federated News Recommendation](https://doi.org/10.1145/3580305.3599923)|Jingwei Yi, Fangzhao Wu, Bin Zhu, Jing Yao, Zhulin Tao, Guangzhong Sun, Xing Xie|; University of Science and Technology of China; Microsoft Research Asia|News recommendation is critical for personalized news distribution. Federated news recommendation enables collaborative model learning from many clients without sharing their raw data. It is promising for privacy-preserving news recommendation. However, the security of federated news recommendation is still unclear. In this paper, we study this problem by proposing an untargeted attack called UA-FedRec. By exploiting the prior knowledge of news recommendation and federated learning, UA-FedRec can effectively degrade the model performance with a small percentage of malicious clients. First, the effectiveness of news recommendation highly depends on user modeling and news modeling. We design a news similarity perturbation method to make representations of similar news farther and those of dissimilar news closer to interrupt news modeling, and propose a user model perturbation method to make malicious user updates in opposite directions of benign updates to interrupt user modeling. Second, updates from different clients are typically aggregated by weighted-averaging based on their sample sizes. We propose a quantity perturbation method to enlarge sample sizes of malicious clients in a reasonable range to amplify the impact of malicious updates. Extensive experiments on two real-world datasets show that UA-FedRec can effectively degrade the accuracy of existing federated news recommendation methods, even when defense is applied. Our study reveals a critical security issue in existing federated news recommendation systems and calls for research efforts to address the issue.|新闻推荐对个性化新闻发布至关重要。联合新闻推荐使得许多客户能够在不共享原始数据的情况下进行协作模型学习。它对于保护隐私的新闻推荐来说是很有前途的。然而，联邦新闻推荐的安全性仍不清楚。在本文中，我们通过提出一种称为 UA-FedRec 的非目标攻击来研究这个问题。通过利用新闻推荐和联邦学习的先验知识，UA-FedRec 能够有效地降低小比例恶意客户端的模型性能。首先，新闻推荐的有效性很大程度上取决于用户建模和新闻建模。设计了一种新闻相似性摄动方法，使相似新闻和不同新闻的表示更接近于中断新闻建模，提出了一种用户模型摄动方法，使恶意用户在良性更新的相反方向更新，以中断用户建模。其次，来自不同客户端的更新通常根据样本大小进行加权平均。我们提出了一种数量扰动方法，在合理的范围内扩大恶意客户端的样本量，以放大恶意更新的影响。在两个实际数据集上的大量实验表明，UA-FedRec 能够有效地降低现有联邦新闻推荐方法的准确性，即使在采用防御策略的情况下也是如此。我们的研究揭示了现有联邦新闻推荐系统中的一个关键安全问题，并呼吁研究人员努力解决这个问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=UA-FedRec:+Untargeted+Attack+on+Federated+News+Recommendation)|0|
|[Group-based Fraud Detection Network on e-Commerce Platforms](https://doi.org/10.1145/3580305.3599836)|Jianke Yu, Hanchen Wang, Xiaoyang Wang, Zhao Li, Lu Qin, Wenjie Zhang, Jian Liao, Ying Zhang||Along with the rapid technological and commercial innovation on the e-commerce platforms, there are an increasing number of frauds that bring great harm to these platforms. Many frauds are conducted by organized groups of fraudsters for higher efficiency and lower costs, which are also known as group-based frauds. Despite the high concealment and strong destructiveness of group-based fraud, there is no existing research work that can thoroughly exploit the information within the transaction networks of e-commerce platforms for group-based fraud detection. In this work, we analyze and summarize the characteristics of group-based frauds, based on which we propose a novel end-to-end semi-supervised Group-based Fraud Detection Network (GFDN) to support such fraud detection in real-world applications. Experimental results on large-scale e-commerce datasets from Taobao and Bitcoin trading datasets show the superior effectiveness and efficiency of our proposed model for group-based fraud detection on bipartite graphs.|随着电子商务平台上技术和商业的快速创新，给这些平台带来巨大危害的欺诈案件日益增多。许多欺诈是由有组织的欺诈者集团为了提高效率和降低成本而进行的，这也被称为集团欺诈。尽管基于群体的欺诈具有高度的隐蔽性和强大的破坏性，但目前还没有一项研究工作能够彻底利用电子商务平台交易网络内的信息进行基于群体的欺诈侦查。本文分析和总结了基于群体的欺诈行为的特点，在此基础上提出了一种新的端到端半监督的基于群体的欺诈检测网络(GFDN) ，以支持现实应用中的欺诈检测。对淘宝和比特币交易数据集的大规模电子商务数据集的实验结果表明，本文提出的基于二分图的群欺诈检测模型具有较好的有效性和高效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Group-based+Fraud+Detection+Network+on+e-Commerce+Platforms)|0|
|[Commonsense Knowledge Graph towards Super APP and Its Applications in Alipay](https://doi.org/10.1145/3580305.3599791)|Xiaoling Zang, Binbin Hu, Jun Chu, Zhiqiang Zhang, Guannan Zhang, Jun Zhou, Wenliang Zhong|Ant Group, Hang Zhou, China|The recently explosive growth of Super Apps brings great convenience to people's daily life by providing a wide variety of services through mini-programs, including online shopping, travel, finance, and so on. Due to the considerable gap between various scenarios, the restriction of effective information transfer and sharing severely blocks the efficient delivery of online services, potentially affecting the user's app experience. To deeply understand users' needs, we propose SupKG, a commonsense knowledge graph towards Super APP to help comprehensively characterize user behaviors across different business scenarios. In particular, our SupKG is carefully established from multiplex and heterogeneous data source in Alipay (a well-known Super App in China), which also emphasize abundant spatiotemporal relations and intent-related entities to answer the fundamental question in life service ''which service do users need at what time and where''. On the hand, the successful application of SupKG hinges on the effective form of network representation ie Knowledge Graph Embedding (KGE). However, a series of unsatisfying issues still need to be carefully considered in the industrial environment: i) bridging language representations with knowledge structure in a unified manner, ii) alleviating the skewed data distribution in SupKG, and iii) effectively characterizing hierarchical structures in SupKG. With these motivations, we develop a novel knowledge graph representation learning framework for SupKG, enabling various downstream applications to benefit from learned representations of entities and relations. Extensive experiments on the standard knowledge graph completion task demonstrate the consistent and significant performance improvement of our representation learning framework, which also greatly benefits the supplementation of potential knowledge of SupKG. Towards real-world applications in Alipay, our SupKG and learned representations show the potential superiority of integrating global behaviors in cold-start scenarios and providing high-quality knowledge for warming up the graph-based ranking.|最近超级应用程序的爆炸式增长为人们的日常生活带来了极大的便利，通过小程序提供各种各样的服务，包括在线购物，旅游，金融等。由于不同场景之间的巨大差距，有效信息传递和共享的限制严重阻碍了在线服务的有效传递，潜在地影响了用户的应用体验。为了深入了解用户的需求，我们提出了 SupKG，一个面向 Super APP 的常识性知识图，帮助全面描述不同业务场景中的用户行为。特别是，我们的 SupKG 是从支付宝(中国著名的超级应用程序)中的多路异构数据源精心建立起来的，它还强调丰富的时空关系和意图相关实体，以回答生活服务中的基本问题“用户在什么时间和地点需要什么服务”。另一方面，SupKG 的成功应用取决于网络表示的有效形式，即知识图嵌入(KGE)。然而，在工业环境中仍然需要仔细考虑一系列不令人满意的问题: i)以统一的方式将语言表示与知识结构连接起来，ii)缓解 SupKG 中的偏斜数据分布，以及 iii)有效表征 SupKG 中的层次结构。基于这些动机，我们为 SupKG 开发了一个新的知识图表示学习框架，使各种下游应用程序能够从实体和关系的学习表示中受益。通过对标准知识图完成任务的大量实验，证明了我们的表示学习框架在性能上的一致性和显著性改进，这也极大地有利于对 SupKG 潜在知识的补充。对于支付宝中的实际应用，我们的 SupKG 和学习表示显示了在冷启动场景中整合全球行为的潜在优势，并为预热基于图表的排名提供高质量的知识。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Commonsense+Knowledge+Graph+towards+Super+APP+and+Its+Applications+in+Alipay)|0|
|[Revisiting Neural Retrieval on Accelerators](https://doi.org/10.1145/3580305.3599897)|Jiaqi Zhai, Zhaojie Gong, Yueming Wang, Xiao Sun, Zheng Yan, Fu Li, Xing Liu|Meta Platforms, Inc.|Retrieval finds a small number of relevant candidates from a large corpus for information retrieval and recommendation applications. A key component of retrieval is to model (user, item) similarity, which is commonly represented as the dot product of two learned embeddings. This formulation permits efficient inference, commonly known as Maximum Inner Product Search (MIPS). Despite its popularity, dot products cannot capture complex user-item interactions, which are multifaceted and likely high rank. We hence examine non-dot-product retrieval settings on accelerators, and propose \textit{mixture of logits} (MoL), which models (user, item) similarity as an adaptive composition of elementary similarity functions. This new formulation is expressive, capable of modeling high rank (user, item) interactions, and further generalizes to the long tail. When combined with a hierarchical retrieval strategy, \textit{h-indexer}, we are able to scale up MoL to 100M corpus on a single GPU with latency comparable to MIPS baselines. On public datasets, our approach leads to uplifts of up to 77.3\% in hit rate (HR). Experiments on a large recommendation surface at Meta showed strong metric gains and reduced popularity bias, validating the proposed approach's performance and improved generalization.|Retrieval 从一个大型语料库中为信息检索和推荐应用程序找到少量相关的候选人。检索的一个关键组成部分是模型(用户，项目)的相似性，这是通常表示为点积的两个学习嵌入。这个公式允许有效的推理，通常称为最大内积搜索(MIPS)。尽管广受欢迎，点产品不能捕捉复杂的用户项目交互，这是多方面的，可能排名很高。因此，我们研究了加速器上的非点积检索设置，并提出了 text { mix of logits }(MoL) ，它将(用户，项目)相似度建模为基本相似度函数的自适应组合。这个新的公式是有表现力的，能够建模高级别(用户，项目)的交互，并进一步推广到长尾。当结合分层检索策略 texttit { h-indexer }时，我们能够在单个 GPU 上扩展 MoL 到100M 语料库，延迟与 MIPS 基线相当。在公共数据集上，我们的方法导致命中率(HR)提高高达77.3% 。在 Meta 的一个大型推荐面上进行的实验表明，该方法具有很强的度量增益和较小的普及偏差，验证了该方法的性能和改进的泛化能力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+Neural+Retrieval+on+Accelerators)|0|
|[Constrained Social Community Recommendation](https://doi.org/10.1145/3580305.3599793)|Xingyi Zhang, Shuliang Xu, Wenqing Lin, Sibo Wang||In online social networks, users with similar interests tend to come together, forming social communities. Nowadays, user-defined communities become a prominent part of online social platforms as people who have joined such communities tend to be more active in social networks. Therefore, recommending explicit communities to users provides great potential to advance online services. In this paper, we focus on the constrained social community recommendation problem in real applications, where each user can only join at most one community. Previous attempts at community recommendation mostly adopt collaborative filtering approaches or random walk-based approaches, while ignoring social relationships between users as well as the local structure of each community. Therefore, they only derive an extremely sparse affinity matrix, which degrades the model performances. To tackle this issue, we propose ComRec which simultaneously captures both global and local information on the extended graph during pre-computation, speeding up the training process on real-world large graphs. In addition, we present a labeling component to improve the expressiveness of our framework. We conduct experiments on three Tencent mobile games to evaluate our proposed method. Extensive experimental results show that our ComRec consistently outperforms other competitors by up to 12.80% and 6.61% in the corresponding evaluation metrics of offline and online experiments, respectively.|在线社交网络中，兴趣相似的用户往往聚集在一起，形成社交社区。如今，用户定义的社区成为在线社交平台的重要组成部分，因为加入这些社区的人往往在社交网络中更加活跃。因此，向用户推荐明确的社区为推进在线服务提供了巨大的潜力。在本文中，我们主要研究实际应用中的受限社区推荐问题，其中每个用户最多只能加入一个社区。以前的社区推荐尝试大多采用协同过滤推荐或者基于随机漫步的方法，而忽略了用户之间的社会关系以及每个社区的本地结构。因此，他们只得到一个极其稀疏的亲和矩阵，这降低了模型的性能。为了解决这一问题，我们提出了 ComRec 算法，该算法在预计算过程中同时捕获扩展图上的全局和局部信息，加快了对真实世界大图的训练过程。此外，我们提出了一个标签组件，以提高我们的框架的表达能力。我们在三款腾讯手机游戏上进行实验，以评估我们提出的方法。广泛的实验结果表明，我们的 ComRec 在相应的离线和在线实验评价指标方面始终优于其他竞争对手，分别高达12.80% 和6.61% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Constrained+Social+Community+Recommendation)|0|
|[Modeling Dual Period-Varying Preferences for Takeaway Recommendation](https://doi.org/10.1145/3580305.3599866)|Yuting Zhang, Yiqing Wu, Ran Le, Yongchun Zhu, Fuzhen Zhuang, Ruidong Han, Xiang Li, Wei Lin, Zhulin An, Yongjun Xu|Institute of Artificial Intelligence, Beihang University; Institute of Computing Technology, Chinese Academy of Sciences; Meituan; Unaffiliated|Takeaway recommender systems, which aim to accurately provide stores that offer foods meeting users' interests, have served billions of users in our daily life. Different from traditional recommendation, takeaway recommendation faces two main challenges: (1) Dual Interaction-Aware Preference Modeling. Traditional recommendation commonly focuses on users' single preferences for items while takeaway recommendation needs to comprehensively consider users' dual preferences for stores and foods. (2) Period-Varying Preference Modeling. Conventional recommendation generally models continuous changes in users' preferences from a session-level or day-level perspective. However, in practical takeaway systems, users' preferences vary significantly during the morning, noon, night, and late night periods of the day. To address these challenges, we propose a Dual Period-Varying Preference modeling (DPVP) for takeaway recommendation. Specifically, we design a dual interaction-aware module, aiming to capture users' dual preferences based on their interactions with stores and foods. Moreover, to model various preferences in different time periods of the day, we propose a time-based decomposition module as well as a time-aware gating mechanism. Extensive offline and online experiments demonstrate that our model outperforms state-of-the-art methods on real-world datasets and it is capable of modeling the dual period-varying preferences. Moreover, our model has been deployed online on Meituan Takeaway platform, leading to an average improvement in GMV (Gross Merchandise Value) of 0.70%.|外卖推荐系统，旨在准确地提供商店，提供符合用户兴趣的食品，已服务于数十亿用户在我们的日常生活。与传统的推荐不同，外卖推荐面临着两个主要挑战: (1)双交互感知偏好建模。传统的推荐方式通常侧重于用户对商品的单一偏好，而外卖推荐方式则需要全面考虑用户对商店和食品的双重偏好。(变周期偏好模型。传统的推荐通常从会话级或日级的角度模拟用户偏好的持续变化。然而，在实际的外卖系统中，用户的偏好在白天的早上、中午、晚上和深夜各不相同。为了应对这些挑战，我们提出了一个外卖推荐的双周期变化偏好模型(DPVP)。具体来说，我们设计了一个双交互感知模块，旨在根据用户与商店和食物的交互来捕捉他们的双重偏好。此外，为了模拟一天中不同时段的不同偏好，我们提出了一个基于时间的分解模块以及一个时间感知的门控机制。大量的离线和在线实验表明，我们的模型优于现实世界数据集的最先进的方法，它能够建模的双周期变化的偏好。此外，我们的模型已经在美团外卖平台上进行了在线部署，导致平均商品总值(GMV)提高了0.70% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modeling+Dual+Period-Varying+Preferences+for+Takeaway+Recommendation)|0|
|[JiuZhang 2.0: A Unified Chinese Pre-trained Language Model for Multi-task Mathematical Problem Solving](https://doi.org/10.1145/3580305.3599850)|Xin Zhao, Kun Zhou, Beichen Zhang, Zheng Gong, Zhipeng Chen, Yuanhang Zhou, JiRong Wen, Jing Sha, Shijin Wang, Cong Liu, Guoping Hu|iFLYTEK Research; School of Information, Renmin University of China; iFLYTEK Research, State Key Laboratory of Cognitive Intelligence; iFLYTEK AI Research (Central China; Gaoling School of Artificial Intelligence, Renmin University of China|Although pre-trained language models~(PLMs) have recently advanced the research progress in mathematical reasoning, they are not specially designed as a capable multi-task solver, suffering from high cost for multi-task deployment (\eg a model copy for a task) and inferior performance on complex mathematical problems in practical applications. To address these issues, in this paper, we propose \textbf{JiuZhang~2.0}, a unified Chinese PLM specially for multi-task mathematical problem solving. Our idea is to maintain a moderate-sized model and employ the \emph{cross-task knowledge sharing} to improve the model capacity in a multi-task setting. Specially, we construct a Mixture-of-Experts~(MoE) architecture for modeling mathematical text, so as to capture the common mathematical knowledge across tasks. For optimizing the MoE architecture, we design \emph{multi-task continual pre-training} and \emph{multi-task fine-tuning} strategies for multi-task adaptation. These training strategies can effectively decompose the knowledge from the task data and establish the cross-task sharing via expert networks. In order to further improve the general capacity of solving different complex tasks, we leverage large language models~(LLMs) as complementary models to iteratively refine the generated solution by our PLM, via in-context learning. Extensive experiments have demonstrated the effectiveness of our model.|尽管预先训练的语言模型 ~ (PLM)最近已经推动了数学推理的研究进展，但是它们并没有被特别设计成一个有能力的多任务解决者，因为多任务部署的高成本(例如一个任务的模型拷贝)和在实际应用中复杂数学问题的低表现。为了解决这些问题，本文提出了一个专门用于多任务数学问题求解的统一中文 PLM textbf {旧掌 ~ 2.0}。我们的想法是维持一个中等规模的模型，并采用跨任务知识共享的方法来提高模型在多任务环境下的能力。特别地，我们构建了一个专家混合模型，用于数学文本建模，以便跨任务获取常见的数学知识。为了优化教学体系结构，我们设计了多任务连续预训练和多任务微调策略来实现多任务自适应。这些训练策略可以有效地分解任务数据中的知识，并通过专家网络建立跨任务共享。为了进一步提高解决不同复杂任务的能力，我们利用大语言模型 ~ (LLM)作为补充模型，通过上下文学习的方法，迭代地完善 PLM 生成的解决方案。大量的实验证明了我们模型的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=JiuZhang+2.0:+A+Unified+Chinese+Pre-trained+Language+Model+for+Multi-task+Mathematical+Problem+Solving)|0|
|[ReLoop2: Building Self-Adaptive Recommendation Models via Responsive Error Compensation Loop](https://doi.org/10.1145/3580305.3599785)|Jieming Zhu, Guohao Cai, Junjie Huang, Zhenhua Dong, Ruiming Tang, Weinan Zhang|Huawei Noah’s Ark Lab; Shanghai Jiao Tong University|Industrial recommender systems face the challenge of operating in non-stationary environments, where data distribution shifts arise from evolving user behaviors over time. To tackle this challenge, a common approach is to periodically re-train or incrementally update deployed deep models with newly observed data, resulting in a continual training process. However, the conventional learning paradigm of neural networks relies on iterative gradient-based updates with a small learning rate, making it slow for large recommendation models to adapt. In this paper, we introduce ReLoop2, a self-correcting learning loop that facilitates fast model adaptation in online recommender systems through responsive error compensation. Inspired by the slow-fast complementary learning system observed in human brains, we propose an error memory module that directly stores error samples from incoming data streams. These stored samples are subsequently leveraged to compensate for model prediction errors during testing, particularly under distribution shifts. The error memory module is designed with fast access capabilities and undergoes continual refreshing with newly observed data samples during the model serving phase to support fast model adaptation. We evaluate the effectiveness of ReLoop2 on three open benchmark datasets as well as a real-world production dataset. The results demonstrate the potential of ReLoop2 in enhancing the responsiveness and adaptiveness of recommender systems operating in non-stationary environments.|工业推荐系统面临着在非平稳环境下运行的挑战，数据分布随着时间的推移而发生变化。为了应对这一挑战，一种常见的方法是定期用新观测数据重新训练或增量更新已部署的深度模型，从而形成持续的训练过程。然而，传统的神经网络学习范式依赖于迭代的基于梯度的更新，学习速度很小，使得大型推荐模型的适应速度变慢。本文介绍了 ReLoop2，一种通过响应误差补偿实现在线推荐系统中模型快速自适应的自校正学习循环。受到在人脑中观察到的慢-快互补学习系统的启发，我们提出了一个错误记忆模块，它直接存储来自输入数据流的错误样本。这些存储的样本随后被用来补偿测试期间的模型预测错误，特别是在分布变化的情况下。错误存储模块设计具有快速访问能力，并在模型服务阶段不断刷新新观察到的数据样本，以支持快速模型适应。我们评估了 ReLoop2在三个开放基准数据集和一个真实生产数据集上的有效性。结果表明，ReLoop2在提高非平稳环境中运行的推荐系统的响应能力和适应能力方面具有潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ReLoop2:+Building+Self-Adaptive+Recommendation+Models+via+Responsive+Error+Compensation+Loop)|0|
|[Trustworthy Recommender Systems: Foundations and Frontiers](https://doi.org/10.1145/3580305.3599575)|Wenqi Fan, Xiangyu Zhao, Lin Wang, Xiao Chen, Jingtong Gao, Qidong Liu, Shijie Wang|The Hong Kong Polytechnic University & Innovation, Chinese Academy of Sciences, Hong Kong SAR, China; City University of Hong Kong, Hong Kong SAR, China; The Hong Kong Polytechnic University, Hong Kong SAR, China; City University of Hong Kong & Xi'an Jiaotong University, Hong Kong SAR, China|Recommender systems aim to provide personalized suggestions to users, helping them make effective decisions. However, recent evidence has revealed the untrustworthy aspects of advanced recommender systems, leading to harmful effects in safety-critical areas like finance and healthcare. This tutorial will offer a comprehensive overview of achieving trustworthy recommender systems. It will cover six important aspects: Safety & Robustness, Non-discrimination & Fairness, Explainability, Privacy, Environmental Well-being, and Accountability & Auditability. Each aspect will be defined and categorized, followed by a discussion of the latest research progress and notable works. Additionally, potential interactions among these aspects and future research directions for trustworthy recommender systems will be explored.|推荐系统旨在为用户提供个性化的建议，帮助他们做出有效的决策。然而，最近的证据显示，先进的推荐系统的不可信方面，导致有害影响的安全关键领域，如金融和医疗保健。本教程将全面介绍如何实现值得信赖的推荐系统。它将涵盖六个重要方面: 安全与稳健、非歧视与公平、可解释性、隐私、环境福祉和问责与审核。每个方面将被定义和分类，然后讨论最新的研究进展和著名的工作。此外，这些方面之间的潜在交互作用和未来的研究方向值得信赖的推荐系统将被探讨。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Trustworthy+Recommender+Systems:+Foundations+and+Frontiers)|0|
|[Mining Electronic Health Records for Real-World Evidence](https://doi.org/10.1145/3580305.3599566)|Chengxi Zang, Weishen Pan, Fei Wang|Department of Clinical Pharmacy and Toxicology, Leiden University Medical Center, Leiden, the Netherlands.|Real-world evidence can close the inferential gap between marketing authorization studies and clinical practice. However, the current standard for real-world data extraction from electronic health records (EHRs) for treatment evaluation is manual review (MR), which is time-consuming and laborious. Clinical Data Collector (CDC) is a novel natural language processing and text mining software tool for both structured and unstructured EHR data and only shows relevant EHR sections improving efficiency. We investigated CDC as a real-world data (RWD) collection method, through application of CDC queries for patient inclusion and information extraction on a cohort of patients with metastatic renal cell carcinoma (RCC) receiving systemic drug treatment. Baseline patient characteristics, disease characteristics, and treatment outcomes were extracted and these were compared with MR for validation. One hundred patients receiving 175 treatments were included using CDC, which corresponded to 99% with MR. Calculated median overall survival was 21.7 months (95% confidence interval (CI) 18.7-24.8) vs. 21.7 months (95% CI 18.6-24.8) and progression-free survival 8.9 months (95% CI 5.4-12.4) vs. 7.6 months (95% CI 5.7-9.4) for CDC vs. MR, respectively. Highest F1-score was found for cancer-related variables (88.1-100), followed by comorbidities (71.5-90.4) and adverse drug events (53.3-74.5), with most diverse scores on international metastatic RCC database criteria (51.4-100). Mean data collection time was 12 minutes (CDC) vs. 86 minutes (MR). In conclusion, CDC is a promising tool for retrieving RWD from EHRs because the correct patient population can be identified as well as relevant outcome data, such as overall survival and progression-free survival.|真实世界的证据可以缩小上市许可研究和临床实践之间的推断差距。然而，目前从电子健康记录(EHRs)中提取真实数据用于治疗评估的标准是人工审查(MR) ，这是一项费时费力的工作。临床数据采集器(CDC)是一种新型的自然语言处理和文本挖掘软件工具，用于结构化和非结构化 EHR 数据，只显示相关的 EHR 部分提高效率。我们研究了 CDC 作为一种现实世界数据(RWD)收集方法，通过应用 CDC 查询对患者进行纳入，并对接受全身药物治疗的转移性信息抽取(rCC)患者队列进行肾细胞癌分析。提取基线患者特征、疾病特征和治疗结果，并与 MR 进行比较验证。接受175次治疗的100名患者使用 CDC，其中99% 为 MR。计算的中位总生存期分别为21.7个月(95% 置信区间(CI)18.7-24.8)和21.7个月(95% CI 18.6-24.8)和无进展生存期分别为 CDC 和 MR 的8.9个月(95% CI 5.4-12.4)和7.6个月(95% CI 5.7-9.4)。发现癌症相关变量(88.1-100)的 F1评分最高，其次是合并症(71.5-90.4)和不良药物事件(53.3-74.5) ，国际转移性 RCC 数据库标准(51.4-100)。平均数据收集时间为12分钟(CDC)比86分钟(MR)。总之，CDC 是从 EHR 中检索 RWD 的有希望的工具，因为可以确定正确的患者人群以及相关的结果数据，如总生存期和无进展生存期。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mining+Electronic+Health+Records+for+Real-World+Evidence)|0|
|[EvalRS 2023: Well-Rounded Recommender Systems for Real-World Deployments](https://doi.org/10.1145/3580305.3599222)|Federico Bianchi, Patrick John Chia, Jacopo Tagliabue, Ciro Greco, Gabriel de Souza P. Moreira, Davide Eynard, Fahd Husain, Claudio Pomo||EvalRS aims to bring together practitioners from industry and academia to foster a debate on rounded evaluation of recommender systems, with a focus on real-world impact across a multitude of deployment scenarios. Recommender systems are often evaluated only through accuracy metrics, which fall short of fully characterizing their generalization capabilities and miss important aspects, such as fairness, bias, usefulness, informativeness. This workshop builds on the success of last year's workshop at CIKM, but with a broader scope and an interactive format.|EvalRS 旨在汇集来自行业和学术界的从业人员，促进关于全面评估推荐系统的辩论，重点是在多种部署情景下的现实世界影响。推荐系统往往只能通过精度指标进行评估，这些指标不能充分表征推荐系统的泛化能力，而且忽略了公平性、偏差性、有用性、信息性等重要方面。这个研讨会建立在去年 CIKM 研讨会的成功基础之上，但是范围更广，而且采用了交互式的形式。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EvalRS+2023:+Well-Rounded+Recommender+Systems+for+Real-World+Deployments)|0|
|[A Multi-stage Framework for Online Bonus Allocation Based on Constrained User Intent Detection](https://doi.org/10.1145/3580305.3599764)|Chao Wang, Xiaowei Shi, Shuai Xu, Zhe Wang, Zhiqiang Fan, Yan Feng, An You, Yu Chen||With the explosive development of e-commerce for service, tens of millions of orders are generated every day on the Meituan platform. By allocating bonuses to new customers when they pay, the Meituan platform encourages them to use its own payment service for a better experience in the future. It can be formulated as a multi-choice knapsack problem (MCKP), and the mainstream solution is usually a two-stage method. The first stage is user intent detection, predicting the effect for each bonus treatment. Then, it serves as the objective of the MCKP, and the problem is solved in the second stage to obtain the optimal allocation strategy. However, this solution usually faces the following challenges: (1) In the user intent detection stage, due to the sparsity of interaction and noise, the traditional multi-treatment effect estimation methods lack interpretability, which may violate the domain knowledge that the marginal gain is non-negative with the increase of the bonus amount in economic theory. (2) There is an optimality gap between the two stages, which limits the upper bound of the optimal value obtained in the second stage. (3) Due to changes in the distribution of orders online, the actual cost consumption often violates the given budget limit. To solve the above challenges, we propose a framework that consists of three modules, i.e., User Intent Detection Module, Online Allocation Module, and Feedback Control Module. In the User Intent Detection Module, we implicitly model the treatment increment based on deep representation learning and constrain it to be non-negative to achieve monotonicity constraints. Then, in order to reduce the optimality gap, we further propose a convex constrained model to increase the upper bound of the optimal value. For the third challenge, to cope with the fluctuation of online bonus consumption, we leverage a feedback control strategy in the framework to make the actual cost more accurately approach the given budget limit. Finally, we conduct extensive offline and online experiments, demonstrating the superiority of our proposed framework, which reduced customer acquisition costs by 5.07% and is still running online.|随着服务性电子商务的迅猛发展，美团平台每天产生数以千万计的订单。通过在新客户付款时向他们发放奖金，美团平台鼓励他们在未来使用自己的支付服务，以获得更好的体验。它可以表述为一个多选择背包问题(MCKP) ，而主流的解决方案通常是一个两阶段的方法。第一个阶段是用户意图检测，预测每个奖金处理的效果。然后以此作为 MCKP 的目标，在第二阶段对问题进行求解，得到最优分配策略。(1)在用户意图检测阶段，由于交互作用和噪声的稀疏性，传统的多处理效应估计方法缺乏可解释性，这可能违反了经济学理论中边际收益随着奖金数额的增加而非负的领域知识。(2)两阶段之间存在一个最优性差距，限制了第二阶段得到的最优值的上界。(3)由于在线订单分布的变化，实际成本消耗往往违反给定的预算限额。为了解决上述问题，我们提出了一个由三个模块组成的框架，即用户意图检测模块、在线分配模块和反馈控制模块。在用户意图检测模块中，我们对基于深度表示学习的治疗增量进行隐式建模，并将其约束为非负值以达到单调性约束。然后，为了减少最优性间隔，我们进一步提出了一个凸约束模型来增加最优值的上界。对于第三个挑战，为了应对在线奖金消费的波动，我们利用框架中的反馈控制策略，使实际成本更准确地接近给定的预算限额。最后，我们进行了广泛的离线和在线实验，证明了我们提出的框架的优越性，它降低了5.07% 的客户获取成本，并仍然在线运行。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Multi-stage+Framework+for+Online+Bonus+Allocation+Based+on+Constrained+User+Intent+Detection)|0|
|[LEA: Improving Sentence Similarity Robustness to Typos Using Lexical Attention Bias](https://doi.org/10.1145/3580305.3599402)|Mario Almagro, Emilio J. Almazán, Diego Ortego, David Jiménez|NielsenIQ Innovation, Madrid, Spain|Textual noise, such as typos or abbreviations, is a well-known issue that penalizes vanilla Transformers for most downstream tasks. We show that this is also the case for sentence similarity, a fundamental task in multiple domains, e.g. matching, retrieval or paraphrasing. Sentence similarity can be approached using cross-encoders, where the two sentences are concatenated in the input allowing the model to exploit the inter-relations between them. Previous works addressing the noise issue mainly rely on data augmentation strategies, showing improved robustness when dealing with corrupted samples that are similar to the ones used for training. However, all these methods still suffer from the token distribution shift induced by typos. In this work, we propose to tackle textual noise by equipping cross-encoders with a novel LExical-aware Attention module (LEA) that incorporates lexical similarities between words in both sentences. By using raw text similarities, our approach avoids the tokenization shift problem obtaining improved robustness. We demonstrate that the attention bias introduced by LEA helps cross-encoders to tackle complex scenarios with textual noise, specially in domains with short-text descriptions and limited context. Experiments using three popular Transformer encoders in five e-commerce datasets for product matching show that LEA consistently boosts performance under the presence of noise, while remaining competitive on the original (clean) splits. We also evaluate our approach in two datasets for textual entailment and paraphrasing showing that LEA is robust to typos in domains with longer sentences and more natural context. Additionally, we thoroughly analyze several design choices in our approach, providing insights about the impact of the decisions made and fostering future research in cross-encoders dealing with typos.|文本噪音，如输入错误或缩写，是一个众所周知的问题，惩罚大多数下游任务的普通变形金刚。我们证明了句子相似性也是如此，这是多领域的基本任务，例如匹配、检索或释义。句子相似性可以使用交叉编码器进行处理，其中两个句子在输入中连接，使模型能够利用它们之间的相互关系。以往的研究主要依靠数据增强策略来解决噪声问题，在处理类似于训练样本的损坏样本时显示出了更好的鲁棒性。但是，所有这些方法仍然受到输入错误引起的令牌分布偏移的影响。在这项工作中，我们提出了解决文本噪声的交叉编码器装备一个新颖的词汇感知注意模块(LEA) ，其中包括词汇之间的相似性在两个句子。通过使用原始文本的相似性，我们的方法避免了标记转移问题，获得了更好的鲁棒性。我们证明了 LEA 引入的注意偏差有助于交叉编码器处理文本噪声的复杂场景，特别是在短文本描述和有限上下文的领域。在五个电子商务数据集中使用三种流行的变压器编码器进行产品匹配的实验表明，LEA 在噪声存在的情况下始终提高性能，同时在原始(干净)分割上保持竞争力。我们还评估了我们的方法在两个数据集的文字蕴涵和释义表明，LEA 是健壮的字体错误的领域与较长的句子和更自然的上下文。此外，我们彻底分析了我们的方法中的几种设计选择，提供了关于所做决定的影响的见解，并促进了交叉编码器处理输入错误的未来研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LEA:+Improving+Sentence+Similarity+Robustness+to+Typos+Using+Lexical+Attention+Bias)|0|
|[IPOC: An Adaptive Interval Prediction Model based on Online Chasing and Conformal Inference for Large-Scale Systems](https://doi.org/10.1145/3580305.3599396)|Jiadong Chen, Yang Luo, Xiuqi Huang, Fuxin Jiang, Yangguang Shi, Tieying Zhang, Xiaofeng Gao|Bytedance Inc.; Shanghai Jiao Tong University; Shandong University|In large-scale systems, due to system complexity and demand volatility, diverse and dynamic workloads make accurate predictions difficult. In this work, we address an online interval prediction problem (OnPred-Int) and adopt ensemble learning to solve it. We depict that the ensemble learning for OnPred-Int is a dynamic deterministic Markov Decision Process (Dd-MDP) and convert it into a stateful online learning task. Then we propose IPOC, a lightweight and flexible model able to produce effective confidence intervals, adapting the dynamics of real-time workload streams. At each time, IPOC selects a target model and executes chasing for it by a designed chasing oracle, during which process IPOC produces accurate confidence intervals. The effectiveness of IPOCis theoretically validated through sublinear regret analysis and satisfaction of confidence interval requirements. Besides, we conduct extensive experiments on 4 real-world datasets comparing with 19 baselines. To the best of our knowledge, we are the first to apply the frontier theory of online learning to time series prediction tasks.|在大型系统中，由于系统的复杂性和需求的不稳定性，多样化和动态的工作负载使得准确的预测变得困难。在这项工作中，我们解决了一个在线时间间隔预测问题(OnPred-Int) ，并采用集成学习来解决它。我们描述了 Onpred-Int 的集成学习是一个动态的确定性马可夫决策过程(dd-mDP) ，并将其转换为一个有状态的在线学习任务。然后我们提出了 IPOC 模型，这是一个轻量级的、灵活的模型，能够产生有效的置信区间，适应实时工作流的动态性。每次，IPOC 选择一个目标模型并通过一个设计的追踪预言执行追踪，在这个过程中 IPOC 产生准确的置信区间。通过次线性遗憾分析和满足置信区间要求，从理论上验证了 IPoc 的有效性。此外，我们对4个真实世界的数据集进行了广泛的实验，比较了19个基线。据我们所知，我们是第一个将在线学习的前沿理论应用到时间序列预测任务中的人。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IPOC:+An+Adaptive+Interval+Prediction+Model+based+on+Online+Chasing+and+Conformal+Inference+for+Large-Scale+Systems)|0|
|[SketchPolymer: Estimate Per-item Tail Quantile Using One Sketch](https://doi.org/10.1145/3580305.3599505)|Jiarui Guo, Yisen Hong, Yuhan Wu, Yunfei Liu, Tong Yang, Bin Cui||1 Estimating the quantile of distribution, especially tail distribution, is an interesting topic in data stream models, and has obtained extensive interest from many researchers. In this paper, we propose a novel sketch, namely SketchPolymer to accurately estimate per-item tail quantile. SketchPolymer uses a technique called Early Filtration to filter infrequent items, and another technique called VSS to reduce error. Our experimental results show that the accuracy of SketchPolymer is on average 32.67 times better than state-of-the-art techniques. We also implement our SketchPolymer on P4 and FPGA platforms to verify its deployment flexibility. All our codes are available at GitHub.[1]|1分布的分位数估计，特别是尾分布的估计，是数据流模型中的一个有趣的课题，已经引起了许多研究者的广泛兴趣。在本文中，我们提出了一个新的草图，即素描聚合物，以准确估计每个项目的尾分位数。素描聚合物使用一种称为早期过滤的技术来过滤不常见的项目，另一种称为 VSS 的技术来减少错误。我们的实验结果表明，素描聚合物的准确性平均是32.67倍以上的国家最先进的技术。我们还在 P4和 FPGA 平台上实现了我们的 SketchPP，以验证其部署灵活性。我们所有的代码都可以在 GitHub 上找到。[1]|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SketchPolymer:+Estimate+Per-item+Tail+Quantile+Using+One+Sketch)|0|
|[Unbiased Locally Private Estimator for Polynomials of Laplacian Variables](https://doi.org/10.1145/3580305.3599537)|Quentin Hillebrand, Vorapong Suppakitpaisarn, Tetsuo Shibuya||This work presents a mechanism to debias polynomial functions computed from locally differentially private data. Local differential privacy is a widely used privacy notion where users add Laplacian noise to their information before submitting it to a central server. That, however, causes bias when we calculate non-linear functions based on those noisy information. Our proposed recursive algorithm debiases these functions, with a calculation time of O(r n log n), where r is the polynomial degree and n is the number of users. We evaluate our method on the problems of k-star counting and variance estimation, comparing results with state-of-the-art algorithms. The results show that our method not only eliminates bias, but also provides at least 100 times more accuracy than previous works.|本文提出了一种从局部差分私有数据中去偏多项式函数的机制。本地差分隐私是一个广泛使用的隐私概念，用户在将信息提交到中央服务器之前，会在其中添加拉普拉斯噪音。然而，当我们基于这些噪声信息计算非线性函数时，这会导致偏差。我们提出的递归算法将这些函数的计算时间缩短为 O (r n logn) ，其中 r 是多项式次数，n 是用户数。我们评估了我们的方法在 k 星计数和方差估计问题，比较结果与国家的最新算法。结果表明，我们的方法不仅消除了偏差，而且提供了至少100倍以上的精度比以往的工作。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unbiased+Locally+Private+Estimator+for+Polynomials+of+Laplacian+Variables)|0|
|[Semantic Dissimilarity Guided Locality Preserving Projections for Partial Label Dimensionality Reduction](https://doi.org/10.1145/3580305.3599496)|Yuheng Jia, Jiahao Jiang, Yongheng Wang||Partial label learning (PLL) is a significant weakly supervised learning framework, where each training example corresponds to a set of candidate labels among which only one is the ground-truth label. Existing works on partial label dimensionality reduction only exploit the disambiguated labels, but overlook the available semantic dissimilarity relationship hidden in the disambiguated labeling confidence, i.e., the smaller the inner product of the labeling confidences of two instances, the less likely they have the same ground-truth label. By combining such global dissimilarity relationship with local neighborhood information, we propose a novel partial label dimensionality reduction method named SDLPP, which employs an alternating procedure including candidate label disambiguation, semantic dissimilarity generation and dimensionality reduction. The labeling confidences of candidate labels and semantic dissimilarity relationship are constantly updated through the alternating procedure, where the processes in each iteration are based on the low-dimensional data obtained in the previous iteration. After the alternating procedure, SDLPP maps the original data to a pre-specified low-dimensional feature space. Comprehensive experiments on both synthetic and real-world data sets validate that SDLPP can improve the generalization performance of different PLL algorithms, and outperform state-of-the-art partial label dimensionality reduction methods. The codes can be publicly accessible on the link https://github.com/jhjiangSEU/SDLPP.|部分标签学习(PLL)是一个重要的弱监督式学习框架，其中每个训练例子对应一组候选标签，其中只有一个是地面真相标签。现有的部分标签降维只利用了消除歧义的标签，但忽略了隐藏在消除歧义标签置信度中的可用语义差异关系，即，两个实例的标签置信度的内积越小，它们具有相同的基本事实标签的可能性就越小。通过将这种全局不相似关系与局部邻域信息相结合，提出了一种新的部分标签降维方法 SDLPP，该方法采用了候选标签消歧、语义不相似生成和降维的交替过程。候选标签的标注置信度和语义不相似关系通过交替过程不断更新，其中每次迭代的过程都基于前一次迭代获得的低维数据。在交替过程之后，SDLPP 将原始数据映射到预先指定的低维特征空间。对合成和真实数据集的综合实验验证了 SDLPP 可以提高不同 PLL 算法的泛化性能，并优于最先进的部分标签降维方法。这些代码可在连结 https://github.com/jhjiangseu/sdlpp 上公开查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semantic+Dissimilarity+Guided+Locality+Preserving+Projections+for+Partial+Label+Dimensionality+Reduction)|0|
|[B2-Sampling: Fusing Balanced and Biased Sampling for Graph Contrastive Learning](https://doi.org/10.1145/3580305.3599262)|Mengyue Liu, Yun Lin, Jun Liu, Bohao Liu, Qinghua Zheng, Jin Song Dong|Xi’an Jiaotong University; Shanghai Jiao Tong University; National University of Singapore|Graph contrastive learning (GCL), aiming for an embedding space where semantically similar nodes are closer, has been widely applied in graph-structured data. Researchers have proposed many approaches to define positive and negative pairs (i.e., semantically similar and dissimilar pairs) on the graph, serving as labels to learn their embedding distances. Despite the effectiveness, those approaches usually suffer from two typical learning challenges. First, the number of candidate negative pairs is enormous. Thus, it is non-trivial to select representative ones to train the model in a more effective way. Second, the heuristics (e.g., graph views or meta-path patterns) to define positive and negative pairs are sometimes less reliable, causing considerable noise for both "labelled" positive and negative pairs. In this work, we propose a novel sampling approach B-2-Sampling to address the above challenges in a unified way. On the one hand, we use balanced sampling to select the most representative negative pairs regarding both the topological and embedding diversities. On the other hand, we use biased sampling to learn and correct the labels of the most error-prone negative pairs during the training. The balanced and biased samplings can be applied iteratively for discriminating and correcting training pairs, boosting the performance of GCL models. B-2-Sampling is designed as a framework to support many known GCL models. Our extensive experiments on node classification, node clustering, and graph classification tasks show that B-2-Sampling significantly improves the performance of GCL models with acceptable run-time overhead. Our website [11] provides access to our codes and additional experiment results.|图形对比学习(GCL)是一种针对语义相似节点更接近的嵌入空间的学习方法，在图结构化数据中得到了广泛的应用。研究人员提出了许多方法来定义图上的正对和负对(即语义相似和不相似的对) ，作为标签来学习它们的嵌入距离。尽管有效，这些方法通常会遇到两个典型的学习挑战。首先，候选负对的数量是巨大的。因此，选择具有代表性的模型对模型进行更有效的训练是非常重要的。其次，用于定义正面和负面对的启发式(例如，图形视图或元路径模式)有时不太可靠，对“标记的”正面和负面对造成相当大的噪音。在这项工作中，我们提出了一种新颖的抽样方法 B-2-抽样，以解决上述挑战在一个统一的方式。一方面，我们采用平衡抽样的方法，从拓扑和嵌入差异两方面选择最具代表性的负对。另一方面，在训练过程中，我们使用偏向抽样来学习和纠正最容易出错的否定对的标签。平衡和偏置抽样可以迭代地应用于训练样本的判别和校正，提高了 GCL 模型的性能。B-2-Sampling 被设计成一个支持许多已知 GCL 模型的框架。我们在节点分类、节点聚类和图分类任务上的大量实验表明，B-2采样能够在可接受的运行时开销下显著提高 GCL 模型的性能。我们的网站[11]提供了我们的代码和额外的实验结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=B2-Sampling:+Fusing+Balanced+and+Biased+Sampling+for+Graph+Contrastive+Learning)|0|
|[DotHash: Estimating Set Similarity Metrics for Link Prediction and Document Deduplication](https://doi.org/10.1145/3580305.3599314)|Igor Nunes, Mike Heddes, Pere Vergés, Danny Abraham, Alexander V. Veidenbaum, Alex Nicolau, Tony Givargis|University of California, Irvine|Metrics for set similarity are a core aspect of several data mining tasks. To remove duplicate results in a Web search, for example, a common approach looks at the Jaccard index between all pairs of pages. In social network analysis, a much-celebrated metric is the Adamic-Adar index, widely used to compare node neighborhood sets in the important problem of predicting links. However, with the increasing amount of data to be processed, calculating the exact similarity between all pairs can be intractable. The challenge of working at this scale has motivated research into efficient estimators for set similarity metrics. The two most popular estimators, MinHash and SimHash, are indeed used in applications such as document deduplication and recommender systems where large volumes of data need to be processed. Given the importance of these tasks, the demand for advancing estimators is evident. We propose DotHash, an unbiased estimator for the intersection size of two sets. DotHash can be used to estimate the Jaccard index and, to the best of our knowledge, is the first method that can also estimate the Adamic-Adar index and a family of related metrics. We formally define this family of metrics, provide theoretical bounds on the probability of estimate errors, and analyze its empirical performance. Our experimental results indicate that DotHash is more accurate than the other estimators in link prediction and detecting duplicate documents with the same complexity and similar comparison time.|集合相似性度量是数据挖掘任务的一个核心方面。例如，为了删除 Web 搜索中的重复结果，通常的方法是查看所有页对之间的 Jaccard 索引。在社会网络分析中，一个著名的度量是阿达姆-阿达尔指数，广泛用于比较节点邻域集在预测链路的重要问题。然而，随着需要处理的数据量的增加，计算所有对之间的精确相似度是很困难的。在这种规模下工作的挑战促使人们研究集合相似度量的有效估计器。MinHash 和 SimHash 这两个最流行的估计器确实用于需要处理大量数据的应用程序，如文档删除重复数据和推荐系统。鉴于这些任务的重要性，提前估算的需求是显而易见的。我们提出了 DotHash，一个两个集合的交集大小的无偏估计。DotHash 可以用来估计 Jaccard 指数，据我们所知，DotHash 是第一种也可以估计 Adam-Adar 指数和一系列相关指标的方法。我们正式地定义了这个度量族，给出了估计误差概率的理论界限，并分析了它的经验性能。实验结果表明，在相同复杂度和相似比较时间的链路预测和重复文档检测方面，DotHash 比其他估计器具有更高的精度。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DotHash:+Estimating+Set+Similarity+Metrics+for+Link+Prediction+and+Document+Deduplication)|0|
|[Domain-Guided Spatio-Temporal Self-Attention for Egocentric 3D Pose Estimation](https://doi.org/10.1145/3580305.3599312)|Jinman Park, Kimathi Kaai, Saad Hossain, Norikatsu Sumi, Sirisha Rambhatla, Paul W. Fieguth||Vision-based ego-centric 3D human pose estimation (ego-HPE) is essential to support critical applications of xR-technologies. However, severe self-occlusions and strong distortion introduced by the fish-eye view from the head mounted camera, make ego-HPE extremely challenging. To address these challenges, we propose a domain-guided spatio-temporal transformer model that leverages information specific to ego-views. Powered by this domain-guided transformer, we build Egocentric Spatio-Temporal Self-Attention Network (Ego-STAN), which uses 2D image representations and spatio-temporal attention to address both distortions and self-occlusions in ego-HPE. Additionally, we introduce a spatial concept called feature map tokens (FMT) which endows Ego-STAN with the ability to draw complex spatio-temporal information encoded in ego-centric videos. Our quantitative evaluation on the contemporary xR-EgoPose dataset, achieves a 38.2% improvement on the highest error joints against the SOTA ego-HPE model, while accomplishing a 22% decrease in the number of parameters. Finally, we also demonstrate the generalization capabilities of our model to real-world HPE tasks beyond ego-views achieving 7.7% improvement on 2D human pose estimation with the Human3.6M dataset. Our code is also made available at: https://github.com/jmpark0808/Ego-STAN|基于视觉的以自我为中心的三维人体姿态估计(ego-HPE)对于支持 xR 技术的关键应用至关重要。然而，严重的自闭和强烈的扭曲所引入的鱼眼视图从头部安装的相机，使自我 HPE 极具挑战性。为了应对这些挑战，我们提出了一个领域引导的时空转换器模型，该模型利用特定于自我视图的信息。在此基础上，构建了以自我为中心的时空自注意网络(Ego-STAN) ，该网络利用二维图像表示和时空注意来解决自我 HPE 中的失真和自我遮挡问题。此外，我们还引入了一个称为特征映射标记(FMT)的空间概念，它赋予自我 STAN 提取以自我为中心的视频中编码的复杂时空信息的能力。我们对当代 xR-EgoPose 数据集的定量评估，对 SOTA ego-HPE 模型的最高误差关节实现了38.2% 的改进，同时实现了22% 的参数数量减少。最后，我们还展示了我们的模型对现实世界 HPE 任务的泛化能力，超越了自我视图，使用 Human3.6 M 数据集对二维人体姿态估计的改进达到了7.7% 。我们的代码也可以在以下 https://github.com/jmpark0808/ego-stan 找到|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Domain-Guided+Spatio-Temporal+Self-Attention+for+Egocentric+3D+Pose+Estimation)|0|
|[Quantitatively Measuring and Contrastively Exploring Heterogeneity for Domain Generalization](https://doi.org/10.1145/3580305.3599481)|Yunze Tong, Junkun Yuan, Min Zhang, Didi Zhu, Keli Zhang, Fei Wu, Kun Kuang|Zhejiang University; Noah’s Ark Lab, Huawei Technologies|Domain generalization (DG) is a prevalent problem in real-world applications, which aims to train well-generalized models for unseen target domains by utilizing several source domains. Since domain labels, i.e., which domain each data point is sampled from, naturally exist, most DG algorithms treat them as a kind of supervision information to improve the generalization performance. However, the original domain labels may not be the optimal supervision signal due to the lack of domain heterogeneity, i.e., the diversity among domains. For example, a sample in one domain may be closer to another domain, its original label thus can be the noise to disturb the generalization learning. Although some methods try to solve it by re-dividing domains and applying the newly generated dividing pattern, the pattern they choose may not be the most heterogeneous due to the lack of the metric for heterogeneity. In this paper, we point out that domain heterogeneity mainly lies in variant features under the invariant learning framework. With contrastive learning, we propose a learning potential-guided metric for domain heterogeneity by promoting learning variant features. Then we notice the differences between seeking variance-based heterogeneity and training invariance-based generalizable model. We thus propose a novel method called Heterogeneity-based Two-stage Contrastive Learning (HTCL) for the DG task. In the first stage, we generate the most heterogeneous dividing pattern with our contrastive metric. In the second stage, we employ an invariance-aimed contrastive learning by re-building pairs with the stable relation hinted by domains and classes, which better utilizes generated domain labels for generalization learning. Extensive experiments show HTCL better digs heterogeneity and yields great generalization performance.|领域广义化(DG)是现实应用中的一个普遍问题，其目的是利用多个源域来训练未知目标域的广义模型。由于领域标签(即每个数据点从哪个领域采样)的自然存在，大多数 DG 算法都将它们视为一种监督信息，以提高泛化性能。然而，由于缺乏领域异质性，即领域之间的差异性，原始的领域标签可能不是最佳的监督信号。例如，一个领域中的样本可能更接近另一个领域，其原始标签因此可能是噪声干扰推广学习。尽管有些方法试图通过重新划分域并应用新生成的划分模式来解决这个问题，但是由于缺乏对异构性的度量，所选择的模式可能不是最异构的。本文指出，在不变学习框架下，领域异质性主要表现在变异特征上。在对比学习的基础上，提出了一种基于学习势引导的领域异构度量方法。然后我们注意到基于方差的异质性寻求和基于训练不变性的可推广模型之间的区别。因此，我们提出了一种新的方法称为异质性为基础的两阶段对比学习(HTCL)的 DG 任务。在第一阶段，我们使用对比度量生成最不均匀的分割模式。在第二阶段，我们采用不变性对比学习方法，通过重新构建由领域和类提示的稳定关系的对，更好地利用生成的领域标签进行泛化学习。大量实验表明，HTCL 能够更好地挖掘异构性，并产生很好的泛化性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Quantitatively+Measuring+and+Contrastively+Exploring+Heterogeneity+for+Domain+Generalization)|0|
|[Grace: Graph Self-Distillation and Completion to Mitigate Degree-Related Biases](https://doi.org/10.1145/3580305.3599368)|Hui Xu, Liyao Xiang, Femke Huang, Yuting Weng, Ruijie Xu, Xinbing Wang, Chenghu Zhou|Shanghai Jiao Tong University, Shanghai, China; Chinese Academy of Sciences, Beijing, China|Due to the universality of graph data, node classification shows its great importance in a wide range of real-world applications. Despite the successes of Graph Neural Networks (GNNs), GNN based methods rely heavily on rich connections and perform poorly on low-degree nodes. Since many real-world graphs follow a long-tailed distribution in node degrees, they suffer from a substantial performance bottleneck as a significant fraction of nodes is of low degree. In this paper, we point out that under-represented self-representations and low neighborhood homophily ratio of low-degree nodes are two main culprits. Based on that, we propose a novel method Grace which improves the node representation by self-distillation, and increases neighborhood homophily ratio of low-degree nodes by graph completion. To avoid error propagation of graph completion, label propagation is further leveraged. Experimental evidence has shown that our method well supports real-world graphs, and is superior in balancing degree-related bias and overall performance on node classification tasks.|由于图形数据的通用性，节点分类在实际应用中显得非常重要。尽管图形神经网络(GNN)取得了成功，但是基于 GNN 的方法在很大程度上依赖于丰富的连接，在低度节点上表现不佳。由于现实世界中的许多图都遵循节点度的长尾分布，因此，由于很大一部分节点的度较低，它们受到了严重的性能瓶颈问题的困扰。本文指出低度节点的自我表征不足和低邻域同调比是造成这种现象的两个主要原因。在此基础上，提出了一种新的格雷斯方法，该方法通过自提取改善了节点的表示，并通过图完成提高了低度节点的邻域同调率。为了避免图完成的错误传播，进一步利用了标签传播。实验结果表明，该方法能够很好地支持实际图形，并且在平衡度相关偏差和节点分类任务的整体性能方面具有优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Grace:+Graph+Self-Distillation+and+Completion+to+Mitigate+Degree-Related+Biases)|0|
|[DisasterNet: Causal Bayesian Networks with Normalizing Flows for Cascading Hazards Estimation from Satellite Imagery](https://doi.org/10.1145/3580305.3599807)|Xuechun Li, Paula M. Bürgi, Wei Ma, Hae Young Noh, David Jay Wald, Susu Xu||Sudden-onset hazards like earthquakes often induce cascading secondary hazards (e.g., landslides, liquefaction, debris flows, etc.) and subsequent impacts (e.g., building and infrastructure damage) that cause catastrophic human and economic losses. Rapid and accurate estimates of these hazards and impacts are critical for timely and effective post-disaster responses. Emerging remote sensing techniques provide pre- and post-event satellite images for rapid hazard estimation. However, hazards and damage often co-occur or colocate with underlying complex cascading geophysical processes, making it challenging to directly differentiate multiple hazards and impacts from satellite imagery using existing single-hazard models. We introduce DisasterNet, a novel family of causal Bayesian networks to model processes that a major hazard triggers cascading hazards and impacts and further jointly induces signal changes in remotely sensed observations. We integrate normalizing flows to effectively model the highly complex causal dependencies in this cascading process. A triplet loss is further designed to leverage prior geophysical knowledge to enhance the identifiability of our highly expressive Bayesian networks. Moreover, a novel stochastic variational inference with normalizing flows is derived to jointly approximate posteriors of multiple unobserved hazards and impacts from noisy remote sensing observations. Integrating with the USGS Prompt Assessment of Global Earthquakes for Response (PAGER) system, our framework is evaluated in recent global earthquake events. Evaluation results show that DisasterNet significantly improves multiple hazard and impact estimation compared to existing USGS products.|像地震这样的突发性灾害通常会引发连锁的次级灾害(如山体滑坡、液化、泥石流等)和随后的影响(如建筑物和基础设施的破坏) ，从而造成灾难性的人员和经济损失。迅速和准确地估计这些灾害和影响对及时和有效的灾后反应至关重要。新兴的遥感技术提供事件发生前后的卫星图像，用于快速评估灾害。然而，危害和损害往往与潜在的复杂的级联地球物理过程同时发生或共同存在，这使得利用现有的单一危害模型直接区分多种危害和影响具有挑战性卫星地图。我们引入灾难网络，一个新的因果贝叶斯网络家族模型的过程中，一个主要的危险触发级联危险和影响，并进一步共同诱导信号变化的遥感观测。我们整合了规范化流程来有效地模拟这个级联过程中高度复杂的因果依赖关系。进一步设计了三元组损失，以利用先前的地球物理知识，提高我们的高度表达贝叶斯网络的可识别性。此外，一个新的随机变分推断与归一化流动推导出联合近似后多个未观测的危险和影响的噪声遥感观测。结合美国地质调查局全球地震快速反应评估(PAGER)系统，对我们的框架在最近的全球地震事件中进行了评估。评估结果表明，与现有的 USGS 产品相比，灾难网络显著改善了多重危害和影响的估计。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DisasterNet:+Causal+Bayesian+Networks+with+Normalizing+Flows+for+Cascading+Hazards+Estimation+from+Satellite+Imagery)|0|
|[Explicit Feature Interaction-aware Uplift Network for Online Marketing](https://doi.org/10.1145/3580305.3599820)|Dugang Liu, Xing Tang, Han Gao, Fuyuan Lyu, Xiuqiang He|McGill University; Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ; FiT, Tencent|As a key component in online marketing, uplift modeling aims to accurately capture the degree to which different treatments motivate different users, such as coupons or discounts, also known as the estimation of individual treatment effect (ITE). In an actual business scenario, the options for treatment may be numerous and complex, and there may be correlations between different treatments. In addition, each marketing instance may also have rich user and contextual features. However, existing methods still fall short in both fully exploiting treatment information and mining features that are sensitive to a particular treatment. In this paper, we propose an explicit feature interaction-aware uplift network (EFIN) to address these two problems. Our EFIN includes four customized modules: 1) a feature encoding module encodes not only the user and contextual features, but also the treatment features; 2) a self-interaction module aims to accurately model the user's natural response with all but the treatment features; 3) a treatment-aware interaction module accurately models the degree to which a particular treatment motivates a user through interactions between the treatment features and other features, i.e., ITE; and 4) an intervention constraint module is used to balance the ITE distribution of users between the control and treatment groups so that the model would still achieve a accurate uplift ranking on data collected from a non-random intervention marketing scenario. We conduct extensive experiments on two public datasets and one product dataset to verify the effectiveness of our EFIN. In addition, our EFIN has been deployed in a credit card bill payment scenario of a large online financial platform with a significant improvement.|作为在线营销的一个关键组成部分，提升模型旨在准确地捕捉不同的治疗激励不同的用户的程度，如优惠券或折扣，也被称为个体治疗效果(ITE)的估计。在实际的业务场景中，治疗的选择可能是多种多样和复杂的，不同治疗之间可能存在相关性。此外，每个营销实例还可能具有丰富的用户和上下文特性。然而，现有方法仍然不能充分利用对特定治疗敏感的治疗信息和挖掘特征。针对这两个问题，本文提出了一种显式的特征交互感知提升网络(EFIN)。我们的 EFIN 包括四个定制模块: 1)特征编码模块不仅编码用户和上下文特征，而且还编码治疗特征; 2)自我交互模块旨在准确地模拟除治疗特征以外的所有用户的自然反应; 3)治疗感知交互模块准确地模拟特定治疗通过治疗特征和其他特征之间的交互激励用户的程度，即 ITE; 和4)干预约束模块用于平衡控制和治疗组之间的用户 ITE 分布，以便该模型仍然能够实现从非随机干预营销场景收集的数据的准确提升排名。我们对两个公共数据集和一个产品数据集进行了广泛的实验，以验证我们的 EFIN 的有效性。此外，我们的 EFIN 已经部署在一个大型在线金融平台的信用卡账单支付场景中，并得到了显著的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explicit+Feature+Interaction-aware+Uplift+Network+for+Online+Marketing)|0|
|[Online Quality Prediction in Windshield Manufacturing using Data-Efficient Machine Learning](https://doi.org/10.1145/3580305.3599880)|Hasan Tercan, Tobias Meisen||The digitization of manufacturing processes opens up the possibility of using machine learning methods on process data to predict future product quality. Based on the model predictions, quality improvement actions can be taken at an early stage. However, significant challenges must be overcome to successfully implement the predictions. Production lines are subject to hardware and memory limitations and are characterized by constant changes in quality influencing factors. In this paper, we address these challenges and present an online prediction approach for real-world manufacturing processes. On the one hand, it includes methods for feature extraction and selection from multimodal process and sensor data. On the other hand, a continual learning method based on memory-aware synapses is developed to efficiently train an artificial neural network over process changes. We deploy and evaluate the approach in a windshield production process. Our experimental evaluation shows that the model can accurately predict windshield quality and achieve significant process improvement. By comparing with other learning strategies such as transfer learning, we also show that the continual learning method both prevents catastrophic forgetting of the model and maintains its data efficiency.|制造过程的数字化为利用机器学习方法对过程数据进行预测未来产品质量提供了可能性。根据模型预测，可以在早期阶段采取质量改进措施。然而，要成功实现这些预测，必须克服重大挑战。生产线受到硬件和内存的限制，拥有属性的质量影响因素不断变化。在本文中，我们解决了这些挑战，并提出了一个在线预测方法的真实世界的制造过程。一方面，它包括从多模态过程和传感器数据中提取和选择特征的方法。另一方面，提出了一种基于记忆感知突触的连续学习方法，以有效地训练人工神经网络的过程变化。我们部署和评估的方法在一个挡风玻璃生产过程。实验结果表明，该模型能够准确地预测挡风玻璃的质量，实现了工艺的显著改进。通过与其他学习策略(如迁移学习)的比较，我们还发现连续学习方法不仅防止了模型的灾难性遗忘，而且保持了数据的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+Quality+Prediction+in+Windshield+Manufacturing+using+Data-Efficient+Machine+Learning)|0|
|[C-AOI: Contour-based Instance Segmentation for High-Quality Areas-of-Interest in Online Food Delivery Platform](https://doi.org/10.1145/3580305.3599786)|Yida Zhu, Liying Chen, Daping Xiong, Shuiping Chen, Fangxiao Du, Jinghua Hao, Renqing He, Zhizhao Sun||Online food delivery (OFD) services have become popular globally, serving people's daily needs. Precise area-of-interest (AOI) boundaries help OFD platforms determine customers' exact locations, which is crucial for maintaining consistency in delivery difficulty and providing a uniform customer experience within an AOI. Existing AOI generation methods primarily rely on predefined shapes or density-based clustering, which limits the quality of the contours. Recently, Meituan has treated the AOI contours as a binary semantic segmentation problem. Their approach involves a multi-step post-process to address the issues with boundary breaks caused by semantic segmentation models, leading to decreased quality and inefficiency in the learning process. In this paper, we propose a novel method for AOI contour generation called C-AOI (Contour-based Area-of-Interest). C-AOI is an instance segmentation model that focuses on generating high-quality AOI contours. Unlike the former method, which relies on pixel-by-pixel classification, C-AOI starts from the center point of the AOI and regresses the boundary. This approach results in a higher-quality boundary and is less computationally intensive. C-AOI first corrects errors on the contour using a local aggregation mechanism. Then, we propose a novel deforming module called the contour transformer, which captures the global geometry of the object. To enhance the positional relationship among vertices, we introduce a learnable cyclic positional encoding applied to the contour transformer. Finally, to improve the boundary details, we propose the Adaptive Matching Loss (AML) that eliminates over-smoothed boundaries and promotes optimized convergence pathways. Experimental results on real-world datasets collected from Meituan have demonstrated that C-AOI significantly improves the mask and boundary quality compared to Meituan's previous work. Moreover, Its inference speed is comparable to that of E2EC, a state-of-the-art real-time contour-based method. It is noteworthy that C-AOI has been deployed in the Meituan platform for producing AOIs.|在线送餐(OFD)服务已经在全球范围内流行起来，满足人们的日常需求。精确的感兴趣区域(AOI)边界有助于 OFD 平台确定客户的确切位置，这对于保持交付困难的一致性和在 AOI 中提供统一的客户体验至关重要。现有的 AOI 生成方法主要依赖于预定义的形状或基于密度的聚类，这限制了轮廓的质量。最近，美团已经将 AOI 轮廓作为一个二进制语义分割问题来处理。他们的方法涉及到一个多步骤的后处理，以解决由于语义分割模型造成的边界断裂问题，导致学习过程中质量下降和效率低下。本文提出了一种新的 AOI 轮廓生成方法，称为基于轮廓的感兴趣区域(C- AOI)。C-AOI 是一个实例分割模型，侧重于生成高质量的 AOI 轮廓。不像前一种方法依赖于像素分类，C-AOI 从 AOI 的中心点开始并回归边界。这种方法会产生一个更高质量的边界，并且计算量更小。C-AOI 首先使用本地聚合机制纠正轮廓上的错误。然后，我们提出了一种新颖的变形模块称为轮廓变换器，它捕捉物体的全局几何形状。为了增强顶点之间的位置关系，我们引入了一种可学习的循环位置编码应用于轮廓变换器。最后，为了改善边界细节，我们提出了自适应匹配损失(AML) ，消除了过于平滑的边界，促进优化收敛路径。实验结果表明，与美团美团以前的工作相比，C-AOI 显著改善了掩模和边界质量。此外，它的推理速度相当于 E2EC，一个国家的最先进的实时轮廓为基础的方法。值得注意的是，C-AOI 已被部署在生产 AOI 的美团平台上。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=C-AOI:+Contour-based+Instance+Segmentation+for+High-Quality+Areas-of-Interest+in+Online+Food+Delivery+Platform)|0|
|[Addressing Bias and Fairness in Machine Learning: A Practical Guide and Hands-on Tutorial](https://doi.org/10.1145/3580305.3599180)|Rayid Ghani, Kit T. Rodolfa, Pedro Saleiro, Sérgio M. Jesus|Feedzai, Porto, Portugal; Feedzai & Universidade do Porto, Porto, Portugal; Stanford University, Stanford, CA, USA; Carnegie Mellon University, Pittsburgh, PA, USA|As data science and machine learning (ML) increasingly shape our society, the importance of developing fair algorithmic decision-making systems becomes paramount. There is a pressing need to train data scientists and practitioners on handling bias and fairness in real-world scenarios, from early stages of a data science project to maintaining ML systems in production. Existing resources are mostly academic and cover the ML training and optimization aspects of bias mitigation, leaving practitioners without comprehensive frameworks for making decisions throughout a real-world project lifecycle. This tutorial aims to bridge the gap between research and practice, providing an in-depth exploration of algorithmic fairness, encompassing metrics and definitions, practical case studies, data bias understanding, bias mitigation and model fairness audits using the Aequitas toolkit. Participants will be equipped to engage in conversations about bias, assist decision-makers in understanding options and trade-offs, evaluate project scoping aspects influencing fairness outcomes, and define actions and interventions based on model predictions. They will also learn to identify cohorts, target variables, evaluation metrics, and establish bias and fairness goals for different groups. Moreover, participants will gain insights into auditing and mitigating model bias, and implementing continuous monitoring to assess retraining needs. The tutorial addresses the current lack of practical training materials, methodologies, and tools for researchers and developers working on real-world algorithmic decision-making systems. By the conclusion of this hands-on tutorial, attendees will be well-versed in navigating bias-related issues, selecting appropriate metrics, and applying bias audit and mitigation frameworks and tools for informed design decisions in real-world data science systems.|随着数据科学和机器学习(ML)对我们社会的影响越来越大，开发公平的算法决策系统变得至关重要。从数据科学项目的早期阶段到维护生产中的机器学习系统，迫切需要培训数据科学家和从业人员如何在现实世界中处理偏见和公平问题。现有的资源大部分是学术性的，涵盖了机器学习培训和减少偏差的优化方面，使得从业者在整个现实世界的项目生命周期中没有全面的框架来做出决策。本教程旨在弥合研究和实践之间的差距，提供对算法公平性的深入探索，包括度量和定义，实际案例研究，数据偏差理解，偏差缓解和模型公平性审计使用 Aequitas 工具包。与会者将有能力参与关于偏见的对话，协助决策者了解备选方案和权衡，评估影响公平结果的项目范围，并根据模型预测确定行动和干预措施。他们还将学习识别队列、目标变量、评估指标，并为不同群体建立偏见和公平目标。此外，参与者将获得审计和减轻模型偏差的见解，并实施持续监测，以评估再培训需求。本教程针对研究人员和开发人员在现实世界中的算法决策系统中缺乏实用的培训材料、方法和工具的问题。通过这个实践教程的结论，与会者将精通导航偏差相关的问题，选择适当的度量标准，并应用偏差审计和缓解框架和工具，在现实世界的数据科学系统的知情设计决策。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Addressing+Bias+and+Fairness+in+Machine+Learning:+A+Practical+Guide+and+Hands-on+Tutorial)|0|
|[Causal Inference and Machine Learning in Practice: Use Cases for Product, Brand, Policy and Beyond](https://doi.org/10.1145/3580305.3599221)|JeongYoon Lee, Yifeng Wu, Keith Battocchi, Fabio Vera, Zhenyu Zhao, Totte Harinen, Jing Pan, Huigang Chen, Zeyu Zheng, Chu Wang, Yingfei Wang, Xinwei Ma|University of California, San Diego, San Diego, CA, USA; AirBnB, San Francisco, CA, USA; University of California, Berkeley, Berkeley, CA, USA; Uber Technologies, Inc., San Francisco, CA, USA; Meta, Los Angeles, CA, USA; Snap, Los Angeles, CA, USA; University of Washington, Seattle, WA, USA; Uber Technologies, Inc., Los Angeles, CA, USA; Tencent, Palo Alto, CA, USA; Amazon, Seattle, WA, USA; Microsoft Research, Cambridge, MA, USA|The increasing demand for data-driven decision-making has led to the rapid growth of machine learning applications in various industries. However, the ability to draw causal inferences from observational data remains a crucial challenge. In recent years, causal inference has emerged as a powerful tool for understanding the effects of interventions in complex systems. Combining causal inference with machine learning has the potential to provide a deeper understanding of the underlying mechanisms and to develop more effective solutions to real-world problems. This workshop aims to bring together researchers and practitioners from academia and industry to share their experiences and insights on applying causal inference and machine learning techniques to real-world problems in the areas of product, brand, policy, and beyond. The workshop welcomes original research that covers machine learning theory, deep learning, causal inference, and online learning. Additionally, the workshop encourages topics that address scalable system design, algorithm bias, and interpretability. Through keynote talks, panel discussions, and contributed talks and posters, the workshop will provide a forum for discussing the latest advances and challenges in applying causal inference and machine learning to real-world problems. The workshop will also offer opportunities for networking and collaboration among researchers and practitioners working in industry, government, and academia.|对数据驱动决策的需求日益增长，导致了机器学习应用在各个行业的迅速增长。然而，从观测数据中得出因果推论的能力仍然是一个关键的挑战。近年来，因果推理已经成为理解复杂系统干预效果的有力工具。将因果推理与机器学习相结合，有可能提供对潜在机制的更深入的理解，并为现实世界的问题找到更有效的解决方案。这个研讨会的目的是聚集来自学术界和工业界的研究人员和从业人员，分享他们的经验和见解，应用因果推理和机器学习技术的现实世界的问题，在产品，品牌，政策和以外的领域。研讨会欢迎包括机器学习理论、深度学习、因果推理和在线学习在内的原创性研究。此外，研讨会鼓励讨论可伸缩系统设计、算法偏差和可解释性的主题。通过主题演讲、小组讨论、贡献的演讲和海报，研讨会将提供一个论坛，讨论在将因果推理和机器学习应用于现实世界问题方面的最新进展和挑战。研讨会还将提供在工业、政府和学术界工作的研究人员和从业人员之间建立联系和开展合作的机会。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causal+Inference+and+Machine+Learning+in+Practice:+Use+Cases+for+Product,+Brand,+Policy+and+Beyond)|0|
|[Sketch-Based Anomaly Detection in Streaming Graphs](https://doi.org/10.1145/3580305.3599504)|Siddharth Bhatia, Mohit Wadhwa, Kenji Kawaguchi, Neil Shah, Philip S. Yu, Bryan Hooi||Given a stream of graph edges from a dynamic graph, how can we assign anomaly scores to edges and subgraphs in an online manner, for the purpose of detecting unusual behavior, using constant time and memory? For example, in intrusion detection, existing work seeks to detect either anomalous edges or anomalous subgraphs, but not both. In this paper, we first extend the count-min sketch data structure to a higher-order sketch. This higher-order sketch has the useful property of preserving the dense subgraph structure (dense subgraphs in the input turn into dense submatrices in the data structure). We then propose 4 online algorithms that utilize this enhanced data structure, which (a) detect both edge and graph anomalies; (b) process each edge and graph in constant memory and constant update time per newly arriving edge, and; (c) outperform state-of-the-art baselines on 4 real-world datasets. Our method is the first streaming approach that incorporates dense subgraph search to detect graph anomalies in constant memory and time.|给定一个动态图的图边流，我们如何以一种在线的方式为边和子图分配异常评分，以便使用恒定的时间和内存来检测异常行为？例如，在入侵检测中，现有的工作寻求检测异常边缘或异常子图，但不是两者兼而有之。在本文中，我们首先将 count-min 草图数据结构扩展到一个高阶草图。这种高阶素描具有保持稠密子图结构的有用性质(输入中的稠密子图变成数据结构中的稠密子矩阵)。然后，我们提出了4个在线算法，利用这种增强的数据结构，其中(a)检测边缘和图形异常; (b)处理每个边缘和图形的恒定记忆和每个新到达的边缘的恒定更新时间，以及(c)在4个真实世界的数据集优于最先进的基线。我们的方法是第一个流的方法，结合密集子图搜索检测图的异常在恒定的记忆和时间。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sketch-Based+Anomaly+Detection+in+Streaming+Graphs)|0|
|[On Hierarchical Disentanglement of Interactive Behaviors for Multimodal Spatiotemporal Data with Incompleteness](https://doi.org/10.1145/3580305.3599448)|Jiayi Chen, Aidong Zhang||Multimodal spatiotemporal data (MST) consists of multiple simultaneous spatiotemporal modalities that interact with each other in a dynamic manner. Due to the complexity of MST and the recent desire for the explainability of artificial intelligent systems, disentangled representation learning for MST (DisentMST) has become a significant task, which aims to learn disentangled representations that can expose the underlying spatial semantics, temporal dynamic patterns, and inter-modality interaction modes of the complex MST. One limitation of existing approaches is that they might fail to tolerate the real-world incomplete MST data, where missing information might break the cross-modal spatiotemporal dynamics and bring noise and ambiguity to the learning process. Another limitation is that no existing work systematically reveals the structure of different types of disentangled information. To tackle the two limitations, we define a novel two-level hierarchically structured disentanglement task for MST, which reveals informative and structured disentangled representations for MST as well as digests the real-world MST with incompleteness. We propose a new framework, BiDisentMST, which leverages Gaussian Processes and Graph Factorization on the latent space to achieve our purposes. The experimental results demonstrate the effectiveness of our proposed framework compared with baselines with respect to disentanglement and imputation results.|多模态时空数据(MST)由多个同时的时空模式组成，它们以动态的方式相互作用。由于 MST 的复杂性和对人工智能系统可解释性的需求，MST 的分离表示学习(DisentMST)已成为一项重要的任务，其目标是学习能够揭示复杂 MST 的潜在空间语义、时间动态模式和模态间交互模式的分离表示。现有方法的一个局限性是它们可能无法容忍现实世界中不完整的 MST 数据，其中缺失的信息可能会破坏跨模态时空动力学，并给学习过程带来噪声和模糊性。另一个限制是现有的工作没有系统地揭示不同类型的分离信息的结构。为了解决这两个局限性，我们定义了一个新的两级分层结构的 MST 解纠缠任务，它揭示了 MST 的信息化和结构化的解纠缠表示，并对现实世界的 MST 进行了不完全消化。我们提出了一个新的框架，BiDisentMST，它利用高斯过程和图因子分解的潜在空间，以实现我们的目的。实验结果表明，与基线相比，我们提出的框架在解缠和插补结果方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Hierarchical+Disentanglement+of+Interactive+Behaviors+for+Multimodal+Spatiotemporal+Data+with+Incompleteness)|0|
|[Multiplex Heterogeneous Graph Neural Network with Behavior Pattern Modeling](https://doi.org/10.1145/3580305.3599441)|Chaofan Fu, Guanjie Zheng, Chao Huang, Yanwei Yu, Junyu Dong||Heterogeneous graph neural networks have gained great popularity in tackling various network analysis tasks on heterogeneous network data. However, most existing works mainly focus on general heterogeneous networks, and assume that there is only one type of edge between two nodes, while ignoring the multiplex characteristics between multi-typed nodes in multiplex heterogeneous networks and the different importance of multiplex structures among nodes for node embedding. In addition, the over-smoothing issue of graph neural networks limits existing models to only capturing local structure signals but hardly learning the global relevant information of the network. To tackle these challenges, this work proposes a model called Behavior Pattern based Heterogeneous Graph Neural Network (BPHGNN) for multiplex heterogeneous network embedding. Specifically, BPHGNN can collaboratively learn node representations across different multiplex structures among nodes with adaptive importance learning from local and global perspectives in multiplex heterogeneous networks through depth behavior pattern aggregation and breadth behavior pattern aggregation. Extensive experiments on six real-world networks with various network analytical tasks demonstrate the significant superiority of BPHGNN against state-of-the-art approaches in terms of various evaluation metrics.|异构图形神经网络在处理异质网路数据的各种网络分析任务方面得到了广泛的应用。然而，现有的大多数研究主要集中在一般的异构网络上，假设两个节点之间只有一种类型的边，而忽略了多类型异构网络中多类型节点之间的多重特性以及节点之间的多重结构对于节点嵌入的不同重要性。此外，图神经网络的过度平滑问题限制了现有的模型只能捕获局部结构信号，而很难学习网络的全局相关信息。为了应对这些挑战，这项工作提出了一个叫做基于行为模式的异构图神经网络(bphGNN)的模型，用于多路异质网路嵌入。具体来说，BPHGNN 可以通过深度行为模式聚合和广度行为模式聚合，在多路异构网络中从局部和全局角度自适应重要性学习，协同学习不同多路结构之间的节点表示。在具有各种网络分析任务的六个现实世界网络上进行的大量实验表明，BPHGNN 在各种评估指标方面明显优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multiplex+Heterogeneous+Graph+Neural+Network+with+Behavior+Pattern+Modeling)|0|
|[Pyramid Graph Neural Network: A Graph Sampling and Filtering Approach for Multi-scale Disentangled Representations](https://doi.org/10.1145/3580305.3599478)|Haoyu Geng, Chao Chen, Yixuan He, Gang Zeng, Zhaobing Han, Hua Chai, Junchi Yan||Spectral methods for graph neural networks (GNNs) have achieved great success. Despite their success, many works have shown that existing approaches are mainly focused on low-frequency information which may not be pertinent to the task at hand. Recent efforts have been made to design new graph filters for wider frequency profiles, but it remains an open problem how to learn multi-scale disentangled node embeddings in the graph Fourier domain. In this paper, we propose a graph (signal) sampling and filtering framework, entitled Pyramid Graph Neural Network (PyGNN), which follows the Downsampling-Filtering-Upsampling-Decoding scheme. To be specific, we develop an ω-bandlimited downsampling approach to split input graph into subgraphs for the reduction of high-frequency components, then perform spectral graph filters on subgraphs to achieve node embeddings with different frequency bands, and propose a Laplacian smoothing-based upsampling approach to extrapolate the node embedding on subgraphs to the full set of vertices on the original graph. In the end, we add frequency-aware gated units to decode node embeddings of different frequencies for downstream tasks. Results on both homophilic and heterophilic graph datasets show its superiority over state-of-the-art methods.|图形神经网络(GNN)的谱方法已经取得了很大的成功。尽管取得了成功，但许多工作表明，现有方法主要侧重于低频率信息，这些信息可能与手头的任务无关。近年来，人们致力于设计更宽频谱的图形滤波器，但如何在图的傅里叶域学习多尺度分离节点嵌入仍然是一个悬而未决的问题。本文提出了一种基于下采样-过滤-上采样-解码的图(信号)采样与滤波框架——金字塔图神经网络(PyGNN)。具体来说，我们提出了一种 ω- 带限下采样方法，将输入图分解为子图以减少高频分量，然后对子图进行谱图滤波以实现不同频带的节点嵌入，并提出了一种基于拉普拉斯平滑的上采样方法将子图上的节点嵌入外推到原始图上的全部顶点集。最后，我们添加频率感知门控单元来解码下游任务中不同频率的节点嵌入。在同质和异质图数据集上的结果显示了其优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pyramid+Graph+Neural+Network:+A+Graph+Sampling+and+Filtering+Approach+for+Multi-scale+Disentangled+Representations)|0|
|[Detecting Interference in Online Controlled Experiments with Increasing Allocation](https://doi.org/10.1145/3580305.3599308)|Kevin Han, Shuangning Li, Jialiang Mao, Han Wu||In the past decade, the technology industry has adopted online controlled experiments (a.k.a. A/B testing) to guide business decisions. In practice, A/B tests are often implemented with increasing treatment allocation: the new treatment is gradually released to an increasing number of units through a sequence of randomized experiments. In scenarios such as experimenting in a social network setting or in a bipartite online marketplace, interference among units may exist, which can harm the validity of simple inference procedures. In this work, we introduce a widely applicable procedure to test for interference in A/B testing with increasing allocation. Our procedure can be implemented on top of an existing A/B testing platform with a separate flow and does not require a priori a specific interference mechanism. In particular, we introduce two permutation tests that are valid under different assumptions. Firstly, we introduce a general statistical test for interference requiring no additional assumption. Secondly, we introduce a testing procedure that is valid under a time fixed effect assumption. The testing procedure is of very low computational complexity, it is powerful, and it formalizes a heuristic algorithm implemented already in industry. We demonstrate the performance of the proposed testing procedure through simulations on synthetic data. Finally, we discuss one application at LinkedIn, where a screening step is implemented to detect potential interference in all their marketplace experiments with the proposed methods in the paper.|在过去的十年中，技术产业已经采用在线控制实验(又称 A/B 测试)来指导商业决策。在实践中，A/B 试验通常是随着治疗分配的增加而实施的: 通过一系列随机试验，新的治疗方法被逐渐释放到越来越多的单位。在诸如在社交网络环境中或在双边在线市场中进行试验的情况下，可能存在单位之间的干扰，这可能损害简单推理程序的有效性。在这项工作中，我们介绍了一个广泛适用的程序，以测试干扰的 A/B 测试与增加分配。我们的程序可以实现在一个现有的 A/B 测试平台上与一个单独的流程，不需要先验的具体干扰机制。特别地，我们引入了两个在不同假设下有效的置换检验。首先，我们介绍了一种不需要额外假设的干扰统计检验方法。其次，我们介绍了一个在时间固定效应假设下有效的测试程序。该测试过程计算复杂度低，功能强大，形成了一个已经在工业中实现的启发式算法。我们通过对合成数据的仿真验证了所提出的测试过程的性能。最后，我们讨论了 LinkedIn 的一个应用程序，其中使用本文提出的方法实现了一个筛选步骤来检测所有市场试验中的潜在干扰。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+Interference+in+Online+Controlled+Experiments+with+Increasing+Allocation)|0|
|[CLUR: Uncertainty Estimation for Few-Shot Text Classification with Contrastive Learning](https://doi.org/10.1145/3580305.3599276)|Jianfeng He, Xuchao Zhang, Shuo Lei, Abdulaziz Alhamadani, Fanglan Chen, Bei Xiao, ChangTien Lu||Few-shot text classification has extensive application where the sample collection is expensive or complicated. When the penalty for classification errors is high, such as early threat event detection with scarce data, we expect to know "whether we should trust the classification results or reexamine them.'' This paper investigates the Uncertainty Estimation for Few-shot Text Classification (UEFTC), an unexplored research area. Given limited samples, a UEFTC model predicts an uncertainty score for a classification result, which is the likelihood that the classification result is false. However, many traditional uncertainty estimation models in text classification are unsuitable for implementing a UEFTC model. These models require numerous training samples, whereas the few-shot setting in UEFTC only provides a few or just one support sample for each class in an episode. We propose Contrastive Learning from Uncertainty Relations (CLUR) to address UEFTC. CLUR can be trained with only one support sample for each class with the help of pseudo uncertainty scores. Unlike previous works that manually set the pseudo uncertainty scores, CLUR self-adaptively learns them using our proposed uncertainty relations. Specifically, we explore four model structures in CLUR to investigate the performance of three common-used contrastive learning components in UEFTC and find that two of the components are effective. Experiment results prove that CLUR outperforms six baselines on four datasets, including an improvement of 4.52% AUPR on an RCV1 dataset in a 5-way 1-shot setting. Our code and data split for UEFTC are in https://github.com/he159ok/CLUR_UncertaintyEst_FewShot_TextCls.|少镜头文本分类在样本收集昂贵或复杂的领域有着广泛的应用。当对分类错误的惩罚很高时，比如用稀缺数据进行早期威胁事件检测，我们希望知道“我们是应该相信分类结果还是应该重新检查它们。”本文研究了少镜头文本分类(UEFTC)的不确定性评估问题，这是一个尚未开发的研究领域。给定有限的样本，UEFTC 模型预测分类结果的不确定性评分，即分类结果可能是错误的。然而，文本分类中许多传统的不确定性估计模型不适合实现 UEFTC 模型。这些模型需要大量的训练样本，而 UEFTC 中的少数镜头设置只能为每一集中的每个班级提供少量或仅仅一个支持样本。我们提出从不确定关系中进行对比学习(CLUR)来解决 UEFTC 问题。通过伪不确定性分数的帮助，CLUR 可以在每个类只有一个支持样本的情况下进行训练。不像以前的工作，手动设置伪不确定性分数，CLUR 自适应地学习他们使用我们提出的不确定性关系。具体而言，我们探讨了 CLUR 中的四个模型结构，以考察 UEFTC 中三个常用的对比学习成分的表现，发现其中两个成分是有效的。实验结果表明，CLUR 在4个数据集上的性能优于6个基线，包括在5路单镜头设置下，RCV1数据集的 AUPR 提高了4.52% 。我们对 UEFTC 的代码和数据分割处于 https://github.com/he159ok/clur_uncertaintyest_fewshot_textcls。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CLUR:+Uncertainty+Estimation+for+Few-Shot+Text+Classification+with+Contrastive+Learning)|0|
|[Planning to Fairly Allocate: Probabilistic Fairness in the Restless Bandit Setting](https://doi.org/10.1145/3580305.3599467)|Christine Herlihy, Aviva Prins, Aravind Srinivasan, John P. Dickerson|University of Maryland|Restless and collapsing bandits are commonly used to model constrained resource allocation in settings featuring arms with action-dependent transition probabilities, such as allocating health interventions among patients [Whittle, 1988; Mate et al., 2020]. However, state-of-the-art Whittle-index-based approaches to this planning problem either do not consider fairness among arms, or incentivize fairness without guaranteeing it [Mate et al., 2021]. Additionally, their optimality guarantees only apply when arms are indexable and threshold-optimal. We demonstrate that the incorporation of hard fairness constraints necessitates the coupling of arms, which undermines the tractability, and by extension, indexability of the problem. We then introduce ProbFair, a probabilistically fair stationary policy that maximizes total expected reward and satisfies the budget constraint, while ensuring a strictly positive lower bound on the probability of being pulled at each timestep. We evaluate our algorithm on a real-world application, where interventions support continuous positive airway pressure (CPAP) therapy adherence among obstructive sleep apnea (OSA) patients, as well as simulations on a broader class of synthetic transition matrices.|不安分和崩溃的土匪通常用于在具有行动依赖性转换概率的武器的环境中建模有限的资源分配，例如在患者中分配卫生干预措施[ Whittle，1988; Mate 等，2020]。然而，基于惠特尔指数的最先进的方法来解决这个规划问题，要么不考虑武器之间的公平，要么在不保证公平的情况下激励公平[ Mate et al。 ，2021]。此外，它们的最优性保证只适用于武器是可索引和阈值最优的情况。我们证明了硬公平约束的加入使得武器的耦合成为必然，这破坏了问题的易处理性，进而也破坏了问题的可索引性。然后，我们引入了“概率公平”，这是一种概率公平的静态策略，它能使预期回报总额最大化并满足预算线，同时确保在每个时间步骤中被拉出的概率有一个严格的正下限。我们评估我们的算法在现实世界中的应用，其中干预支持连续式阳压唿吸机(CPAP)治疗在阻塞性睡眠唿吸暂停(OSA)患者中的依从性，以及在更广泛类别的合成转换矩阵上的模拟。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Planning+to+Fairly+Allocate:+Probabilistic+Fairness+in+the+Restless+Bandit+Setting)|0|
|[Similarity Preserving Adversarial Graph Contrastive Learning](https://doi.org/10.1145/3580305.3599503)|Yeonjun In, Kanghoon Yoon, Chanyoung Park|KAIST|Recent works demonstrate that GNN models are vulnerable to adversarial attacks, which refer to imperceptible perturbation on the graph structure and node features. Among various GNN models, graph contrastive learning (GCL) based methods specifically suffer from adversarial attacks due to their inherent design that highly depends on the self-supervision signals derived from the original graph, which however already contains noise when the graph is attacked. To achieve adversarial robustness against such attacks, existing methods adopt adversarial training (AT) to the GCL framework, which considers the attacked graph as an augmentation under the GCL framework. However, we find that existing adversarially trained GCL methods achieve robustness at the expense of not being able to preserve the node feature similarity. In this paper, we propose a similarity-preserving adversarial graph contrastive learning (SP-AGCL) framework that contrasts the clean graph with two auxiliary views of different properties (i.e., the node similarity-preserving view and the adversarial view). Extensive experiments demonstrate that SP-AGCL achieves a competitive performance on several downstream tasks, and shows its effectiveness in various scenarios, e.g., a network with adversarial attacks, noisy labels, and heterophilous neighbors. Our code is available at https://github.com/yeonjun-in/torch-SP-AGCL.|最近的工作表明，GNN 模型是脆弱的对手攻击，这是指不可察觉的扰动图结构和节点特征。在各种 GNN 模型中，基于图形对比学习(GCL)的方法由于其固有的设计，高度依赖于来自原始图形的自我监督信号，而这些信号在图形受到攻击时已经含有噪声，因此特别容易受到攻击。为了实现对抗这种攻击的鲁棒性，现有的方法对 GCL 框架采用了对抗训练(AT) ，该框架将被攻击图视为 GCL 框架下的一种增强。然而，我们发现现有的对抗训练的 GCL 方法在不能保持节点特征相似性的前提下达到了鲁棒性。本文提出了一个保持相似性的对抗图对比学习(SP-AGCL)框架，该框架将干净图与具有不同性质的两个辅助视图(即节点相似性保持视图和对抗视图)进行对比。广泛的实验表明，SP-AGCL 在几个下游任务上取得了有竞争力的性能，并且在多种情况下显示了其有效性，例如，一个具有对抗性攻击、噪声标签和异质邻居的网络。我们的代码可以在 https://github.com/yeonjun-in/torch-sp-agcl 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Similarity+Preserving+Adversarial+Graph+Contrastive+Learning)|0|
|[Fast and Accurate Dual-Way Streaming PARAFAC2 for Irregular Tensors - Algorithm and Application](https://doi.org/10.1145/3580305.3599342)|JunGi Jang, Jeongyoung Lee, Yongchan Park, U Kang|Seoul National University|How can we efficiently and accurately analyze an irregular tensor in a dual-way streaming setting where the sizes of two dimensions of the tensor increase over time? What types of anomalies are there in the dual-way streaming setting? An irregular tensor is a collection of matrices whose column lengths are the same while their row lengths are different. In a dual-way streaming setting, both new rows of existing matrices and new matrices arrive over time. PARAFAC2 decomposition is a crucial tool for analyzing irregular tensors. Although real-time analysis is necessary in the dual-way streaming, static PARAFAC2 decomposition methods fail to efficiently work in this setting since they perform PARAFAC2 decomposition for accumulated tensors whenever new data arrive. Existing streaming PARAFAC2 decomposition methods work in a limited setting and fail to handle new rows of matrices efficiently. In this paper, we propose Dash, an efficient and accurate PARAFAC2 decomposition method working in the dual-way streaming setting. When new data are given, Dash efficiently performs PARAFAC2 decomposition by carefully dividing the terms related to old and new data and avoiding naive computations involved with old data. Furthermore, applying a forgetting factor makes Dash follow recent movements. Extensive experiments show that Dash achieves up to 14.0x faster speed than existing PARAFAC2 decomposition methods for newly arrived data. We also provide discoveries for detecting anomalies in real-world datasets, including Subprime Mortgage Crisis and COVID-19.|我们如何才能有效和准确地分析一个不规则张量在双向流设置，其中张量的二维尺寸随着时间的推移增加？在双向流设置中有哪些类型的异常？不规则张量是列长相同而行长不同的矩阵集合。在双向流设置中，现有矩阵的新行和新矩阵都会随着时间的推移到达。PARAFAC2分解是分析不规则张量的重要工具。尽管实时分析在双向流中是必需的，但是静态 PARAFAC2分解方法在这种情况下无法有效工作，因为每当新数据到达时，它们都会对累积的张量执行 PARAFAC2分解。现有的流 PARAFAC2分解方法在有限的设置下工作，无法有效地处理新的矩阵行。本文提出了一种高效、准确的双向流设置 PARAFAC2分解方法 Dash。当给定新数据时，Dash 通过仔细划分与旧数据和新数据相关的术语并避免涉及旧数据的幼稚计算，有效地执行 PARAFAC2分解。此外，应用遗忘因子使达什跟随最近的动作。大量的实验表明，Dash 对新到达的数据的分解速度比现有的 PARAFAC2分解方法快14.0倍。我们还提供发现，以检测现实世界数据集中的异常，包括次贷危机和2019冠状病毒疾病。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+and+Accurate+Dual-Way+Streaming+PARAFAC2+for+Irregular+Tensors+-+Algorithm+and+Application)|0|
|[Predicting Information Pathways Across Online Communities](https://doi.org/10.1145/3580305.3599470)|Yiqiao Jin, YeonChang Lee, Kartik Sharma, Meng Ye, Karan Sikka, Ajay Divakaran, Srijan Kumar|Georgia Institute of Technology; SRI International|The problem of community-level information pathway prediction (CLIPP) aims at predicting the transmission trajectory of content across online communities. A successful solution to CLIPP holds significance as it facilitates the distribution of valuable information to a larger audience and prevents the proliferation of misinformation. Notably, solving CLIPP is non-trivial as inter-community relationships and influence are unknown, information spread is multi-modal, and new content and new communities appear over time. In this work, we address CLIPP by collecting large-scale, multi-modal datasets to examine the diffusion of online YouTube videos on Reddit. We analyze these datasets to construct community influence graphs (CIGs) and develop a novel dynamic graph framework, INPAC (Information Pathway Across Online Communities), which incorporates CIGs to capture the temporal variability and multi-modal nature of video propagation across communities. Experimental results in both warm-start and cold-start scenarios show that INPAC outperforms seven baselines in CLIPP.|社区层面的信息路径预测(CLIPP)问题旨在预测内容在网络社区之间的传播轨迹。CLIPP 的成功解决方案具有重要意义，因为它有助于向更多的受众传播有价值的信息，并防止错误信息的扩散。值得注意的是，解决 CLIPP 是非常重要的，因为社区之间的关系和影响是未知的，信息传播是多模式的，新的内容和新的社区随着时间的推移出现。在这项工作中，我们通过收集大规模的多模态数据集来检查在线 YouTube 视频在 Reddit 上的传播，从而解决 CLIPP 问题。我们分析这些数据集来构建社区影响图(CIGs) ，并开发一种新的动态图框架 INPAC (在线社区信息路径) ，其结合 CIGs 来捕获跨社区视频传播的时间变异性和多模态性质。在热启动和冷启动两种情况下的实验结果表明，INPAC 的性能优于 CLIPP 中的七个基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Predicting+Information+Pathways+Across+Online+Communities)|0|
|[Task Relation-aware Continual User Representation Learning](https://doi.org/10.1145/3580305.3599516)|Sein Kim, Namkyeong Lee, Donghyun Kim, MinChul Yang, Chanyoung Park|NAVER Corporation; KAIST|User modeling, which learns to represent users into a low-dimensional representation space based on their past behaviors, got a surge of interest from the industry for providing personalized services to users. Previous efforts in user modeling mainly focus on learning a task-specific user representation that is designed for a single task. However, since learning task-specific user representations for every task is infeasible, recent studies introduce the concept of universal user representation, which is a more generalized representation of a user that is relevant to a variety of tasks. Despite their effectiveness, existing approaches for learning universal user representations are impractical in real-world applications due to the data requirement, catastrophic forgetting and the limited learning capability for continually added tasks. In this paper, we propose a novel continual user representation learning method, called TERACON, whose learning capability is not limited as the number of learned tasks increases while capturing the relationship between the tasks. The main idea is to introduce an embedding for each task, i.e., task embedding, which is utilized to generate task-specific soft masks that not only allow the entire model parameters to be updated until the end of training sequence, but also facilitate the relationship between the tasks to be captured. Moreover, we introduce a novel knowledge retention module with pseudo-labeling strategy that successfully alleviates the long-standing problem of continual learning, i.e., catastrophic forgetting. Extensive experiments on public and proprietary real-world datasets demonstrate the superiority and practicality of TERACON. Our code is available at https://github.com/Sein-Kim/TERACON.|用户建模是根据用户过去的行为学习如何将用户表示成一个低维的表示空间，因此为用户提供个性化的服务引起了业界的极大兴趣。以前的用户建模工作主要集中在学习为单个任务设计的特定于任务的用户表示。然而，由于学习任务特定的每个任务的用户表示是不可行的，最近的研究引入了通用用户表示的概念，这是一个更广泛的用户表示相关的各种任务。尽管现有的学习通用用户表示的方法很有效，但是由于数据需求、灾难性遗忘以及对不断增加的任务的学习能力有限，这些方法在实际应用中是不切实际的。在本文中，我们提出了一种新的持续用户表征学习方法，称为 TERACON，它的学习能力不受任务数量增加的限制，同时捕捉任务之间的关系。其主要思想是为每个任务引入一个嵌入，即任务嵌入，用于生成任务特定的软掩码，不仅允许整个模型参数更新直到训练序列结束，而且有利于任务之间的关系被捕获。此外，我们还引入了一个新的知识保留模块，该模块采用伪标记策略，成功地解决了长期以来存在的连续学习问题，即灾难性遗忘问题。在公共和专有的真实世界数据集上的大量实验证明了 TERACON 的优越性和实用性。我们的代码可以在 https://github.com/sein-kim/teracon 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Task+Relation-aware+Continual+User+Representation+Learning)|0|
|[GraphSHA: Synthesizing Harder Samples for Class-Imbalanced Node Classification](https://doi.org/10.1145/3580305.3599374)|WenZhi Li, ChangDong Wang, Hui Xiong, JianHuang Lai|The Hong Kong University of Science and Technology (Guangzhou); Sun Yat-sen University|Class imbalance is the phenomenon that some classes have much fewer instances than others, which is ubiquitous in real-world graph-structured scenarios. Recent studies find that off-the-shelf Graph Neural Networks (GNNs) would under-represent minor class samples. We investigate this phenomenon and discover that the subspaces of minor classes being squeezed by those of the major ones in the latent space is the main cause of this failure. We are naturally inspired to enlarge the decision boundaries of minor classes and propose a general framework GraphSHA by Synthesizing HArder minor samples. Furthermore, to avoid the enlarged minor boundary violating the subspaces of neighbor classes, we also propose a module called SemiMixup to transmit enlarged boundary information to the interior of the minor classes while blocking information propagation from minor classes to neighbor classes. Empirically, GraphSHA shows its effectiveness in enlarging the decision boundaries of minor classes, as it outperforms various baseline methods in class-imbalanced node classification with different GNN backbone encoders over seven public benchmark datasets. Code is avilable at https://github.com/wenzhilics/GraphSHA.|类不平衡是一些类的实例比其他类少得多的现象，这种现象在真实世界的图形结构场景中普遍存在。最近的研究发现，现成的图形神经网络(GNN)会低估次要类样本。我们研究了这一现象，发现潜空间中次类的子空间被主类的子空间挤压是导致这一失败的主要原因。我们很自然地受到了扩大次类决策边界的启发，并通过综合 HArder 次样本提出了一个通用的 GraphSHA 框架。此外，为了避免扩大的次边界侵犯邻居类的子空间，我们还提出了一个称为 SemiMixup 的模块来传输扩大的边界信息到次类的内部，同时阻止信息从次类传播到邻居类。实验表明，GraphSHA 在扩大次类的决策边界方面是有效的，因为它在七个公共基准数据集上使用不同的 GNN 骨干编码器进行类不平衡节点分类时优于各种基准方法。代码可在 https://github.com/wenzhilics/graphsha 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphSHA:+Synthesizing+Harder+Samples+for+Class-Imbalanced+Node+Classification)|0|
|[Physics-Guided Discovery of Highly Nonlinear Parametric Partial Differential Equations](https://doi.org/10.1145/3580305.3599466)|Yingtao Luo, Qiang Liu, Yuntian Chen, Wenbo Hu, Tian Tian, Jun Zhu||Partial differential equations (PDEs) fitting scientific data can represent physical laws with explainable mechanisms for various mathematically-oriented subjects. The data-driven discovery of PDEs from scientific data thrives as a new attempt to model complex phenomena in nature, but the effectiveness of current practice is typically limited by the scarcity of data and the complexity of phenomena. Especially, the discovery of PDEs with highly nonlinear coefficients from low-quality data remains largely under-addressed. To deal with this challenge, we propose a novel physics-guided learning method, which can not only encode observation knowledge such as initial and boundary conditions but also incorporate the basic physical principles and laws to guide the model optimization. We empirically demonstrate that the proposed method is more robust against data noise and sparsity, and can reduce the estimation error by a large margin; moreover, for the first time we are able to discover PDEs with highly nonlinear coefficients. With the promising performance, the proposed method pushes forward the boundary of the PDEs that can be found by machine learning models for scientific discovery.|拟合科学数据的偏微分方程(PDE)可以表示各种数学导向学科的具有可解释机制的物理规律。数据驱动发现偏微分方程的科学数据蓬勃发展，作为一种新的尝试模拟自然界中的复杂现象，但目前的做法的有效性通常受到数据稀缺和现象复杂性的限制。特别是，从低质量数据中发现具有高度非线性系数的偏微分方程仍然是一个很大的问题。为了解决这一问题，我们提出了一种新的物理导向学习方法，它不仅可以对初始条件和边界条件等观测知识进行编码，而且可以结合基本的物理原理和规律来指导模型的优化。实验结果表明，该方法对数据噪声和稀疏性具有较强的鲁棒性，能够大幅度地减小估计误差，并且首次发现了具有高度非线性系数的偏微分方程。该方法具有良好的性能，为科学发现推进了机器学习模型所能找到的偏微分方程的边界。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Physics-Guided+Discovery+of+Highly+Nonlinear+Parametric+Partial+Differential+Equations)|0|
|[Online Fairness Auditing through Iterative Refinement](https://doi.org/10.1145/3580305.3599454)|Pranav Maneriker, Codi Burley, Srinivasan Parthasarathy||A sizable proportion of deployed machine learning models make their decisions in a black-box manner. Such decision-making procedures are susceptible to intrinsic biases, which has led to a call for accountability in deployed decision systems. In this work, we investigate mechanisms that help audit claimed mathematical guarantees of the fairness of such systems. We construct AVOIR, a system that reduces the number of observations required for the runtime monitoring of probabilistic assertions over fairness metrics specified on decision functions associated with black-box AI models. AVOIR provides an adaptive process that automates the inference of probabilistic guarantees associated with estimating a wide range of fairness metrics. In addition, AVOIR enables the exploration of fairness violations aligned with governance and regulatory requirements. We conduct case studies with fairness metrics on three different datasets and demonstrate how AVOIR can help detect and localize fairness violations and ameliorate the issues with faulty fairness metric design.|相当一部分已部署的机器学习模型以黑盒方式作出决策。这种决策程序容易产生内在的偏见，导致要求在已部署的决策系统中实行问责制。在这项工作中，我们研究的机制，有助于审计声称的数学保证，这些系统的公平性。我们构建了 AVOIR 系统，该系统减少了运行时监测与黑盒 AI 模型相关的决策函数中指定的公平性指标的概率断言所需的观测数量。AVOIR 提供了一个自适应过程，自动推断与估计广泛的公平性度量相关的概率保证。此外，AVOIR 还支持探索与治理和监管需求相一致的违反公平性的行为。我们在三个不同的数据集上进行了公平性度量的案例研究，并演示了 AVOIR 如何帮助检测和定位违反公平性的行为，以及如何改善错误公平性度量设计的问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+Fairness+Auditing+through+Iterative+Refinement)|0|
|[Online Level-wise Hierarchical Clustering](https://doi.org/10.1145/3580305.3599455)|Nicholas Monath, Manzil Zaheer, Andrew McCallum||Online hierarchical clustering algorithms, compared to their scalable batch setting counterparts, typically provide more limited accuracy and efficiency performance. Yet, when data is incrementally arriving, a crucial setting in many clustering applications (e.g., entity resolution and concept discovery), these batch setting algorithms do not apply. This paper presents a family of new algorithms for online hierarchical clustering that combine high quality trees and fast per-point insertion time--made possible through a limited number of parallel non-greedy tree re-arrangements. We analyze our methods under assumptions about the data and the separability of clusters. Empirically, we find that our proposed algorithms yield state-of-the-art results in hierarchical clustering dendrogram purity and in building compressed prototypes for a k-nearest representative classifier.|在线分层聚类算法与其可扩展的批处理设置算法相比，通常提供更有限的准确性和效率性能。然而，当数据以递增方式到达时，这是许多集群应用程序(例如，实体解析和概念发现)中的关键设置，这些批处理设置算法并不适用。本文提出了一系列新的在线层次聚类算法，它们结合了高质量的树和快速的每点插入时间——通过有限数量的并行非贪婪树重排实现。我们分析了我们的方法在假设的数据和集群的可分性。经验上，我们发现我们提出的算法在分层聚类树状图纯度和建立 k- 最近代表性分类器的压缩原型方面取得了最先进的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+Level-wise+Hierarchical+Clustering)|0|
|[Cracking White-box DNN Watermarks via Invariant Neuron Transforms](https://doi.org/10.1145/3580305.3599291)|Xudong Pan, Mi Zhang, Yifan Yan, Yining Wang, Min Yang|; Fudan University|Recently, how to protect the Intellectual Property (IP) of deep neural networks (DNN) becomes a major concern for the AI industry. To combat potential model piracy, recent works explore various watermarking strategies to embed secret identity messages into the prediction behaviors or the internals (e.g., weights and neuron activation) of the target model. Sacrificing less functionality and involving more knowledge about the target model, the latter branch of watermarking schemes (i.e., white-box model watermarking) is claimed to be accurate, credible and secure against most known watermark removal attacks, with emerging research efforts and applications in the industry. In this paper, we present the first effective removal attack which cracks almost all the existing white-box watermarking schemes with provably no performance overhead and no required prior knowledge. By analyzing these IP protection mechanisms at the granularity of neurons, we for the first time discover their common dependence on a set of fragile features of a local neuron group, all of which can be arbitrarily tampered by our proposed chain of invariant neuron transforms. On $9$ state-of-the-art white-box watermarking schemes and a broad set of industry-level DNN architectures, our attack for the first time reduces the embedded identity message in the protected models to be almost random. Meanwhile, unlike known removal attacks, our attack requires no prior knowledge on the training data distribution or the adopted watermark algorithms, and leaves model functionality intact.|近年来，如何保护深层神经网络(DNN)的知识产权成为人工智能产业关注的主要问题。为了打击潜在的盗版模型，最近的研究探索了各种水印策略，将秘密身份信息嵌入到目标模型的预测行为或内部(如权重和神经元激活)中。水印技术的后一个分支(即白盒模型水印)牺牲了较少的功能，涉及更多关于目标模型的知识，被认为是准确、可靠和安全的，能够抵御大多数已知的水印去除攻击，并且在业界中得到了新的研究和应用。在本文中，我们提出了第一个有效的移除攻击，这个攻击破坏了几乎所有现有的白盒水印方案，并且可以证明没有性能开销和不需要先验知识。通过分析神经元粒度上的这些 IP 保护机制，我们首次发现它们对局部神经元群的一组脆弱特征的共同依赖性，所有这些特征都可以被我们提出的不变神经元变换链任意篡改。在9美元的国家最先进的白盒水印方案和广泛的行业级 DNN 架构，我们的攻击第一次减少嵌入的身份信息在受保护的模型几乎是随机的。同时，与已知的移除攻击不同，我们的攻击不需要关于训练数据分布或所采用的水印算法的先验知识，并且保留了模型的功能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cracking+White-box+DNN+Watermarks+via+Invariant+Neuron+Transforms)|0|
|[Graph Neural Bandits](https://doi.org/10.1145/3580305.3599371)|Yunzhe Qi, Yikun Ban, Jingrui He|University of Illinois, Urbana Champaign; University of Illinois at Urbana-Champaign|Contextual bandits aim to choose the optimal arm with the highest reward out of a set of candidates based on their contextual information, and various bandit algorithms have been applied to personalized recommendation due to their ability of solving the exploitation-exploration dilemma. Motivated by online recommendation scenarios, in this paper, we propose a framework named Graph Neural Bandits (GNB) to leverage the collaborative nature among users empowered by graph neural networks (GNNs). Instead of estimating rigid user clusters, we model the "fine-grained'' collaborative effects through estimated user graphs in terms of exploitation and exploration individually. Then, to refine the recommendation strategy, we utilize separate GNN-based models on estimated user graphs for exploitation and adaptive exploration. Theoretical analysis and experimental results on multiple real data sets in comparison with state-of-the-art baselines are provided to demonstrate the effectiveness of our proposed framework.|关联强盗的目标是根据候选人的关联信息从一组候选人中选择报酬最高的最优组合，各种强盗算法因其解决开发-探索两难问题的能力而被应用于个性化推荐。受在线推荐场景的启发，本文提出了一种基于图形神经网络的用户协作框架 GNB。我们没有对刚性用户集群进行估计，而是根据开发和探索的不同，通过估计的用户图对“细粒度”的协作效果进行建模。然后，为了完善推荐策略，我们利用基于 GNN 的分离模型对估计用户图进行开发和自适应探索。通过对多个实际数据集的理论分析和实验结果与最新基线的比较，证明了我们提出的框架的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Neural+Bandits)|0|
|[Source-Free Domain Adaptation with Temporal Imputation for Time Series Data](https://doi.org/10.1145/3580305.3599507)|Mohamed Ragab, Emadeldeen Eldele, Min Wu, ChuanSheng Foo, Xiaoli Li, Zhenghua Chen|Institute for Infocomm Research, Agency for Science Technology and Research (A*STAR); Center for Frontier AI Research, Agency for Science and Technology and Research (A*STAR)|Source-free domain adaptation (SFDA) aims to adapt a pretrained model from a labeled source domain to an unlabeled target domain without access to the source domain data, preserving source domain privacy. Despite its prevalence in visual applications, SFDA is largely unexplored in time series applications. The existing SFDA methods that are mainly designed for visual applications may fail to handle the temporal dynamics in time series, leading to impaired adaptation performance. To address this challenge, this paper presents a simple yet effective approach for source-free domain adaptation on time series data, namely MAsk and imPUte (MAPU). First, to capture temporal information of the source domain, our method performs random masking on the time series signals while leveraging a novel temporal imputer to recover the original signal from a masked version in the embedding space. Second, in the adaptation step, the imputer network is leveraged to guide the target model to produce target features that are temporally consistent with the source features. To this end, our MAPU can explicitly account for temporal dependency during the adaptation while avoiding the imputation in the noisy input space. Our method is the first to handle temporal consistency in SFDA for time series data and can be seamlessly equipped with other existing SFDA methods. Extensive experiments conducted on three real-world time series datasets demonstrate that our MAPU achieves significant performance gain over existing methods. Our code is available at \url{https://github.com/mohamedr002/MAPU_SFDA_TS}.|无源域适应(SFDA)的目标是在不访问源域数据的情况下，将预先训练好的模型从标记的源域适应到未标记的目标域，从而保护源域的隐私。尽管 SFDA 在视觉应用方面很流行，但在时间序列应用方面却很大程度上没有得到探索。现有的 SFDA 方法主要是为视觉应用而设计的，可能无法处理时间序列中的时间动态，导致适应性能受损。为了解决这一问题，本文提出了一种简单而有效的时间序列数据无源域自适应方法，即 MAsk 和 imPUte (MAPU)。首先，为了获取源域的时间信息，该方法对时间序列信号进行随机掩蔽，同时利用一种新的时间计算机在嵌入空间中从掩蔽版本中恢复原始信号。其次，在适应步骤中，利用计算机网络引导目标模型产生与源特征在时间上一致的目标特征。为此，我们的 MAPU 可以明确地说明在适应期间的时间依赖性，同时避免了在噪声输入空间的插补。我们的方法是第一个处理时间序列数据时间一致性的 SFDA 方法，可以与其他现有的 SFDA 方法无缝配备。在三个实际时间序列数据集上进行的大量实验表明，我们的 MAPU 比现有的方法获得了显著的性能提高。我们的代码可以在 url { https://github.com/mohamedr002/mapu_sfda_ts }找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Source-Free+Domain+Adaptation+with+Temporal+Imputation+for+Time+Series+Data)|0|
|[Causal Effect Estimation on Hierarchical Spatial Graph Data](https://doi.org/10.1145/3580305.3599269)|Koh Takeuchi, Ryo Nishida, Hisashi Kashima, Masaki Onishi||Estimating individual treatment effects from observational data is a fundamental problem in causal inference. To accurately estimate treatment effects in the spatial domain, we need to address certain aspects such as how to use the spatial coordinates of covariates and treatments and how the covariates and the treatments interact spatially. We introduce a new problem of predicting treatment effects on time series outcomes from spatial graph data with a hierarchical structure. To address this problem, we propose a spatial intervention neural network (SINet) that leverages the hierarchical structure of spatial graphs to learn a rich representation of the covariates and the treatments and exploits this representation to predict a time series of treatment outcome. Using a multi-agent simulator, we synthesized a crowd movement guidance dataset and conduct experiments to estimate the conditional average treatment effect, where we considered the initial locations of the crowds as covariates, route guidance as a treatment, and number of agents reaching a goal at each time stamp as the outcome. We employed state-of-the-art spatio-temporal graph neural networks and neural network-based causal inference methods as baselines, and show that our proposed method outperformed baselines both quantitatively and qualitatively.|根据观测数据估计个体治疗效果是因果推断中的一个基本问题。为了准确地估计空间域中的治疗效果，我们需要解决某些方面的问题，例如如何使用协变量和治疗的空间坐标，以及协变量和治疗如何在空间上相互作用。我们提出了一个新的问题，预测治疗效果的时间序列结果的空间图数据与层次结构。为了解决这个问题，我们提出了一种空间干预神经网络(SINet) ，它利用空间图的层次结构来学习协变量和治疗的丰富表示，并利用这种表示来预测治疗结果的时间序列。使用多智能体模拟器，我们合成了一个人群运动指导数据集，并进行实验来估计条件平均治疗效果，其中我们考虑了人群的初始位置作为协变量，路径指导作为治疗，以及在每个时间戳达到目标的代理人数量作为结果。我们采用了最先进的时空图神经网络和基于神经网络的因果推理方法作为基线，并表明我们提出的方法在定量和定性上都优于基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causal+Effect+Estimation+on+Hierarchical+Spatial+Graph+Data)|0|
|[Networked Time Series Imputation via Position-aware Graph Enhanced Variational Autoencoders](https://doi.org/10.1145/3580305.3599444)|Dingsu Wang, Yuchen Yan, Ruizhong Qiu, Yada Zhu, Kaiyu Guan, Andrew Margenot, Hanghang Tong|IBM Research; University of Illinois at Urbana-Champaign|Multivariate time series (MTS) imputation is a widely studied problem in recent years. Existing methods can be divided into two main groups, including (1) deep recurrent or generative models that primarily focus on time series features, and (2) graph neural networks (GNNs) based models that utilize the topological information from the inherent graph structure of MTS as relational inductive bias for imputation. Nevertheless, these methods either neglect topological information or assume the graph structure is fixed and accurately known. Thus, they fail to fully utilize the graph dynamics for precise imputation in more challenging MTS data such as networked time series (NTS), where the underlying graph is constantly changing and might have missing edges. In this paper, we propose a novel approach to overcome these limitations. First, we define the problem of imputation over NTS which contains missing values in both node time series features and graph structures. Then, we design a new model named PoGeVon which leverages variational autoencoder (VAE) to predict missing values over both node time series features and graph structures. In particular, we propose a new node position embedding based on random walk with restart (RWR) in the encoder with provable higher expressive power compared with message-passing based graph neural networks (GNNs). We further design a decoder with 3-stage predictions from the perspective of multi-task learning to impute missing values in both time series and graph structures reciprocally. Experiment results demonstrate the effectiveness of our model over baselines.|多变量时间序列(MTS)插补是近年来研究较多的一个问题。现有的方法可以分为两大类，包括(1)主要关注时间序列特征的深度递归或生成模型，和(2)基于图神经网络(GNN)的模型，这些模型利用 MTS 固有图结构的拓扑信息作为关系归纳偏差进行插补。然而，这些方法或者忽略了拓扑信息，或者假设图的结构是固定的并且已知的。因此，他们未能充分利用图动力学精确插补更具挑战性的 MTS 数据，如网络时间序列(NTS) ，其中底层图是不断变化的，可能有缺失的边。在本文中，我们提出了一种新的方法来克服这些限制。首先，我们定义了包含节点时间序列特征和图结构缺失值的 NTS 插补问题。然后，我们设计了一个新的模型 PoGeVon，它利用变分自动编码器(VAE)来预测节点时间序列特征和图结构上的缺失值。特别地，我们提出了一种新的基于重启随机游走(RWR)的节点位置嵌入编码器，与基于消息传递的图形神经网络(GNN)相比，具有可证明的更高的表达能力。我们进一步从多任务学习的角度设计了一个具有三阶段预测的解码器来相互推算时间序列和图结构中的缺失值。实验结果证明了该模型在基线上的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Networked+Time+Series+Imputation+via+Position-aware+Graph+Enhanced+Variational+Autoencoders)|0|
|[Incremental Causal Graph Learning for Online Root Cause Analysis](https://doi.org/10.1145/3580305.3599392)|Dongjie Wang, Zhengzhang Chen, Yanjie Fu, Yanchi Liu, Haifeng Chen|; University of Central Florida|The task of root cause analysis (RCA) is to identify the root causes of system faults/failures by analyzing system monitoring data. Efficient RCA can greatly accelerate system failure recovery and mitigate system damages or financial losses. However, previous research has mostly focused on developing offline RCA algorithms, which often require manually initiating the RCA process, a significant amount of time and data to train a robust model, and then being retrained from scratch for a new system fault. In this paper, we propose CORAL, a novel online RCA framework that can automatically trigger the RCA process and incrementally update the RCA model. CORAL consists of Trigger Point Detection, Incremental Disentangled Causal Graph Learning, and Network Propagation-based Root Cause Localization. The Trigger Point Detection component aims to detect system state transitions automatically and in near-real-time. To achieve this, we develop an online trigger point detection approach based on multivariate singular spectrum analysis and cumulative sum statistics. To efficiently update the RCA model, we propose an incremental disentangled causal graph learning approach to decouple the state-invariant and state-dependent information. After that, CORAL applies a random walk with restarts to the updated causal graph to accurately identify root causes. The online RCA process terminates when the causal graph and the generated root cause list converge. Extensive experiments on three real-world datasets demonstrate the effectiveness and superiority of the proposed framework.|根本原因分析的任务是透过分析系统监察数据，找出系统故障/失效的根本原因。有效的 RCA 可以大大加速系统故障恢复，减轻系统损坏或财务损失。然而，以往的研究主要集中在开发离线 RCA 算法，这往往需要人工启动 RCA 过程，需要大量的时间和数据来训练鲁棒模型，然后从头开始对新的系统故障进行再训练。在本文中，我们提出了一个新的在线 RCA 框架 CORAL，它可以自动触发 RCA 过程，并逐步更新 RCA 模型。CORAL 包括触发点检测、增量分离因果图学习和基于网络传播的根源定位。触发点检测组件的目的是自动检测系统状态转换并接近实时。为此，提出了一种基于多元奇异谱分析和累积和统计量的在线触发点检测方法。为了有效地更新 RCA 模型，我们提出了一种增量式解纠缠因果图学习方法来解耦状态不变和状态相关的信息。之后，CORAL 对更新的因果图应用随机游走并重新启动，以准确识别根本原因。当因果图和生成的根原因列表收敛时，在线 RCA 进程终止。在三个实际数据集上的大量实验表明了该框架的有效性和优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Incremental+Causal+Graph+Learning+for+Online+Root+Cause+Analysis)|0|
|[Treatment Effect Estimation with Adjustment Feature Selection](https://doi.org/10.1145/3580305.3599531)|Haotian Wang, Kun Kuang, Haoang Chi, Longqi Yang, Mingyang Geng, Wanrong Huang, Wenjing Yang||In causal inference, it is common to select a subset of observed covariates, named the adjustment features, to be adjusted for estimating the treatment effect. For real-world applications, the abundant covariates are usually observed, which contain extra variables partially correlating to the treatment (treatment-only variables, e.g., instrumental variables) or the outcome (outcome-only variables, e.g., precision variables) besides the confounders (variables that affect both the treatment and outcome). In principle, unbiased treatment effect estimation is achieved once the adjustment features contain all the confounders. However, the performance of empirical estimations varies a lot with different extra variables. To solve this issue, variable separation/selection for treatment effect estimation has received growing attention when the extra variables contain instrumental variables and precision variables. In this paper, assuming no mediator variables exist, we consider a more general setting by allowing for the existence of post-treatment and post-outcome variables rather than instrumental and precision variables in observed covariates. Our target is to separate the treatment-only variables from the adjustment features. To this end, we establish a metric named Optimal Adjustment Features(OAF), which empirically measures the asymptotic variance of the estimation. Theoretically, we show that our OAF metric is minimized if and only if adjustment features consist of the confounders and outcome-only variables, i.e., the treatment-only variables are perfectly separated. As optimizing the OAF metric is a combinatorial optimization problem, we introduce Reinforcement Learning (RL) and adopt the policy gradient to search for the optimal adjustment set. Empirical results on both synthetic and real-world datasets demonstrate that (a) our method successfully searches the optimal adjustment features and (b) the searched adjustment features achieve a more precise estimation of the treatment effect.|在因果推断中，通常选择一个观察到的协变量子集，命名为调整特征，进行调整以估计治疗效果。对于真实世界的应用，通常观察到丰富的协变量，其包含与治疗(仅治疗变量，例如工具变量)或结果(仅结果变量，例如精度变量)部分相关的额外变量除了混杂因素(影响治疗和结果的变量)。原则上，一旦调整特征包含所有的混杂因素，就可以实现无偏的治疗效果估计。然而，不同的额外变量对经验估计的表现有很大的影响。为了解决这个问题，当额外变量包含工具变量和精度变量时，用于治疗效果估计的变量分离/选择越来越受到关注。在本文中，假设不存在中介变量，我们考虑一个更一般的设置，允许存在治疗后和结果后变量，而不是观察到的协变量中的工具和精度变量。我们的目标是从调整特征中分离出仅用于治疗的变量。为此，我们建立了一个称为最优平差特征(OAF)的度量，它经验地测量估计的渐近方差。理论上，我们表明，我们的 OAF 度量是最小化的，当且仅当调整特征由混杂因素和仅结果变量组成，即，仅治疗变量是完全分离的。由于优化经营成本调整幅度是一个组合优化问题，我们引入强化学习，并采用政策梯度来寻找最佳的调整集合。在合成数据集和实际数据集上的实验结果表明: (a)我们的方法成功地搜索到了最优的平差特征; (b)搜索到的平差特征实现了对治疗效果的更精确的估计。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Treatment+Effect+Estimation+with+Adjustment+Feature+Selection)|0|
|[Adversarial Constrained Bidding via Minimax Regret Optimization with Causality-Aware Reinforcement Learning](https://doi.org/10.1145/3580305.3599254)|Haozhe Wang, Chao Du, Panyan Fang, Li He, Liang Wang, Bo Zheng|Alibaba Group|The proliferation of the Internet has led to the emergence of online advertising, driven by the mechanics of online auctions. In these repeated auctions, software agents participate on behalf of aggregated advertisers to optimize for their long-term utility. To fulfill the diverse demands, bidding strategies are employed to optimize advertising objectives subject to different spending constraints. Existing approaches on constrained bidding typically rely on i.i.d. train and test conditions, which contradicts the adversarial nature of online ad markets where different parties possess potentially conflicting objectives. In this regard, we explore the problem of constrained bidding in adversarial bidding environments, which assumes no knowledge about the adversarial factors. Instead of relying on the i.i.d. assumption, our insight is to align the train distribution of environments with the potential test distribution meanwhile minimizing policy regret. Based on this insight, we propose a practical Minimax Regret Optimization (MiRO) approach that interleaves between a teacher finding adversarial environments for tutoring and a learner meta-learning its policy over the given distribution of environments. In addition, we pioneer to incorporate expert demonstrations for learning bidding strategies. Through a causality-aware policy design, we improve upon MiRO by distilling knowledge from the experts. Extensive experiments on both industrial data and synthetic data show that our method, MiRO with Causality-aware reinforcement Learning (MiROCL), outperforms prior methods by over 30%.|互联网的扩散导致了在线广告的出现，这是由在线拍卖的机制所驱动的。在这些重复的拍卖中，软件代理商代表广告主集合参与，以优化他们的长期效用。为了满足不同的需求，投标策略被用来优化受不同支出约束的广告目标。现有的限制性投标方法通常依赖于身份证培训和测试条件，这与在线广告市场的对抗性质相矛盾，因为在线广告市场中，不同的当事人拥有潜在的相互冲突的目标。在这方面，我们探讨了在不考虑竞争因素的情况下，在竞争性投标环境下的约束投标问题。我们的洞察力不是依赖于内部识别假设，而是使环境的列车分布与潜在的测试分布保持一致，同时最大限度地减少政策遗憾。基于这种观点，我们提出了一种实用的极大极小遗憾优化(Miniax Regret Optimation，MiRO)方法，该方法在教师寻找对抗性的辅导环境和学习者元学习策略之间进行交叉。此外，我们率先采用专家演示学习投标策略。通过一个因果关系感知策略设计，我们从专家那里提取知识来改进 MiRO。对工业数据和合成数据的大量实验表明，我们的方法，带有因果感知强化学习(miROCL)的 miRO，比之前的方法性能高出30% 以上。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adversarial+Constrained+Bidding+via+Minimax+Regret+Optimization+with+Causality-Aware+Reinforcement+Learning)|0|
|[Efficient Sparse Linear Bandits under High Dimensional Data](https://doi.org/10.1145/3580305.3599329)|Xue Wang, Mike Mingcheng Wei, Tao Yao||We propose a computationally efficient Lasso Random Project Bandit (LRP-Bandit) algorithm for sparse linear bandit problems under high-dimensional settings with limited samples. LRP-Bandit bridges Lasso and Random Projection as feature selection and dimension reduction techniques to alleviate the computational complexity and improve the regret performance. We demonstrate that for the total feature dimension d, the significant feature dimension s, and the sample size T, the expected cumulative regret under LRP-Bandit is upper bounded by Õ (T 2 over 3 s 3 over 2 log 7 over 6 d), where Õ suppresses the logarithmic dependence on T. Further, we show that when available samples are larger than a problem-dependent threshold, the regret upper bound for LRP-Bandit can be further improved to Õ (s√T log d). These regret upper bounds on T for both data-poor and data-rich regimes match the theoretical minimax lower bounds up to logarithmic factors. Through experiments, we show that LRP-Bandit is computationally efficient and outperforms other benchmarks on the expected cumulative regret.|针对有限样本条件下高维环境下的稀疏线性匪问题，提出了一种计算效率较高的套索随机投影匪(LRP-Bandit)算法。LRP-Bandit 桥接套索和随机投影作为特征选择和维度减化技术，以减轻计算复杂性和提高后悔性能。我们证明了对于总特征维数 d，显著特征维数 s 和样本大小 T，LRP-Bandit 下的预期累积遗憾上界为 Õ (T2/3s3/2log7/6d) ，其中 Õ 抑制了对 T 的对数依赖性。此外，我们还证明了当可用样本大于问题相关阈值时，LRP-Bandit 的遗憾上界可以进一步改进为 Õ (s √ T log d)。这些遗憾的上界 T 的数据贫乏和数据丰富的制度匹配的理论极大极小下界直到对数因子。实验结果表明，LRP-Bandit 算法具有较高的计算效率，在预期累积遗憾方面优于其他基准。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Sparse+Linear+Bandits+under+High+Dimensional+Data)|0|
|[MicroscopeSketch: Accurate Sliding Estimation Using Adaptive Zooming](https://doi.org/10.1145/3580305.3599432)|Yuhan Wu, Shiqi Jiang, Siyuan Dong, Zheng Zhong, Jiale Chen, Yutong Hu, Tong Yang, Steve Uhlig, Bin Cui||High-accuracy real-time data stream estimations are critical for various applications, and sliding-window-based techniques have attracted wide attention. However, existing solutions struggle to achieve high accuracy, generality, and low memory usage simultaneously. To overcome these limitations, we present MicroscopeSketch, a high-accuracy sketch framework. Our key technique, called adaptive zooming, dynamically adjusts the granularity of counters to maximize accuracy while minimizing memory usage. By applying MicroscopeSketch to three specific tasks---frequency estimation, top-k frequent items discovery, and top-k heavy changes identification-we demonstrate substantial improvements over existing methods, reducing errors by roughly 4 times for frequency estimation and 3 times for identifying top-k items. The relevant source code is available in a GitHub repository.|高精度的实时数据流估计是各种应用的关键，基于滑动窗口的数据流估计技术已经引起了人们的广泛关注。然而，现有的解决方案难以同时实现高精度、通用性和低内存使用率。为了克服这些限制，我们提出了一个高精度的草图框架 MicrocopeSketch。我们的关键技术，称为自适应缩放，动态调整计数器的粒度，以最大限度地提高准确性，同时最大限度地减少内存使用。通过将 Microscope Sketch 应用于三个特定的任务——频率估计、 top-k 频繁项目发现和 top-k 重大变化识别——我们证明了对现有方法的实质性改进，频率估计的误差减少了大约4倍，识别 top-k 项目的误差减少了3倍。相关的源代码可以在 GitHub 存储库中找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MicroscopeSketch:+Accurate+Sliding+Estimation+Using+Adaptive+Zooming)|0|
|[Learning Behavior-oriented Knowledge Tracing](https://doi.org/10.1145/3580305.3599407)|Bihan Xu, Zhenya Huang, Jiayu Liu, Shuanghong Shen, Qi Liu, Enhong Chen, Jinze Wu, Shijin Wang|iFLYTEK Research, iFLYTEK Co., Ltd; School of Computer Science and Technology, University of Science and Technology of China; School of Data Science, University of Science and Technology of China; Anhui Province Key Laboratory of Big Data Analysis and Application, University of Science and Technology of China; State Key Laboratory of Cognitive Intelligence|Exploring how learners' knowledge states evolve during the learning activities is a critical task in online learning systems, which can facilitate personalized services downstream, such as course recommendation. Most of existing methods have devoted great efforts to analyzing learners' knowledge states according to their responses (i.e., right or wrong) to different questions. However, the significant effect of learners' learning behaviors (e.g., answering speed, the number of attempts) is omitted, which can reflect their knowledge acquisition deeper and ensure the reliability of the response. In this paper, we propose a Learning Behavior-oriented Knowledge Tracing (LBKT) model, with the goal of explicitly exploring the learning behavior effects on learners' knowledge states. Specifically, we first analyze and summarize several dominated learning behaviors including Speed, Attempts and Hints in the learning process. As the characteristics of different learning behaviors vary greatly, we separately estimate their various effects on learners' knowledge acquisition in a quantitative manner. Then, considering that different learning behaviors are closely dependent with each other, we assess the fused effect of multiple learning behaviors by capturing their complex dependent patterns. Finally, we integrate the forgetting factor with learners' knowledge acquisition to comprehensively update their changing knowledge states in learning. Extensive experimental results on several public datasets demonstrate that our model generates better performance prediction for learners against existing methods. Moreover, LBKT shows good interpretability in tracking learners' knowledge state by incorporating the learning behavior effects. Our codes are available at https://github.com/xbh0720/LBKT.|研究学习者的知识状态在学习活动中如何演变是在线学习系统的一个关键任务，它可以促进个性化的下游服务，如课程推荐。现有的大多数方法都致力于根据学习者对不同问题的反应(即对或错)来分析学习者的知识状态。然而，学习者的学习行为(如答题速度、尝试次数)的显著影响被忽略了，这可以更深刻地反映他们的知识获得，并保证反应的可靠性。本文提出了一个面向学习行为的知识追踪模型(LBKT) ，旨在明确探讨学习行为对学习者知识状态的影响。具体来说，我们首先分析和总结了学习过程中的几种主导性学习行为，包括速度、尝试和提示。由于不同学习行为的特点差异很大，我们分别以定量的方式估计了它们对学习者知识习得的不同影响。然后，考虑到不同学习行为之间的相互依赖性，我们通过捕捉多个学习行为的复杂依赖模式来评估它们的融合效应。最后，将遗忘因素与学习者的知识习得相结合，全面更新学习过程中不断变化的知识状态。在几个公共数据集上的大量实验结果表明，与现有的方法相比，我们的模型能够为学习者提供更好的性能预测。LBKT 结合学习行为效应对学习者的知识状态进行跟踪，表现出良好的可解释性。我们的代码可以在 https://github.com/xbh0720/lbkt 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Behavior-oriented+Knowledge+Tracing)|0|
|[MimoSketch: A Framework to Mine Item Frequency on Multiple Nodes with Sketches](https://doi.org/10.1145/3580305.3599433)|Yuchen Xu, Wenfei Wu, Bohan Zhao, Tong Yang, Yikai Zhao||We abstract a MIMO scenario in distributed data stream mining, where a stream of multiple items is mined by multiple nodes. We design a framework named MimoSketch for the MIMO-specific scenario, which improves the fundamental mining task of item frequency estimation. MimoSketch consists of an algorithm design and a policy to schedule items to nodes. MimoSketch's algorithm applies random counting to preserve a mathematically proven unbiasedness property, which makes it friendly to the aggregate query on multiple nodes; its memory layout is dynamically adaptive to the runtime item size distribution, which maximizes the estimation accuracy by storing more items. MimoSketch's scheduling policy balances items among nodes, avoiding nodes being overloaded or underloaded, which improves the overall mining accuracy. Our prototype and evaluation show that our algorithm can improve the item frequency estimation accuracy by an order of magnitude compared with the state-of-the-art solutions, and the scheduling policy further promotes the performance in MIMO scenarios.|在分布式数据流挖掘中，我们抽象出一个 MIMO 场景，其中多个节点挖掘多个项目流。针对 MIMO 特定场景设计了一个 MimoSketch 框架，改进了项目频率估计的基本挖掘任务。MimoSketch 由算法设计和节点调度策略组成。MimoSketch 算法采用随机计数的方法，保留了经过数学证明的无偏性质，对多节点的聚合查询更加友好; 其内存布局动态适应运行时项目大小分布，通过存储更多项目来最大化估计精度。MimoSketch 的调度策略平衡了节点之间的项目，避免了节点的过载或过载，提高了整体挖掘的准确性。我们的原型和评估表明，与最先进的解决方案相比，我们的算法可以提高项目频率估计的准确性数量级，并且调度策略进一步提高了在 MIMO 场景中的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MimoSketch:+A+Framework+to+Mine+Item+Frequency+on+Multiple+Nodes+with+Sketches)|0|
|[Kernel Ridge Regression-Based Graph Dataset Distillation](https://doi.org/10.1145/3580305.3599398)|Zhe Xu, Yuzhong Chen, Menghai Pan, Huiyuan Chen, Mahashweta Das, Hao Yang, Hanghang Tong||The huge volume of emerging graph datasets has become a double-bladed sword for graph machine learning. On the one hand, it empowers the success of a myriad of graph neural networks (GNNs) with strong empirical performance. On the other hand, training modern graph neural networks on huge graph data is computationally expensive. How to distill the given graph dataset while retaining most of the trained models' performance is a challenging problem. Existing efforts try to approach this problem by solving meta-learning-based bilevel optimization objectives. A major hurdle lies in that the exact solutions of these methods are computationally intensive and thus, most, if not all, of them are solved by approximate strategies which in turn hurt the distillation performance. In this paper, inspired by the recent advances in neural network kernel methods, we adopt a kernel ridge regression-based meta-learning objective which has a feasible exact solution. However, the computation of graph neural tangent kernel is very expensive, especially in the context of dataset distillation. As a response, we design a graph kernel, named LiteGNTK, tailored for the dataset distillation problem which is closely related to the classic random walk graph kernel. An effective model named Kernel rıdge regression-based graph Dataset Distillation (KIDD) and its variants are proposed. KIDD shows nice efficiency in both the forward and backward propagation processes. At the same time, KIDD shows strong empirical performance over 7 real-world datasets compared with the state-of-the-art distillation methods. Thanks to the ability to find the exact solution of the distillation objective, the learned training graphs by KIDD can sometimes even outperform the original whole training set with as few as 1.65% training graphs.|海量的新兴图形数据集已经成为图机学习的双刃剑。一方面，它赋予了无数图形神经网络(GNN)以强大的经验性能的成功。另一方面，在海量图形数据上训练现代图形神经网络计算量很大。如何提取给定的图形数据集，同时保留训练模型的大部分性能是一个具有挑战性的问题。现有的研究试图通过解决基于元学习的双层优化目标来解决这个问题。一个主要的障碍在于这些方法的精确解是计算密集型的，因此，如果不是全部的话，它们中的大多数是通过近似策略来解决的，这反过来又会损害精馏性能。本文受神经网络核方法的启发，采用了一种基于核岭回归的元学习目标，该目标具有可行的精确解。然而，图神经切核的计算是非常昂贵的，特别是在数据集精馏的背景下。作为响应，我们针对与经典随机游走图核密切相关的数据集精馏问题，设计了一个图核 LiteGNTK。提出了一种有效的基于核边缘回归的图形数据集提取(KIDD)模型及其变体。KIDD 在正向和反向传播过程中都表现出良好的效率。同时，与现有的精馏方法相比，KIDD 算法在7个实际数据集上表现出了较强的经验性能。由于 KIDD 能够找到精馏目标的精确解，所学到的训练图有时甚至能够以1.65% 的训练图优于原来的整个训练集。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Kernel+Ridge+Regression-Based+Graph+Dataset+Distillation)|0|
|[BatchSampler: Sampling Mini-Batches for Contrastive Learning in Vision, Language, and Graphs](https://doi.org/10.1145/3580305.3599263)|Zhen Yang, Tinglin Huang, Ming Ding, Yuxiao Dong, Rex Ying, Yukuo Cen, Yangliao Geng, Jie Tang|Yale University; Tsinghua University|In-Batch contrastive learning is a state-of-the-art self-supervised method that brings semantically-similar instances close while pushing dissimilar instances apart within a mini-batch. Its key to success is the negative sharing strategy, in which every instance serves as a negative for the others within the mini-batch. Recent studies aim to improve performance by sampling hard negatives \textit{within the current mini-batch}, whose quality is bounded by the mini-batch itself. In this work, we propose to improve contrastive learning by sampling mini-batches from the input data. We present BatchSampler\footnote{The code is available at \url{https://github.com/THUDM/BatchSampler}} to sample mini-batches of hard-to-distinguish (i.e., hard and true negatives to each other) instances. To make each mini-batch have fewer false negatives, we design the proximity graph of randomly-selected instances. To form the mini-batch, we leverage random walk with restart on the proximity graph to help sample hard-to-distinguish instances. BatchSampler is a simple and general technique that can be directly plugged into existing contrastive learning models in vision, language, and graphs. Extensive experiments on datasets of three modalities show that BatchSampler can consistently improve the performance of powerful contrastive models, as shown by significant improvements of SimCLR on ImageNet-100, SimCSE on STS (language), and GraphCL and MVGRL on graph datasets.|批内对比学习是一种最先进的自我监督方法，它可以使语义相似的实例关闭，同时在小批内将不同的实例分开。其成功的关键是消极分享策略，在这种策略中，每一个实例对于小批次中的其他实例都是消极的。最近的研究旨在提高性能抽样硬负面文本{在当前的小批量} ，其质量是有界的小批量本身。在这项工作中，我们提出改善对比学习的抽样小批量的输入数据。我们提供 BatchSampler 脚注{该代码可在 url { https://github.com/thudm/BatchSampler }}获得，用于对难以区分(即彼此之间的硬负片和真负片)的迷你批次实例进行抽样。为了使每个小批量产品具有较少的假阴性，我们设计了随机选择实例的接近图。为了形成迷你批处理，我们利用在接近图上重新启动的随机游动来帮助抽样难以区分的实例。BatchSampler 是一种简单而通用的技术，可以直接插入到视觉、语言和图形中现有的对比学习模型中。对三种模式的数据集进行的广泛实验表明，BatchSampler 可以持续改善强大的对比模型的性能，如 ImageNet-100上的 SimCLR，STS (语言)上的 SimCSE 以及图形数据集上的 GraphCL 和 MVGRL 的显着改进所示。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BatchSampler:+Sampling+Mini-Batches+for+Contrastive+Learning+in+Vision,+Language,+and+Graphs)|0|
|[Web-based Long-term Spine Treatment Outcome Forecasting](https://doi.org/10.1145/3580305.3599545)|Hangting Ye, Zhining Liu, Wei Cao, Amir M. Amiri, Jiang Bian, Yi Chang, Jon D. Lurie, Jim Weinstein, TieYan Liu||The aging of global population is witnessing increasing prevalence of spinal disorders. According to latest statistics, nearly five percent of the global population is suffering from spinal disorders. To relieve the pain, many spine patients tend to choose surgeries. However, recent evidences reveal that some spine patients can self-heal over time with nonoperative treatment and even surgeries may not ease the pain for some others, which raises a critical question regarding the appropriateness of such surgeries. Furthermore, the complex and time-consuming diagnostic process places a great burden on both clinicians and patients. Due to the development of web technology, it is possible for spine patients to obtain decision making suggestions on the Internet. The uniqueness of web technology, including its popularity, convenience, and immediacy, makes intelligent healthcare techniques, especially Treatment Outcome Forecasting (TOF), able to support clinical decision-making for doctors and healthcare providers. Despite a few machine-learning-based methods have been proposed for TOF, their performance and feasibility are mostly unsatisfactory due to the neglect of a few practical challenges (caused by applying on the Internet), including biased data selection, noisy supervision, and patient noncompliance. In light of this, we propose DeepTOF, a novel end-to-end deep learning model to cope with the unique challenges in web-based long-term continuous spine TOF. In particular, we combine different patient groups and train a unified predictive model to eliminate the data selection bias. Towards robust learning, we further take advantage of indirect but fine-grained supervision signals to mutually calibrate with the noisy training labels. Additionally, a feature selector was co-trained with DeepTOF to select the most important features (i.e., answers/indicators that need to be collected) for inference, thus easing the use of DeepTOF during web-based real-world application. The proposed DeepTOF could bring great benefits to the rehabilitation of spine patients. Comprehensive experiments and analysis show that DeepTOF outperforms conventional solutions by a large margin.|随着全球人口的老龄化，脊柱疾病的发病率不断上升。根据最新的统计数据，全球近5% 的人口患有脊柱疾病。为了减轻疼痛，许多脊柱病人倾向于选择手术。然而，最近的证据表明，随着时间的推移，一些脊柱患者可以通过非手术治疗自我愈合，甚至手术可能不会减轻其他一些人的疼痛，这就提出了关于这种手术的适当性的关键问题。此外，复杂而耗时的诊断过程给临床医生和患者都带来了很大的负担。随着网络技术的发展，脊柱疾病患者可以在因特网上获得决策建议。网络技术的独特性，包括它的普及性、方便性和即时性，使得智能医疗技术，特别是治疗结果预测(TOF) ，能够支持医生和医疗保健提供者的临床决策。尽管已经提出了一些基于机器学习的 TOF 方法，但是由于忽视了一些实际挑战(由于在互联网上的应用) ，包括有偏见的数据选择，嘈杂的监督和患者不遵从性，其性能和可行性大多不令人满意。鉴于此，我们提出了 DeepTOF，一种新颖的端到端深度学习模型，以应对基于网络的长期连续脊柱 TOF 的独特挑战。特别是，我们结合不同的患者组，并训练一个统一的预测模型，以消除数据选择偏倚。在稳健学习方面，我们进一步利用间接但细粒度的监督信号与噪声训练标签进行相互校准。此外，特征选择器与 DeepTOF 共同训练，以选择最重要的特征(即需要收集的答案/指标)进行推断，从而简化了 DeepTOF 在基于网络的现实世界应用中的使用。提出的 DeepTOF 可以为脊柱病人的康复带来巨大的好处。综合实验和分析表明，DeepTOF 的性能大大优于传统的解决方案。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Web-based+Long-term+Spine+Treatment+Outcome+Forecasting)|0|
|[Optimal Dynamic Subset Sampling: Theory and Applications](https://doi.org/10.1145/3580305.3599458)|Lu Yi, Hanzhi Wang, Zhewei Wei|Renmin University of China|We study the fundamental problem of sampling independent events, called subset sampling. Specifically, consider a set of $n$ events $S=\{x_1, \ldots, x_n\}$, where each event $x_i$ has an associated probability $p(x_i)$. The subset sampling problem aims to sample a subset $T \subseteq S$, such that every $x_i$ is independently included in $S$ with probability $p_i$. A naive solution is to flip a coin for each event, which takes $O(n)$ time. However, the specific goal is to develop data structures that allow drawing a sample in time proportional to the expected output size $\mu=\sum_{i=1}^n p(x_i)$, which can be significantly smaller than $n$ in many applications. The subset sampling problem serves as an important building block in many tasks and has been the subject of various research for more than a decade. However, most of the existing subset sampling approaches are conducted in a static setting, where the events or their associated probability in set $S$ is not allowed to be changed over time. These algorithms incur either large query time or update time in a dynamic setting despite the ubiquitous time-evolving events with changing probability in real life. Therefore, it is a pressing need, but still, an open problem, to design efficient dynamic subset sampling algorithms. In this paper, we propose ODSS, the first optimal dynamic subset sampling algorithm. The expected query time and update time of ODSS are both optimal, matching the lower bounds of the subset sampling problem. We present a nontrivial theoretical analysis to demonstrate the superiority of ODSS. We also conduct comprehensive experiments to empirically evaluate the performance of ODSS. Moreover, we apply ODSS to a concrete application: influence maximization. We empirically show that our ODSS can improve the complexities of existing influence maximization algorithms on large real-world evolving social networks.|我们研究抽样独立事件的基本问题，称为子集抽样。具体来说，考虑一组 $n $事件 $S = { x _ 1，ldot，x _ n } $，其中每个事件 $x _ i $具有相关的概率 $p (x _ i) $。子集抽样问题的目标是抽样一个子集 $T 子集 S $，这样每个 $x _ i $都独立地包含在 $S $中，概率为 $p _ i $。一个天真的解决方案是为每个事件抛硬币，这需要花费 $O (n) $时间。然而，我们的具体目标是开发一种数据结构，它允许在与预期输出大小成正比的时间内绘制样本 $mu = sum _ { i = 1} ^ n p (x _ i) $，在许多应用程序中，它可以明显小于 $n $。子集抽样问题是许多工作中的一个重要组成部分，也是近十多年来各种研究的主题。但是，大多数现有的子集抽样方法都是在静态环境中进行的，其中不允许随着时间的推移更改集 $S $中的事件或其相关概率。尽管在现实生活中随时间演化的事件随概率的变化无处不在，但这些算法在动态环境中会产生大量的查询时间或更新时间。因此，设计高效的动态子集采样算法是一个迫切而又尚未解决的问题。本文提出了第一种最优动态子集抽样算法 ODSS。ODSS 的期望查询时间和更新时间均为最优，与子集抽样问题的下界相匹配。我们提出了一个非平凡的理论分析，以证明 ODSS 的优越性。我们还进行了综合性的实验，对 ODSS 的性能进行了实证评估。此外，我们将 ODSS 应用于一个具体的应用: 影响最大化。我们的实验表明，我们的 ODSS 可以改善现有的影响最大化算法在大型真实世界演化的社会网络上的复杂性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimal+Dynamic+Subset+Sampling:+Theory+and+Applications)|0|
|[Sharpness-Aware Minimization Revisited: Weighted Sharpness as a Regularization Term](https://doi.org/10.1145/3580305.3599501)|Yun Yue, Jiadi Jiang, Zhiling Ye, Ning Gao, Yongchao Liu, Ke Zhang|Ant Group|Deep Neural Networks (DNNs) generalization is known to be closely related to the flatness of minima, leading to the development of Sharpness-Aware Minimization (SAM) for seeking flatter minima and better generalization. In this paper, we revisit the loss of SAM and propose a more general method, called WSAM, by incorporating sharpness as a regularization term. We prove its generalization bound through the combination of PAC and Bayes-PAC techniques, and evaluate its performance on various public datasets. The results demonstrate that WSAM achieves improved generalization, or is at least highly competitive, compared to the vanilla optimizer, SAM and its variants. The code is available at https://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers.|深度神经网络(DNN)泛化与最小值的平坦性密切相关，导致锐度感知最小化(SAM)的发展，以寻求更平坦的最小值和更好的泛化。在本文中，我们重新审视了 SAM 的损失，并提出了一种更一般的方法，称为 WSAM，通过合并锐度作为一个正则项。通过结合 PAC 和 Bayes-PAC 技术证明了其泛化界，并对其在各种公共数据集上的性能进行了评估。结果表明，与普通的优化器 SAM 及其变体相比，WSAM 实现了改进的泛化，或者至少具有很强的竞争力。密码可在 https://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sharpness-Aware+Minimization+Revisited:+Weighted+Sharpness+as+a+Regularization+Term)|0|
|[Doubly Robust AUC Optimization against Noisy and Adversarial Samples](https://doi.org/10.1145/3580305.3599316)|Chenkang Zhang, Wanli Shi, Lei Luo, Bin Gu|Nanjing University of Science and Technology; Nanjing University of Information Science and Technology|Area under the ROC curve (AUC) is an important and widely used metric in machine learning especially for imbalanced datasets. In current practical learning problems, not only adversarial samples but also noisy samples seriously threaten the performance of learning models. Nowadays, there have been a lot of research works proposed to defend the adversarial samples and noisy samples separately. Unfortunately, to the best of our knowledge, none of them with AUC optimization can secure against the two kinds of harmful samples simultaneously. To fill this gap and also address the challenge, in this paper, we propose a novel doubly robust dAUC optimization (DRAUC) algorithm. Specifically, we first exploit the deep integration of self-paced learning and adversarial training under the framework of AUC optimization, and provide a statistical upper bound to the AUC adversarial risk. Inspired by the statistical upper bound, we propose our optimization objective followed by an efficient alternatively stochastic descent algorithm, which can effectively improve the performance of learning models by guarding against adversarial samples and noisy samples. Experimental results on several standard datasets demonstrate that our DRAUC algorithm has better noise robustness and adversarial robustness than the state-of-the-art algorithms.|ROC 曲线下面积(aUC)是机器学习中广泛使用的一个重要指标，特别是对于不平衡的数据集。在当前的实际学习问题中，不仅对手样本严重威胁着学习模型的性能，而且噪声样本也严重威胁着学习模型的性能。目前，已经有很多研究工作提出将对抗样本和噪声样本分别进行辩护。不幸的是，据我们所知，没有一个 AUC 优化能够同时抵抗这两种有害样品。为了填补这一空白，并解决这一挑战，本文提出了一种新的双鲁棒 dAUC 优化(DRAUC)算法。具体来说，我们首先利用 AUC 优化框架下自主学习和对抗性训练的深度整合，并提供了 AUC 对抗性风险的统计上界。受统计上界的启发，我们提出了优化目标和一个有效的交替随机下降算法，它可以有效地提高学习模型的性能，防止对手样本和噪声样本。在几个标准数据集上的实验结果表明，我们的 DRAUC 算法具有比现有算法更好的噪声鲁棒性和对抗鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Doubly+Robust+AUC+Optimization+against+Noisy+and+Adversarial+Samples)|0|
|[Finding Favourite Tuples on Data Streams with Provably Few Comparisons](https://doi.org/10.1145/3580305.3599352)|Guangyi Zhang, Nikolaj Tatti, Aristides Gionis|HIIT, University of Helsinki; Shenzhen Institute of Computing Sciences; KTH Royal Institute of Technology|One of the most fundamental tasks in data science is to assist a user with unknown preferences in finding high-utility tuples within a large database. To accurately elicit the unknown user preferences, a widely-adopted way is by asking the user to compare pairs of tuples. In this paper, we study the problem of identifying one or more high-utility tuples by adaptively receiving user input on a minimum number of pairwise comparisons. We devise a single-pass streaming algorithm, which processes each tuple in the stream at most once, while ensuring that the memory size and the number of requested comparisons are in the worst case logarithmic in $n$, where $n$ is the number of all tuples. An important variant of the problem, which can help to reduce human error in comparisons, is to allow users to declare ties when confronted with pairs of tuples of nearly equal utility. We show that the theoretical guarantees of our method can be maintained for this important problem variant. In addition, we show how to enhance existing pruning techniques in the literature by leveraging powerful tools from mathematical programming. Finally, we systematically evaluate all proposed algorithms over both synthetic and real-life datasets, examine their scalability, and demonstrate their superior performance over existing methods.|数据科学中最基本的任务之一是帮助具有未知偏好的用户在大型数据库中查找高效用元组。为了准确地获得未知的用户首选项，一种被广泛采用的方法是要求用户比较元组对。在本文中，我们研究识别一个或多个高效用元组的问题，通过自适应接收用户输入的最小数目的成对比较。我们设计了一个单通道流式算法，它最多处理流中的每个元组一次，同时确保内存大小和请求比较的数量在最坏的情况下是以 $n $为对数的，其中 $n $是所有元组的数量。该问题的一个重要变体是允许用户在遇到效用几乎相等的元组对时声明关系，这有助于减少比较中的人为错误。我们表明，对于这个重要的问题变量，我们方法的理论保证是可以保持的。此外，我们还展示了如何通过利用数学编程中的强大工具来增强文献中现有的剪枝技术。最后，我们系统地评估了所有提出的算法在合成和实际数据集上的性能，检验了它们的可伸缩性，并证明了它们优于现有方法的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Finding+Favourite+Tuples+on+Data+Streams+with+Provably+Few+Comparisons)|0|
|[Domain-Specific Risk Minimization for Domain Generalization](https://doi.org/10.1145/3580305.3599313)|YiFan Zhang, Jindong Wang, Jian Liang, Zhang Zhang, Baosheng Yu, Liang Wang, Dacheng Tao, Xing Xie||Domain generalization (DG) approaches typically use the hypothesis learned on source domains for inference on the unseen target domain. However, such a hypothesis can be arbitrarily far from the optimal one for the target domain, induced by a gap termed ''adaptivity gap.'' Without exploiting the domain information from the unseen test samples, adaptivity gap estimation and minimization are intractable, which hinders us to robustify a model to any unknown distribution. In this paper, we first establish a generalization bound that explicitly considers the adaptivity gap. Our bound motivates two strategies to reduce the gap: the first one is ensembling multiple classifiers to enrich the hypothesis space, then we propose effective gap estimation methods for guiding the selection of a better hypothesis for the target. The other method is minimizing the gap directly by adapting model parameters using online target samples. We thus propose Domain-specific Risk Minimization (DRM). During training, DRM models the distributions of different source domains separately; for inference, DRM performs online model steering using the source hypothesis for each arriving target sample. Extensive experiments demonstrate the effectiveness of the proposed DRM for domain generalization. Code is available at: https://github.com/yfzhang114/AdaNPC.|域泛化(DG)方法通常使用在源域上学到的假设来推断未知的目标域。然而，这样的假设可以任意地远离目标领域的最佳假设，由一个称为“适应性缺口”的缺口引起如果不利用未知测试样本的域信息，自适应间隙估计和最小化是难以解决的问题，这阻碍了我们将模型鲁棒化到任何未知分布。在本文中，我们首先建立一个显式考虑自适应差距的泛化界。本文提出了两种缩小差距的策略: 第一种是将多个分类器集成在一起来丰富假设空间，然后提出了有效的差距估计方法来指导目标选择更好的假设;。另一种方法是利用在线目标样本自适应模型参数，直接最小化间隙。因此，我们提出领域特定风险最小化(DRM)。在训练过程中，DRM 分别对不同源域的分布进行建模; 为了推理，DRM 使用每个到达目标样本的源假设进行在线模型导向。大量的实验证明了该方法的有效性。密码可于以下 https://github.com/yfzhang114/adanpc 索取:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Domain-Specific+Risk+Minimization+for+Domain+Generalization)|0|
|[Towards Fair Disentangled Online Learning for Changing Environments](https://doi.org/10.1145/3580305.3599523)|Chen Zhao, Feng Mi, Xintao Wu, Kai Jiang, Latifur Khan, Christan Grant, Feng Chen|The University of Texas at Dallas; University of Arkansas; Baylor University; University of Texas at Dallas; University of Florida|In the problem of online learning for changing environments, data are sequentially received one after another over time, and their distribution assumptions may vary frequently. Although existing methods demonstrate the effectiveness of their learning algorithms by providing a tight bound on either dynamic regret or adaptive regret, most of them completely ignore learning with model fairness, defined as the statistical parity across different sub-population (e.g., race and gender). Another drawback is that when adapting to a new environment, an online learner needs to update model parameters with a global change, which is costly and inefficient. Inspired by the sparse mechanism shift hypothesis, we claim that changing environments in online learning can be attributed to partial changes in learned parameters that are specific to environments and the rest remain invariant to changing environments. To this end, in this paper, we propose a novel algorithm under the assumption that data collected at each time can be disentangled with two representations, an environment-invariant semantic factor and an environment-specific variation factor. The semantic factor is further used for fair prediction under a group fairness constraint. To evaluate the sequence of model parameters generated by the learner, a novel regret is proposed in which it takes a mixed form of dynamic and static regret metrics followed by a fairness-aware long-term constraint. The detailed analysis provides theoretical guarantees for loss regret and violation of cumulative fairness constraints. Empirical evaluations on real-world datasets demonstrate our proposed method sequentially outperforms baseline methods in model accuracy and fairness.|在变化环境下的在线学习问题中，数据随着时间的推移依次接收，其分布假设可能会频繁变化。尽管现有的方法通过提供动态后悔或适应性后悔的紧密界限来证明其学习算法的有效性，但大多数方法完全忽略了模型公平性的学习，这种模型公平性被定义为不同子群(例如种族和性别)的统计平价。另一个缺点是，在适应新环境时，在线学习者需要根据全局变化更新模型参数，这样做成本高，效率低。受稀疏机制转移假说的启发，我们认为在线学习中不断变化的环境可以归因于特定于环境的学习参数的部分变化，而其余的参数对不断变化的环境保持不变。为此，本文提出了一种新的算法，该算法假设每次采集的数据可以分解为两种表示: 环境不变的语义因子和环境特定的变化因子。语义因子进一步用于群体公平约束下的公平预测。为了评估学习者生成的模型参数序列，提出了一种新的遗憾度量方法，该方法采用动态和静态遗憾度量的混合形式，并且具有公平意识的长期约束。详细的分析为损失后悔和违反累积公平约束提供了理论保证。对实际数据集的实证分析表明，该方法在模型精度和公平性方面均优于基线方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Fair+Disentangled+Online+Learning+for+Changing+Environments)|0|
|[SMILE: Evaluation and Domain Adaptation for Social Media Language Understanding](https://doi.org/10.1145/3580305.3599907)|Vasilisa Bashlovkina, Riley Matthews, Zhaobin Kuang, Simon Baumgartner, Michael Bendersky|Google Research|We study the ability of transformer-based language models (LMs) to understand social media language. Social media (SM) language is distinct from standard written language, yet existing benchmarks fall short of capturing LM performance in this socially, economically, and politically important domain. We quantify the degree to which social media language differs from conventional language and conclude that the difference is significant both in terms of token distribution and rate of linguistic shift. Next, we introduce a new benchmark for Social MedIa Language Evaluation (SMILE) that covers four SM platforms and eleven tasks. Finally, we show that learning a tokenizer and pretraining on a mix of social media and conventional language yields an LM that outperforms the best similar-sized alternative by 4.2 points on the overall SMILE score.|我们研究了基于转换器的语言模型(LM)理解社交媒体语言的能力。社会媒体语言(SM)与标准的书面语言不同，然而现有的基准在这个社会、经济和政治重要的领域还不足以捕捉 LM 的表现。我们量化了社交媒体语言与传统语言的差异程度，并得出结论: 社交媒体语言与传统语言的差异在表征分布和语言转换率方面都是显著的。接下来，我们将为社会媒体语言评估(SMILE)引入一个新的基准，它涵盖了四个 SM 平台和十一个任务。最后，我们表明，学习一个标记器和预训练的社会媒体和传统语言的混合产生了一个 LM 的表现最好的类似大小的选择4.2分的总体 SMILE 得分。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SMILE:+Evaluation+and+Domain+Adaptation+for+Social+Media+Language+Understanding)|0|
|[Augmenting Rule-based DNS Censorship Detection at Scale with Machine Learning](https://doi.org/10.1145/3580305.3599775)|Jacob Alexander Markson Brown, Xi Jiang, Van Tran, Arjun Nitin Bhagoji, Nguyen Phong Hoang, Nick Feamster, Prateek Mittal, Vinod Yegneswaran|University of Chicago; Princeton University; SRI International|The proliferation of global censorship has led to the development of a plethora of measurement platforms to monitor and expose it. Censorship of the domain name system (DNS) is a key mechanism used across different countries. It is currently detected by applying heuristics to samples of DNS queries and responses (probes) for specific destinations. These heuristics, however, are both platform-specific and have been found to be brittle when censors change their blocking behavior, necessitating a more reliable automated process for detecting censorship. In this paper, we explore how machine learning (ML) models can (1) help streamline the detection process, (2) improve the usability of large-scale datasets for censorship detection, and (3) discover new censorship instances and blocking signatures missed by existing heuristic methods. Our study shows that supervised models, trained using expert-derived labels on instances of known anomalies and possible censorship, can learn the detection heuristics employed by different measurement platforms. More crucially, we find that unsupervised models, trained solely on uncensored instances, can identify new instances and variations of censorship missed by existing heuristics. Moreover, both methods demonstrate the capability to uncover a substantial number of new DNS blocking signatures, i.e., injected fake IP addresses overlooked by existing heuristics. These results are underpinned by an important methodological finding: comparing the outputs of models trained using the same probes but with labels arising from independent processes allows us to more reliably detect cases of censorship in the absence of ground-truth labels of censorship.|全球审查制度的扩散导致了监测和揭露它的大量测量平台的发展。域名系统(DNS)的审查是各国使用的一个关键机制。目前，通过对特定目的地的 DNS 查询和响应(探测)样本应用启发式方法来检测它。然而，这些启发式方法都是针对特定平台的，当审查者改变他们的拦截行为时，这些方法被发现是脆弱的，这就需要一个更可靠的自动化过程来检测审查。本文探讨了机器学习(ML)模型在以下几个方面的作用: (1)简化检测过程; (2)提高大规模数据集在检测中的可用性; (3)发现新的检测实例和现有启发式方法遗漏的阻塞签名。我们的研究表明，监督模型，训练使用专家派生的标签对已知的异常和可能的检查的实例，可以学习检测启发采用不同的测量平台。更重要的是，我们发现，无监督模型，仅仅训练未经审查的实例，可以识别新的实例和变化的审查错过了现有的启发。此外，这两种方法都证明了能够发现大量新的 DNS 阻塞签名，即注入的假 IP 地址被现有的启发式方法忽略。这些结果得到了一个重要的方法论发现的支持: 比较使用相同探针训练的模型的输出，但是与独立过程产生的标签进行比较，使我们能够更可靠地检测在没有审查的地面真相标签的情况下的审查情况。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Augmenting+Rule-based+DNS+Censorship+Detection+at+Scale+with+Machine+Learning)|0|
|[Taming the Domain Shift in Multi-source Learning for Energy Disaggregation](https://doi.org/10.1145/3580305.3599910)|Xiaomin Chang, Wei Li, Yunchuan Shi, Albert Y. Zomaya||Non-intrusive load monitoring (NILM) is a cost-effective energy disaggregation means to estimate the energy consumption of individual appliances from a central load reading. Learning-based methods are the new trends in NILM implementations but require large labeled data to work properly at end-user premises. We first formulate an unsupervised multi-source domain adaptation problem to address this challenge by leveraging rich public datasets for building the NILM model. Then, we prove a new generalization bound for the target domain under multi-source settings. A hybrid loss-driven multi-source domain adversarial network (HLD-MDAN) is developed by approximating and optimizing the bound to tackle the domain shift between source and target domains. We conduct extensive experiments on three real-world residential energy datasets to evaluate the effectiveness of HLD-MDAN, showing that it is superior to other methods in single-source and multi-source learning scenarios.|非侵入性负荷监测(NILM)是一种具有成本效益的能源分解方法，可以从中央负荷读数估计单个电器的能源消耗。基于学习的方法是 NILM 实现的新趋势，但是需要大量的标记数据才能在最终用户前提下正常工作。我们首先提出一个无监督的多源域自适应问题来解决这个挑战，利用丰富的公共数据集来建立 NILM 模型。然后，在多源设置下证明了目标域的一个新的泛化界。针对源域和目标域之间的域漂移问题，提出了一种混合损耗驱动的多源域对抗网络(HLD-MDAN)。我们在三个真实的住宅能源数据集上进行了广泛的实验来评估 HLD-MDAN 的有效性，结果表明它在单源和多源学习情景下优于其他方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Taming+the+Domain+Shift+in+Multi-source+Learning+for+Energy+Disaggregation)|0|
|[Variance Reduction Using In-Experiment Data: Efficient and Targeted Online Measurement for Sparse and Delayed Outcomes](https://doi.org/10.1145/3580305.3599928)|Alex Deng, Michelle Du, Anna Matlin, Qing Zhang||Improving statistical power is a common challenge for online experimentation platforms so that more hypotheses can be tested and lower effect sizes can be detected. To increase the power without increasing the sample size, it is necessary to consider the variance of experimental outcome metrics. Variance reduction was previously applied to online experimentation based on the idea of using pre-experiment covariate data to account for noise in the final metrics. Since this method relies on correlations between pre-experiment covariates and experiment outcomes, its effectiveness can be limited when testing features for specific product surfaces. We were also motivated by the challenge of attributing sparse, delayed binary outcomes to individual user-product interactions. We present two novel methods for variance reduction that rely exclusively on in-experiment data. The first method is a framework for a model-based leading indicator metric which continually estimates progress toward a delayed binary outcome. The second method is a counterfactual treatment exposure index that quantifies the amount that a user is impacted by the treatment. We applied these methods to past experiments and found that both can achieve variance reduction of 50% or more compared to the delayed outcome metric. The substantial reduction in variance afforded by the two methods presented in this paper has enabled Airbnb's experimentation platform to become more agile and innovative.|提高统计能力是在线实验平台面临的一个共同挑战，这样就可以检验更多的假设，检测更低的效应大小。为了在不增加样本量的情况下增加功率，有必要考虑实验结果指标的方差。基于使用预实验协变量数据来解决最终指标中的噪声问题的思想，方差降低已经被应用到在线实验中。由于该方法依赖于实验前协变量与实验结果之间的相关性，因此在测试特定产品表面的特征时，其有效性可能受到限制。我们还受到将稀疏、延迟的二进制结果归因于个人用户产品交互的挑战的激励。我们提出了两种新的方差减少的方法，完全依赖于实验中的数据。第一种方法是一个基于模型的领先指标度量的框架，它不断地估计延迟的二进制结果的进展。第二种方法是反事实治疗暴露指数，它量化了使用者受治疗影响的数量。我们将这些方法应用于以往的实验，发现与延迟结果指标相比，两者都可以实现50% 或更多的方差减少。本文提出的两种方法大大减少了方差，使 Airbnb 的实验平台变得更加灵活和创新。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Variance+Reduction+Using+In-Experiment+Data:+Efficient+and+Targeted+Online+Measurement+for+Sparse+and+Delayed+Outcomes)|0|
|[Modelling Delayed Redemption with Importance Sampling and Pre-Redemption Engagement](https://doi.org/10.1145/3580305.3599867)|Samik Datta, Anshuman Mourya, Anirban Majumder, Vineet Chaoji||Rewards-based programs are popular within e-commerce online stores, with the goal of providing serendipitous incentives to delight customers. These rewards (or incentives) could be in the form of cashback, free-shipping or discount coupons on purchases within specific categories. The success of such programs relies on their ability to identify relevant rewards for customers, from a wide variety of incentives available on the online store. Estimating the likelihood of a customer redeeming an incentive is challenging due to 1) data sparsity: relatively rare occurrence of coupon redemptions as compared to issuances, and 2) delayed feedback: customers taking time to redeem, resulting in inaccurate model refresh, compounded by data drift due to new customers and coupons. To overcome these challenges, we present a novel framework, DRESS (Delayed Redemption Entire Space Sampling), that jointly models the effect of data sparsity and delayed feedback on redemptions. Our solution entails an architecture based on the recently proposed Entire Space Model ([12]), where we leverage pre-redemption engagement of customers (e.g. clipping of coupon) to overcome the sparsity challenge. The effect of delayed feedback is mitigated via a novel importance sampling mechanism, whose efficacy we formally analyze via a novel application of Influence Function ([10]). Experimental evaluation suggests that DRESS achieves significant lift in offline metric in comparison to state-of-the-art alternatives. Additionally, a live A/B test with DRESS resulted in a lift of 10 basis points in the redemption rate.|基于奖励的项目在电子商务网店中很流行，其目标是提供意外的奖励来取悦顾客。这些奖励(或激励)可以是现金返还、免运费或特定类别购物的折扣券。这些方案的成功取决于它们能够从网上商店提供的各种各样的激励措施中为顾客确定相关的奖励。估计客户兑现激励的可能性是具有挑战性的，因为1)数据稀少: 与发行相比，优惠券兑现的发生相对较少，2)延迟反馈: 客户需要时间来兑现，导致不准确的模型刷新，由于新客户和优惠券导致的数据漂移。为了克服这些挑战，我们提出了一个新的框架 DRESS (延迟赎回整个空间抽样) ，它共同模拟了数据稀疏和延迟反馈对赎回的影响。我们的解决方案需要一个基于最近提出的整体空间模型([12])的体系结构，在这个体系结构中，我们利用客户的提前赎回约定(例如削减优惠券)来克服稀缺性挑战。延迟反馈的影响通过一种新的重要性抽样机制得到缓解，我们通过一种新的影响函数的应用正式分析了这种机制的功效([10])。实验评估表明，与最先进的替代方案相比，DRESS 在离线测量方面取得了显著的提升。此外，通过 DRESS 的 A/B 测试，赎回率上升了10个基点。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modelling+Delayed+Redemption+with+Importance+Sampling+and+Pre-Redemption+Engagement)|0|
|[From Human Days to Machine Seconds: Automatically Answering and Generating Machine Learning Final Exams](https://doi.org/10.1145/3580305.3599827)|Iddo Drori, Sarah J. Zhang, Reece Shuttleworth, Sarah Zhang, Keith Tyser, Zad Chin, Pedro Lantigua, Saisamrit Surbehera, Gregory Hunter, Derek Austin, Leonard Tang, Yann Hicke, Sage Simhon, Sathwik Karnik, Darnell Granberry, Madeleine Udell||A final exam in machine learning at a top institution such as MIT, Harvard, or Cornell typically takes faculty days to write, and students hours to solve. We demonstrate that large language models pass machine learning finals at a human level on finals available online and automatically generate new human-quality final exam questions in seconds. Previous work has developed program synthesis and few-shot learning methods to solve university-level problem set questions in mathematics and STEM courses. In this work, we develop and compare methods that solve final exams, which differ from problem sets in several ways: the questions are longer, have multiple parts, are more complicated, and span a broader set of topics. We curate a dataset and benchmark of questions from machine learning final exams available online and code for answering these questions and generating new questions. We show how to generate new questions from other questions and course notes. For reproducibility and future research on this final exam benchmark, we use automatic checkers for multiple-choice, numeric, and questions with expression answers. A student survey comparing the quality, appropriateness, and difficulty of machine-generated questions with human-written questions shows that across multiple aspects, machine-generated questions are indistinguishable from human-generated questions and are suitable for final exams. We perform ablation studies comparing zero-shot learning with few-shot learning and chain-of-thought prompting using GPT-3, OPT, Codex, and ChatGPT across machine learning topics and find that few-shot learning methods perform best. We highlight the transformative potential of language models to streamline the writing and solution of large-scale assessments, significantly reducing the workload from human days to mere machine seconds. Our results suggest that rather than banning large language models such as ChatGPT in class, instructors should teach students to harness them by asking students meta-questions about correctness, completeness, and originality of the responses generated, encouraging critical thinking in academic studies.|麻省理工学院、哈佛大学或康奈尔大学等顶尖学府的机器学习期末考试，通常需要教师花几天时间写作，而学生则需要几个小时来解答。我们证明了大型语言模型通过期末考试在人类水平上的机器学习期末考试可在线获得，并自动生成新的人类质量期末考试问题在几秒钟内。以前的工作已经开发了程序综合和少拍学习方法来解决大学水平的数学和 STEM 课程中的问题集问题。在这项工作中，我们开发和比较解决期末考试的方法，它不同于问题集在几个方面: 问题更长，有多个部分，更复杂，并跨越更广泛的主题集。我们策划了一个数据集和基准的问题，从机器学习期末考试可在线和代码回答这些问题和产生新的问题。我们展示了如何从其他问题和课程笔记中生成新的问题。为了重复性和这个期末考试基准的未来研究，我们使用自动检查器来检查多项选择、数字和带有表达式答案的问题。一项学生调查比较了机器生成的问题和人写的问题的质量、适当性和难度，结果显示，在多个方面，机器生成的问题与人写的问题没有什么区别，适合期末考试。我们使用 GPT-3，OPT，Codex 和 ChatGPT 对机器学习主题进行比较零镜头学习和少镜头学习以及思维链激励的消融研究，发现少镜头学习方法表现最好。我们强调语言模型在简化大规模评估的写作和解决方案方面的变革潜力，大大减少了工作量，从人类的日子减少到仅仅是机器秒。我们的研究结果表明，与其在课堂上禁止像 ChatGPT 这样的大型语言模式，教师应该通过问学生关于回答的正确性、完整性和原创性的元问题来教会学生利用这些模式，鼓励学术研究中的批判性思维。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Human+Days+to+Machine+Seconds:+Automatically+Answering+and+Generating+Machine+Learning+Final+Exams)|0|
|[Learning Multi-Agent Intention-Aware Communication for Optimal Multi-Order Execution in Finance](https://doi.org/10.1145/3580305.3599856)|Yuchen Fang, Zhenggang Tang, Kan Ren, Weiqing Liu, Li Zhao, Jiang Bian, Dongsheng Li, Weinan Zhang, Yong Yu, TieYan Liu|University of Illinois Urbana-Champaign; Microsoft; Shanghai Jiao Tong University; Microsoft Research Asia|Order execution is a fundamental task in quantitative finance, aiming at finishing acquisition or liquidation for a number of trading orders of the specific assets. Recent advance in model-free reinforcement learning (RL) provides a data-driven solution to the order execution problem. However, the existing works always optimize execution for an individual order, overlooking the practice that multiple orders are specified to execute simultaneously, resulting in suboptimality and bias. In this paper, we first present a multi-agent RL (MARL) method for multi-order execution considering practical constraints. Specifically, we treat every agent as an individual operator to trade one specific order, while keeping communicating with each other and collaborating for maximizing the overall profits. Nevertheless, the existing MARL algorithms often incorporate communication among agents by exchanging only the information of their partial observations, which is inefficient in complicated financial market. To improve collaboration, we then propose a learnable multi-round communication protocol, for the agents communicating the intended actions with each other and refining accordingly. It is optimized through a novel action value attribution method which is provably consistent with the original learning objective yet more efficient. The experiments on the data from two real-world markets have illustrated superior performance with significantly better collaboration effectiveness achieved by our method.|定单执行是定量金融的一项基本任务，其目的是完成对特定资产的多个交易定单的收购或清算。无模型强化学习的最新进展为订单执行问题提供了一个数据驱动的解决方案。然而，现有的工作总是优化单个订单的执行，忽视了多个订单指定同时执行的做法，导致次优性和偏差。本文首先提出了一种考虑实际约束的多代理 RL (MARL)多订单执行方法。具体来说，我们把每个代理当作一个独立的经营者来交易一个特定的订单，同时保持相互之间的沟通和合作，以实现总体利润的最大化。然而，现有的 MARL 算法往往只交换代理人的部分观测信息，而不考虑代理人之间的通信，在复杂的金融市场中效率低下。为了改进协作，我们提出了一个可学习的多轮通信协议，用于代理之间相互通信预期的操作并相应地进行细化。通过一种新的行为价值归因方法对其进行优化，该方法与原有的学习目标一致，但效率更高。对两个实际市场的数据进行的实验表明，该方法具有更好的协作效率和更好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Multi-Agent+Intention-Aware+Communication+for+Optimal+Multi-Order+Execution+in+Finance)|0|
|[iETA: A Robust and Scalable Incremental Learning Framework for Time-of-Arrival Estimation](https://doi.org/10.1145/3580305.3599842)|Jindong Han, Hao Liu, Shui Liu, Xi Chen, Naiqiang Tan, Hua Chai, Hui Xiong||Time-of-arrival estimation or Estimated Time of Arrival (ETA) has become an indispensable building block of modern intelligent transportation systems. While many efforts have been made for time-of-arrival estimation, most of them have scalability and robustness issues when dealing with real-world large-scale ETA scenarios, where billions of vehicle trajectories and ETA requests have been continuously generating every day. To this end, in this paper, we propose a robust and scalable incremental ETA learning framework, iETA, to continuously exploit spatio-temporal traffic patterns from massive floating-car data and thus achieve better estimation performances. Specifically, we first build an incremental travel time predictor that can be incrementally updated based on newly generated traffic data. The incremental travel time predictor not only reduces the overall learning overhead but also improves the model's robustness toward urban traffic distribution shifts. Then, we propose a historical traffic knowledge consolidation module to preserve critical spatio-temporal knowledge from previous ETA predictors under the incremental learning setting. Moreover, to reduce interference induced by low-quality traffic data, we propose an adversarial training module to improve the learning robustness by proactively mitigating and resisting traffic noise perturbations. Finally, extensive experiments demonstrate the effectiveness and efficiency of the proposed system against state-of-the-art baselines in large-scale ETA scenarios. Most importantly, iETA has been deployed on the Didi Chuxing platform, handling real-time billions of ETA queries every day, and substantially improves the prediction accuracy.|到达时间估计(ETA)已成为现代智能交通系统不可或缺的组成部分。虽然在到达时间估计方面作出了许多努力，但其中大多数在处理现实世界中的大规模预计到达时间情景时都存在可扩展性和稳健性问题，在这些情景中，每天不断产生数十亿车辆轨迹和预计到达时间请求。为此，本文提出了一个鲁棒的、可扩展的增量式 ETA 学习框架 iETA，它可以从大量的浮动车数据中不断地利用时空交通模式，从而获得更好的估计性能。具体来说，我们首先构建一个增量旅行时间预测器，它可以根据新生成的交通数据进行增量更新。增量式行程时间预测器不仅降低了整体学习开销，而且提高了模型对城市交通分布变化的鲁棒性。然后，我们提出了一个历史交通知识整合模块，以保存关键的时空知识从以前的预测 ETA 的在线机机器学习设置。此外，为了减少低质量交通数据引起的干扰，我们提出了一个对抗训练模块，通过主动减轻和抵抗交通噪声扰动来提高学习的鲁棒性。最后，大量的实验证明了该系统在大规模 ETA 场景中对抗最先进基线的有效性和效率。最重要的是，iETA 已经部署在滴滴出行平台上，每天处理数十亿的实时 ETA 查询，大大提高了预测的准确性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=iETA:+A+Robust+and+Scalable+Incremental+Learning+Framework+for+Time-of-Arrival+Estimation)|0|
|[Identifying Complicated Contagion Scenarios from Cascade Data](https://doi.org/10.1145/3580305.3599841)|Galen Harrison, Amro Alabsi Aljundi, Jiangzhuo Chen, S. S. Ravi, Anil Kumar S. Vullikanti, Madhav V. Marathe, Abhijin Adiga|University of Virginia|We consider the setting of cascades that result from contagion dynamics on large realistic contact networks. We address the question of whether the structural properties of a (partially) observed cascade can characterize the contagion scenario and identify the interventions that might be in effect. Using epidemic spread as a concrete example, we study how social interventions such as compliance in social distancing, extent (and efficacy) of vaccination, and the transmissibility of disease can be inferred. The techniques developed are more generally applicable to other contagions as well. Our approach involves the use of large realistic social contact networks of certain regions of USA and an agent-based model (ABM) to simulate spread under two interventions, namely vaccination and generic social distancing (GSD). Through a machine learning approach, coupled with parameter significance analysis, our experimental results show that subgraph counts of the graph induced by the cascade can be used effectively to characterize the contagion scenario even during the initial stages of the epidemic, when traditional information such as case counts alone are not adequate for this task. Further, we show that our approach performs well even for partially observed cascades. These results demonstrate that cascade data collected from digital tracing applications under poor digital penetration and privacy constraints can provide valuable information about the contagion scenario.|我们考虑在大型现实接触网络上由传染动力学产生的级联的设置。我们解决的问题是，一个(部分)观察级联的结构特性是否可以表征传染情景，并确定可能有效的干预措施。以流行病传播为具体例子，我们研究如何推断社会干预措施，如社会距离的依从性、疫苗接种的范围(和有效性)以及疾病的传播性。所开发的技术也更普遍地适用于其他传染病。我们的方法包括使用美国某些地区的大型现实社会接触网络和个体为本模型(ABM)来模拟两种干预措施下的传播，即疫苗接种和通用社会距离(gSD)。通过机器学习方法，结合参数显著性分析，我们的实验结果表明，即使在传染病流行的初始阶段，当病例计数等传统信息不足以完成这项任务时，级联诱导的图的子图计数也可以有效地用于描述传染病情景。此外，我们还展示了我们的方法即使在部分观察到的级联情况下也表现良好。这些结果表明，从数字追踪应用程序收集的级联数据在不良的数字渗透和隐私约束下，可以提供有价值的信息传染情况。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Identifying+Complicated+Contagion+Scenarios+from+Cascade+Data)|0|
|[Large-scale Urban Cellular Traffic Generation via Knowledge-Enhanced GANs with Multi-Periodic Patterns](https://doi.org/10.1145/3580305.3599853)|Shuodi Hui, Huandong Wang, Tong Li, Xinghao Yang, Xing Wang, Junlan Feng, Lin Zhu, Chao Deng, Pan Hui, Depeng Jin, Yong Li||With the rapid development of the cellular network, network planning is increasingly important. Generating large-scale urban cellular traffic contributes to network planning via simulating the behaviors of the planned network. Existing methods fail in simulating the long-term temporal behaviors of cellular traffic while cannot model the influences of the urban environment on the cellular networks. We propose a knowledge-enhanced GAN with multi-periodic patterns to generate large-scale cellular traffic based on the urban environment. First, we design a GAN model to simulate the multi-periodic patterns and long-term aperiodic temporal dynamics of cellular traffic via learning the daily patterns, weekly patterns, and residual traffic between long-term traffic and periodic patterns step by step. Then, we leverage urban knowledge to enhance traffic generation via constructing a knowledge graph containing multiple factors affecting cellular traffic in the surrounding urban environment. Finally, we evaluate our model on a real cellular traffic dataset. Our proposed model outperforms three state-of-art generation models by over 32.77%, and the urban knowledge enhancement improves the performance of our model by 4.71%. Moreover, our model achieves good generalization and robustness in generating traffic for urban cellular networks without training data in the surrounding areas.|随着蜂窝网络的迅速发展，网络规划变得越来越重要。生成大规模的城市蜂窝网络流量有助于通过模拟规划网络的行为进行网络规划。现有的方法不能模拟蜂窝网络的长期时间行为，也不能模拟城市环境对蜂窝网络的影响。提出了一种基于知识增强的多周期模式 GAN，用于产生基于城市环境的大规模蜂窝业务。首先，我们设计了一个 GAN 模型，通过逐步学习长期流量和周期流量之间的日流量、周流量和剩余流量，来模拟蜂窝网络流量的多周期模式和长期非周期时间动态。然后，我们利用城市知识，通过构建一个包含多个因素影响周围城市环境中蜂窝式交通的知识图来提高交通生成。最后，我们在一个真实的蜂窝网络流量数据集上评估我们的模型。我们提出的模型比三种最先进的生成模型的性能提高了32.77% 以上，城市知识增强使模型的性能提高了4.71% 。此外，该模型在不需要周边训练数据的情况下，对于城市蜂窝网络的流量生成具有良好的泛化性和鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large-scale+Urban+Cellular+Traffic+Generation+via+Knowledge-Enhanced+GANs+with+Multi-Periodic+Patterns)|0|
|[SentiGOLD: A Large Bangla Gold Standard Multi-Domain Sentiment Analysis Dataset and Its Evaluation](https://doi.org/10.1145/3580305.3599904)|Md. Ekramul Islam, Labib Chowdhury, Faisal Ahamed Khan, Shazzad Hossain, Md. Sourave Hossain, Mohammad Mamun Or Rashid, Nabeel Mohammed, Mohammad Ruhul Amin|Giga Tech Limited; Fordham University; Bangladesh Computer Council; North South University|This study introduces SentiGOLD, a Bangla multi-domain sentiment analysis dataset. Comprising 70,000 samples, it was created from diverse sources and annotated by a gender-balanced team of linguists. SentiGOLD adheres to established linguistic conventions agreed upon by the Government of Bangladesh and a Bangla linguistics committee. Unlike English and other languages, Bangla lacks standard sentiment analysis datasets due to the absence of a national linguistics framework. The dataset incorporates data from online video comments, social media posts, blogs, news, and other sources while maintaining domain and class distribution rigorously. It spans 30 domains (e.g., politics, entertainment, sports) and includes 5 sentiment classes (strongly negative, weakly negative, neutral, and strongly positive). The annotation scheme, approved by the national linguistics committee, ensures a robust Inter Annotator Agreement (IAA) with a Fleiss' kappa score of 0.88. Intra- and cross-dataset evaluation protocols are applied to establish a standard classification system. Cross-dataset evaluation on the noisy SentNoB dataset presents a challenging test scenario. Additionally, zero-shot experiments demonstrate the generalizability of SentiGOLD. The top model achieves a macro f1 score of 0.62 (intra-dataset) across 5 classes, setting a benchmark, and 0.61 (cross-dataset from SentNoB) across 3 classes, comparable to the state-of-the-art. Fine-tuned sentiment analysis model can be accessed at https://sentiment.bangla.gov.bd.|本文介绍了孟加拉语多领域情感分析数据集 SentiGOLD。它包括70,000个样本，由不同的来源创建，并由一个性别平衡的语言学家团队进行注释。SentiGOLD 遵守孟加拉国政府和孟加拉语言学委员会商定的既定语言公约。与英语和其他语言不同，由于缺乏国家语言学框架，孟加拉语缺乏标准的情感分析数据集。该数据集合并了来自在线视频评论、社交媒体帖子、博客、新闻和其他来源的数据，同时严格维护了域和类的分布。它跨越30个领域(例如，政治，娱乐，体育) ，包括5个情绪类(强烈消极，弱消极，中立，和强烈积极)。由国家语言学委员会批准的注释方案确保了一个强有力的内部注释协议(IAA) ，Fleiss 的 kappa 得分为0.88。数据集内和数据集间的评估协议被用来建立一个标准的分类方案。对噪声 SentNoB 数据集进行跨数据集评估是一个具有挑战性的测试场景。此外，零拍实验证明了 SentiGOLD 的通用性。顶级模型在5个类别中实现了0.62(数据集内)的宏观 f1评分，设定了基准，并在3个类别中实现了0.61(来自 SentNoB 的跨数据集) ，与最先进的技术相当。微调的情绪分析模型可以在 https://sentiment.bangla.gov.bd 访问。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SentiGOLD:+A+Large+Bangla+Gold+Standard+Multi-Domain+Sentiment+Analysis+Dataset+and+Its+Evaluation)|0|
|[Off-Policy Learning-to-Bid with AuctionGym](https://doi.org/10.1145/3580305.3599877)|Olivier Jeunen, Sean Murphy, Ben Allison||Online advertising opportunities are sold through auctions, billions of times every day across the web. Advertisers who participate in those auctions need to decide on a bidding strategy: how much they are willing to bid for a given impression opportunity. Deciding on such a strategy is not a straightforward task, because of the interactive and reactive nature of the repeated auction mechanism. Indeed, an advertiser does not observe counterfactual outcomes of bid amounts that were not submitted, and successful advertisers will adapt their own strategies based on bids placed by competitors. These characteristics complicate effective learning and evaluation of bidding strategies based on logged data alone. The interactive and reactive nature of the bidding problem lends itself to a bandit or reinforcement learning formulation, where a bidding strategy can be optimised to maximise cumulative rewards. Several design choices then need to be made regarding parameterisation, model-based or model-free approaches, and the formulation of the objective function. This work provides a unified framework for such "learning to bid'' methods, showing how many existing approaches fall under the value-based paradigm. We then introduce novel policy-based and doubly robust formulations of the bidding problem. To allow for reliable and reproducible offline validation of such methods without relying on sensitive proprietary data, we introduce AuctionGym: a simulation environment that enables the use of bandit learning for bidding strategies in online advertising auctions. We present results from a suite of experiments under varying environmental conditions, unveiling insights that can guide practitioners who need to decide on a model class. Empirical observations highlight the effectiveness of our newly proposed methods. AuctionGym is released under an open-source license, and we expect the research community to benefit from this tool.|在线广告机会是通过拍卖出售的，每天在网络上出售数十亿次。参与这些拍卖的广告商需要决定一个投标策略: 他们愿意为给定的印象机会出多少价。由于重复拍卖机制的互动性和反应性，决定这样一种战略不是一项直截了当的任务。事实上，广告客户不会观察到没有提交的出价金额的反事实结果，成功的广告客户将根据竞争对手的出价调整自己的策略。这些特点使得单独基于日志数据的投标策略的有效学习和评价复杂化。招标问题的互动性和反应性，使其成为一种强盗或强化学习的表述，在这种表述中，可以优化招标策略，以实现累积回报的最大化。然后，需要对参量化、基于模型或无模型的方法以及目标函数的表述做出几种设计选择。这项工作为这种“学习投标”方法提供了一个统一的框架，显示了有多少现有的方法属于基于价值的范式。然后，我们引入新的基于策略和双稳健公式的投标问题。为了能够在不依赖敏感的专有数据的情况下对这些方法进行可靠和可重复的离线验证，我们介绍 AuctionGym: 一个模拟环境，它能够在在线广告拍卖中使用强盗学习竞价策略。我们展示了在不同环境条件下的一系列实验结果，揭示了可以指导从业者决定模型课程的见解。经验观察突出了我们新提出的方法的有效性。AuctionGym 是在开源许可下发布的，我们希望研究社区能从这个工具中获益。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Off-Policy+Learning-to-Bid+with+AuctionGym)|0|
|[FairCod: A Fairness-aware Concurrent Dispatch System for Large-scale Instant Delivery Services](https://doi.org/10.1145/3580305.3599824)|Lin Jiang, Shuai Wang, Baoshen Guo, Hai Wang, Desheng Zhang, Guang Wang||In recent years, we have been witnessing a rapid prevalence of instant delivery services (e,g., UberEats, Instacart, and Eleme) due to their convenience and timeliness. A unique characteristic of instant delivery services is the concurrent dispatch mode, where (i) one courier usually simultaneously delivers multiple orders, especially during rush hours, and (ii) couriers can receive new orders when delivering existing orders. Most existing concurrent dispatch systems are efficiency-oriented, which means they usually dispatch a group of orders that have a similar delivery route to a courier. Although this strategy may achieve high overall efficiency, it also potentially causes a huge disparity of earnings between different couriers. To address the problem, in this paper, we design a Fairness-aware Concurrent dispatch system called FairCod, which aims to optimize the overall operation efficiency and individual fairness at the same time. Specifically, in FairCod, we design a Dynamic Advantage Actor-Critic algorithm with Fairness constrain (DA2CF). The basic idea is that it includes an Actor network to make dispatch decisions based on dynamic action space and a Critic network to evaluate the dispatch decisions from the fairness perspective. More importantly, we extensively evaluate our FairCod system based on one-month real-world data consisting of 36.38 million orders from 42,000 couriers collected by one of the largest instant delivery companies in China. Experimental results show that our FairCod improves courier fairness by 30.3% without sacrificing the overall system benefit compared to state-of-the-art baselines.|近年来，我们见证了即时递送服务(例如，g. ，UberEats，Instacart 和饿了么)的迅速普及，这是由于它们的方便性和及时性。即时递送服务的一个独特特征是并发分发模式，其中(i)一个快递员通常同时递送多个订单，特别是在高峰时间，以及(ii)快递员可以在递送现有订单时接收新订单。现有的大多数并发调度系统都是以效率为导向的，这意味着它们通常将一组具有相似配送路线的订单调度给快递员。虽然这种策略可以达到很高的整体效率，但它也有可能造成不同快递公司之间收入的巨大差距。为了解决这个问题，本文设计了一个基于公平感知的并发调度系统 FairCod，目的是同时优化整体运行效率和个体公平性。具体来说，在 FairCod，我们设计了一个带有公平约束的动态优势行为者批判算法(da2CF)。其基本思想是包括一个基于动态行为空间的调度决策参与者网络和一个从公平角度评价调度决策的批判网络。更重要的是，我们广泛评估我们的 FairCod 系统的基础上，一个月的现实世界的数据，包括3638万订单从42,000快递公司收集在中国最大的快递公司之一。实验结果表明，与最先进的基准相比，我们的 FairCod 在不牺牲系统整体效益的情况下，提高了30.3% 的信使公平性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FairCod:+A+Fairness-aware+Concurrent+Dispatch+System+for+Large-scale+Instant+Delivery+Services)|0|
|[CBLab: Supporting the Training of Large-scale Traffic Control Policies with Scalable Traffic Simulation](https://doi.org/10.1145/3580305.3599789)|Chumeng Liang, Zherui Huang, Yicheng Liu, Zhanyu Liu, Guanjie Zheng, Hanyuan Shi, Kan Wu, Yuhao Du, Fuliang Li, Zhenhui Jessie Li||Traffic simulation provides interactive data for the optimization of traffic control policies. However, existing traffic simulators are limited by their lack of scalability and shortage in input data, which prevents them from generating interactive data from traffic simulation in the scenarios of real large-scale city road networks. In this paper, we present City Brain Lab, a toolkit for scalable traffic simulation. CBLab consists of three components: CBEngine, CBData, and CBScenario. CBEngine is a highly efficient simulator supporting large-scale traffic simulation. CBData includes a traffic dataset with road network data of 100 cities all around the world. We also develop a pipeline to conduct a one-click transformation from raw road networks to input data of our traffic simulation. Combining CBEngine and CBData allows researchers to run scalable traffic simulations in the road network of real large-scale cities. Based on that, CBScenario implements an interactive environment and a benchmark for two scenarios of traffic control policies respectively, with which traffic control policies adaptable for large-scale urban traffic can be trained and tuned. To the best of our knowledge, CBLab is the first infrastructure supporting traffic control policy optimization in large-scale urban scenarios. CBLab has supported the City Brain Challenge @ KDD CUP 2021. The project is available on GitHub:~https://github.com/CityBrainLab/CityBrainLab.git.|交通仿真为交通控制策略的优化提供了交互数据。然而，现有的交通模拟器由于缺乏可扩展性和输入数据的不足，使得它们无法在真实的大规模城市道路网络场景中通过交通模拟生成交互式数据。在本文中，我们介绍了城市大脑实验室，一个可扩展的交通仿真工具包。CBLab 由三个组件组成: CBEngine、 CBData 和 CBScenario。CBEngine 是一个支持大规模交通仿真的高效仿真器。CBData 包括一个包含全球100个城市道路网数据的交通数据集。我们还开发了一个管道，从原始道路网络进行一键转换，以输入我们的交通模拟数据。CBEngine 和 CBData 的结合使得研究人员能够在真实的大规模城市的道路网络中运行可扩展的交通模拟。在此基础上，CBScenario 分别为两个交通控制策略场景实现了一个交互式环境和一个基准，用于训练和调整适应大规模城市交通的交通控制策略。据我们所知，CBLab 是支持大规模城市场景中交通控制政策优化的第一个基础设施。CBLab 支持城市大脑挑战@KDD CUP 2021。这个项目可以在 gitHub 上找到: ~  https://GitHub.com/citybrainlab/citybrainlab.git。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CBLab:+Supporting+the+Training+of+Large-scale+Traffic+Control+Policies+with+Scalable+Traffic+Simulation)|0|
|[Practical Synthetic Human Trajectories Generation Based on Variational Point Processes](https://doi.org/10.1145/3580305.3599888)|Qingyue Long, Huandong Wang, Tong Li, Lisi Huang, Kun Wang, Qiong Wu, Guangyu Li, Yanping Liang, Li Yu, Yong Li|Department of Electronic Engineering, Tsinghua University; China Mobile Research Institute|Human trajectories, reflecting people's travel patterns and the range of activities, are crucial for the applications like urban planning and epidemic control. However, the real-world human trajectory data tends to be limited by user privacy or device acquisition issues, leading to its insufficient quality to support the above applications. Hence, generating human trajectory data is a crucial but challenging task, which suffers from the following two critical challenges: 1) how to capture the user distribution in human trajectories (group view), and 2) how to model the complex mobility patterns of each user trajectory (individual view). In this paper, we propose a novel human trajectories generator (named VOLUNTEER), consisting of a user VAE and a trajectory VAE, to address the above challenges. Specifically, in the user VAE, we propose to learn the user distribution with all human trajectories from a group view. In the trajectory VAE, from the individual view, we model the complex mobility patterns by decoupling travel time and dwell time to accurately simulate individual trajectories. Extensive experiments on two real-world datasets show the superiority of our model over the state-of-the-art baselines. Further application analysis in the industrial system also demonstrates the effectiveness of our model.|反映人们旅行模式和活动范围的人类轨迹对于城市规划和流行病控制等应用至关重要。然而，现实世界中的人类轨迹数据往往受到用户隐私或设备获取问题的限制，导致其质量不足以支持上述应用程序。因此，生成人类轨迹数据是一项至关重要但具有挑战性的任务，它面临以下两个关键挑战: 1)如何捕获人类轨迹中的用户分布(组视图) ，以及2)如何建模每个用户轨迹的复杂移动模式(个人视图)。为了解决上述问题，本文提出了一种新的人体轨迹生成器(VOLUNTEER) ，它由用户 VAE 和轨迹 VAE 组成。具体来说，在用户 VAE 中，我们建议从组视图中了解具有所有人类轨迹的用户分布。在轨迹 VAE 中，从个体的角度出发，通过解耦行程时间和停留时间来建立复杂的移动模式，以准确地模拟个体的轨迹。在两个真实世界数据集上的大量实验表明，我们的模型优于最先进的基线。在工业系统中的进一步应用分析也证明了该模型的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Practical+Synthetic+Human+Trajectories+Generation+Based+on+Variational+Point+Processes)|0|
|[Deep Landscape Forecasting in Multi-Slot Real-Time Bidding](https://doi.org/10.1145/3580305.3599799)|Weitong Ou, Bo Chen, Yingxuan Yang, Xinyi Dai, Weiwen Liu, Weinan Zhang, Ruiming Tang, Yong Yu||Real-Time Bidding (RTB) has shown remarkable success in display advertising and has been employed in other advertising scenarios, e.g., sponsored search advertising with multiple ad slots. Many current RTB techniques built for single-slot display advertising are thus no longer applicable, especially in the bid landscape forecasting. Landscape forecasting predicts market competition, including the highest bid price and winning probability, which is preliminary and crucial for the subsequent bidding strategy design. In the multi-slot advertising, predicting the winning prices for each position requires a more precise differentiation of bids among top advertisers. Furthermore, defining the winning probability and addressing censorship issues are not as straightforward as in the case of a single slot. In view of these challenges, how to forecast the bidding landscape in the multi-slot environment remains open. In this work, we are the first to study the landscape forecasting problem in multi-slot RTB, considering the correlation between ad slots in the same pageview. Specifically, we formulate the research topic into two subproblems: predicting the distribution of the winning price and predicting the winning probability of the bid price for each position. Based on the observation from the production data and survival analysis techniques, we propose a deep recurrent model to predict the distribution of the winning price as well as the winning probability for each position. A comprehensive loss function is proposed to learn from the censoring data. Experiments on two public semi-synthetic datasets and one private industrial dataset demonstrate the effectiveness of our method.|实时竞价(RTB)在展示广告方面取得了显著的成功，并被用于其他广告场景，例如，具有多个广告时段的赞助商搜索广告。因此，目前许多为单插槽显示广告建立的 RTB 技术已不再适用，特别是在投标前景预测方面。景观预测预测市场竞争，包括最高投标价格和中标概率，是后续投标策略设计的初步和关键。在多插槽广告中，要预测每个广告位的中标价格，就需要对顶级广告商的出价进行更精确的区分。此外，确定获胜的可能性和解决审查问题并不像单一时段那样简单。鉴于这些挑战，如何预测多时段环境下的投标景观仍然是开放的。本文首次研究了多时隙 RTB 的景观预测问题，考虑了同一页面视图中广告时隙之间的相关性。具体来说，我们将研究课题分为两个子问题: 预测中标价格的分布和预测每个位置的中标价格的中标概率。基于对生产数据的观察和生存分析技术，我们提出了一个深度递归模型来预测中标价格的分布以及每个位置的中标概率。提出了一种综合损失函数，以便从截尾数据中学习。在两个公共半合成数据集和一个私有工业数据集上的实验表明了该方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Landscape+Forecasting+in+Multi-Slot+Real-Time+Bidding)|0|
|[NFT-Based Data Marketplace with Digital Watermarking](https://doi.org/10.1145/3580305.3599876)|Saeed Ranjbar Alvar, Mohammad Akbari, David (Ming Xuan) Yue, Yong Zhang|Huawei Technologies Canada Co., Ltd.; Ming Xuan) Yue (Huawei Technologies Canada Co., Ltd.|In today's digital world, enterprises and individuals are generating massive data that is potentially useful for many data consumers with data driven applications. The emergence of data marketplaces is a step toward helping the data owners to monetize their digital assets and get connected to the potential buyers. The current data marketplaces cannot handle the challenges related to data ownership claims, illegal redistribution, and data ownership traceability. To overcome these problems in a general-purpose market, we propose a marketplace based on watermarking and Non-Fungible Token (NFT) technologies. In the proposed NFT-based marketplace, the owner's data is stored as an NFT where the underlying content of the NFT holds the watermarked data. The watermarked data is obtained by embedding some information about the owners and the buyers into the original data. The embedded information can later be extracted to identify the owner and the buyer of the traded data. Furthermore, the transactions corresponding to the NFT provide verifiable ownership proof and traceable ownership history. A Proof-Of-Concept (POC) implementation of the proposed marketplace that will be integrated within AI-Gallery Data Marketplace service in Huawei Cloud is presented for trading image data. An extensive set of experiments to measure the gas consumption on the blockchain and evaluate the robustness of the watermarked assets against 51 attacks are performed. Finally, a method based on error correction codes is proposed for improving the watermarking robustness in the implemented marketplace. The link for the codes and the POC demo is provided in the appendix.|在当今的数字世界中，企业和个人正在生成大量数据，这些数据对于许多使用数据驱动应用程序的数据消费者来说可能非常有用。数据市场的出现是帮助数据所有者将其数字资产货币化并与潜在买家建立联系的一个步骤。当前的数据市场无法处理与数据所有权声明、非法重新分配和数据所有权可追踪性相关的挑战。为了克服这些问题，在一个通用市场，我们提出了一个基于水印和非可替换令牌(NFT)技术的市场。在提议的基于 NFT 的市场中，所有者的数据作为 NFT 存储，其中 NFT 的基础内容保存有水印数据。水印数据是通过在原始数据中嵌入所有者和购买者的信息来获得的。随后可以提取嵌入的信息以识别交易数据的所有者和买家。此外，与 NFT 相对应的交易提供了可验证的所有权证明和可追踪的所有权历史。为了交易图像数据，华为云计算的人工智能画廊数据市场服务(AI-Gallery Data Marketplace service)将集成一个拟议市场的概念验证(POC)实现。为了测量区块链上的耗气量并评估水印资产对51种攻击的鲁棒性，进行了一系列广泛的实验。最后，提出了一种基于纠错编码的水印鲁棒性提高方法。附录中提供了代码和 POC 演示的链接。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NFT-Based+Data+Marketplace+with+Digital+Watermarking)|0|
|[Rover: An Online Spark SQL Tuning Service via Generalized Transfer Learning](https://doi.org/10.1145/3580305.3599953)|Yu Shen, Xinyuyang Ren, Yupeng Lu, Huaijun Jiang, Huanyong Xu, Di Peng, Yang Li, Wentao Zhang, Bin Cui|Peking University; ByteDance Inc.; Mila – Québec AI Institute|Distributed data analytic engines like Spark are common choices to process massive data in industry. However, the performance of Spark SQL highly depends on the choice of configurations, where the optimal ones vary with the executed workloads. Among various alternatives for Spark SQL tuning, Bayesian optimization (BO) is a popular framework that finds near-optimal configurations given sufficient budget, but it suffers from the re-optimization issue and is not practical in real production. When applying transfer learning to accelerate the tuning process, we notice two domain-specific challenges: 1) most previous work focus on transferring tuning history, while expert knowledge from Spark engineers is of great potential to improve the tuning performance but is not well studied so far; 2) history tasks should be carefully utilized, where using dissimilar ones lead to a deteriorated performance in production. In this paper, we present Rover, a deployed online Spark SQL tuning service for efficient and safe search on industrial workloads. To address the challenges, we propose generalized transfer learning to boost the tuning performance based on external knowledge, including expert-assisted Bayesian optimization and controlled history transfer. Experiments on public benchmarks and real-world tasks show the superiority of Rover over competitive baselines. Notably, Rover saves an average of 50.1% of the memory cost on 12k real-world Spark SQL tasks in 20 iterations, among which 76.2% of the tasks achieve a significant memory reduction of over 60%.|像 Spark 这样的分布式数据分析引擎是工业中处理海量数据的常见选择。然而，Spark SQL 的性能在很大程度上取决于配置的选择，其中最佳配置随执行的工作负载而变化。在各种 Spark SQL 调优方案中，贝叶斯优化(BO)是一种流行的框架，它能够在预算充足的情况下找到接近最优的配置，但是它存在重新优化的问题，在实际生产中并不实用。当应用转移学习来加速调优过程时，我们注意到两个领域特有的挑战: 1)大多数以前的工作集中在转移调优历史，而来自 Spark 工程师的专家知识对于提高调优性能具有巨大的潜力，但是目前还没有得到很好的研究; 2)历史任务应该被仔细地利用，在使用不同的任务导致生产性能恶化的情况下。在本文中，我们介绍了 Rover，一个已部署的在线 Spark SQL 调优服务，用于在工业工作负载上进行高效和安全的搜索。针对这一挑战，我们提出了基于外部知识的广义迁移学习来提高调优性能，包括专家辅助的贝叶斯优化和受控历史迁移。在公共基准测试和实际任务上的实验表明，Rover 优于竞争基准测试。值得注意的是，在20次迭代中，Rover 为12k 实际 Spark SQL 任务平均节省了50.1% 的内存成本，其中76.2% 的任务实现了超过60% 的显著内存减少。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rover:+An+Online+Spark+SQL+Tuning+Service+via+Generalized+Transfer+Learning)|0|
|[Root Cause Analysis for Microservice Systems via Hierarchical Reinforcement Learning from Human Feedback](https://doi.org/10.1145/3580305.3599934)|Lu Wang, Chaoyun Zhang, Ruomeng Ding, Yong Xu, Qihang Chen, Wentao Zou, Qingjun Chen, Meng Zhang, Xuedong Gao, Hao Fan, Saravan Rajmohan, Qingwei Lin, Dongmei Zhang||In microservice systems, the identification of root causes of anomalies is imperative for service reliability and business impact. This process is typically divided into two phases: (i)constructing a service dependency graph that outlines the sequence and structure of system components that are invoked, and (ii) localizing the root cause components using the graph, traces, logs, and Key Performance Indicators (KPIs) such as latency. However, both phases are not straightforward due to the highly dynamic and complex nature of the system, particularly in large-scale commercial architectures like Microsoft Exchange. In this paper, we propose a new framework that employs Hierarchical Reinforcement Learning from Human Feedback (HRLHF) to address these challenges. Our framework leverages the static topology of the microservice system and efficiently employs the feedback of engineers to reduce uncertainty in the discovery of the service dependency graph. The framework utilizes reinforcement learning to reduce the number of queries required from O(N 2 ) to O(1), enabling the construction of the dependency graph with high accuracy and minimal human effort. Additionally, we extend the discovered dependency graphs to window causal graphs that capture the characteristics of time series over a specified time period, resulting in improved root cause analysis accuracy and robustness. Evaluations on both real datasets from Microsoft Exchange and synthetic datasets with injected anomalies demonstrate superior performance on various metrics compared to state-of-the-art methods. It is worth mentioning that, our framework has been integrated as a crucial component in Microsoft M365 Exchange service.|在微服务系统中，识别异常的根本原因对于服务的可靠性和业务影响是必不可少的。这个过程通常分为两个阶段: (i)构建一个服务依赖关系图，概述被调用的系统组件的顺序和结构; (ii)使用图、跟踪、日志和关键性能指标(KPI)(比如延迟)本地化根源组件。然而，由于系统的高度动态性和复杂性，这两个阶段并不简单，特别是在 Microsoft Exchange 这样的大型商业体系结构中。在本文中，我们提出了一个新的框架，利用人类反馈(HRLHF)的分层强化学习来解决这些挑战。我们的框架利用了微服务系统的静态拓扑结构，并有效地利用了工程师的反馈来减少服务依赖图发现中的不确定性。该框架利用强化学习来减少从 O (n2)到 O (1)所需的查询数量，从而能够以高精度和最小的人工成本构建依赖关系图。此外，我们将发现的依赖关系图扩展到窗口因果关系图，以捕获特定时间段内时间序列的特征，从而提高根本原因分析的准确性和鲁棒性。对来自 MicrosoftExchange 的实际数据集和注入异常的合成数据集的评估表明，与最先进的方法相比，在各种指标上具有更好的性能。值得一提的是，我们的框架已经集成为 MicrosoftM365Exchange 服务中的一个关键组件。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Root+Cause+Analysis+for+Microservice+Systems+via+Hierarchical+Reinforcement+Learning+from+Human+Feedback)|0|
|[Knowledge Based Prohibited Item Detection on Heterogeneous Risk Graphs](https://doi.org/10.1145/3580305.3599852)|Tingyan Xiang, Ao Li, Yugang Ji, Dong Li||With the popularity of online shopping in recent years, various prohibited items are continuously attacking e-commerce portals. Searching and deleting such risk items online has played a fundamental role in protecting the health of e-commerce trades. To mitigate negative impact of limited supervision and adversarial behaviors of malicious sellers, current state-of-the-art work mainly introduces heterogeneous graph neural network with further improvements such as graph structure learning, pairwise training mechanism, etc. However, performance of these models is highly limited since domain knowledge is indispensable for identifying prohibited items but ignored by these methods. In this paper, we propose a novel Knowledge Based Prohibited item Detection system (named KBPD) to break through this limitation. To make full use of rich risk knowledge, the proposed method introduces the Risk-Domain Knowledge Graph (named RDKG), which is encoded by a path-based graph neural network method. Furthermore, to utilize information from both the RDKG and the Heterogeneous Risk Graph (named HRG), an interactive fusion framework is proposed and further improves the detection performance. We collect real-world datasets from the largest Chinese second-hand commodity trading platform, Xianyu. Both offline and online experimental results consistently demonstrate that KBPD outperforms the state-of-the-art baselines. The improvement over the second-best method is up to 22.67% in the AP metric.|随着近年来网上购物的普及，各种违禁商品不断攻击电子商务门户网站。在网上搜索和删除此类风险项目，对保护电子商务行业的健康发挥了根本性作用。为了减轻有限监管和恶意卖方对抗行为的负面影响，当前的研究主要是引入异构图神经网络，并对其进行了图结构学习、成对训练机制等改进。然而，这些模型的性能是高度有限的，因为领域知识是必不可少的，以确定违禁项目，但忽视了这些方法。为了突破这一局限，本文提出了一种新的基于知识的违禁品检测系统(KBPD)。为了充分利用丰富的风险知识，该方法引入了风险领域知识图(RDKG) ，采用基于路径的图神经网络方法对其进行编码。此外，为了同时利用 RDKG 和异构风险图(HRG)的信息，提出了一种交互式融合框架，进一步提高了检测性能。我们从中国最大的二手商品交易平台 Xianyu 收集真实世界的数据集。离线和在线实验结果一致表明，KBPD 优于最先进的基线。在 AP 指标中，相对于次优方法的改进率高达22.67% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Based+Prohibited+Item+Detection+on+Heterogeneous+Risk+Graphs)|0|
|[A Data-Driven Decision Support Framework for Player Churn Analysis in Online Games](https://doi.org/10.1145/3580305.3599759)|Yu Xiong, Runze Wu, Shiwei Zhao, Jianrong Tao, Xudong Shen, Tangjie Lyu, Changjie Fan, Peng Cui||Faced with saturated market and fierce competition of online games, it is of great value to analyze the causes of the player churn for improving the game product, maintaining the player retention. A large number of research efforts on churn analysis have been made into churn prediction, which can achieve a sound accuracy benefiting from the booming of AI technologies. However, game publishers are usually unable to apply high-accuracy prediction methods in practice for preventing or relieving the churn due to the lack of the specific decision support (e.g., why they leave and what to do next). In this study, we fully exploit the expertise in online games and propose a comprehensive data-driven decision support framework for addressing game player churn. We first define the churn analysis in online games from a commercial perspective and elaborate the core demands of game publishers for churn analysis. Then we employ and improve the cutting-edge eXplainable AI (XAI) methods to predict player churn and analyze the potential churn causes. The possible churn causes can finally guide game publishers to make specific decisions of revision or intervention in our designed procedure. We demonstrate the effectiveness and high practical value of the framework by conducting extensive experiments on a real-world large-scale online game, Justice PC. The whole decision support framework, bringing interesting and valuable insights, also receives quite positive reviews from the game product and operation teams. Notably, the whole pipeline is readily transplanted to other online systems for decision support to address similar issues.|面对网络游戏市场的饱和和激烈竞争，分析网络游戏玩家流失的原因，对于提高网络游戏产品质量，保持网络游戏玩家的持久性具有重要的参考价值。人工智能技术的蓬勃发展，使得人工智能的预测精度得到了很大的提高。然而，由于缺乏具体的决策支持(例如，他们为什么离开以及下一步做什么) ，游戏发行商通常无法在实践中应用高精度的预测方法来防止或缓解流失。在这项研究中，我们充分利用在线游戏的专业知识，并提出了一个全面的数据驱动的决策支持框架，以解决游戏玩家流失。我们首先从商业角度定义了在线游戏的流失分析，并阐述了游戏发行商对流失分析的核心要求。然后采用改进的可解释人工智能(XAI)方法对球员流失进行预测，分析潜在的流失原因。可能的变动原因，最终可以指导游戏发行商作出具体的决定，修订或干预我们的设计程序。通过在现实大型网络游戏 Justice PC 上进行广泛的实验，验证了该框架的有效性和较高的实用价值。整个决策支持框架，带来了有趣和有价值的见解，也从游戏产品和运营团队得到了相当积极的评价。值得注意的是，整个流水线很容易移植到其他在线系统中，以便为解决类似问题提供决策支持。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Data-Driven+Decision+Support+Framework+for+Player+Churn+Analysis+in+Online+Games)|0|
|[Multi Datasource LTV User Representation (MDLUR)](https://doi.org/10.1145/3580305.3599871)|Junwoo Yun, Wonryeol Kwak, Joohyun Kim||In this paper, we propose a novel user representation methodology called Multi Datasource LTV User Representation (MDLUR). Our model aims to establish a universal user embedding for downstream tasks, specifically lifetime value (LTV) prediction on specific days after installation. MDLUR uses a combination of various data sources, including user information, portrait, and behavior data from the first n days after installation of the social casino game "Club Vegas Slots" developed by Bagelcode. This model overcomes the limitation of conventional approaches that struggle with effectively utilizing various data sources or accurately capturing interactions in sparse datasets. MDLUR adopts unique model architectures tailored to each data source. Coupled with robust dimensionality reduction techniques, this model succeeds in the effective integration of insights from various data sources. Comprehensive experiments on real-world industrial data demonstrate the superiority of the proposed methods compared to SOTA baselines including Two-Stage XGBoost, WhalesDector, MSDMT, and BST. Not only did it outperform these models, but it has also been efficiently deployed and tested in a live environment using MLOps demonstrating its maintainability. The representation may potentially be applied to a wide range of downstream tasks, including conversion, churn, and retention prediction, as well as user segmentation and item recommendation.|本文提出了一种新的用户表示方法——多数据源 LTV 用户表示(MDLUR)。我们的模型旨在建立一个通用的用户嵌入下游任务，特别是生命周期价值(LTV)预测安装后的具体天数。MDLUR 使用各种数据源的组合，包括用户信息、肖像和行为数据，这些数据来自安装 Bagelcode 开发的社交赌场游戏“ Club Vegas Slot”后的第一个 n 天。该模型克服了传统方法的局限性，传统方法难以有效地利用各种数据源或准确地捕获稀疏数据集中的交互。MDLUR 采用针对每个数据源量身定制的独特模型体系结构。加上强大的降维技术，这个模型成功地有效地整合了来自各种数据源的见解。对真实世界工业数据的综合实验表明，与包括两阶段 XgBoost、 WhalesDector、 MSDMT 和 BST 在内的 SOTA 基线相比，提出的方法具有优越性。它不仅性能优于这些模型，而且已经在实时环境中使用 MLOps 进行了有效的部署和测试，证明了它的可维护性。这种表示可能会应用于广泛的下游任务，包括转换、流失和保留预测，以及用户细分和项目推荐。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi+Datasource+LTV+User+Representation+(MDLUR))|0|
|[Understanding the Semantics of GPS-based Trajectories for Road Closure Detection](https://doi.org/10.1145/3580305.3599926)|Jiasheng Zhang, Kaiqiang An, Guoping Liu, Xiang Wen, Runbo Hu, Jie Shao||The accurate detection of road closures is of great value for real-time updating of digital maps. The existing methods mainly follow the paradigm of detecting the drastic changes in traffic statistical values (e.g., traffic flow), but they may lead to misidentifying since 1) drastic changes of traffic statistical values are hard to be observed in low-heat roads where the passing vehicles are sparse; 2) statistical values are sensitive to noise (e.g., traffic flow for tiny roads and tunnels is prone to miscounting); and 3) statistical values are naturally delayed, and misidentifying may occur when they have not yet shown significant changes. Surprisingly, since GPS-based trajectories can also exhibit significant abnormal patterns for road closures and have the superiority in fine granularity and timeliness, they can naturally tackle the above challenges. In this paper, we present a novel road closure detection framework based on mining the semantics of trajectories, called T-Closure. We first construct a heterogeneous graph based on the trajectory and the planned route to extract the spatial-topological property of each trajectory, where a node-level auxiliary task is proposed to guide the learning of feature encoders. A multi-view heterogeneous graph neural network (MVH-GNN) with a graph-level auxiliary task is then introduced to capture the semantics of trajectories, where intra-category relevance and inter-category interaction are both considered. Finally, a sequence-level auxiliary task refines the ability of LSTM in modeling the semantic relevance among trajectories while enhancing the robustness of our framework. Experiments on four real-world road closure datasets demonstrate the superiority of T-Closure. Online performance shows that T-Closure can detect 7000+ closure events monthly, with a delay within 1.5 hours.|道路封闭的准确检测对数字地图的实时更新具有重要意义。现有的方法主要是按照检测交通统计数值(例如交通流量)急剧变化的模式，但这些方法可能会导致错误识别，因为1)在过往车辆稀少的低热道路上，难以观察到交通统计数值的急剧变化; 2)统计数值对噪音敏感(例如小型道路及隧道的交通流量容易出现错误计算) ; 3)统计数值自然会被延迟，而在尚未显示明显变化时，可能会出现错误识别的情况。令人惊讶的是，由于基于全球定位系统的轨迹还可能显示出明显的道路封闭异常模式，并且具有细粒度和及时性的优势，因此它们自然能够应对上述挑战。本文提出了一种基于轨迹语义挖掘的道路闭合检测框架，称为 T 闭合。首先构造一个基于轨迹和规划路径的异构图，提取每个轨迹的空间拓扑特性，提出一个节点级辅助任务来指导特征编码器的学习。然后引入一个具有图级辅助任务的多视图异构图神经网络(MVH-GNN)来捕获轨迹的语义，其中既考虑了类别内相关性，又考虑了类别间的相互作用。最后，一个序列级的辅助任务提高了 LSTM 建模轨迹间语义相关性的能力，同时增强了框架的鲁棒性。通过对四个实际道路封闭数据集的实验，验证了 T 封闭算法的优越性。在线性能表明，T-Closure 每月可以检测到7000多个闭合事件，延迟时间在1.5小时内。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+the+Semantics+of+GPS-based+Trajectories+for+Road+Closure+Detection)|0|
|[TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations at Twitter](https://doi.org/10.1145/3580305.3599921)|Xinyang Zhang, Yury Malkov, Omar Florez, Serim Park, Brian McWilliams, Jiawei Han, Ahmed ElKishky|Univ Illinois, Urbana, IL 61801 USA; Twitter Cortex, San Francisco, CA 94103 USA|Pre-trained language models (PLMs) are fundamental for natural language processing applications. Most existing PLMs are not tailored to the noisy user-generated text on social media, and the pre-training does not factor in the valuable social engagement logs available in a social network. We present TwHIN-BERT, a multilingual language model productionized at Twitter, trained on indomain data from the popular social network. TwHIN-BERT differs from prior pre-trained language-models as it is trained with not only text-based self-supervision but also with a social objective based on the rich social engagements within a Twitter heterogeneous information network (TwHIN). Our model is trained on 7 billion tweets covering over 100 distinct languages, providing a valuable representation to model short, noisy, user-generated text. We evaluate our model on various multilingual social recommendation and semantic understanding tasks and demonstrate significant metric improvement over established pre-trained language models. We open-source TwHIN-BERT and our curated hashtag prediction and social engagement benchmark datasets to the research community(1).|预训练语言模型(PLM)是自然语言处理应用程序的基础。大多数现有的 PLM 都没有针对社交媒体上用户生成的嘈杂文本进行调整，而且预先培训也没有考虑到社交网络中可用的有价值的社交参与日志。我们介绍 TwHIN-BERT，一个在 Twitter 上生产的多语言模型，它使用流行社交网络的域名数据进行训练。TwHIN-BERT 不同于先前预先训练的语言模型，因为它不仅受到基于文本的自我监督的训练，而且还受到基于 Twitter 异构信息网络(TwHIN)内丰富的社会参与的社会目标的训练。我们的模型是在涵盖100多种不同语言的70亿条 tweet 上进行训练的，为建立简短、嘈杂、用户生成的文本模型提供了有价值的表示。我们评估了我们的模型在各种多语言社会推荐和语义理解任务上的表现，并证明了在已建立的预先训练的语言模型上有显著的度量改进。我们开源的 TwHIN-BERT 和我们策划的标签预测和社会参与基准数据集提供给研究界(1)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TwHIN-BERT:+A+Socially-Enriched+Pre-trained+Language+Model+for+Multilingual+Tweet+Representations+at+Twitter)|0|
|[Online Few-Shot Time Series Classification for Aftershock Detection](https://doi.org/10.1145/3580305.3599879)|Sheng Zhong, Vinicius M. A. Souza, Glenn Eli Baker, Abdullah Mueen||Seismic monitoring systems sift through seismograms in real-time, searching for target events, such as underground explosions. In this monitoring system, a burst of aftershocks (minor earthquakes occur after a major earthquake over days or even years) can be a source of confounding signals. Such a burst of aftershock signals can overload the human analysts of the monitoring system. To alleviate this burden at the onset of a sequence of events (e.g., aftershocks), a human analyst can label the first few of these events and start an online classifier to filter out subsequent aftershock events. We propose an online few-shot classification model FewSig for time series data for the above use case. The framework of FewSig consists of a selective model to identify the high-confidence positive events which are used for updating the models and a general classifier to label the remaining events. Our specific technique uses a %two-level decision tree selective model based on sliding DTW distance and a general classifier model based on distance metric learning with Neighborhood Component Analysis (NCA). The algorithm demonstrates surprising robustness when tested on univariate datasets from the UEA/UCR archive. Furthermore, we show two real-world earthquake events where the FewSig reduces the human effort in monitoring applications by filtering out the aftershock events.|地震监测系统通过实时筛选地震图，搜索目标事件，如地下爆炸。在这个监测系统中，一连串的余震(大地震发生几天甚至几年后的小地震)可能是混淆信号的来源。这种余震信号的爆发会使监测系统的人类分析员超负荷工作。为了减轻一系列事件(例如余震)发生时的这种负担，人类分析师可以标记前几个事件，并启动在线分类器来过滤后续的余震事件。针对上述用例，我们提出了一种针对时间序列数据的在线少镜头分类模型 FewSig。FewSig 的框架包括一个选择性模型来识别用于更新模型的高置信度正事件，以及一个通用分类器来标记剩余事件。我们的具体技术使用了基于滑动 DTW 距离的% 两级决策树选择模型和基于邻域分量分析(NCA)距离度量学习的通用分类器模型。该算法在 UEA/UCR 存档的单变量数据集上进行测试时表现出惊人的鲁棒性。此外，我们展示了两个真实世界的地震事件，其中 FewSig 通过过滤掉余震事件减少了人们监测应用程序的工作量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+Few-Shot+Time+Series+Classification+for+Aftershock+Detection)|0|
|[A Feature-Based Coalition Game Framework with Privileged Knowledge Transfer for User-tag Profile Modeling](https://doi.org/10.1145/3580305.3599761)|Xianghui Zhu, Peng Du, Shuo Shao, Chenxu Zhu, Weinan Zhang, Yang Wang, Yang Cao||User-tag profiling is an effective way of mining user attributes in modern recommender systems. However, prior researches fail to extract users' precise preferences for tags in the items due to their incomplete feature-input patterns. To convert user-item interactions to user-tag preferences, we propose a novel feature-based framework named Coalition Tag Multi-View Mapping (CTMVM), which identifies and investigates two special features, Coalition Feature and Privileged Feature. The former indicates decisive tags in each click where relationships between tags in one item are treated as a coalition game. The latter represents highly informative features that only occur during training. For the coalition feature, we adopt Shapley Value based Empowerment (SVE) to model the tags in items with a game-theoretic paradigm and charge the network to straight master user preferences for essential tags. For the privileged feature, we present Privileged Knowledge Mapping (PKM) to explicitly distill privileged feature knowledge for each tag into one single embedding, which assists the model in predicting user-tag preferences at a more fine-grained level. However, the barren capacity of single embeddings limits the diverse relations between each tag and different privileged features. Therefore, we further propose Adaptive Multi-View Mapping (AMVM) model to enhance effect by handling multiple mapping networks. Excellent offline experiment results on two public and one private datasets show the out-standing performance of CTMVM. After the deployment on Alibaba large-scale recommendation systems, CTMVM achieved improvement by 10.81% and 6.74% in terms of Theme-CTR and Item-CTR respectively, which validates the effectiveness of taking in the two particular features for training.|用户标签剖析是现代推荐系统中挖掘用户属性的一种有效方法。然而，由于特征输入模式的不完整性，以往的研究未能提取出用户对标签的精确偏好。为了将用户交互转化为用户标签偏好，提出了一种基于特征的联盟标签多视图映射(Coalition Tag Multi-View Mapping，CTMVM)框架。前者表示每次点击决定性的标签，其中一个项目中的标签之间的关系被视为一个联盟游戏。后者代表了只有在训练期间才会出现的高度信息化的特征。对于联盟功能，我们采用基于 Shapley 值的授权(SVE)模型的项目中的标签与博弈论的范式和收费网络直接主人的用户偏好的重要标签。对于特权特征，我们提出了特权知识映射(PKM) ，将每个标签的特权知识显式地提取为一个单独的嵌入，这有助于模型在更细粒度的层次上预测用户标签的偏好。然而，单个嵌入容量的不足限制了每个标签和不同特权特性之间的不同关系。因此，我们进一步提出自适应多视点映射(AMVM)模型，通过处理多个映射网络来增强效果。在两个公共数据集和一个私有数据集上的优秀离线实验结果表明，CTMVM 具有出色的性能。在使用阿里巴巴大型推荐系统后，CTMVM 的主题-点击率和项目-点击率分别提高了10.81% 和6.74% ，验证了采用这两个特定功能进行培训的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Feature-Based+Coalition+Game+Framework+with+Privileged+Knowledge+Transfer+for+User-tag+Profile+Modeling)|0|
|[Fairness in Graph Machine Learning: Recent Advances and Future Prospectives](https://doi.org/10.1145/3580305.3599555)|Yushun Dong, Oyku Deniz Kose, Yanning Shen, Jundong Li|; netherlands environmental assessment agency; vienna university of economics and business; university of reading; potsdam institute for climate impact research; university of grenoble; university of cambridge; polish academy of sciences; vu university amsterdam|Scenarios are used to explore the consequences of different adaptation and mitigation strategies under uncertainty. In this paper, two scenarios are used to explore developments with (1) no mitigation leading to an increase of global mean temperature of 4 °C by 2100 and (2) an ambitious mitigation strategy leading to 2 °C increase by 2100. For the second scenario, uncertainties in the climate system imply that a global mean temperature increase of 3 °C or more cannot be ruled out. Our analysis shows that, in many cases, adaptation and mitigation are not trade-offs but supplements. For example, the number of people exposed to increased water resource stress due to climate change can be substantially reduced in the mitigation scenario, but adaptation will still be required for the remaining large numbers of people exposed to increased stress. Another example is sea level rise, for which, from a global and purely monetary perspective, adaptation (up to 2100) seems more effective than mitigation. From the perspective of poorer and small island countries, however, stringent mitigation is necessary to keep risks at manageable levels. For agriculture, only a scenario based on a combination of adaptation and mitigation is able to avoid serious climate change impacts. Keywords Scenarios Integrated assessment Climate change Mitigation Adaptation Climate impacts 1 Introduction Scenario analysis forms a very important tool in the assessment of climate change and climate change policy, allowing analysts to explore the complex and uncertain future interactions between factors like economic development, greenhouse gas (GHG) emissions, climate and ecosystems. Together these factors determine the need and the possibilities for mitigation and adaptation policy. Scenarios can also act as a means to harmonize assumptions across very different research communities that are involved in the fields of climate research, allowing a better comparison of their results. As such, scenarios have been used extensively in both mitigation and adaptation studies (see Metz et al., 2007; Parry et al., 2007 ) (especially the scenarios from Special Report on Emission Scenarios (SRES) ( Nakicenovic et al., 2000 )). Moss et al. (2010) point out that since the SRES information requirements from scenario analysis are changing. First, there is an increasing interest in exploring the relationships between adaptation and mitigation. As indicated by Moss et al. (2010) , this would require a further integration of information across the different analytical traditions involved in climate research. Secondly, there is also an increased interest in scenarios that explicitly explore the impact of climate policies in addition to the climate policy-free scenarios explored so far. Specifically, there is a strong interest in being able to evaluate the “costs” and “benefits” of long-term climate goals vis-à-vis the situation without climate policy. In this paper, we follow this line of thought and explore how scenario analysis can contribute to a joint assessment of future adaptation and mitigation strategies. Such a joint assessment can be useful for several reasons: (1) the preferred mitigation strategy depends on expected climate impacts and adaptation costs, (2) it takes account of the limitations of adaptation to climate change, (3) some adaptation and mitigation strategies may interact and (4) finally, impacts of climate change may have important feedbacks that need to be taken into account. Such analysis is most useful at a strategic level, and not for individual adaptation (or mitigation) decisions. Given this purpose, we discuss in the paper two main scenarios that include elements of adaptation and mitigation strategies (see further in this paper), resulting in an increase of global mean temperature of 4 °C and 2 °C by the end of this century. These two temperature levels have started to become iconic numbers, representing a potential outcome in the situation without mitigation policy (4 °C) and the temperature target of international climate negotiations (2 °C) ( Copenhagen Accord, 2009 ). Arguably, understanding the implications of these two temperature levels is essential if political leaders are to make informed choices about the balance between mitigation, adaptation and climate impacts ( Environmental Change Institute, 2009 ). Integrated assessment of mitigation and adaptation strategies is hampered by methodological differences. Integrated assessment models have difficulties describing adaptation processes given the importance of local circumstances ( Patt et al., 2010 ). A practical problem is that to date a considerable part of the impact literature has concentrated on impacts under no-policy scenarios (exceptions include Arnell et al., 2002; Bakkenes et al., 2006; Hayashi et al., 2010; Krol et al., 1997; Nicholls and Lowe, 2004 ). This paper therefore presents a generalised scenario assessment based on coupled pieces of information – but without pretending to be complete or to be fully integrated. As a learning-by-doing exercise, the paper intends to show important differences between a 4 °C and a 2 °C world, but also to identify some of the practical issues involved in performing integrated scenario analysis. This implies that the most important advancement compared to existing literature is that we present a multi-sector analysis based on consistent scenarios. Given the state-of-the-art of current integrated assessment models, the experiments have been done using several loosely coupled models. As a result, several important linkages could not be addressed such as between the adaptation responses for agriculture, which may involve irrigation (see Section 5.3 ) and water demand (Section 5.4 ). In fact, an important question raised in the paper is whether a fully integrated analysis is needed or whether partial integration is sufficient. The paper is organized as follows: we first discuss some of the methodological complications in developing scenarios that can provide information for both adaptation and mitigation policy decisions. Next, we discuss the differences between the two main scenarios in terms of socio-economic drivers (Sections 3 and 4 ). In Section 5 we explore the potential consequences of adaptation and mitigation strategies on various impacts of climate change. 2 Assessment of climate strategies and scenario development (theory and methods) 2.1 Different strategies in response to climate change Climate change and the responses to it can lead to three forms of costs (not necessarily monetary): (1) the (residual) costs of climate impacts, (2) the costs of adaptation and (3) the costs of mitigation. At least theoretically, this corresponds to three different strategies: (1) “laissez faire” (accept climate change), (2) focus on adaptation and (3) focus on mitigation as illustrated conceptually in Fig. 1 (see also Klein et al., 2007 ). While Fig. 1 suggests that the costs and benefits of mitigation, adaptation and residual damages can be traded-off against each other, there are conceptual and analytical problems that complicate such an approach. These relate to spatial and temporal scales, and risks and uncertainty ( Swart and Raes, 2007 ). Mitigation and adaptation are processes that take place at different spatial s cales. While mitigation action is often taken at the national or local scale, the benefits are shared globally. As a result, a critical factor in the success and costs of climate policy is the degree of international cooperation ( Barker et al., 2009; Clarke et al., 2010; van Vliet et al., 2009; van Vuuren et al., 2009 ). For adaptation, in contrast, both costs and benefits occur on multiple scales from local to national and even international. An enabling environment at a larger scale can still enhance adaptation at a smaller scale (e.g. local capacity-building funded by international financing mechanisms). For these kinds of reasons, assessment of mitigation tend to concentrate on the global level, while by contrast, adaptation research is mostly focusing at the local scale. The dynamics over time of mitigation and adaptation is also an important factor. Stringent mitigation scenarios typically require strong, early reduction of emissions. Climate change impacts of these scenarios, however, will in the short-term (first decades) hardly differ from those in scenarios without climate change policy due to the large inertia within the climate system. In contrast, some associated impacts (e.g. co-benefits in reduced local air pollution) may be realized at a much faster pace. Adaptation measures are likely to yield private and social benefits over the near-term. For instance, simple adaptation measures such as air conditioning can bring clear short-term benefits. Some important exceptions exist which may require decades to implement, such as changes in spatial planning or large-scale engineering works for flood protection (see Hallegatte, 2009 ). Other important factors are risk and uncertainty . Our understanding of climate change faces many uncertainties. Key uncertainties to be identified comprise epistemic, data, model, and ontic uncertainties ( Schneider and Kuntz-Duriseti, 2002; van Vuuren et al., 2008a ). Examples of factors that involve uncertainty are (i) future emissions, (ii) the climate system, (iii) future vulnerability and exposure to climate risks and (iv) mitigation costs. Taking mitigative action reduces some uncertainties, since it reduces the originating sources of climate change and reveals the actual mitigation costs ( Barker, 2003; Piani et al., 2005 ). Mitigation may, however, also add to risks. For example, bio-energy, if implemented unsustainably, may offset one set of risks (climate change) while creating another set of different risks (biodiversity loss and reduced food security). One way of dealing with risks is to include assessments of probabilities. This is often done using past evidence, extrapolated to cover specific future circumstances. Other uncertainties (for instance unknowable shocks and surprises) are more difficult to deal with in quantitative sense, but justify acknowledgement of ignorance. Scenarios can be used to explore the potential for extreme events and the robustness of various policy portfolios but this is not often done ( Berkhout et al., 2002 ). Traditionally, the disciplines involved in mitigation research and adaptation research have different ways of describing uncertainty. While mitigation research often uses quantitative methods and concentrates on mean estimates, adaptation research often focuses more on qualitative descriptions of uncertainty and concentrates on the risks of hazardous events even if these have a low probability of occurrence. These different perceptions of uncertainty may complicate an integrated assessment of different strategies ( Swart et al., 2009 ). 2.2 Types of scenarios We can characterize scenarios into different classes based on the considerations about mitigation and adaptation. First, we define a baseline scenario, as a trajectory of events assuming no major feedbacks from climate change and no specific policy efforts on either mitigation or adaptation (such a scenario may still include many actions that indirectly influence the ability to mitigate or adapt to climate change; for instance, increasing income levels can be expected to coincide with greater investment in health services reducing the risks of climate-related diseases such as malaria). The main purpose of this type of scenario is analytical, serving as a point of reference for other scenarios. Second, adaptation scenarios describe a world in which societies are responding to climate change impacts. Their purpose is to explore the type of technologies and policies required to adapt to climate change, the avoided damage and the associated costs. Adaptation includes so-called autonomous adaptation (i.e. actions that occur without specific government action) and planned adaptation. Third, mitigation scenarios describe a world including policies aiming to limit climate change. Their purpose is to explore the type of technologies and policies required to minimize climate change and the associated costs. As there will always be remaining impacts, the fourth set, adaptation and mitigation scenarios combine both types of responses to climate change. Possibly, this fourth category of scenarios could re-order policy options according to the synergies that might exists between adaptation and mitigation options, e.g. for some re-afforestation options. Each of these scenarios is connected to a broader social, political and cultural context in which they are assumed to arise. In exploring a preferred mix of mitigation, adaptation and residual damage, two main approaches exist: (i) the impact and risk-based approach that describes potential impacts as function of global mean temperature increase (and thus mitigation), and (ii) the cost–benefit analysis, which identifies monetary costs and benefits in order to maximize welfare (see for instance Nordhaus, 2008; Tol, 2002c ). In both cases, we believe it to be more useful and reflective of the issue to describe the relationships between different response strategies than to seek to determine an optimum. Given the complexities and uncertainties laid out in Section 2.1 , we believe no optimal mitigation, adaptation or combined strategy can be pursued in reality. 2.3 Integrated analysis An integrated analysis of mitigation and adaptation can be achieved in different ways: e.g., by using one single, so-called integrated assessment model, or by exchanging information between different models and disciplines, assessing available literature and making results comparable. Both methods are organized around the cause–effect chain of climate change, i.e. describing the relationship between economic activities (income, energy use, agriculture, etc.), emissions, climate change and impacts – and the related feedbacks ( Fig. 2 ). The scheme in fact also forms the backbone of information flows around scenarios for the IPCC reports ( Moss et al., 2010 ). Scenarios are developed first by integrated assessment and emission modelers (focusing on economic driving forces, energy and land use and GHG emissions (IPCC “Working Group III”)). Subsequently, the emission trajectories are used in climate models to assess the impacts of climate change (IPCC “Working Group I”). Finally, the scenarios are used for impact, adaptation and vulnerability analyses (IPCC “Working Group II”). The involvement of different research disciplines and working groups implies that it is difficult to account for feedbacks between the different areas. Integrated Assessment models capture only a limited number of the possible feedbacks (frequently omitted feedbacks include the impact of food and water security on population and economic drivers; relationships between water scarcity and food production, impact of climate change on energy use, etc.). Ignoring (some of) these feedbacks may be reasonable if they are not substantial enough to significantly influence the system. For analytical reasons, there are major advantages to organizing scenario development within disciplinary fields and consider a limited number of feedbacks. It allows researchers to focus on elements of the chain that they understand well and to add the required amount of detail, without being confronted with the complications of interlinkages. However, this may change in a situation of increased focus on integrated analysis of mitigation and adaptation strategies. Some examples of why an integrated approach may be necessary are: i. Climate impacts, such as those triggered by extreme events, may be so severe that they undermine the economic assumptions of the original scenario; ii. Climate impacts could be substantial in agriculture so that estimates of land-use related emissions not taking impacts into account might be wrong, and the mitigation potential of bio-energy may be affected; and iii. There may be competing claims for land areas attractive for both mitigation and adaptation purposes. Thus, an interesting question is whether the need for more integrated analysis is so urgent that more complex modes of integration are needed (interactive coupling of models; one complex model), or whether the impacts can be handled separately simplifying the analysis framework. The time horizon and the decision focus may also be important here, e.g. whether potential tipping points are taken into account ( Lenton et al., 2008 ). The few available studies that have looked into this question seem to suggest that in most sectors the adaptation implications of any mitigation project are small as well as the emissions generated by most adaptation activities ( Klein et al., 2007 ). The most integrated analyses to date come from the cost–benefit oriented integrated assessment models like FUND, DICE and MERGE ( Manne and Richels, 2005; Nordhaus, 2008; Tol, 2002c ) – but these models typically aggregated climate impacts into a limited amount of rather abstract damage functions. We believe that over time, with growing intensity of both mitigation and adaptation measures across many sectors, the need for joint assessment with sufficient detail will intensify. The scenarios presented here, based on the current state of the art in modeling and scenario development, take a first step. The same scenarios are used in one assessment for mitigation and impact assessment and we explicitly address mitigation and adaptation strategies (either as part of the scenarios or within the models used for the different impacts). However, many feedbacks are not accounted for. We come back at the end of the paper to the role of more integrated (but also more complex) scenarios. 2.4 Methods used in this paper As described above, several types of scenarios can be identified: baseline, mitigation, adaptation and adaptation–mitigation scenarios. These scenario types are also presented in this paper. For the baseline/adaptation scenario, we assume intermediate assumptions for most socio-economic drivers. Scenarios assumptions are described in Sections 3 and 4 . The scenarios do not include mitigation, leading to a global mean temperature increase of 4 °C above pre-industrial levels by 2100. While we describe possible impacts and adaptation in these scenarios, we do not include feedbacks on the original drivers. In the mitigation scenarios, stringent mitigation efforts are included leading to a global mean temperature increase of 2 °C. Using the median value for climate sensitivity given by IPCC of 3 °C ( Meehl et al., 2007 ), this translates into a stabilization level of around 450 ppm CO 2 -equivalent (CO 2 -equiv.). The impacts of climate policy on economic drivers are not accounted for – but several other relationships are coupled (e.g. land use). In most of the paper, we thus ignore potential impacts of climate change and climate policy on the economic assumptions. In Section 5.8 , however, we discuss their impacts within a simple, economic model (FAIR) to provide some insight in the possible size of the economic consequences on the global scale. Several model tools are used. The scenarios are mainly developed using the IMAGE integrated assessment model ( Bouwman et al., 2006 ). The IMAGE model describes developments in energy and land use in the 21st century based on assumptions for population and the world economy, combined with assumptions for technology development and consumption patterns. The model projects climate change (as indexed by global mean temperature change and sea level rise) at the global scale, and constructs spatial scenarios for change in monthly temperature and rainfall at a 0.5° × 0.5° grid by pattern-scaling downscaled climate model patterns. The output of IMAGE is used in the model DIVA to describe sea-level rise; in the global hydrology model Mac-PDM to estimate consequences for water stress; in the TIMER energy model to estimate implications for heating and cooling demand; in the MARA/ARMA malaria suitability model for impacts on malaria and in the FAIR model for a monetary cost–benefit analysis. Moreover, we discuss more generally the implications for agriculture (based on IPCC AR4) and extreme events. Appendix A provides a brief description of all models used. In our descriptions, we focus on the global level (in view of the limited space). Clearly, this leads to limitations in our discussion of adaptation. The experiments depend on the design each model and thus the number of scenarios that can be presented differs between different impacts. This implies that the study should be interpreted as a first illustration of an integrated assessment, and not as a holistic study on adaptation and its limits. 3 Results: socio-economic trends in the baseline scenario 3.1 Population development and economic growth We assume that population follows medium-fertility variant of the 2004 revision of the World Population Projections ( UN, 2005 ) up to 2050, and the UN's long-range medium projections up to 2100 ( Fig. 3 ). This implies that the global population steadily increases to almost 9.1 billion people by 2050 and stabilizes at about 9.2 billion people over the subsequent 50 years up to 2100. The scenario takes a middle ground within the range of population forecasting (see Fig. 3 ). For economic growth up to 2050, the scenario follows projections linked to the Cambridge model E3MG ( Barker and Scrieciu, 2010; Barker et al., 2008 ). The scenario was extended beyond 2050 using the economic growth projections of the SRES-based B2 scenario ( IMAGE-team, 2001 ). Quantitatively, the scenario is a medium to high economic growth scenario, which is mainly the result of optimistic growth assumptions for China and India. The OECD economies are projected to remain the richest in the world in per capita terms, but in terms of total economic activity the importance of developing regions grows rapidly. The growth of GDP per capita is between 0 and 2% per annum in Africa, the Middle East and Latin America. In Asia, it falls from the current high levels to 3% per annum in 2050. 3.2 Energy use and greenhouse gas emissions for the baseline scenario Energy use in the baseline scenario is made consistent with a baseline published by the European Commission ( EC, 2006 ). Despite a further decrease of energy intensity, world energy consumption more than doubles in the 2000–2050 period and increases by another 25% in the 2050–2100 period ( Fig. 4 ). Over the whole century, energy supply remains dominated by fossil fuels. While oil and natural gas production peak and decline during the century, the use of coal increases during the whole scenario period. Also non-fossil energy production increases rapidly. Nuclear energy use increases by a factor of two to three to 76 EJ over the period until 2100, the use of biomass increases strongly, while hydro-electricity production increases by about 60–80%. The largest relative increase is that of wind and solar energy; this rises from less than 1% of all non-fossil energy to between 10 and 14% in 2050. Total renewable energy use in 2050 is 120–140 EJ, and 190 EJ in 2100. The trends described above imply that emissions of CO 2 from energy activities more than double in the period to 2050, and rise by another third between 2050 and 2100 (see Fig. 3 ). As such, the scenario forms an intermediate baseline scenario within the literature range ( Fisher et al., 2007 ). Non-CO 2 GHGs (in particular methane) increase steadily in the period 2000–2050, but at a slower rate than CO 2 (as their driver, agriculture, is expected to grow more slowly than the energy sector). CO 2 emissions from land-use fall back to zero during the first half of the century. The area of agricultural land lies within the range of similar scenarios that have recently been published, although at the low end of the range ( Rose et al., 2007 ). 4 Results for the mitigation scenario and climate scenarios 4.1 Energy use and greenhouse gas emissions The mitigation scenario aims at stabilising GHGs at around 450 ppm CO 2 -equiv. (see also van Vuuren et al., 2007, 2010 ). The scenario allows for an initial overshoot of concentration to about 510 ppm CO 2 -equiv. Den Elzen and van Vuuren (2007) have shown earlier that a limited overshoot of concentration allows for meeting similar climate targets at lower costs. Emission reductions are achieved in various ways. One element is to increase energy efficiency, which reduces the total amount of energy use (a 20% reduction in 2050 compared to baseline) (see Fig. 4 ). The scenario also shows an increasing use of energy from non-fossil sources, which account for most of the growth in total energy use. Non-fossil energy use increases from about 15% of total primary energy use in 2010 to more than 30% in 2050 and is over 40% of the total by the end of the century. Most of this growth is due to an increase in bio-energy use. Carbon capture and storage is applied in most remaining stationary uses of fossil fuels. Finally, also non-carbon dioxide greenhouse gas emissions are reduced. As a result, global emissions peak around 2020, and reduce further with time. Emissions are reduced by more than 70% compared to the baseline in 2050 and more than 80% by 2100. The consequences of mitigation policies affect not only the energy sector, but also land use. Substantial additional land areas are used for afforestation and bio-energy (see Fig. 5 ). Model comparison studies show that the mitigation scenarios presented here are consistent with the current literature, although models show significant differences in the contribution of various reduction measures ( Clarke et al., 2010; Edenhofer et al., 2010 ). According to the IMAGE model calculations, the abatement costs of the emission reductions are in the order of 1–2% of GDP (i.e. the annual additional expenditures which can be compared to the current expenditure of around 1.5% of GDP on environmental policy in OECD countries) ( Fig. 6 ). The literature range of comparable scenarios is in the order 0.5–5.5% in 2100. Most studies agree that these additional expenditures would lead to a reduction of GDP. We discuss this further in Section 5.8 . 4.2 Climate change under the baseline and mitigation scenario The atmospheric GHG concentration and associated mean global temperature change resulting from the emissions of the two scenarios is shown in Fig. 7 (solid lines indicate best-guess values), based on the IMAGE model calculations. The IMAGE model uses the MAGICC model to calculate changes in global mean temperature. The MAGICC model was used earlier for similar IMAGE scenarios by van Vuuren et al. (2008b) to calculate trajectories for greenhouse gas concentration and temperature including uncertainty ranges. Here, the uncertainty ranges used for the MAGICC calculations were based on existing runs of more complex carbon cycle and climate models. We have used the implications for ranges in greenhouse gas concentration and temperature outcomes to also depict the uncertainty ranges here as is indicated by the shaded areas in this graph. For temperature, the wider shaded area indicates the uncertainty as result of uncertainty in the carbon cycle and climate sensitivity. For the baseline scenario, global mean temperature increases almost linearly to 2.1 °C above the pre-industrial levels in 2050 and to 3.7 °C in 2100 (uncertainty range 3–5 °C). In the mitigation scenario, the global mean temperature increase by 2100 is limited to 1.9 °C. Again, there is considerable uncertainty. Fig. 7 indicates that by the end of the century the mitigation case could also lead to a temperature increase of 2.6 °C compared to pre-industrial levels. As the mitigation scenario presented here is among the most stringent in the scientific literature (cf. Clarke et al., 2010; Edenhofer et al., 2010; Fisher et al., 2007 ), two important conclusions can be drawn. First, the analysis indicates that global warming can be moderated but not halted. Second, the observation that a stringent scenario could also lead to considerably greater climate change than 2 °C may imply that hedging adaptation policies against more warming might have considerable value. For example, such policies may be to ‘… aim for 2 °C, but prepare for 3 °C’. In the assessment of impacts below, we focus on the central climate change projections. Changes in mean monthly temperature and precipitation across the globe at the 0.5° × 0.5° scale, associated with the global average temperature changes, have been constructed by rescaling patterns derived from the HadCM2 climate model ( Fig. 8 ). These patterns show that the change in annual mean temperature is larger at high latitudes than at low latitudes, and show considerable spatial variation in change in rainfall. Considerable disagreement about the expected patterns of climate change exists, especially for precipitation: the impact results presented in this paper therefore represent only one possible outcome. 5 Results: impacts and adaptation in the different scenarios 5.1 Introduction IPCC's Fourth Assessment Report ( IPCC, 2007 ) gives an overview of climate impacts. Some of these impacts result from changes in average climate, but other impacts may result from changes in extreme events. Table 1 summarizes some of the impacts, for health, agriculture, water availability, coastal flooding, urban areas and energy system, and large-scale disruptions of the climate system (in contrast, biodiversity and ecosystem services have not been included). As noted earlier, most of the literature has treated climate change as “a gradual phenomena” ( Agrawala and Fankhauser, 2008 ). This is problematic for impacts characterized by low probabilities coupled with high impacts (see below). In this exploratory analysis, we sketch some of the impacts and adaptation requirements. We aimed to cover several key impacts mentioned in Table 1 , but the assessment was limited by the availability of models that could easily be coupled. Therefore, rather than intending to be exhaustive, the descriptions provide some indication of the magnitude of some impacts and key adaptation challenges. In presenting our results, we have used several new model runs based on the scenario discussed above (e.g. for malaria, water resources, sea-level rise, heating and cooling demand). We have, however, also assessed existing information from IPCC 4th Assessment Report in the context of the two scenarios presented here (temperature-related mortality, agriculture and extreme events). 5.2 Human health: temperature-related mortality and malaria Health impacts of climate change need to be seen in the context of other, more important drivers of human health, including lifestyle-related factors ( Hilderink et al., 2008 ). We focus here on temperature-related mortality and malaria. 5.2.1 Temperature-related mortality Temperature-related mortality impacts may occur via changes in extreme temperatures, changes in average temperatures, or in seasonal variation of temperatures, with the literature showing varying results. McMichael et al. (1996) made an estimation of temperature-related mortality using relative risk ratios, showing that there is an optimum temperature at which the death rate is lowest (also know as the U-shaped dose–response relation). If temperature increases, heat stress-related mortality increases, but cold-related mortality decreases. Tol (2002a) concluded that in monetary terms the reduction in cold-related mortality due to climate change outnumbers the increase in heat-related mortality. This conclusion is however, influenced by the approach used to value a life and also subject to the large uncertainties with respect to the relationships between average and regional temperatures and temperature and health. Adaptation may occur both by the adjustment of the human physiology to higher temperatures ( McMichael et al., 1996 ), changes in behavior and an increase of air conditioning use ( Kinney et al., 2008 ). Given the complexities in using dose–response relationships between temperature and mortality, we have not attempted to quantify these here. 5.2.2 Malaria Considerable attention has been paid to the relationship between malaria and climate change. In this paper, we also focus on climate-induced changes in malaria risks. Annually more than one million people, mostly African children, die from malaria, a vector-born infectious disease. The anopheles mosquitoes (the vector which spreads the malaria infection) can only survive in climates with high average temperatures, no frost and sufficient precipitation. The MARA/ARMA malaria suitability model ( Craig et al., 1999 ) incorporates these factors to determine climatically suitable areas. Mortality due to malaria is, however, also heavily influenced by factors such as access to preventative measures (including indoor spraying and insecticide-treated bed nets) and access to health care. In the MARA/ARMA model these factors are linked to income and urbanization. Fig. 9 shows the results of this model for the scenarios of this paper. The impact of autonomous adaptation (as function of rising income) reduces malaria deaths by around 50%, especially in Africa (mainly due to better provision of health care). In contrast, the impacts of climate – and especially the difference between the mitigation scenario and the baseline case are much smaller. Mitigation reduces malaria health risks by about 2% (2050). Adaptation, therefore, has a much more decisive influence on malaria control than mitigation (this finding seems to be robust with available literature). 5.3 Agriculture: impacts on yields Easterling et al. (2007) have synthesized a large amount of research on the impacts of climate change on crop growth, with and without adaptation. The results were summarized as a function of global mean temperature increase, although in reality changes in temperature and precipitation patterns and CO 2 fertilisation all play a role. For instance, the impacts of CO 2 fertilisation partly offset the impact of climate change. The results can be used to assess the climate impacts for our scenarios by using the best-fit polynomials from Easterling et al. (2007) , that indicate the impact on yield as a function of mean temperature change. 1 1 We have in each case taken the global mean temperature change for a scenario and used that as an indication of the average local temperature change to be expected. This means that our impact estimates are likely to be conservative, as temperature increase is likely to be stronger the global average over many land areas. We looked at the impacts for the baseline (4 °C) and mitigation (2 °C) scenario, with and without adaptation, for maize, wheat and rice (see Fig. 10 ; results are presented for tropical and temperate zones in 2100; these impacts are additional to the yield increases as a result of other factors than climate change). Although the results are very uncertain, some conclusions seem to be possible. First, the baseline scenario (no adaptation) causes a very substantial decrease in yields (relative to the situation without climate change) for all cases shown: Climate change impacts may reduce yields for the aggregated regions shown by 10–35% for the crops studied (2050). Second, engaging in either mitigation or adaptation limits the decrease in yields. In the tropics, however, impacts remain negative and typically in the order of a 10% loss. Third, the combination of mitigation and adaptation may result in an improvement from today's situation. Agricultural impacts may be more positive for temperate regions, but only if the advantages of higher temperature are not offset by impacts of extreme weather. These results underline the need to look at both mitigation and adaptation. The results presented are based on the IPCC assessment and represent a wide range of models. The results can also be illustrated by individual studies. Tubiello and Fischer (2007) , for instance, found that a mitigation scenario could reduce the global costs of climate change in agriculture significantly. Similarly, Fischer et al. (2007) illustrated the importance of adaptation for water irrigation requirements. They found that mitigation reduced agricultural water requirements by about 40%, leaving 60% of the impacts requiring adaptation. When dealing with impacts on agriculture both drought and heat wave stress play important roles. Fig. 11 shows, for Europe, the impact of drought and heat wave stress on crop yields for a 2 °C warming scenario, assuming various forms of adaptation ( Mechler et al., 2010; Moriondo et al., 2010 ). 2 2 Calculations were done using the Cropsyst model on the basis of the HADCM3 climate model for the 2030–2060 time slice. Winter and summer crop yields were simulated for spring wheat with today's and future crop management practices. Adaptation options considered comprised shifting the sowing date by a few days and using cultivars with a longer/shorter growth cycle. Results show that Southern Europe and parts of France are today already particularly exposed to drought and heat stress, and this situation is expected to worsen even under the 2 °C (mitigation) scenario ( Fig. 11 panel A). When considering the two adaptation strategies in combination with mitigation ( Fig. 11 panels B and C), many regions in Europe may actually benefit. Northern Europe, in particular, could exploit the advantage of higher precipitation by using crop varieties with a longer growing cycle. In contrast, in Southern Europe the same adaptation options would result in an added negative impact, since crop development would shift towards summer when longer dry spells and heat waves may significantly affect crop growth. Also, the results show that while there are some region-specific limits to adaptation, overall adaptation would effectively reduce impacts on the agricultural sector in Europe. 5.4 Water resources: potential water availability The effects of the two scenarios on exposure to changes in water resources stress are assessed using a global-scale water resources impact model ( Arnell, 2003 ). Fig. 12 shows the percentage change in average annual runoff by 2100 (relative to the 1961–1990 mean) under the baseline scenario and the mitigation scenario (with the HadCM2 climate model pattern). We define watersheds to be in a water-stressed condition if average annual runoff is less than 1000 m 3 /capita/year (other definitions are also used in the literature). The effect of climate change is indexed by summing (i) the populations living in water-stressed watersheds where runoff decreases (increases) significantly (typically by more than 5–10%) and (ii) the population living in watersheds that become water-stressed (cease to be water-stressed) due to climate change. The number of people exposed to an increase or decrease in water stress due to climate change have not been summed for two reasons: (i) the adverse effects of having less water are greater than the beneficial effects of having more water in a water-stressed catchment, and (ii) the regions with an increase and decrease in exposure to water resources stress are widely separated, and “surpluses” in one area do not offset “deficits” in another. The results show substantial differences in exposure to increased water resource stress in 2050, 2080 and 2100 between the mitigation and baseline scenarios. In 2020, there is little difference in runoff between the two scenarios. Fig. 13 shows the numbers of people exposed to an increase or decrease in water resource stress due to climate change under the two scenarios. In both the baseline and the mitigation scenario, the numbers of people living in water-stressed watersheds who apparently benefit from increased water availability is larger than the numbers exposed to a reduction in runoff, but – as outlined above – we do not focus on the net effect. The numbers of people exposed to change in water resources stresses are sensitive to the assumed pattern of climate change. Compared to the baseline, the mitigation scenario reduces the numbers exposed to an increase in water resources stress by 135 million (reducing impacts by 12%), 281 million (20% reduction) and 457 (30% reduction) million in 2050, 2080 and 2100 respectively. At the same time, however, there are also people benefiting from climate change. The relative size of the groups with positive and negative impacts depends on the climate model used (here only the Hadley pattern has been used). Clearly, mitigation also decreases the number of people benefiting from climate change. It is also clear that mitigation does not eliminate water supply impacts of climate change, and adaptation will be required for the remaining billion people exposed to increased water resource stress due to climate change. Adaptation may include measures to increase water storage, transport of water, or reduction of water demand by increasing efficiency. Underlying results show that the effects of mitigation vary significantly by region. In fact, in some regions mitigation may even increase the numbers of people exposed to increased stress. Specific uncertainty analysis shows that results are highly dependent on the uncertainty in the changes in the precipitation pattern due to climate change. 5.5 Sea level rise Another important impact of climate change is rising sea levels. Global mean sea-level rise has been projected for both scenarios using the MAGICC component of the IMAGE model. Due to the delayed response of sea-level to global warming, the projections mainly diverge in the second part of the century: sea level rise is 35 and 31 cm in 2050 in the 4 °C and 2 °C scenario, respectively and 71 and 49 cm in 2100. These projections do not include a potential accelerated contribution of the ice sheets of Greenland and Antarctica, which could lead to higher sea-level rises but the underlying processes are insufficiently understood and are currently not included in climate models ( Meehl et al., 2007; Nicholls et al., 2010; Vermeer and Rahmstorf, 2009 ). We use the DIVA model to assess both damage and adaptation costs of sea-level rise, associated storm surges and socio-economic development under the two scenarios taking into account coastal erosion (both direct and indirect), forced migration, coastal flooding (including rivers) and salinity intrusion into deltas and estuaries. For each scenario the model is run first without and then with adaptation in terms of raising dikes and nourishing beaches ( DINAS-COAST Consortium, 2006; Hinkel and Klein, 2009 ). Further impacts such as salinity intrusion in coastal aquifers, loss of coastal wetlands and biodiversity as well as further adaptation options such as salinity intrusion barriers, port upgrade, set-back zones and ecosystem-based protection could not be included due to the unavailability of global data and general models of these processes. Fig. 14 shows that independent of the level of mitigation, adaptation reduces global overall costs rather effectively, which illustrates the necessity for engaging in adaptation even under ambitious mitigation. At the aggregated scale more damages can be avoided through an adaptation-only strategy than through a mitigation-only strategy, although a combination of the two has the strongest positive impact. From the perspective of poorer and small island countries, however, stringent mitigation is necessary to keep risks at manageable levels. Even without sea-level rise, adaptation would be cost-effective in order to protect the assets situated in the floodplain, which increase due to socio-economic development alone. While this would involve substantial investment flows (tens of billions of US$ worldwide), they are a relatively small fraction of global GDP, even for sea level rise at the level of the baseline scenario. However, for individual countries or regions (particularly small island states) these costs can be a much larger fraction of GDP, including the risk of a complete loss. 5.6 Heating and cooling demand (settlements and society) Climate change is likely to influence the demand for space cooling and heating. Therefore, we have developed a set of simple relationships to describe heating and air conditioning demand in the residential sector and explored the impacts of climate change on this simulated energy demand ( Isaac and van Vuuren, 2009 ). Clearly, changes in population and income are projected to lead to a considerable growth in the energy demand for heating and air conditioning in the coming century (see Fig. 15 , no climate change case). Driven by climate, changes in cooling and heating practices are examples of autonomous adaptation (i.e. without policy intervention). Adaptation is not universal, however, since the population will not always be able to respond. Unfulfilled demand for heating and cooling can lead to health impacts (as described in Section 5.2 ) and to loss of labour productivity. In addition to these effects, there is reduced comfort when indoor temperatures rise above a given level. Fig. 15 shows that, globally, the autonomous increase in energy demand without taking climate change into account due to increasing income and wealth is much larger than the difference between the energy demand in the baseline scenario and the mitigation scenario ( Isaac and van Vuuren (2009) show this a robust result also for other baselines). The effect of climate change on combined energy demand is also smaller than the effect on heating and air conditioning separately, since increases in air conditioning compensate decreases in heating. On the regional and country level, impacts can be far more significant: for example, in India we project a large increase in energy demand due to increased cooling, while in Western Europe and the USA, we project a substantial decrease due to reduced heating. 5.7 Extreme events Climate change is expected to lead to changes in the frequency and intensity of some weather-related extreme events ( Parry et al., 2007 ). Extremes like floods, droughts, heat waves and storm surges could become more frequent and intense, while cold-extremes, such as cold spells, are likely to become less frequent and weaker. Assessing risks of climate change based on changes in average conditions-only runs the risk that changes in extreme event risks are averaged out. A more risk-based, geographically explicit method is therefore preferable. However, knowledge on disaster impacts is complex and contested. To date, there are only a limited number of national level studies taking a probabilistic approach to projecting future risk in the presence of climate change, mostly focusing on flood risk ( Mechler et al., 2010 ). One such study on the pan-European scale by Feyen et al. (2009) computed that expected annual damages would triple under a baseline scenario. A key constraint to quantitative risk approaches is the uncertainty in the climate projections. For precipitation, for instance, models often disagree on the sign of changes at the local scale. This is especially important for studies looking for instance flood risk. While the Mechler et al. (2010) study aimed to project future risk, they found future projection to be so uncertain that the authors refrained from projecting future flood risk based on an estimate of today's flood impacts. Current models and data, however, seem to be sufficient to assess the combined risk of drought and heat wave stress on agriculture with a relatively high level of certainty (slower phenomena). Some examples of work in the context of the 2 °C and 4 °C scenarios are provided here. Several studies looked into flood-affected people at the global scale ( Hirabayashi and Kanae, 2009; Kundzewicz et al., 2010 ). Regression of samples shows that the average global number of people affected by 100-year floods per year for the mitigation scenario (2 °C) is projected to be 211 million compared to 544 million for the baseline (4 °C). Mirza et al. (2003) showed that for Bangladesh, a flood-vulnerable country, even the 2 °C scenario is expected to increase the projected flooded area by at least 23–29%. It should be noted, however, that the uncertainties about exposure, vulnerability and adaptation still lead to a wide range of estimates for the costs of future flood damage. With respect to drought, the projections for the 2090s made by Burke et al. (2006) show that the number of extreme drought events per 100 years and mean drought duration are likely to increase by factors of two and six, respectively, for the baseline scenario by the 2090s. Evidence suggests that damage of weather and climate related impacts has already increased in the present-day, but these are mainly due to the wealth and population increases ( Bouwer, 2010 ). However, climate change is expected to increase over time, and is likely to become a more significant contributor to rising damages in the future. The most recent IPCC report indicates that the costs of major events are expected to range from several percent of annual regional GDP and income in very large regions with very strong economies, to more than 25% in smaller areas ( Parry et al., 2007 ). Disaster losses for highly exposed small island states in the past have in fact exceeded annual GDP ( Cummins and Mahul, 2009 ). 5.8 Economic evaluation of impacts Cost–benefit analysis (CBA) is used to express the costs and benefits of climate change of different strategies in terms of a common monetary unit. We use the CBA module of the FAIR model (see model Appendix A ) here to obtain some idea of impacts at a more aggregated scale. For mitigation costs, the FAIR model uses the information of the IMAGE model presented earlier. The climate damage and adaptation cost functions used in FAIR are derived from the AD-DICE model ( De Bruin et al., 2009a; Hof et al., 2009a ). In short, AD-DICE estimates adaptation costs based on the damage function of the DICE model ( Nordhaus and Boyer, 2000 ). The AD-DICE separates these functions into a damage cost function and residual damage function based on an assessment of each impact category described in the DICE model – agriculture, coastal zones, health, settlements, non-market time use, other vulnerable markets and catastrophic impacts. For this study, we assumed an optimal adaptation response to climate change (i.e. given a level of temperature change the model minimizes the sum of adaptation costs and residual impacts). The impact estimates used in DICE (and thus FAIR) include: (i) real, measurable, economic costs (so-called market costs); and (ii) other, intangible losses (non-market losses), which are monetized using the willingness-to-pay concept. The damage functions are not directly related to the physical or economic damages described earlier in this section, as they are derived from a separate source. It has been shown earlier that the FAIR results of adaptation costs are consistent with the range of values reported in the literature ( Hof et al., 2009a ). Under default settings of the FAIR model and a discount rate of 2.5%, the discounted costs as a share of global GDP due to climate change impacts for the period 2005–2200 amount to nearly 4.5% in the baseline ( Fig. 16 ). These costs may seem higher than suggested by the limited set of sectoral analyses presented above, but include more sectors and also the impacts of possible catastrophic events ( Nordhaus and Boyer, 2000 ). Annual costs rise sharply over time, reaching 17% in 2200 (note that impact estimates are very uncertain and both higher and lower values can be found in the literature ( Parry et al., 2007; Stern, 2006; Tol, 2002b )). Scenarios with only adaptation or mitigation reduce discounted costs substantially to around 2.5% ( Fig. 16 ). Hof et al. (2008) have shown that the results of CBA of climate change are very sensitive to model assumptions, with the discount rate playing the most important role. The discount rate is especially important due to the different costs function over time related to the adaptation only and mitigation only scenarios. 3 3 A discount rate of 5% leads to discounted costs of 0.8% and 1.9% for the adaptation-only scenario and mitigation-only scenario, respectively. If a discount rate of 1.4% is used (equal to the discount rate used by Stern (2006) ), the discounted costs are 3.2% and 2.5% for the adaptation-only scenario and mitigation-only scenario, respectively. With our discount rate of 2.5%, the combination of mitigation and adaptation leads to the lowest discounted costs, namely 2% of GDP. Consistent with literature, the adaptation investments are assessed to be smaller than mitigation investments and residual damages. However, they are very important in limiting residual damages. Some important caveats need to be mentioned. First, calculations cannot be regarded as reliable for the extreme tails of risks (i.e. low probability, high impact events). As a subjective assessment on how to handle such risks is involved, Weitzman (2008) questioned the usefulness of CBA for policymakers. Secondly, the value of the discount rate to account for time preference and risk is currently heavily debated, with arguments relating to subjective time preference and risk perception ( Nordhaus, 2008; Price, 2005; Stern, 2006 ). As mentioned above, the value of the discount rate can have a large effect on the results. Finally, non-market impacts need subjective quantification of damages; while it is difficult to monetize these impacts, in general, it is even more difficult for irreversible changes, for example a warming of the oceans leading to the loss of coral reefs ( Ackerman and Heinzerling, 2004 ). 5.9 Uncertainties in climate change, impacts and adaptation There are many sources of uncertainty in projections of future climate change and its impacts. Uncertainties are associated with every step in the causal chain: emissions, climatic drivers (e.g. the carbon cycle), climate (mainly climate sensitivity and pattern of climate change), and impacts (including adaptive capacity). As a result, different studies might give very different results for the same emission scenario. In fact, these differences are often larger than those arising in a particular model under different emission scenarios. For example, for precipitation changes at the end of the century, the multi-model ensemble mean exceeds the inter-model standard deviation only at high latitudes ( Kundzewicz et al., 2007 ). Uncertainties in climate change projections increase with the length of the time horizon. In the near term (e.g., the 2020s), climate model uncertainties play the most important role; while over longer time horizons (e.g. the 2090s), uncertainties due to the selection of emissions scenario become increasingly significant ( Jenkins and Lowe, 2003 ). The impact of future climate change on extreme events is particularly uncertain. This is partly due to a mismatch between the larger spatial and temporal scale of coarse-resolution climate models, and the local occurrence and short life of some weather extremes (e.g. cloudburst precipitation and flash floods). As impacts and adaptation take place at the local scale, detailed information is needed – which implies an increase in uncertainty. The large uncertainty ranges suggests that planning for adaptation should not be based on a single scenarios, but that a large range of projections need to be account for. 6 Conclusions In this paper, we have discussed how scenario analysis may contribute to the assessment of mitigation and adaptation strategies. We have also presented two integrated scenarios as a starting point for analysis. The scenarios have explicitly treated mitigation and adaptation action for several indicators – and cover several important linkages and feedbacks between socio-economic development and impacts (e.g. the impacts of climate change on land use and mitigation are accounted for). We specified impacts in those scenarios for a selected number of indicators, focusing mainly on mean climate changes. Based on our work, we draw the following conclusions: • By describing two contrasting sets of possible climate change trajectories for the world, we have created the basis for a more integrated analysis of the interaction between mitigation, adaptation and climate impacts. The first scenario (no mitigation) is expected to lead to a global mean temperature increase by the end of the century of around 4 °C (for the most likely values for climate parameters, and current economic trends). This scenario has high adaptation needs as has been shown in some of our analyses. The second scenario assumes stringent mitigation and limits global mean temperature change to 2 °C, with a probability of 50%. Even under this scenario, substantial adaptation measures will be needed. • Integrated scenario analysis as presented here can form a good basis for exploring the different consequences of policy choices (including uncertainties); it is not feasible, given uncertainties to determine an optimal mix between mitigation, adaptation and residual damages. As discussed in this paper, the weighing of the consequences of climate change and the various policy responses is complicated by large differences in scale, space and time; large uncertainties; and clear differences in interest between actors (whether they are perpetrators or victims of climate change, for instance). As a result, subjective interpretation of risks will always play an important role. Still, scenario analysis can provide a description of possible consequences and risks. At this stage, the monetary assessment of cost and benefits (Section 5.8 ) could not be linked to the description of physical change in the preceding sections. • Effective climate policy includes both adaptation and mitigation. Model calculations show that mitigation scenarios can be designed that lead to an increase of global mean temperature increase 2 °C for a best-guess climate sensitivity. However, even these stringent scenarios can still also result in a global mean temperature increase of more than 2.5 °C (and at best a temperature increase of 1.5 °C) and regional temperature change which is far greater. The need for a combination of mitigation and adaptation has been shown for most of the impacts explored in this paper. For example, adaptation can be more effective than mitigation in dealing with sea-level rise (at least during the 21st century), but mitigation still has a role to play in reducing damages and costs of adaptation. Agriculture presents an example where adaptation and mitigation are both clearly necessary. Crop yields in agriculture are projected to suffer negative impacts in many regions due to climate change in the absence of both adaptation and mitigation action. Without stringent mitigation, adaptation could limit negative impacts, but not remove them. An advantage of mitigation is that it affects all impact categories, while adaptation needs to be tailored to impacts and contexts. • While impacts of climate change can be severe and, depending on subjective choices, may warrant stringent climate policy, the impacts assessed in this study (given the state of the art) are likely to remain secondary influences of population change and economic growth at a global scale. Yet important caveats apply (see below). While climate change may have an impact on millions of people, other challenges are likely to influence people and governance more significantly. It should be noted, however, that we have covered only a limited set of impacts and focused mostly on mean estimates of gradual climate change and, for instance, not on catastrophic, very high-impact, extremely low-probability events ( Weitzman, 2008 ). Such events in fact may be so severe that the conclusion above no longer holds. If costs at a global scale remain relatively low, there is less need for global analysis to include all feedbacks on main drivers based on the consistency of the storylines. Clearly, at the local scale the situation is likely to be very different; impacts for individual countries can be far more substantial than at the global scale. For example, sea level rise is very important for some low-lying island states and countries that could be significantly affected by either large adaptation costs and/or damages (up to complete destruction). For agriculture, positive and negative impacts are projected to occur in different places and at different times – with low-income countries often experiencing relatively more negative impacts. Agriculture in temperate regions, where it is currently temperature-limited, could benefit. All in all, we believe that it useful to pursue further the development of integrated scenarios specifying these further on a regional scale. While this paper presents a useful first step, it also has left many feedbacks still unaccounted for. • The overall mitigation costs in this study are estimated to be in the order of 1–2% of GDP for the 2 °C scenario. The mitigation scenario reduces the risks of climate change. There are several types of benefits of investments in mitigation. First, climate-related damages and the costs of adaptation are reduced. Second, also uncertainty is reduced, which is important given the risks involved. While we argue there can be no optimal trade-off between mitigation and adaptation at a global level, we have shown that over the longer-run the costs and benefits of mitigation and adaptation are of an equivalent magnitude. • Important foci for further analysis include the linkages between assessment of physical changes and monetary impact analysis, variability and changes in extreme events, the potential role of large scale disruptions and governance. In our and other assessments, the focus has mostly been on changes in mean values, yet there is considerable concern about extreme events (resulting in natural disasters) associated with climate variability, but also in large scale disruptions (such as the disintegration of the West Antarctic Ice Shield), which are not accurately described by average values. Projections of changes in climate variability have been highly uncertain, and to date often hinder analyses from robustly predicting future extreme event risk. The role of different actors is another issue; some forms of adaptation require active governmental involvement; other forms are likely to be implemented by private investors, such as installation of space cooling systems. The differences between these two adaptation protagonists are relevant for future scenario development. Acknowledgements The research presented in this paper was performed as part of the EU-funded ADAM research project. An earlier version of this paper was published as part of the book “Making Climate Change work for us” edited by Hulme and Neufeld and published by Cambridge University Press in 2010. Appendix A Model descriptions A.1 IMAGE 2.4 The IMAGE 2.4 Integrated Assessment model ( Bouwman et al., 2006 ) consists of a set of linked and integrated models that together describe important elements of the long-term dynamics of global environmental change, such as air pollution, climate change, and land-use change. As part of IMAGE, the global energy model TIMER ( van Vuuren et al., 2006 ) describes the long-term dynamics of demand and production of primary and secondary energy and the related emissions of greenhouse gases and regional air pollutants. The model behavior is mainly determined by substitution processes of various technologies on the basis of long-term prices and fuel-preferences. The agricultural model of IMAGE models the productivity of 7 crop groups and 5 animal categories ( Leemans and Born, 1994 ). The regional production of agricultural goods is distributed spatially (at 0.5° × 0.5°) on the basis of a set of allocation rules ( Alcamo et al., 1998 ). Both the land use change maps and the agricultural activity data are used to model emissions from land use (change). The emissions of GHGs are used by the MAGICC model to calculate global mean temperature change ( Wigley and Raper, 2001 ). Patterns of temperature change are obtained by making a link to climate change patterns generated by a general circulation models (GCM). Limitations : IMAGE is provides a physically oriented description of human activities (use of tons of oil, production of tons of cereals, etc.). A fuller macro-economic description only emerges from cooperation with other models. The broad coverage of IMAGE as Integrated Assessment Model implies that many critical uncertainties influence the model outcomes. In this context, use of a single baseline (as in the ADAM project) does not do fully justice to the fundament uncertainties involved. A.2 FAIR The climate policy model FAIR ( Den Elzen et al., 2008 ) is used in conjunction with the IMAGE model to determine the reduction rates across different emission sources. Global climate calculations make use of the simple climate model, MAGICC 4.1 ( Wigley, 2003; Wigley and Raper, 2001 ). Required global emission reductions are derived by taking the difference between the baseline and a global emission pathway. The FAIR cost model distributes these between the regions following a least-cost approach using regional marginal abatement costs curves (MACs) for the different emissions sources. Recently, the FAIR model has been extended with damage and adaptation costs curves (based on the AD-DICE model ( De Bruin et al., 2009b ) and the ability to estimate macro-economic impacts on GDP growth ( Hof et al., 2008 )). This allows the model to explore the economic impacts of combined mitigation and adaptation strategies. Limitations : In its aim to be flexible, the FAIR model does not include a sectoral macro-economic model or an energy model. The model thus works from a partial equilibrium approach – and more underlying consequences of climate policy can only be studied by forwarding the FAIR results to other (linked) models. A.3 DIVA DIVA (Dynamic and Interactive Vulnerability Assessment) is an integrated model of coastal systems that was developed, together with its proper coastal database, within the EU-funded project DINAS-COAST 4 4 Dynamic and Interactive Assessment of National, Regional and Global Vulnerability of Coastal Zones to Sea-Level Rise; http://www.pik-potsdam.de/dinas-coast/ . ( DINAS-COAST Consortium, 2006; Hinkel and Klein, 2009 ). DIVA produces quantitative information on a range of ecological, social and economic coastal vulnerability indicators from sub-national to global scales, covering all coastal nations. The model consists of a number of modules developed by experts from various engineering, natural and social science disciplines. Based on climatic and socio-economic scenarios, the model assesses coastal erosion (both direct and indirect), coastal flooding (including rivers), wetland change and salinity intrusion into deltas and estuaries. DIVA also considers coastal adaptation in terms of raising dikes and nourishing beaches and includes several predefined adaption strategies such as no protection, full protection or optimal protection. Limitations : DIVA excludes the following processes that are likely to affect coastal impacts, but can currently not be modeled with confidence: changes in storm frequency and intensity, local distribution of GDP and population growth due to rapid coastal development and urbanization, and salinity intrusion into coastal aquifers. Further important uncertainties arise due to the coarse resolution and accuracy of elevation data. A.4 TIMER-cooling/heating energy demand The TIMER cooling/heating energy demand model ( Isaac and van Vuuren, 2009 ) describes the energy use for cooling and heating as a function of several factors, including population levels, changing income levels and climate. For both heating and cooling, empirical data is used to calibrate a set of system-dynamic demand functions. Climate (cooling and heating degree days) plays an important role. The model is able to account for the impacts of climate change. Limitations : The empirical basis on which the model is calibrated is relatively poor for developing countries. The model does not contain a description of different ways cooling and heating demand can be supplied and the costs involved in substituting one technology for the other. A.5 Water resources impact model The water resources impact model ( Arnell, 2003, 2004 ) has two components. The first simulates river runoff across the entire global land surface (at 0.5° × 0.5°) using the macro-scale hydrological model Mac-PDM, and the second determines indicators of water resources stress at the watershed level by calculating per capita water resource availability. A watershed is assumed to be exposed to water resources stress if it has an annual average runoff equivalent to less than 1000 m 3 /capita/year, a semi-arbitrary threshold widely used to identify water-stressed regions. Climate change leads to an increase in exposure to water resources stress if it causes runoff in a water-stressed watershed to decrease significantly, or causes the watershed to fall below the threshold. Climate change leads to an apparent reduction in exposure for the opposite trends. These changes cannot be directly compared; whilst a reduction in runoff (and an increase in exposure) is highly likely to be adverse, an increase in runoff (and apparent decrease in exposure) may not be beneficial if the additional water cannot be stored or if it occurs during high flow seasons as increased flooding. The number of people living in watersheds exposed to an increase in water resources stress can be used as an indicator of exposure to climate change. The actual impacts (in terms of real water shortages) will depend on water management structures in place. Limitations : The hydrological model does not simulate perfectly the volume of river runoff, and in particular tends to overestimate runoff in semi-arid regions. The water resources indicator is a measure of exposure to impact, not actual impact; it can be seen as a surrogate for the demand for adaptation. A.6 Malaria risks Malaria vectors, the mosquitoes spreading the infection, can only survive in suitable climates with high average temperatures, no frost and enough precipitation. The MARA/ARMA malaria suitability model ( Craig et al., 1999 ) incorporates these climatic factors to determine climatic suitable areas. The climatic levels required for the maximum suitability of 1, and for the minimum suitability of 0, are shown in Table A.1 . For indicators with levels between those required for 0 or 1 suitability a level is calculated using s simple function ( Craig et al., 1999 ). All these factors are calculated at half by half degree grid level, making use of the output from the IMAGE-model ( Bouwman et al., 2006 ). Total climatic malaria suitability for each grid cell is determined by the lowest of these three indices. Limitations : The MARA/ARMA model describes suitability for malaria vectors. It does not provide a process description of the spread of mosquitos, nor does it explicitly describe how people may react to increased risk levels. References Ackerman and Heinzerling, 2004 F. Ackerman L. Heinzerling Priceless: On Knowing the Price of Everything and the Value of Nothing 2004 The New Press New York Agrawala and Fankhauser, 2008 S. Agrawala S. Fankhauser Economic Aspects of Adaptation to Climate Change. Costs, Benefits and Policy Instruments 2008 OECD Paris Alcamo et al., 1998 J. Alcamo E. Kreileman M. Krol R. Leemans J. Bollen J.V. Minnen M. Schaeffer S. Toet B. de Vries Global modelling of environmental change: an overview of IMAGE 2.1 J. Alcamo R. Leemans E. Kreileman Global Change Scenarios of the 21st Century. Results from the IMAGE 2.1 Model 1998 Elsevier Science Ltd. Oxford 3 94 Arnell, 2003 N. Arnell Effects of IPCC SRES emissions scenarios on river runoff: a global perspective Hydrology and Earth System Sciences 7 5 2003 619 641 Arnell, 2004 N. Arnell Climate change and global water resources: SRES emissions and socio-economic scenarios Global Environmental Change 14 1 2004 31 52 Arnell et al., 2002 N.W. Arnell M.G.R. Cannell M. Hulme R.S. Kovats J.F.B. Mitchell R.J. Nicholls M.L. Parry M.J.L. Livermore A. White The consequences of CO 2 stabilisation for the impacts of climate change Climatic Change 53 4 2002 413 446 Bakkenes et al., 2006 M. Bakkenes B. Eickhout R. Alkemade Impacts of different climate stabilisation scenarios on plant species in Europe Global Environmental Change 16 1 2006 19 28 Barker, 2003 T. Barker Representing global climate change, adaptation and mitigation Global Environmental Change 13 2003 1 6 Barker et al., 2009 Barker, T., Kenber, M., Scrieciu, S., Ryan, D., 2009. Breaking the Climate Deadlock. Cutting the Cost: The Economic Benefits of Collaborative Climate Action. The Climate Group, The Office of Tony Blair, 4CMR – University of Cambridge and Cambridge Econometrics. Barker and Scrieciu, 2010 T. Barker S.S. Scrieciu Modelling low stabilisation with E3MG: towards a ‘New Economics’ approach to simulating energy-environment-economy system dynamics The Energy Journal 31 Special issue 1 2010 137 164 Barker et al., 2008 T. Barker S.S. Scrieciu T. Foxon Achieving the G8 50% target: modelling induced and accelerated technological change using the macro-econometric model E3MG Climate Policy 8 2008 S30 S45 Berkhout et al., 2002 F. Berkhout J. Hertin A. Jordan Socio-economic futures in climate change impact assessment: using scenarios as ‘learning machines’ Global Environmental Change 12 2 2002 83 95 Bouwer, 2010 L.M. Bouwer Have disaster losses increased due to anthropogenic climate change? Bulletin of the American Meteorological Society 2010 10.1175/2010BAMS3092.1 Bouwman et al., 2006 A.F. Bouwman T. Kram K. Klein Goldewijk Integrated Modelling of Global Environmental Change. An Overview of IMAGE 2.4 2006 Netherlands Environmental Assessment Agency Bilthoven 228 pp. (Publication 500110002/2006) Burke et al., 2006 E.J. Burke S.J. Brown N. Christidis Modelling the recent evolution of global drought and projections for the 21st century with the Hadley Centre climate model Journal of Hydrometeorology 7 2006 1113 1125 Clarke et al., 2010 L. Clarke J. Edmonds V. Krey R. Richels S. Rose M. Tavoni International climate policy architectures: overview of the EMF 22 international scenarios Energy Economics 31 Suppl. 2 2010 S64 S81 Copenhagen Accord, 2009 Copenhagen Accord, 2009. (Copenhagen Accord of 18 December 2009). United Nations Climate Change Conference 2009, Copenhagen. Craig et al., 1999 M.H. Craig R.W. Snow D. le Sueur A climate-based distribution model of malaria transmission in Africa Parasitology Today 15 3 1999 105 111 Cummins and Mahul, 2009 J.D. Cummins O. Mahul Catastrophe Risk Financing in Developing Countries: Principles for Public Intervention 2009 The World Bank Washington, DC De Bruin et al., 2009a K.C. De Bruin R.B. Dellink S. Agrawala Economic Aspects of Adaptation to Climate Change: Integrated Assessment Modelling of Adaptation Costs and Benefits 2009 OECD Paris De Bruin et al., 2009b K.C. De Bruin R.B. Dellink R.S.J. Tol AD-DICE: an implementation of adaptation in the DICE model Climatic Change 95 1–2 2009 63 81 Den Elzen et al., 2008 M.G.J. Den Elzen P.L. Lucas D.P. van Vuuren Regional abatement action and costs under allocation schemes for emission allowances for achieving low CO 2 -equivalent concentrations Climatic Change 90 3 2008 243 268 Den Elzen and van Vuuren, 2007 M.G.J. Den Elzen D.P. van Vuuren Peaking profiles for achieving long-term temperature targets with more likelihood at lower costs Proceedings of the National Academy of Sciences of the United States of America 104 46 2007 17931 17936 DINAS-COAST Consortium, 2006 DINAS-COAST Consortium, 2006. DIVA 1.5.5 CD-ROM, Potsdam Institute for Climate Impact Research, Potsdam, Germany. Easterling et al., 2007 W. Easterling P. Aggarwal P. Batima K. Brander L. Erda M. Howden A. Kirilenko J. Morton J.-F. Soussana J. Schmidhuber F.N. Tubiello Food, fibre and forest products M.L. Parry O.F. Canziani J.P. Palutikof P.J. van der Linden C.E. Hanson Climate Change 2007: Impacts, Adaptation and Vulnerability. Contribution of Working Group II to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change 2007 Cambridge University Press Cambridge, UK EC, 2006 EC World Energy Technology Outlook 2050 (WETO H2) 2006 European Commission Brussels Edenhofer et al., 2010 O. Edenhofer B. Knopf T. Barker L. Baumstark E. Bellevrat B. Chateau P. Criqui M. Isaac A. Kitous S. Kypreos M. Leimbach K. Lessmann B. Magné S. Scrieciu H. Turton D.P. van Vuuren The economics of low stabilization: model comparison of mitigation strategies and costs The Energy Journal 31 SI-1 2010 11 48 Environmental Change Institute, 2009 Environmental Change Institute, 2009. International Climate Conference – 4 Degrees and Beyond. Environmental Change Institute, Oxford University, 28–30 September, Oxford, UK. Feyen et al., 2009 L. Feyen J.I. Barredo R. Dankers Implications of global warming and urban land use change on flooding in Europe J. Feyen K. Shannon M. Neville Water and Urban Development Paradigms. Towards an Integration of Engineering, Design and Management Approaches 2009 Taylor and Francis Group London Fischer et al., 2007 G. Fischer F.N. Tubiello H. van Velthuizen D.A. Wiberg Climate change impacts on irrigation water requirements: effects of mitigation, 1990–2080 Technological Forecasting and Social Change 74 7 2007 1083 1107 Fisher et al., 2007 B. Fisher N. Nakicenovic K. Alfsen J. Corfee Morlot F. de la Chesnaye J.-C. Hourcade K. Jiang M. Kainuma E. La Rovere A. Matysek A. Rana K. Riahi R. Richels S. Rose D. van Vuuren R. Warren P. Ambrosi F. Birol D. Bouille C. Clapp B. Eickhout T. Hanaoka M.D. Mastrandrea Y. Matsuoko B. O’Neill H. Pitcher S. Rao F. Toth Issues related to mitigation in the long-term context B. Metz O. Davidson P. Bosch R. Dave L. Meyer Climate Change 2007. Mitigation of Climate Change. Contribution of Working Group III to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change 2007 Cambridge University Press New York 169 250 Hallegatte, 2009 S. Hallegatte Strategies to adapt to an uncertain climate change Global Environmental Change 19 2009 240 247 Hayashi et al., 2010 A. Hayashi K. Akimoto F. Sano S. Mori T. Tomoda Evaluation of global warming impacts for different levels of stabilization as a step toward determination of the long-term stabilization target Climatic Change 98 2010 87 112 Hilderink et al., 2008 H. Hilderink P.L. Lucas A. ten Hove M. Kok M. de Vos P. Janssen J. Meijer A. Faber A. Ignaciuk A. Petersen H.J.M. de Vries Towards a Global Integrated Sustainability Model 2008 Netherlands Environmental Assessment Agency Bilthoven Hinkel and Klein, 2009 J. Hinkel R.J.T. Klein Integrating knowledge to assess coastal vulnerability to sea-level rise: the development of the DIVA tool Global Environmental Change 19 3 2009 384 395 Hirabayashi and Kanae, 2009 Y. Hirabayashi S. Kanae First estimate of the future global population at risk of flooding Hydrological Research Letters 3 2009 6 9 Hof et al., 2009a A.F. Hof K.C. de Bruin R.B. Dellink M.G.J. den Elzen D.P. van Vuuren The effect of different mitigation strategies on international financing of adaptation Environmental Science and Policy 12 7 2009 832 843 Hof et al., 2009b A.F. Hof K. de Bruin R. Dellink M.G.J. den Elzen D.P. van Vuuren Costs, benefits and inter-linkages between adaptation and mitigation F. Biermann P. Pattberg F. Zelli Global Climate Governance After 2012: Architecture, Agency and Adaptation 2009 Cambridge University Press Cambridge Hof et al., 2008 A.F. Hof M.G.J. den Elzen D.P. van Vuuren Analysing the costs and benefits of climate policy: value judgements and scientific uncertainties Global Environmental Change 18 3 2008 412 424 IMAGE-team, 2001 IMAGE-team, 2001. The IMAGE 2.2 implementation of the IPCC SRES scenarios. A comprehensive analysis of emissions, climate change and impacts in the 21st century. RIVM CD-ROM publication 481508018, National Institute for Public Health and the Environment, Bilthoven, the Netherlands. IPCC, 2007 IPCC (Ed.), 2007. Climate Change 2007: Synthesis Report. Contribution of Working Groups I, II and III to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change. IPCC, Geneva, 104 pp. Isaac and van Vuuren, 2009 M. Isaac D.P. van Vuuren Modeling global residential sector energy demand for heating and air conditioning in the context of climate change Energy Policy 37 2 2009 507 521 Jenkins and Lowe, 2003 Jenkins, G., Lowe, J., 2003. Handling uncertainties in the UKCIP02 scenarios of climate change. Hadley Centre Technical Note 44, Met Office, Exeter. Kinney et al., 2008 P.L. Kinney M.S. O’Neill M.L. Bell J. Schwartz Approaches for estimating effects of climate change on heat-related deaths: challenges and opportunities Environmental Science and Policy 11 87 2008 Klein et al., 2007 R.J.T. Klein S. Huq F. Denton T.E. Downing R.G. Richels J.B. Robinson F.L. Toth Inter-relationships between Adaptation and Mitigation. Climate Change 2007. Impacts, Adaptation and Vulnerability. Contribution of Working Group II. Report of the Intergovernmental Panel on Climate Change 2007 Cambridge University Press Cambridge 745 777 Krol et al., 1997 M. Krol J. Alcamo R. Leemans Global and regional impacts of stabilizing atmospheric CO 2 Mitigation and Adaptation Strategies for Global Change 1 1997 341 361 Kundzewicz et al., 2010 Z.W. Kundzewicz Y. Hirabayashi S. Kanae River floods in the changing climate – observations and projections Water Resources Management 2010 10.1007/s11269-009-9571-6 Kundzewicz et al., 2007 Z.W. Kundzewicz L.J. Mata N. Arnell P. Döll P. Kabat B. Jiménez K. Miller T. Oki Z. Şen I. Shiklomanov Freshwater resources and their management M.L. Parry O.F. Canziani J.P. Palutikof C.E. Hanson P.J. van der Linden Climate Change 2007: Impacts, Adaptation and Vulnerability. Contribution of Working Group II to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change 2007 Cambridge University Press Cambridge, UK Leemans and Born, 1994 R. Leemans G.J.v.d. Born Determining the potential global distribution of natural vegetation, crops and agricultural productivity Water, Air and Soil Pollution 76 1994 133 161 Lenton et al., 2008 T.M. Lenton H. Held E. Kriegler J.W. Hall W. Lucht S. Rahmstorf H.J. Schellnhuber Tipping elements in the Earth's climate system Proceedings of the National Academy of Sciences of the United States of America 105 6 2008 1786 1793 Manne and Richels, 2005 A.S. Manne R.G. Richels Merge: an integrated assessment model for global climate change R. Loulou J.-P. Waaub G. Zaccour Energy and Environment 2005 Springer USA McMichael et al., 1996 A. McMichael A. Haines R. Slooff S. Kovats Climate Change and Human Health 1996 World Health Organization Geneva Mechler et al., 2010 R. Mechler S. Hochrainer A. Aaheim Z. Kundzewicz N. Lugeri M. Moriondo H. Salen M. Bindi I. Banaszak A. Chorynski E. Genovese H. Kalirai J. Linnerooth-Bayer C. Lavalle D. McEvoy P. Matczak M. Radziejewski D. Rübbelke M.-J. Schelhaas M. Szwed A. Wreford Risk management approach for assessing adaptation to changing flood and drought risks in Europe M. Hulme H. Neufeldt Making Climate Change Work for Us: European Perspectives on Adaptation and Mitigation Strategies 2010 Cambridge University Cambridge, UK Meehl et al., 2007 G.A. Meehl T.F. Stocker W.D. Collins P. Friedlingstein A.T. Gaye J.M. Gregory A. Kitoh R. Knutti J.M. Murphy A. Noda S.C.B. Raper I.G. Watterson A.J. Weaver Z.-C. Zhao Global climate projections S. Solomon Climate Change 2007: The Physical Science Basis. Contribution of Working Group I to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change 2007 Cambridge University Press Cambridge Metz et al., 2007 B. Metz O.R. Davidson P.R. Bosch R. Dave L.A. Meyer Climate Change. Mitigation of Climate Change. Contribution of Working Group III to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change 2007 Cambridge University Press Cambridge, United Kingdom Mirza et al., 2003 M.M.Q. Mirza R.A. Warrick N.J. Ericksen The implications of climate change on floods of the Ganges, Brahmaputra and Meghna Rrivers in Bangladesh Climatic Change 57 2003 287 318 Moriondo et al., 2010 M. Moriondo M. Bindi Z.W. Kundzewicz M. Szwed A. Chorynski P. Matczak M. Radziejewski D. McEvoy A. Wreford Impact and adaptation opportunities for European agriculture in response to climatic change and variability Mitigation and Adaptation Strategies for Global Change 15 7 2010 657 679 Moss et al., 2010 R.H. Moss J.A. Edmonds K.A. Hibbard M.R. Manning S.K. Rose D.P. van Vuuren T.R. Carter S. Emori M. Kainuma T. Kram G.A. Meehl J.F.B. Mitchell N. Nakicenovic K. Riahi S.J. Smith R.J. Stouffer A.M. Thomson J.P. Weyant T.J. Wilbanks The next generation of scenarios for climate change research and assessment Nature 2010 10.1038/nature08823 Nakicenovic et al., 2000 N. Nakicenovic Special Report on Emissions Scenarios (SRES) 2000 Cambridge University Press Cambridge, UK Nakicenovic et al., 2006 N. Nakicenovic P. Kolp K. Riahi M. Kainuma T. Hanaoka Assessment of Emissions Scenarios Revisited Environmental Economics and Policy Studies 7 3 2006 137 173 Nicholls and Lowe, 2004 R.J. Nicholls J.A. Lowe Benefits of mitigation of climate change for coastal areas Global Environmental Change 14 3 2004 229 244 Nicholls et al., 2010 R.J. Nicholls N. Marinova J.A. Lowe S. Brown P. Vellinga D. de Gusmao J. Hinkel R.S.J. Tol Sea-level rise and its possible impacts given a “beyond 4 degree world” in the 21st century Philosophical Transactions of the Royal Society 2010 10.1098/rsta.2010.029 Nordhaus and Boyer, 2000 W.D. Nordhaus J. Boyer Warming the World: Economic Models for Global Warming 2000 MIT Press Cambridge, MA pp. 315–328 Nordhaus, 2008 W.D. Nordhaus A Question of Balance Weighing the Options on Global Warming Policies 2008 Yale University Press New Haven and London Parry et al., 2007 M.L. Parry O.F. Canziani J.P. Palutikof P.J. van der Linden C.E. Hanson Climate Change 2007: Impacts, Adaptation and Vulnerability. Contribution of Working Group II to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change 2007 Cambridge University Press Cambridge Patt et al., 2010 A.G. Patt D.P. van Vuuren F. Berkhout A. Aaheim A.F. Hof M. Isaac R. Mechler Adaptation in integrated assessment modeling: Where do we stand? Climatic Change 99 3 2010 383 402 Piani et al., 2005 C. Piani D.J. Frame D.A. Stainforth M.R. Allen Constraints on climate change from a multi-thousand member ensemble of simulations Geophysical Research Letters 32 2005 L23825 Price, 2005 C. Price An intergenerational perspective on effects of environmental changes: discounting the future's viewpoint J.L. Innes G.M. Hickey H.F. Hoen Forestry and Environmental Change: Socioeconomic and Political Dimensions 2005 International Union on Forestry Research Organisations (IUFRO) Vienna Rose et al., 2007 Rose, S., Ahammad, H., Eickhout, B., Fisher, B., Kurosawa, A., Rao, S., Riahi, K., van Vuuren, D. 2007. Land in climate stabilization modeling: initial observations Energy Modeling Forum Report. Stanford University. Schneider and Kuntz-Duriseti, 2002 S.H. Schneider K. Kuntz-Duriseti Uncertainty and climate change policy S.H. Schneider A. Rosencranz O. Niles Climate Change Policy: A Survey 2002 Island Press Washington, DC Stern, 2006 N. Stern Stern Review on the Economics of Climate Change 2006 Cambridge University Press Cambridge Swart et al., 2009 R. Swart L. Bernstein M. Ha-Duong A. Petersen Agreeing to disagree: uncertainty management in assessing climate change, impacts and responses by the IPCC Climatic Change 92 2009 1 29 Swart and Raes, 2007 R. Swart F. Raes Making integration of adaptation and mitigation work: mainstreaming into sustainable development policies? Climate Policy 7 4 2007 288 303 Tol, 2002a R. Tol Estimates of the damage costs of climate change. Part II. Dynamic estimates Environmental and Resource Economics 21 2 2002 135 160 Tol, 2002b R.S.J. Tol Estimates of the damage costs of climate change. Part 1. Benchmark estimates Environmental and Resource Economics 21 1 2002 47 73 Tol, 2002c R.S.J. Tol Welfare specifications and optimal control of climate change: an application of fund Energy Economics 24 4 2002 367 376 Tubiello and Fischer, 2007 F.N. Tubiello G. Fischer Reducing climate change impacts on agriculture: global and regional effects of mitigation, 2000–2080 Technological Forecasting and Social Change 74 7 2007 1030 1056 UN, 2005 UN, 2005. World Population Prospects: The 2004 Revision. CD-ROM Edition – Extended Dataset. United Nations publications, Sales No. E.05.XIII.12, United Nations, Department of Economic and Social Affairs, Population Division. van Vliet et al., 2009 J. van Vliet M.G.J. den Elzen D.P. van Vuuren Meeting radiative forcing targets under delayed participation Energy Economics 31 Suppl. 2 2009 S152 S162 van Vuuren et al., 2006 D.P. van Vuuren B. van Ruijven M. Hoogwijk M. Isaac B. De Vries TIMER 2: model description and application L. Bouwman T. Kram K. Klein-Goldewijk Integrated Modelling of Global Environmental Change. An overview of IMAGE 2.4 2006 MNP - Netherlands Environmental Assessment Agency Bilthoven van Vuuren et al., 2007 D.P. van Vuuren M.G.J. Den Elzen P.L. Lucas B. Eickhout B.J. Strengers B. Van Ruijven S. Wonink R. Van Houdt Stabilizing greenhouse gas concentrations at low levels: an assessment of reduction strategies and costs Climatic Change 81 2 2007 119 159 van Vuuren et al., 2008a D.P. van Vuuren B. De Vries A. Beusen P.S.C. Heuberger Conditional probabilistic estimates of 21st century greenhouse gas emissions based on the storylines of the IPCC-SRES scenarios Global Environmental Change 18 4 2008 635 654 van Vuuren et al., 2008b D.P. van Vuuren M. Meinshausen G.K. Plattner F. Joos K.M. Strassmann S.J. Smith T.M.L. Wigley S.C.B. Raper K. Riahi F. De La Chesnaye M.G.J. Den Elzen J. Fujino K. Jiang N. Nakicenovic S. Paltsev J.M. Reilly Temperature increase of 21st century mitigation scenarios Proceedings of the National Academy of Sciences of the United States of America 105 40 2008 15258 15262 van Vuuren et al., 2009 D.P. van Vuuren M.G.J. den Elzen J. van Vliet T. Kram P. Lucas M. Isaac Comparison of different climate regimes: the impact of broadening participation Energy Policy 37 12 2009 5351 5362 van Vuuren et al., 2010 D.P. van Vuuren E. Stehfest M.G.J. den Elzen J. Van Vliet M. Isaac Exploring scenarios that keep greenhouse gas radiative forcing below 3 W/m 2 in 2100 Energy Economics 31 Special Issue 1 2010 165 192 Vermeer and Rahmstorf, 2009 M. Vermeer S. Rahmstorf Global sea level linked to global temperature Proceedings of the National Academy of Sciences of the United States of America 106 2009 21527 21532 Weitzman, 2008 Weitzman, M.L., 2008. On modeling and interpreting the economics of catastrophic climate change. Wigley, 2003 T.M.L. Wigley MAGICC/SCENGEN 4.1: Technical Manual 2003 UCAR - Climate and Global Dynamics Division Boulder, CO Wigley and Raper, 2001 T.M.L. Wigley S.C.B. Raper Interpretation of high projections for global-mean warming Science 293 2001 451 454|设想情景用于探讨不确定情况下不同适应和缓解战略的后果。在本文中，我们使用了两种情景来探讨发展: (1)没有缓解措施导致全球平均气温到2100年上升4摄氏度; (2)一个雄心勃勃的缓解策略导致到2100年上升2摄氏度。就第二种情况而言，气候系统的不确定性意味着不能排除全球平均气温上升3摄氏度或更多的可能性。我们的分析表明，在许多情况下，适应和减缓不是权衡，而是补充。例如，在缓解设想方案中，因气候变化而面临更大水资源压力的人数可以大幅度减少，但仍然需要对面临更大压力的其余大量人口进行适应。另一个例子是海平面上升，从全球和纯货币的角度来看，适应(直到2100年)似乎比缓解更有效。然而，从较贫穷和小岛屿国家的角度来看，严格的缓解措施对于将风险保持在可控水平是必要的。就农业而言，只有基于适应和缓解相结合的设想方案才能避免严重的气候变化影响。关键词情景综合评估气候变化缓解适应气候影响1引言情景分析是评估气候变化和气候变化政策的一个非常重要的工具，它使分析人员能够探索经济发展、温室气体排放、气候和生态系统等因素之间复杂而不确定的未来相互作用。这些因素共同决定了缓解和适应政策的必要性和可能性。设想情景还可以作为一种手段，协调参与气候研究领域的各种不同研究群体的假设，从而更好地比较其结果。因此，情景在缓解和适应研究中得到了广泛的应用(参见 Metz 等，2007; Parry 等，2007)(特别是来自排放情景特别报告(SRES)的情景(Nakicenovic 等，2000))。Moss 等人(2010)指出，由于 SRES 对场景分析的信息需求正在发生变化。首先，人们对探索适应与缓解之间的关系越来越感兴趣。正如 Moss 等人(2010)所指出的，这将需要进一步整合气候研究中涉及的不同分析传统的信息。第二，除了迄今为止探讨的无气候政策情景之外，人们对明确探讨气候政策影响的情景也越来越感兴趣。具体而言，在没有气候政策的情况下，能够评估长期气候目标的“成本”和“收益”是非常有意义的。在本文中，我们遵循这一思路，探讨情景分析如何能够促进对未来适应和缓解战略的联合评估。这样的联合评估有以下几个原因: (1)首选的缓解策略取决于预期的气候影响和适应成本; (2)考虑到适应气候变化的局限性; (3)一些适应和缓解策略可能相互作用; (4)最后，气候变化的影响可能有需要考虑的重要反馈。这种分析在战略层面上是最有用的，而不是针对个人的适应(或缓解)决策。鉴于这一目的，我们在本文中讨论了两个主要的情景，其中包括适应和减缓战略的要素(见本文的进一步内容) ，导致本世纪末全球平均气温上升4摄氏度和2摄氏度。这两个温度水平已经开始成为标志性的数字，代表着在没有减缓政策(4摄氏度)和国际气候谈判的温度目标(2摄氏度)(2009年哥本哈根协议)的情况下的潜在结果。可以说，如果政治领导人要在减缓、适应和气候影响之间做出明智的选择，了解这两个温度水平的影响是至关重要的(环境变化研究所，2009)。缓解和适应战略的综合评估由于方法上的差异而受到阻碍。考虑到当地环境的重要性，综合评估模型很难描述适应过程(Patt et al。 ，2010)。一个实际问题是，迄今为止，影响文献的相当一部分集中在非政策情景下的影响(例外包括 Arnell 等，2002; Bakkenes 等，2006; Hayashi 等，2010; Krol 等，1997; Nicholls 和 Lowe，2004)。因此，本文提出了一个基于耦合信息的广义情景评估——但没有假装是完整的或完全集成的。作为一项边做边学的活动，本文件打算说明4摄氏度和2摄氏度世界之间的重要区别，但也要确定进行综合情景分析所涉及的一些实际问题。这意味着，与现有文献相比，最重要的进步是我们提出了一个基于一致情景的多部门分析。鉴于目前综合评估模型的先进水平，已经使用几个松散耦合模型进行了试验。因此，一些重要的联系无法得到解决，如农业的适应性反应，这可能涉及灌溉(见第5.3节)和水需求(第5.4节)。事实上，本文提出的一个重要问题是，是否需要进行全面综合分析，或者部分综合是否足够。本文的内容安排如下: 我们首先讨论在开发能够为适应和缓解政策决策提供信息的设想方案时所遇到的一些方法上的复杂问题。接下来，我们讨论两种主要情景在社会经济驱动因素方面的差异(第3和第4部分)。在第5节中，我们探讨了适应和减缓战略对气候变化各种影响的潜在后果。2评估气候战略和情景发展(理论和方法)2.1应对气候变化的不同战略气候变化及其响应可能导致三种形式的成本(不一定是货币) : (1)气候影响的(剩余)成本，(2)适应的成本和(3)缓解的成本。至少在理论上，这对应于三种不同的策略: (1)“自由放任”(接受气候变化) ，(2)关注适应，(3)关注缓解，如图1所示(另见 Klein 等，2007)。虽然图1表明，缓解、适应和剩余损害的成本和收益可以相互交换，但存在一些概念和分析问题，使这种办法复杂化。这些与空间和时间尺度、风险和不确定性有关(SwartandRaes，2007)。缓解和适应是在不同空间尺度上发生的过程。虽然缓解行动通常是在国家或地方范围内采取的，但好处是全球共享的。因此，气候政策成功和成本的关键因素是国际合作的程度(Barker 等，2009; Clarke 等，2010; van Vliet 等，2009; van Vuuren 等，2009)。相比之下，对于适应而言，成本和收益在从地方到国家乃至国际的多个尺度上都存在。较大规模的扶持性环境仍然可以在较小规模上加强适应(例如，由国际融资机制资助的地方能力建设)。由于这些原因，缓解评估往往集中在全球一级，而相比之下，适应研究大多集中在地方一级。随着时间的推移，缓解和适应的动态也是一个重要因素。严格的缓解方案通常需要强有力的早期减排。然而，由于气候系统内部的巨大惯性，这些假设情景的气候变化影响在短期(前几十年)与没有气候变化政策的假设情景几乎没有差别。相比之下，一些相关的影响(例如减少当地空气污染的共同利益)可以以更快的速度实现。适应措施可能在短期内产生私人和社会效益。例如，空气调节等简单的适应措施可以带来明显的短期效益。一些重要的例外存在，可能需要几十年的实施，如空间规划的变化或大规模的工程工程防洪(见哈勒盖特，2009年)。其他重要因素是风险和不确定性。我们对气候变化的理解面临许多不确定性。要确定的关键不确定性包括认知、数据、模型和实体不确定性(施奈德和 Kuntz-Duriseti，2002; van Vuuren 等，2008a)。涉及不确定因素的例子有: (i)未来的排放量，(ii)气候系统，(iii)未来的脆弱性和对气候风险的暴露，以及(iv)缓解成本。采取缓解行动减少了一些不确定性，因为它减少了气候变化的源头，并揭示了实际的缓解成本(Barker，2003; Piani 等，2005)。然而，缓解措施也可能增加风险。例如，如果以不可持续的方式实施生物能源，可能会抵消一组风险(气候变化) ，同时产生另一组不同的风险(生物多样性丧失和粮食安全下降)。处理风险的一种方法是包括概率评估。这通常是使用过去的证据，推断以涵盖特定的未来情况。其他不确定性(例如不可知的冲击和意外)在量化意义上更难处理，但它们证明了承认无知的合理性。情景可以用来探索极端事件的可能性和各种政策组合的稳健性，但这并不常见(Berkhout et al。 ，2002)。传统上，涉及缓解研究和适应研究的学科对不确定性有不同的描述方式。虽然缓解研究往往使用定量方法并侧重于平均估计，但适应研究往往更侧重于对不确定性的定性描述，并侧重于危险事件的风险，即使这些事件发生的概率很低。这些不同的不确定性感知可能会使不同策略的综合评估复杂化(Swartet al。 ，2009)。2.2场景的类型我们可以根据缓解和适应的考虑将场景分为不同的类别。首先，我们将基线情景定义为一个事件的轨迹，假设没有来自气候变化的重大反馈，也没有关于缓解或适应的具体政策努力(这种情景可能仍然包括许多间接影响缓解或适应气候变化能力的行动; 例如，可以预期收入水平的增加与对减少疟疾等气候相关疾病风险的卫生服务的更大投资相一致)。这种类型的场景的主要目的是进行分析，作为其他场景的参考点。其次，适应情景描述了一个社会正在应对气候变化影响的世界。其目的是探讨适应气候变化所需的技术和政策类型、避免的损害和相关费用。适应包括所谓的自主适应(即在没有特定政府行动的情况下发生的行动)和有计划的适应。第三，缓解方案描述了一个包括旨在限制气候变化的政策的世界。其目的是探讨最大限度地减少气候变化及相关成本所需的技术和政策类型。由于总是存在剩余的影响，第四组，适应和缓解情景综合了两种类型的气候变化应对措施。可能的话，这第四类情景可以根据适应和缓解备选办法之间可能存在的协同作用，例如对于一些重新造林备选办法，重新排列政策备选办法。每一种情况都与更广泛的社会、政治和文化背景有关，在这种背景下，它们被认为会出现。在探索缓解、适应和残余损害的优选组合时，存在两种主要方法: (i)将潜在影响描述为全球平均气温上升(从而缓解)的功能的影响和基于风险的方法，以及(ii)成本效益分析，其中确定货币成本和收益，以最大限度地提高福利(例如，参见 Nordhaus，2008; Tol，2002c)。在这两种情况下，我们认为，描述不同应对战略之间的关系比寻求确定最佳办法更有用，也更能反映问题。鉴于第2.1节所列出的复杂性和不确定性，我们认为在现实中不可能采取任何最佳的缓解、适应或联合策略。2.3综合分析缓解和适应的综合分析可以通过不同方式实现: 例如，使用单一的所谓综合评估模型，或在不同模型和学科之间交流信息，评估现有文献并使结果具有可比性。这两种方法都是围绕气候变化的因果链进行组织的，即描述经济活动(收入、能源使用、农业等)、排放、气候变化和影响之间的关系——以及相关的反馈(图2)。实际上，该方案也构成了 IPCC 报告情景信息流的主干(Moss et al。 ，2010)。情景首先由综合评估和排放模型制定(侧重于经济驱动力、能源和土地使用以及温室气体排放(IPCC“工作组 III”))。随后，排放轨迹在气候模型中被用来评估气候变化的影响(IPCC“工作组 I”)。最后，这些情景被用于影响、适应和脆弱性分析(IPCC“工作组 II”)。不同研究学科和工作组的参与意味着很难说明不同领域之间的反馈意见。综合评估模型只能获取有限数量的可能反馈(经常被忽略的反馈包括粮食和水安全对人口和经济驱动因素的影响; 水资源短缺与粮食生产之间的关系; 气候变化对能源使用的影响等)。如果这些反馈不足以对系统产生重大影响，忽略(其中一些)可能是合理的。出于分析原因，在学科领域内组织场景开发并考虑有限数量的反馈有很大的优势。它使研究人员能够专注于他们很好地理解的链条要素，并增加所需的细节数量，而不必面对相互联系的复杂性。然而，在更加注重对缓解和适应战略进行综合分析的情况下，这种情况可能会改变。关于为什么需要采取综合办法的一些例子是: 一、气候影响，例如极端事件引发的影响，可能非常严重，破坏了原先设想的经济假设; 二。气候影响可能对农业产生重大影响，因此，对未考虑影响的土地使用相关排放量的估计可能是错误的，生物能源的缓解潜力可能受到影响;。对于对缓解和适应都有吸引力的土地面积，可能存在相互竞争的权利主张。因此，一个有趣的问题是，是否需要更加集成的分析是如此迫切，以至于需要更加复杂的集成模式(模型的交互耦合; 一个复杂的模型) ，或者是否可以单独处理影响，简化分析框架。时间范围和决策焦点在这里可能也很重要，例如是否考虑到潜在的临界点(Lenton et al。 ，2008)。研究这个问题的少数现有研究似乎表明，在大多数部门，任何缓解项目的适应影响都很小，大多数适应活动产生的排放量也很小(Klein et al。 ，2007)。迄今为止，最完整的分析来自以成本效益为导向的综合评估模型，如基金、 DICE 和 MERGE (Manne and Richels，2005; Nordhaus，2008; Tol，2002c) ，但这些模型通常将气候影响聚合为有限数量的相当抽象的损害函数。我们认为，随着时间的推移，随着许多部门减缓和适应措施的力度不断加大，将更加需要进行足够详细的联合评估。这里提供的场景基于建模和场景开发的当前技术状态，迈出了第一步。缓解和影响评估的一项评估使用了相同的设想方案，我们明确提出了缓解和适应战略(作为设想方案的一部分或在用于不同影响的模型中)。然而，许多反馈没有被考虑进去。在本文的最后，我们回到了更加集成(但也更加复杂)的场景的作用。2.4本文件使用的方法如上所述，可以确定几种情景: 基线情景、缓解情景、适应情景和适应-缓解情景。本文还介绍了这些场景类型。对于基准/适应情景，我们假设大多数社会经济驱动因素的中间假设。情景假设在第3和第4节中描述。这些设想方案不包括缓解措施，导致到2100年全球平均气温比工业化前水平上升4摄氏度。虽然我们描述了在这些情况下可能产生的影响和适应，但我们没有包括对原始驱动因素的反馈。在缓解设想方案中，包括严格的缓解努力，导致全球平均气温上升2摄氏度。使用 IPCC 给出的3 ° C 的气候敏感性中值(Meehl 等，2007) ，这意味着稳定水平约为450 ppm CO2当量(CO2当量).气候政策对经济驱动因素的影响没有被考虑在内——但是其他几个关系是耦合的(例如土地使用)。因此，在大多数文章中，我们忽略了气候变化和气候政策对经济假设的潜在影响。然而，在第5.8节中，我们用一个简单的经济模型(FAIR)来讨论它们的影响，以提供对全球范围内经济后果的可能规模的一些见解。使用了几个模型工具。这些情景主要是使用 IMAGE 综合评估模型开发的(Bouwman 等，2006)。IMAGE 模型根据对人口和世界经济的假设，结合对技术发展和消费模式的假设，描述了21世纪能源和土地使用的发展情况。该模型预测了全球范围内的气候变化(以全球平均温度变化和海平面上升为指标) ，并通过模式缩放缩小的气候模式模式构建了0.5 ° × 0.5 ° 网格上月气温和降雨量变化的空间场景。IMAGE 的产出用于描述海平面上升的 DIVA 模型; 用于估计水资源紧张后果的 Mac-PDM 全球水文模型; 用于估计加热和降温需求影响的 TIMER 能源模型; 用于疟疾对疟疾影响的 MARA/ARMA 适宜性模型和用于货币成本效益分析的 FAIR 模型。此外，我们更一般地讨论对农业的影响(基于 IPCC AR4)和极端事件。附录 A 提供了所有使用模型的简要描述。在我们的描述中，我们关注于全局级别(考虑到空间有限)。显然，这导致了我们讨论适应的局限性。实验取决于每个模型的设计，因此，不同影响之间可以提出的假设情景的数量是不同的。这意味着研究报告应被解释为综合评估的第一个例证，而不是关于适应及其限制的全面研究。3结果: 基线情景中的社会经济趋势3.1人口发展和经济增长我们假设人口遵循2004年世界人口预测修订版(UN，2005)到2050年的中等生育率变量，以及联合国到2100年的长期中等预测(图3)。这意味着，到2050年，全球人口将稳步增至近91亿，并在随后的50年中稳定在约92亿人，直至2100年。这种情况在人口预测范围内采取中间立场(见图3)。对于直到2050年的经济增长，这种情况遵循与剑桥模型 E3MG 相关的预测(巴克和 Scrieciu，2010; 巴克等人，2008)。利用基于 SRES 的 B2情景(IMAGE-team，2001)的经济增长预测，该情景延伸到2050年以后。从数量上看，这是一个中高速经济增长的情景，主要是对中国和印度经济增长的乐观假设的结果。按人均计算，经合组织经济体预计仍将是世界上最富有的经济体，但就经济活动总量而言，发展中区域的重要性迅速增加。在非洲、中东和拉丁美洲，人均国内生产总值的年增长率在0% 到2% 之间。在亚洲，到2050年，这一比例将从目前的高水平降至每年3% 。3.2基线情景下的能源使用和温室气体排放基线情景下的能源使用与欧盟委员会(EC，2006)公布的基线保持一致。尽管能源强度进一步降低，世界能源消费在2000-2050年期间增加了一倍以上，在2050-2100年期间又增加了25% (图4)。整个世纪以来，能源供应仍然以化石燃料为主。当石油和天然气的生产在本世纪达到高峰和下降时，煤炭的使用在整个情景期间增加。此外，非化石能源的产量也在迅速增长。在截至2100年的这段时间里，核能使用量增加了2到3倍，达到76 EJ，生物质使用量强劲增加，而水力发电产量增加了大约60% 到80% 。相对增长最大的是风能和太阳能; 到2050年，风能和太阳能在所有非化石能源中的比例将从不到1% 上升到10% 至14% 。2050年可再生能源总使用量为120-140 EJ，2100年为190 EJ。上述趋势表明，能源活动的二氧化碳排放量在2050年之前增加了一倍以上，在2050年至2100年之间又增加了三分之一(见图3)。因此，该方案在文献范围内形成了一个中间基线方案(Fisher et al。 ，2007)。非二氧化碳温室气体(尤其是甲烷)在2000-2050年期间稳步增长，但增长速度低于二氧化碳(作为其驱动因素，农业的增长速度预计将低于能源部门)。本世纪上半叶，土地利用产生的二氧化碳排放量回落至零。农业用地面积位于最近公布的类似情景的范围内，尽管处于该范围的低端(Rose et al。 ，2007)。4缓解方案和气候方案的结果4.1能源使用和温室气体排放缓解方案旨在将温室气体稳定在大约450 ppm CO2-当量。(参见 van Vuuren 等人，2007,2010)。这种情况允许初始浓度超调至大约510 ppm 二氧化碳当量。DenElzen 和 van Vuuren (2007)早些时候已经表明，有限的过度集中可以以较低的成本达到类似的气候目标。减少排放的方法有很多种。一个要素是提高能源效率，从而减少能源使用总量(2050年与基准相比减少20%)(见图4)。该设想方案还显示，非化石能源的使用日益增加，占能源使用总量增长的大部分。非化石能源的使用从2010年占一次能源使用总量的15% 增加到2050年的30% 以上，到本世纪末占总量的40% 以上。这种增长主要是由于生物能源使用的增加。碳捕获和储存应用于化石燃料的大多数固定用途。最后，还减少了非二氧化碳温室气体的排放。其结果是，全球排放量在2020年左右达到峰值，并随着时间的推移进一步减少。与2050年的基准相比，排放量减少了70% 以上，到2100年减少了80% 以上。减缓政策的后果不仅影响能源部门，而且影响土地使用。大量额外的土地用于造林和生物能源(见图5)。模型比较研究表明，这里提出的缓解方案与目前的文献是一致的，尽管模型显示各种减排措施的贡献有显着差异(Clarke 等，2010; Edenhofer 等，2010)。根据 IMAGE 模型计算，减排成本约为 GDP 的1-2% (即每年的额外支出，可与经合组织国家目前约占 GDP 1.5% 的环境政策支出相比较)(图6)。可比情景的文献范围在2100年的0.5.5% 左右。大多数研究都认为，这些额外的支出将导致国内生产总值的减少。我们将在第5.8节进一步讨论这个问题。4.2基线和缓解情景下的气候变化基于 IMAGE 模型计算，大气温室气体浓度和由这两种情景排放引起的相关平均全球温度变化如图7所示(实线表示最佳猜测值)。IMAGE 模型使用 MAGICC 模型来计算全球平均温度的变化。早期，van Vuuren 等人(2008b)将 MAGICC 模型用于类似的 IMAGE 场景，以计算温室气体浓度和温度(包括不确定范围)的轨迹。在这里，用于 MAGICC 计算的不确定性范围是基于现有的更复杂的碳循环和气候模型。我们使用了温室气体浓度范围和温度结果的含义来描述这里的不确定性范围，如图中阴影区域所示。对于温度，较宽的阴影区域表示由于碳循环和气候敏感性的不确定性而产生的不确定性。对于基线情景，全球平均气温在2050年几乎与工业化前水平线性上升至2.1摄氏度，在2100年上升至3.7摄氏度(不确定性范围为3-5摄氏度)。在缓解方案中，到2100年全球平均气温上升幅度限制在1.9摄氏度。同样，存在相当大的不确定性。图7表明，到本世纪末，与工业化前水平相比，减缓气候变化的情况也可能导致气温上升2.6摄氏度。由于这里提出的缓解方案是科学文献中最严格的方案之一(参考 Clarke 等，2010; Edenhofer 等，2010; Fisher 等，2007) ，可以得出两个重要的结论。首先，分析表明，全球变暖可以得到缓解，但不能停止。第二，严格的设想也可能导致气候变化大大超过2摄氏度这一观察结果可能意味着，对冲适应政策以防止气候变暖加剧可能具有相当大的价值。例如，这些政策可能是“ ... ... 目标是2摄氏度，但准备3摄氏度”。在下面的影响评估中，我们关注的是中央气候变化预测。通过源自 HadCM2气候模型的重新尺度模式(图8)构建了与全球平均温度变化相关的全球0.5 ° × 0.5 ° 尺度上的月平均温度和降水量的变化。结果表明，高纬度地区年平均气温变化大于低纬度地区，降水量变化具有明显的空间变化特征。关于气候变化的预期模式，特别是降水的模式，存在着相当大的分歧: 因此，本文件提出的影响结果只代表一种可能的结果。5结果: 不同情景下的影响和适应5.1导言 IPCC 的第四次评估报告(IPCC，2007)概述了气候影响。其中一些影响来自平均气候的变化，但其他影响可能来自极端事件的变化。表1总结了一些影响，包括健康、农业、水资源供应、沿海洪水、城市地区和能源系统，以及气候系统的大规模破坏(相比之下，生物多样性和生态系统服务没有包括在内)。如前所述，大多数文献都将气候变化视为“渐进现象”(Agrawala and Fankhauser，2008)。这对于低概率和高拥有属性的影响是有问题的(见下文)。在这个探索性的分析中，我们描绘了一些影响和适应需求。我们的目标是涵盖表1中提到的几个关键影响，但是评估受到可以很容易耦合的模型的限制。因此，这些描述并不打算详尽无遗，而是在一定程度上说明了一些影响的严重程度和关键的适应挑战。在介绍我们的结果时，我们使用了几个基于上述情景的新模型运行(例如疟疾、水资源、海平面上升、加热和降温需求)。然而，我们也根据这里提出的两种情景(与温度相关的死亡率、农业和极端事件)评估了 IPCC 第四次评估报告中的现有信息。5.2人类健康: 与温度相关的死亡率和疟疾气候变化的健康影响需要在其他更重要的人类健康驱动因素的背景下看待，包括与生活方式相关的因素(Hilderink 等，2008)。我们在这里关注与温度有关的死亡率和疟疾。5.2.1与温度有关的死亡率与温度有关的死亡率影响可能通过极端温度的变化、平均温度的变化或温度的季节性变化而发生，文献显示的结果各不相同。McMichael 等(1996)使用相对风险比对温度相关死亡率进行了估计，表明存在死亡率最低的最佳温度(也称为 U 形剂量-反应关系)。如果温度升高，热应激相关的死亡率增加，但寒冷相关的死亡率下降。Tol (2002a)的结论是，从货币角度来看，由于气候变化导致的与寒冷有关的死亡率的下降超过与热有关的死亡率的增加。然而，这一结论受到用于评估生命价值的方法的影响，也受到平均和区域温度以及温度和健康之间关系的巨大不确定性的影响。适应可能发生在人体生理学适应更高的温度(麦克迈克尔等人，1996) ，行为的改变和空气调节使用的增加(Kinney 等人，2008)。考虑到使用温度和死亡率之间的剂量-反应关系的复杂性，我们还没有尝试在这里量化这些。5.2.2疟疾疟疾与气候变化之间的关系引起了人们的极大关注。在本文中，我们还重点讨论了气候引起的疟疾风险变化。每年有100多万人死于疟疾，其中大多数是非洲儿童。疟疾是一种由病媒传播的传染病。按蚊(传播疟疾感染的媒介)只能在平均温度高、没有霜冻和降水充足的气候中生存。MARA/ARMA 疟疾适宜性模型(Craig et al。 ，1999)综合了这些因素来确定气候适宜的地区。然而，疟疾造成的死亡率也受到诸如获得预防措施(包括室内喷洒和经杀虫剂处理的蚊帐)和获得医疗保健等因素的严重影响。在 MARA/ARMA 模型中，这些因素与收入和城市化有关。图9显示了这个模型在本文情景下的结果。自主适应的影响(作为收入增加的功能)减少了约50% 的疟疾死亡，特别是在非洲(主要是由于更好地提供卫生保健)。相比之下，气候的影响——特别是缓解设想方案与基准设想方案之间的差异要小得多。减缓措施将疟疾健康风险降低约2% (2050年)。因此，适应对疟疾控制的影响比缓解更具决定性(这一发现在现有文献中似乎很有说服力)。5.3农业: 对产量的影响伊斯特林等人(2007)综合了大量关于气候变化对作物生长的影响的研究，包括适应和不适应。结果总结为全球平均气温升高的函数，尽管实际上气温和降水模式的变化以及 CO2施肥都起作用。例如，二氧化碳施肥的影响部分抵消了气候变化的影响。结果可以用来评估我们的情景的气候影响使用最佳拟合多项式从伊斯特林等人(2007年) ，这表明产量的影响作为平均温度变化的函数。我们在每种情况下都采用全球平均气温变化作为一种假设，并将其作为预期的局部平均气温变化的指示。这意味着我们的影响估计可能是保守的，因为在许多陆地地区，气温上升可能比全球平均水平更强。我们研究了基线(4摄氏度)和缓解(2摄氏度)情景对玉米、小麦和水稻的影响，包括适应和不适应(见图10; 2100年热带和温带地区的结果; 这些影响是由于气候变化以外的其他因素导致的产量增加的额外影响)。虽然结果很不确定，但有些结论似乎是可能的。首先，基准情景(无适应性)导致所有情况下的产量(相对于没有气候变化的情况)大幅度下降: 气候变化影响可能会使所研究作物(2050年)的总体产量下降10-35% 。其次，无论是采取缓解还是适应措施，都会限制产量的下降。然而，在热带地区，影响仍然是负面的，通常在10% 左右的损失。第三，缓解和适应相结合可能会导致从今天的情况得到改善。对温带地区的农业影响可能更为积极，但前提是气温较高的优势不被极端天气的影响所抵消。这些结果突出表明，需要同时考虑缓解和适应问题。所提出的结果以气专委的评估为基础，代表了范围广泛的模型。结果也可以通过个别研究来说明。比如，Tubiello 和 Fischer (2007)发现，缓解方案可以显著降低全球农业气候变化的成本。同样，Fischer 等人(2007)阐述了适应水灌溉需求的重要性。他们发现，缓解措施减少了约40% 的农业用水需求，剩下60% 的影响需要适应。在处理对农业的影响时，干旱和热浪胁迫都起着重要作用。图11显示了在欧洲，假设各种形式的适应(Mechler 等，2010; Moriondo 等，2010) ，干旱和热浪胁迫对2 °C 升温情景下作物产量的影响。22在 HADCM3气候模式的基础上，利用 Cropsyst 模式对2030-2060年的时间片进行了计算。利用当前和未来的作物管理措施，模拟了春小麦的冬夏季作物产量。所考虑的适应选择包括将播种日期推迟几天和使用生长周期较长/较短的品种。结果显示，南欧和法国部分地区如今已经特别容易受到干旱和热应激的影响，即使在2摄氏度(缓解)的情况下，这种情况预计也会恶化(图11图 A)。当考虑两种适应策略与缓解措施相结合时(图11图 B 和 C) ，欧洲的许多地区实际上可能会受益。尤其是北欧，可以利用降水量较高的优势，使用生长周期较长的作物品种。相比之下，在南欧，同样的适应办法将产生额外的负面影响，因为作物发展将转向夏季，而夏季较长的干旱期和热浪可能会严重影响作物生长。此外，结果表明，虽然适应存在一些区域特有的限制，但整体适应将有效减少对欧洲农业部门的影响。5.4水资源: 潜在的水资源可利用性使用全球范围的水资源影响模型(Arnell，2003)评估了这两种情景对水资源压力变化的影响。图12显示了在基线情景和缓解情景(使用 HadCM2气候模型模式)下，到2100年(相对于1961-1990年的平均值)平均年径流量的百分比变化。如果年平均径流量小于1000 m3/人均/年，我们将流域定义为处于水资源紧张状态(文献中也使用了其他定义)。气候变化的影响是通过总结(i)生活在径流显着减少(增加)的水资源紧张流域的人口(通常超过5-10%)和(ii)生活在由于气候变化而变得水资源紧张(不再水资源紧张)的流域的人口。因气候变化而面对水资源压力上升或下降的人数没有计算在内，原因有二: (i)水资源压力下降的负面影响大于水资源压力下降的有利影响; (ii)面对水资源压力上升或下降的地区分布广泛，一个地区的“盈余”不能抵消另一个地区的“亏损”。结果显示，在2050年、2080年和2100年，缓解和基线假设情景之间，水资源紧张程度增加的风险暴露存在显著差异。到2020年，这两种情况的径流量几乎没有差别。图13显示了在这两种情景下，由于气候变化而面临水资源压力增加或减少的人数。在基线和缓解方案中，生活在缺水流域的人们显然从增加的可用水量中受益，这个数字大于暴露于径流减少的人数，但是，如上所述，我们并不关注净效应。面临水资源压力变化的人数对假定的气候变化模式很敏感。与基线相比，缓解方案在2050年、2080年和2100年分别减少了1.35亿(减少12% 的影响)、2.81亿(减少20%)和4.57亿(减少30%)面临水资源压力增加的人口。然而，与此同时，也有人从气候变化中受益。具有积极和消极影响的群体的相对规模取决于所使用的气候模型(这里只使用了哈德利模式)。显然，减缓气候变化也减少了从气候变化中受益的人数。同样显而易见的是，缓解并不能消除气候变化对供水的影响，因此需要对气候变化而面临更大水资源压力的其余10亿人进行适应。适应措施可以包括增加水的储存、水的运输或通过提高效率来减少水的需求。基本结果表明，缓解效果因地区而异。事实上，在一些地区，缓解甚至可能增加暴露于压力增加的人数。具体的不确定性分析表明，结果高度依赖于气候变化引起的降水模式变化的不确定性。5.5海平面上升气候变化的另一个重要影响是海平面上升。利用 IMAGE 模型的 MAGICC 组成部分，预测了这两种情况下的全球平均海平面上升。由于海平面对全球变暖的反应迟缓，预测主要在本世纪下半叶出现分歧: 在4摄氏度和2摄氏度的情况下，2050年海平面上升分别为35厘米和31厘米，在2100年分别为71厘米和49厘米。这些预测不包括格陵兰岛和南极洲冰盖的潜在加速贡献，这可能导致更高的海平面上升，但潜在的过程没有得到充分的理解，目前不包括在气候模型中(Meehl 等，2007; Nicholls 等，2010; Vermeer 和 Rahmstorf，2009)。我们使用 DIVA 模型来评估海平面上升、相关风暴潮和社会经济发展在这两种情况下的损害和适应成本，同时考虑到海岸侵蚀(直接和间接)、强迫迁移、沿海洪水(包括河流)以及盐度入侵三角洲和河口。对于每种情况，模型首先在没有堤坝的情况下运行，然后根据提高堤坝和滋养海滩的情况进行适应(DINAS-COAST Consortium，2006; Hinkel and Klein，2009)。由于缺乏全球数据和这些过程的一般模型，无法列入诸如沿海含水层盐度入侵、沿海湿地和生物多样性丧失等进一步影响，以及诸如盐度入侵屏障、港口升级、倒退区和基于生态系统的保护等进一步适应办法。图14显示，与缓解水平无关的是，适应相当有效地降低了全球总体成本，这说明即使在目标远大的缓解情况下也必须进行适应。在总体规模上，仅通过适应战略比仅通过缓解战略可以避免更多的损害，尽管两者结合起来产生的积极影响最大。然而，从较贫穷和小岛屿国家的角度来看，严格的缓解措施对于将风险保持在可控水平是必要的。即使没有海平面上升，为了保护仅由于社会经济发展而增加的泛滥平原资产，适应措施也是具有成本效益的。虽然这将涉及大量的投资流动(全球数百亿美元) ，但它们在全球 GDP 中所占的比例相对较小，即使对于基线情景下的海平面上升而言也是如此。然而，对于个别国家或地区(特别是小岛屿国家)来说，这些成本可能占 GDP 的比例要大得多，包括完全丧失的风险。5.6供热和供冷需求(居住区和社会)气候变化可能会影响空间供冷和供热的需求。因此，我们建立了一套简单的关系来描述住宅部门的供暖和空气调节需求，并探索了气候变化对这种模拟能源需求的影响(Isaac and van Vuuren，2009)。显然，人口和收入的变化预计将导致下个世纪取暖和空气调节的能源需求大幅增长(见图15，没有气候变化的例子)。在气候的驱动下，制冷和制热实践的变化是自主适应的例子(即没有政策干预)。然而，适应并不是普遍的，因为人们并不总是能够做出反应。供暖和制冷需求得不到满足可能导致健康影响(如第5.2节所述)和劳动生产率的损失。除了这些影响，当室内温度超过一定水平时，舒适度也会降低。图15显示，在全球范围内，由于收入和财富的增加而不考虑气候变化的能源需求的自主增长远远大于基线情景和缓解情景中的能源需求之间的差异(Isaac 和 van Vuuren (2009)显示，这对其他基线也是一个强有力的结果)。气候变化对综合能源需求的影响也小于单独对供暖和空气调节的影响，因为空气调节的增加弥补了供暖的减少。在区域和国家层面，影响可能更为显著: 例如，在印度，我们预计由于冷却增加，能源需求将大幅增加，而在西欧和美国，我们预计由于供暖减少，能源需求将大幅减少。5.7极端事件气候变化预计将导致一些与天气有关的极端事件的频率和强度发生变化(Parry et al。 ，2007)。像洪水、干旱、热浪和风暴潮这样的极端天气可能会变得更加频繁和强烈，而寒冷的极端天气，如寒潮，可能会变得不那么频繁和弱化。根据平均条件的变化来评估气候变化的风险——只有极端事件风险的变化被平均化的风险。因此，一种更基于风险、更明确地理位置的方法更可取。然而，关于灾害影响的知识是复杂和有争议的。迄今为止，只有有限的国家级研究采用概率方法预测气候变化存在的未来风险，主要集中在洪水风险(Mechler et al。 ，2010)。Feyen 等人(2009)在泛欧范围内进行的一项研究计算出，在基线情景下，预期的年度损失将增加三倍。定量风险方法的一个关键制约因素是气候预测的不确定性。对于降水，例如，模型往往不同意在局部尺度的变化迹象。这对于寻找例如洪水风险的研究尤其重要。虽然 Mechler 等人(2010)的研究旨在预测未来的风险，但他们发现未来的预测是如此的不确定，以至于作者没有根据对当今洪水影响的估计来预测未来的洪水风险。然而，目前的模型和数据似乎足以以相对较高的确定性(较慢的现象)评估干旱和热浪对农业造成的综合风险。这里提供了在2 °C 和4 °C 情景下的一些工作实例。一些研究调查了全球范围内受洪水影响的人们(Hirabayashi 和 Kanae，2009; Kundzewicz 等，2010)。样本回归分析显示，在缓解方案(2摄氏度)中，全球每年受100年洪灾影响的人口平均为2.11亿人，而基线(4摄氏度)为5.44亿人。Mirza 等人(2003)指出，对于孟加拉国这样一个易受水灾影响的国家来说，即使是2摄氏度的情况，预计也会使预计的洪水泛滥面积至少增加23-29% 。然而，应当指出的是，由于暴露、脆弱性和适应方面的不确定性，对未来洪灾损失的估计范围仍然很广。关于干旱，Burke 等人(2006)对2090年代的预测表明，对于2090年代的基线情景，每100年的极端干旱事件数量和平均干旱持续时间可能分别增加2倍和6倍。有证据表明，天气和气候相关影响造成的损害在当今已经增加，但这主要是由于财富和人口的增加(Bouwer，2010)。然而，气候变化预计将随着时间的推移而增加，并可能成为未来损害增加的一个更重要的因素。IPCC 最近的报告指出，重大事件的成本预计从占地区年 GDP 和收入的几个百分点到经济强劲的大地区的25% 不等(Parry et al。 ，2007)。过去受灾严重的小岛屿国家的灾害损失实际上已经超过了年度 GDP (Cummins and Mahul，2009)。5.8影响的经济评估成本-收益分析(CBA)是用一个共同的货币单位来表示不同战略的气候变化的成本和收益。我们在这里使用 FAIR 模型的 CBA 模块(参见模型附录 A)来获得一些关于更加聚合规模的影响的概念。对于缓解成本，FAIR 模型使用了前面介绍的 IMAGE 模型的信息。FAIR 中使用的气候损害和适应成本函数是从 AD-DICE 模型中推导出来的(De Bbu 等，2009a; Hof 等，2009a)。简而言之，AD-DICE 根据 DICE 模型的损伤函数估计适应成本(Nordhaus and Boyer，2000)。AD-DICE 将这些功能分为损害成本函数和残余损害函数，基于 DICE 模型中描述的每个影响类别的评估-农业，沿海地区，健康，住区，非市场时间使用，其他脆弱市场和灾难性影响。在这项研究中，我们假设了对气候变化的最佳适应响应(即给定一个温度变化水平，模型将适应成本和剩余影响的总和最小化)。DICE (因此 FAIR)中使用的影响估计包括: (i)实际的，可测量的经济成本(所谓的市场成本) ; 和(ii)其他的，无形的损失(非市场损失) ，使用支付意愿概念货币化。损害函数与本节前面所述的物理或经济损害没有直接关系，因为它们来自单独的来源。早先已经表明，适应成本的 FAIR 结果与文献中报道的值范围一致(Hof et al。 ，2009a)。在 FAIR 模型的默认设置和2.5% 的贴现率下，2005-2200年期间气候变化影响造成的贴现成本占全球 GDP 的比例在基线水平上接近4.5% (图16)。这些成本可能看起来高于上面提到的有限的部门分析，但是包括更多的部门和可能的灾难性事件的影响(Nordhaus and Boyer，2000)。年度成本随着时间的推移急剧上升，到2200年达到17% (注意，影响估计非常不确定，文献中可以找到更高和更低的值(Parry 等，2007; Stern，2006; Tol，2002b))。只有适应或缓解的情况下，折扣成本大幅降低到2.5% 左右(图16)。Hof 等人(2008)的研究表明，气候变化的 CBA 结果对模型假设非常敏感，其中折现率起着最重要的作用。贴现率特别重要，因为随着时间的推移，与仅适应和仅缓解设想相关的成本函数不同。3.5% 的贴现率导致仅适应情景和仅缓解情景的贴现成本分别为0.8% 和1.9% 。如果使用1.4% 的贴现率(相当于 Stern (2006)使用的贴现率) ，仅适应情景和仅缓解情景的贴现成本分别为3.2% 和2.5% 。在贴现率为2.5% 的情况下，缓解与适应相结合，贴现成本最低，即占 GDP 的2% 。与文献资料一致，适应性投资被评估为小于缓解投资和剩余损害。然而，它们在限制残余损伤方面是非常重要的。需要提及一些重要的警告。首先，对于风险的极端尾部(即低概率、高影响事件) ，计算不能被视为可靠。作为对如何处理这些风险的主观评估，Weitzman (2008)质疑 CBA 对决策者的有用性。其次，考虑到时间偏好和风险的贴现率的价值目前正在激烈争论，争论涉及主观时间偏好和风险感知(Nordhaus，2008; Price，2005; Stern，2006)。如上所述，贴现率的价值可以对结果产生很大的影响。最后，非市场影响需要对损害进行主观量化; 虽然这些影响很难货币化，但一般来说，不可逆转的变化更加困难，例如导致珊瑚礁丧失的海洋变暖(Ackerman and Heinzerling，2004)。5.9气候变化、影响和适应方面的不确定性对未来气候变化及其影响的预测有许多不确定性的来源。不确定性与因果链中的每一步都有关联: 排放、气候驱动因素(如碳循环)、气候(主要是气候敏感性和气候变化模式)以及影响(包括适应能力)。因此，对于同样的排放情景，不同的研究可能会给出非常不同的结果。事实上，这些差异往往大于不同排放情景下某一特定模型产生的差异。例如，对于本世纪末的降水变化，多模式集合平均值仅在高纬度地区超过模式间的标准差(Kundzewicz et al。 ，2007)。气候变化预测的不确定性随着时间的推移而增加。在近期(例如2020年代) ，气候模型的不确定性起着最重要的作用; 而在较长的时期(例如2090年代) ，由于排放情景的选择而产生的不确定性变得越来越重要(Jenkins and Lowe，2003)。未来气候变化对极端事件的影响尤其不确定。这部分是由于粗分辨率气候模型的较大空间和时间尺度与某些极端天气(如暴雨降水和山洪)的局部发生和短期生命之间的不匹配。由于影响和适应是在当地范围内发生的，因此需要详细的信息——这意味着不确定性的增加。较大的不确定性范围表明，适应规划不应基于单一的假设情景，而是需要考虑到大范围的预测。6结论在本文中，我们讨论了情景分析如何有助于评估缓解和适应战略。我们还提出了两个集成的场景作为分析的起点。这些设想方案明确地将缓解和适应行动纳入了几个指标，并涵盖了社会经济发展与影响之间的几个重要联系和反馈(例如，考虑了气候变化对土地利用和缓解的影响)。我们为选定的一些指标确定了这些设想方案的影响，主要侧重于平均气候变化。基于我们的工作，我们得出以下结论: •通过描述两套对比鲜明的世界可能的气候变化轨迹，我们为对减缓、适应和气候影响之间的相互作用进行更加综合的分析奠定了基础。第一种情况(不采取缓解措施)预计将导致全球平均气温在本世纪末上升4摄氏度左右(气候参数和当前经济趋势的最有可能值)。正如我们的一些分析所显示的，这种情况有很高的适应需求。第二种设想假设有严格的缓解措施，并将全球平均气温变化限制在2 °C，概率为50% 。即使在这种情况下，也需要大量的适应措施。•这里提出的综合情景分析可以为探讨政策选择的不同后果(包括不确定性)奠定良好基础; 鉴于不确定性，确定缓解、适应和剩余损害之间的最佳组合是不可行的。正如本文所讨论的那样，衡量气候变化的后果和各种政策回应的复杂性在于规模、空间和时间上的巨大差异; 巨大的不确定性; 以及行为者之间利益的明显差异(例如，他们是气候变化的肇事者还是受害者)。因此，对风险的主观解释将始终发挥重要作用。尽管如此，情景分析可以提供对可能结果和风险的描述。在这个阶段，成本和收益的货币评估(第5.8节)不能与前面章节中对物理变化的描述联系起来。有效的气候政策包括适应和减缓。模型计算表明，可以设计缓解设想方案，使全球平均气温上升2摄氏度，从而达到对气候敏感性的最佳猜测。然而，即使是这些严格的情况也可能导致全球平均气温上升超过2.5摄氏度(最多上升1.5摄氏度)和区域气温变化更大。本文件探讨的大多数影响都表明需要将缓解和适应结合起来。例如，在应对海平面上升(至少在21世纪)方面，适应措施可能比缓解措施更有效，但缓解措施在减少损害和降低适应成本方面仍然可以发挥作用。农业提供了一个明显需要适应和缓解的例子。如果不采取适应和减缓行动，预计许多区域的农作物产量将因气候变化而受到不利影响。如果没有严格的缓解措施，适应只能限制负面影响，而不能消除它们。缓解的一个好处是，它影响到所有影响类别，而适应需要根据影响和环境进行调整。•尽管气候变化的影响可能很严重，而且根据主观选择，可能需要制定严格的气候政策，但本研究评估的影响(鉴于目前的技术水平)可能仍然是全球范围内人口变化和经济增长的次要影响。然而，需要注意的是(见下文)。虽然气候变化可能对数百万人产生影响，但其他挑战可能对人民和治理产生更大的影响。然而，应该指出的是，我们只涉及了有限的一组影响，并且主要集中在对逐渐变化的气候的平均估计上，例如，没有涉及灾难性的、影响非常大的、极低概率的事件(Weitzman，2008)。这些事件实际上可能非常严重，以至于上述结论不再成立。如果全球范围的成本仍然相对较低，就不太需要进行全球分析，以便根据故事情节的一致性纳入对主要驱动因素的所有反馈。显然，在地方一级，情况可能大不相同; 对个别国家的影响可能远远大于全球一级的影响。例如，海平面上升对一些低洼岛国和国家非常重要，这些国家可能会受到巨大的适应成本和/或损害(直至完全毁灭)的显著影响。对农业而言，预计积极和消极影响将在不同地方和不同时间发生，低收入国家往往受到相对较为负面的影响。目前温度受到限制的温带地区的农业可能会受益。总之，我们认为，进一步制定在区域范围内进一步具体说明这些情况的综合设想是有益的。虽然本文提出了一个有用的第一步，它也留下了许多反馈意见仍然没有说明。本研究中的总体缓解成本估计为2 °C 情景下国内生产总值的1-2% 左右。缓解方案降低了气候变化的风险。在缓解方面的投资有几种类型的好处。首先，与气候相关的损害和适应成本得到降低。其次，不确定性也会减少，考虑到所涉及的风险，这一点很重要。虽然我们认为，在全球一级，缓解和适应之间不可能存在最佳的平衡，但我们已经表明，从长远来看，缓解和适应的成本和收益是相当的。进一步分析的重点包括评估实际变化与货币影响分析之间的联系、极端事件的可变性和变化、大规模干扰和治理的潜在作用。在我们和其他评估中，主要关注的是平均值的变化，然而，人们对与气候变化有关的极端事件(导致自然灾害) ，以及大规模的破坏(如西南极冰盾的解体)有相当大的关注，这些破坏并没有被平均值准确地描述。对气候变异性变化的预测高度不确定，迄今为止常常妨碍分析有力地预测未来的极端事件风险。不同行为者的作用是另一个问题; 某些形式的适应需要政府的积极参与; 其他形式的适应可能由私人投资者实施，例如安装空间冷却系统。这两个适应主体之间的差异与未来的情景发展有关。本文中提出的研究是作为欧盟资助的 ADAM 研究项目的一部分进行的。这篇论文的早期版本是作为《让气候变化为我们服务》一书的一部分出版的，该书由休姆和纽菲尔德编辑，剑桥大学出版社于2010年出版。附录 A.模型描述 A.1 IMAGE 2.4 IMAGE 2.4综合评估模型(Bouwman et al。 ，2006)由一系列相互关联的综合模型组成，这些模型共同描述了全球环境变化长期动态的重要因素，如空气污染、气候变化和土地使用变化。作为 IMAGE 的一部分，全球能源模型 TIMER (van Vuuren et al。 ，2006)描述了一次能源和二次能源需求和生产的长期动态以及温室气体和区域空气污染物的相关排放。模型行为主要是由各种技术的替代过程决定的，基于长期价格和燃料偏好。IMAGE 的农业模型模拟了7个作物类别和5个动物类别的生产力(Leemans 和 Born，1994)。根据一套分配规则，农产品的区域生产在空间上(0.5 ° × 0.5 °)分布(Alcamo et al。土地利用变化图和农业活动数据都被用来模拟土地利用(变化)产生的排放。MAGICC 模型利用温室气体排放量来计算全球平均气温变化(Wigley and Raper，2001)。温度变化模式是通过与大气环流模式(GCM)产生的气候变化模式联系而获得的。局限性: IMAGE 提供了对人类活动的物理描述(使用成吨的石油，生产成吨的谷物等)。更全面的宏观经济描述只能通过与其他模型的合作得到。IMAGE 作为综合评估模型的广泛覆盖范围意味着许多关键的不确定性影响模型的结果。在这种情况下，使用单一的基线(如在 ADAM 项目中)并不能完全满足所涉及的基本不确定性。A. 2 FAIR 气候政策模型 FAIR (Den Elzen 等，2008)与 IMAGE 模型一起用于确定不同排放源的减排率。全球气候计算利用了简单的气候模型 MAGICC 4.1(Wigley，2003; Wigley and Raper，2001)。所要求的全球减排量是通过计算基准和全球排放路径之间的差额得出的。FAIR 成本模型使用不同排放源的区域边际减排成本曲线(MAC) ，采用最小成本方法，在各区域之间进行分配。最近，FAIR 模型已经扩展到损害和适应成本曲线(基于 AD-DICE 模型(De Bbu 等，2009b)和估计宏观经济对 GDP 增长的影响的能力(Hof 等，2008))。这使模型能够探讨减缓和适应综合战略的经济影响。限制: 为了灵活起见，公平竞争模式不包括部门宏观经济模式或能源模式。因此，该模型从一个局部均衡的方法工作-和更深层次的后果气候政策只能通过转发公平的结果到其他(相关)模型研究。A.3 DIVA DIVA (动态和互动脆弱性评估)是在欧盟资助的项目 DINAS-COAST 44中开发的一个沿海系统的综合模型，连同其适当的沿海数据库，对沿海地区对海平面上升的国家、区域和全球脆弱性进行动态和互动评估;  http://www.pik-potsdam.de/DINAS-COAST/。(DINAS-COAST Consortium，2006; Hinkel and Klein，2009).《综合发展战略》提供了一系列生态、社会和经济沿海脆弱性指标的定量信息，从国家以下各级到全球各级，涵盖所有沿海国家。该模型由来自各种工程、自然和社会科学学科的专家开发的若干模块组成。根据气候和社会经济情景，该模型评估了海岸侵蚀(直接和间接)、海岸洪水(包括河流)、湿地变化和盐度入侵三角洲和河口。家庭影响评估还从提高堤坝和滋养海滩的角度考虑沿海适应问题，并包括一些预先确定的适应战略，例如不予保护、充分保护或最佳保护。限制因素: 《综合可持续发展战略》排除了可能影响沿海影响的下列进程，但目前无法有把握地建立模型: 风暴频率和强度的变化、沿海快速发展和城市化导致的国内生产总值的地方分布和人口增长，以及盐度侵入沿海含水层。由于高程数据的粗分辨率和精度，还会产生更多的重要不确定性。A. 4 TIMER ——冷却/加热能源需求 TIMER 冷却/加热能源需求模型(Isaac and van Vuuren，2009)描述了冷却和加热能源的使用是几个因素的函数，包括人口水平、不断变化的收入水平和气候。对于加热和冷却，经验数据被用来校准一组系统-动态需求函数。气候(降温和升温度日)起着重要作用。该模型能够解释气候变化的影响。局限性: 对发展中国家而言，校准模型的经验基础相对较差。该模型没有描述可以提供冷却和加热需求的不同方式以及用一种技术替代另一种技术所涉及的成本。水资源影响模型水资源影响模型(Arnell，2003,2004)有两个组成部分。第一阶段采用宏观尺度水文模型 Mac-PDM 模拟全球地表(0.5 ° × 0.5 °)的径流，第二阶段通过计算人均水资源可利用率确定流域水资源压力指标。如果一个流域的年平均径流量低于每人每年1000立方米，则假定该流域面临水资源压力，这是一个半任意的阈值，广泛用于确定水资源压力区域。如果气候变化导致缺水流域的径流量显著减少，或者导致流域降低到阈值以下，那么气候变化将导致水资源压力暴露的增加。气候变化导致对相反趋势的暴露明显减少。这些变化不能直接比较; 虽然径流量的减少(和暴露量的增加)极有可能是不利的，但如果额外的水不能储存，或者在洪水增加的高流量季节发生，则径流量的增加(和暴露量的明显减少)可能不是有益的。生活在水资源压力增加的流域的人口数量可以作为暴露于气候变化的一个指标。实际影响(就真正的水资源短缺而言)将取决于现有的水资源管理结构。局限性: 水文模型不能完全模拟河流径流量，特别是在半干旱地区倾向于高估径流量。水资源指标是衡量受影响程度的指标，而不是实际影响; 它可以被视为适应需求的替代指标。答.6疟疾的危险疟疾媒介，蚊子传播感染，只能生存在适当的气候与高平均温度，没有霜冻和足够的降水。MARA/ARMA 疟疾适宜性模型(Craig et al。 ，1999)综合了这些气候因素来确定气候适宜区域。最大适合度为1和最小适合度为0所需的气候水平见表 A.1。对于水平介于0或1适合性所需水平之间的指标，使用简单函数计算水平(Craig et al。 ，1999)。利用 IMAGE 模型的输出结果，所有这些因子都是在半乘以半度的网格水平上计算的(Bouwman et al。 ，2006)。每个栅格细胞的总气候疟疾适应性是由这三个指数中的最低值决定的。局限性: MARA/ARMA 模型描述了对疟疾病媒的适用性。它没有提供蚊子传播的过程说明，也没有明确说明人们可能对增加的风险水平作出何种反应。参考文献 Ackerman and Heinzerling，2004 F。 Ackerman L。 Heinzerling 无价之宝: 关于了解一切事物的价格和一无所有的价值2004 The New Press 纽约 Agrawala and Fankhauser，2008 S。 Agrawala S。 Fankhauser 适应气候变化的经济方面。成本、收益和政策工具2008经合组织巴黎 Alcamo 等人，1998年 J。 Alcamo E。 Krol R。 Leemans J。 Bolen J。 Minnen M。 Schaeffer S。 Toet B。 Vries 全球环境变化模型: IMAGE 2.1 J。IMAGE 2.1型号1998爱思唯尔科技有限公司测试结果。Arnell，2004 N.Arnell 气候变化与全球水资源: 气候变化与社会经济情景全球环境变化14120043152 Arnell 等，2002 N.W。Arnell M.G.R.Cannell M. Hulme R.S.Kovats J.F.B.Mitchell R.J.Nicholls M.L.Parry M.J.L.利弗莫尔 · A · 怀特二氧化碳稳定化对气候变化影响的后果气候变化5342002413446 Bakkenes 等，2006 M。 Bakkenes B. Eickhout R. Alkemade 不同气候稳定化情景对欧洲植物物种的影响全球环境变化16120061928,2003年代表全球气候变化、适应和减缓全球环境变化132003年16巴克等人，2009年巴克，t。 ，肯伯，M。 ，Scrieciu，S。打破气候僵局。降低成本: 合作气候行动的经济效益。气候小组，托尼 · 布莱尔办公室，4CMR-剑桥大学和 Cambridge Econometrics。Barker and Scrieciu，2010 T Barker SS.Scrieciu 用 E3MG 模拟低稳定性: 走向模拟能源-环境-经济系统动力学的“新经济学”方法能源期刊31特刊12010137164 Barker 等，2008 T。Scrieciu T. Foxon 实现八国集团50% 的目标: 使用宏观经济计量模型 E3MG 气候政策82008 S30 S45 Berkhout 等,2002 F. Berkhout J. Hertin A. Jordan 气候变化影响评估中的社会经济未来: 使用情景作为“学习机器”全球环境变化12220028395 Bouwer，2010 L.M。人为气候变化造成的灾害损失增加了吗？2010年美国气象学会简报10.1175/2010BAMS 3092.1 Bouwman 等人，2006年 A.F. Bouwman T. Kram K. Klein Goldewijk 全球环境变化综合模拟。2006年荷兰环境评估机构 Bilthoven 228页。(出版物500110002/2006) Burke 等人，2006 E.J。 Burke S.J。 Brown N. Christidis 利用哈德利中心气候模型对21世纪全球干旱的近期演变和预测进行建模。《水文气象学杂志》2006年7月1113日1125 Clarke 等人，2010年 L。 Clarke J. Edmonds V. Krey R. Richels S. Rose M. Tavoni 国际气候政策架构: EMF 22国际情景的概述能源经济学31号补编。2010年 S64/s81哥本哈根协议，2009年哥本哈根协议，2009年。(2009年12月18日哥本哈根协议)。2009年联合国气候变化会议，哥本哈根。Craig 等人，1999 M.H。Craig R.W.基于气候的疟疾在非洲传播的分布模型寄生虫学今天1531999105111康明斯和马胡尔，2009 J.D。发展中国家巨灾风险融资: 公共干预原则，2009年世界银行华盛顿，DC De Bbu 等，2009a K.C。德布鲁姆 R.B。Dellink S. Agrawala 适应气候变化的经济方面: 适应成本和效益综合评估模型，2009年经合组织巴黎德布鲁恩等，2009 b。德布鲁姆 R.B。Dellink R.S.J.Tol AD-DICE: DICE 模型气候变化951-220096381 Den Elzen 等，2008 M.G。J.登伊尔森私人侦探社。Lucas D.P.为实现低二氧化碳当量浓度而采取的区域减排行动和分配办法下的排放限额费用气候变化9032008243268 Den Elzen 和 van Vuuren，2007 M.G。J.Den Elzen D.P.104-46/2007/17931/17936 DINAS-COAST 财团，2006年 DINAS-COAST 财团，2006年，更有可能以更低的成本实现长期温度目标的范维伦峰值美国国家科学院院刊。DIVA 1.5.5光盘，德国波茨坦气候影响研究所。伊斯特林等，2007 W。伊斯特林 P。阿加瓦尔 P。巴蒂玛 K。布兰德 L。埃尔达 M。霍登 A。基里连科 J。莫顿 J。Soussana J. Schmidhuber F.N. Tubiello 食品、纤维和森林产品 M.L. Parry O.F. Canziani J.P. Palutikof P.J. van der Linden C.E. Hanson 气候变化2007: 影响、适应和脆弱性。第二工作组对政府间气候变化专门委员会2007年剑桥大学出版社第四次评估报告的贡献剑桥，英国欧共体，2006年欧共体2050年世界能源技术展望(WETO H2)2006年欧盟委员会布鲁塞尔埃登霍费尔等人,2010 O。 Edenhofer B。 Knopf T。 Barker L。 Baumstart E。 Bellevrat B。 Chateau P。 Criqui M。 Isaac A。 Kitous S。 Kypreos M。 Leimbach K。 Lessmann B。 Magné S。 Scrieciu H。 Turton D。Van Vuuren 低稳定性的经济学: 减缓战略和成本的模型比较能源杂志31 SI-120101148环境变化研究所，2009环境变化研究所，2009。国际气候会议-4度及以上。英国牛津大学环境变化研究所，9月28日至30日。Feyen J.I. Barredo R. Dankers 全球变暖和城市土地利用变化对欧洲洪水的影响。迈向工程、设计和管理方法的一体化2009 Taylor and Francis Group London Fischer et al。 ，2007 G. Fischer F.N. Tubiello H. van Velthuizen D.Wiberg 气候变化对灌溉用水需求的影响: 缓解的影响，1990-2080技术预测和社会变化747200710831107 Fisher et al。时间序列: K.Jiang M.Kainuma E. La Rovere A. Matysek A. Rana K. Riahi R. Richels S. Rose D. van Vuuren R. Warren P. Ambrosi F. Birol D. Bouille C. Clapp B. Eickhout T. Hanaoka M.D. Mastrandrea Y.Matsuoko B.O’Neill H. Pitcher S. Rao F. Toth 与长期减缓有关的问题:。减缓气候变化。第三工作组对2007年政府间气候变化专门委员会剑桥大学出版社第四次评估报告的贡献,2010年 A. Hayashi K. Akimoto F. Sano S. Mori T. Tomoda 评估全球变暖对不同稳定水平的影响，作为确定长期稳定目标气候变化的一个步骤98201087112 Hilderink et al。 ，2008 H. Hilderink P.L。卢卡斯 · A · 滕霍夫 · 科克 · M · 德沃斯 · P · 詹森 · J · 梅耶尔 · A · 费伯尔 · A · 伊格纳修克 · A · 彼得森 · H · J。M.2008年荷兰环境评估机构 Bilthoven Hinkel 和 Klein，2009年 J。T.Klein 整合知识以评估海平面上升对沿海脆弱性的影响: DIVA 工具全球环境变化的发展1932009384395 Hirabayashi and Kanae，2009 Y. Hirabayashi S. Kanae 第一次估计未来全球人口面临洪水风险水文研究快报3200969 Hof 等，2009a A.F。霍夫 K.C。德布鲁姆 R.B。Dellink M.G.J.Elzen D.P.不同减缓战略对国际适应融资的影响环境科学和政策1272009832843 Hof 等，2009b A.F。霍夫・德・布鲁姆・ R ・德林克・ M.G。J.Elzen D.P.2012年后的全球气候治理: 架构、机构和适应2009年剑桥大学出版社剑桥霍夫等人，2008年 A.F。Hof M.G.J.Elzen D.P.范维伦分析气候政策的成本和收益: 价值判断和科学不确定性全球环境变化1832008412424 IMAGE-team，2001 IMAGE-team，2001。IPCC SRES 情景的图像2.2实现。全面分析21世纪的排放、气候变化和影响。RIVM 光盘出版物481508018，比尔特霍芬国家公共卫生与环境研究所。政府间气候变化专门委员会，2007年，2007年。2007年气候变化: 综合报告。第一、第二和第三工作组对政府间气候变化专门委员会第四次评估报告的贡献。政府间气候变化专门委员会，日内瓦，104页。能源政策背景下的全球住宅部门供暖和空气调节的能源需求建模。处理 UKCIP02气候变化情景中的不确定性。埃克塞特气象局哈德利中心技术说明44。Kinney et al。 ，2008 P.L。 Kinney M.S. O’Neill M.L。 Bell J. Schwartz 用于估计气候变化对与热有关的死亡的影响的方法: 挑战和机会环境科学和政策11872008 Klein et al。 ，2007 R.J.T. Klein S. Huq F. Denton T.E. Downing R.G. Richels J.B. Robinson F.L. 适应和缓解之间的相互关系。2007年气候变化。影响、适应和脆弱性。第二工作组的贡献。2007年政府间气候变化专门委员会报告剑桥大学出版社剑桥745777 Krol 等人，1997年 M. Krol J. Alcamo R. Leemans 稳定大气中二氧化碳的全球和区域影响全球变化的缓解和适应策略11997年341361 Kundzewicz 等人，2010 Z.W。Kundzewicz y. Hirabayashi 气候变化中的 S. Kanae River 洪水——水资源管理的观察和预测2010年10.1007/s11269-009-9571-6 Kundzewicz 等，2007 z.W。Kundzewicz L.J.Mata N. Arnell P. Döll P. Kabat B. Jiménez K. Miller T. Oki Z. en I. Shiklomanov 淡水资源及其管理。招架。坎齐亚尼 J.P。普鲁提克。Hanson P.J.2007年范德林登气候变化: 影响、适应和脆弱性。第二工作组对2007年政府间气候变化专门委员会剑桥大学出版社第四次评估报告的贡献。翻译。D.确定自然植被、作物和农业生产力的潜在全球分布水、空气和土壤污染761994133161 Lenton 等人，2008 T.M。Lenton H 拘留了 E. Kriegler J.W。鲁赫特 · S · 拉姆斯托夫 · H · J 大厅。Schellnhuber 地球气候系统中的倾斜元素美国国家科学院院刊1056200817861793。Manne R.G.Richels Merge: 全球气候变化综合评估模型。Zaccour Energy and Environment 2005 Springer USA McMichael et al。 ，1996 A. McMichael A. Haines R. Sloff S. Kovats Climate Change and Human Health 1996 World Health Organization Geneva Mechler et al。 ，2010 R. Mechler S. Hochrainer A. Aaheim Z. Kundzewicz N. Lugeri M. Moriondo H. Salen M. Bindi I. Banaszak A. Chorynski E. Genovese H. Kalirai J. Linnerooth-Bayer C. Lavalle D. McEvoy P. Matczak M. RadzieJewski D. Rübbelke M.-J。评估欧洲适应不断变化的洪水和干旱风险的风险管理方法 M.Hulme H. Neufeldt 让气候变化为我们服务: 关于适应和减缓战略的欧洲观点2010年剑桥大学，英国 Meehl 等人，2007年 G.A. Meehl T.F. Stocker W.D. Collins Friedlingstein A.T. Gaye J.M. Gregory A. Kitoh R. Knutti J.M. Murphy A. Noda S.C.B. Raper I.G. Watterson A.J. Weaver Z- C。赵全球气候预测2007所罗门气候变化: 物理科学基础。第一工作组对2007年剑桥大学出版社第四次评估报告的政府间气候变化专门委员会。减缓气候变化。第三工作组对2007年政府间气候变化专门委员会剑桥大学出版社第四次评估报告的贡献剑桥，United Kingdom Mirza 等，2003 M.M。问:。Mirza R.A.Warrick N.J.气候变化对恒河、 Brahmaputra 和梅格纳河洪水的影响孟加拉国气候变化572003287318 Moriondo et al。欧洲农业应对气候变化和可变性的影响和适应机会全球变化的缓解和适应战略1572010657679 Moss 等，2010 R.H。Moss J.A.Edmonds K.A.Hibbard M.R.Manning S.K.Rose D.P.Van Vuuren T.R.Kainuma T. Kram G.A.Meehl J.F.B.Mitchell N. Nakicenovic K. Riahi S.J.史密斯 R.J。早上吃饱。汤姆森 J.P。Weyant T.J.Nakicenovic 等，2000 N。 Nakicenovic 排放情景特别报告(SRES)2000剑桥大学出版社剑桥，英国 Nakicenovic 等，2000,nakicenovic P. Kolp K. Riahi M. Kainuma T. Hanaoka 排放情景评估重新审视环境经济学和政策研究732006137173 Nicholls and Lowe，2004 R.J。Nicholls J.A.全球环境变化1432004229244 Nicholls 等，2010 R.J。Nicholls N. Marinova J.A.劳 · S · 布朗 · P · 维林加 · D · 德 · 古斯芒 · J · 欣克尔。J.21世纪英国皇家学会哲学汇刊2010年10月10日。2010.029 Nordhaus and Boyer，2000 W.D.诺德豪斯 · J · 博耶(Nordhaus J. Boyer)《全球变暖: 2000年全球变暖的经济模型》麻省理工学院出版社，剑桥，马萨诸塞州，第10页。315-328 Nordhaus，2008 W.D.平衡的问题衡量全球变暖政策的选择2008年纽黑文和伦敦帕里等人的耶鲁大学出版社，2007年。招架。坎齐亚尼 J.P。PJ 的 Palutikof。Van der Linden C.E.2007年汉森气候变化: 影响、适应和脆弱性。返回文章页面第二工作组对2007年剑桥大学出版社第四次评估报告的贡献政府间气候变化专门委员会:？气候变化9932010383402 Piani 等人，2005 C. Piani D.J。陷害地方检察官。Stainforth M.R.艾伦对气候变化的约束来自数千名成员的模拟地球物理研究通讯322005 L23825 Price，2005 c 价格关于环境变化影响的代际视角: 折现未来的观点 J.L。Innes 通用汽车。Hickey H.F.2005年国际林业研究组织联合会(国际林研组织)维也纳罗斯等人，2007年，Ahammad，h,a，Rao，S. ，Riahi，K. ，van Vuuren，D. 2007.土地在气候稳定模拟中的作用: 初步观测能源模拟论坛报告。斯坦福大学。Schneider 和 Kuntz-Duriseti，2002 S.H。不确定性与气候变化政策。奈尔斯气候变化政策: 一项调查2002年岛屿出版社华盛顿特区斯特恩，2006年 N。斯特恩气候变化经济学评论2006年剑桥大学出版社剑桥斯沃特等人,斯瓦特 · L · 伯恩斯坦 · M · Ha-Duong A. Petersen 同意不同意: 政府间气候变化专门委员会评估气候变化、影响和应对措施的不确定性管理922009129斯瓦特和雷斯，2007 R · 斯瓦特 · F · 雷斯将适应和缓解工作结合起来: 纳入可持续发展政策的主流？气候政策742007288303 Tol，2002a。第二部分。环境和资源经济学2122002135160托尔，2002年 b。第一部分。基准估计环境和资源经济学21120024773 Tol，2002 c R.S.J. Tol 福利规格和气候变化的最佳控制: 基金的应用能源经济学2442002367376 Tubiello and Fischer，2007 F.N. Tubiello G. Fischer 减少气候变化对农业的影响: 减缓的全球和区域影响，2000-2080年技术预测和社会变化747200710301056联合国，2005年。世界人口前景: 2004年修订本。光盘版-扩展数据集。联合国出版物。E.05.十三.12，联合国，经济和社会事务部，人口司。Van Vliet et al。 ，2009 J.van Vliet M.G.J.den Elzen D.P. van Vuuren 会议根据延迟参与的能源经济31号补充辐射效应制定了目标。22009 S152 S162 van Vuuren et al。 ，2006 D.P. van Vuuren B。 van Ruijven M。 Hoogwijk M。 Isaac B。 De Vries TIMER 2: 模型描述和应用 L。 Bouwman T。 Kram K。 Klein-Goldewijk 全球环境变化综合模型。IMAGE 2.42006 MNP 概述-荷兰环境评估机构 Bilthoven van Vuuren 等，2007 D.P。Van Vuuren M.G.J.登伊尔森私人侦探社。Lucas B. Eickhout B.J.稳定低水平的温室气体浓度: 减少战略和成本的评估气候变化8122007119159 Van Vuuren 等，2008 a D.P。Van Vuuren B. De Vries A. Beusen P.S.C.21世纪温室气体排放的有条件概率估计基于 IPCC-SRES 设想的全球环境变化1842008635654 van Vuuren 等，2008b D.P。Van Vuuren M Meinshausen G.K.普拉特纳 · F · 乔斯 · K.M。斯特拉斯曼 S.J。Smith T.M.L.Wigley S.C.B.Raper K. Riahi F. De La Chesnaye M.G.J.登藤野 K。江 N。 Nakicenovic S。 Paltsev J.M。21世纪减缓美国国家科学院院刊的温度上升。Van Vuuren M.G.J.艾萨克不同气候制度的比较: 扩大参与的影响能源政策3712200953515362 van Vuuren 等，2010 D.P。Van Vuuren E. Stehfest M.G.J.艾萨克探索将温室气体辐射效应控制在3瓦/平方米以下的情景能源经济学31特刊12010165192维梅尔和拉姆斯托夫,2009年 M. Vermeer S. Rahmstorf 与全球气温美国国家科学院院刊相关的全球海平面10620092152721532 Weitzman，2008 Weitzman，M.L。，2008年。灾难性气候变化的经济学模型与解释。Wigley，2003 T.M.L. Wigley MAGICC/SCENGEN 4.1: 技术手册2003 UCAR-气候和全球动力学分部博尔德，CO Wigley and Raper，2001 T.M.L. Wigley S.C.B. Raper 全球平均变暖科学高预测的解释2932001451454|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fairness+in+Graph+Machine+Learning:+Recent+Advances+and+Future+Prospectives)|0|
|[Socially Responsible Machine Learning: A Causal Perspective](https://doi.org/10.1145/3580305.3599571)|Raha Moraffah, AmirHossein Karimi, Adrienne Raglin, Huan Liu|Shanghai Lixin Univ Accounting & Finance, Shanghai, Peoples R China; Pukyong Natl Univ, Grad Sch Management Technol, Busan, South Korea|The underlying assumption of using investor sentiment to predict stock prices, stock market returns, and liquidity is that of synergy between stock prices and investor sentiment. However, this synergistic relationship has received little attention in the literature. This paper investigates the synergistic pattern between stock prices and investor sentiment using social media messages from stock market investors and natural language processing techniques. At the macro level, we reveal extremely significant positive synergy between investor sentiment and stock prices. That is, when a stock price rises, investor sentiment rises, and when a stock price falls, investor sentiment falls. However, this synergy may be reversed or even disappear over a specific time period. Through a segmented measurement of the synergy between stock prices and investor sentiment over the course of a day, we also find that investor sentiment on social media is forward looking. This provides theoretical support for using investor sentiment in stock price prediction. We also examine the effect of lockdowns, the most draconian response to COVID-19, on synergy between stock prices and investor sentiment through causal inference machine learning. Our analysis shows that external anxiety can significantly affect synergy between stock prices and investor sentiment, but this effect can promote either positive or negative synergy. This paper offers a new perspective on stock price forecasting, investor sentiment, behavioral finance, and the impact of COVID-19 on the stock markets. Copyright (c) 2022 Borsa Istanbul Anonim S, irketi. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).|利用投资者情绪来预测股价、股市回报和流动性的基本假设是股价和投资者情绪之间的协同作用。然而，这种协同关系在文献中很少受到关注。本文利用来自股市投资者的社交媒体信息和自然语言处理技术，研究了股票价格与投资者情绪之间的协同关系。在宏观层面，我们发现投资者情绪与股价之间存在极其显著的创建力量。也就是说，当股价上涨时，投资者情绪上升，当股价下跌时，投资者情绪下降。然而，这种协同作用可能会逆转，甚至在特定的时间段内消失。通过对股票价格和投资者情绪在一天内的协同效应进行分段测量，我们还发现，社交媒体上的投资者情绪具有前瞻性。这为利用投资者情绪进行股价预测提供了理论支持。我们亦会透过因果推理机器学习，研究封锁对股价与投资者情绪之间的协同效应的影响。封锁是对2019冠状病毒疾病最严厉的回应。我们的分析表明，外部焦虑可以显著影响股票价格和投资者情绪之间的协同效应，但这种效应可以促进正面或负面的协同效应。本文提供了一个新的视角股票价格预测，投资者情绪，行为金融学，以及2019冠状病毒疾病对股票市场的影响。版权所有(c)2022伊斯坦布尔博尔萨 Anonim S，irketi。由 Elsevier B.V 出版。这是 CC BY-NC-nd 许可证下的一篇开放存取文章( http://creativecommons.org/licenses/BY-NC-ND/4.0/)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Socially+Responsible+Machine+Learning:+A+Causal+Perspective)|0|
|[Training Large-scale Foundation Models on Emerging AI Chips](https://doi.org/10.1145/3580305.3599573)|Aashiq Muhamed, Christian Bock, Rahul Solanki, Youngsuk Park, Yida Wang, Jun Huan|AWS AIRE, Santa Clara, CA, USA; AWS Neuron, Cupertino, CA, USA; AWS AI Labs, Munich, Germany; AWS AI Labs, Santa Clara, CA, USA|Foundation models such as ChatGPT and GPT-4 have garnered significant interest from both academia and industry due to their emergent capabilities, such as few-shot prompting, multi-step reasoning, instruction following, and model calibration. Such capabilities were previously only attainable with specially designed models, such as those using knowledge graphs, but can now be achieved on a much larger scale with foundation models. As the capabilities of foundation models have increased, so too have their sizes at a rate much faster than Moore's law. For example, the BERT large model was initially released as a 334M model in 2018, and by 2023, the largest GPT-4 models are estimated to range between 200-300B, representing an increase of three orders of magnitude in just five years. The training of foundation models requires massive computing power. For instance, training a BERT model on a single state-of-the-art GPU machine with multi-A100 chips can take several days, while training GPT-3 models on a large multi-instance GPU cluster can take several months to complete the estimated 3 X 10 23 flops. This tutorial provides an overview of the latest progress in supporting foundation model training and inference with new AI chips. It reviews progress on the modeling side, with an emphasis on the transformer architecture, and presents the system architecture supporting training and serving foundation models. This includes programming language frameworks such as PyTorch and Tensorflow, graph compilers, 3D parallelisms, and accelerators such as the GPU H100, TPU, and Trainium. Finally, the tutorial presents our experience of training foundation models using different systems.|基础模型，如 ChatGPT 和 GPT-4，由于它们的突现能力，如小镜头提示、多步推理、指令跟踪和模型校准，已经引起了学术界和工业界的极大兴趣。这种能力以前只能通过特别设计的模型来实现，例如使用知识图表的模型，但现在可以通过基础模型在更大规模上实现。随着基础模型能力的提高，它们的大小也以比摩尔定律快得多的速度增长。例如，BERT 大型机型最初于2018年作为334M 型号发布，到2023年，最大的 GPT-4型号估计在200-300b 之间，这意味着在短短五年内增加了三个数量级。基础模型的训练需要大量的计算能力。例如，在一台配有多个 A100芯片的最先进的 GPU 机器上训练 BERT 模型可能需要几天时间，而在一个大型多实例 GPU 集群上训练 GPT-3模型可能需要几个月才能完成估计的3 × 1023次失败。本教程概述了用新的 AI 芯片支持基础模型训练和推理的最新进展。在建模方面回顾了进展，重点介绍了变压器体系结构，并提出了支持培训和服务的系统体系结构基础模型。这包括编程语言框架，如 PyTorch 和 Tensorflow，图形编译器，3 d 并行，以及加速器，如 GPU H100，TPU 和 Trainium。最后，本教程介绍了使用不同系统培训基础模型的经验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Training+Large-scale+Foundation+Models+on+Emerging+AI+Chips)|0|
|[How to DP-fy ML: A Practical Tutorial to Machine Learning with Differential Privacy](https://doi.org/10.1145/3580305.3599561)|Natalia Ponomareva, Sergei Vassilvitskii, Zheng Xu, Brendan McMahan, Alexey Kurakin, Chiyaun Zhang|Google Research, Seattle, USA; Google Research, New York, USA; Google Research, Mountain View, USA|Machine Learning (ML) models are ubiquitous in real world applications and are a constant focus of research. At the same time, the community has started to realize the importance of protecting the privacy of models' training data. Differential Privacy (DP) has become a gold standard for making formal statements about data anonymization. However, while some adoption of DP has happened in industry, attempts to apply DP to real world ML models are still few and far between. The adoption of DP is hindered by limited practical guidance of what DP protection entails, what privacy guarantees to aim for, and the difficulty of achieving good privacy-utility-computation trade-offs for ML models. Tricks for tuning and maximizing performance are scattered among papers or stored in the heads of practitioners. Furthermore, the literature seems to present conflicting evidence on how and whether to apply architectural adjustments and which components are "safe'' to use with DP. The question of hyperparameter-tuning for DP models is also often overlooked. Even in academic work, best practices for rigorous reporting of privacy guarantees, like the privacy cost of any data touches and amplification by sampling peculiarities, are often glanced over. In this tutorial, we guide the attendees through in-depth overview of the field of DP ML models. We present information about achieving the best possible DP ML model with rigorous privacy guarantees. People interested in applying DP to their ML models will benefit from a clear overview of current advances and areas for improvement. We also highlight important topics such as privacy accounting and its assumptions, as well as convergence. Additionally we provide an overview of how architectural decisions affect privacy and utility of the ML models. Finally, we have a write-up survey paper that covers all these topics and can serve as further reading material for interested parties.|机器学习(ML)模型在实际应用中无处不在，一直是研究的热点。与此同时，社区已经开始意识到保护模特训练数据隐私的重要性。数据差分隐私(DP)已经成为正式声明数据匿名化的黄金标准。然而，尽管工业界已经出现了一些使用 DP 的情况，但是将 DP 应用于实际机器学习模型的尝试仍然很少。DP 的采用受到实际指导有限的阻碍，这些指导包括 DP 保护意味着什么，隐私保护的目标是什么，以及在机器学习模型中实现良好的隐私-效用-计算权衡的困难。调优和最大化性能的技巧分散在各种论文中，或存储在从业者的头脑中。此外，关于如何以及是否应用体系结构调整，以及哪些组件与 DP 一起使用是“安全”的，文献似乎提供了相互矛盾的证据。DP 模型的超参数调整问题也经常被忽视。即使在学术工作中，严格报告隐私保障的最佳实践，例如任何数据接触的隐私成本和通过采样特性放大，也经常被粗略地浏览。在本教程中，我们通过对 DP ML 模型领域的深入概述来引导与会者。我们提供的信息，以实现最好的可能的 DP 机器学习模型与严格的隐私保障。对将 DP 应用于机器学习模型感兴趣的人们将从当前进展和需要改进的领域的清晰概述中受益。我们还强调了重要的主题，如隐私会计及其假设，以及趋同。此外，我们还提供了架构决策如何影响机器学习模型的私密性和实用性的概述。最后，我们有一份书面调查报告，涵盖所有这些主题，可以作为感兴趣的各方的进一步阅读材料。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+to+DP-fy+ML:+A+Practical+Tutorial+to+Machine+Learning+with+Differential+Privacy)|0|
|[Trustworthy Machine Learning: Robustness, Generalization, and Interpretability](https://doi.org/10.1145/3580305.3599574)|Jindong Wang, Haoliang Li, Haohan Wang, Sinno Jialin Pan, Xing Xie|Google, Mountain View, CA USA; Univ Wisconsin, Madison, WI 53706 USA|An emerging problem in trustworthy machine learning is to train models that produce robust interpretations for their predictions. We take a step towards solving this problem through the lens of axiomatic attribution of neural networks. Our theory is grounded in the recent work, Integrated Gradients (IG) [STY17], in axiomatically attributing a neural network's output change to its input change. We propose training objectives in classic robust optimization models to achieve robust IG attributions. Our objectives give principled generalizations of previous objectives designed for robust predictions, and they naturally degenerate to classic soft-margin training for one-layer neural networks. We also generalize previous theory and prove that the objectives for different robust optimization models are closely related. Experiments demonstrate the effectiveness of our method, and also point to intriguing problems which hint at the need for better optimization techniques or better neural network architectures for robust attribution training.|值得信赖的机器学习中一个新出现的问题是训练模型，为它们的预测产生可靠的解释。我们通过神经网络的公理属性透镜来解决这个问题。我们的理论是基于最近的工作，综合梯度(IG)[ STY17] ，在公理归因于一个神经网络的输出变化的输入变化。在经典的鲁棒优化模型中，我们提出训练目标来获得鲁棒 IG 属性。我们的目标提供了原则性的概括以前的目标设计的稳健预测，他们自然退化到经典的软边际训练的一层神经网络。我们还推广了以往的理论，证明了不同鲁棒优化模型的目标是密切相关的。实验证明了我们的方法的有效性，也指出了有趣的问题，暗示需要更好的优化技术或更好的神经网络架构的鲁棒性归因训练。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Trustworthy+Machine+Learning:+Robustness,+Generalization,+and+Interpretability)|0|
|[Large-Scale Graph Neural Networks: The Past and New Frontiers](https://doi.org/10.1145/3580305.3599565)|Rui Xue, Haoyu Han, Tong Zhao, Neil Shah, Jiliang Tang, Xiaorui Liu|Maekawa Manufacturing Co., Ltd., Tatsuzawa 2000, Moriya-shi, Ibaraki-Prefecture 302-0118, Japan; College of Biosystems Engineering and Food Science, Zhejiang University, Huajiachi Campus, Hangzhou, PR China; Graduate School of Agricultural and Life Sciences, The University of Tokyo, Yayoi 1-1-1, Bunkyo-ku, Tokyo 113-8657, Japan|Highlights ► A real-time detection method by UV–Vis spectroscopy was developed for monitoring of ATP and viable cells on meat surface. ► The linear relationship was observed between the ATP amount and plate count with the determination coefficient of 0.95. ► The 2nd derivative of reflectance spectra gave a high correlation for the first 48 h with both ATP amount and viable cell count at 318 nm. Abstract Cleanliness monitoring at slaughterhouses depend on traditional methods, e.g. , visual inspection or swabbing. The visual inspection is not always accurate. Swabbing requires skilled workers and further plate count or ATP bioluminescence technique. To solve these problems, a rapid technique based on non-destructive UV–Vis reflectance was developed to monitor the ATP and viable cells. Samples were lean part of pork loin. The samples stored at 15 °C were analyzed at 0, 24, 48, 72, 84 and 96 h for ATP, plate count and UV–Vis reflectance. The reflectance spectra were measured from 240 to 540 nm at 20 °C, and then the area of 40 × 40 mm 2 of the sample surface was swabbed for the determination of plate count and ATP amount. The plate count on the sample surface increased from the initial count of 29 to 3.2 × 10 7 CFU/cm 2 after 84 h. The ATP amount also increased with time from the initial amount of 9.2 × 10 −15 to 2.8 × 10 −10 mol/cm 2 after 84 h. The linear relationship was observed between the ATP amount and plate count with the determination coefficient of 0.95. The 2nd derivative of raw spectra gave a high correlation for the first 48 h with both ATP amount and viable cell count showing the determination coefficient of 0.89 and 0.83, respectively at 318 nm. The results strongly suggested that the UV–Vis reflectance spectrum analysis could be used as the real-time monitoring of ATP and/or plate count on meat surface with the optimal wavelength. Keywords ATP Sanitation Real-time monitoring Non-destructive detection Pork Spectroscopy Plate count Absorbance Reflectance Meat Quality 1 Introduction Muscle foods that include both meat and poultry are an integral part of the human diet and have been so for several thousand years. However, within the past two decades public concern, as well as awareness, has been raised due to high profile food safety issues such as the BSE and foot and mouth epidemics ( Fox, 2001; Pickrell and Enserink, 2001 ). These outbreaks, along with concerns over specific pathogenic bacteria within meats have illustrated the requirement for a rapid and accurate detection system for microbial spoilage of meats within what is a large-scale production industry whose turn over is billions of £ and $ per annum ( Ellis and Goodacre, 2001 ). The major role of microorganisms in the spoilage of food and the role of food as a vector for the transmission of microbes responsible for food-borne disease are well recognized. At a slaughterhouse of poultry, pork and beef, monitoring of cleanliness depends mainly on traditional methods of visual inspection, swabbing and subsequent viable cell count or ATP bioluminescence technique ( Hawronskyj and Holah, 1997 ). This is especially important for microbial hazards associated with food process. In the case of poultry, pork and beef processing, verification of the efficacy of preventive measures to reduce or eliminate microbial hazards may be achieved by routine carcass analysis using cultural method, i.e., classical Standard Plate Count ( Bautista et al., 1997 ). However, the development of more rapid methods on a ‘real time basis’ for microbiological quality control has been in the interest of scientists ever since routine microbiological analysis was applied to foods. Rapid detection methods based on the detection of whole cells or their metabolites can be divided into two main classes: direct methods are based on the detection of cells with or without incubation and indirect methods are based on the measurement of metabolic products or other changes caused by the cell growth ( Vanne et al., 1996 ). Although rapid detection methods have been under development, conventional methods for microbial monitoring are used on the job site of slaughterhouse. However, such methods usually require operator’s skill, long analysis time and high expenses. Moreover, visual inspection is not always accurate, the swabbing requires skilled worker and further plate count analysis which usually requires 24–48 h. The conventional microbiological approach to food sampling has changed over the last half century and it has been estimated that there are currently in excess of 40 methods to measure and detect bacterial spoilage in meats ( Jay, 2005; Nychas et al., 1988 ). The development of rapid microbiological test procedures over the last two decades can be divided into two main groups: enumeration and presence–absence tests. Several commercial presence–absence (P-A) test kits area available and were evaluated over a 6-month period in 1990 by using the Ontario Ministry of the Environment P-A test for comparison by Clark and El-Shaarawi (1993) . Current rapid enumeration methods are generally based on microscopy, ATP bioluminescence or the measurement of electrical phenomena ( Ellis and Goodacre, 2001 ). The use of ATP bioluminescence assay is a logical approach and relies on the fact that all living cells contain adenosine 5’-triphosphate (ATP), which is a universal energy donor for metabolism ( Bautista et al., 1997 ). Detection of the high-energy molecule adenosine triphosphate (ATP) extracted from cells is a widely used indirect assay method. The ATP amount is measured as the light energy released by the luciferin–luciferase system in the presence of magnesium ions ( Stanley, 1989 ). The assay is rapid, only a few seconds in hygiene monitoring applications and less than an hour of most other samples. Previously, it was thought that this technology has limitations – because of the fact that ATP is present in all viable cells. Therefore, intrinsic ATP originating from the target cells must be removed enzymatically before the assay ( Vanne et al., 1996 ). Siragusa et al. (1996) stated that the major challenge in using microbial ATP as a means of determining total microbial populations in food samples is the separation of nonmicrobial ATP from microbial ATP. The basis of their described Rapid-microbial ATP assay was the use of a filtration device in which somatic ATP was extracted: then within the same device, extraction of bacterial ATP was followed by its quantification. In the case of microscopic methods sophisticated techniques have been developed where microorganisms are stained with fluorescent dyes and viewed with an epifluorescent microscope. ATP bioluminescence acts by measuring ATP levels in bacterial cells in culture in order to calculate the number of cells present in that culture ( Champiat et al., 2001; de Boer and Beurmer, 1999; D’Souza, 2001; Siragusa et al., 1996 ). The problem with this method is that ATP is the primary energy source of all living cells and the food samples themselves will also contain large amounts of this chemical which have to be destroyed before microbial ATP can be measured. Consequently, the measurement of ATP bioluminescence is probably the best suited to detection of contaminated surfaces on equipment and machinery associated with food production and preparation ( Ellis and Goodacre, 2001 ). In the case that the viable cells should only be detected, the above-mentioned limitation of ATP bioluminescence technology and drawback of microscopic method have to be taken into account. However, the total amount of ATP originating from both meat and viable cell has sufficient importance in the cleanliness evaluation, because the ATP of meat origin acts as a nutrient source for the bacteria leading to bacterial spoilage. Due to the advantages of nondestructive, free of chemical preparation and fast inspection speed, spectroscopy has been studied extensively for determining properties of agricultural products, but less for meat products as compared to plant materials ( Chan et al., 2002 ).According to the literature, VIS/NIRS technology has been used in pork to determine intramuscular fat ( Hoving-Bolink et al., 2005; Savenije et al., 2006 ), fatty acid composition ( Fernandez-Cabanas et al., 2007; Gonzalez-Martin et al., 2003, 2005 ), color ( Cozzoliono et al., 2003 ), water-holding capacity ( Brondum et al., 2000 ), presence of RN − genetic allele ( Josell et al., 2000 ), and Doroc and Iberian porl neural network classification ( del Moral et al., 2009 ), but it has not been applied for the direct qualitative classification of meats of varied quality and price ( del Moral et al., 2009 ). Moreover, only a few reports are available for determination of quality of food products by using reflectance data. From these current conditions, the objective of this study was to develop a real-time detection method for monitoring of ATP and viable cells on meat surface by using reflectance spectra that could be used for sanitation management. 2 Materials and methods 2.1 Meat samples The lean part of pork loin samples sliced in 5-mm thick was obtained from a retailer. It was slaughtered 3 days ago and kept in the marketing conditions at the retailer shop. A total of 24-sliced samples were cut into pieces of about 6 × 6 cm 2 , and were individually placed in sterilized Petri dishes. 2.2 Experimental setup The samples were separated into six groups with four samples of each and were stored in a constant temperature chamber at 15 °C. The storage temperature was selected as the highest temperature in a working room of a slaughterhouse, where the temperature is usually controlled from 10 to 15 °C in consideration of worker’s health, according to our conversation with the slaughterhouse management. Measurements were conducted after 0, 24, 48, 72, 84 and 96 h of storage. The each value shown is a mean of four pieces. The experiment was repeated thrice to validate the results. Similar results were obtained in all the repeated experiments. Here, for simplicity, results of only one experiment are shown. 2.3 UV–Vis reflectance spectrum A dual beam spectrometer (UV-3600, Shimadzu Co., Kyoto, Japan) equipped with an integrating sphere setup was used for recording reflectance spectrum from a surface of meat sample (9 × 20 mm 2 ). Measured range of wave length was 240–1200 nm with the resolution of 2 nm; however, the results from 240 to 540 nm are only shown in the Section 3 . In order to confirm the maximum absorption wavelength of ATP, the transmittance of serial dilutions of ATP standard solution (LL-100-1, TOYO B-Net Co., Tokyo, Japan) was obtained with 10 mm quartz cells. 2.4 Spectral data pre-treatment Spectral data are often pre-processed to reduce undesirable systemtic noise, such as baseline variation, light scattering, path length differences and so on, and enhance the contribution of the chemical composition (Tigabu and Oden, 2002). In this study, two types of pre-processing were employed: Savitzky–Golay 1st and 2nd derivative. In our case, the possible sources of systematic variation could be due to the path length slight difference arising from the positioning of individual meat samples with slight different sizes during scanning. 2.5 Sampling protocol and microbiological analysis 2.5.1 Sampling protocol Sampling of materials on pork meat surface (40 × 40 mm 2 ) covering the area for spectroscopic measurement was carried out using a swab technique. To ensure adequate sampling, the sample was swabbed in a horizontal pattern and again in a vertical pattern being rotated between the index finger and the thumb in a back and forth motion according to Bautista et al. (1997) . The end of cotton bud used for swabbing was cut into 9 ml of sterilized water and then the swab sample was stirred well for the further examination for plate count and ATP determination. 2.5.2 Plate count Serial dilutions of the swab sample were prepared from the phosphate buffer solution in which the swab was immersed and 1 ml of the dilution was dispensed onto Petrifilms™ (AC plate, Sumitomo 3M Ltd., Tokyo, Japan) for total aerobic counts. The Petrifilms™ were incubated for 48 h at 35 °C. 2.5.3 ATP bioluminescence assay One hundred microliters of the swab sample (phosphate buffer solution in which the swab was immersed) was injected into a fresh cuvette placed in a luminometer (Luminescenser MCA, Atto Corporation, Tokyo, Japan), and then, 100 μl of Extractant (LL-100-2, Toyo B-Net Co. Ltd., Tokyo, Japan) was added into it. After 10 s, 100 μl of Luciferin–luciferase complex (LL-100-1, Toyo B-Net Co. Ltd., Tokyo, Japan) was added, and the light output was measured. From each swab, two measurements were taken and means were calculated to determine relative light units (RLU). The RLU was then converted into the amount of ATP by a standard curve constructed with ATP standard solution (LL-100-1, Toyo B-Net Co. Ltd., Tokyo, Japan) in the range of 10 −16 –10 −11 mole/100 microliters. 2.6 Statistical analysis The samples of four pieces of pork meat were selected at random for the storage time period. The regression analysis was carried out to know the relationship between ATP contents and plate count. The raw data had background information; therefore, it was converted by using the 1st and the 2nd derivatives, and the best one was selected. 3 Results 3.1 Plate count The plate count on the sample meat surface increased with the storage time period. At the outset of the experiment, the initial count was 29 CFU/cm 2 and 84 h after storage it was 3.2 × 10 7 CFU/cm 2 . 3.2 ATP content The amount of ATP increased with storage time period from the initial amount of 9.2 × 10 −5 to 2.8 × 10 −10 mol/cm 2 (84 h after storage). A linear relationship was observed between the amount of ATP and the plate count with the determination coefficient (R 2 ) of 0.95 as shown in Fig. 1 . 3.3 Absorption maximum of pure ATP The transmittance of ATP solutions of different concentration from 1 × 10 −4 to 5.85 × 10 −6 M are shown in Fig. 2 . It shows that the transmittance decreased with an increase in the ATP concentration, and spectra taken for all samples of different ATP concentrations showed that the maximum absorbance related to the decrease in transmittance was at 260 nm ( Fig. 2 ). 3.4 Estimation of ATP and plate count from reflectance The reflectance spectra obtained at 0–84 h of storage are shown in Fig. 3 in the UV–Vis range (from 240 to 540 nm). There was a very little difference between the reflectance at 0 h and that at 24 h. The reflectance of samples taken at 48, 72 and 84 h, however, showed a decreasing trend with increase in the storage time period. The 2nd derivative of reflectance data selected as the best between 1st and 2nd derivatives of reflectance is shown in Fig. 4 . Many upward and downward peaks were observed and the analysis of correlation of peaks at 298, 318, 344 and 374 nm in UV range was conducted. Fig. 5 shows the correlation coefficient between the 2nd derivative of reflectance and log (ATP). This gave a high correlation between the values of the 2nd derivative and log (ATP). Considering bathochromic shift, any of these four wave lengths could be taken as the maximum absorption of ATP. 4 Discussion Spectroscopic methods have gained importance in the evaluation of food quality attributes during the last decades (Nadai, 1983; Nadai and Mihalyi-Kengyel, 1984). Although NIR spectra reflect several parameters relating to complex quality of food (Williams and Norris, 2001), the information on ATP and/or microorganisms can not be detected in the range of NIR. Therefore, in the present study, UV–Vis was applied ranging from 240 to 540 nm. 4.1 Plate count In this study, samples were evaluated as fresh until that time when bacterial counts crossed the boundary line of 107 CFU/g and no putrid odor could be perceived. After 72 h, the plate count reached the order of 107 CFU/g and samples gave off a faint putrid odor. These samples were in the initial stage of spoilage and would be regarded as unacceptable. Plate count is a fundamental index of meat spoilage, and count of 10 7 CFU/g in meat is regarded as unacceptable ( Brown, 1982 ). Detection of the order of 10 6 CFU/g is important as this is achieved just before the meat reaches the unacceptable stage. Fresh meats generally have a pH range between 5.5 and 5.9 and contain sufficient glucose and other simple carbohydrates to support approximately 10 9 CFU/cm 2 . The organisms that grow the fastest and utilize glucose at refrigeration temperatures are the pseudomonas ( Gill and Newton, 1977; Jay, 2005; Seymour et al., 1994 ). At levels of 10 7 CFU/cm 2 off-odors may become evident in the form of a faint ‘dairy’ type aroma and once the surface population of bacteria has reached 10 8 CFU/cm 2 the supply of simple carbohydrates has been exhausted and recognizable off-odors develop leading to what is known as ‘sensory’ spoilage ( Jackson et al., 1997; Jay, 2005; Stanbridge and Davies, 1998 ). The development of off-odors is dependent upon the extent to which free amino acid utilization has occurred and these odors have been variously described as dairy/buttery/fatty/cheesy at 10 7 CFU/cm 2 through to a sickly sweet/fruity aroma at 10 8 CFU/cm 2 and finally putrid odor at 10 9 CFU/cm 2 ( Adams and Moss, 2007; Dainty et al., 1985 ). 4.2 ATP content Fig. 1 shows the linear relationship between log 10 ATP and log 10 plate count. From this figure both the ATP analysis and the plate count methods were able to assess the hygienence of pork meat samples. The ATP analysis provides only an estimation of the total bacterial count, and cannot differentiate between bacteria ( Baumgart, 1993 ). Theoretically, ATP amounts as low as 100 fg (10 −13 g) can be measured, corresponding to about 100 bacterial cells. Under practical conditions the sensitivity is about 1000 fg (10 −12 g), which corresponds to about 1000 bacterial cells or one to two yeast cells ( Heeschen et al., 1991 ). Stressed cells and cells in the stationary growth phase contain less ATP, which also affects the results ( Bulte and Reuter, 1985 ). On the other hand, however, the amount of ATP in a sample provides an estimate of the active microbial population, which is important when considering the shelf life of the product. Stressed cells can also be allowed to resuscitate before the ATP assay ( Graumlich, 1985 ). The enzyme, luciferase, converts the chemical energy provided by ATP into light by a stochiometric reaction. Thus, the amount of light produced is proportional to the concentration of ATP present, which in turn, is directly related to the number of cells in the sample ( Bautista et al., 1997 ). ATP bioluminescence is also useful for monitoring microbial contamination in scalding and chilling tanks within a meat processing operation. In the ATP bioluminescence assays for carcass contamination and process water quality, microbial cells are removed by filtration before they are lysed to release intracellular ATP. To simplify the method, it would be desirable if the step could be eliminated to allow direct detection of ATP on swabs of the carcass surface, in much the same way as for the ATP bioluminescence hygiene monitoring tests ( Griffiths, 1996 ). However, there would be no way of differentiating ATP from microbial and non-microbial sources using a swab assay, but results would be obtained within 2 min, as opposed to the 10–15 min required when a filtration step is incorporated ( Bautista et al., 1997 ). Siragusa et al. (1996) developed segmented-model statistical approach to determine the lower limits of assay sensitivity and by using this model analyzed in-plant data. According to them, the rapid microbial-ATP test responded in a linear fashion to levels of microbial contamination of >log 10 3.2 aerobic CFU/cm 2 for pork carcasses. 4.3 Absorption maximum of pure ATP As shown in Fig. 2 , the transmittance decreased with an increase in ATP concentration, and different spectra showed the minimum absorbance at 260 nm. The wave length of 260 nm was in accordance with the maximum absorbance of ATP (259 nm) as previously reported by Bagshaw (2001) . 4.4 Estimation of ATP and plate count from reflectance Reflectance ( Fig. 3 ) showed a decreasing trend with time in the UV–Vis range, although there was a very little difference between the reflectance at 0 h and that at 24 h. To remove the background effect the raw data was transformed by using the 1st and the 2nd derivates. However, the 2nd derivative was chosen because the effect was more clearer in it. The 2nd derivative technique is often used to process NIR data. It helps to separate overlapping absorption bands, remove baseline shifts and increase apparent spectral resolution (Lin et al., 2004), although the derivatives are notoriously sensitive to noise (Tsai and Philpot, 1998). Many upward and downward peaks were observed when the 2nd derivative of raw reflectance spectra for all storage time periods (from 0 to 96 h) was taken ( Fig. 4 ). The analysis of correlation of peaks at 298, 318, 344 and 374 nm was conducted. These selected wavelengths were in the UV range, i.e., less than 400 nm. The greatest differences were obtained for all selected wavelengths, between time 0 and 96 h. The maximum differences between time 0 and 96 h were in the range of 318 nm. This wavelength range could mainly differentiate between samples at 0 and 96 h. Fig. 5 shows the correlation coefficient between the 2nd derivative of reflectance and log (ATP). This gave a high correlation between the value of the 2nd derivative and log (ATP). Considering bathochromic shift, any of these four wave lengths could be taken as the maximum absorption of ATP. On the other hand, it is widely known that the spectral absorption by ATP is usually masked by protein absorbance and cannot be exploited in spectroscopic studies ( Bagshaw, 2001 ). However, the graph of correlation coefficient between the 2nd derivative of reflectance and log (plate count) shown in Fig. 6 became very similar in shape to Fig. 5 . This indicated that the 2nd derivative of reflectance involved the information of ATP in viable cells. The understanding of this is also supported by the result that the amount of ATP corresponded to the plate count ( Fig. 1 ). From these considerations, the wave length of 318 nm showing the highest correlation coefficient was selected. The linear relationship between the value of the 2nd derivative and log (ATP) for the first 48 h at 318 nm is shown in Fig. 7 with the determination coefficient of 0.89. The similar relationship was also observed between the value of the 2nd derivative and log (plate counts) for the first 48 h at 318 nm with the determination coefficient of 0.83. The duration of the first 48 h chosen here means that pork meat samples were fresh. From these results, it is expected that the selection of appropriate wave length could give the real-time monitoring of ATP and/or viable cell count on meat surface by the use of reflectance information. The plate count gives an estimate of microbial contamination whereas the ATP bioluminescence method used in this study measures total ATP, from both microbial and non-microbial sources, and may be a better measure of the overall cleanliness of the carcass. Therefore, an exact relationship between the two methods should not be expected and results obtained from the two assay systems should be interpreted separately. Multiple linear regression analysis using more than one reflectance at different wave length is a powerful tool in estimating ATP and/or viable cell count on meat surface, and can lead to higher predictive power. However, such paradigm may lead to overfitting. Accordingly, in this study, only one wave length (i.e., 318 nm) was selected for the prediction of ATP. 5 Conclusions A real-time detection method for monitoring of ATP and viable cells on meat surface by using reflectance spectra was developed. The data showed that the plate count on the sample meat surface increased and it corresponded exactly to the increase in the amount of ATP during 84 h storage at 15 °C. The linear relationship between the amount of ATP and plate count was supported by its determination coefficient of 0.95. Reflectance showed a decreasing trend with time in UV–Vis range and at the peak of 318 nm, 2nd derivative of reflectance gave a high correlation with log (ATP). As a similar high correlation was also observed between the 2nd derivative of reflectance and log (plate count), it is suggested that the 2nd derivative of reflectance involved the information of ATP in viable cells. From these observations, a linear relationship was given for the estimation of the amount of microbialy-derived ATP on the basis of reflectance analysis of meat surface. Hence, the developed technique can give a powerful way for monitoring of cleanness at a slaughterhouse. Acknowledgements This research was partly funded by the Japan Society for the Promotion of Science (JSPS) Grant No. 19:07178 . References Adams and Moss, 2007 Adams, M.R., Moss, M.O., 2007. Food Microbiology, third ed. The Royal Society of Chemistry, Cambridge, pp. 138. Bagshaw, 2001 C.R. Bagshaw ATP analogues at a glance Journal of Cell Science 114 2001 459 460 Baumgart, 1993 J. Baumgart Lebensmitteluberwachung und–qualitatssicherung Mikrobiologisch- hygienische Schnellverfahren Fleischwirtschaft 73 1993 292 396 Bautista et al., 1997 D.A. Bautista D.W. Sprung S. Barbut M.W. Griffiths A sampling regime based on an ATP bioluminescence assay to assess the quality of poultry carcasses at critical control points during processing Food Research International 30 1997 803 809 Brondum et al., 2000 J. Brondum L. Munck P. Henckel A. Karlsson E. Tornberg S.B. Engelsen Prediction and water-holding capacity and composition of porcine meat by comparative spectroscopy Meat Science 55 2000 177 185 Brown, 1982 Brown, M. H. (1982). Meat microbiology. (p. 410). New York: Applied Science Publications. Bulte and Reuter, 1985 M. Bulte G. Reuter The bioluminescence as a rapid method for the determination of the microflora of meat International Journal of Food Microbiology 2 1985 371 381 Champiat et al., 2001 D. Champiat N. Matas B. Monofort H. Fraass Applications of bioluminescence to HACCP Luminescence 16 2001 193 198 Chan et al., 2002 D.E. Chan P.N. Walker E.W. Mills Prediction of pork quality characteristics using visible and NIR spectroscopy Transactions of ASAE 45 2002 1519 1527 Clark and El-Shaarawi, 1993 J.A. Clark A.H. El-Shaarawi Evaluation of commercial presence-absence test kits for detection of total coliforms, Escherichia coli, and other indicator bacteria Applied and Environmental Microbiology 59 2 1993 380 388 Cozzoliono et al., 2003 D. Cozzoliono N. Barlocco A. Vadell F. Ballesteros G. Gallieta The use of visible and near-infrared reflectance spectroscopy to predict colour on both intact and homogenized pork muscle LWT-Food Science and Technology 36 2003 195 202 de Boer and Beurmer, 1999 E. de Boer R.R. Beurmer Methodology for detection and typing of food borne microorganisms International Journal of Food Microbiology 50 1999 119 130 del Moral et al., 2009 F.G. del Moral A. Guillen K.G. del Moral F. O’Valle L. Martinez R.G. del Moral Duroc and Iberian porl neural network classification by visible and near infrared reflectance spectroscopy Journal of Food Engineering 90 2009 540 547 Dainty et al., 1985 R.H. Dainty R.A. Edwards C.M. Hibbard Time course of volatile compound formation during refrigerated storage of naturally contaminated beef in air Journal of Applied Bacteriology 59 1985 303 309 D’Souza, 2001 S.F. D’Souza Microbial biosensors Biosensors and Bioelectronics 16 2001 337 353 Ellis and Goodacre, 2001 D.I. Ellis R. Goodacre Rapid and quantitative detection of the microbial spoilage of muscle foods: Current status and future trends Trends in Food Science & Technology. 12 2001 414 424 Fernandez-Cabanas et al., 2007 V.M. Fernandez-Cabanas A. Garrido-Varo J. Gracia-Olmo E. De Pedro P. Dardenne Optimisation of the spectral pre-treatments used for Iberian pig fat NIR calibration Chemometrics and Intelligent Laboratory System 87 2007 104 112 Fox, 2001 S. Fox WHO to convene on worldwide risk of BSE and CJD Infections in Medicine. 18 2001 69 Gonzalez-Martin et al., 2003 I. Gonzalez-Martin C. Gonzalez-Perez J. Hernandez-Menderz N. Alvarez-Gracia Determination of fatty acids in the subcutaneous fat of Iberian breed swine by near infrared spectroscopy (NIRS) with a fiber-optic probe Meat Science 65 2003 713 719 Gonzalez-Martin et al., 2005 I. Gonzalez-Martin C. Gonzalez-Perez N. Alvarez-Gracia J.M. Gonzalez-Cabrera On-line determination of fatty acids composition in intramuscular fat of Iberian pork loin by NIRS with a remote reflectance fibre optic probe Meat Science 65 2005 713 719 Gill and Newton, 1977 C.O. Gill K.G. Newton The development of aerobic spoilage flora on meat stored at chill temperatures Journal of Applied Bacteriology 43 1977 189 195 Graumlich, 1985 T.R. Graumlich Estimation of microbial populations in orange juice by bioluminescence Journal of Food Science 50 1985 116 117, 124 Griffiths, 1996 M.W. Griffiths The role of ATP bioluminescence in the food industry: new light on old problems Food Technology 50 6 1996 62 72 Hawronskyj and Holah, 1997 J.M. Hawronskyj J. Holah ATP: A universal hygiene monitor Trends in Food Science & Technology 8 1997 79 84 Heeschen et al., 1991 W.H. Heeschen G. Suhren G. Hahn Rapid methods in the dairy industry A. Vaheri R.C. Tilton A. Balows Rapid methods and Automation in Microbiology and Immunology 1991 Springer-Verlag Berlin Heidelberg 520 532 Hoving-Bolink et al., 2005 A.H. Hoving-Bolink H.W. Vedder J.W.M. Merks W.J.H. de Klein H.G.M. Reimert R. Frankhuizen W.H.A.M. van den Broek enE. Lambooji Perspective of NIRS measurements early post mortem for prediction of pork quality Meat Science 69 2005 417 423 Jackson et al., 1997 T.C. Jackson G.R. Acuff J.S. Dickson Meat, poultry, and seafood M.P. Doyle L.R. Beuchat T.J. Montville Food microbiology: fundamentals and frontiers 1997 ASM Press Washington DC 83 100 Jay, 2005 J.M. Jay Modern food microbiology sixth ed. 2005 Aspen Publishers Maryland Josell et al., 2000 A. Josell L. Martinsson C. Borggaard J.R. Anderson E. Tornberg Determination of RN - phenotype in pigs at slaughter-line using visual and near-infrared spectroscopy Meat Science 55 2000 273 278 Nychas et al., 1988 G.J. Nychas V.M. Dillon R.G. Board Glucose, the key substrate in the microbiological changes occurring in meat and certain meat products Biotechnology and applied biochemistry 10 1988 203 231 Pickrell and Enserink, 2001 J. Pickrell M. Enserink Foot-and-mouth disease – UK outbreak is latest in global epidemic Science 291 2001 1677 Savenije et al., 2006 B. Savenije G.H. Geesink J.G.P. van der Palen G. Hemke Prediction of pork quality using visible/near infrared reflectance spectroscopy Meat Science 73 2006 181 184 Seymour et al., 1994 I.J. Seymour M.B. Cole P.J. Coote A substrate-mediated assay of bacterial proton efflux/influx to predict the degree of spoilage of beef mince stored at chill temperatures Journal of Applied Bacteriology 76 1994 608 615 Siragusa et al., 1996 G.R. Siragusa W.J. Dorsa C.N. Cutter Perino L.J. Kooh-maraie Use of a newly developed rapid microbial ATP bioluminescence assay to detect microbial contamination on poultry carcasses Journal of Bioluminescence and Chemilumoscence 11 1996 297 301 Stanbridge and Davies, 1998 L.H. Stanbridge A.R. Davies The microbiology of chill-stored meat Davies R. Board The microbiology of meat and poultry 1998 Blackie Academic & Professional London 174 219 Stanley, 1989 P.E. Stanley A review of bioluminescent ATP techniques in rapid microbiology Journal of Bioluminescence and Chemiluminescence 4 1989 375 380 Vanne et al., 1996 L. Vanne M. Karwoski S. Karppinen A.M. Sjoberg HACCP-based food quality control and rapid detection methods for microorganisms Food Control 7 1996 263 276|建立了一种紫外-可见光谱实时检测肉表面 ATP 和活细胞的方法。ATP 含量与平板计数呈线性关系，测定系数为0.95。反射光谱的二阶导数与前48h 的 ATP 含量和318nm 的活细胞计数均有较高的相关性。摘要屠宰场的清洁度监测依赖于传统的方法，例如目视检查或擦拭。目视检查并不总是准确的。采样需要熟练的工人和进一步的平板计数或 ATP 生物发光技术。为了解决这些问题，开发了一种基于无损紫外-可见光反射的快速检测技术来监测 ATP 和活细胞。样本为猪腰瘦肉。分别在0、24、48、72、84和96h 分析15 °C 保存的样品的 ATP、平板计数和紫外-可见光反射率。测定了样品在20 °C 下240 ~ 540nm 范围内的反射光谱，然后采集样品表面40 × 40mm2的面积，测定平板计数和 ATP 含量。84h 后，样品表面的平板计数由最初的29个增加到3.2 × 107CFU/cm2。84h 后，ATP 含量从最初的9.2 × 10 ~ (-15) mol/cm2增加到2.8 × 10 ~ (-10) mol/cm2。ATP 含量与平板计数呈线性关系，测定系数为0.95。原始光谱的二阶导数与 ATP 含量和活细胞计数在318nm 处的相关系数分别为0.89和0.83。实验结果表明，紫外-可见光反射光谱分析可以用于实时监测肉品表面 ATP 和/或平板计数，并且具有最佳波长。关键词 ATP 卫生实时监测无损检测猪肉光谱板计数吸光度反射率肉类质量1简介包括肉类和家禽的肌肉食品是人类饮食的一个组成部分，并已经存在了几千年。然而，在过去的二十年中，由于诸如疯牛病和口蹄疫等引人注目的食品安全问题(Fox，2001; Pickrell and Enserink，2001) ，公众的关注和意识已经提高。这些疾病的爆发，以及对肉类中特定病原菌的担忧，说明了在这个大规模生产行业中，对肉类微生物腐败的快速和准确检测系统的要求，这个行业每年的营业额达数十亿美元(Ellis and Goodacre，2001)。微生物在食物变质中的主要作用以及食物作为传播导致食源性疾病的微生物的媒介的作用已得到公认。在家禽、猪肉和牛肉的屠宰场，清洁度的监测主要依赖于传统的目视检查、拭子和随后的活细胞计数或 ATP 生物发光技术(Hawronskyj and Holah，1997)。这对于与食物加工过程有关的微生物危害尤其重要。就家禽、猪肉和牛肉加工而言，可以通过使用培养方法(即经典的标准盘计数法)进行常规屠体分析，来验证预防措施减少或消除微生物危害的有效性(Bautista et al。 ，1997)。然而，自从常规微生物分析应用于食品以来，科学家一直对发展更快速的“实时”微生物质量控制方法感兴趣。基于检测整个细胞或其代谢物的快速检测方法可以分为两大类: 直接方法基于检测有或没有孵育的细胞，间接方法基于测量代谢产物或由细胞生长引起的其他变化(Vanne 等，1996)。虽然快速检测方法正在发展中，传统的微生物监测方法在屠宰场的工作现场使用。然而，这些方法往往需要操作人员的技能，分析时间长，费用高。此外，目测检查并不总是准确的，擦拭需要熟练的工人和进一步的板计数分析，通常需要24-48小时。在过去的半个世纪中，传统的食品采样微生物学方法已经发生了变化，据估计，目前有超过40种方法来测量和检测肉类中的细菌变质(Jay，2005; Nychas 等，1988)。近二十年来微生物快速检测技术的发展可分为两大类: 计数检测和存在-缺失检测。1990年，通过使用安大略省环境部的 P-A 测试，Clark 和 El-Shaarawi (1993)比较了几种商业存在缺失(P-A)测试试剂盒区域，并在6个月内进行了评估。目前的快速计数方法一般基于显微镜、 ATP 生物发光或电现象的测量(Ellis and Goodacre，2001)。ATP 生物发光测定的使用是一种合乎逻辑的方法，并且依赖于所有活细胞都含有腺苷5’-三磷酸(ATP)的事实，ATP 是代谢的通用能量供体(Bautista 等，1997)。检测从细胞中提取的高能分子三磷酸腺苷(ATP)是一种广泛使用的间接测定方法。三磷酸腺苷(ATP)量是以镁离子存在下荧光素-荧光素酶系统释放的光能量来测量的(Stanley，1989)。该分析是快速的，只有几秒钟的卫生监测应用和不到一个小时的大多数其他样品。以前，人们认为这种技术有局限性，因为 ATP 存在于所有活细胞中。因此，来自靶细胞的内源性 ATP 必须在测定前被酶去除(Vanne 等，1996)。Siragusa 等(1996)指出，使用微生物 ATP 作为确定食品样品中微生物总数的手段的主要挑战是从微生物 ATP 中分离非微生物 ATP。他们描述的快速微生物 ATP 测定的基础是使用过滤装置提取体细胞 ATP: 然后在同一装置内，提取细菌 ATP 后进行定量。在显微镜方法的情况下，复杂的技术已经开发出来，其中微生物被荧光染料染色，并用荧光显微镜观察。ATP 生物发光通过测量培养细菌细胞中的 ATP 水平来计算该培养物中存在的细胞数量(Champiat 等，2001; de Boer 和 Beurmer，1999; D’Souza，2001; Siragusa 等，1996)。这种方法的问题在于，ATP 是所有活细胞的主要能量来源，而且食品样本本身也含有大量这种化学物质，在测量微生物 ATP 之前，必须将其销毁。因此，ATP 生物发光的测量可能最适合于检测与食品生产和制备相关的设备和机械的污染表面(Ellis and Goodacre，2001)。在只检测活细胞的情况下，必须考虑 ATP 生物发光技术的上述局限性和显微技术的缺陷。然而，来源于肉类和活细胞的 ATP 总量在清洁度评估中具有足够的重要性，因为来源于肉类的 ATP 是导致细菌腐败的营养来源。由于无损、无化学制备和快速检测的优点，光谱学已被广泛研究用于确定农产品的性质，但与植物材料相比，肉类产品的性质较少(Chan et al。 ，2002)。根据文献，VIS/NIRS 技术已应用于猪肉中以测定肌间脂肪(Hoving-Bolink 等，2005; Savenije 等，2006) ，脂肪酸组成(Fernandez-Cabanas 等，2007; Gonzalez-Martin 等，2003,2005) ，颜色(Cozzoliono 等，2003) ，持水能力(Brondum 等，2000)(Josell et al。 ，2000) ，以及 Doroc 和伊比利亚 porl 神经网络分类(del Moral。 ，2009) ，但是它还没有被应用于不同质量和价格的肉类的直接定性分类(del Moral。 ，2009)。此外，利用反射率数据测定食品质量的报告屈指可数。从目前的情况来看，这项研究的目的是开发一种实时检测方法，以监测 ATP 和活细胞在肉表面的反射光谱，可用于卫生管理。2材料及方法2.1肉类样本2.1从零售商取得切成5毫米厚的猪腰样本的瘦肉部分。3天前被宰杀，在零售商店保存在市场条件下。共有24个切片的样品被切成约6 × 6cm2的块，并分别放置在消毒的培养皿中。2.2实验装置样品分为6组，每组4个样品，在15 °C 恒温箱中保存。根据我们与屠宰场管理人员的谈话，我们选择了屠宰场工作室的最高温度作为储存温度，考虑到工人的健康，通常将温度控制在10 °C 至15 °C 之间。在储存0,24,48,72,84和96小时后进行测量。显示的每个值是四个部分的平均值。实验重复三次以验证结果。在所有的重复实验中都得到了相似的结果。在这里，为了简单起见，只显示了一个实验的结果。2.3 UV-Vis 反射光谱日本京都岛津公司的 UV-3600双光束光谱仪配备了积分球装置，用于记录肉样表面(9 × 20mm2)的反射光谱。测量的波长范围为240-1200纳米，分辨率为2纳米; 然而，从240-540纳米的结果只显示在第3节。为了确定 ATP 的最大吸收波长，用10mm 石英电池测定了 ATP 标准溶液(LL-100-1，日本东京东洋 B-Net 公司)系列稀释液的透过率。2.4光谱数据预处理光谱数据经常被预处理，以减少不良的系统噪声，如基线变化、光散射、路径长度差异等，并增强化学成份的贡献(Tigabu 和 Oden，2002)。在这项研究中，两种类型的预处理采用: 萨维茨基-戈雷一阶和二阶导数。在我们的案例中，系统变化的可能来源可能是由于路径长度的微小差异产生的定位个别肉类样本与轻微不同的大小在扫描过程中。2.5采样方案和微生物分析2.5.1采样方案采用拭子技术对覆盖光谱测量区域的猪肉表面(40 × 40mm2)材料进行采样。为了确保足够的采样，根据 Bautista 等人(1997) ，以水平模式擦拭样品，并在食指和拇指之间来回旋转的垂直模式中再次擦拭样品。将棉签末端切入9ml 无菌水中，搅拌均匀，进一步检测棉签平板计数和 ATP 含量。2.5.2平板计数从浸入拭子的磷酸盐缓冲溶液中制备拭子样品的连续稀释液，并将1ml 稀释液分配到 Petri ilmsTM (AC 板，Sumitomo 3M Ltd. ，Tokyo，Japan)上以进行总有氧计数。在35 °C 下培养48小时。2.5.3 ATP 生物发光测定将100微升拭子样品(将拭子浸入其中的磷酸盐缓冲溶液)注射到置于发光计(Luminescenser MCA，Atto Corporation，Tokyo，Japan)中的新鲜试管中，然后将100μl 萃取剂(LL-100-2，Toyo B-Net Co. Ltd. ，Tokyo，Japan)加入其中。10秒后，加入100μl 萤光素-荧光素酶复合物(LL-100-1，日本东京东洋 B-Net Co. Ltd. ，Tokyo，Japan) ，并测量光输出。从每个拭子，采取两个测量和手段计算确定相对光单位(RLU)。然后通过用 ATP 标准溶液(LL-100-1，Toyo B-Net Co. Ltd. ，Tokyo，Japan)在10-16-10-11摩尔/100微升范围内构建的标准曲线将 RLU 转化为 ATP 的量。2.6统计分析在贮存期间，随机抽取四块猪肉样本作统计分析。研究人员进行回归分析测试，以了解三磷酸腺苷(ATP)含量与盘子数量的关系。由于原始数据具有背景信息，因此采用一阶导数和二阶导数对原始数据进行转换，从中选出最优的一个。3结果3.1平板计数样品肉表面平板计数随贮藏时间延长而增加。试验开始时，初始计数为29CFU/cm2，贮藏84h 后为3.2 × 107CFU/cm2。3.2 ATP 含量随着贮藏时间的延长，ATP 含量由最初的9.2 × 10-5增加到2.8 × 10-10mol/cm2(贮藏后84h)。ATP 含量与平板计数呈线性关系，测定系数(R2)为0.95，如图1所示。3.3纯 ATP 的最大吸收量不同浓度(1 × 10-4 ~ 5.85 × 10-6M)的 ATP 溶液的透过率如图2所示。它表明，透过率随着 ATP 浓度的增加而降低，并且对不同 ATP 浓度的所有样品进行的光谱显示与透过率降低相关的最大吸光度在260nm 处(图2)。3.4根据反射率估计 ATP 和平板计数储存0-84小时获得的反射光谱如图3所示，在 UV-Vis 范围内(从240到540nm)。0小时和24小时的反射率差别很小。在48、72和84小时采集的样品的反射率随着储存时间的延长呈下降趋势。反射率数据的二阶导数被选为反射率一阶导数和二阶导数之间的最佳值，如图4所示。在紫外光谱范围内，观察到多个向上和向下的峰，并对298,318,344和374nm 的峰进行了相关性分析。图5显示了反射率二阶导数与对数(ATP)之间的相关系数。这给出了二阶导数和对数(ATP)之间的高度相关性。考虑到暗色移动，这四个波长中的任何一个都可以作为 ATP 的最大吸收。4在过去的几十年中，光谱方法在食品质量属性的评价中获得了重要性(Nadai，1983; Nadai 和 Mihalyi-Kengyel，1984)。尽管近红外光谱反映了与食品复杂质量有关的几个参数(Williams 和 Norris，2001) ，但是在近红外光谱范围内不能检测到 ATP 和/或微生物的信息。因此，在本研究中，UV-Vis 的应用范围从240到540纳米。4.1平板计数在这项研究中，样品被评估为新鲜，直到当细菌计数超过107CFU/g 的边界线，没有腐臭气味可以察觉。72小时后，平板计数达到107 CFU/g，样品散发出微弱的腐臭气味。这些样品处于变质的初始阶段，将被视为不可接受。盘子计数是肉类腐败的一个基本指标，肉类中107CFU/g 的计数被认为是不可接受的(Brown，1982)。检测106 CFU/g 的量级很重要，因为这是在肉类达到不可接受阶段之前完成的。新鲜肉类的 pH 值一般在5.5至5.9之间，并含有足够的葡萄糖和其他简单碳水化合物，以支持大约109 CFU/cm2。在冷藏温度下生长最快并利用葡萄糖的生物是假单胞菌(Gill and Newton，1977; Jay，2005; Seymour et al。 ，1994)。在107 CFU/cm2的水平下，异味可能会以淡淡的“奶制品”香味的形式变得明显，一旦表面细菌数量达到108 CFU/cm2，简单碳水化合物的供应已经耗尽，可识别的异味发展导致所谓的“感官”腐败(Jackson 等，1997; Jay，2005; Stanbridge and Davies，1998)。异味的发展取决于游离氨基酸利用发生的程度，这些气味已经被不同地描述为107CFU/cm2的乳制品/黄油/脂肪/奶酪，直到108CFU/cm2的病态甜味/水果香味，最后是109CFU/cm2的腐臭气味(Adams 和 Moss，2007; Dainty 等，1985)。4.2 ATP 含量图1显示了对数10 ATP 和对数10平板计数之间的线性关系。根据这个数字，ATP 分析法和平板计数法都能够评估猪肉样本的卫生情况。ATP 分析只能提供总细菌计数的估计，不能区分细菌(Baumgart，1993)。理论上，ATP 含量低至100fg (10-13g) ，相当于约100个细菌细胞。在实际条件下，灵敏度约为1000fg (10-12g) ，相当于约1000个细菌细胞或一至两个酵母细胞(Heeschen 等，1991)。应激细胞和处于静止生长阶段的细胞含有较少的 ATP，这也影响了结果(Bulte 和 Reuter，1985)。然而，另一方面，样品中 ATP 的含量提供了对活性微生物种群的估计，这在考虑产品的货架期时是很重要的。应激细胞也可以在 ATP 测定前复苏(Graumlich，1985)。这种酶，萤光素酶，通过化学计量反应将 ATP 提供的化学能转化为光。因此，产生的光量与存在的 ATP 浓度成正比，而 ATP 浓度又与样品中细胞的数量直接相关(Bautista 等，1997)。ATP 生物发光也可用于监测肉类加工过程中烫伤和冷却罐中的微生物污染。在对屠体污染和处理水质的 ATP 生物发光测定中，微生物细胞在裂解释放细胞内 ATP 之前通过过滤除去。为了简化这种方法，如果能够消除这一步骤以允许在屠体表面的拭子上直接检测 ATP，就像 ATP 生物发光卫生监测测试(Griffiths，1996)一样是可取的。然而，使用拭子测定将无法区分 ATP 与微生物和非微生物来源，但是结果将在2分钟内获得，而不是当纳入过滤步骤时所需的10-15分钟(Bautista 等，1997)。Siragusa 等(1996)开发了分段模型统计方法来确定检测灵敏度的下限，并使用该模型分析植物内数据。根据他们的研究，快速微生物 ATP 测试以线性方式响应猪肉胴体 > log 103.2需氧 CFU/cm2的微生物污染水平。4.3纯 ATP 的吸收峰如图2所示，透过率随 ATP 浓度的增加而降低，不同的光谱在260nm 处表现出最小的吸光度。260nm 的波长与之前 Bagshaw (2001)报道的 ATP (259nm)的最大吸光度一致。4.4从反射反射率估计 ATP 和平板计数(图3)在 UV-Vis 范围内随着时间的推移显示出减少的趋势，尽管在0小时和24小时的反射率之间差异很小。为了消除背景效应原始数据通过使用第一和第二导数进行转换。然而，选择二阶导数是因为它的效果更加明显。二阶导数技术是近红外数据处理的常用方法。它有助于分离重叠吸收带，消除基线偏移和增加明显的光谱分辨率(Lin et al。 ，2004) ，尽管衍生物是众所周知的对噪声敏感(Tsai 和 Philpot，1998)。在所有贮存时间段(0-96小时)的原始反射光谱二阶导数测量时，观察到许多向上和向下的峰值(图4)。对298、318、344和374nm 波段的峰进行了相关性分析。这些选定的波长在紫外线范围内，即小于400纳米。最大的差异获得所有选定的波长，在时间0和96小时之间。时间0 ~ 96h 的最大差异在318nm 范围内。这个波长范围主要可以区分0和96小时的样品。图5显示了反射率的二阶导数与对数(ATP)之间的相关系数。这给出了二阶导数的值与对数(ATP)之间的高度相关性。考虑到暗色移动，这四个波长中的任何一个都可以作为 ATP 的最大吸收。另一方面，众所周知，ATP 的光谱吸收通常被蛋白质吸收所掩盖，在光谱学研究中不能被利用(Bagshaw，2001)。然而，图6所示的反射率二阶导数与对数(平板计数)之间的相关系数图在形状上与图5非常相似。这表明反射率的二阶导数涉及活细胞内 ATP 的信息。对这一点的理解也得到了 ATP 含量与平板计数相对应的结果的支持(图1)。从这些考虑，波长318纳米显示最高的相关系数被选中。在318nm 处的前48h，二阶导数的值与对数(ATP)之间的线性关系如图7所示，测定系数为0.89。在318nm 处测定前48h，二阶导数值与对数(平板计数)之间也存在相似的关系，测定系数为0.83。这里选择的前48小时的持续时间意味着猪肉样本是新鲜的。从这些结果可以看出，选择合适的波长可以利用反射率信息实时监测肉表面 ATP 和/或活细胞计数。平板计数给出了微生物污染的估计值，而本研究中使用的 ATP 生物发光法测量来自微生物和非微生物来源的总 ATP，可能是对屠体整体清洁度的更好测量。因此，不应期望两种方法之间有确切的关系，从两种测定系统获得的结果应该分开解释。在不同波长下使用多个反射率的多个线性回归分析是估计肉表面 ATP 和/或存活细胞数量的有力工具，并且可以导致更高的预测能力。然而，这样的范式可能会导致过度拟合。因此，在这项研究中，只有一个波长(即，318纳米)被选择用于 ATP 的预测。5结论建立了一种利用反射光谱实时监测肉表面 ATP 和活细胞的方法。结果表明，在15 °C 下贮藏84h，样品表面的平板数量增加，与 ATP 含量的增加完全一致。其测定系数为0.95，支持 ATP 含量与平板计数之间的线性关系。在紫外-可见光范围内，反射率随时间呈下降趋势，在318nm 的峰值，反射率的二阶导数与对数(ATP)呈高度相关。由于反射率的二阶导数与对数(平板计数)之间也存在相似的高相关性，提示反射率的二阶导数涉及活细胞中 ATP 的信息。根据这些观察结果，在肉表面反射率分析的基础上，给出了估算微生物源性 ATP 含量的线性关系。因此，开发的技术可以提供一个强有力的方式监测清洁度在屠宰场。这项研究部分由日本科学促进会(JSPS)拨款19:07178资助。参考文献 Adams and Moss 2007 Adams M.R. Moss 作案手法2007。食品微生物学。英国皇家化学学会，剑桥，第138页。巴格肖，2001年产。Bagshaw ATP 类似物一目了然细胞科学杂志1142001459460 Baumgart，1993 J. Baumgart Lebensmitteluberwachung und-qualitatssecherung Mikrobiologch-hygienische Schnellverfahren Fleischwirtschaft 731993292396 Bautista 等，1997 D.A。Bautista D.W.Sprung S. Barbut M.W.Griffiths 基于 ATP 生物发光测定的采样制度，用于评估食品研究国际组织在加工过程中关键控制点的家禽尸体的质量。301997803809 Brondum et al。 ，2000 J. Brondum L. Munck P. Henckel A. Karlsson E. Tornberg S.B。利用比较光谱法对猪肉的恩格尔森预测和持水性及其组成的研究[55]。肉类微生物学。(p. 410).纽约: 应用科学出版物。生物发光法作为一种快速测定肉类微生物区系的方法，国际食品微生物学杂志，1985年2月37日，381 Champiat 等人,2001 D. Champiat N. Matas B. Monofort H. Fraass 生物发光在 HACCP 发光中的应用162001193198 Chan 等，2002 D.E。陈 PN。Walker E.W.利用 ASAE 45200215191527 Clark 和 El-Shaarawi 的可见光和近红外光谱法预测猪肉品质特性。Clark A.H.用于检测总大肠菌群、大肠桿菌和其他指示细菌应用与环境微生物学的商业存在-缺失试剂盒的评估5921993380388 Cozzoliono 等人，2003 D. Cozzoliono N. Barlocco A. Vadell F. Ballesteros G. Gallieta 使用可见光和近红外反射光谱来预测完整和均化猪肉的颜色 LWT-Food Science and Technology 362003195202 de Boer and Beurmer，1999E.De Boer R.R.Beurmer 食源性微生物检测和分型方法国际食品微生物学杂志501999119130 del Moral 等，2009 F.G。德尔 · 道德 · A · 吉伦 · KG。马丁内斯 R.G。可见光和近红外反射光谱学对杜洛克和伊比利亚 Porl 神经网络的分类食品工程杂志902009540547 Dainty 等，1985 R.H。小巧的 R.A。爱德华兹 C.M。空气中自然污染牛肉冷藏过程中挥发性化合物形成的时间过程应用细菌学杂志591985303309 d’Souza，2001 S.F. 。D’Souza 微生物生物传感器生物传感器和生物电子学162001337353 Ellis and Goodacre，2001 D。肌肉食品微生物腐败的快速定量检测: 食品科学与技术的现状与未来趋势。122001414424 Fernandez-Cabanas 等，2007 V.M. Fernandez-Cabanas A. Garrido-Varo J. Gracia-Olmo E. De Pedro P. Dardenne 优化用于伊比利亚猪脂肪近红外校准的光谱预处理化学计量学和智能实验室系统872007104112 Fox，2001 S.Fox 世界卫生组织召开关于世界范围内疯牛病和 CJD 医学感染风险的会议。冈萨雷斯-马丁等人，2003 I. 冈萨雷斯-马丁冈萨雷斯-佩雷斯 · 埃尔南德斯 · 门德斯 · N · 阿尔瓦雷斯 · 格拉西亚用光纤探针近红外光谱学(NIRS)测定伊比利亚种猪皮下脂肪中的脂肪酸。肉类科学652003713719冈萨雷斯-马丁等人,2005年 I. Gonzalez-Martin C. Gonzalez-Perez N. Alvarez-Gracia J.M。冈萨雷斯-卡布雷拉利用远程反射光纤探针在线测定伊比利亚猪腰肌间脂肪脂肪酸组成。Gill K.G.牛顿冷藏肉类上有氧腐败菌群的发展应用细菌学杂志431977189195 Graumlich，1985 T.R。《食品科学生物发光杂志》1985116117,124 Griffiths，1996 M.W 对橙汁中微生物种群的 Graumlich 估算。三磷酸腺苷生物发光在食品工业中的作用: 对旧问题的新认识食品技术50619966272 Hawronskyj and Holah，1997 J.M。Hawronskyj J. Holah ATP: 食品科学与技术通用卫生监测器趋势819977984 Heeschen 等，1991 W.H。乳品工业中的快速方法。《微生物学和免疫学中的快速方法和自动化》 ，1991年，施普林格-出版社，Berlin Heidelberg 520532霍文-博林克等，2005年。霍温-博林克 H.W。Vedder J.W.M.Merks W.J.H.De Klein H.G.M.Reimert R. Frankhuizen W.H.A.M.范登布鲁克。返回文章页面兰布吉对近红外光谱仪测量结果的透视译者: pestwave 肉类科学692005417423杰克逊等人，1997年杰克逊 G.R. Acuff J.S. 迪克森肉类，家禽和海鲜 M.P. 道尔 L.R. 博伊查特 T.J. 蒙特维尔食品微生物学: 基础和前沿1997年美国广播公司出版社华盛顿 DC 83100杰伊，2005 J.M. 杰伊现代食品微生物学第六版。2005年 Aspen 出版社 Maryland Josell 等人，2000年 A. Josell L. Martinsson C. Borggaard J.R。利用视觉和近红外光谱技术肉类科学552000273278尼查斯等，1988 G.J。Nychas V.M.Dillon R.G.O </o < o </o < o </o < > < o </o < > < o </o < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o < o </o < > < o </o < > < o </o.Geesink J.G.P.利用可见/近红外反射光谱技术预测猪肉质量。Seymour M.B.Cole PJ.库特基质介导的细菌质子流出/内流的测定，以预测在寒冷温度下储存的牛肉肉糜的变质程度应用细菌学杂志761994608615 Siragusa 等，1996 G.R。天竺葵 W.J。Dorsa C.N.Cutter Perino L.J.Kooh-maraie 使用一种新开发的快速微生物 ATP 生物发光测定法来检测家禽尸体上的微生物污染。Stanbridge A.R.冷藏肉的微生物学 Davies R. Board 肉和家禽的微生物学1998 Blackie Academy & Professional London 174219 Stanley，1989 P.E。快速微生物学中的生物发光 ATP 技术综述《生物发光杂志》和化学发光4,1989375380。基于 Sjoberg HACCP 的食品质量控制和微生物快速检测方法食品控制71996263276|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large-Scale+Graph+Neural+Networks:+The+Past+and+New+Frontiers)|0|
|[Knowledge-augmented Graph Machine Learning for Drug Discovery: From Precision to Interpretability](https://doi.org/10.1145/3580305.3599563)|Zhiqiang Zhong, Davide Mottin||The integration of Artificial Intelligence (AI) into the field of drug discovery has been a growing area of interdisciplinary scientific research. However, conventional AI models are heavily limited in handling complex biomedical structures (such as 2D or 3D protein and molecule structures) and providing interpretations for outputs, which hinders their practical application. As of late, Graph Machine Learning (GML) has gained considerable attention for its exceptional ability to model graph-structured biomedical data and investigate their properties and functional relationships. Despite extensive efforts, GML methods still suffer from several deficiencies, such as the limited ability to handle supervision sparsity and provide interpretability in learning and inference processes, and their ineffectiveness in utilising relevant domain knowledge. In response, recent studies have proposed integrating external biomedical knowledge into the GML pipeline to realise more precise and interpretable drug discovery with limited training instances. However, a systematic definition for this burgeoning research direction is yet to be established. This survey presents a comprehensive overview of long-standing drug discovery principles, provides the foundational concepts and cutting-edge techniques for graph-structured data and knowledge databases, and formally summarises Knowledge-augmented Graph Machine Learning (KaGML) for drug discovery. we propose a thorough review of related KaGML works, collected following a carefully designed search methodology, and organise them into four categories following a novel-defined taxonomy. To facilitate research in this promptly emerging field, we also share collected practical resources that are valuable for intelligent drug discovery and provide an in-depth discussion of the potential avenues for future advancements.|人工智能(AI)与药物发现领域的结合已经成为跨学科科学研究的一个新兴领域。然而，传统的人工智能模型在处理复杂的生物医学结构(如2D 或3D 蛋白质和分子结构)和提供输出解释方面受到严重限制，这阻碍了它们的实际应用。近年来，图形机器学习(Graph Machine Learning，GML)由于其对图形结构化生物医学数据的建模能力以及对其性质和功能关系的研究而引起了人们的广泛关注。尽管付出了广泛的努力，GML 方法仍然存在一些缺陷，例如处理监督稀疏性和在学习和推理过程中提供可解释性的能力有限，以及它们在利用相关领域知识方面的无效性。作为回应，最近的研究提出将外部生物医学知识整合到 GML 管道中，以便在有限的培训实例下实现更精确和可解释的药物发现。然而，对这一新兴的研究方向的系统定义还有待确立。本调查全面概述了长期存在的药物发现原则，提供了图形结构数据和知识数据库的基本概念和尖端技术，并正式总结了用于药物发现的知识增强图形机器学习(KaGML)。我们建议对相关的 KaGML 作品进行一次彻底的回顾，这些作品是按照一个精心设计的搜索方法收集的，并按照一个新定义的分类法将它们组织成四个类别。为了促进这一迅速兴起的领域的研究，我们还分享了收集到的对智能药物发现有价值的实用资源，并对未来进展的潜在途径进行了深入讨论。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge-augmented+Graph+Machine+Learning+for+Drug+Discovery:+From+Precision+to+Interpretability)|0|
|[Foundations and Applications in Large-scale AI Models: Pre-training, Fine-tuning, and Prompt-based Learning](https://doi.org/10.1145/3580305.3599209)|Derek Cheng, Dhaval Patel, Linsey Pang, Sameep Mehta, Kexin Xie, Ed H. Chi, Wei Liu, Nitesh V. Chawla, James Bailey|University of Melbourne, Melbourne, VIC, Australia; University of Technology Sydney, Sydney, NSW, Australia; Salesforce, San Francisco, CA, USA; University of Notre Dame, Indiana, IN, USA; Google Research, Mountain View, CA, USA; IBM Research, Bengaluru, India; IBM Research, New York, NY, USA|Deep learning techniques have advanced rapidly in recent years, leading to significant progress in pre-trained and fine-tuned large-scale AI models. For example, in the natural language processing domain, the traditional "pre-train, fine-tune" paradigm is shifting towards the "pre-train, prompt, and predict" paradigm, which has achieved great success on many tasks across different application domains such as ChatGPT/BARD for Conversational AI and P5 for a unified recommendation system. Moreover, there has been a growing interest in models that combine vision and language modalities (vision-language models) which are applied to tasks like Visual Captioning/Generation. Considering the recent technological revolution, it is essential to emphasize these paradigm shifts and highlight the paradigms with the potential to solve different tasks. We thus provide a platform for academic and industrial researchers to showcase their latest work, share research ideas, discuss various challenges, and identify areas where further research is needed in pre-training, fine-tuning, and prompt-learning methods for large-scale AI models. We foster the development of a strong research community focused on solving challenges related to large-scale AI models, providing superior and impactful strategies that can change people's lives in the future.|近年来，深度学习技术迅速发展，导致预先训练和微调的大规模人工智能模型取得重大进展。例如，在自然语言处理领域，传统的“预训练，微调”范式正在向“预训练，提示和预测”范式转变，该范式在不同应用领域的许多任务上取得了巨大的成功，如 ChatGPT/BARD 用于会话 AI 和 P5用于统一推荐系统。此外，人们对将视觉和语言模式(视觉-语言模型)结合起来的模型越来越感兴趣，这些模型适用于视觉字幕/生成等任务。考虑到最近的技术革命，有必要强调这些范式转变，并突出具有解决不同任务潜力的范式。因此，我们为学术界和工业界的研究人员提供了一个平台，以展示他们的最新工作，分享研究思路，讨论各种挑战，并确定在大规模人工智能模型的预训练、微调和及时学习方法方面需要进一步研究的领域。我们致力于发展一个强大的研究团体，致力于解决与大规模人工智能模型有关的挑战，提供优秀和有影响力的战略，可以改变人们未来的生活。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Foundations+and+Applications+in+Large-scale+AI+Models:+Pre-training,+Fine-tuning,+and+Prompt-based+Learning)|0|
|[Minimizing Hitting Time between Disparate Groups with Shortcut Edges](https://doi.org/10.1145/3580305.3599434)|Florian Adriaens, Honglian Wang, Aristides Gionis|KTH Royal Institute of Technology; University of Helsinki|Structural bias or segregation of networks refers to situations where two or more disparate groups are present in the network, so that the groups are highly connected internally, but loosely connected to each other. In many cases it is of interest to increase the connectivity of disparate groups so as to, e.g., minimize social friction, or expose individuals to diverse viewpoints. A commonly-used mechanism for increasing the network connectivity is to add edge shortcuts between pairs of nodes. In many applications of interest, edge shortcuts typically translate to recommendations, e.g., what video to watch, or what news article to read next. The problem of reducing structural bias or segregation via edge shortcuts has recently been studied in the literature, and random walks have been an essential tool for modeling navigation and connectivity in the underlying networks. Existing methods, however, either do not offer approximation guarantees, or engineer the objective so that it satisfies certain desirable properties that simplify the optimization~task. In this paper we address the problem of adding a given number of shortcut edges in the network so as to directly minimize the average hitting time and the maximum hitting time between two disparate groups. Our algorithm for minimizing average hitting time is a greedy bicriteria that relies on supermodularity. In contrast, maximum hitting time is not supermodular. Despite, we develop an approximation algorithm for that objective as well, by leveraging connections with average hitting time and the asymmetric k-center problem.|网络的结构性偏差或隔离是指网络中存在两个或两个以上不同的群体，这些群体在内部高度连接，但彼此之间却松散地连接在一起。在许多情况下，增加不同群体之间的联系是有意义的，这样可以减少社会摩擦，或者使个人接触到不同的观点。增加网络连接性的一种常用机制是在节点对之间添加边快捷方式。在许多感兴趣的应用程序中，边缘快捷方式通常转化为推荐，例如，要看什么视频，或者接下来要读什么新闻文章。通过边缘捷径减少结构偏差或分离的问题最近在文献中进行了研究，并且随机游动已经成为一个必不可少的工具建模导航和连通性在基础网络。然而，现有的方法要么不提供近似保证，要么设计目标，使其满足某些理想的性质，简化了优化任务。本文讨论了在网络中增加一定数量的快捷边，以直接最小化两个不同组之间的平均命中时间和最大命中时间的问题。我们的最小化平均命中时间的算法是一个依赖于超模性的贪婪的双准则。相比之下，最大命中时间不是超模块化的。尽管如此，通过利用平均命中时间和非对称 k 中心问题之间的联系，我们还是为这个目标制定了一个近似演算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Minimizing+Hitting+Time+between+Disparate+Groups+with+Shortcut+Edges)|0|
|[Fair Allocation Over Time, with Applications to Content Moderation](https://doi.org/10.1145/3580305.3599340)|Amine Allouah, Christian Kroer, Xuan Zhang, Vashist Avadhanula, Nona Bohanon, Anil Dania, Caner Gocmen, Sergey Pupyrev, Parikshit Shah, Nicolás Stier Moses, Ken Rodríguez Taarup||In today's digital world, interaction with online platforms is ubiquitous, and thus content moderation is important for protecting users from content that do not comply with pre-established community guidelines. Given the vast volume of content generated online daily, having an efficient content moderation system throughout every stage of planning is particularly important. We study the short-term planning problem of allocating human content reviewers to different harmful content categories. We use tools from fair division and study the application of competitive equilibrium and leximin allocation rules for addressing this problem. On top of the traditional Fisher market setup, we additionally incorporate novel aspects that are of practical importance. The first aspect is the forecasted workload of different content categories, which puts constraints on the allocation chosen by the planner. We show how a formulation that is inspired by the celebrated Eisenberg-Gale program allows us to find an allocation that not only satisfies the forecasted workload, but also fairly allocates the remaining working hours from the content reviewers among all content categories. A fair allocation of oversupply provides a guardrail in cases where the actual workload deviates from the predicted workload. The second practical consideration is time dependent allocation that is motivated by the fact that partners need scheduling guidance for the reviewers across days to achieve efficiency. To address the time component, we introduce new extensions of the various fair allocation approaches for the single-time period setting, and we show that many properties extend in essence, albeit with some modifications. Lastly, related to the time component, we additionally investigate how to satisfy markets' desire for smooth allocation (i.e, an allocation that does not vary much from time to time) so that the switch in staffing is minimized. We demonstrate the performance of our proposed approaches through real-world data obtained from Meta.|在当今的数字世界中，与在线平台的互动无处不在，因此内容管制对于保护用户免受不符合预先建立的社区指导方针的内容的影响非常重要。鉴于每天在线生成的内容量巨大，在每个规划阶段都有一个高效的内容审核系统尤其重要。我们研究了将人工内容审阅者分配到不同有害内容类别的短期规划问题。我们利用公平分割的工具，研究了竞争均衡和利克明分配规则在解决这一问题中的应用。除了传统的费雪市场设置，我们还纳入了新颖的方面，是实际的重要性。第一个方面是不同内容类别的预测工作量，它对规划者选择的分配施加约束。我们展示了一个受著名的 Eisenberg-Gale 项目启发的公式如何让我们找到一个分配，不仅满足预测的工作量，而且在所有内容类别中公平地分配来自内容审查者的剩余工作时间。在实际工作量偏离预期工作量的情况下，公平分配供应过剩提供了保护。第二个实际考虑因素是时间依赖性分配，其动机是合作伙伴需要为审评人员提供跨天的时间安排指导，以实现效率。为了解决时间成分的问题，我们引入了对单时间周期设置的各种公平分配方法的新的扩展，并且我们展示了许多属性在本质上的扩展，尽管有一些修改。最后，关于时间部分，我们还研究了如何满足市场对于平稳分配的需求(即，分配不会随时间变化很大) ，从而最大限度地减少人员调动。我们通过从 Meta 获得的真实世界数据来演示我们提出的方法的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fair+Allocation+Over+Time,+with+Applications+to+Content+Moderation)|0|
|[Maximizing Neutrality in News Ordering](https://doi.org/10.1145/3580305.3599425)|Rishi Advani, Paolo Papotti, Abolfazl Asudeh|University of Illinois Chicago; EURECOM|The detection of fake news has received increasing attention over the past few years, but there are more subtle ways of deceiving one's audience. In addition to the content of news stories, their presentation can also be made misleading or biased. In this work, we study the impact of the ordering of news stories on audience perception. We introduce the problems of detecting cherry-picked news orderings and maximizing neutrality in news orderings. We prove hardness results and present several algorithms for approximately solving these problems. Furthermore, we provide extensive experimental results and present evidence of potential cherry-picking in the real world.|在过去的几年里，假新闻的发现已经受到越来越多的关注，但是欺骗受众的方式更加微妙。除了新闻报道的内容之外，它们的表述也可能具有误导性或偏见性。在本研究中，我们研究了新闻故事的顺序对受众知觉的影响。我们介绍了检测精选新闻排序和最大化新闻排序中立性的问题。我们证明了硬度结果，并提出了几种近似求解这些问题的算法。此外，我们提供了广泛的实验结果和现实世界中潜在的樱桃采摘的证据。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Maximizing+Neutrality+in+News+Ordering)|0|
|[Knowledge Graph Reasoning over Entities and Numerical Values](https://doi.org/10.1145/3580305.3599399)|Jiaxin Bai, Chen Luo, Zheng Li, Qingyu Yin, Bing Yin, Yangqiu Song|Amazon.com Inc; HKUST|A complex logic query in a knowledge graph refers to a query expressed in logic form that conveys a complex meaning, such as where did the Canadian Turing award winner graduate from? Knowledge graph reasoning-based applications, such as dialogue systems and interactive search engines, rely on the ability to answer complex logic queries as a fundamental task. In most knowledge graphs, edges are typically used to either describe the relationships between entities or their associated attribute values. An attribute value can be in categorical or numerical format, such as dates, years, sizes, etc. However, existing complex query answering (CQA) methods simply treat numerical values in the same way as they treat entities. This can lead to difficulties in answering certain queries, such as which Australian Pulitzer award winner is born before 1927, and which drug is a pain reliever and has fewer side effects than Paracetamol. In this work, inspired by the recent advances in numerical encoding and knowledge graph reasoning, we propose numerical complex query answering. In this task, we introduce new numerical variables and operations to describe queries involving numerical attribute values. To address the difference between entities and numerical values, we also propose the framework of Number Reasoning Network (NRN) for alternatively encoding entities and numerical values into separate encoding structures. During the numerical encoding process, NRN employs a parameterized density function to encode the distribution of numerical values. During the entity encoding process, NRN uses established query encoding methods for the original CQA problem. Experimental results show that NRN consistently improves various query encoding methods on three different knowledge graphs and achieves state-of-the-art results.|知识图中的复杂逻辑查询是指以逻辑形式表达的、传达复杂意义的查询，比如加拿大图灵奖获得者是从哪里毕业的？基于知识图推理的应用程序，如对话系统和交互式搜索引擎，依赖于回答复杂逻辑查询的能力，这是一项基本任务。在大多数知识图中，边通常用于描述实体之间的关系或其相关属性值。属性值可以是分类格式或数字格式，如日期、年份、大小等。但是，现有的复杂查询应答(CQA)方法只是以处理实体的方式处理数值。这可能会导致回答某些问题的困难，例如哪位澳大利亚普利策奖得主出生于1927年以前，哪种药物是止痛药，副作用比扑热息痛少。本文受数字编码和知识图推理技术的启发，提出了数字复杂查询应答方法。在这个任务中，我们引入了新的数值变量和操作来描述涉及数值属性值的查询。为了解决实体和数值之间的差异，我们还提出了数字推理网络(NRN)的框架，将实体和数值交替编码成独立的编码结构。在数值编码过程中，NRN 采用参数化密度函数对数值的分布进行编码。在实体编码过程中，NRN 对原始的 CQA 问题使用已建立的查询编码方法。实验结果表明，NRN 能够一致地改进三种不同知识图上的各种查询编码方法，取得了较好的效果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Graph+Reasoning+over+Entities+and+Numerical+Values)|0|
|[Communication Efficient and Differentially Private Logistic Regression under the Distributed Setting](https://doi.org/10.1145/3580305.3599279)|Ergute Bao, Dawei Gao, Xiaokui Xiao, Yaliang Li||We study the classic machine learning problem of logistic regression with differential privacy (DP), under the distributed setting. While logistic regression with DP has been extensively studied in the literature, most of the research is focused on the centralized setting, where a centralized server is trusted with the entire private training dataset. However, in many real-world scenarios (e.g., federated learning), the data is distributed among multiple clients who may not trust others, including clients and the server. While the server tries to learn a model using the clients' private datasets, the clients should provide each individual record in their local datasets with a formal privacy guarantee. Towards this end, we propose a general mechanism for logistic regression with DP under the distributed setting, based on output perturbation. We show that our solution satisfies differential privacy and enjoys privacy amplification by secure aggregation, a recent technique for DP under the distributed setting. In addition, our solution also incurs much lower communication costs (which is considered as a huge overhead in federated learning), compared with existing ones. In particular, our solution requires the clients to communicate only once throughout the entire FL process. Finally, we provide experimental results on real-world datasets to demonstrate the effectiveness of our solution.|我们研究了分布式环境下 Logit模型差分隐私的经典机器学习问题。虽然 DP 的 Logit模型已经在文献中得到了广泛的研究，但是大多数的研究都集中在集中式设置上，即一个集中式服务器可以信任整个私人培训数据集。然而，在许多现实场景(例如，联合学习)中，数据分布在多个客户机之间，这些客户机可能不信任其他客户机，包括客户机和服务器。当服务器尝试使用客户机的私有数据集学习模型时，客户机应该为其本地数据集中的每个单独记录提供正式的隐私保证。为此，我们提出了一个基于输出扰动的分布式设置下的 DP Logit模型的通用机制。我们展示了我们的解决方案通过安全聚合来满足差分隐私要求并享受隐私放大，这是分布式环境下 DP 的最新技术。此外，与现有的解决方案相比，我们的解决方案还带来了更低的通信成本(这在联邦学习中被认为是一个巨大的开销)。特别是，我们的解决方案要求客户端在整个 FL 过程中只进行一次通信。最后，我们提供了实际数据集的实验结果，以证明我们的解决方案的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Communication+Efficient+and+Differentially+Private+Logistic+Regression+under+the+Distributed+Setting)|0|
|[Preemptive Detection of Fake Accounts on Social Networks via Multi-Class Preferential Attachment Classifiers](https://doi.org/10.1145/3580305.3599471)|Adam Breuer, Nazanin Khosravani Tehrani, Michael Tingley, Bradford Cottel|Dartmouth; Meta|In this paper, we describe a new algorithm called Preferential Attachment k-class Classifier (PreAttacK) for detecting fake accounts in a social network. Recently, several algorithms have obtained high accuracy on this problem. However, they have done so by relying on information about fake accounts' friendships or the content they share with others--the very things we seek to prevent. PreAttacK represents a significant departure from these approaches. We provide some of the first detailed distributional analyses of how new fake (and real) accounts first attempt to request friends after joining a major network (Facebook). We show that even before a new account has made friends or shared content, these initial friend request behaviors evoke a natural multi-class extension of the canonical Preferential Attachment model of social network growth. We use this model to derive a new algorithm, PreAttacK. We prove that in relevant problem instances, PreAttacK near-optimally approximates the posterior probability that a new account is fake under this multi-class Preferential Attachment model of new accounts' (not-yet-answered) friend requests. These are the first provable guarantees for fake account detection that apply to new users, and that do not require strong homophily assumptions. This principled approach also makes PreAttacK the only algorithm with provable guarantees that obtains state-of-the-art performance on new users on the global Facebook network, where it converges to AUC=0.9 after new users send + receive a total of just 20 not-yet-answered friend requests. For comparison, state-of-the-art benchmarks do not obtain this AUC even after observing additional data on new users' first 100 friend requests. Thus, unlike mainstream algorithms, PreAttacK converges before the median new fake account has made a single friendship (accepted friend request) with a human.|本文提出了一种新的识别社交网络中虚假账户的算法——偏好依恋 k 类分类器(PreAttacK)。近年来，一些算法已经在这个问题上取得了很高的精度。然而，他们这样做的依据是关于虚假账户的友谊或与他人分享的内容的信息——这正是我们试图阻止的事情。PreAttacK 代表了对这些方法的重大背离。我们首先提供了一些详细的分布式分析，分析新的假账户(和真账户)如何在加入主流社交网络(Facebook)后首次尝试请求好友。我们发现，甚至在一个新账户交到朋友或分享内容之前，这些初始的好友请求行为就会自然地唤起社交网络成长的规范偏好依恋模型的多级扩展。我们使用这个模型来推导一个新的算法，PreAttacK。我们证明在相关的问题实例中，PreAttack 近似最优地近似了在这种新帐户的好友请求(尚未回复)的多类优先附件模型下，新帐户是假的后验概率。这些都是第一个可证明的假账户检测的保证，适用于新用户，并不需要强烈的同质假设。这种原则性的方法也使 PreAttacK 成为唯一一种可证实的算法，它可以在全球 Facebook 网络的新用户身上获得最先进的性能，在新用户发送 + 收到总共20个尚未回复的好友请求后，它会收敛到 AUC = 0.9。作为比较，即使在观察了新用户前100个好友请求的额外数据之后，最先进的基准测试也不能获得这个 AUC。因此，与主流算法不同的是，PreAttacK 会在新假账号与人类建立单个友谊(接受好友请求)之前收敛。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Preemptive+Detection+of+Fake+Accounts+on+Social+Networks+via+Multi-Class+Preferential+Attachment+Classifiers)|0|
|[On Improving the Cohesiveness of Graphs by Merging Nodes: Formulation, Analysis, and Algorithms](https://doi.org/10.1145/3580305.3599449)|Fanchen Bu, Kijung Shin|KAIST|Graphs are a powerful mathematical model, and they are used to represent real-world structures in various fields. In many applications, real-world structures with high connectivity and robustness are preferable. For enhancing the connectivity and robustness of graphs, two operations, adding edges and anchoring nodes, have been extensively studied. However, merging nodes, which is a realistic operation in many scenarios (e.g., bus station reorganization, multiple team formation), has been overlooked. In this work, we study the problem of improving graph cohesiveness by merging nodes. First, we formulate the problem mathematically using the size of the $k$-truss, for a given $k$, as the objective. Then, we prove the NP-hardness and non-modularity of the problem. After that, we develop BATMAN, a fast and effective algorithm for choosing sets of nodes to be merged, based on our theoretical findings and empirical observations. Lastly, we demonstrate the superiority of BATMAN over several baselines, in terms of speed and effectiveness, through extensive experiments on fourteen real-world graphs.|图是一种强大的数学模型，用于表示各个领域的真实世界结构。在许多应用中，具有高连通性和鲁棒性的实际结构是可取的。为了增强图的连通性和鲁棒性，增加边和锚定节点这两种操作已经被广泛研究。然而，合并节点，这是一个现实的操作在许多情况下(例如，公共汽车站重组，多个团队形成) ，一直被忽视。本文研究了通过合并节点来提高图的内聚性问题。首先，我们以给定的 $k $为目标，使用 $k $- 桁架的大小数学地表述问题。然后，证明了问题的 NP- 硬度和非模性。然后，根据我们的理论研究结果和经验观察，开发了一种快速有效的节点合并选择算法 BATMAN。最后，我们通过对14个真实世界图的大量实验，证明了蝙蝠侠在速度和效率方面优于几个基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Improving+the+Cohesiveness+of+Graphs+by+Merging+Nodes:+Formulation,+Analysis,+and+Algorithms)|0|
|[When to Pre-Train Graph Neural Networks? From Data Generation Perspective!](https://doi.org/10.1145/3580305.3599548)|Yuxuan Cao, Jiarong Xu, Carl Yang, Jiaan Wang, Yunchao Zhang, Chunping Wang, Lei Chen, Yang Yang|Zhejiang University, Fudan University.; Zhejiang University.; Fudan University.; Soochow University.; Finvolution Group.; Emory University.|In recent years, graph pre-training has gained significant attention, focusing on acquiring transferable knowledge from unlabeled graph data to improve downstream performance. Despite these recent endeavors, the problem of negative transfer remains a major concern when utilizing graph pre-trained models to downstream tasks. Previous studies made great efforts on the issue of what to pre-train and how to pre-train by designing a variety of graph pre-training and fine-tuning strategies. However, there are cases where even the most advanced "pre-train and fine-tune" paradigms fail to yield distinct benefits. This paper introduces a generic framework W2PGNN to answer the crucial question of when to pre-train (i.e., in what situations could we take advantage of graph pre-training) before performing effortful pre-training or fine-tuning. We start from a new perspective to explore the complex generative mechanisms from the pre-training data to downstream data. In particular, W2PGNN first fits the pre-training data into graphon bases, each element of graphon basis (i.e., a graphon) identifies a fundamental transferable pattern shared by a collection of pre-training graphs. All convex combinations of graphon bases give rise to a generator space, from which graphs generated form the solution space for those downstream data that can benefit from pre-training. In this manner, the feasibility of pre-training can be quantified as the generation probability of the downstream data from any generator in the generator space. W2PGNN offers three broad applications: providing the application scope of graph pre-trained models, quantifying the feasibility of pre-training, and assistance in selecting pre-training data to enhance downstream performance. We provide a theoretically sound solution for the first application and extensive empirical justifications for the latter two applications.|近年来，图预训练引起了人们的广泛关注，主要集中在从未标记的图数据中获取可转移的知识以提高下游性能。尽管最近做出了这些努力，但在利用图预训练模型处理下游任务时，负迁移问题仍然是一个主要关注的问题。以往的研究通过设计各种图形预训练和微调策略来解决预训练内容和预训练方式的问题。然而，在某些情况下，即使是最先进的“预先训练和微调”范式也不能产生明显的效益。本文介绍了一个通用框架 W2PGNN，以回答何时进行预训练(即在什么情况下可以利用图预训练) ，然后再进行有效的预训练或微调的关键问题。我们从一个新的角度来探讨从预训练数据到下游数据的复杂生成机制。特别是，W2PGNN 首先将预训练数据放入图形基础中，每个图形基础元素(即图形)标识一个由预训练图集合共享的基本可转移模式。所有图形基的凸组合都产生一个生成器空间，从这个空间生成的图形形成了那些可以从预训练中获益的下游数据的解空间。通过这种方式，预训练的可行性可以量化为来自发生器空间中任意发生器的下游数据的生成概率。W2PGNN 提供了三种广泛的应用: 提供图预训练模型的应用范围，量化预训练的可行性，以及协助选择预训练数据以提高下游性能。我们为第一个应用提供了一个理论上合理的解决方案，并为后两个应用提供了广泛的经验论证。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=When+to+Pre-Train+Graph+Neural+Networks?+From+Data+Generation+Perspective!)|0|
|[MBrain: A Multi-channel Self-Supervised Learning Framework for Brain Signals](https://doi.org/10.1145/3580305.3599426)|Donghong Cai, Junru Chen, Yang Yang, Teng Liu, Yafeng Li|Nuozhu Technology Co., Ltd.; Zhejiang University|Brain signals are important quantitative data for understanding physiological activities and diseases of human brain. Meanwhile, rapidly developing deep learning methods offer a wide range of opportunities for better modeling brain signals, which has attracted considerable research efforts recently. Most existing studies pay attention to supervised learning methods, which, however, require high-cost clinical labels. In addition, the huge difference in the clinical patterns of brain signals measured by invasive (e.g., SEEG) and non-invasive (e.g., EEG) methods leads to the lack of a unified method. To handle the above issues, in this paper, we propose to study the self-supervised learning (SSL) framework for brain signals that can be applied to pre-train either SEEG or EEG data. Intuitively, brain signals, generated by the firing of neurons, are transmitted among different connecting structures in human brain. Inspired by this, we propose to learn implicit spatial and temporal correlations between different channels (i.e., contacts of the electrode, corresponding to different brain areas) as the cornerstone for uniformly modeling different types of brain signals. Specifically, we capture the temporal correlation by designing the delayed-time-shift prediction task; we represent the spatial correlation by a graph structure, which is built with the goal to maximize the mutual information of each channel and its correlated ones. We further theoretically prove that our design can lead to a better predictive representation. Extensive experiments of seizure detection on both EEG and SEEG large-scale real- world datasets demonstrate our model outperforms several state-of-the-art time series SSL and unsupervised models.|脑信号是了解人脑生理活动和疾病的重要定量数据。与此同时，快速发展的深度学习方法为更好地建立大脑信号模型提供了广泛的机会，近年来引起了相当多的研究工作。大多数现有的研究都关注监督式学习疗法，然而，这需要高成本的临床标签。此外，由于有创(例如 SEEG)和无创(例如 EEG)方法测量的大脑信号的临床模式存在巨大差异，导致缺乏统一的方法。为了解决上述问题，本文提出了一种基于自监督学习(SSL)的大脑信号预训练方法。直观地说，由神经元放电产生的大脑信号在人类大脑的不同连接结构之间传递。受此启发，我们建议学习不同通道之间的内隐空间和时间相关性(即，电极的接触，对应于不同的大脑区域)作为统一建模不同类型的大脑信号的基石。具体来说，我们通过设计延迟时移预测任务来捕获时间相关性，我们用图形结构来表示空间相关性，其目的是最大化各个信道及其相关信道之间的互信息。我们进一步从理论上证明了我们的设计可以导致更好的预测表示。在 EEG 和 SEEG 大规模真实世界数据集上的大量癫痫发作检测实验表明，我们的模型优于几个最先进的时间序列 SSL 和无监督模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MBrain:+A+Multi-channel+Self-Supervised+Learning+Framework+for+Brain+Signals)|0|
|[Efficient Coreset Selection with Cluster-based Methods](https://doi.org/10.1145/3580305.3599326)|Chengliang Chai, Jiayi Wang, Nan Tang, Ye Yuan, Jiabin Liu, Yuhao Deng, Guoren Wang||Coreset selection is a technique for efficient machine learning, which selects a subset of the training data to achieve similar model performance as using the full dataset. It can be performed with or without training machine learning models. Coreset selection with training, which iteratively trains the machine model and updates data items in the coreset, is time consuming. Coreset selection without training can select the coreset before training. Gradient approximation is the typical method, but it can also be slow when dealing with large training datasets as it requires multiple iterations and pairwise distance computations for each iteration. The state-of-the-art (SOTA) results w.r.t. effectiveness are achieved by the latter approach, i.e. gradient approximation. In this paper, we aim to significantly improve the efficiency of coreset selection while ensuring good effectiveness, by improving the SOTA approaches of using gradient descent without training machine learning models. Specifically, we present a highly efficient coreset selection framework that utilizes an approximation of the gradient. This is achieved by dividing the entire training set into multiple clusters, each of which contains items with similar feature distances (calculated using the Euclidean distance). Our framework further demonstrates that the full gradient can be bounded based on the maximum feature distance between each item and each cluster, allowing for more efficient coreset selection by iterating through these clusters. Additionally, we propose an efficient method for estimating the maximum feature distance using the product quantization technique. Our experiments on multiple real-world datasets demonstrate that we can improve the efficiency 3-10 times comparing with SOTA almost without sacrificing the accuracy.|Coreset 选择是一种有效的机器学习技术，它选择训练数据的一个子集来实现与使用完整数据集相似的模型性能。它可以执行或没有训练机器学习模型。带有训练的共重置选择，迭代地训练机器模型并更新共重置中的数据项，是非常耗时的。未经训练的复位选择可以在训练前进行复位选择。梯度近似是一种典型的方法，但在处理大型训练数据集时，由于每次迭代都需要进行多次迭代和成对距离计算，因此速度也较慢。通过后一种方法，即梯度近似，可以得到最先进的 SOTA 结果。在本文中，我们的目标是通过改进使用不需要训练机器学习模型的梯度下降法的 SOTA 方法，在确保良好有效性的同时，显著提高复位选择的效率。具体来说，我们提出了一个高效的共重置选择框架，利用梯度的近似值。这是通过将整个训练集划分为多个集群来实现的，每个集群包含具有相似特征距离的项目(使用欧几里得度量计算)。我们的框架进一步证明了完全梯度可以基于每个条目和每个簇之间的最大特征距离进行约束，通过迭代遍历这些簇允许更有效的协同重置选择。此外，本文还提出了一种利用产品量化技术估计最大特征距离的有效方法。我们在多个实际数据集上的实验表明，与 SOTA 相比，我们可以在几乎不牺牲精度的情况下提高效率3-10倍。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Coreset+Selection+with+Cluster-based+Methods)|0|
|[SURE: Robust, Explainable, and Fair Classification without Sensitive Attributes](https://doi.org/10.1145/3580305.3599514)|Deepayan Chakrabarti||A classifier that is accurate on average may still underperform for "sensitive" subsets of people. Such subsets could be based on race, gender, age, etc. The goal of a fair classifier is to perform well for all sensitive subsets. But often, the sensitive subsets are not known a priori. So we may want the classifier to perform well on all subsets that are likely to be sensitive. We propose an iterative algorithm called SURE for this problem. In each iteration, SURE identifies high-risk zones in feature space where the risk of unfair classification is statistically significant. By changing the loss function's weights for points from these zones, SURE builds a fair classifier. The emphasis on statistical significance makes SURE robust to noise. The high-risk zones are intuitive and interpretable. Every step of our method is explainable in terms of significance tests. Finally, SURE is fast and parameter-free. Experiments on both simulated and real-world datasets show that SURE is competitive with the state-of-the-art.|一个平均精确的分类器对于“敏感”人群仍然表现不佳。这些子集可以基于种族、性别、年龄等。公平分类器的目标是为所有敏感子集提供良好的性能。但是，通常情况下，敏感子集是不知道的先验。因此，我们可能希望分类器在所有可能是敏感的子集上执行良好。针对这个问题，我们提出了一种迭代算法 SURE。在每次迭代中，SURE 在特征空间中识别出不公平分类风险具有统计学意义的高风险区域。通过改变这些区域的损失函数的权重，SURE 构建了一个公平的分类器。强调统计显著性使得 SURE 对噪声具有鲁棒性。高风险区域是直观和可解释的。我们方法的每一步都可以用显著性测试来解释。最后，SURE 是快速和无参数的。在模拟数据集和真实数据集上的实验表明，SURE 算法与最先进的算法相比具有竞争力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SURE:+Robust,+Explainable,+and+Fair+Classification+without+Sensitive+Attributes)|0|
|[Data-Efficient and Interpretable Tabular Anomaly Detection](https://doi.org/10.1145/3580305.3599294)|ChunHao Chang, Jinsung Yoon, Sercan Ö. Arik, Madeleine Udell, Tomas Pfister|University of Toronto; Stanford University; Google|Anomaly detection (AD) plays an important role in numerous applications. We focus on two understudied aspects of AD that are critical for integration into real-world applications. First, most AD methods cannot incorporate labeled data that are often available in practice in small quantities and can be crucial to achieve high AD accuracy. Second, most AD methods are not interpretable, a bottleneck that prevents stakeholders from understanding the reason behind the anomalies. In this paper, we propose a novel AD framework that adapts a white-box model class, Generalized Additive Models, to detect anomalies using a partial identification objective which naturally handles noisy or heterogeneous features. In addition, the proposed framework, DIAD, can incorporate a small amount of labeled data to further boost anomaly detection performances in semi-supervised settings. We demonstrate the superiority of our framework compared to previous work in both unsupervised and semi-supervised settings using diverse tabular datasets. For example, under 5 labeled anomalies DIAD improves from 86.2\% to 89.4\% AUC by learning AD from unlabeled data. We also present insightful interpretations that explain why DIAD deems certain samples as anomalies.|异常检测(AD)在许多应用中起着重要作用。我们关注 AD 的两个被忽视的方面，这两个方面对于集成到真实世界的应用程序是至关重要的。首先，大多数 AD 方法不能合并标记数据，这些数据通常在实践中少量可用，并且可能是实现高 AD 准确性的关键。其次，大多数 AD 方法是不可解释的，这是一个瓶颈，阻碍了涉众理解异常背后的原因。在本文中，我们提出了一个新的 AD 框架，它适用于一个白盒模型类，广义加法模型，以检测异常使用一个部分识别目标，自然处理噪声或异质特征。此外，建议的 DIAD 框架可以合并少量标记数据，以进一步提高半监督环境下的异常检测性能。我们证明了我们的框架比以前的工作在无监督和半监督设置使用不同的表格数据集优越。例如，在5个标记异常下，通过从未标记的数据中学习 AD，DIAD 从86.2% 提高到89.4% AUC。我们还提出了有见地的解释，解释为什么 DIAD 认为某些样本作为异常。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Data-Efficient+and+Interpretable+Tabular+Anomaly+Detection)|0|
|[Open-Set Semi-Supervised Text Classification with Latent Outlier Softening](https://doi.org/10.1145/3580305.3599456)|Junfan Chen, Richong Zhang, Junchi Chen, Chunming Hu, Yongyi Mao||Semi-supervised text classification (STC) has been extensively researched and reduces human annotation. However, existing research assuming that unlabeled data only contains in-distribution texts is unrealistic. This paper extends STC to a more practical Open-set Semi-supervised Text Classification (OSTC) setting, which assumes that the unlabeled data contains out-of-distribution (OOD) texts. The main challenge in OSTC is the false positive inference problem caused by inadvertently including OOD texts during training. To address the problem, we first develop baseline models using outlier detectors for hard OOD-data filtering in a pipeline procedure. Furthermore, we propose a Latent Outlier Softening (LOS) framework that integrates semi-supervised training and outlier detection within probabilistic latent variable modeling. LOS softens the OOD impacts by the Expectation-Maximization (EM) algorithm and weighted entropy maximization. Experiments on 3 created datasets show that LOS significantly outperforms baselines.|半监督文本分类(STC)已被广泛研究，并减少了人工标注。然而，现有的研究假设未标记的数据只包含在分发文本是不现实的。本文将 STC 扩展到一个更实用的开集半监督文本分类(OSTC)设置，该设置假定未标记的数据包含分布外(OOD)文本。OSTC 的主要挑战是在训练过程中由于不小心包含了 OOD 文本而导致的假阳性推理问题。为了解决这个问题，我们首先在流水线过程中使用离群值检测器来建立基线模型，用于硬 OOD 数据过滤。此外，我们提出了一个潜在异常软化(LOS)框架，将半监督训练和异常检测集成在概率潜变量建模中。LOS 通过期望最大化(EM)算法和加权熵最大化来软化面向对象的影响。在3个已建立的数据集上进行的实验表明，LOS 的性能明显优于基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Open-Set+Semi-Supervised+Text+Classification+with+Latent+Outlier+Softening)|0|
|[Improving Expressivity of GNNs with Subgraph-specific Factor Embedded Normalization](https://doi.org/10.1145/3580305.3599388)|Kaixuan Chen, Shunyu Liu, Tongtian Zhu, Ji Qiao, Yun Su, Yingjie Tian, Tongya Zheng, Haofei Zhang, Zunlei Feng, Jingwen Ye, Mingli Song|China Electric Power Research Institute, Beijing, China; State Grid Shanghai Municipal Electric Power Company, Shanghai, China; Zhejiang University, Hangzhou, China|Graph Neural Networks~(GNNs) have emerged as a powerful category of learning architecture for handling graph-structured data. However, existing GNNs typically ignore crucial structural characteristics in node-induced subgraphs, which thus limits their expressiveness for various downstream tasks. In this paper, we strive to strengthen the representative capabilities of GNNs by devising a dedicated plug-and-play normalization scheme, termed as SUbgraph-sPEcific FactoR Embedded Normalization (SuperNorm), that explicitly considers the intra-connection information within each node-induced subgraph. To this end, we embed the subgraph-specific factor at the beginning and the end of the standard BatchNorm, as well as incorporate graph instance-specific statistics for improved distinguishable capabilities. In the meantime, we provide theoretical analysis to support that, with the elaborated SuperNorm, an arbitrary GNN is at least as powerful as the 1-WL test in distinguishing non-isomorphism graphs. Furthermore, the proposed SuperNorm scheme is also demonstrated to alleviate the over-smoothing phenomenon. Experimental results related to predictions of graph, node, and link properties on the eight popular datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/chenchkx/SuperNorm.|图形神经网络已经成为处理图形结构数据的一种强大的学习体系结构。然而，现有的 GNN 通常忽略节点诱导子图的关键结构特征，从而限制了它们对各种下游任务的表达能力。本文通过设计一个专用的即插即用规范化方案，即 SUbgraph-sPEFtoR 嵌入式规范化(SuperNorm) ，明确考虑每个节点诱导子图中的内部连接信息，力求增强 GNN 的代表性能力。为此，我们在标准 BatchNorm 的开头和结尾嵌入了子图特定的因子，并且为了改进可区分性能而合并了图实例特定的统计信息。同时，我们提供了理论分析来支持这样的观点，即在区分非同构图时，任意的 GNN 至少和1-WL 检验一样强大。此外，本文提出的 SuperNorm 方案也被证明可以减轻过平滑现象。实验结果表明，该方法可以有效地预测八个流行数据集的图、节点和链接特性。密码可在 https://github.com/chenchkx/supernorm 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Expressivity+of+GNNs+with+Subgraph-specific+Factor+Embedded+Normalization)|0|
|[Neural-Hidden-CRF: A Robust Weakly-Supervised Sequence Labeler](https://doi.org/10.1145/3580305.3599445)|Zhijun Chen, Hailong Sun, Wanhao Zhang, Chunyi Xu, Qianren Mao, Pengpeng Chen||We propose a neuralized undirected graphical model called Neural-Hidden-CRF to solve the weakly-supervised sequence labeling problem. Under the umbrella of probabilistic undirected graph theory, the proposed Neural-Hidden-CRF embedded with a hidden CRF layer models the variables of word sequence, latent ground truth sequence, and weak label sequence with the global perspective that undirected graphical models particularly enjoy. In Neural-Hidden-CRF, we can capitalize on the powerful language model BERT or other deep models to provide rich contextual semantic knowledge to the latent ground truth sequence, and use the hidden CRF layer to capture the internal label dependencies. Neural-Hidden-CRF is conceptually simple and empirically powerful. It obtains new state-of-the-art results on one crowdsourcing benchmark and three weak-supervision benchmarks, including outperforming the recent advanced model CHMM by 2.80 F1 points and 2.23 F1 points in average generalization and inference performance, respectively.|我们提出了一个神经化无向图模型，称为神经隐 CRF 来解决弱监督序列标记问题。在概率无向图理论的框架下，提出了一种嵌入隐式 CRF 层的神经隐式 CRF 模型，该模型利用无向图模型特别喜欢的全局视角对词序列、潜在地真序列和弱标号序列的变量进行建模。在神经隐 CRF 中，我们可以利用功能强大的语言模型 BERT 或其他深度模型为潜在的地面真相序列提供丰富的上下文语义知识，并利用隐 CRF 层捕获内部标签依赖。神经隐藏 CRF 在概念上很简单，在经验上也很强大。它在一个众包基准和三个弱监督基准上获得了新的最先进的结果，包括在平均概括性和推理性能方面分别比最近的先进模型 CHMM 高出2.80 F1和2.23 F1。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural-Hidden-CRF:+A+Robust+Weakly-Supervised+Sequence+Labeler)|0|
|[Classification of Edge-dependent Labels of Nodes in Hypergraphs](https://doi.org/10.1145/3580305.3599274)|Minyoung Choe, Sunwoo Kim, Jaemin Yoo, Kijung Shin|Carnegie Mellon University; Korea Advanced Institute of Science and Technology|A hypergraph is a data structure composed of nodes and hyperedges, where each hyperedge is an any-sized subset of nodes. Due to the flexibility in hyperedge size, hypergraphs represent group interactions (e.g., co-authorship by more than two authors) more naturally and accurately than ordinary graphs. Interestingly, many real-world systems modeled as hypergraphs contain edge-dependent node labels, i.e., node labels that vary depending on hyperedges. For example, on co-authorship datasets, the same author (i.e., a node) can be the primary author in a paper (i.e., a hyperedge) but the corresponding author in another paper (i.e., another hyperedge). In this work, we introduce a classification of edge-dependent node labels as a new problem. This problem can be used as a benchmark task for hypergraph neural networks, which recently have attracted great attention, and also the usefulness of edge-dependent node labels has been verified in various applications. To tackle this problem, we propose WHATsNet, a novel hypergraph neural network that represents the same node differently depending on the hyperedges it participates in by reflecting its varying importance in the hyperedges. To this end, WHATsNet models the relations between nodes within each hyperedge, using their relative centrality as positional encodings. In our experiments, we demonstrate that WHATsNet significantly and consistently outperforms ten competitors on six real-world hypergraphs, and we also show successful applications of WHATsNet to (a) ranking aggregation, (b) node clustering, and (c) product return prediction.|超图是由节点和超边组成的数据结构，其中每个超边是节点的任意大小的子集。由于超边大小的灵活性，超图比普通图更自然、更准确地表示群体交互(例如，两个以上作者的合著)。有趣的是，许多建模为超图的现实世界系统包含依赖于边缘的节点标签，即依赖于超边缘而变化的节点标签。例如，在合著数据集上，同一作者(即节点)可以是一篇论文的主要作者(即超边缘) ，但是可以是另一篇论文的相应作者(即另一个超边缘)。在这项工作中，我们引入了一个新的问题，即边缘相关节点标签的分类。该问题可以作为超图神经网络的一个基准任务，近年来受到了广泛的关注，边缘相关节点标记在各种应用中的有效性也得到了验证。为了解决这个问题，我们提出了 WHATsNet，这是一种新的超图神经网络，通过在超边界中反映其不同的重要性，它可以根据所参与的超边界不同地表示同一个节点。为此，WHATsNet 使用每个超边界内的节点的相对中心性作为位置编码来建模它们之间的关系。在我们的实验中，我们证明了 WHATsNet 在六个真实世界的超图上显着而持续地优于十个竞争对手，我们还显示了 WHATsNet 在(a)排名聚合，(b)节点聚类和(c)产品返回预测方面的成功应用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Classification+of+Edge-dependent+Labels+of+Nodes+in+Hypergraphs)|0|
|[Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers](https://doi.org/10.1145/3580305.3599490)|Chanyoung Chung, Jaejun Lee, Joyce Jiyoung Whang|KAIST|A hyper-relational knowledge graph has been recently studied where a triplet is associated with a set of qualifiers; a qualifier is composed of a relation and an entity, providing auxiliary information for a triplet. While existing hyper-relational knowledge graph embedding methods assume that the entities are discrete objects, some information should be represented using numeric values, e.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at, Oxford Univ.) can be associated with a qualifier such as (start time, 1911). In this paper, we propose a unified framework named HyNT that learns representations of a hyper-relational knowledge graph containing numeric literals in either triplets or qualifiers. We define a context transformer and a prediction transformer to learn the representations based not only on the correlations between a triplet and its qualifiers but also on the numeric information. By learning compact representations of triplets and qualifiers and feeding them into the transformers, we reduce the computation cost of using transformers. Using HyNT, we can predict missing numeric values in addition to missing entities or relations in a hyper-relational knowledge graph. Experimental results show that HyNT significantly outperforms state-of-the-art methods on real-world datasets.|最近研究了一个超关系知识图，其中一个三元组与一组限定符相关联; 一个限定符由一个关系和一个实体组成，为一个三元组提供辅助信息。虽然现有的超关系知识图嵌入方法假设实体是离散的对象，一些信息应该使用数值表示，例如，(J.R.R。 ，诞生于，1892)。此外，三连体(J.R.R，牛津大学毕业)可以与限定词，如(开始时间，1911年)。本文提出了一个统一的知识表示框架 HyNT，它可以学习包含三联体或限定符的数值文字的超关系知识图的表示。我们定义了一个上下文转换器和一个预测转换器来学习不仅基于三元组及其限定符之间的相关性，而且基于数值信息的表示。通过学习三联体和限定符的简洁表示并将它们输入变形金刚，我们降低了使用变压器的计算成本。使用 HyNT，我们可以预测缺失的数值，除了缺失的实体或关系在一个超关系知识图。实验结果表明，HyNT 在真实世界数据集上的性能明显优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Representation+Learning+on+Hyper-Relational+and+Numeric+Knowledge+Graphs+with+Transformers)|0|
|[Reducing Exposure to Harmful Content via Graph Rewiring](https://doi.org/10.1145/3580305.3599489)|Corinna Coupette, Stefan Neumann, Aristides Gionis|Max Planck Institute for Informatics; KTH Royal Institute of Technology|Most media content consumed today is provided by digital platforms that aggregate input from diverse sources, where access to information is mediated by recommendation algorithms. One principal challenge in this context is dealing with content that is considered harmful. Striking a balance between competing stakeholder interests, rather than block harmful content altogether, one approach is to minimize the exposure to such content that is induced specifically by algorithmic recommendations. Hence, modeling media items and recommendations as a directed graph, we study the problem of reducing the exposure to harmful content via edge rewiring. We formalize this problem using absorbing random walks, and prove that it is NP-hard and NP-hard to approximate to within an additive error, while under realistic assumptions, the greedy method yields a (1-1/e)-approximation. Thus, we introduce Gamine, a fast greedy algorithm that can reduce the exposure to harmful content with or without quality constraints on recommendations. By performing just 100 rewirings on YouTube graphs with several hundred thousand edges, Gamine reduces the initial exposure by 50%, while ensuring that its recommendations are at most 5% less relevant than the original recommendations. Through extensive experiments on synthetic data and real-world data from video recommendation and news feed applications, we confirm the effectiveness, robustness, and efficiency of Gamine in practice.|当今消费的大多数媒体内容都是由数字平台提供的，这些平台汇集了来自不同来源的输入，在这些平台中，信息的获取是通过推荐算法来实现的。这方面的一个主要挑战是处理被认为有害的内容。在相互竞争的利益相关者利益之间取得平衡，而不是完全屏蔽有害的内容，一种方法是尽量减少对这些内容的暴露，这些内容是由算法推荐引起的。因此，建模媒体项目和推荐作为一个有向图，我们研究的问题，减少接触到有害的内容通过边缘重新布线。利用吸收随机游动将该问题形式化，证明了在加性误差范围内逼近是 NP- 困难和 NP- 困难的，而在现实的假设条件下，贪婪方法得到了(1-1/e)-逼近。因此，我们引入 Gamine，一个快速贪婪的算法，可以减少对有害内容的暴露，有或没有质量约束的建议。通过在 YouTube 上执行100次有几十万条边的图表重新布线，Gamine 减少了50% 的初始曝光，同时确保它的建议最多比最初的建议少5% 的相关性。通过对合成数据和来自视频推荐和新闻馈送应用的真实数据的大量实验，我们证实了 Gamine 在实际应用中的有效性、鲁棒性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reducing+Exposure+to+Harmful+Content+via+Graph+Rewiring)|0|
|[MGNN: Graph Neural Networks Inspired by Distance Geometry Problem](https://doi.org/10.1145/3580305.3599431)|Guanyu Cui, Zhewei Wei||Graph Neural Networks (GNNs) have emerged as a prominent research topic in the field of machine learning. Existing GNN models are commonly categorized into two types: spectral GNNs, which are designed based on polynomial graph filters, and spatial GNNs, which utilize a message-passing scheme as the foundation of the model. For the expressive power and universality of spectral GNNs, a natural approach is to improve the design of basis functions for better approximation ability. As for spatial GNNs, models like Graph Isomorphism Networks (GIN) analyze their expressive power based on Graph Isomorphism Tests. Recently, there have been attempts to establish connections between spatial GNNs and geometric concepts like curvature and cellular sheaves, as well as physical phenomena like oscillators. However, despite the recent progress, there is still a lack of comprehensive analysis regarding the universality of spatial GNNs from the perspectives of geometry and physics. In this paper, we propose MetricGNN (MGNN), a spatial GNN model inspired by the congruent-insensitivity property of classifiers in the classification phase of GNNs. We demonstrate that a GNN model is universal in the spatial domain if it can generate embedding matrices that are congruent to any given embedding matrix. This property is closely related to the Distance Geometry Problem (DGP). Since DGP is an NP-Hard combinatorial optimization problem, we propose optimizing an energy function derived from spring networks and the Multi-Dimensional Scaling (MDS) problem. This approach also allows our model to handle both homophilic and heterophilic graphs. Finally, we propose employing the iteration method to optimize our energy function. We extensively evaluate the effectiveness of our model through experiments conducted on both synthetic and real-world datasets. Our code is available at: https://github.com/GuanyuCui/MGNN.|图神经网络(GNN)已经成为机器学习领域中一个突出的研究课题。现有的 GNN 模型一般分为两类: 基于多项式图滤波器的谱 GNN 和基于消息传递机制的空间 GNN。为了提高谱 GNN 的表达能力和通用性，一种自然的方法是改进基函数的设计以获得更好的逼近能力。对于空间 GNN，类似于图同构网络(GIN)这样的模型基于图同构检验来分析它们的表达能力。最近，有人试图在空间 GNN 和曲率、细胞层等几何概念以及振子等物理现象之间建立联系。然而，尽管最近取得了进展，但从几何学和物理学的角度对空间全球导航卫星系统的普遍性仍缺乏全面的分析。在本文中，我们提出了一个空间 GNN 模型 MetricGNN (MGNN) ，该模型的灵感来自于分类器在 GNN 分类阶段的同余不敏感性。我们证明了一个 GNN 模型在空间域是通用的，如果它能生成与任意给定的嵌入矩阵相合的嵌入矩阵。此属性与 Cayley–Menger行列式问题(dGP)密切相关。由于 DGP 是一个 NP 难组合优化问题，我们建议优化一个由弹簧网络和多维尺度(MDS)问题导出的能量函数。这种方法也允许我们的模型同时处理同型和异型图。最后，我们提出采用迭代法来优化我们的能量函数。我们通过在合成数据集和真实数据集上进行的实验，广泛地评估了模型的有效性。我们的代码可以在以下 https://github.com/guanyucui/mgnn 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MGNN:+Graph+Neural+Networks+Inspired+by+Distance+Geometry+Problem)|0|
|[Deep Encoders with Auxiliary Parameters for Extreme Classification](https://doi.org/10.1145/3580305.3599301)|Kunal Dahiya, Sachin Yadav, Sushant Sondhi, Deepak Saini, Sonu Mehta, Jian Jiao, Sumeet Agarwal, Purushottam Kar, Manik Varma||The task of annotating a data point with labels most relevant to it from a large universe of labels is referred to as Extreme Classification (XC). State-of-the-art XC methods have applications in ranking, recommendation, and tagging and mostly employ a combination architecture comprised of a deep encoder and a high-capacity classifier. These two components are often trained in a modular fashion to conserve compute. This paper shows that in XC settings where data paucity and semantic gap issues abound, this can lead to suboptimal encoder training which negatively affects the performance of the overall architecture. The paper then proposes a lightweight alternative DEXA that augments encoder training with auxiliary parameters. Incorporating DEXA into existing XC architectures requires minimal modifications and the method can scale to datasets with 40 million labels and offer predictions that are up to 6% and 15% more accurate than embeddings offered by existing deep XC methods on benchmark and proprietary datasets, respectively. The paper also analyzes DEXA theoretically and shows that it offers provably superior encoder training than existing Siamese training strategies in certain realizable settings. Code for DEXA is available at https://github.com/Extreme-classification/dexa.|使用来自大量标签的与数据点最相关的标签对数据点进行注释的任务称为极端分类(XC)。最先进的 XC 方法在排名、推荐和标记方面都有应用，主要采用由深度编码器和高容量分类器组成的组合体系结构。这两个组件通常以模块化的方式进行训练，以保存计算。本文指出，在 XC 环境中，数据缺乏和语义缺口问题比比皆是，这可能导致编码器训练次优，从而对整个体系结构的性能产生负面影响。然后提出了一种轻量级的 DEXA 替代算法，该算法利用辅助参数增强编码器训练。将 DEXA 整合到现有的 XC 体系结构中需要最小的修改，该方法可以扩展到4000万标签的数据集，并提供比基准数据集和专有数据集上现有深层 XC 方法提供的嵌入更准确的预测，分别高出6% 和15% 。本文还对 DEXA 进行了理论分析，结果表明，在一定的可实现性条件下，DEXA 能够提供比现有暹罗训练策略更好的编码器训练。有关 DEXA 的代码可于 https://github.com/extreme-classification/DEXA 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Encoders+with+Auxiliary+Parameters+for+Extreme+Classification)|0|
|[A Unified Framework of Graph Information Bottleneck for Robustness and Membership Privacy](https://doi.org/10.1145/3580305.3599248)|Enyan Dai, Limeng Cui, Zhengyang Wang, Xianfeng Tang, Yinghan Wang, Monica Xiao Cheng, Bing Yin, Suhang Wang|Amazon; Pennsylvania State University|Graph Neural Networks (GNNs) have achieved great success in modeling graph-structured data. However, recent works show that GNNs are vulnerable to adversarial attacks which can fool the GNN model to make desired predictions of the attacker. In addition, training data of GNNs can be leaked under membership inference attacks. This largely hinders the adoption of GNNs in high-stake domains such as e-commerce, finance and bioinformatics. Though investigations have been made in conducting robust predictions and protecting membership privacy, they generally fail to simultaneously consider the robustness and membership privacy. Therefore, in this work, we study a novel problem of developing robust and membership privacy-preserving GNNs. Our analysis shows that Information Bottleneck (IB) can help filter out noisy information and regularize the predictions on labeled samples, which can benefit robustness and membership privacy. However, structural noises and lack of labels in node classification challenge the deployment of IB on graph-structured data. To mitigate these issues, we propose a novel graph information bottleneck framework that can alleviate structural noises with neighbor bottleneck. Pseudo labels are also incorporated in the optimization to minimize the gap between the predictions on the labeled set and unlabeled set for membership privacy. Extensive experiments on real-world datasets demonstrate that our method can give robust predictions and simultaneously preserve membership privacy.|图形神经网络(GNN)在建立图形结构数据模型方面取得了巨大的成功。然而，最近的工作表明，GNN 是脆弱的对手攻击，可以欺骗 GNN 模型，使所需的预测攻击者。另外，在成员推理攻击下，GNN 的训练数据可能会泄漏。这在很大程度上阻碍了 GNN 在电子商务、金融和生物信息学等高风险领域的采用。虽然在进行强有力的预测和保护成员隐私方面已经进行了调查，但是他们通常没有同时考虑到强健性和成员隐私。因此，在这项工作中，我们研究了一个新的问题，开发健壮和成员隐私保护 GNN。我们的分析表明，信息瓶颈(IB)可以帮助过滤掉噪声信息，规范标记样本的预测，这有利于鲁棒性和成员隐私。然而，节点分类中存在的结构噪声和缺乏标记等问题，给图结构数据 IB 的应用带来了挑战。为了解决这些问题，我们提出了一种新的图形信息瓶颈框架，可以减轻结构噪声的邻居瓶颈。在优化过程中还引入了伪标签，以最大限度地缩小标签集上的预测与未标签集上的成员隐私之间的差距。对真实世界数据集的大量实验表明，我们的方法可以给出稳健的预测，同时保护成员隐私。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Unified+Framework+of+Graph+Information+Bottleneck+for+Robustness+and+Membership+Privacy)|0|
|[Investigating Trojan Attacks on Pre-trained Language Model-powered Database Middleware](https://doi.org/10.1145/3580305.3599395)|Peiran Dong, Song Guo, Junxiao Wang|King Abdullah University of Science and Technology; Hong Kong Polytechnic University|The recent success of pre-trained language models (PLMs) such as BERT has resulted in the development of various beneficial database middlewares, including natural language query interfaces and entity matching. This shift has been greatly facilitated by the extensive external knowledge of PLMs. However, as PLMs are often provided by untrusted third parties, their lack of standardization and regulation poses significant security risks that have yet to be fully explored. This paper investigates the security threats posed by malicious PLMs to these emerging database middleware. We specifically propose a novel type of Trojan attack, where a maliciously designed PLM causes unexpected behavior in the database middleware. These Trojan attacks possess the following characteristics: (1) Triggerability: The Trojan-infected database middleware will function normally with normal input, but will likely malfunction when triggered by the attacker. (2) Imperceptibility: There is no need for noticeable modification of the input to trigger the Trojan. (3) Generalizability: The Trojan is capable of targeting a variety of downstream tasks, not just one specific task. We thoroughly evaluate the impact of these Trojan attacks through experiments and analyze potential countermeasures and their limitations. Our findings could aid in the creation of stronger mechanisms for the implementation of PLMs in database middleware.|近年来，诸如 BERT 之类的预训练语言模型的成功使得各种有益的数据库中间件得以开发，包括自然语言查询接口和实体匹配。PLM 的广泛外部知识极大地促进了这种转变。然而，由于 PLM 往往由不可信任的第三方提供，它们缺乏标准化和监管构成了重大的安全风险，尚未得到充分探讨。本文研究了恶意 PLM 对这些新兴的数据库中间件所构成的安全威胁。我们特别提出了一种新型的木马攻击，其中恶意设计的 PLM 在数据库中间件中导致意外行为。这些木马攻击具有以下特点: (1)可触发性: 被木马感染的数据库中间件在正常输入情况下运行正常，但在被攻击者触发时可能出现故障。(2)不可感知性: 不需要对输入进行明显的修改来触发木马。(3)概括性: 木马能够针对多种下游任务，而不仅仅是一个特定的任务。我们通过实验深入评估了这些木马攻击的影响，并分析了潜在的对策及其局限性。我们的研究结果可以帮助创建更强大的机制来实现数据库中间件中的 PLM。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Investigating+Trojan+Attacks+on+Pre-trained+Language+Model-powered+Database+Middleware)|0|
|[Localised Adaptive Spatial-Temporal Graph Neural Network](https://doi.org/10.1145/3580305.3599418)|Wenying Duan, Xiaoxi He, Zimu Zhou, Lothar Thiele, Hong Rao|City University of Hong Kong; Nanchang University; University of Macau; ETH Zurich|Spatial-temporal graph models are prevailing for abstracting and modelling spatial and temporal dependencies. In this work, we ask the following question: \textit{whether and to what extent can we localise spatial-temporal graph models?} We limit our scope to adaptive spatial-temporal graph neural networks (ASTGNNs), the state-of-the-art model architecture. Our approach to localisation involves sparsifying the spatial graph adjacency matrices. To this end, we propose Adaptive Graph Sparsification (AGS), a graph sparsification algorithm which successfully enables the localisation of ASTGNNs to an extreme extent (fully localisation). We apply AGS to two distinct ASTGNN architectures and nine spatial-temporal datasets. Intriguingly, we observe that spatial graphs in ASTGNNs can be sparsified by over 99.5\% without any decline in test accuracy. Furthermore, even when ASTGNNs are fully localised, becoming graph-less and purely temporal, we record no drop in accuracy for the majority of tested datasets, with only minor accuracy deterioration observed in the remaining datasets. However, when the partially or fully localised ASTGNNs are reinitialised and retrained on the same data, there is a considerable and consistent drop in accuracy. Based on these observations, we reckon that \textit{(i)} in the tested data, the information provided by the spatial dependencies is primarily included in the information provided by the temporal dependencies and, thus, can be essentially ignored for inference; and \textit{(ii)} although the spatial dependencies provide redundant information, it is vital for the effective training of ASTGNNs and thus cannot be ignored during training. Furthermore, the localisation of ASTGNNs holds the potential to reduce the heavy computation overhead required on large-scale spatial-temporal data and further enable the distributed deployment of ASTGNNs.|时空图模型主要用于抽象和建模空间和时间依赖关系。在这项工作中，我们提出了以下问题: texttit {我们是否可以局部化时空图模型，以及在多大程度上可以局部化? }我们限制我们的范围，自适应时空图神经网络(ASTGNN) ，国家的最先进的模型架构。我们的定位方法包括稀疏空间图的邻接矩阵。为此，我们提出了自适应图稀疏化(AGS)算法，这是一种图稀疏化算法，它成功地使 ASTGNN 的本地化达到了一个极端的程度(完全本地化)。我们将 AGS 应用于两个不同的 ASTGNN 体系结构和九个时空数据集。有趣的是，我们观察到 ASTGNN 中的空间图可以稀疏超过99.5% ，而且测试精度没有任何下降。此外，即使当 ASTGNN 完全本地化，成为无图和纯粹的时间，我们没有记录大多数测试数据集的准确性下降，只有在其余数据集中观察到轻微的准确性恶化。然而，当部分或完全本地化的 ASTGNN 在相同的数据上重新初始化和再训练时，精度会有相当大的一致性下降。基于这些观测结果，我们认为在测试数据中的 texttit {(i)} ，空间依赖提供的信息主要包含在时间依赖提供的信息中，因此基本上可以被忽略进行推理; 而 texttit {(ii)}尽管空间依赖提供了冗余信息，但对于 ASTGNN 的有效训练是至关重要的，因此在训练过程中不能被忽略。此外，ASTGNN 的本地化有可能减少大规模时空数据所需的大量计算开销，并进一步实现 ASTGNN 的分布式部署。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Localised+Adaptive+Spatial-Temporal+Graph+Neural+Network)|0|
|[TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://doi.org/10.1145/3580305.3599533)|Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam|IBM Research|Transformers have gained popularity in time series forecasting for their ability to capture long-sequence interactions. However, their high memory and computing requirements pose a critical bottleneck for long-term forecasting. To address this, we propose TSMixer, a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules. TSMixer is designed for multivariate forecasting and representation learning on patched time series, providing an efficient alternative to Transformers. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approach to effectively handle noisy channel interactions and generalization across diverse datasets, a common challenge in existing patch channel-mixing methods. Additionally, a simple gated attention mechanism is introduced in the backbone to prioritize important features. By incorporating these lightweight components, we significantly enhance the learning capability of simple MLP structures, outperforming complex Transformer models with minimal computing usage. Moreover, TSMixer's modular design enables compatibility with both supervised and masked self-supervised learning methods, making it a promising building block for time-series Foundation Models. TSMixer outperforms state-of-the-art MLP and Transformer models in forecasting by a considerable margin of 8-60%. It also outperforms the latest strong benchmarks of Patch-Transformer models (by 1-2%) with a significant reduction in memory and runtime (2-3X).|变压器已获得流行的时间序列预测，因为他们的能力捕捉长期序列的相互作用。然而，它们的高内存和计算需求成为长期预测的一个关键瓶颈。为了解决这个问题，我们提出了 TSMixer，一个轻量级的神经结构完全由多层感知器(MLP)模块组成。TSMixer 设计用于对修补后的时间序列进行多变量预测和表示学习，为变压器提供了一种有效的替代方案。我们的模型从计算机视觉中 MLP-Mixer 模型的成功中得到启发。我们展示了将视觉 MLP 混频器应用于时间序列所面临的挑战，并引入了经验验证的组件来提高精度。这包括一个新颖的设计范例，将在线协调头附加到 MLP-Mixer 骨干，用于显式建模时间序列属性，如层次结构和通道相关性。我们还提出了一种混合信道建模方法来有效地处理不同数据集之间的噪声信道交互和泛化，这是现有补丁信道混合方法的一个共同挑战。此外，在主干中引入了一个简单的门控注意机制来对重要特性进行优先排序。通过结合这些轻量级组件，我们显著提高了简单 MLP 结构的学习能力，以最少的计算使用超过了复杂的变压器模型。此外，TSMixer 的模块化设计能够兼容监督和掩蔽自监督学习方法，使其成为时间序列基础模型的一个有前途的组成部分。TSMixer 在预测方面优于最先进的 MLP 和变压器模型，优势可达8-60% 。它还优于最新的补丁变压器模型的强大基准测试(1-2%) ，大大减少了内存和运行时(2-3倍)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TSMixer:+Lightweight+MLP-Mixer+Model+for+Multivariate+Time+Series+Forecasting)|0|
|[Dependence and Model Selection in LLP: The Problem of Variants](https://doi.org/10.1145/3580305.3599307)|Gabriel Franco, Mark Crovella, Giovanni Comarela||The problem of Learning from Label Proportions (LLP) has received considerable research attention and has numerous practical applications. In LLP, a hypothesis assigning labels to items is learned using knowledge of only the proportion of labels found in predefined groups, called bags. While a number of algorithmic approaches to learning in this context have been proposed, very little work has addressed the model selection problem for LLP. Nonetheless, it is not obvious how to extend straightforward model selection approaches to LLP, in part because of the lack of item labels. More fundamentally, we argue that a careful approach to model selection for LLP requires consideration of the dependence structure that exists between bags, items, and labels. In this paper we formalize this structure and show how it affects model selection. We show how this leads to improved methods of model selection that we demonstrate outperform the state of the art over a wide range of datasets and LLP algorithms.|从标签比例学习问题(LLP)已经得到了相当多的研究关注，并且有着广泛的实际应用。在 LLP 中，我们只需要知道预定义群体中标签所占的比例，即袋子，就可以学会为物品分配标签的假设。虽然在这种情况下已经提出了一些算法学习方法，但是很少有工作涉及到 LLP 的模型选择问题。尽管如此，如何将直接的模型选择方法扩展到 LLP 还不是很明显，部分原因是因为缺少项目标签。更为根本的是，我们认为，为 LLP 仔细选择模型的方法需要考虑袋子、物品和标签之间存在的依赖结构。在本文中，我们将这种结构形式化，并说明它如何影响模型选择。我们展示了这如何导致改进的模型选择方法，我们证明了在广泛的数据集和 LLP 算法中优于最先进的状态。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dependence+and+Model+Selection+in+LLP:+The+Problem+of+Variants)|0|
|[Pre-training Antibody Language Models for Antigen-Specific Computational Antibody Design](https://doi.org/10.1145/3580305.3599468)|Kaiyuan Gao, Lijun Wu, Jinhua Zhu, Tianbo Peng, Yingce Xia, Liang He, Shufang Xie, Tao Qin, Haiguang Liu, Kun He, TieYan Liu|Microsoft Research AI4Science; School of Computer Science and Technology, Huazhong University of Science and Technology; CAS Key Laboratory of GIPAS, University of Science and Technology of China; School of Life Sciences and Biomedical Pioneering Innovation Center, Peking University|Antibodies are proteins that effectively protect the human body by binding to pathogens. Recently, deep learning-based computational antibody design has attracted popular attention since it automatically mines the antibody patterns from data that could be complementary to human experiences. However, the computational methods heavily rely on high-quality antibody structure data, which is quite limited. Besides, the complementarity-determining region (CDR), which is the key component of an antibody that determines the specificity and binding affinity, is highly variable and hard to predict. Therefore, the limited availability of high-quality antibody structure data exacerbates the difficulty of CDR generation. Fortunately, there is a large amount of sequence data for antibodies that can help model the CDR and reduce reliance on structure data. By witnessing the success of pre-training models for protein modeling, in this paper, we develop the antibody pre-training language model and incorporate it into the antigen-specific antibody design model in a systemic way. Specifically, we first pre-train a novel antibody language model based on the sequence data, then propose a one-shot way for sequence and structure generation of CDR to mitigate the high cost and error propagation associated with autoregressive methods, and finally leverage the pre-trained antibody model for the antigen-specific antibody generation model with some carefully designed modules. Our experiments demonstrate the superiority of our method over previous baselines in tasks such as sequence and structure generation, CDR-H3 design for antigen binding, and antibody optimization 1 . The code is available at https://github.com/KyGao/ABGNN.|抗体是通过与病原体结合而有效保护人体的蛋白质。最近，基于深度学习的计算抗体设计引起了人们的广泛关注，因为它可以自动地从数据中挖掘抗体模式，这些数据可以补充人类的经验。然而，计算方法严重依赖于高质量的抗体结构数据，这是相当有限的。此外，互补决定区(CDR)是决定特异性和结合亲和力的抗体的关键组分，具有高度的可变性和难以预测性。因此，高质量抗体结构数据的有限可用性加剧了 CDR 产生的困难。幸运的是，有大量的抗体序列数据可以帮助建模 CDR 和减少对结构数据的依赖。本文在目睹蛋白质建模预训练模型成功的基础上，建立了抗体预训练语言模型，并将其系统地引入抗原特异性抗体设计模型。具体来说，我们首先基于序列数据预训练一种新的抗体语言模型，然后提出一种一次性生成 CDR 序列和结构的方法，以减轻与自回归方法相关的高成本和错误传播，最后利用预训练的抗体模型生成抗原特异性抗体模型和一些精心设计的模块。我们的实验证明了我们的方法在序列和结构生成、抗原结合的 CDR-H3设计和抗体优化1等任务上优于以前的基线。密码可在 https://github.com/kygao/abgnn 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pre-training+Antibody+Language+Models+for+Antigen-Specific+Computational+Antibody+Design)|0|
|[GAL-VNE: Solving the VNE Problem with Global Reinforcement Learning and Local One-Shot Neural Prediction](https://doi.org/10.1145/3580305.3599358)|Haoyu Geng, Runzhong Wang, Fei Wu, Junchi Yan|Zhejiang University; Shanghai Jiao Tong University|The NP-hard combinatorial Virtual Network Embedding (VNE) Problem refers to finding the node and edge mapping between a virtual net (request) and the physical net (resource). Learning-based methods are recently devised beyond traditional heuristic solvers. However, the efficiency and scalability hinder its applicability as reinforcement learning (RL) is often adopted in an auto-regressive node-by-node mapping manner to handle complex mapping constraints, for each coming request for mapping. Moreover, existing learning-based works often independently consider each online request, limiting the long-term online service performance. In this paper, we present a synergistic Global-And-Local learning approach for the VNE problem (GAL-VNE). At the global level across requests, RL is employed to capture the cross-request relation for better global resource accommodation to improve overall performance. At the local level within each request, we aim to replace the sequential decision-making procedure which relies much on the network size, with a more efficient one-shot solution generation scheme. The main challenge for such a one-shot model is how to encode the constraints under an end-to-end learning and inference paradigm. Accordingly, within the "rank-then-search" paradigm, we propose to first pretrain a graph neural network (GNN)-based node ranker with imitation supervision from an off-the-shelf solver (moderately expensive yet high quality), which is meanwhile regularized by a neighboring smooth prior. Then RL is used to finetune the GNN ranker whose supervision directly refers to the final (undifferentiable) business objectives concerning revenue and cost, etc. Experiments on benchmarks show that our method outperforms classic and learning-based methods in both efficacy and efficiency.|NP 难组合虚拟网络嵌入(VNE)问题是指在虚拟网络(请求)和物理网络(资源)之间寻找节点和边缘映射。基于学习的方法最近被设计出来，超越了传统的启发式解决方案。然而，效率和可扩展性妨碍了它的适用性，因为强化学习(RL)通常采用逐个节点自动回归映射的方式来处理复杂的映射约束，以满足每个即将到来的映射请求。此外，现有的基于学习的工作往往独立考虑每个在线请求，限制了长期的在线服务性能。本文提出了一种求解 VNE 问题的全局-局部协同学习方法(GAL-VNE)。在全球层面上，RL 被用来捕获跨请求关系，以便更好地提供全球资源，从而提高总体性能。在每个请求的局部层次上，我们的目标是用一种更有效的一次性解决方案生成方案来代替依赖于网络规模的顺序决策过程。这种一次性模型的主要挑战是如何在端到端学习和推理范式下对约束进行编码。因此，在“先排序后搜索”的范式内，我们提出首先预训练一个基于图神经网络(GNN)的节点排序器，模拟来自现成的解决方案(中等昂贵但高质量)的监督，同时由相邻的平滑先验正规化。然后使用 RL 对 GNN 排名进行微调，GNN 排名的监督直接指涉及收入和成本等最终(不可区分的)业务目标。基准测试的实验结果表明，该方法在效率和效果上均优于经典和基于学习的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GAL-VNE:+Solving+the+VNE+Problem+with+Global+Reinforcement+Learning+and+Local+One-Shot+Neural+Prediction)|0|
|[Sparse Binary Transformers for Multivariate Time Series Modeling](https://doi.org/10.1145/3580305.3599508)|Matt Gorbett, Hossein Shirazi, Indrakshi Ray|Colorado State University|Compressed Neural Networks have the potential to enable deep learning across new applications and smaller computational environments. However, understanding the range of learning tasks in which such models can succeed is not well studied. In this work, we apply sparse and binary-weighted Transformers to multivariate time series problems, showing that the lightweight models achieve accuracy comparable to that of dense floating-point Transformers of the same structure. Our model achieves favorable results across three time series learning tasks: classification, anomaly detection, and single-step forecasting. Additionally, to reduce the computational complexity of the attention mechanism, we apply two modifications, which show little to no decline in model performance: 1) in the classification task, we apply a fixed mask to the query, key, and value activations, and 2) for forecasting and anomaly detection, which rely on predicting outputs at a single point in time, we propose an attention mask to allow computation only at the current time step. Together, each compression technique and attention modification substantially reduces the number of non-zero operations necessary in the Transformer. We measure the computational savings of our approach over a range of metrics including parameter count, bit size, and floating point operation (FLOPs) count, showing up to a 53x reduction in storage size and up to 10.5x reduction in FLOPs.|压缩神经网络具有在新的应用程序和较小的计算环境中实现深度学习的潜力。然而，理解这些模型能够成功的学习任务的范围并没有得到很好的研究。将稀疏变压器和二进制加权变压器应用于多变量时间序列问题，结果表明，轻量化模型的精度可以与同一结构的稠密浮点变压器相媲美。我们的模型在三个时间序列学习任务中取得了良好的结果: 分类、异常检测和单步预测。此外，为了降低注意力机制的计算复杂性，我们应用了两个修改，这两个修改显示模型性能几乎没有下降: 1)在分类任务中，我们对查询、键和值激活应用了一个固定的掩码; 2)预测和异常检测，这依赖于预测单个时间点的输出，我们提出了一个注意掩码，只允许在当前的时间步骤进行计算。总之，每种压缩技术和注意力修改大大减少了变压器中必需的非零操作的数量。我们通过一系列指标(包括参数计数、位大小和浮点运算(FLOPs)计数)来衡量我们的方法的计算节省，显示出存储大小减少了53倍，浮点运算减少了10.5倍。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sparse+Binary+Transformers+for+Multivariate+Time+Series+Modeling)|0|
|[3D-Polishing for Triangular Mesh Compression of Point Cloud Data](https://doi.org/10.1145/3580305.3599239)|Jiaqi Gu, Guosheng Yin||Triangular meshes are commonly used to reconstruct the surfaces of 3-dimensional (3D) objects based on the point cloud data. With an increasing demand for high-quality approximation, the sizes of point cloud data and the generated triangular meshes continue to increase, resulting in high computational cost in data processing, visualization, analysis, transmission and storage. Motivated by the process of sculpture polishing, we develop a novel progressive mesh compression approach, called the greedy 3D-polishing algorithm, to sequentially remove redundant points and triangles in a greedy manner while maintaining the approximation quality of the surface. Based on the polishing algorithm, we propose the approximate curvature radius to evaluate the scale of features polished at each iteration. By reformulating the compression rate selection as a change-point detection problem, a rank-based procedure is developed to select the optimal compression rate so that most of the global features of the 3D object surface can be preserved with statistical guarantee. Experiments on both moderate- and large-scale 3D shape datasets show that the proposed method can substantially reduce the size of the point cloud data and the corresponding triangular mesh, so that most of the surface information can be retained by a much smaller number of points and triangles.|三角网格是基于点云数据重建三维物体表面的常用方法。随着人们对高质量逼近要求的不断提高，点云数据和生成的三角网格的大小不断增加，导致数据处理、可视化、分析、传输和存储的计算成本增加。受雕塑抛光过程的启发，我们开发了一种新的渐进网格压缩方法，称为贪婪3D 抛光算法，以贪婪的方式顺序去除冗余点和三角形，同时保持表面的逼近质量。基于抛光算法，我们提出了近似曲率半径来评价每次迭代抛光的特征尺度。通过将压缩率选择重构为一个变点检测问题，提出了一种基于秩的最优压缩率选择方法，使得三维物体表面的大部分全局特征能够在统计保证的情况下得到保留。在中大规模三维形状数据集上的实验表明，该方法可以大大减小点云数据和相应的三角形网格的尺寸，使得大部分的表面信息可以由少得多的点和三角形保留。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=3D-Polishing+for+Triangular+Mesh+Compression+of+Point+Cloud+Data)|0|
|[ESSA: Explanation Iterative Supervision via Saliency-guided Data Augmentation](https://doi.org/10.1145/3580305.3599336)|Siyi Gu, Yifei Zhang, Yuyang Gao, Xiaofeng Yang, Liang Zhao||Explanation supervision is a technique in which the model is guided by human-generated explanations during training. This technique aims to improve both the interpretability and predictability of the model by incorporating human understanding into the training process. Since explanation supervision requires a large scale of training data, the data augmentation technique is necessary to be applied to increase the size and diversity of the original dataset. However, data augmentation on sophisticated data like medical images is particularly challenging due to the following: 1) scarcity of data in training the learning-based data augmenter, 2) difficulty in generating realistic and sophisticated images, and 3) difficulty in ensuring the augmented data indeed boosts the performance of explanation-guided learning. To solve these challenges, we propose an Explanation Iterative Supervision via Saliency-guided Data Augmentation (ESSA) framework for conducting explanation supervision and adversarial-trained image data augmentation via a synergized iterative loop that handles the translation from annotation to sophisticated images and the generation of synthetic image-annotation pairs with an alternating training strategy. Extensive experiments on two datasets from the medical imaging domain demonstrate the effectiveness of our proposed framework in improving both the predictability and explainability of the model.|说明监督是一种在培训过程中以人工生成的说明为指导的模型技术。这种技术旨在通过将人的理解纳入培训过程来提高模型的可解释性和可预测性。由于解释监督需要大量的训练数据，因此需要应用数据增强技术来增加原始数据集的大小和多样性。然而，像医学图像这样的复杂数据的数据增强尤其具有挑战性，因为以下原因: 1)训练基于学习的数据增强器的数据稀缺，2)难以生成真实和复杂的图像，3)难以确保增强数据确实提高解释指导学习的性能。为了解决这些挑战，我们提出了一个通过显著性引导的数据增强(ESSA)框架的解释迭代监督，以通过协同迭代循环进行解释监督和对手训练的图像数据增强，该迭代循环处理从注释到复杂图像的翻译以及用交替训练策略生成合成图像注释对。对来自医学成像领域的两个数据集的大量实验证明了我们提出的框架在提高模型的可预测性和可解释性方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ESSA:+Explanation+Iterative+Supervision+via+Saliency-guided+Data+Augmentation)|0|
|[CounterNet: End-to-End Training of Prediction Aware Counterfactual Explanations](https://doi.org/10.1145/3580305.3599290)|Hangzhi Guo, Thanh Hong Nguyen, Amulya Yadav|University of Oregon; Pennsylvania State University|Counterfactual (or CF) explanations are a type of local explanations for Machine Learning (ML) model predictions, which offer a contrastive case as an explanation by finding the smallest changes (in feature space) to the input data point, which will lead to a different prediction by the ML model. Existing CF explanation techniques suffer from two major limitations: (i) all of them are post-hoc methods designed for use with proprietary ML models --- as a result, their procedure for generating CF explanations is uninformed by the training of the ML model, which leads to misalignment between model predictions and explanations; and (ii) most of them rely on solving separate time-intensive optimization problems to find CF explanations for each input data point (which negatively impacts their runtime). This work makes a novel departure from the prevalent post-hoc paradigm (of generating CF explanations) by presenting CounterNet, an end-to-end learning framework which integrates predictive model training and the generation of counterfactual (CF) explanations into a single pipeline. We adopt a block-wise coordinate descent procedure which helps in effectively training CounterNet's network. Our extensive experiments on multiple real-world datasets show that CounterNet generates high-quality predictions, and consistently achieves 100% CF validity and very low proximity scores (thereby achieving a well-balanced cost-invalidity trade-off) for any new input instance, and runs 3X faster than existing state-of-the-art baselines.|反事实(CF)解释是对机器学习(ML)模型预测的一种局部解释，它通过寻找输入数据点的最小变化(在特征空间)来提供一个对比的案例作为解释，这将导致机器学习模型的一个不同的预测。现有的 CF 解释技术有两个主要的局限性: (i)它们都是专门为使用专有机器学习模型而设计的事后方法——因此，它们生成 CF 解释的过程没有受到机器学习模型训练的影响，这导致模型预测和解释之间的不一致; (ii)它们中的大多数依赖于解决单独的时间密集型优化问题来为每个输入数据点找到 CF 解释(这对它们的运行时间有负面影响)。这项工作通过提出 CounterNet，一个将预测模型训练和反事实(CF)解释生成集成到一个单一管道的端到端学习框架，从流行的事后范式(生成 CF 解释)做出了新的背离。我们采用分组坐标下降法的程序，有助有效地训练 CounterNet 的网络。我们在多个真实世界数据集上的广泛实验表明，CounterNet 产生高质量的预测，并始终达到100% CF 有效性和非常低的接近得分(从而实现了良好的成本-无效性权衡) ，对于任何新的输入实例，运行速度比现有的最先进的基线快3倍。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CounterNet:+End-to-End+Training+of+Prediction+Aware+Counterfactual+Explanations)|0|
|[Clenshaw Graph Neural Networks](https://doi.org/10.1145/3580305.3599275)|Yuhe Guo, Zhewei Wei|Renmin University of China|Graph Convolutional Networks (GCNs), which use a message-passing paradigm with stacked convolution layers, are foundational methods for learning graph representations. Recent GCN models use various residual connection techniques to alleviate the model degradation problem such as over-smoothing and gradient vanishing. Existing residual connection techniques, however, fail to make extensive use of underlying graph structure as in the graph spectral domain, which is critical for obtaining satisfactory results on heterophilic graphs. In this paper, we introduce ClenshawGCN, a GNN model that employs the Clenshaw Summation Algorithm to enhance the expressiveness of the GCN model. ClenshawGCN equips the standard GCN model with two straightforward residual modules: the adaptive initial residual connection and the negative second-order residual connection. We show that by adding these two residual modules, ClenshawGCN implicitly simulates a polynomial filter under the Chebyshev basis, giving it at least as much expressive power as polynomial spectral GNNs. In addition, we conduct comprehensive experiments to demonstrate the superiority of our model over spatial and spectral GNN models.|图卷积网络(GCNs)是学习图表示的基本方法，它使用一种带有层叠卷积层的消息传递范式。最近的 GCN 模型使用各种残差连接技术来缓解模型退化问题，如过度平滑和梯度消失。然而，现有的残差连接技术不能像在图谱域中那样广泛地利用底层图结构，这对于获得满意的异质图结果是至关重要的。在本文中，我们介绍了 Clenshaw GCN，一个使用 Clenshaw 求和算法来增强 GCN 模型表达能力的 GNN 模型。标准 GCN 模型具有两个简单的残差模块: 自适应初始残差连接和负二阶残差连接。通过添加这两个残差模块，ClenshawGCN 隐式地模拟了 Chebyshev 基下的多项式滤波器，使其至少具有与多项式谱 GNN 相同的表达能力。此外，我们进行了全面的实验，以证明我们的模型优于空间和光谱 GNN 模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Clenshaw+Graph+Neural+Networks)|0|
|[CampER: An Effective Framework for Privacy-Aware Deep Entity Resolution](https://doi.org/10.1145/3580305.3599266)|Yuxiang Guo, Lu Chen, Zhengjie Zhou, Baihua Zheng, Ziquan Fang, Zhikun Zhang, Yuren Mao, Yunjun Gao||Entity Resolution (ER) is a fundamental problem in data preparation. Standard deep ER methods have achieved state-of-the-art effectiveness, assuming that relations from different organizations are centrally stored. However, due to privacy concerns, it can be difficult to centralize data in practice, rendering standard deep ER solutions inapplicable. Despite efforts to develop rule-based privacy-preserving ER methods, they often neglect subtle matching mechanisms and have poor effectiveness as a result. To bridge effectiveness and privacy, in this paper, we propose CampER, an effective framework for privacy-aware deep entity resolution. Specifically, we first design a training pair self-generation strategy to overcome the absence of manually labeled data in privacy-aware scenarios. Based on the self-constructed training pairs, we present a collaborative fine-tuning approach to learn the match-aware and uni-space individual tuple embeddings for accurate matching decisions. During the matching decision-making process, we first introduce a cryptographically secure approach to determine matches. Furthermore, we propose an order-preserving perturbation strategy to significantly accelerate the matching computation while guaranteeing the consistency of ER results. Extensive experiments on eight widely-used benchmark datasets demonstrate that CampER not only is comparable with the state-of-the-art standard deep ER solutions in effectiveness, but also preserves privacy.|实体解析(ER)是数据准备中的一个基本问题。假设集中存储来自不同组织的关系，标准的深度 ER 方法已经取得了最先进的效果。然而，由于隐私问题，在实践中很难集中数据，使得标准的深层 ER 解决方案不适用。尽管努力开发基于规则的隐私保护 ER 方法，但它们往往忽视了微妙的匹配机制，结果效果不佳。为了在有效性和隐私性之间建立桥梁，本文提出了一种有效的隐私感知深层实体解析框架 CampER。具体来说，我们首先设计了一个训练对自生成策略，以克服在隐私感知场景中缺乏手动标记数据的问题。基于自构造的训练对，我们提出了一种协同微调的方法来学习匹配感知和单空间个体元组嵌入以获得准确的匹配决策。在匹配决策过程中，我们首先引入一种加密安全的方法来确定匹配。此外，我们提出了一种保序摄动策略，以显著加快匹配计算，同时保证 ER 结果的一致性。对八个广泛使用的基准数据集进行的大量实验表明，CampER 不仅在有效性方面可以与最先进的标准深度 ER 解决方案相媲美，而且还保护了隐私。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CampER:+An+Effective+Framework+for+Privacy-Aware+Deep+Entity+Resolution)|0|
|[A Data-centric Framework to Endow Graph Neural Networks with Out-Of-Distribution Detection Ability](https://doi.org/10.1145/3580305.3599244)|Yuxin Guo, Cheng Yang, Yuluo Chen, Jixi Liu, Chuan Shi, Junping Du||Out-of-distribution (OOD) detection, which aims to identify OOD samples from in-distribution (ID) ones in test time, has become an essential problem in machine learning. However, existing works are mostly conducted on Euclidean data, and the problem in graph-structured data remains under-explored. Several recent works begin to study graph OOD detection, but they all need to train a graph neural network (GNN) from scratch with high computational cost. In this work, we make the first attempt to endow a well-trained GNN with the OOD detection ability without modifying its parameters. To this end, we design a post-hoc framework with Adaptive Amplifier for Graph OOD Detection, named AAGOD, concentrating on data-centric manipulation. The insight of AAGOD is to superimpose a parameterized amplifier matrix on the adjacency matrix of each original input graph. The amplifier can be seen as prompts and is expected to emphasize the key patterns helpful for graph OOD detection, thereby enlarging the gap between OOD and ID graphs. Then well-trained GNNs can be reused to encode the amplified graphs into vector representations, and pre-defined scoring functions can further convert the representations into detection scores. Specifically, we design a Learnable Amplifier Generator (LAG) to customize amplifiers for different graphs, and propose a Regularized Learning Strategy (RLS) to train parameters with no OOD data required. Experiment results show that AAGOD can be applied on various GNNs to enable the OOD detection ability. Compared with the state-of-the-art baseline in graph OOD detection, on average AAGOD has 6.21% relative enhancement in AUC and a 34 times faster training speed. Code and data are available at https://github.com/BUPT-GAMMA/AAGOD.|分布外(OOD)检测是机器学习中的一个重要问题，其目的是在测试时间内从分布内(ID)样本中识别出 OOD 样本。然而，现有的工作大多是在欧几里德数据上进行的，图结构化数据的问题仍然没有得到充分的研究。最近的一些工作开始研究图形 OOD 检测，但都需要从头开始训练一个计算量大的图形神经网络(GNN)。本文首次尝试在不改变参数的情况下，赋予训练有素的 GNN OOD 检测能力。为此，我们设计了一个基于自适应放大器的图形面向对象检测框架 AAGOD，主要用于以数据为中心的操作。AAGOD 的见解是在每个原始输入图的邻接矩阵上叠加一个参数化的放大器矩阵。该放大器可以作为一种提示器，用于强调有助于图形 OOD 检测的关键模式，从而扩大了 OOD 和 ID 图之间的差距。然后，训练有素的 GNN 可以被重用来将放大后的图编码成矢量表示，并且预定义的评分函数可以进一步将表示转换成检测分数。具体来说，我们设计了一个可学习放大器生成器(LAG)来为不同的图定制放大器，并提出了一个正则化学习策略(RLS)来训练不需要 OOD 数据的参数。实验结果表明，AAGOD 可以应用于各种 GNN，使 OOD 检测能力。与目前最先进的图形 OOD 检测基线相比，AAGOD 在 AUC 中平均有6.21% 的相对增强，训练速度提高了34倍。代码和数据可在 https://github.com/bupt-gamma/aagod 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Data-centric+Framework+to+Endow+Graph+Neural+Networks+with+Out-Of-Distribution+Detection+Ability)|0|
|[Frigate: Frugal Spatio-temporal Forecasting on Road Networks](https://doi.org/10.1145/3580305.3599357)|Mridul Gupta, Hariprasad Kodamana, Sayan Ranu|Indian Institute of Technology Delhi|Modelling spatio-temporal processes on road networks is a task of growing importance. While significant progress has been made on developing spatio-temporal graph neural networks (Gnns), existing works are built upon three assumptions that are not practical on real-world road networks. First, they assume sensing on every node of a road network. In reality, due to budget-constraints or sensor failures, all locations (nodes) may not be equipped with sensors. Second, they assume that sensing history is available at all installed sensors. This is unrealistic as well due to sensor failures, loss of packets during communication, etc. Finally, there is an assumption of static road networks. Connectivity within networks change due to road closures, constructions of new roads, etc. In this work, we develop FRIGATE to address all these shortcomings. FRIGATE is powered by a spatio-temporal Gnn that integrates positional, topological, and temporal information into rich inductive node representations. The joint fusion of this diverse information is made feasible through a novel combination of gated Lipschitz embeddings with Lstms. We prove that the proposed Gnn architecture is provably more expressive than message-passing Gnns used in state-of-the-art algorithms. The higher expressivity of FRIGATE naturally translates to superior empirical performance conducted on real-world network-constrained traffic data. In addition, FRIGATE is robust to frugal sensor deployment, changes in road network connectivity, and temporal irregularity in sensing.|道路网络时空过程建模是一项日益重要的任务。虽然时空图形神经网络(Gnns)的开发已经取得了重大进展，但现有的工作是建立在三个假设之上的，这些假设在现实世界的道路网络中是不实际的。首先，它们假设对道路网络的每个节点进行检测。实际上，由于预算限制或传感器故障，所有位置(节点)可能不配备传感器。其次，他们假设所有安装的传感器都具有传感历史。由于传感器故障、数据包在通信过程中丢失等原因，这也是不现实的。最后，提出了静态路网的假设。由于道路封闭、新建道路等原因，网络内的连通性会发生变化。在这项工作中，我们开发 FRIGATE 来解决所有这些缺点。FRIGATE 由一个时空 Gnn 驱动，该 Gnn 将位置、拓扑和时间信息集成到丰富的归纳节点表示中。通过门控 Lipschitz 嵌入与 Lstms 的新颖结合，使得这种多样化信息的联合融合成为可能。我们证明了所提出的 Gnn 结构比最先进的算法中使用的消息传递 Gnn 具有更好的表达能力。FRIGATE 更高的表达能力自然地转化为在现实世界网络约束的流量数据上进行的更好的经验性能。此外，FRIGATE 对节约的传感器部署、道路网络连通性的变化和传感的时间不规则性具有鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Frigate:+Frugal+Spatio-temporal+Forecasting+on+Road+Networks)|0|
|[Mitigating Action Hysteresis in Traffic Signal Control with Traffic Predictive Reinforcement Learning](https://doi.org/10.1145/3580305.3599528)|Xiao Han, Xiangyu Zhao, Liang Zhang, Wanyu Wang||Traffic signal control plays a pivotal role in the management of urban traffic flow. With the rapid advancement of reinforcement learning, the development of signal control methods has seen a significant boost. However, a major challenge in implementing these methods is ensuring that signal lights do not change abruptly, as this can lead to traffic accidents. To mitigate this risk, a time-delay is introduced in the implementation of control actions, but usually has a negative impact on the overall efficacy of the control policy. To address this challenge, this paper presents a novel Traffic Signal Control Framework (PRLight), which leverages an On-policy Traffic Control Model (OTCM) and an Online Traffic Prediction Model (OTPM) to achieve efficient and real-time control of traffic signals. The framework collects multi-source traffic information from a local-view graph in real-time and employs a novel fast attention mechanism to extract relevant traffic features. To be specific, OTCM utilizes the predicted traffic state as input, eliminating the need for communication with other agents and maximizing computational efficiency while ensuring that the most relevant information is used for signal control. The proposed framework was evaluated on both simulated and real-world road networks and compared to various state-of-the-art methods, demonstrating its effectiveness in preventing traffic congestion and accidents.|交通信号控制在城市交通流管理中起着举足轻重的作用。随着强化学习的迅速发展，信号控制方法的发展也得到极大的推动。然而，实施这些方法的一个主要挑战是确保信号灯不会突然变化，因为这可能导致交通事故。为了降低这种风险，在实施控制行动时引入了时间延迟，但通常会对控制策略的总体效率产生负面影响。为了解决这一问题，本文提出了一种新的交通信号控制框架(PRLight) ，该框架利用在线交通控制模型(OTCM)和在线交通预测模型(OTPM)来实现对交通信号的高效实时控制。该框架实时从局部视图中收集多源交通信息，并采用一种新的快速注意机制提取相关的交通特征。具体来说，OTCM 利用预测的流量状态作为输入，消除了与其他代理通信的需要，最大限度地提高了计算效率，同时确保最相关的信息被用于信号控制。建议的架构在模拟和现实世界的道路网络上进行了评估，并与各种最先进的方法进行了比较，显示了其在预防交通堵塞和事故方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mitigating+Action+Hysteresis+in+Traffic+Signal+Control+with+Traffic+Predictive+Reinforcement+Learning)|0|
|[GAT-MF: Graph Attention Mean Field for Very Large Scale Multi-Agent Reinforcement Learning](https://doi.org/10.1145/3580305.3599359)|Qianyue Hao, Wenzhen Huang, Tao Feng, Jian Yuan, Yong Li||Recent advancements in reinforcement learning have witnessed remarkable achievements by intelligent agents ranging from game-playing to industrial applications. Of particular interest is the area of multi-agent reinforcement learning (MARL), which holds significant potential for real-world scenarios. However, typical MARL methods are limited in their ability to handle tens of agents, leaving scenarios with up to hundreds or even thousands of agents almost unexplored. The scaling up of the number of agents presents two primary challenges: (1) agent-agent interactions are crucial in multi-agent systems while the number of interactions grows quadratically with the number of agents, resulting in substantial computational complexity and difficulty in strategies-learning; (2) the strengths of interactions among agents exhibit variations both across agents and over time, making it difficult to precisely model such interactions. In this paper, we propose a novel approach named Graph Attention Mean Field (GAT-MF). By converting agent-agent interactions into interactions between each agent and a weighted mean field, we achieve a substantial reduction in computational complexity. The proposed method offers a precise modeling of interaction dynamics with mathematical proofs of its correctness. Additionally, we design a graph attention mechanism to automatically capture the diverse and time-varying strengths of interactions, ensuring an accurate representation of agent interactions. Through extensive experimentation conducted in both manual and real-world scenarios involving over 3000 agents, we validate the efficacy of our method. The results demonstrate that our method outperforms the best baseline method with a remarkable improvement of 42.7%. Furthermore, our method saves 86.4% training time and 19.2% GPU memory compared to the best baseline method. For reproducibility, our source codes and data are available at https://github.com/tsinghua-fib-lab/Large-Scale-MARL-GATMF.|从游戏到工业应用，智能代理在强化学习方面的最新进展已经取得了显著的成就。特别令人感兴趣的是多代理强化学习领域，它在现实世界中具有巨大的潜力。然而，典型的 MARL 方法在处理数十个代理的能力上是有限的，这使得数百个甚至数千个代理的场景几乎没有被探索。智能体数量的增加提出了两个主要挑战: (1)智能体-智能体相互作用在多智能体系统中是至关重要的，而相互作用的数量随着智能体数量的二次增长而增长，导致大量的计算复杂性和策略学习的困难; (2)智能体之间相互作用的强度在智能体之间和随着时间的推移都表现出变化，使得精确模拟这种相互作用变。在本文中，我们提出了一种新的方法称为图注意力平均场(GAT-MF)。通过将代理-代理之间的相互作用转换为每个代理和一个加权平均场之间的相互作用，我们实现了计算复杂度的大幅度降低。该方法提供了一个精确的交互动力学建模及其正确性的数学证明。此外，我们还设计了一个图形注意机制来自动捕捉交互的不同和时变的优势，确保代理交互的准确表示。通过在涉及3000多个代理的手工和现实场景中进行的大量实验，我们验证了该方法的有效性。实验结果表明，该方法的性能优于最佳基线方法，显著提高了42.7% 。此外，与最佳基线方法相比，该方法节省了86.4% 的训练时间和19.2% 的 GPU 内存。为了可重复性，我们的源代码和数据可在 https://github.com/tsinghua-fib-lab/large-scale-marl-gatmf 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GAT-MF:+Graph+Attention+Mean+Field+for+Very+Large+Scale+Multi-Agent+Reinforcement+Learning)|0|
|[Prescriptive PCA: Dimensionality Reduction for Two-stage Stochastic Optimization](https://doi.org/10.1145/3580305.3599474)|Long He, HoYin Mak|Georgetown University; George Washington University|In this paper, we consider the alignment between an upstream dimensionality reduction task of learning a low-dimensional representation of a set of high-dimensional data and a downstream optimization task of solving a stochastic program parameterized by said representation. In this case, standard dimensionality reduction methods (e.g., principal component analysis) may not perform well, as they aim to maximize the amount of information retained in the representation and do not generally reflect the importance of such information in the downstream optimization problem. To address this problem, we develop a prescriptive dimensionality reduction framework that aims to minimize the degree of suboptimality in the optimization phase. For the case where the downstream stochastic optimization problem has an expected value objective, we show that prescriptive dimensionality reduction can be performed via solving a distributionally-robust optimization problem, which admits a semidefinite programming relaxation. Computational experiments based on a warehouse transshipment problem and a vehicle repositioning problem show that our approach significantly outperforms principal component analysis with real and synthetic data sets.|在这篇文章中，我们考虑了上游的降维任务(学习一组高维数据的低维表示)和下游的优化任务(解决一个由该表示参数化的随机程序)之间的一致性。在这种情况下，标准的降维方法(例如主成分分析)可能不会有很好的效果，因为它们的目标是最大限度地提高表示中所保留的信息的数量，而且通常不能反映这些信息在下游最佳化问题中的重要性。为了解决这个问题，我们开发了一个规范的降维框架，旨在最小化优化阶段的次优化程度。对于下游随机最佳化问题具有期望值目标的情况，我们证明了规范降维可以通过求解一个允许半定规划松弛的分布鲁棒最佳化问题来实现。基于一个仓库转运问题和一个车辆重新定位问题的计算实验表明，我们的方法显著优于真实和合成数据集的主成分分析。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Prescriptive+PCA:+Dimensionality+Reduction+for+Two-stage+Stochastic+Optimization)|0|
|[Graph Neural Processes for Spatio-Temporal Extrapolation](https://doi.org/10.1145/3580305.3599372)|Junfeng Hu, Yuxuan Liang, Zhencheng Fan, Hongyang Chen, Yu Zheng, Roger Zimmermann|Hong Kong University of Science and Technology (Guangzhou); JD Intelligent Cities Research; University of Technology Sydney; Zhejiang Lab; National University of Singapore|We study the task of spatio-temporal extrapolation that generates data at target locations from surrounding contexts in a graph. This task is crucial as sensors that collect data are sparsely deployed, resulting in a lack of fine-grained information due to high deployment and maintenance costs. Existing methods either use learning-based models like Neural Networks or statistical approaches like Gaussian Processes for this task. However, the former lacks uncertainty estimates and the latter fails to capture complex spatial and temporal correlations effectively. To address these issues, we propose Spatio-Temporal Graph Neural Processes (STGNP), a neural latent variable model which commands these capabilities simultaneously. Specifically, we first learn deterministic spatio-temporal representations by stacking layers of causal convolutions and cross-set graph neural networks. Then, we learn latent variables for target locations through vertical latent state transitions along layers and obtain extrapolations. Importantly during the transitions, we propose Graph Bayesian Aggregation (GBA), a Bayesian graph aggregator that aggregates contexts considering uncertainties in context data and graph structure. Extensive experiments show that STGNP has desirable properties such as uncertainty estimates and strong learning capabilities, and achieves state-of-the-art results by a clear margin.|我们研究的任务时空外推生成数据在目标位置从周围环境在一个图。这项任务至关重要，因为收集数据的传感器部署得很少，由于部署和维护成本高昂，导致缺乏细粒度信息。现有的方法要么使用像神经网络这样的基于学习的模型，要么使用像高斯过程这样的统计方法来完成这项任务。然而，前者缺乏不确定性估计，后者未能有效捕捉复杂的时空相关性。为了解决这些问题，我们提出了时空图形神经过程(STGNP) ，一个神经潜变量模型，它同时命令这些能力。具体来说，我们首先通过叠加因果卷积层和交集图神经网络来学习确定性时空表示。然后，通过层间的垂直潜状态转换来学习目标位置的潜变量，并得到外推结果。重要的是，在转换过程中，我们提出了图贝叶斯聚集(GBA) ，一个贝叶斯图聚集器，聚集上下文考虑不确定性的上下文数据和图结构。广泛的实验表明，STGNP 具有不确定性估计和强大的学习能力等优良特性，并以明显的优势取得了最先进的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Neural+Processes+for+Spatio-Temporal+Extrapolation)|0|
|[ST-iFGSM: Enhancing Robustness of Human Mobility Signature Identification Model via Spatial-Temporal Iterative FGSM](https://doi.org/10.1145/3580305.3599513)|Mingzhi Hu, Xin Zhang, Yanhua Li, Xun Zhou, Jun Luo||The Human Mobility Signature Identification (HuMID) problem aims at determining whether the incoming trajectories were generated by a claimed agent from the historical movement trajectories of a set of individual human agents such as pedestrians and taxi drivers. The HuMID problem is significant, and its solutions have a wide range of real-world applications, such as criminal identification for police departments, risk assessment for auto insurance providers, driver verification in ride-sharing services, and so on. Though Deep neural networks (DNN) based HuMID models on spatial-temporal mobility fingerprint similarity demonstrate remarkable performance in effectively identifying human agents' mobility signatures, it is vulnerable to adversarial attacks as other DNN-based models. Therefore, in this paper, we propose a Spatial-Temporal iterative Fast Gradient Sign Method with L 0 regularization - ST-iFGSM - to detect the vulnerability and enhance the robustness of HuMID models. Extensive experiments with real-world taxi trajectory data demonstrate the efficiency and effectiveness of our ST-iFGSM algorithm. We tested our method on both the ST-SiameseNet and an LSTM-based HuMID classification model. It shows that ST-iFGSM can generate successful attacks to fool the HuMID models with only a few steps of attack in a small portion of the trajectories. The generated attacks can be used as augmented data to update and improve the HuMID model accuracy significantly from 47.36% to 76.18% on testing samples after the attack(86.25% on the original testing samples).|人类移动特征识别(HuMID)问题旨在确定进入的轨迹是否由声称的代理从一组个体人类代理(如行人和出租车司机)的历史运动轨迹生成。HuMID 问题意义重大，其解决方案在现实世界中有着广泛的应用，如警察部门的刑事鉴定、汽车保险提供商的风险评估、拼车服务中的司机验证等。基于深度神经网络(DNN)的时空移动性指纹相似度 HuMID 模型在有效识别人类智能体的移动性特征方面表现出显著的性能，但与其他基于 DNN 的模型一样，该模型易受敌对攻击。为此，本文提出了一种基于 L0正则化的时空迭代快速梯度符号方法-ST-iFGSM，用于检测 HuMID 模型的漏洞并增强其鲁棒性。通过对实际出租车轨迹数据的大量实验，验证了 ST-iFGSM 算法的有效性。我们在 ST-SiameseNet 和一个基于 LSTM 的 HuMID 分类模型上测试了我们的方法。结果表明，ST-iFGSM 只需在一小部分轨迹上进行几个步骤的攻击，就可以成功地对 HuMID 模型进行攻击。生成的攻击可以作为增强数据用于更新和提高 HuMID 模型的准确性，攻击后测试样本的准确率从47.36% 显著提高到76.18% (原始测试样本的准确率为86.25%)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ST-iFGSM:+Enhancing+Robustness+of+Human+Mobility+Signature+Identification+Model+via+Spatial-Temporal+Iterative+FGSM)|0|
|[Leveraging Relational Graph Neural Network for Transductive Model Ensemble](https://doi.org/10.1145/3580305.3599414)|Zhengyu Hu, Jieyu Zhang, Haonan Wang, Siwei Liu, Shangsong Liang||Traditional methods of pre-training, fine-tuning, and ensembling often overlook essential relational data and task interconnections. To address this gap, our study presents a novel approach to harnessing this relational information via a relational graph-based model. We introduce Relational grAph Model ensemBLE model, abbreviated as RAMBLE. This model distinguishes itself by performing class label inference simultaneously across all data nodes and task nodes, employing the relational graph in a transductive manner. This fine-grained approach allows us to better comprehend and model the intricate interplay between data and tasks. Furthermore, we incorporate a novel variational information bottleneck-guided scheme for embedding fusion and aggregation. This innovative technique facilitates the creation of an informative fusion embedding, honing in on embeddings beneficial for the intended task while simultaneously filtering out potential noise-laden embeddings. Our theoretical analysis, grounded in information theory, confirms that the use of relational information for embedding fusion allows us to achieve higher upper and lower bounds on our target task's accuracy. We thoroughly assess our proposed model across eight diverse datasets, and the experimental results demonstrate the model's effective utilization of relational knowledge derived from all pre-trained models, thereby enhancing its performance on our target tasks.|传统的预训练、微调和集成方法常常忽略了基本的关系数据和任务互连。为了解决这一差距，我们的研究提出了一种新的方法来利用这一关系信息通过一个关系图为基础的模型。我们介绍了关系图模型集成模型，简称 RAMBLE。该模型通过在所有数据节点和任务节点之间同时执行类标签推理，以传导的方式使用关系图来区分自身。这种细粒度的方法使我们能够更好地理解和建模数据和任务之间错综复杂的相互作用。在此基础上，提出了一种新的变分信息瓶颈引导的嵌入式融合和聚合方案。这种创新技术有助于创建信息融合嵌入，在过滤掉潜在噪声的同时，对有利于预期任务的嵌入进行磨练。基于信息论的理论分析表明，利用关系信息进行嵌入融合，可以提高目标任务的精度上下限。我们在八个不同的数据集上全面评估了我们提出的模型，实验结果表明该模型有效地利用了来自所有预先训练的模型的关系知识，从而提高了它在目标任务上的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leveraging+Relational+Graph+Neural+Network+for+Transductive+Model+Ensemble)|0|
|[One for All: Unified Workload Prediction for Dynamic Multi-tenant Edge Cloud Platforms](https://doi.org/10.1145/3580305.3599453)|Shaoyuan Huang, Zheng Wang, Heng Zhang, Xiaofei Wang, Cheng Zhang, Wenyu Wang|Tianjin University of Finance; Tianjin University; Paiou Cloud Computing (Shanghai) Co., Ltd|Workload prediction in multi-tenant edge cloud platforms (MT-ECP) is vital for efficient application deployment and resource provisioning. However, the heterogeneous application patterns, variable infrastructure performance, and frequent deployments in MT-ECP pose significant challenges for accurate and efficient workload prediction. Clustering-based methods for dynamic MT-ECP modeling often incur excessive costs due to the need to maintain numerous data clusters and models, which leads to excessive costs. Existing end-to-end time series prediction methods are challenging to provide consistent prediction performance in dynamic MT-ECP. In this paper, we propose an end-to-end framework with global pooling and static content awareness, DynEformer, to provide a unified workload prediction scheme for dynamic MT-ECP. Meticulously designed global pooling and information merging mechanisms can effectively identify and utilize global application patterns to drive local workload predictions. The integration of static content-aware mechanisms enhances model robustness in real-world scenarios. Through experiments on five real-world datasets, DynEformer achieved state-of-the-art in the dynamic scene of MT-ECP and provided a unified end-to-end prediction scheme for MT-ECP.|多租户边缘云平台(MT-ECP)中的工作负载预测对于有效的应用程序部署和资源配置至关重要。然而，异构的应用程序模式、可变的基础设施性能以及 MT-ECP 中的频繁部署对准确有效的工作负载预测提出了严峻的挑战。基于聚类的动态 MT-ECP 建模方法由于需要维护大量的数据集群和模型，往往会产生过高的成本。现有的端到端时间序列预测方法难以在动态 MT-ECP 中提供一致的预测性能。本文提出了一个具有全局池和静态内容感知的端到端框架 DynEformer，为动态 MT-ECP 提供一个统一的工作负载预测方案。精心设计的全局池和信息合并机制可以有效地识别和利用全局应用程序模式来驱动本地工作负载预测。静态内容感知机制的集成增强了真实场景中模型的健壮性。通过对五个实际数据集的实验，DynEformer 实现了 MT-ECP 动态场景中的最新技术，为 MT-ECP 提供了一个统一的端到端预测方案。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=One+for+All:+Unified+Workload+Prediction+for+Dynamic+Multi-tenant+Edge+Cloud+Platforms)|0|
|[Generalizing Graph ODE for Learning Complex System Dynamics across Environments](https://doi.org/10.1145/3580305.3599362)|Zijie Huang, Yizhou Sun, Wei Wang|University of California, Los Angeles|Learning multi-agent system dynamics has been extensively studied for various real-world applications, such as molecular dynamics in biology. Most of the existing models are built to learn single system dynamics from observed historical data and predict the future trajectory. In practice, however, we might observe multiple systems that are generated across different environments, which differ in latent exogenous factors such as temperature and gravity. One simple solution is to learn multiple environment-specific models, but it fails to exploit the potential commonalities among the dynamics across environments and offers poor prediction results where per-environment data is sparse or limited. Here, we present GG-ODE (Generalized Graph Ordinary Differential Equations), a machine learning framework for learning continuous multi-agent system dynamics across environments. Our model learns system dynamics using neural ordinary differential equations (ODE) parameterized by Graph Neural Networks (GNNs) to capture the continuous interaction among agents. We achieve the model generalization by assuming the dynamics across different environments are governed by common physics laws that can be captured via learning a shared ODE function. The distinct latent exogenous factors learned for each environment are incorporated into the ODE function to account for their differences. To improve model performance, we additionally design two regularization losses to (1) enforce the orthogonality between the learned initial states and exogenous factors via mutual information minimization; and (2) reduce the temporal variance of learned exogenous factors within the same system via contrastive learning. Experiments over various physical simulations show that our model can accurately predict system dynamics, especially in the long range, and can generalize well to new systems with few observations.|学习多智能体系统动力学已经被广泛研究用于各种现实世界的应用，例如生物学中的分子动力学。现有的大多数模型都是从观测的历史数据中学习单个系统的动力学，并预测未来的轨迹。然而，在实践中，我们可能会观察到在不同环境中产生的多个系统，这些系统在温度和重力等潜在的外部因素上有所不同。一个简单的解决方案是学习多个特定于环境的模型，但是它无法利用跨环境动态之间的潜在共性，并且在每个环境的数据稀少或有限的情况下提供较差的预测结果。在这里，我们介绍了广义图常微分方程(gg-ODE) ，一个机器学习框架，用于学习跨环境的连续多智能体系统动力学。该模型利用图神经网络(GNN)参数化的神经常微分方程(ODE)来学习系统动力学，以捕捉智能体之间的连续相互作用。我们通过假设不同环境中的动力学是由可以通过学习一个共享的 ODE 函数来捕获的公共物理定律控制的，从而实现了模型的泛化。每个环境所学到的不同的潜在外部因素被纳入 ODE 功能，以解释它们之间的差异。为了提高模型的性能，我们另外设计了两个正则化损失: (1)通过相互信息最小化增强学习初始状态与外生因素之间的正交性; (2)通过对比学习减少同一系统中学习外生因素的时间方差。通过各种物理仿真实验表明，该模型能够准确地预测系统动力学，特别是在长期范围内，并且能够很好地推广到观测较少的新系统。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generalizing+Graph+ODE+for+Learning+Complex+System+Dynamics+across+Environments)|0|
|[The Information Pathways Hypothesis: Transformers are Dynamic Self-Ensembles](https://doi.org/10.1145/3580305.3599520)|Md. Shamim Hussain, Mohammed J. Zaki, Dharmashankar Subramanian|International Business Machines; Rensselaer Polytechnic Institute|Transformers use the dense self-attention mechanism which gives a lot of flexibility for long-range connectivity. Over multiple layers of a deep transformer, the number of possible connectivity patterns increases exponentially. However, very few of these contribute to the performance of the network, and even fewer are essential. We hypothesize that there are sparsely connected sub-networks within a transformer, called information pathways which can be trained independently. However, the dynamic (i.e., input-dependent) nature of these pathways makes it difficult to prune dense self-attention during training. But the overall distribution of these pathways is often predictable. We take advantage of this fact to propose Stochastically Subsampled self-Attention (SSA) - a general-purpose training strategy for transformers that can reduce both the memory and computational cost of self-attention by 4 to 8 times during training while also serving as a regularization method - improving generalization over dense training. We show that an ensemble of sub-models can be formed from the subsampled pathways within a network, which can achieve better performance than its densely attended counterpart. We perform experiments on a variety of NLP, computer vision and graph learning tasks in both generative and discriminative settings to provide empirical evidence for our claims and show the effectiveness of the proposed method.|变压器采用密集的自我注意机制，为长距离连接提供了很大的灵活性。在深度变压器的多层上，可能的连接模式的数量呈指数增长。然而，其中很少有对网络性能有贡献的，甚至更少是必不可少的。我们假设在一个变压器内部存在稀疏连接的子网络，称为信息路径，可以独立地进行训练。然而，这些通路的动态(即依赖输入)特性使得在训练期间很难修剪密集的自我注意力。但是这些通路的总体分布通常是可以预测的。我们利用这一事实提出了随机次采样自我注意(SSA)——一种通用的变压器训练策略，它可以在训练过程中将自我注意的记忆和计算成本降低4 ~ 8倍，同时也作为一种正则化方法——提高了密集训练的泛化能力。我们证明了子模型的集合可以从网络中的子采样路径形成，它可以达到比其密集参与的对应物更好的性能。我们在生成性和区分性环境下进行各种自然语言处理、计算机视觉和图形学习任务的实验，为我们的声称提供经验证明，并显示建议方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Information+Pathways+Hypothesis:+Transformers+are+Dynamic+Self-Ensembles)|0|
|[Generative Perturbation Analysis for Probabilistic Black-Box Anomaly Attribution](https://doi.org/10.1145/3580305.3599365)|Tsuyoshi Idé, Naoki Abe|IBM Research, Thomas J. Watson Research Center|We address the task of probabilistic anomaly attribution in the black-box regression setting, where the goal is to compute the probability distribution of the attribution score of each input variable, given an observed anomaly. The training dataset is assumed to be unavailable. This task differs from the standard XAI (explainable AI) scenario, since we wish to explain the anomalous deviation from a black-box prediction rather than the black-box model itself. We begin by showing that mainstream model-agnostic explanation methods, such as the Shapley values, are not suitable for this task because of their ``deviation-agnostic property.'' We then propose a novel framework for probabilistic anomaly attribution that allows us to not only compute attribution scores as the predictive mean but also quantify the uncertainty of those scores. This is done by considering a generative process for perturbations that counter-factually bring the observed anomalous observation back to normalcy. We introduce a variational Bayes algorithm for deriving the distributions of per variable attribution scores. To the best of our knowledge, this is the first probabilistic anomaly attribution framework that is free from being deviation-agnostic.|我们在黑盒回归设置中处理概率异常归因的任务，其目标是计算每个输入变量的归因得分的概率分布，给定一个观察到的异常。假定训练数据集不可用。这个任务不同于标准的 XAI (可解释的 AI)场景，因为我们希望解释与黑盒预测的异常偏差，而不是黑盒模型本身。我们首先展示了主流的模型无关解释方法，例如 Shapley 值，由于它们的“偏差无关性”，不适合这个任务然后，我们提出了一个新的概率异常归因框架，使我们不仅计算归因分数作为预测平均值，而且量化这些分数的不确定性。这是通过考虑扰动的生成过程来完成的，扰动反事实地将观测到的反常观测恢复到正常状态。我们引入了一个变分贝叶斯算法来推导每个变量属性得分的分布。据我们所知，这是第一个没有偏差不可知的概率异常归因框架。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+Perturbation+Analysis+for+Probabilistic+Black-Box+Anomaly+Attribution)|0|
|[Parameter-free Spikelet: Discovering Different Length and Warped Time Series Motifs using an Adaptive Time Series Representation](https://doi.org/10.1145/3580305.3599310)|Makoto Imamura, Takaaki Nakamura||Over the last two decades, time series motif discovery has emerged as a useful primitive for many downstream analytical tasks, including clustering, classification, rule discovery, segmentation, and summarization. In parallel, it has long been known that Dynamic Time Warping (DTW) is superior to other similarity measures such as Euclidean Distance under most settings. Recently an algorithm to allow scalable DTW motif discovery was proposed; however, it is limited to finding pairs of subsequences whose subsequence lengths are the same. Moreover, that length must be provided by the user ahead of time. In this work, we propose a novel method to discover "warped" motifs whose lengths may differ. Moreover, our method allows input parameters that are not fixed lengths but rather just bounds on the maximum length of motifs to find. This allows us to quickly find different-length motifs without the burdensome trial-and-error of conventional methods. With extensive empirical work, we show that our method is scalable enough for real-world datasets and enables us to find variable-length and "warped" motifs that would otherwise escape the attention of conventional algorithms.|在过去的二十年中，时间序列基序发现已经成为许多下游分析任务的有用原始，包括聚类、分类、规则发现、分割和摘要。与此同时，动态时间规整(dtW)在大多数情况下都优于其他相似性指标，比如欧几里得度量。最近提出了一种允许可伸缩 DTW 基序发现的算法，但该算法仅限于发现子序列长度相同的子序列对。此外，该长度必须由用户提前提供。在这项工作中，我们提出了一个新的方法来发现“扭曲”图案的长度可能不同。此外，我们的方法允许输入参数不是固定的长度，而只是边界上的最大长度的主题找到。这使我们能够快速地找到不同长度的图案，而不必像传统方法那样反复试错。通过大量的实验工作，我们证明了我们的方法对于真实世界的数据集是足够可扩展的，并且使我们能够找到可变长度和“扭曲”的图案，否则这些图案会逃避传统算法的注意。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Parameter-free+Spikelet:+Discovering+Different+Length+and+Warped+Time+Series+Motifs+using+an+Adaptive+Time+Series+Representation)|0|
|[Hierarchical Proxy Modeling for Improved HPO in Time Series Forecasting](https://doi.org/10.1145/3580305.3599378)|Arindam Jati, Vijay Ekambaram, Shaonli Pal, Brian Quanz, Wesley M. Gifford, Pavithra Harsha, Stuart Siegel, Sumanta Mukherjee, Chandra Narayanaswami||Selecting the right set of hyperparameters is crucial in time series forecasting. The classical temporal cross-validation framework for hyperparameter optimization (HPO) often leads to poor test performance because of a possible mismatch between validation and test periods. To address this test-validation mismatch, we propose a novel technique, H-Pro to drive HPO via test proxies by exploiting data hierarchies often associated with time series datasets. Since higher-level aggregated time series often show less irregularity and better predictability as compared to the lowest-level time series which can be sparse and intermittent, we optimize the hyperparameters of the lowest-level base-forecaster by leveraging the proxy forecasts for the test period generated from the forecasters at higher levels. H-Pro can be applied on any off-the-shelf machine learning model to perform HPO. We validate the efficacy of our technique with extensive empirical evaluation on five publicly available hierarchical forecasting datasets. Our approach outperforms existing state-of-the-art methods in Tourism, Wiki, and Traffic datasets, and achieves competitive result in Tourism-L dataset, without any model-specific enhancements. Moreover, our method outperforms the winning method of the M5 forecast accuracy competition.|在时间序列预测中，选择正确的超参数集是至关重要的。超参数优化的经典时间交叉验证框架(hPO)往往由于验证和测试周期之间可能的不匹配而导致测试性能较差。为了解决这种测试验证不匹配问题，我们提出了一种新的技术，H-Pro 通过测试代理驱动 HPO 通过利用数据层次往往与时间序列数据集。由于较高层次的汇总时间序列相对于较低层次的稀疏和间歇时间序列具有较少的不规则性和较好的可预测性，因此我们利用较高层次的预测者产生的测试周期的代理预测来优化最低层次的基础预测者的超参数。H-Pro 可以应用于任何现成的机器学习模型来执行 HPO。通过对五个公开的层次预测数据集进行广泛的实证评估，验证了该方法的有效性。我们的方法优于现有的最先进的方法在旅游，维基，和交通数据集，并实现竞争结果在旅游 L 数据集，没有任何模型特定的增强。此外，我们的方法优于 M5预报精度竞赛的获胜方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Proxy+Modeling+for+Improved+HPO+in+Time+Series+Forecasting)|0|
|[Precursor-of-Anomaly Detection for Irregular Time Series](https://doi.org/10.1145/3580305.3599469)|Sheo Yon Jhin, Jaehoon Lee, Noseong Park|LG AI Research; Yonsei University|Anomaly detection is an important field that aims to identify unexpected patterns or data points, and it is closely related to many real-world problems, particularly to applications in finance, manufacturing, cyber security, and so on. While anomaly detection has been studied extensively in various fields, detecting future anomalies before they occur remains an unexplored territory. In this paper, we present a novel type of anomaly detection, called \emph{\textbf{P}recursor-of-\textbf{A}nomaly} (PoA) detection. Unlike conventional anomaly detection, which focuses on determining whether a given time series observation is an anomaly or not, PoA detection aims to detect future anomalies before they happen. To solve both problems at the same time, we present a neural controlled differential equation-based neural network and its multi-task learning algorithm. We conduct experiments using 17 baselines and 3 datasets, including regular and irregular time series, and demonstrate that our presented method outperforms the baselines in almost all cases. Our ablation studies also indicate that the multitasking training method significantly enhances the overall performance for both anomaly and PoA detection.|异常检测是一个重要的领域，旨在识别意想不到的模式或数据点，它与许多现实世界的问题密切相关，特别是在金融、制造业、网络安全等方面的应用。虽然异常检测已经在各个领域得到了广泛的研究，但是在未来的异常发生之前探测到它们仍然是一个未知的领域。在这篇文章中，我们提出了一种新的异常检测检测方法，称为 emph { textbf { P }-textbf { A }异常}(PoA)检测。与传统的异常检测不同，PoA 检测的重点是确定给定的时间序列观测是否是异常现象，而 PoA 检测的目的是在未来的异常现象发生之前检测出来。为了同时解决这两个问题，本文提出了一种基于神经控制微分方程的神经网络及其多任务学习算法。我们使用17个基线和3个数据集(包括规则和不规则时间序列)进行实验，并证明我们提出的方法在几乎所有情况下都优于基线。我们的消融研究还表明，多任务训练方法显著提高了异常和 PoA 检测的整体性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Precursor-of-Anomaly+Detection+for+Irregular+Time+Series)|0|
|[Community-based Dynamic Graph Learning for Popularity Prediction](https://doi.org/10.1145/3580305.3599281)|Shuo Ji, Xiaodong Lu, Mingzhe Liu, Leilei Sun, Chuanren Liu, Bowen Du, Hui Xiong||Popularity prediction, which aims to forecast how many users would like to interact with a target item or online content in the future, can help online shopping or social media platforms to identify popular items or digital contents. Many efforts have been made to study how the multi-faceted factors, such as item features, user preferences, and social influence, affect user-item interactions, but little work has focused on the evolutionary dynamics of these factors for individuals or groups. In that light, this paper develops a community-based dynamic graph learning method for popularity prediction. First, a dynamic graph learning framework is proposed to maintain a dynamic representation for each item or user entity and update the representations according to the newly observed user-item interactions. Second, a community detection module is designed to capture the evolving community structures and identify the most influential nodes. More importantly, our framework leverages a community-level message passing during the learning process to balance local and global information propagation. Finally, we predict the popularity of the target item or online content based on the learned representations. Our experimental results based on three real-world datasets demonstrate that the proposed method achieves better performance than the baselines. Our method could not only model the changes in a user's preferences, but also capture how the communities evolve over time.|流行度预测旨在预测未来有多少用户愿意与目标项目或在线内容进行互动，它可以帮助在线购物或社交媒体平台识别流行项目或数字内容。人们已经做出了很多努力来研究多方面的因素，如项目特征、用户偏好和社会影响，如何影响用户-项目的交互，但很少有工作集中在这些因素对个人或群体的进化动力学。基于此，本文提出了一种基于社区的动态图学习方法用于流行度预测。首先，提出了一个动态图学习框架，以维护每个项目或用户实体的动态表示，并根据新观察到的用户-项目交互更新表示。其次，设计了一个社区检测模块来捕捉不断演化的社区结构，并识别最有影响力的节点。更重要的是，我们的框架利用在学习过程中传递的社区级消息来平衡本地和全球信息传播。最后，我们根据学习表征预测目标项目或在线内容的受欢迎程度。基于三个实际数据集的实验结果表明，该方法比基线方法具有更好的性能。我们的方法不仅可以为用户偏好的变化建模，还可以捕捉社区随着时间的推移是如何演变的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Community-based+Dynamic+Graph+Learning+for+Popularity+Prediction)|0|
|[GetPt: Graph-enhanced General Table Pre-training with Alternate Attention Network](https://doi.org/10.1145/3580305.3599366)|Ran Jia, Haoming Guo, Xiaoyuan Jin, Chao Yan, Lun Du, Xiaojun Ma, Tamara Stankovic, Marko Lozajic, Goran Zoranovic, Igor Ilic, Shi Han, Dongmei Zhang||Tables are widely used for data storage and presentation due to their high flexibility in layout. The importance of tables as information carriers and the complexity of tabular data understanding attract a great deal of research on large-scale pre-training for tabular data. However, most of the works design models for specific types of tables, such as relational tables and tables with well-structured headers, neglecting tables with complex layouts. In real-world scenarios, there are many such tables beyond their target scope that cannot be well supported. In this paper, we propose GetPt, a unified pre-training architecture for general table representation applicable even to tables with complex structures and layouts. First, we convert a table to a heterogeneous graph with multiple types of edges to represent the layout of the table. Based on the graph, a specially designed transformer is applied to jointly model the semantics and structure of the table. Second, we devise the Alternate Attention Network (AAN) to better model the contextual information across multiple granularities of a table including tokens, cells, and the table. To better support a wide range of downstream tasks, we further employ three pre-training objectives and pre-train the model on a large table dataset. We fine-tune and evaluate GetPt model on two representative tasks, table type classification, and table structure recognition. Experiments show that GetPt outperforms existing state-of-the-art methods on these tasks.|由于表在布局上具有很高的灵活性，因此被广泛用于数据存储和表示。表格作为信息载体的重要性和表格数据理解的复杂性引起了人们对表格数据的大规模预训练研究。但是，大多数工作都是为特定类型的表设计模型，例如关系表和具有结构良好的标头的表，而忽略了具有复杂布局的表。在实际场景中，有许多这样的表超出了它们的目标范围，无法得到很好的支持。在本文中，我们提出了 GetPt，一个统一的通用表表示预训练体系结构，甚至适用于具有复杂结构和布局的表。首先，我们将一个表转换为一个具有多种边类型的异构图，以表示该表的布局。在此基础上，采用专门设计的变压器对表的语义和结构进行联合建模。其次，我们设计了交替注意力网络(AAN)来更好地模拟跨表格的多个粒度的上下文信息，包括令牌、单元格和表格。为了更好地支持广泛的下游任务，我们进一步采用了三个预训练目标，并在一个大型表数据集上预训练模型。我们对 GetPt 模型进行了微调和评估，包括两个代表性的任务: 表类型分类和表结构识别。实验表明，GetPt 在这些任务上的性能优于现有的最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GetPt:+Graph-enhanced+General+Table+Pre-training+with+Alternate+Attention+Network)|0|
|[Enhancing Node-Level Adversarial Defenses by Lipschitz Regularization of Graph Neural Networks](https://doi.org/10.1145/3580305.3599335)|Yaning Jia, Dongmian Zou, Hongfei Wang, Hai Jin||Graph neural networks (GNNs) have shown considerable promise for graph-structured data. However, they are also known to be unstable and vulnerable to perturbations and attacks. Recently, the Lipschitz constant has been adopted as a control on the stability of Euclidean neural networks, but calculating the exact constant is also known to be difficult even for very shallow networks. In this paper, we extend the Lipschitz analysis to graphs by providing a systematic scheme for estimating upper bounds of the Lipschitz constants of GNNs. We also derive concrete bounds for widely used GNN architectures including GCN, GraphSAGE and GAT. We then use these Lipschitz bounds for regularized GNN training for improved stability. Our numerical results on Lipschitz regularization of GNNs not only illustrate enhanced test accuracy under random noise, but also show consistent improvement for state-of-the-art defense methods against adversarial attacks.|图形神经网络(GNN)在图形结构化数据方面已经显示出相当大的潜力。然而，它们也被认为是不稳定的，容易受到干扰和攻击。近年来，利普希兹常数被用来控制欧氏神经网络的稳定性，但是精确的常数计算对于非常浅的神经网络来说也是非常困难的。本文将 Lipschitz 分析推广到图上，给出了估计 GNN 的 Lipschitz 常数上界的一个系统方案。我们还推导出广泛使用的 GNN 体系结构的具体界限，包括 GCN、 GraphSAGE 和 GAT。然后我们使用这些 Lipschitz 边界进行正则化 GNN 训练以提高稳定性。我们对 GNN 的 Lipschitz 正则化的数值结果不仅说明了在随机噪声下测试精度的提高，而且也显示了对抗对手攻击的最新防御方法的一致性改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Node-Level+Adversarial+Defenses+by+Lipschitz+Regularization+of+Graph+Neural+Networks)|0|
|[Complementary Classifier Induced Partial Label Learning](https://doi.org/10.1145/3580305.3599282)|Yuheng Jia, Chongjie Si, MinLing Zhang|Shanghai Jiao Tong University; Southeast University|In partial label learning (PLL), each training sample is associated with a set of candidate labels, among which only one is valid. The core of PLL is to disambiguate the candidate labels to get the ground-truth one. In disambiguation, the existing works usually do not fully investigate the effectiveness of the non-candidate label set (a.k.a. complementary labels), which accurately indicates a set of labels that do not belong to a sample. In this paper, we use the non-candidate labels to induce a complementary classifier, which naturally forms an adversarial relationship against the traditional PLL classifier, to eliminate the false-positive labels in the candidate label set. Besides, we assume the feature space and the label space share the same local topological structure captured by a dynamic graph, and use it to assist disambiguation. Extensive experimental results validate the superiority of the proposed approach against state-of-the-art PLL methods on 4 controlled UCI data sets and 6 real-world data sets, and reveal the usefulness of complementary learning in PLL. The code has been released in the link https://github.com/Chongjie-Si/PL-CL.|在部分标签学习(PLL)中，每个训练样本都与一组候选标签相关联，其中只有一个是有效的。锁相环的核心是消除候选标签的歧义，从而得到地面真值。在消歧中，现有的作品通常没有充分考察非候选标签集(又称互补标签)的有效性，它准确地指出了一组不属于样本的标签。本文利用非候选标签诱导出一个与传统 PLL 分类器自然形成对抗关系的互补分类器，以消除候选标签集中的假阳性标签。此外，我们假设特征空间和标签空间共享一个动态图所捕获的相同的局部拓扑结构，并利用它来协助消歧。大量的实验结果验证了该方法在4个受控 UCI 数据集和6个实际数据集上与现有的锁相环方法相比的优越性，并揭示了补充学习在锁相环中的有效性。代码已经在链接 https://github.com/chongjie-si/pl-cl 中发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Complementary+Classifier+Induced+Partial+Label+Learning)|0|
|[Anomaly Detection with Score Distribution Discrimination](https://doi.org/10.1145/3580305.3599258)|Minqi Jiang, Songqiao Han, Hailiang Huang|Shanghai University of Finance and Economics|Recent studies give more attention to the anomaly detection (AD) methods that can leverage a handful of labeled anomalies along with abundant unlabeled data. These existing anomaly-informed AD methods rely on manually predefined score target(s), e.g., prior constant or margin hyperparameter(s), to realize discrimination in anomaly scores between normal and abnormal data. However, such methods would be vulnerable to the existence of anomaly contamination in the unlabeled data, and also lack adaptation to different data scenarios. In this paper, we propose to optimize the anomaly scoring function from the view of score distribution, thus better retaining the diversity and more fine-grained information of input data, especially when the unlabeled data contains anomaly noises in more practical AD scenarios. We design a novel loss function called Overlap loss that minimizes the overlap area between the score distributions of normal and abnormal samples, which no longer depends on prior anomaly score targets and thus acquires adaptability to various datasets. Overlap loss consists of Score Distribution Estimator and Overlap Area Calculation, which are introduced to overcome challenges when estimating arbitrary score distributions, and to ensure the boundness of training loss. As a general loss component, Overlap loss can be effectively integrated into multiple network architectures for constructing AD models. Extensive experimental results indicate that Overlap loss based AD models significantly outperform their state-of-the-art counterparts, and achieve better performance on different types of anomalies.|最近的研究更多地关注异常检测(AD)方法，这种方法可以利用一些标记的异常和大量未标记的数据。这些现有的反常信息 AD 方法依赖于人工预定义的评分目标，如先验常数或边界超参数，以实现正常数据和异常数据之间的异常评分判别。然而，这些方法容易受到未标记数据中异常污染的影响，并且缺乏对不同数据场景的适应性。本文提出从分数分布的角度对异常评分函数进行优化，从而更好地保留输入数据的多样性和更细粒度的信息，特别是当未标记数据中含有异常噪声时，在更实际的 AD 场景中更能保持这种多样性和更细粒度的信息。我们设计了一个新的丢失函数叫做重叠丢失，它使得正常和异常样本的分数分布之间的重叠区域最小化，不再依赖于先前的异常分数目标，从而获得了对各种数据集的适应性。重叠损失包括分数分布估计和重叠面积计算，它们被用来克服估计任意分数分布时的困难，并确保训练损失的有界性。重叠损耗作为一种通用损耗分量，可以有效地集成到多种网络结构中，用于构建 AD 模型。大量的实验结果表明，基于重叠损耗的 AD 模型显著优于其最先进的同类模型，并在不同类型的异常情况下取得更好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Anomaly+Detection+with+Score+Distribution+Discrimination)|0|
|[CF-GODE: Continuous-Time Causal Inference for Multi-Agent Dynamical Systems](https://doi.org/10.1145/3580305.3599272)|Song Jiang, Zijie Huang, Xiao Luo, Yizhou Sun|University of California, Los Angeles|Multi-agent dynamical systems refer to scenarios where multiple units interact with each other and evolve collectively over time. To make informed decisions in multi-agent dynamical systems, such as determining the optimal vaccine distribution plan, it is essential for decision-makers to estimate the continuous-time counterfactual outcomes. However, existing studies of causal inference over time rely on the assumption that units are mutually independent, which is not valid for multi-agent dynamical systems. In this paper, we aim to bridge this gap and study how to estimate counterfactual outcomes in multi-agent dynamical systems. Causal inference in a multi-agent dynamical system has unique challenges: 1) Confounders are time-varying and are present in both individual unit covariates and those of other units; 2) Units are affected by not only their own but also others' treatments; 3) The treatments are naturally dynamic, such as receiving vaccines and boosters in a seasonal manner. We model a multi-agent dynamical system as a graph and propose CounterFactual GraphODE (CF-GODE), a causal model that estimates continuous-time counterfactual outcomes in the presence of inter-dependencies between units. To facilitate continuous-time estimation, we propose Treatment-Induced GraphODE, a novel ordinary differential equation based on GNN, which incorporates dynamical treatments as additional inputs to predict potential outcomes over time. To remove confounding bias, we propose two domain adversarial learning based objectives that learn balanced continuous representation trajectories, which are not predictive of treatments and interference. We further provide theoretical justification to prove their effectiveness. Experiments on two semi-synthetic datasets confirm that CF-GODE outperforms baselines on counterfactual estimation. We also provide extensive analyses to understand how our model works.|多代理动态系统是指多个单元相互作用并随着时间的推移共同发展的场景。在多智能体动态系统中，为了作出知情决策，如确定最优疫苗分配计划，决策者必须估计连续时间的反事实结果。然而，现有的因果推理研究都是基于单元之间相互独立的假设，这种假设对于多智能体动力系统是不成立的。本文主要研究多智能体动力系统中反事实结果的估计问题。多动力系统因果推理面临着独特的挑战: 1)混杂因素是随时间变化的，存在于个体单位协变量和其他单位的协变量中; 2)单位不仅受到自身治疗的影响，还受到其他单位治疗的影响; 3)治疗是自然动态的，例如以季节性方式接种疫苗和加强剂。我们将一个多动力系统模型建模为一个图形，并提出 CounterFact GraphODE (CF-GODE) ，这是一个因果模型，在单位之间存在相互依赖的情况下，估计连续时间的反事实结果。为了便于连续时间估计，我们提出了治疗诱导的 GraphoDE，这是一种基于 GNN 的新型常微分方程，它将动态治疗作为额外的输入来预测随着时间的推移的潜在结果。为了消除混杂偏差，我们提出了两个领域对抗性学习的目标，学习平衡连续表征轨迹，这是不预测治疗和干扰。我们进一步提供理论论证来证明它们的有效性。在两个半合成数据集上的实验证实了 CF-GODE 算法在反事实估计上优于基线算法。我们还提供了广泛的分析，以了解我们的模型是如何工作的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CF-GODE:+Continuous-Time+Causal+Inference+for+Multi-Agent+Dynamical+Systems)|0|
|[FedSkill: Privacy Preserved Interpretable Skill Learning via Imitation](https://doi.org/10.1145/3580305.3599349)|Yushan Jiang, Wenchao Yu, Dongjin Song, Lu Wang, Wei Cheng, Haifeng Chen||Imitation learning that replicates experts' skills via their demonstrations has shown significant success in various decision-making tasks. However, two critical challenges still hinder the deployment of imitation learning techniques in real-world application scenarios. First, existing methods lack the intrinsic interpretability to explicitly explain the underlying rationale of the learned skill and thus making learned policy untrustworthy. Second, due to the scarcity of expert demonstrations from each end user (client), learning a policy based on different data silos is necessary but challenging in privacy-sensitive applications such as finance and healthcare. To this end, we present a privacy-preserved interpretable skill learning framework (FedSkill) that enables global policy learning to incorporate data from different sources and provides explainable interpretations to each local user without violating privacy and data sovereignty. Specifically, our proposed interpretable skill learning model can capture the varying patterns in the trajectories of expert demonstrations, and extract prototypical information as skills that provide implicit guidance for policy learning and explicit explanations in the reasoning process. Moreover, we design a novel aggregation mechanism coupled with the based skill learning model to preserve global information utilization and maintain local interpretability under the federated framework. Thoroughly experiments on three datasets and empirical studies demonstrate that our proposed FedSkill framework not only outperforms state-of-the-art imitation learning methods but also exhibits good interpretability under a federated setting. Our proposed FedSkill framework is the first attempt to bridge the gaps among federated learning, interpretable machine learning, and imitation learning.|通过演示复制专家技能的模仿学习在各种决策任务中表现出显著的成功。然而，两个关键的挑战仍然阻碍了模仿学习技术在真实应用场景中的部署。首先，现有方法缺乏内在的可解释性，无法明确解释所学技能的基本原理，从而使所学政策不可信。其次，由于缺乏来自每个终端用户(客户)的专家演示，学习基于不同数据竖井的策略是必要的，但在金融和医疗等对隐私敏感的应用程序中具有挑战性。为此，我们提出了一个保护隐私的可解释技能学习框架(FedSkills) ，使全球政策学习能够合并来自不同来源的数据，并向每个本地用户提供可解释的解释，而不侵犯隐私和数据主权。具体来说，我们提出的可解释的技能学习模型可以捕捉专家演示轨迹中的不同模式，并提取原型信息作为技能，在推理过程中为政策学习提供隐式指导和显式解释。在此基础上，设计了一种新的聚合机制，结合基于技能学习模型，在联邦框架下保持全局信息利用率和局部可解释性。通过对三个数据集的深入实验和实证研究表明，我们提出的 FedSkills 框架不仅优于最先进的模拟学习方法，而且在联邦环境下表现出良好的可解释性。我们提出的 FedSkills 框架是第一次尝试弥合联邦学习、可解释机器学习和模仿学习之间的差距。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedSkill:+Privacy+Preserved+Interpretable+Skill+Learning+via+Imitation)|0|
|[Heterformer: Transformer-based Deep Node Representation Learning on Heterogeneous Text-Rich Networks](https://doi.org/10.1145/3580305.3599376)|Bowen Jin, Yu Zhang, Qi Zhu, Jiawei Han|University of Illinois at Urbana-Champaign|Representation learning on networks aims to derive a meaningful vector representation for each node, thereby facilitating downstream tasks such as link prediction, node classification, and node clustering. In heterogeneous text-rich networks, this task is more challenging due to (1) presence or absence of text: Some nodes are associated with rich textual information, while others are not; (2) diversity of types: Nodes and edges of multiple types form a heterogeneous network structure. As pretrained language models (PLMs) have demonstrated their effectiveness in obtaining widely generalizable text representations, a substantial amount of effort has been made to incorporate PLMs into representation learning on text-rich networks. However, few of them can jointly consider heterogeneous structure (network) information as well as rich textual semantic information of each node effectively. In this paper, we propose Heterformer, a Heterogeneous Network-Empowered Transformer that performs contextualized text encoding and heterogeneous structure encoding in a unified model. Specifically, we inject heterogeneous structure information into each Transformer layer when encoding node texts. Meanwhile, Heterformer is capable of characterizing node/edge type heterogeneity and encoding nodes with or without texts. We conduct comprehensive experiments on three tasks (i.e., link prediction, node classification, and node clustering) on three large-scale datasets from different domains, where Heterformer outperforms competitive baselines significantly and consistently. The code can be found at https://github.com/PeterGriffinJin/Heterformer.|网络表示学习的目的是为每个节点推导出有意义的向量表示，从而促进下游任务，如链路预测、节点分类和节点聚类。在异构文本丰富的网络中，这项任务更具挑战性，因为(1)存在或不存在文本: 一些节点与丰富的文本信息相关联，而另一些则不是; (2)类型的多样性: 多种类型的节点和边形成一个异质网路结构。由于预训练语言模型(PLM)已经证明了它们在获得广泛泛用的文本表示方面的有效性，因此已经做了大量的工作将 PLM 融入到文本丰富的网络的表示学习中。然而，他们很少能够有效地联合考虑每个节点的异构结构(网络)信息和丰富的文本语义信息。在本文中，我们提出了异构变压器，一个异构网络授权的变压器，执行上下文文本编码和异构结构编码在一个统一的模型。具体来说，在编码节点文本时，我们将异构结构信息注入到每个 former 层中。同时，异质形成器能够表征节点/边缘类型的异质性，并能够编码有文本或无文本的节点。我们对来自不同领域的三个大规模数据集进行了三个任务(即链路预测、节点分类和节点聚类)的综合实验，其中异形优势显著且一致地优于竞争基线。密码可以在 https://github.com/petergriffinjin/heterformer 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Heterformer:+Transformer-based+Deep+Node+Representation+Learning+on+Heterogeneous+Text-Rich+Networks)|0|
|[Transferable Graph Structure Learning for Graph-based Traffic Forecasting Across Cities](https://doi.org/10.1145/3580305.3599529)|Yilun Jin, Kai Chen, Qiang Yang|Hong Kong University of Science and Technology|Graph-based deep learning models are powerful in modeling spatio-temporal graphs for traffic forecasting. In practice, accurate forecasting models rely on sufficient traffic data, which may not be accessible in real-world applications. To address this problem, transfer learning methods are designed to transfer knowledge from the source graph with abundant data to the target graph with limited data. However, existing methods adopt pre-defined graph structures for knowledge extraction and transfer, which may be noisy or biased and negatively impact the performance of knowledge transfer. To address the problem, we propose TransGTR, a transferable structure learning framework for traffic forecasting that jointly learns and transfers the graph structures and forecasting models across cities. TransGTR consists of a node feature network, a structure generator, and a forecasting model. We train the node feature network with knowledge distillation to extract city-agnostic node features, such that the structure generator, taking the node features as inputs, can be transferred across both cities. Furthermore, we train the structure generator via a temporal decoupled regularization, such that the spatial features learned with the generated graphs share similar distributions across cities and thus facilitate knowledge transfer for the forecasting model. We evaluate TransGTR on real-world traffic speed datasets, where under a fair comparison, TransGTR outperforms state-of-the-art baselines by up to 5.4%.|基于图的深度学习模型在交通流量预测的时空图建模方面具有很强的应用前景。在实践中，准确的预测模型依赖于充足的交通数据，这些数据在现实应用中可能无法访问。针对这一问题，设计了迁移学习方法，将知识从数据丰富的源图转移到数据有限的目标图。然而，现有的知识提取和转移方法都采用预先定义的图结构，这可能会产生噪声或有偏差，对知识转移的性能产生负面影响。为了解决这个问题，我们提出了 TransGTR，一个交通预测的可转移结构学习框架，它可以在城市之间共同学习和转移图形结构和预测模型。TransGTR 由节点特征网络、结构生成器和预测模型组成。利用知识提取技术对节点特征网络进行训练，提取城市不可知节点特征，使得以节点特征为输入的结构生成器可以在两个城市之间传递。此外，我们通过时间解耦正则化训练结构生成器，使得生成的图所学到的空间特征在城市之间具有相似的分布，从而促进预测模型的知识转移。我们在实际交通速度数据集上对 TransGTR 进行评估，在公平的比较下，TransGTR 的性能比最先进的基线高出5.4% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Transferable+Graph+Structure+Learning+for+Graph-based+Traffic+Forecasting+Across+Cities)|0|
|[When Rigidity Hurts: Soft Consistency Regularization for Probabilistic Hierarchical Time Series Forecasting](https://doi.org/10.1145/3580305.3599547)|Harshavardhan Kamarthi, Lingkai Kong, Alexander Rodríguez, Chao Zhang, B. Aditya Prakash||Probabilistic hierarchical time-series forecasting is an important variant of time-series forecasting, where the goal is to model and forecast multivariate time-series that have underlying hierarchical relations. Most methods focus on point predictions and do not provide well-calibrated probabilistic forecasts distributions. Recent state-of-art probabilistic forecasting methods also impose hierarchical relations on point predictions and samples of distribution which does not account for coherency of forecast distributions. Previous works also silently assume that datasets are always consistent with given hierarchical relations and do not adapt to real-world datasets that show deviation from this assumption. We close both these gap and propose PROFHiT, which is a fully probabilistic hierarchical forecasting model that jointly models forecast distribution of entire hierarchy. PROFHiT uses a flexible probabilistic Bayesian approach and introduces a novel Distributional Coherency regularization to learn from hierarchical relations for entire forecast distribution that enables robust and calibrated forecasts as well as adapt to datasets of varying hierarchical consistency. On evaluating PROFHiT over wide range of datasets, we observed 41-88% better performance in accuracy and significantly better calibration. Due to modeling the coherency over full distribution, we observed that PROFHiT can robustly provide reliable forecasts even if up to 10% of input time-series data is missing where other methods' performance severely degrade by over 70%.|概率分层时间序列预测是时间序列预测的一个重要变体，其目标是建模和预测具有层次关系的多变量时间序列。大多数方法侧重于点预测，不能提供精确校准的概率预测分布。目前最先进的概率预测方法也在点预测和分布样本上施加了层次关系，这没有考虑到预测分布的一致性。以前的工作也默认数据集总是与给定的层次关系一致，不适应现实世界中显示偏离这一假设的数据集。在此基础上提出了一种全概率层次预测模型 PROFHiT，该模型可以联合预测整个层次结构的分布。PROFHiT 采用灵活的概率贝叶斯方法，引入了一种新的分布相干正则化方法，从整个预报分布的层次关系中学习，使稳健和校准的预报，以及适应不同层次一致性的数据集。在评估范围广泛的数据集 PROFHiT，我们观察到41-88% 更好的准确性和显着更好的校准性能。由于对全分布的一致性建模，我们观察到 PROFHiT 可以稳健地提供可靠的预测，即使高达10% 的输入时间序列数据丢失，其他方法的性能严重下降超过70% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=When+Rigidity+Hurts:+Soft+Consistency+Regularization+for+Probabilistic+Hierarchical+Time+Series+Forecasting)|0|
|[R-Mixup: Riemannian Mixup for Biological Networks](https://doi.org/10.1145/3580305.3599483)|Xuan Kan, Zimu Li, Hejie Cui, Yue Yu, Ran Xu, Shaojun Yu, Zilong Zhang, Ying Guo, Carl Yang|Georgia Institute of Technology; Emory University; University of Chicago; University of International Business and Economics|Biological networks are commonly used in biomedical and healthcare domains to effectively model the structure of complex biological systems with interactions linking biological entities. However, due to their characteristics of high dimensionality and low sample size, directly applying deep learning models on biological networks usually faces severe overfitting. In this work, we propose R-MIXUP, a Mixup-based data augmentation technique that suits the symmetric positive definite (SPD) property of adjacency matrices from biological networks with optimized training efficiency. The interpolation process in R-MIXUP leverages the log-Euclidean distance metrics from the Riemannian manifold, effectively addressing the swelling effect and arbitrarily incorrect label issues of vanilla Mixup. We demonstrate the effectiveness of R-MIXUP with five real-world biological network datasets on both regression and classification tasks. Besides, we derive a commonly ignored necessary condition for identifying the SPD matrices of biological networks and empirically study its influence on the model performance. The code implementation can be found in Appendix E.|生物网络通常用于生物医学和医疗保健领域，以有效地建模复杂的生物系统的结构与连接生物实体的相互作用。然而，由于生物网络具有高维数、低样本量的特点，直接将深度学习模型应用于生物网络往往面临严重的过拟合问题。在这项工作中，我们提出了 R-MIXUP，一种基于混合的数据增强技术，适合于对称正定(SPD)性质的邻接矩阵从生物网络与优化训练效率。在 R-MIXUP 中的插值过程利用了来自黎曼流形的 log-Euclidean 距离度量，有效地解决了香草 Mixup 的膨胀效应和任意错误的标签问题。我们用五个真实世界的生物网络数据集在回归和分类任务上证明了 R-MIXUP 的有效性。此外，我们推导了一个常被忽略的识别生物网络 SPD 矩阵的必要条件，并对其对模型性能的影响进行了实证研究。代码实现可以在附录 E 中找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=R-Mixup:+Riemannian+Mixup+for+Biological+Networks)|0|
|[Task-Equivariant Graph Few-shot Learning](https://doi.org/10.1145/3580305.3599515)|Sungwon Kim, Junseok Lee, Namkyeong Lee, Wonjoong Kim, Seungyoon Choi, Chanyoung Park|KAIST|Although Graph Neural Networks (GNNs) have been successful in node classification tasks, their performance heavily relies on the availability of a sufficient number of labeled nodes per class. In real-world situations, not all classes have many labeled nodes and there may be instances where the model needs to classify new classes, making manual labeling difficult. To solve this problem, it is important for GNNs to be able to classify nodes with a limited number of labeled nodes, known as few-shot node classification. Previous episodic meta-learning based methods have demonstrated success in few-shot node classification, but our findings suggest that optimal performance can only be achieved with a substantial amount of diverse training meta-tasks. To address this challenge of meta-learning based few-shot learning (FSL), we propose a new approach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEG framework enables the model to learn transferable task-adaptation strategies using a limited number of training meta-tasks, allowing it to acquire meta-knowledge for a wide range of meta-tasks. By incorporating equivariant neural networks, TEG can utilize their strong generalization abilities to learn highly adaptable task-specific strategies. As a result, TEG achieves state-of-the-art performance with limited training meta-tasks. Our experiments on various benchmark datasets demonstrate TEG's superiority in terms of accuracy and generalization ability, even when using minimal meta-training data, highlighting the effectiveness of our proposed approach in addressing the challenges of meta-learning based few-shot node classification. Our code is available at the following link: https://github.com/sung-won-kim/TEG|尽管图神经网络(GNN)在节点分类任务中取得了成功，但它的性能在很大程度上依赖于每个类有足够数量的标记节点。在实际情况中，并不是所有的类都有许多带标签的节点，而且可能存在模型需要对新类进行分类的实例，这使得手动标签变得困难。为了解决这个问题，GNN 必须能够对标记节点数量有限的节点进行分类，这就是所谓的少镜头节点分类。以往的基于情节的元学习方法已经证明了在少镜头节点分类中的成功，但是我们的研究结果表明，只有通过大量不同的训练元任务才能获得最佳性能。为了解决基于元学习的少镜头学习(FSL)面临的挑战，我们提出了一种新的学习方法——任务等变图少镜头学习(TEG)框架。我们的 TEG 框架使模型能够使用有限数量的训练元任务学习可转移的任务适应策略，允许模型获取广泛元任务的元知识。通过结合等变神经网络，TEG 可以利用其强大的泛化能力来学习高度适应性的任务特定策略。因此，TEG 在有限的训练元任务的情况下达到了最高水平的表现。我们在各种基准数据集上的实验证明了 TEG 在准确性和泛化能力方面的优势，即使使用最小的元训练数据，突出了我们提出的方法在解决基于元学习的少镜头节点分类的挑战方面的有效性。我们的代码可在以下连结下载:  https://github.com/sung-won-kim/teg|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Task-Equivariant+Graph+Few-shot+Learning)|0|
|[Deception by Omission: Using Adversarial Missingness to Poison Causal Structure Learning](https://doi.org/10.1145/3580305.3599297)|Deniz Koyuncu, Alex Gittens, Bülent Yener, Moti Yung|Rensselaer Polytechnic Institute; Google LLC|Inference of causal structures from observational data is a key component of causal machine learning; in practice, this data may be incompletely observed. Prior work has demonstrated that adversarial perturbations of completely observed training data may be used to force the learning of inaccurate causal structural models (SCMs). However, when the data can be audited for correctness (e.g., it is crytographically signed by its source), this adversarial mechanism is invalidated. This work introduces a novel attack methodology wherein the adversary deceptively omits a portion of the true training data to bias the learned causal structures in a desired manner. Theoretically sound attack mechanisms are derived for the case of arbitrary SCMs, and a sample-efficient learning-based heuristic is given for Gaussian SCMs. Experimental validation of these approaches on real and synthetic data sets demonstrates the effectiveness of adversarial missingness attacks at deceiving popular causal structure learning algorithms.|从观测数据推断因果结构是因果机器学习的一个关键组成部分，在实践中，这些数据可能不完全被观测到。先前的工作已经证明，完全观察到的训练数据的对抗性扰动可能被用来强制学习不准确的因果结构模型(SCM)。然而，当数据可以被审计为正确(例如，它是由它的来源签名) ，这个对抗机制是无效的。这项工作介绍了一种新的攻击方法，其中对手欺骗性省略了一部分真实的训练数据，以偏向学习的因果结构在一个理想的方式。从理论上推导了任意单片机情况下的声音攻击机制，并给出了高斯单片机的一种基于样本有效学习的启发式算法。这些方法在真实和合成数据集上的实验验证证明了对抗性缺失攻击在欺骗流行的因果结构学习算法上的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deception+by+Omission:+Using+Adversarial+Missingness+to+Poison+Causal+Structure+Learning)|0|
|[Optimizing Traffic Control with Model-Based Learning: A Pessimistic Approach to Data-Efficient Policy Inference](https://doi.org/10.1145/3580305.3599459)|Mayuresh Kunjir, Sanjay Chawla, Siddarth Chandrasekar, Devika Jay, Balaraman Ravindran||Traffic signal control is an important problem in urban mobility with a significant potential for economic and environmental impact. While there is a growing interest in Reinforcement Learning (RL) for traffic signal control, the work so far has focussed on learning through simulations which could lead to inaccuracies due to simplifying assumptions. Instead, real experience data on traffic is available and could be exploited at minimal costs. Recent progress in offline or batch RL has enabled just that. Model-based offline RL methods, in particular, have been shown to generalize from the experience data much better than others. We build a model-based learning framework that infers a Markov Decision Process (MDP) from a dataset collected using a cyclic traffic signal control policy that is both commonplace and easy to gather. The MDP is built with pessimistic costs to manage out-of-distribution scenarios using an adaptive shaping of rewards which is shown to provide better regularization compared to the prior related work in addition to being PAC-optimal. Our model is evaluated on a complex signalized roundabout and a large multi-intersection environment, demonstrating that highly performant traffic control policies can be built in a data-efficient manner.|交通信号控制是城市交通中的一个重要问题，具有巨大的经济和环境影响潜力。虽然人们对交通信号控制的强化学习(rL)越来越感兴趣，但到目前为止的工作主要集中在通过模拟学习，这种模拟可能由于简化假设而导致不准确性。相反，有关流量的真实经验数据是可用的，并且可以以最低的成本加以利用。最近在离线或批处理 RL 方面的进展正是启用了这一点。特别是基于模型的离线 RL 方法，已经被证明能够比其他方法更好地从经验数据中归纳出来。我们建立了一个基于模型的学习框架，从一个使用循环交通信号控制策略收集的数据集中推断出一个马可夫决策过程(mDP)。MDP 是建立在悲观成本管理分配外的情况下使用自适应形成的奖励，显示提供更好的正规化比以前的相关工作，除了是 PAC 最优的。在一个复杂的信号环形交叉口和一个大型多交叉口环境下对模型进行了评估，表明可以以数据高效的方式建立高性能的交通控制策略。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+Traffic+Control+with+Model-Based+Learning:+A+Pessimistic+Approach+to+Data-Efficient+Policy+Inference)|0|
|[Shift-Robust Molecular Relational Learning with Causal Substructure](https://doi.org/10.1145/3580305.3599437)|Namkyeong Lee, Kanghoon Yoon, Gyoung S. Na, Sein Kim, Chanyoung Park|KAIST; KRICT|Recently, molecular relational learning, whose goal is to predict the interaction behavior between molecular pairs, got a surge of interest in molecular sciences due to its wide range of applications. In this work, we propose CMRL that is robust to the distributional shift in molecular relational learning by detecting the core substructure that is causally related to chemical reactions. To do so, we first assume a causal relationship based on the domain knowledge of molecular sciences and construct a structural causal model (SCM) that reveals the relationship between variables. Based on the SCM, we introduce a novel conditional intervention framework whose intervention is conditioned on the paired molecule. With the conditional intervention framework, our model successfully learns from the causal substructure and alleviates the confounding effect of shortcut substructures that are spuriously correlated to chemical reactions. Extensive experiments on various tasks with real-world and synthetic datasets demonstrate the superiority of CMRL over state-of-the-art baseline models. Our code is available at https://github.com/Namkyeong/CMRL.|近年来，以预测分子间相互作用行为为目标的分子关系学习由于其广泛的应用而引起了分子科学界的极大兴趣。在这项工作中，我们提出的 CMRL 是鲁棒的分子关系学习的分布转移，通过检测核心子结构的因果关系相关的化学反应。为了做到这一点，我们首先假设一个因果关系的基础上的领域知识的分子科学和建立一个结构的因果模型(SCM) ，揭示变量之间的关系。在 SCM 的基础上，我们引入了一种新的条件干预框架，其干预以配对分子为条件。在条件干预框架下，我们的模型成功地从因果子结构中学习，并减轻了与化学反应虚假相关的捷径子结构的混杂效应。通过对现实世界和合成数据集的各种任务的大量实验，证明了 CMRL 相对于最先进的基线模型的优越性。我们的代码可以在 https://github.com/namkyeong/cmrl 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Shift-Robust+Molecular+Relational+Learning+with+Causal+Substructure)|0|
|[Boosting Multitask Learning on Graphs through Higher-Order Task Affinities](https://doi.org/10.1145/3580305.3599265)|Dongyue Li, Haotian Ju, Aneesh Sharma, Hongyang R. Zhang|Northeastern University; Google|Predicting node labels on a given graph is a widely studied problem with many applications, including community detection and molecular graph prediction. This paper considers predicting multiple node labeling functions on graphs simultaneously and revisits this problem from a multitask learning perspective. For a concrete example, consider overlapping community detection: each community membership is a binary node classification task. Due to complex overlapping patterns, we find that negative transfer is prevalent when we apply naive multitask learning to multiple community detection, as task relationships are highly nonlinear across different node labeling. To address the challenge, we develop an algorithm to cluster tasks into groups based on a higher-order task affinity measure. We then fit a multitask model on each task group, resulting in a boosting procedure on top of the baseline model. We estimate the higher-order task affinity measure between two tasks as the prediction loss of one task in the presence of another task and a random subset of other tasks. Then, we use spectral clustering on the affinity score matrix to identify task grouping. We design several speedup techniques to compute the higher-order affinity scores efficiently and show that they can predict negative transfers more accurately than pairwise task affinities. We validate our procedure using various community detection and molecular graph prediction data sets, showing favorable results compared with existing methods. Lastly, we provide a theoretical analysis to show that under a planted block model of tasks on graphs, our affinity scores can provably separate tasks into groups.|对给定图上的节点标签进行预测是一个有着广泛应用前景的问题，包括社区检测和分子图预测。本文从多任务学习的角度研究了图上多节点标记函数的同时预测问题。对于一个具体的例子，考虑重叠社区检测: 每个社区成员是一个二进制节点分类任务。由于复杂的重叠模式，我们发现负迁移在幼稚多任务学习应用于多社区检测时是普遍存在的，因为不同节点间的任务关系是高度非线性的。为了解决这一问题，我们提出了一种基于高阶任务亲和度的任务分组算法。然后，我们在每个任务组上安装一个多任务模型，从而在基线模型之上形成一个推进过程。我们将两个任务之间的高阶任务亲和度估计为一个任务在另一个任务存在时的预测损失和其他任务的随机子集。然后，我们使用亲和力得分矩阵上的 SVD 来识别任务分组。我们设计了几种加速技术来有效地计算高阶亲和力得分，并表明它们能比成对任务亲和力更准确地预测负向转移。我们验证了我们的程序使用各种社区检测和分子图预测数据集，显示良好的结果与现有的方法相比。最后，我们提供了一个理论分析，表明在图上的任务种植块模型下，我们的亲和力得分可以证明任务分组。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Boosting+Multitask+Learning+on+Graphs+through+Higher-Order+Task+Affinities)|0|
|[Interpretable Sparsification of Brain Graphs: Better Practices and Effective Designs for Graph Neural Networks](https://doi.org/10.1145/3580305.3599394)|Gaotang Li, Marlena Duda, Xiang Zhang, Danai Koutra, Yujun Yan|Georgia State University; University of Michigan, Ann Arbor; University of North Carolina, Charlotte; Dartmouth College|Brain graphs, which model the structural and functional relationships between brain regions, are crucial in neuroscientific and clinical applications involving graph classification. However, dense brain graphs pose computational challenges including high runtime and memory usage and limited interpretability. In this paper, we investigate effective designs in Graph Neural Networks (GNNs) to sparsify brain graphs by eliminating noisy edges. While prior works remove noisy edges based on explainability or task-irrelevant properties, their effectiveness in enhancing performance with sparsified graphs is not guaranteed. Moreover, existing approaches often overlook collective edge removal across multiple graphs. To address these issues, we introduce an iterative framework to analyze different sparsification models. Our findings are as follows: (i) methods prioritizing interpretability may not be suitable for graph sparsification as they can degrade GNNs' performance in graph classification tasks; (ii) simultaneously learning edge selection with GNN training is more beneficial than post-training; (iii) a shared edge selection across graphs outperforms separate selection for each graph; and (iv) task-relevant gradient information aids in edge selection. Based on these insights, we propose a new model, Interpretable Graph Sparsification (IGS), which enhances graph classification performance by up to 5.1% with 55.0% fewer edges. The retained edges identified by IGS provide neuroscientific interpretations and are supported by well-established literature.|脑图，模拟大脑区域之间的结构和功能关系，在涉及图形分类的神经科学和临床应用中是至关重要的。然而，密集的大脑图形带来了计算上的挑战，包括高运行时间和内存使用率以及有限的可解释性。本文研究了图神经网络(GNN)中通过去除噪声边缘来稀疏大脑图形的有效设计方法。先前的工作基于可解释性或任务无关性质去除噪声边缘，但其在稀疏图增强性能方面的有效性并不能得到保证。此外，现有的方法往往忽略了跨多个图的集体边缘去除。为了解决这些问题，我们引入了一个迭代框架来分析不同的稀疏化模型。我们的研究结果如下: (i)优先考虑可解释性的方法可能不适合于图的稀疏化，因为它们可能会降低 GNN 在图分类任务中的表现; (ii)同时学习 GNN 训练的边缘选择比训练后更有益; (iii)跨图的共享边缘选择优于每个图的单独选择; 和(iv)任务相关的梯度信息有助于边缘选择。在此基础上，我们提出了一种新的图分类模型——可解释图稀疏化(IGS)模型，该模型将图分类性能提高了5.1% ，边数减少了55.0% 。由 IGS 确定的保留边缘提供了神经科学的解释，并得到了成熟文献的支持。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interpretable+Sparsification+of+Brain+Graphs:+Better+Practices+and+Effective+Designs+for+Graph+Neural+Networks)|0|
|[What's Behind the Mask: Understanding Masked Graph Modeling for Graph Autoencoders](https://doi.org/10.1145/3580305.3599546)|Jintang Li, Ruofan Wu, Wangbin Sun, Liang Chen, Sheng Tian, Liang Zhu, Changhua Meng, Zibin Zheng, Weiqiang Wang||The last years have witnessed the emergence of a promising self-supervised learning strategy, referred to as masked autoencoding. However, there is a lack of theoretical understanding of how masking matters on graph autoencoders (GAEs). In this work, we present masked graph autoencoder (MaskGAE), a self-supervised learning framework for graph-structured data. Different from standard GAEs, MaskGAE adopts masked graph modeling (MGM) as a principled pretext task - masking a portion of edges and attempting to reconstruct the missing part with partially visible, unmasked graph structure. To understand whether MGM can help GAEs learn better representations, we provide both theoretical and empirical evidence to comprehensively justify the benefits of this pretext task. Theoretically, we establish close connections between GAEs and contrastive learning, showing that MGM significantly improves the self-supervised learning scheme of GAEs. Empirically, we conduct extensive experiments on a variety of graph benchmarks, demonstrating the superiority of MaskGAE over several state-of-the-arts on both link prediction and node classification tasks.|最近几年出现了一种很有前途的自我监督学习策略，称为掩蔽自动编码。然而，对于图形自动编码器(GAE)中的掩码问题，目前还缺乏理论上的认识。在这项工作中，我们提出了掩码图自动编码器(MaskGAE) ，一个自我监督学习框架的图结构化数据。与标准的 GAE 不同，MaskGAE 采用掩盖图建模(MGM)作为掩盖部分边的原则，并尝试用部分可见的、未掩盖的图结构重建缺失的部分。为了了解米高梅公司是否能够帮助 GAEs 学习更好的表述，我们提供了理论和经验证明来全面证明这项借口任务的好处。理论上，我们建立了遗传算法与对比学习之间的紧密联系，表明 MGM 算法显著改善了遗传算法的自监督学习方案。实验结果表明，MaskGAE 在链路预测和节点分类任务方面优于现有的几种算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=What's+Behind+the+Mask:+Understanding+Masked+Graph+Modeling+for+Graph+Autoencoders)|0|
|[OPORP: One Permutation + One Random Projection](https://doi.org/10.1145/3580305.3599457)|Ping Li, Xiaoyun Li|LinkedIn Ads|Consider two $D$-dimensional data vectors (e.g., embeddings): $u, v$. In many embedding-based retrieval (EBR) applications where the vectors are generated from trained models, $D=256\sim 1024$ are common. In this paper, OPORP (one permutation + one random projection) uses a variant of the ``count-sketch'' type of data structures for achieving data reduction/compression. With OPORP, we first apply a permutation on the data vectors. A random vector $r$ is generated i.i.d. with moments: $E(r_i) = 0, E(r_i^2)=1, E(r_i^3) =0, E(r_i^4)=s$. We multiply (as dot product) $r$ with all permuted data vectors. Then we break the $D$ columns into $k$ equal-length bins and aggregate (i.e., sum) the values in each bin to obtain $k$ samples from each data vector. One crucial step is to normalize the $k$ samples to the unit $l_2$ norm. We show that the estimation variance is essentially: $(s-1)A + \frac{D-k}{D-1}\frac{1}{k}\left[ (1-\rho^2)^2 -2A\right]$, where $A\geq 0$ is a function of the data ($u,v$). This formula reveals several key properties: (1) We need $s=1$. (2) The factor $\frac{D-k}{D-1}$ can be highly beneficial in reducing variances. (3) The term $\frac{1}{k}(1-\rho^2)^2$ is actually the asymptotic variance of the classical correlation estimator. We illustrate that by letting the $k$ in OPORP to be $k=1$ and repeat the procedure $m$ times, we exactly recover the work of ``very spars random projections'' (VSRP). This immediately leads to a normalized estimator for VSRP which substantially improves the original estimator of VSRP. In summary, with OPORP, the two key steps: (i) the normalization and (ii) the fixed-length binning scheme, have considerably improved the accuracy in estimating the cosine similarity, which is a routine (and crucial) task in modern embedding-based retrieval (EBR) applications.|考虑两个 $D $- 维数据向量(例如，嵌入) : $u，v $。在许多基于嵌入的检索(EBR)应用中，向量是由训练好的模型生成的，$D = 256 sim 1024 $是常见的。OPORP (一个置换 + 一个随机投影)采用“计数-示意图”类型的数据结构的一种变体来实现数据约简/压缩。使用 OPORP，我们首先对数据向量进行排列。生成一个随机向量 $r $，其矩为: $E (r _ i) = 0，E (r _ i ^ 2) = 1，E (r _ i ^ 3) = 0，E (r _ i ^ 4) = s $。我们用所有置换的数据向量乘以(作为点乘) $r $。然后，我们将 $D $列分解成 $k $等长的容器并聚合(即和)每个容器中的值，以从每个数据向量获取 $k $样本。一个关键的步骤是将 $k $样本标准化为单位 $l _ 2 $范数。我们证明了估计方差本质上是: $(s-1) A + frac { D-k }{ D-1} frac {1}{ k } left [(1-rho ^ 2) ^ 2 -2 A right ] $，其中 $A geq 0 $是数据的函数($u，v $)。这个公式揭示了几个关键性质: (1)我们需要 $s = 1 $。(2)因子 $frac { D-k }{ D-1} $在减少方差方面是非常有益的。(3)项 $frac {1}{ k }(1-rho ^ 2) ^ 2 $实际上是经典相关估计的渐近方差。我们举例说明，通过让 OPORP 中的 $k $为 $k = 1 $并重复过程 $m $次，我们可以精确地恢复“ very spars 随机投影”(VSRP)的工作。这立即导致了 VSRP 的归一化估计，从而大大改进了 VSRP 的原始估计。总之，使用 OPORP，两个关键步骤: (i)归一化和(ii)固定长度分组方案，大大提高了估计余弦距离的准确性，这是现代嵌入式检索(EBR)应用中的一个常规(和关键)任务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OPORP:+One+Permutation+++One+Random+Projection)|0|
|[Multi-Temporal Relationship Inference in Urban Areas](https://doi.org/10.1145/3580305.3599440)|Shuangli Li, Jingbo Zhou, Ji Liu, Tong Xu, Enhong Chen, Hui Xiong|The Hong Kong University of Science and Technology (Guangzhou); Baidu Research; University of Science and Technology of China|Finding multiple temporal relationships among locations can benefit a bunch of urban applications, such as dynamic offline advertising and smart public transport planning. While some efforts have been made on finding static relationships among locations, little attention is focused on studying time-aware location relationships. Indeed, abundant location-based human activities are time-varying and the availability of these data enables a new paradigm for understanding the dynamic relationships in a period among connective locations. To this end, we propose to study a new problem, namely multi-Temporal relationship inference among locations (Trial for short), where the major challenge is how to integrate dynamic and geographical influence under the relationship sparsity constraint. Specifically, we propose a solution to Trial with a graph learning scheme, which includes a spatially evolving graph neural network (SEENet) with two collaborative components: spatially evolving graph convolution module (SEConv) and spatially evolving self-supervised learning strategy (SE-SSL). SEConv performs the intra-time aggregation and inter-time propagation to capture the multifaceted spatially evolving contexts from the view of location message passing. In addition, SE-SSL designs time-aware self-supervised learning tasks in a global-local manner with additional evolving constraint to enhance the location representation learning and further handle the relationship sparsity. Finally, experiments on four real-world datasets demonstrate the superiority of our method over several state-of-the-art approaches.|寻找位置之间的多个时间关系可以有利于一系列城市应用，如动态离线广告和智能公共交通规划。虽然在寻找位置之间的静态关系方面已经做了一些努力，但是很少有人关注时间感知的位置关系的研究。事实上，丰富的基于位置的人类活动是随时间变化的，这些数据的可用性为理解连通位置之间一个时期的动态关系提供了一个新的范例。为此，我们提出了一个新的研究问题，即多时间地点间关系推理(简称“试用”) ，其主要挑战是如何在关系稀疏约束下整合动态和地理影响。具体来说，我们提出了一种基于图形学习方案的解决方案，该方案包括一个具有两个协作组件的空间演化图形神经网络(SEENet) : 空间演化图卷积模块(SEConv)和空间演化自监督学习策略(SE-SSL)。SEConv 执行时间内聚合和时间间传播，从位置消息传递的角度捕获多方面的空间演化上下文。此外，SE-SSL 以全局-局部方式设计具有时间感知的自监督学习任务，并附加进化约束，以增强位置表示学习，进一步处理关系稀疏性。最后，在四个真实世界数据集上的实验证明了我们的方法优于几种最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Temporal+Relationship+Inference+in+Urban+Areas)|0|
|[HomoGCL: Rethinking Homophily in Graph Contrastive Learning](https://doi.org/10.1145/3580305.3599380)|WenZhi Li, ChangDong Wang, Hui Xiong, JianHuang Lai|The Hong Kong University of Science and Technology (Guangzhou); Sun Yat-sen University|Contrastive learning (CL) has become the de-facto learning paradigm in self-supervised learning on graphs, which generally follows the "augmenting-contrasting" learning scheme. However, we observe that unlike CL in computer vision domain, CL in graph domain performs decently even without augmentation. We conduct a systematic analysis of this phenomenon and argue that homophily, i.e., the principle that "like attracts like", plays a key role in the success of graph CL. Inspired to leverage this property explicitly, we propose HomoGCL, a model-agnostic framework to expand the positive set using neighbor nodes with neighbor-specific significances. Theoretically, HomoGCL introduces a stricter lower bound of the mutual information between raw node features and node embeddings in augmented views. Furthermore, HomoGCL can be combined with existing graph CL models in a plug-and-play way with light extra computational overhead. Extensive experiments demonstrate that HomoGCL yields multiple state-of-the-art results across six public datasets and consistently brings notable performance improvements when applied to various graph CL methods. Code is avilable at https://github.com/wenzhilics/HomoGCL.|对比学习已成为图形自监督学习的事实学习范式，一般遵循“增强-对比”学习模式。然而，我们观察到，与计算机视觉领域的 CL 不同，图形领域的 CL 在没有增强的情况下仍然表现良好。我们对这一现象进行了系统的分析，认为同调性，即“相似吸引相似”的原则，在图 CL 的成功中起着关键作用。受此启发，我们提出了 HomoGCL，一个模型不可知的框架，使用具有邻居特定意义的邻居节点来扩展正集。理论上，HomoGCL 在增广视图中引入了更严格的原始节点特征和节点嵌入之间的互信息下界。此外，HomoGCL 还可以以即插即用的方式与现有的图形 CL 模型结合，从而减少额外的计算开销。大量的实验表明，HomoGCL 在六个公共数据集中产生了多个最先进的结果，并且在应用于各种图形 CL 方法时始终带来显著的性能改进。代码可在 https://github.com/wenzhilics/homogcl 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HomoGCL:+Rethinking+Homophily+in+Graph+Contrastive+Learning)|0|
|[Urban Region Representation Learning with OpenStreetMap Building Footprints](https://doi.org/10.1145/3580305.3599538)|Yi Li, Weiming Huang, Gao Cong, Hao Wang, Zheng Wang||The prosperity of crowdsourcing geospatial data provides increasing opportunities to understand our cities. In particular, OpenStreetMap (OSM) has become a prominent vault of geospatial data on the Web. In this context, learning urban region representations from OSM data, which is unexplored in previous work, could be profitable for various downstream tasks. In this work, we utilize OSM buildings (footprints) complemented with points of interest (POIs) to learn region representations, as buildings' shapes, spatial distributions, and properties have tight linkages to different urban functions. However, appealing as it seems, urban buildings often exhibit complex patterns to form dense or sparse areas, which brings significant challenges for unsupervised feature extraction. To address the challenges, we propose RegionDCL 1 , an unsupervised framework to deeply mine urban buildings. In a nutshell, we leverage random points generated by Poisson Disk Sampling to tackle data-sparse areas and utilize triplet loss with a novel adaptive margin to preserve inter-region correlations. Furthermore, we train our model with group-level and region-level contrastive learning, making it adaptive to varying region partitions. Extensive experiments in two global cities demonstrate that RegionDCL consistently outperforms the state-of-the-art counterparts across different region partitions, and outputs effective representations for inferring urban land use and population density.|众包地理空间数据的繁荣为理解我们的城市提供了越来越多的机会。特别是，OpenStreetMap (OSM)已经成为 Web 上突出的地理空间数据库。在这种情况下，从 OSM 数据中学习城市地区表示，这在以前的工作中没有得到探索，可能有利于各种下游任务。在这项工作中，我们利用 OSM 建筑物(足迹)与兴趣点(POI)互补来学习区域表示，因为建筑物的形状、空间分布和属性与不同的城市功能有紧密的联系。然而，城市建筑往往表现出复杂的模式，形成密集或稀疏的区域，这给无监督的特征提取带来了巨大的挑战。为了应对这些挑战，我们提出了“区域 DCL 1”，这是一个深度挖掘城市建筑的无监督框架。简而言之，我们利用泊松磁盘采样产生的随机点来处理数据稀疏区域，并利用具有新的自适应边界的三元组损失来保持区域间的相关性。此外，我们利用群组层次和区域层次的对比学习训练模型，使其能够适应不同的区域划分。在两个全球城市进行的大量实验表明，RegionDCL 在不同区域划分方面的表现一直优于最先进的对应方，并输出推断城市土地利用和人口密度的有效表示。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Urban+Region+Representation+Learning+with+OpenStreetMap+Building+Footprints)|0|
|[Machine Unlearning in Gradient Boosting Decision Trees](https://doi.org/10.1145/3580305.3599420)|Huawei Lin, Jun Woo Chung, Yingjie Lao, Weijie Zhao||Various machine learning applications take users' data to train the models. Recently enforced legislation requires companies to remove users' data upon requests, i.e.,the right to be forgotten. In the context of machine learning, the trained model potentially memorizes the training data. Machine learning algorithms have to be able to unlearn the user data that are requested to delete to meet the requirement. Gradient Boosting Decision Trees (GBDT) is a widely deployed model in many machine learning applications. However, few studies investigate the unlearning on GBDT. This paper proposes a novel unlearning framework for GBDT. To the best of our knowledge, this is the first work that considers machine unlearning on GBDT. It is not straightforward to transfer the unlearning methods of DNN to GBDT settings. We formalized the machine unlearning problem and its relaxed version. We propose an unlearning framework that efficiently and effectively unlearns a given collection of data without retraining the model from scratch. We introduce a collection of techniques, including random split point selection and random partitioning layers training, to the training process of the original tree models to ensure that the trained model requires few subtree retrainings during the unlearning. We investigate the intermediate data and statistics to store as an auxiliary data structure during the training so that we can immediately determine if a subtree is required to be retrained without touching the original training dataset. Furthermore, a lazy update technique is proposed as a trade-off between unlearning time and model functionality. We experimentally evaluate our proposed methods on public datasets. The empirical results confirm the effectiveness of our framework.|各种机器学习应用程序采用用户数据对模型进行训练。最近实施的法律要求公司在用户提出要求时删除用户的数据，例如，被遗忘的权利。在机器学习的背景下，训练模型潜在地记忆训练数据。机器学习算法必须能够忘记被要求删除的用户数据，以满足需求。梯度提升决策树(GBDT)是一种广泛应用于许多机器学习应用程序的模型。然而，很少有研究探讨对 GBDT 的忘记。本文提出了一种新的 GBDT 忘却学习框架。据我们所知，这是第一个工作，考虑机器忘却 GBDT。将 DNN 的去学习方法转换为 GBDT 设置并不简单。我们正式化了机器学习问题及其松弛版本。我们提出了一个忘却学习框架，它可以有效地忘却给定的数据集，而不需要从头开始重新训练模型。在原始树模型的训练过程中引入了随机分割点选择和随机分割层训练等技术，以保证训练后的模型在去学习过程中只需要很少的子树再训练。我们研究了中间数据和统计信息作为辅助数据结构存储在训练过程中，以便我们可以立即确定是否需要一个子树重新训练，而不需要接触原始的训练数据集。此外，提出了一种延迟更新技术，作为取消学习时间和模型功能之间的权衡。我们在公共数据集上实验性地评估我们提出的方法。实证结果证实了该框架的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Machine+Unlearning+in+Gradient+Boosting+Decision+Trees)|0|
|[Fire: An Optimization Approach for Fast Interpretable Rule Extraction](https://doi.org/10.1145/3580305.3599353)|Brian Liu, Rahul Mazumder|Massachusetts Institute of Technology|We present FIRE, Fast Interpretable Rule Extraction, an optimization-based framework to extract a small but useful collection of decision rules from tree ensembles. FIRE selects sparse representative subsets of rules from tree ensembles, that are easy for a practitioner to examine. To further enhance the interpretability of the extracted model, FIRE encourages fusing rules during selection, so that many of the selected decision rules share common antecedents. The optimization framework utilizes a fusion regularization penalty to accomplish this, along with a non-convex sparsity-inducing penalty to aggressively select rules. Optimization problems in FIRE pose a challenge to off-the-shelf solvers due to problem scale and the non-convexity of the penalties. To address this, making use of problem-structure, we develop a specialized solver based on block coordinate descent principles; our solver performs up to 40x faster than existing solvers. We show in our experiments that FIRE outperforms state-of-the-art rule ensemble algorithms at building sparse rule sets, and can deliver more interpretable models compared to existing methods.|我们提出 FIRE，快速解释规则提取，一个基于优化的框架，提取一个小但有用的决策规则集合从树集合。FIRE 从树集合中选择稀疏的代表性规则子集，这对于从业者来说很容易检查。为了进一步提高提取模型的可解释性，FIRE 鼓励在选择过程中融合规则，使得所选择的决策规则具有共同的前因。优化框架利用融合正则化惩罚来实现这一点，同时利用非凸稀疏诱导惩罚来积极选择规则。FIRE 中的优化问题由于问题的规模和处罚的非凸性而对现成的求解器提出了挑战。为了解决这个问题，利用问题结构，我们开发了一个基于块坐标下降法原则的专业解决方案，我们的解决方案比现有的解决方案快40倍。在实验中，我们发现 FIRE 在构建稀疏规则集方面优于最先进的规则集合算法，并且与现有的方法相比，它能够提供更多可解释的模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fire:+An+Optimization+Approach+for+Fast+Interpretable+Rule+Extraction)|0|
|[Robust Spatiotemporal Traffic Forecasting with Reinforced Dynamic Adversarial Training](https://doi.org/10.1145/3580305.3599492)|Fan Liu, Weijia Zhang, Hao Liu|The Hong Kong University of Science and Technology (Guangzhou)|Machine learning-based forecasting models are commonly used in Intelligent Transportation Systems (ITS) to predict traffic patterns and provide city-wide services. However, most of the existing models are susceptible to adversarial attacks, which can lead to inaccurate predictions and negative consequences such as congestion and delays. Therefore, improving the adversarial robustness of these models is crucial for ITS. In this paper, we propose a novel framework for incorporating adversarial training into spatiotemporal traffic forecasting tasks. We demonstrate that traditional adversarial training methods designated for static domains cannot be directly applied to traffic forecasting tasks, as they fail to effectively defend against dynamic adversarial attacks. Then, we propose a reinforcement learning-based method to learn the optimal node selection strategy for adversarial examples, which simultaneously strengthens the dynamic attack defense capability and reduces the model overfitting. Additionally, we introduce a self-knowledge distillation regularization module to overcome the "forgetting issue" caused by continuously changing adversarial nodes during training. We evaluate our approach on two real-world traffic datasets and demonstrate its superiority over other baselines. Our method effectively enhances the adversarial robustness of spatiotemporal traffic forecasting models. The source code for our framework is available at https://github.com/usail-hkust/RDAT.|基于机器学习的预测模型是智能交通系统(ITS)中常用的预测交通模式和提供城市服务的模型。然而，现有的大多数模型容易受到对抗性攻击，这可能导致不准确的预测和负面后果，如拥挤和延迟。因此，提高这些模型的对抗鲁棒性是智能交通系统的关键。在本文中，我们提出了一个新的框架结合对抗训练的时空交通预测任务。我们证明了传统的针对静态域的对抗性训练方法不能直接应用于流量预测任务，因为它们不能有效地防御动态对抗性攻击。然后，提出了一种基于强化学习的方法来学习对手实例的最优节点选择策略，同时增强了动态攻击防御能力，减少了模型的过拟合。此外，为了克服训练过程中对手节点不断变化所引起的“遗忘问题”，本文还引入了自知识精馏正则化模型。我们评估了我们的方法在两个真实世界的交通数据集，并证明了其优势超过其他基线。该方法有效地增强了时空流量预测模型的对抗鲁棒性。我们框架的源代码可在 https://github.com/usail-hkust/rdat 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Spatiotemporal+Traffic+Forecasting+with+Reinforced+Dynamic+Adversarial+Training)|0|
|[Discovering Dynamic Causal Space for DAG Structure Learning](https://doi.org/10.1145/3580305.3599309)|Fangfu Liu, Wenchang Ma, An Zhang, Xiang Wang, Yueqi Duan, TatSeng Chua|University of Science and Technology of China; National University of Singapore; Tsinghua University|Discovering causal structure from purely observational data (i.e., causal discovery), aiming to identify causal relationships among variables, is a fundamental task in machine learning. The recent invention of differentiable score-based DAG learners is a crucial enabler, which reframes the combinatorial optimization problem into a differentiable optimization with a DAG constraint over directed graph space. Despite their great success, these cutting-edge DAG learners incorporate DAG-ness independent score functions to evaluate the directed graph candidates, lacking in considering graph structure. As a result, measuring the data fitness alone regardless of DAG-ness inevitably leads to discovering suboptimal DAGs and model vulnerabilities. Towards this end, we propose a dynamic causal space for DAG structure learning, coined CASPER, that integrates the graph structure into the score function as a new measure in the causal space to faithfully reflect the causal distance between estimated and ground truth DAG. CASPER revises the learning process as well as enhances the DAG structure learning via adaptive attention to DAG-ness. Grounded by empirical visualization, CASPER, as a space, satisfies a series of desired properties, such as structure awareness and noise robustness. Extensive experiments on both synthetic and real-world datasets clearly validate the superiority of our CASPER over the state-of-the-art causal discovery methods in terms of accuracy and robustness.|从纯观察数据中发现因果结构(即因果发现) ，目的是确定变量之间的因果关系，是机器学习的基本任务。最近发明的基于可微分数的 DAG 学习器是一个关键的推动者，它将组合优化问题重新定义为在有向图空间上带有 DAG 约束的可微优化问题。这些前沿的 DAG 学习者虽然取得了很大的成功，但是他们缺乏对图结构的考虑，因此在评价有向图候选者时引入了独立的 DAG 评分函数。因此，单独测量数据适应性而不考虑 DAG 的性质，不可避免地会发现次优的 DAG 和模型漏洞。为此，我们提出了 DAG 结构学习的动态因果空间，称为 CASPER，它将图结构集成到评分函数中，作为因果空间中的一个新度量，以忠实地反映估计的 DAG 和地面真值之间的因果距离。CASPER 通过对 DAG 性质的自适应注意，修正了学习过程，提高了 DAG 结构的学习效率。CASPER 作为一个空间，以经验可视化为基础，满足结构感知和噪声鲁棒性等一系列要求。在合成和真实世界数据集上的大量实验清楚地验证了我们的 CASPER 在准确性和鲁棒性方面优于最先进的因果发现方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Discovering+Dynamic+Causal+Space+for+DAG+Structure+Learning)|0|
|[Semi-Supervised Graph Imbalanced Regression](https://doi.org/10.1145/3580305.3599497)|Gang Liu, Tong Zhao, Eric Inae, Tengfei Luo, Meng Jiang|Snap Inc.; University of Notre Dame|Data imbalance is easily found in annotated data when the observations of certain continuous label values are difficult to collect for regression tasks. When they come to molecule and polymer property predictions, the annotated graph datasets are often small because labeling them requires expensive equipment and effort. To address the lack of examples of rare label values in graph regression tasks, we propose a semi-supervised framework to progressively balance training data and reduce model bias via self-training. The training data balance is achieved by (1) pseudo-labeling more graphs for under-represented labels with a novel regression confidence measurement and (2) augmenting graph examples in latent space for remaining rare labels after data balancing with pseudo-labels. The former is to identify quality examples from unlabeled data whose labels are confidently predicted and sample a subset of them with a reverse distribution from the imbalanced annotated data. The latter collaborates with the former to target a perfect balance using a novel label-anchored mixup algorithm. We perform experiments in seven regression tasks on graph datasets. Results demonstrate that the proposed framework significantly reduces the error of predicted graph properties, especially in under-represented label areas.|当某些连续标号值的观测值难以收集用于回归任务时，注释数据中很容易出现数据不平衡。当涉及到分子和聚合物特性预测时，注释图表数据集通常很小，因为标记它们需要昂贵的设备和努力。针对图形回归任务中缺乏稀有标号值的问题，提出了一种半监督框架，通过自学习逐步平衡训练数据，减少模型偏差。训练数据的平衡是通过: (1)使用一种新的回归置信度度量方法对未被充分表示的标签进行伪标记，从而获得更多的图; (2)使用伪标记进行数据平衡后，在潜在空间中增加剩余稀有标签的图示例。前者是从标签可信预测的未标记数据中识别出高质量样本，并从不平衡的注释数据中采用逆向分布对其子集进行抽样。后者与前者协作，使用一种新的标签锚定混合算法来实现完美的平衡。我们在图形数据集上进行七个回归任务的实验。结果表明，该框架显著降低了预测图性质的误差，特别是在表示不足的标号区域。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semi-Supervised+Graph+Imbalanced+Regression)|0|
|[Enhancing Graph Representations Learning with Decorrelated Propagation](https://doi.org/10.1145/3580305.3599334)|Hua Liu, Haoyu Han, Wei Jin, Xiaorui Liu, Hui Liu|Michigan State University; North Carolina State University; Shandong University|In recent years, graph neural networks (GNNs) have been widely used in many domains due to their powerful capability in representation learning on graph-structured data. While a majority of extant studies focus on mitigating the over-smoothing problem, recent works also reveal the limitation of GNN from a new over-correlation perspective which states that the learned representation becomes highly correlated after feature transformation and propagation in GNNs. In this paper, we thoroughly re-examine the issue of over-correlation in deep GNNs, both empirically and theoretically. We demonstrate that the propagation operator in GNNs exacerbates the feature correlation. In addition, we discovered through empirical study that existing decorrelation solutions fall short of maintaining a low feature correlation, potentially encoding redundant information. Thus, to more effectively address the over-correlation problem, we propose a decorrelated propagation scheme (DeProp) as a fundamental component to decorrelate the feature learning in GNN models, which achieves feature decorrelation at the propagation step. Comprehensive experiments on multiple real-world datasets demonstrate that DeProp can be easily integrated into prevalent GNNs, leading to significant performance enhancements. Furthermore, we find that it can be used to solve over-smoothing and over-correlation problems simultaneously and significantly outperform state-of-the-art methods on missing feature settings. The code is available at https://github.com/hualiu829/DeProp.|近年来，图神经网络(GNN)由于其对图结构数据的表示学习能力强大，在许多领域得到了广泛的应用。虽然大多数现存的研究集中在缓解过于平滑的问题，最近的工作也揭示了限制 GNN 从一个新的过度相关的观点，即学习表示成为高度相关后，特征转换和传播 GNN。本文从实证和理论两个方面对深度 GNN 中的过度相关问题进行了重新审视。我们证明了 GNN 中的传播算子加剧了特征相关性。此外，我们通过实证研究发现，现有的去相关解决方案不能保持较低的特征相关性，存在编码冗余信息的潜在可能。因此，为了更有效地解决过相关问题，我们提出了一种去相关传播方案(DeProp)作为 GNN 模型中特征学习去相关的基本组成部分，在传播过程中实现特征去相关。对多个真实世界数据集的综合实验表明，DeProp 可以很容易地集成到流行的 GNN 中，从而显著提高性能。此外，我们发现它可以同时解决过平滑和过相关问题，并且在缺失特征设置方面显著优于最先进的方法。密码可在 https://github.com/hualiu829/deprop 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Graph+Representations+Learning+with+Decorrelated+Propagation)|0|
|[Guiding Mathematical Reasoning via Mastering Commonsense Formula Knowledge](https://doi.org/10.1145/3580305.3599375)|Jiayu Liu, Zhenya Huang, Zhiyuan Ma, Qi Liu, Enhong Chen, Tianhuang Su, Haifeng Liu||Math formulas (e.g., "distance = speed X time'') serve as one of the fundamental commonsense knowledge in human cognition, where humans naturally acquire and manipulate them in logical thinking for mathematical reasoning problems. However, existing reasoning models mainly focus on learning heuristic linguistics or patterns to generate answers, but do not pay enough attention on learning with such formula knowledge. Thus, they are not transparent (thus uninterpretable) in terms of understanding and grasping basic mathematical logic. In this paper, to promote a step forward in the domain, we first construct two datasets (Math23K-F and MAWPS-F) with precise annotations of formula usage in each reasoning step for math word problems. Especially, our datasets are refined on the benchmark datasets, and thus ensure the generality and comparability for relevant research. Then, we propose a novel Formula-mastered Solver (FOMAS) with the guidance of mastering formula knowledge to solve the problems. Specifically, we establish FOMAS with two systems drawing insight from the dual process theory, including a Knowledge System and a Reasoning System, to learn and apply formula knowledge, respectively. The Knowledge System accumulates the math formulas, where we propose a novel pretraining manner to mimic how humans grasp the mathematical logic behind them. Then, in the Reasoning System, we develop elaborate formula-guided symbol prediction and goal generation methods that retrieve the necessary formula knowledge from Knowledge System to improve both reasoning accuracy and interpretability. It organically simulates how humans conduct complex reasoning under the explicit instruction of math formulas. Experimental results prove that FOMAS has a stronger reasoning ability and achieves a more interpretable reasoning process, which verifies the necessity of introducing formula knowledge transparently.|数学公式(例如“距离 = 速度 X 时间”)是人类认知的基本常识之一，人类在数学推理问题的逻辑思维中自然地获得和操纵这些常识。然而，现有的推理模型主要集中在学习启发式语言学或模式来产生答案，而没有充分重视利用这种公式知识进行学习。因此，它们在理解和掌握基本数学逻辑方面是不透明的(因此是不可解释的)。本文首先构建了两个数据集(Math23K-F 和 MAWPS-F) ，并对数学词汇问题的每个推理步骤的公式用法进行了精确的注释，以推动数学词汇问题领域的进一步发展。特别是在基准数据集上对数据集进行了细化，从而保证了相关研究的通用性和可比性。然后，我们提出了一种新的公式掌握求解器(FOMAS) ，以掌握公式知识为指导来解决这些问题。具体来说，我们建立 FOMAS 的两个系统，其中包括知识系统和推理系统，分别学习和应用公式知识。知识系统积累了数学公式，在这里我们提出了一种新的预训练方式来模拟人类如何掌握背后的数学逻辑。然后，在推理系统中，开发了公式引导的符号预测和目标生成方法，从知识系统中检索必要的公式知识，以提高推理的精度和可解释性。它有机地模拟了人类如何在数学公式的明确指导下进行复杂的推理。实验结果表明，FOMAS 具有较强的推理能力，推理过程更具可解释性，验证了公式知识透明引入的必要性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Guiding+Mathematical+Reasoning+via+Mastering+Commonsense+Formula+Knowledge)|0|
|[Using Motif Transitions for Temporal Graph Generation](https://doi.org/10.1145/3580305.3599540)|Penghang Liu, Ahmet Erdem Sariyüce||Graph generative models are highly important for sharing surrogate data and benchmarking purposes. Real-world complex systems often exhibit dynamic nature, where the interactions among nodes change over time in the form of a temporal network. Most temporal network generation models extend the static graph generation models by incorporating temporality in the generation process. More recently, temporal motifs are used to generate temporal networks with better success. However, existing models are often restricted to a small set of predefined motif patterns due to the high computational cost of counting temporal motifs. In this work, we develop a practical temporal graph generator, Motif Transition Model (MTM), to generate synthetic temporal networks with realistic global and local features. Our key idea is modeling the arrival of new events as temporal motif transition processes. We first calculate the transition properties from the input graph and then simulate the motif transition processes based on the transition probabilities and transition rates. We demonstrate that our model consistently outperforms the baselines with respect to preserving various global and local temporal graph statistics and runtime performance.|图形生成模型对于共享代理数据和基准测试非常重要。现实世界中的复杂系统往往表现出动态特性，其中节点之间的交互作用随时间以时间网络的形式发生变化。大多数时态网络生成模型通过在生成过程中引入时态性来扩展静态图生成模型。最近，时间模式被用来生成更好的时间网络。然而，由于计算时间模式的计算成本较高，现有模型往往局限于一小组预定义的模式。在这项工作中，我们开发了一个实用的时态图生成器，基元转换模型(MTM) ，以生成具有真实的全局和局部特征的合成时态网络。我们的关键思想是将新事件的到来建模为时序转换过程。我们首先从输入图中计算出转换特性，然后基于转换概率和转换速率模拟基序转换过程。我们证明了我们的模型在保持各种全局和局部时态图统计和运行时性能方面始终优于基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Using+Motif+Transitions+for+Temporal+Graph+Generation)|0|
|[Fairness-Aware Continuous Predictions of Multiple Analytics Targets in Dynamic Networks](https://doi.org/10.1145/3580305.3599341)|Ruifeng Liu, Qu Liu, Tingjian Ge||We study a novel problem of continuously predicting a number of user-subscribed continuous analytics targets (CATs) in dynamic networks. Our architecture includes any dynamic graph neural network model as the back end applied over the network data, and per CAT front end models that return results with their confidence to users. We devise a data filtering algorithm that feeds a provably optimal subset of data in the embedding space from back end model to front end models. Secondly, to ensure fairness in terms of query result accuracy for different CATs and users, we propose a fairness metric and a fairness-aware training scheduling algorithm, along with accuracy guarantees on fairness estimation. Our experiments over five real-world datasets show that our proposed solution is effective, efficient, fair, extensible, and adaptive.|研究动态网络中用户订阅的连续分析目标(CATs)的连续预测问题。我们的体系结构包括任何应用于网络数据的后端的动态图形神经网络模型，以及每个 CAT 前端模型，这些模型可以将结果返回给用户。我们设计了一种数据过滤算法，它将嵌入空间中可证明的最优数据子集从后端模型提供给前端模型。其次，为了保证不同的 CATs 和用户在查询结果准确性方面的公平性，提出了一种公平性度量和公平感知的训练调度算法，以及公平性估计的准确性保证。我们在五个真实世界数据集上的实验表明，我们提出的解决方案是有效的、高效的、公平的、可扩展的和自适应的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fairness-Aware+Continuous+Predictions+of+Multiple+Analytics+Targets+in+Dynamic+Networks)|0|
|[Decoupled Rationalization with Asymmetric Learning Rates: A Flexible Lipschitz Restraint](https://doi.org/10.1145/3580305.3599299)|Wei Liu, Jun Wang, Haozhao Wang, Ruixuan Li, Yang Qiu, Yuankai Zhang, Jie Han, Yixiong Zou|iWudao.tech; School of Computer Science and Technology, Huazhong University of Science and Technology|A self-explaining rationalization model is generally constructed by a cooperative game where a generator selects the most human-intelligible pieces from the input text as rationales, followed by a predictor that makes predictions based on the selected rationales. However, such a cooperative game may incur the degeneration problem where the predictor overfits to the uninformative pieces generated by a not yet well-trained generator and in turn, leads the generator to converge to a sub-optimal model that tends to select senseless pieces. In this paper, we theoretically bridge degeneration with the predictor's Lipschitz continuity. Then, we empirically propose a simple but effective method named DR, which can naturally and flexibly restrain the Lipschitz constant of the predictor, to address the problem of degeneration. The main idea of DR is to decouple the generator and predictor to allocate them with asymmetric learning rates. A series of experiments conducted on two widely used benchmarks have verified the effectiveness of the proposed method. Codes: https://github.com/jugechengzi/Rationalization-DR.|一个自我解释的合理化模型通常是由一个合作博弈构建的，其中一个生成器从输入文本中选择最人类可理解的部分作为原理，然后一个预测器根据选择的原理做出预测。然而，这样的合作博弈可能会引起退化问题，预测器会过度适应由未经良好训练的发生器产生的无信息的部分，从而导致发生器收敛到一个次优模型，倾向于选择无意义的部分。在本文中，我们从理论上将退化与预测器的利普希茨连续联系起来。在此基础上，本文提出了一种简单而有效的预测方法 DR，该方法可以自然而灵活地抑制预测器的 Lipschitz 常数，从而解决预测器退化的问题。DR 的主要思想是解耦生成器和预测器，以非对称的学习速率分配它们。在两个广泛使用的基准上进行了一系列实验，验证了该方法的有效性。密码:  https://github.com/jugechengzi/rationalization-dr。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Decoupled+Rationalization+with+Asymmetric+Learning+Rates:+A+Flexible+Lipschitz+Restraint)|0|
|[FLOOD: A Flexible Invariant Learning Framework for Out-of-Distribution Generalization on Graphs](https://doi.org/10.1145/3580305.3599355)|Yang Liu, Xiang Ao, Fuli Feng, Yunshan Ma, Kuan Li, TatSeng Chua, Qing He|Institute of Computing Technology, CAS|Graph Neural Networks (GNNs) have achieved remarkable success in various domains but most of them are developed under the in-distribution assumption. Under out-of-distribution (OOD) settings, they suffer from the distribution shift between the training set and the test set and may not generalize well to the test distribution. Several methods have tried the invariance principle to improve the generalization of GNNs in OOD settings. However, in previous solutions, the graph encoder is immutable after the invariant learning and cannot be adapted to the target distribution flexibly. Confronting the distribution shift, a flexible encoder with refinement to the target distribution can generalize better on the test set than the stable invariant encoder. To remedy these weaknesses, we propose a Flexible invariant Learning framework for Out-Of-Distribution generalization on graphs (FLOOD), which comprises two key components, invariant learning and bootstrapped learning. The invariant learning component constructs multiple environments from graph data augmentation and learns invariant representation under risk extrapolation. Besides, the bootstrapped learning component is devised to be trained in a self-supervised way with a shared graph encoder with the invariant learning part. During the test phase, the shared encoder is flexible to be refined with the bootstrapped learning on the test set. Extensive experiments are conducted for both transductive and inductive node classification tasks. The results demonstrate that FLOOD consistently outperforms other graph OOD generalization methods and effectively improves the generalization ability.|图神经网络(GNN)在各个领域都取得了显著的成功，但大多数是在内分布假设下发展起来的。在分布外(OOD)设置下，它们受到训练集和测试集之间分布偏移的影响，并且可能不能很好地推广到测试分布。有几种方法尝试了不变性原理来提高 GNN 在面向对象设置中的推广能力。然而，在以往的解决方案中，图形编码器在不变学习后是不变的，不能灵活地适应目标分布。面对分布偏移，对目标分布进行细化的柔性编码器比稳定不变编码器能更好地在测试集上进行泛化。针对这些不足，提出了一种基于柔性不变学习的图分布外泛化框架(FLOOD) ，该框架包括不变学习和自举学习两个关键部分。不变学习组件通过图形数据增强构造多个环境，并在风险外推下学习不变表示。此外，设计了自举学习组件，使用具有不变学习部分的共享图形编码器进行自监督训练。在测试阶段，共享编码器可以灵活地通过测试集上的自举学习进行细化。广泛的实验进行了转导和归纳节点分类任务。结果表明，FLOOD 方法的泛化性能优于其他图形 OOD 方法，有效地提高了泛化能力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FLOOD:+A+Flexible+Invariant+Learning+Framework+for+Out-of-Distribution+Generalization+on+Graphs)|0|
|[QTIAH-GNN: Quantity and Topology Imbalance-aware Heterogeneous Graph Neural Network for Bankruptcy Prediction](https://doi.org/10.1145/3580305.3599479)|Yucheng Liu, Zipeng Gao, Xiangyang Liu, Pengfei Luo, Yang Yang, Hui Xiong||The timely prediction of bankruptcy is highly desirable to guarantee an upward spiral for overall societal well-being. By extracting multifaceted information from the business interaction networks, Graph Neural Networks (GNNs) may be able to automatically make more informed predictions for bankruptcy, as compared to methods that rely heavily on abundant manpower to a large extent. Yet in real applications, bankruptcy prediction faces the key issue of quantity-imbalance: data usually comes with a long-tailed distribution wherein bankrupt corporates occupy the least of the data proportion but are our target to be identified. Apart from that, the topology-imbalance issue behind graph-structural data exacerbates prediction deterioration: feature propagation is dominated by non-bankrupt nodes through messages passing between nodes; thus, bankrupt nodes receive highly confusing information and could be easily assimilated by nearby non-bankrupt nodes. Unfortunately, the existing GNN methods are not immune to these two imbalance issues. To tackle the challenging but practically useful scenario, we propose a novel bankruptcy prediction model called the Quantity and Topology Imbalance-Aware Heterogeneous Graph Neural Network (QTIAH-GNN) to boost the final performance. Specifically, QTIAH-GNN employs the multi-hierarchy label-aware neighbor selection to conquer the topology-imbalance issue by using the class-semantic representation and the learnable parameterized similarity metric, and employs the imbalance-oriented loss to obtain the optimal tradeoff between the accuracies of the majority and minority classes. In experiments, we evaluate the proposed QTIAH-GNN on two large-scale, real-world datasets. The results show that QTIAH-GNN outperforms other state-of-the-art baselines in terms of prediction accuracy with superior efficiency and generalization ability, has stronger robustness to data imbalance, and provides meaningful model interpretation.|及时预测破产是非常可取的，以保证整个社会福祉的螺旋式上升。通过从商业交互网络中提取多方面的信息，与在很大程度上依赖于大量人力资源的方法相比，图形神经网络(GNN)可以自动地对破产进行更为明智的预测。然而在实际应用中，破产预测面临着数量不平衡的关键问题: 数据通常具有长尾分布，其中破产企业占据的数据比例最小，但却是我们要确定的目标。此外，图结构数据背后的拓扑不平衡问题加剧了预测的恶化: 特征传播主要由非破产节点通过节点之间的信息传递进行，因此破产节点接收到的信息非常混乱，容易被附近的非破产节点同化。不幸的是，现有的 GNN 方法并不能免疫这两个不平衡问题。为了解决这一具有挑战性但实际有用的情况，我们提出了一种新的破产预测模型，称为数量和拓扑不平衡感知异构图神经网络(QTIAH-GNN) ，以提高最终的性能。特别地，QTIAH-GNN 利用类语义表示和可学习的参数化相似性度量，采用多层次标签感知邻居选择来克服拓扑不平衡问题，并利用不平衡导向损失来获得多数类和少数类精度之间的最优折衷。在实验中，我们评估了所提出的 QTIAH-GNN 在两个大规模，真实世界的数据集。结果表明，QTIAH-GNN 在预测精度和泛化能力方面优于其他最先进的基线，对数据不平衡具有较强的鲁棒性，并提供了有意义的模型解释。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=QTIAH-GNN:+Quantity+and+Topology+Imbalance-aware+Heterogeneous+Graph+Neural+Network+for+Bankruptcy+Prediction)|0|
|[Multi-Grained Multimodal Interaction Network for Entity Linking](https://doi.org/10.1145/3580305.3599439)|Pengfei Luo, Tong Xu, Shiwei Wu, Chen Zhu, Linli Xu, Enhong Chen|BOSS Zhipin; University of Science and Technology of China|Multimodal entity linking (MEL) task, which aims at resolving ambiguous mentions to a multimodal knowledge graph, has attracted wide attention in recent years. Though large efforts have been made to explore the complementary effect among multiple modalities, however, they may fail to fully absorb the comprehensive expression of abbreviated textual context and implicit visual indication. Even worse, the inevitable noisy data may cause inconsistency of different modalities during the learning process, which severely degenerates the performance. To address the above issues, in this paper, we propose a novel Multi-GraIned Multimodal InteraCtion Network $\textbf{(MIMIC)}$ framework for solving the MEL task. Specifically, the unified inputs of mentions and entities are first encoded by textual/visual encoders separately, to extract global descriptive features and local detailed features. Then, to derive the similarity matching score for each mention-entity pair, we device three interaction units to comprehensively explore the intra-modal interaction and inter-modal fusion among features of entities and mentions. In particular, three modules, namely the Text-based Global-Local interaction Unit (TGLU), Vision-based DuaL interaction Unit (VDLU) and Cross-Modal Fusion-based interaction Unit (CMFU) are designed to capture and integrate the fine-grained representation lying in abbreviated text and implicit visual cues. Afterwards, we introduce a unit-consistency objective function via contrastive learning to avoid inconsistency and model degradation. Experimental results on three public benchmark datasets demonstrate that our solution outperforms various state-of-the-art baselines, and ablation studies verify the effectiveness of designed modules.|多模态实体连接任务是近年来受到广泛关注的一种解决多模态知识图中模糊提及问题的任务。尽管人们在探索多种语言形式之间的互补效应方面做出了很大的努力，但是，它们可能无法充分吸收语篇缩略语境和隐含视觉表征的综合表达。更糟糕的是，在学习过程中，不可避免的噪声数据可能导致不同模式的不一致性，从而严重影响学习效果。针对上述问题，本文提出了一种新的多粒度多模式交互网络解决 MEL 任务的框架。具体来说，提及和实体的统一输入首先由文本/可视化编码器分别进行编码，以提取全局描述特征和局部详细特征。然后，为了得到每个提及实体对的相似性匹配得分，我们设计了三个交互单元来全面探索实体和提及特征之间的模式内交互和模式间融合。特别是基于文本的全局-局部交互单元(TGLU)、基于视觉的双向交互单元(VDLU)和基于交叉模态融合的交互单元(CMFU)这三个模块被设计用来捕获和整合缩略文本和隐式视觉线索中的细粒度表示。然后，通过对比学习引入单位一致性目标函数，避免了不一致性和模型退化。在三个公共基准数据集上的实验结果表明，我们的解决方案优于各种最先进的基准，烧蚀研究验证了所设计模块的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Grained+Multimodal+Interaction+Network+for+Entity+Linking)|0|
|[Learning Strong Graph Neural Networks with Weak Information](https://doi.org/10.1145/3580305.3599410)|Yixin Liu, Kaize Ding, Jianling Wang, Vincent C. S. Lee, Huan Liu, Shirui Pan|Griffith University; Texas AM University; Arizona State University; Monash University|Graph Neural Networks (GNNs) have exhibited impressive performance in many graph learning tasks. Nevertheless, the performance of GNNs can deteriorate when the input graph data suffer from weak information, i.e., incomplete structure, incomplete features, and insufficient labels. Most prior studies, which attempt to learn from the graph data with a specific type of weak information, are far from effective in dealing with the scenario where diverse data deficiencies exist and mutually affect each other. To fill the gap, in this paper, we aim to develop an effective and principled approach to the problem of graph learning with weak information (GLWI). Based on the findings from our empirical analysis, we derive two design focal points for solving the problem of GLWI, i.e., enabling long-range propagation in GNNs and allowing information propagation to those stray nodes isolated from the largest connected component. Accordingly, we propose D$^2$PT, a dual-channel GNN framework that performs long-range information propagation not only on the input graph with incomplete structure, but also on a global graph that encodes global semantic similarities. We further develop a prototype contrastive alignment algorithm that aligns the class-level prototypes learned from two channels, such that the two different information propagation processes can mutually benefit from each other and the finally learned model can well handle the GLWI problem. Extensive experiments on eight real-world benchmark datasets demonstrate the effectiveness and efficiency of our proposed methods in various GLWI scenarios.|图形神经网络(GNN)在许多图形学习任务中表现出令人印象深刻的性能。然而，当输入图数据的结构不完整、特征不完整、标签不充分等信息不充分时，GNN 的性能会下降。大多数先前的研究试图从具有特定类型的薄弱信息的图表数据中学习，但在处理存在不同数据缺陷并相互影响的情况方面远远不够有效。为了填补这一空白，本文旨在开发一种有效的原则性方法来解决弱信息图学习问题(GLWI)。基于我们的实证分析结果，我们得出了解决 GLWI 问题的两个设计重点，即在 GNN 中实现远程传播和允许信息传播到那些从最大的连接元件(图论)中隔离出来的杂散节点。因此，我们提出了一个双通道 GNN 框架 D $^ 2 $PT，它不仅在结构不完整的输入图上进行远程信息传播，而且在编码全局语义相似性的全局图上进行远程信息传播。我们进一步开发了一个原型对比对齐算法，该算法对齐了从两个通道学习的类级原型，使得两个不同的信息传播过程能够相互受益，最终学习的模型能够很好地处理 GLWI 问题。在八个真实世界的基准数据集上的大量实验证明了我们提出的方法在不同 GLWI 场景下的有效性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Strong+Graph+Neural+Networks+with+Weak+Information)|0|
|[Augmenting Recurrent Graph Neural Networks with a Cache](https://doi.org/10.1145/3580305.3599260)|Guixiang Ma, Vy A. Vo, Theodore L. Willke, Nesreen K. Ahmed||While graph neural networks (GNNs) provide a powerful way to learn structured representations, it remains challenging to learn long-range dependencies in graphs. Recurrent GNNs only partly address this problem. In this paper, we propose a general approach for augmenting recurrent GNNs with a cache memory to improve their expressivity, especially for modeling long-range dependencies. Specifically, we first introduce a method of augmenting recurrent GNNs with a cache of previous hidden states. Then we further propose a general Cache-GNN framework by adding additional modules, including attention mechanism and positional/structural encoders, to improve the expressivity. We show that the Cache-GNNs outperforms other models on synthetic datasets as well as tasks on real-world datasets that require long-range information.|尽管图神经网络(GNN)为学习结构化表示提供了一种强有力的方法，但学习图中的长期依赖性仍然具有挑战性。经常性 GNN 只是部分地解决了这个问题。在本文中，我们提出了一个通用的方法来增加经常性 GNN 与缓存内存，以提高它们的表达能力，特别是建模长期依赖。具体来说，我们首先介绍一种用以前的隐藏状态缓存来增加周期性 GNN 的方法。然后，我们进一步提出了一个通用的 Cache-GNN 框架，通过添加额外的模块，包括注意机制和位置/结构编码器，以提高表达能力。我们展示了 Cache-GNN 在合成数据集上优于其他模型，在需要长距离信息的实际数据集上优于其他模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Augmenting+Recurrent+Graph+Neural+Networks+with+a+Cache)|0|
|[Learning for Counterfactual Fairness from Observational Data](https://doi.org/10.1145/3580305.3599408)|Jing Ma, Ruocheng Guo, Aidong Zhang, Jundong Li|University of Virginia; Bytedance Research|Fairness-aware machine learning has attracted a surge of attention in many domains, such as online advertising, personalized recommendation, and social media analysis in web applications. Fairness-aware machine learning aims to eliminate biases of learning models against certain subgroups described by certain protected (sensitive) attributes such as race, gender, and age. Among many existing fairness notions, counterfactual fairness is a popular notion defined from a causal perspective. It measures the fairness of a predictor by comparing the prediction of each individual in the original world and that in the counterfactual worlds in which the value of the sensitive attribute is modified. A prerequisite for existing methods to achieve counterfactual fairness is the prior human knowledge of the causal model for the data. However, in real-world scenarios, the underlying causal model is often unknown, and acquiring such human knowledge could be very difficult. In these scenarios, it is risky to directly trust the causal models obtained from information sources with unknown reliability and even causal discovery methods, as incorrect causal models can consequently bring biases to the predictor and lead to unfair predictions. In this work, we address the problem of counterfactually fair prediction from observational data without given causal models by proposing a novel framework CLAIRE. Specifically, under certain general assumptions, CLAIRE effectively mitigates the biases from the sensitive attribute with a representation learning framework based on counterfactual data augmentation and an invariant penalty. Experiments conducted on both synthetic and real-world datasets validate the superiority of CLAIRE in both counterfactual fairness and prediction performance.|公平感知机器学习在许多领域引起了广泛的关注，如在线广告、个性化推荐和网络应用中的社会媒体分析。公平意识机器学习旨在消除学习模型对某些受保护(敏感)属性(如种族、性别和年龄)描述的子群体的偏见。在众多现有的公平观念中，反事实公平是一个从因果关系角度定义的流行概念。它通过比较原始世界中每个个体的预测和反事实世界中敏感属性值被修改的预测来衡量预测的公平性。现有方法实现反事实公平的一个先决条件是人类对数据的因果模型的先验知识。然而，在现实世界的情况下，潜在的因果模型往往是未知的，并获得这样的人类知识可能是非常困难的。在这些情况下，直接相信从信息来源获得的具有未知可靠性甚至因果发现方法的因果模型是有风险的，因为不正确的因果模型会因此给预测者带来偏差并导致不公平的预测。在这项工作中，我们通过提出一个新的框架 CLAIRE 来解决没有给定因果模型的观测数据的反事实公平预测问题。特别地，在一定的一般假设下，CLAIRE 通过一个基于反事实数据增强和不变惩罚的表示学习框架有效地减轻了敏感属性的偏差。在合成数据集和真实数据集上进行的实验验证了 CLAIRE 在反事实公平性和预测性能方面的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+for+Counterfactual+Fairness+from+Observational+Data)|0|
|[Towards Graph-level Anomaly Detection via Deep Evolutionary Mapping](https://doi.org/10.1145/3580305.3599524)|Xiaoxiao Ma, Jia Wu, Jian Yang, Quan Z. Sheng|Macquarie University|Graph-level anomaly detection aims at depicting anomalous individual graphs in a graph set. Due to its significance in various real-world application fields, such as identifying rare molecules in chemistry and detecting potential frauds in online social networks, graph-level anomaly detection has received great attention. In distinction from node- and edge-level anomaly detection that is devoted to identifying anomalies on a single graph, graph-level anomaly detection faces more significant challenges because both the intra- and inter-graph structural and attribute patterns need to be taken into account to distinguish anomalies that exhibit deviating structures, rare attributes or the both. Although deep graph representation learning shows effectiveness in fusing high-level representations and capturing characters of individual graphs, most of the existing works are defective in graph-level anomaly detection because of their limited capability in exploring information across graphs, the imbalanced data distribution of anomalies, and low interpretability of the black-box graph neural networks (GNNs). To bridge these gaps, we propose a novel deep evolutionary graph mapping framework named GmapAD, which can adaptively map each graph into a new feature space based on its similarity to a set of representative nodes chosen from the graph set. By automatically adjusting the candidate nodes using a specially designed evolutionary algorithm, anomalies and normal graphs are mapped to separate areas in the new feature space where a clear boundary between them can be learned. The selected candidate nodes can therefore be regarded as a benchmark for explaining anomalies because anomalies are more dissimilar/similar to the benchmark than normal graphs. Through our extensive experiments on nine real-world datasets, we demonstrate that exploring both intra- and inter-graph structural and attribute information are critical to spot anomalous graphs, and our framework outperforms the state of the art on all datasets used in the experiments.|图级异常检测的目的是在一个图集中描述不规则的单个图。由于其在各种现实应用领域的重要性，例如识别化学中的稀有分子和在线社交网络中发现潜在的欺诈行为，图级异常检测已经受到了极大的关注。与专门用于识别单个图表上的异常的节点级和边界级异常检测不同，图表级异常检测面临着更大的挑战，因为需要考虑图表内部和图表间的结构和属性模式，以区分表现出偏离结构、罕见属性或两者兼而有之的异常。尽管深度图表示学习在融合高层次表示和捕获单个图的特征方面显示出有效性，但是现有的大多数工作在图级异常检测方面存在缺陷，因为它们在跨图探索信息方面的能力有限，异常数据分布不平衡，以及黑盒图神经网络(GNN)的可解释性较低。为了弥补这些不足，我们提出了一种新的深度进化图映射框架 GmapAD，它可以根据每个图与从图集中选择的一组代表性节点的相似性，自适应地将每个图映射到一个新的特征空间。通过使用专门设计的进化算法自动调整候选节点，异常和正态图被映射到新特征空间中的分离区域，在这些区域之间可以学习到清晰的边界。因此，所选择的候选节点可以被视为解释异常的基准，因为异常与基准比正常图表更不相似/更相似。通过对9个真实世界数据集的广泛实验，我们证明了探索图内和图间结构和属性信息对于发现异常图至关重要，并且我们的框架在实验中使用的所有数据集上优于最先进的状态。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Graph-level+Anomaly+Detection+via+Deep+Evolutionary+Mapping)|0|
|[Context-aware Event Forecasting via Graph Disentanglement](https://doi.org/10.1145/3580305.3599285)|Yunshan Ma, Chenchen Ye, Zijian Wu, Xiang Wang, Yixin Cao, TatSeng Chua|University of Science and Technology of China; Singapore Management University; National University of Singapore|Event forecasting has been a demanding and challenging task throughout the entire human history. It plays a pivotal role in crisis alarming and disaster prevention in various aspects of the whole society. The task of event forecasting aims to model the relational and temporal patterns based on historical events and makes forecasting to what will happen in the future. Most existing studies on event forecasting formulate it as a problem of link prediction on temporal event graphs. However, such pure structured formulation suffers from two main limitations: 1) most events fall into general and high-level types in the event ontology, and therefore they tend to be coarse-grained and offers little utility which inevitably harms the forecasting accuracy; and 2) the events defined by a fixed ontology are unable to retain the out-of-ontology contextual information. To address these limitations, we propose a novel task of context-aware event forecasting which incorporates auxiliary contextual information. First, the categorical context provides supplementary fine-grained information to the coarse-grained events. Second and more importantly, the context provides additional information towards specific situation and condition, which is crucial or even determinant to what will happen next. However, it is challenging to properly integrate context into the event forecasting framework, considering the complex patterns in the multi-context scenario. Towards this end, we design a novel framework named Separation and Collaboration Graph Disentanglement (short as SeCoGD) for context-aware event forecasting. Since there is no available dataset for this novel task, we construct three large-scale datasets based on GDELT. Experimental results demonstrate that our model outperforms a list of SOTA methods.|在整个人类历史中，事件预测一直是一项艰巨而富有挑战性的任务。它在全社会的各个方面对危机预警和灾害预防起着举足轻重的作用。事件预测的任务是建立基于历史事件的关系模型和时间模型，并对未来发生的事件进行预测。现有的大多数事件预测研究都将其归结为时间事件图上的链接预测问题。然而，这种纯结构化公式存在两个主要的局限性: 1)大多数事件在事件本体中属于一般的和高级的类型，因此它们往往是粗粒度的，提供的效用很小，这不可避免地损害了预测的准确性; 2)由固定本体定义的事件不能保留本体外的上下文信息。为了解决这些局限性，我们提出了一个新的任务上下文感知事件预测，其中包括辅助上下文信息。首先，分类上下文为粗粒度事件提供补充的细粒度信息。其次，也是更重要的一点，上下文为特定的情况和条件提供了额外的信息，这对于接下来会发生什么是至关重要的，甚至是决定性的。然而，考虑到多上下文场景中的复杂模式，将上下文适当地集成到事件预测框架中是一个挑战。为此，我们设计了一个新的框架，命名为分离和协作图分离(简称 SeCoGD) ，用于上下文感知事件预测。由于这个新的任务没有可用的数据集，我们构建了三个基于 GDELT 的大规模数据集。实验结果表明，该模型的性能优于一系列 SOTA 方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Context-aware+Event+Forecasting+via+Graph+Disentanglement)|0|
|[End-to-End Inventory Prediction and Contract Allocation for Guaranteed Delivery Advertising](https://doi.org/10.1145/3580305.3599332)|Wuyang Mao, Chuanren Liu, Yundu Huang, Zhonglin Zu, M. Harshvardhan, Liang Wang, Bo Zheng||Guaranteed Delivery (GD) advertising plays an essential part in e-commerce marketing, where the ad publisher signs contracts with advertisers in advance by promising delivery of advertising impressions to fulfill targeting requirements for advertisers. Previous research on GD advertising mainly focused on online serving yet overlooked the importance of contract allocation at the GD selling stage. Traditional GD selling approaches consider impression inventory prediction and contract allocation as two separate stages. However, such a two-stage optimization often leads to inferior contract allocation performance. In this paper, our goal is to reduce this performance gap with a novel end-to-end approach. Specifically, we propose the Neural Lagrangian Selling (NLS) model to jointly predict the impression inventory and optimize the contract allocation of advertising impressions with a unified learning objective. To this end, we first develop a differentiable Lagrangian layer to backpropagate the allocation problem through the neural network and allow direct optimization of the allocation regret. Then, for effective optimization with various allocation targets and constraints, we design a graph convolutional neural network to extract predictive features from the bipartite allocation graph. Extensive experiments show that our approach can improve GD selling performance compared with existing two-stage approaches. Particularly, our optimization layer can outperform the baseline solvers in both computational efficiency and solution quality. To the best of our knowledge, this is the first study to apply the end-to-end prediction and optimization approach for industrial GD selling problems. Our work has implications for general prediction and allocation problems as well.|在电子商务营销中，广告发布者通过承诺提供广告印象来满足广告主的定位需求，从而提前与广告主签订合同。以往对 GD 广告的研究主要集中在在线服务上，忽视了在 GD 销售阶段合同分配的重要性。传统的 GD 销售方法将印象库存预测和合同分配视为两个独立的阶段。然而，这样的两阶段优化往往导致较差的合同分配绩效。在本文中，我们的目标是通过一种新的端到端方法来缩小这种性能差距。具体来说，我们提出了神经拉格朗日销售(NLS)模型来联合预测印象库存和优化广告印象的合同分配，具有统一的学习目标。为此，我们首先开发一个可微拉格朗日层，通过神经网络反向传播分配问题，并允许分配遗憾的直接优化。然后，为了在不同的分配目标和约束下进行有效的优化，我们设计了一个图形卷积神经网络，从二部分分配图中提取预测特征。大量的实验表明，与现有的两阶段方法相比，我们的方法可以提高 GD 的销售绩效。特别地，我们的优化层在计算效率和解决方案质量方面都优于基线求解器。据我们所知，这是第一个应用端到端预测和优化方法的工业 GD 销售问题的研究。我们的工作对一般的预测和分配问题也有启示。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=End-to-End+Inventory+Prediction+and+Contract+Allocation+for+Guaranteed+Delivery+Advertising)|0|
|[Densest Diverse Subgraphs: How to Plan a Successful Cocktail Party with Diversity](https://doi.org/10.1145/3580305.3599306)|Atsushi Miyauchi, Tianyi Chen, Konstantinos Sotiropoulos, Charalampos E. Tsourakakis|CENTAI Institute; Boston University|Dense subgraph discovery methods are routinely used in a variety of applications including the identification of a team of skilled individuals for collaboration from a social network. However, when the network's node set is associated with a sensitive attribute such as race, gender, religion, or political opinion, the lack of diversity can lead to lawsuits. In this work, we focus on the problem of finding a densest diverse subgraph in a graph whose nodes have different attribute values/types that we refer to as colors. We propose two novel formulations motivated by different realistic scenarios. Our first formulation, called the densest diverse subgraph problem (DDSP), guarantees that no color represents more than some fraction of the nodes in the output subgraph, which generalizes the state-of-the-art due to Anagnostopoulos et al. (CIKM 2020). By varying the fraction we can range the diversity constraint and interpolate from a diverse dense subgraph where all colors have to be equally represented to an unconstrained dense subgraph. We design a scalable $\Omega(1/\sqrt{n})$-approximation algorithm, where $n$ is the number of nodes. Our second formulation is motivated by the setting where any specified color should not be overlooked. We propose the densest at-least-$\vec{k}$-subgraph problem (Dal$\vec{k}$S), a novel generalization of the classic Dal$k$S, where instead of a single value $k$, we have a vector ${\mathbf k}$ of cardinality demands with one coordinate per color class. We design a $1/3$-approximation algorithm using linear programming together with an acceleration technique. Computational experiments using synthetic and real-world datasets demonstrate that our proposed algorithms are effective in extracting dense diverse clusters.|密集子图发现方法常用于各种应用程序中，包括从社交网络中识别一组技术熟练的个人进行协作。然而，当网络的节点集与一个敏感的属性(如种族、性别、宗教或政治观点)相关联时，缺乏多样性可能导致诉讼。在这项工作中，我们的重点是在一个图的节点有不同的属性值/类型，我们称之为颜色的图中找到一个密度最大的不同子图的问题。我们提出了两个新的公式动机不同的现实情景。我们的第一个公式称为最密集多样化子图问题(DDSP) ，保证没有颜色代表输出子图中的一些节点部分，这推广了由于 Anagnostopoulos 等人(CIKM 2020)的最先进状态。通过改变分数，我们可以扩大多样性约束，并从一个不同的密集子图插值，其中所有颜色必须平等地表示为一个无约束的密集子图。我们设计了一个可伸缩的 $Omega (1/sqrt { n }) $- 近似演算法，其中 $n $是节点数。我们的第二个配方是动机的设置，其中任何指定的颜色不应该被忽视。我们提出了最密集的至少 $vec { k } $- 子图问题(Dal $vec { k } $S) ，这是经典 Dal $k $S 的一个新的推广，其中，我们有一个向量 ${ mathbf k } $的基数需求，每个颜色类有一个坐标。我们设计了一个1/3美元的近似演算法，使用了线性规划和加速技术。使用合成和真实世界数据集的计算实验表明，我们提出的算法是有效的提取密集不同的簇。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Densest+Diverse+Subgraphs:+How+to+Plan+a+Successful+Cocktail+Party+with+Diversity)|0|
|[Causal Inference via Style Transfer for Out-of-distribution Generalisation](https://doi.org/10.1145/3580305.3599270)|Toan Nguyen, Kien Do, Duc Thanh Nguyen, Bao Duong, Thin Nguyen|Applied Artificial Intelligence Institute, Deakin University; School of Information Technology, Deakin University|Out-of-distribution (OOD) generalisation aims to build a model that can generalise well on an unseen target domain using knowledge from multiple source domains. To this end, the model should seek the causal dependence between inputs and labels, which may be determined by the semantics of inputs and remain invariant across domains. However, statistical or non-causal methods often cannot capture this dependence and perform poorly due to not considering spurious correlations learnt from model training via unobserved confounders. A well-known existing causal inference method like back-door adjustment cannot be applied to remove spurious correlations as it requires the observation of confounders. In this paper, we propose a novel method that effectively deals with hidden confounders by successfully implementing front-door adjustment (FA). FA requires the choice of a mediator, which we regard as the semantic information of images that helps access the causal mechanism without the need for observing confounders. Further, we propose to estimate the combination of the mediator with other observed images in the front-door formula via style transfer algorithms. Our use of style transfer to estimate FA is novel and sensible for OOD generalisation, which we justify by extensive experimental results on widely used benchmark datasets.|分布外(OOD)泛化的目的是建立一个模型，可以很好地泛化一个看不见的目标领域使用来自多个来源领域的知识。为此，模型应该寻求输入和标签之间的因果依赖关系，这可能由输入的语义决定，并在不同的领域保持不变。然而，统计或非因果方法往往无法捕捉这种依赖性，并且由于没有考虑通过未观察到的混杂因素从模型训练中学到的虚假相关性而表现不佳。一个众所周知的现有因果推断方法，如后门调整不能应用于消除虚假的相关性，因为它需要观察混杂因素。在本文中，我们提出了一种新的方法，有效地处理隐藏的混杂因素，成功地实现前门调整(FA)。足总需要选择一个调解人，我们认为这是一个图像语义信息，有助于进入因果机制，而无需观察混杂因素。进一步，我们建议通过样式转移算法估计前门公式中的中介与其他观察图像的组合。我们使用样式转移来估计 FA 是新颖的和合理的 OOD 概括，我们在广泛使用的基准数据集上的广泛实验结果证明了这一点。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causal+Inference+via+Style+Transfer+for+Out-of-distribution+Generalisation)|0|
|[A Higher-Order Temporal H-Index for Evolving Networks](https://doi.org/10.1145/3580305.3599242)|Lutz Oettershagen, Nils M. Kriege, Petra Mutzel|KTH Royal Institute of Technology; University of Vienna; University of Bonn|The H-index of a node in a static network is the maximum value $h$ such that at least $h$ of its neighbors have a degree of at least $h$. Recently, a generalized version, the $n$-th order H-index, was introduced, allowing to relate degree centrality, H-index, and the $k$-core of a node. We extend the $n$-th order H-index to temporal networks and define corresponding temporal centrality measures and temporal core decompositions. Our $n$-th order temporal H-index respects the reachability in temporal networks leading to node rankings, which reflect the importance of nodes in spreading processes. We derive natural decompositions of temporal networks into subgraphs with strong temporal coherence. We analyze a recursive computation scheme and develop a highly scalable streaming algorithm. Our experimental evaluation demonstrates the efficiency of our algorithms and the conceptional validity of our approach. Specifically, we show that the $n$-th order temporal H-index is a strong heuristic for identifying super-spreaders in evolving social networks and detects temporally well-connected components.|静态网络中节点的 H 指标是最大值 $h $，这样至少 $h $的邻居具有至少 $h $的度。最近，引入了一个通用版本 $n $th 阶 H 指标，它允许关联节点的度中心性、 H 指标和 $k $- 核。将 n 阶 H 指标扩展到时态网络，定义了相应的时态中心度量和时态核分解。我们的 n 阶时态 H 指数考虑了时态网络中节点排名的可达性，反映了节点在扩展过程中的重要性。我们将时间网络自然分解为具有强时间相干性的子图。我们分析了一个递归计算方案，并开发了一个高度可扩展的流式算法。我们的实验评估证明了我们的算法的有效性和我们的方法的概念的有效性。具体地说，我们证明了 $n 阶时间 H 指数是一个强大的启发式识别超级传播者在不断发展的社会网络和检测时间上良好连接的组件。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Higher-Order+Temporal+H-Index+for+Evolving+Networks)|0|
|[Deep Weakly-supervised Anomaly Detection](https://doi.org/10.1145/3580305.3599302)|Guansong Pang, Chunhua Shen, Huidong Jin, Anton van den Hengel|University of Adelaide; Zhejiang University; Data61; Singapore Management University|Anomaly detection is typically posited as an unsupervised learning task in the literature due to the prohibitive cost and difficulty to obtain large-scale labeled anomaly data, but this ignores the fact that a very small number (eg,, a few dozens) of labeled anomalies can often be made available with small/trivial cost in many real-world anomaly detection applications. To leverage such labeled anomaly data, we study an important anomaly detection problem termed weakly-supervised anomaly detection, in which, in addition to a large amount of unlabeled data, a limited number of labeled anomalies are available during modeling. Learning with the small labeled anomaly data enables anomaly-informed modeling, which helps identify anomalies of interest and address the notorious high false positives in unsupervised anomaly detection. However, the problem is especially challenging, since (i) the limited amount of …|由于成本过高和难以获得大规模的标记异常数据，异常检测在文献中通常被假定为一个非监督式学习任务，但这忽略了一个事实，即在许多现实世界的异常检测应用中，只有很少数量(例如，几十个)的标记异常可以用很小的成本获得。为了利用这些已标记的异常数据，我们研究了一个重要的异常检测问题，称为弱监督异常检测。在这个问题中，除了大量未标记的数据之外，在建模过程中只有有限数量的已标记的异常可用。利用小标记的异常数据进行学习，可以建立异常信息模型，有助于识别感兴趣的异常，并解决无监督异常检测中出名的高假阳性问题。然而，这个问题是特别具有挑战性的，因为(i)数量有限..。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Weakly-supervised+Anomaly+Detection)|0|
|[FedDefender: Client-Side Attack-Tolerant Federated Learning](https://doi.org/10.1145/3580305.3599346)|Sungwon Park, Sungwon Han, Fangzhao Wu, Sundong Kim, Bin Zhu, Xing Xie, Meeyoung Cha|KAIST; GIST; IBS; Microsoft Research Asia|Federated learning enables learning from decentralized data sources without compromising privacy, which makes it a crucial technique. However, it is vulnerable to model poisoning attacks, where malicious clients interfere with the training process. Previous defense mechanisms have focused on the server-side by using careful model aggregation, but this may not be effective when the data is not identically distributed or when attackers can access the information of benign clients. In this paper, we propose a new defense mechanism that focuses on the client-side, called FedDefender, to help benign clients train robust local models and avoid the adverse impact of malicious model updates from attackers, even when a server-side defense cannot identify or remove adversaries. Our method consists of two main components: (1) attack-tolerant local meta update and (2) attack-tolerant global knowledge distillation. These components are used to find noise-resilient model parameters while accurately extracting knowledge from a potentially corrupted global model. Our client-side defense strategy has a flexible structure and can work in conjunction with any existing server-side strategies. Evaluations of real-world scenarios across multiple datasets show that the proposed method enhances the robustness of federated learning against model poisoning attacks.|联合学习能够在不损害隐私的情况下从分散的数据源中学习，这使得它成为一种关键技术。但是，它很容易受到模型中毒攻击，因为恶意客户机会干扰培训过程。以前的防御机制通过使用谨慎的模型聚合将重点放在服务器端，但是当数据分布不完全相同或者当攻击者可以访问良性客户端的信息时，这可能不会有效。在本文中，我们提出了一种新的以客户端为中心的防御机制，称为 FedDefender，以帮助良性客户端训练健壮的本地模型，并避免来自攻击者的恶意模型更新的负面影响，即使服务器端防御不能识别或移除对手。该方法由两个主要部分组成: (1)容忍攻击的局部元更新和(2)容忍攻击的全局知识提取。这些分量被用来寻找抗噪声的模型参数，同时准确地从潜在损坏的全局模型中提取知识。我们的客户端防御策略具有灵活的结构，可以与任何现有的服务器端策略协同工作。对多个数据集的实际场景的评估表明，该方法增强了联邦学习对模型中毒攻击的鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedDefender:+Client-Side+Attack-Tolerant+Federated+Learning)|0|
|[Few-shot Low-resource Knowledge Graph Completion with Multi-view Task Representation Generation](https://doi.org/10.1145/3580305.3599350)|Shichao Pei, Ziyi Kou, Qiannan Zhang, Xiangliang Zhang||Despite their capacity to convey knowledge, most existing knowledge graphs (KGs) are created for specific domains using low-resource data sources, especially those in non-global languages, and thus unavoidably suffer from the incompleteness problem. The automatic discovery of missing triples for KG completion is thus hindered by the challenging long-tail relations problem in low-resource KGs. Few-shot learning models trained on rich-resource KGs are unable to tackle this challenge due to a lack of generalization. To alleviate the impact of the intractable long-tail problem on low-resource KG completion, in this paper, we propose a novel few-shot learning framework empowered by multi-view task representation generation. The framework consists of four components, i.e., few-shot learner, perturbed few-shot learner, relation knowledge distiller, and pairwise contrastive distiller. The key idea is to utilize the different views of each few-shot task to improve and regulate the training of the few-shot learner. For each few-shot task, instead of augmenting it by complicated task designs, we generate its representation of different views using the relation knowledge distiller and perturbed few-shot learner, which are obtained by distilling knowledge from a KG encoder and perturbing the few-shot learner. Then, the generated representation of different views is utilized by the pairwise contrastive distiller based on a teacher-student framework to distill the knowledge of how to represent relations from different views into the few-shot learner and facilitate few-shot learning. Extensive experiments conducted on several real-world low-resource KGs validate the effectiveness of our proposed method.|尽管知识图具有传递知识的能力，但是大多数现有的知识图都是使用低资源的数据源(尤其是非全局语言的数据源)为特定领域创建的，因此不可避免地会遇到不完备性问题。因此，缺失三元组的自动发现受到低资源幼儿园长尾关系问题的阻碍。由于缺乏普遍性，在资源丰富的幼儿园接受培训的极少数学习模式无法应对这一挑战。为了缓解难以解决的长尾问题对低资源 KG 完成的影响，本文提出了一种新的基于多视图任务表示生成的少镜头学习框架。该框架由四部分组成，即小镜头学习器、扰动小镜头学习器、关系知识提取器和成对对比提取器。其核心思想是利用每个少拍任务的不同视角来改进和规范少拍学习者的训练。对于每一个小镜头任务，我们不需要通过复杂的任务设计来增强它，而是使用关系知识提取器和扰动小镜头学习器，通过从 KG 编码器中提取知识并扰动小镜头学习器来生成不同视图的表示。然后，利用基于师生框架的成对对比蒸馏器生成不同视图的表示，将不同视图表示关系的知识提取到少镜头学习者中，促进少镜头学习。在几个现实世界的低资源幼儿园中进行的大量实验验证了我们提出的方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Few-shot+Low-resource+Knowledge+Graph+Completion+with+Multi-view+Task+Representation+Generation)|0|
|[Efficient Centrality Maximization with Rademacher Averages](https://doi.org/10.1145/3580305.3599325)|Leonardo Pellegrina|University of Padova|The identification of the set of k most central nodes of a graph, or centrality maximization, is a key task in network analysis, with various applications ranging from finding communities in social and biological networks to understanding which seed nodes are important to diffuse information in a graph. As the exact computation of centrality measures does not scale to modern-sized networks, the most practical solution is to resort to rigorous, but efficiently computable, randomized approximations. In this work we present CentRA, the first algorithm based on progressive sampling to compute high-quality approximations of the set of k most central nodes. CentRA is based on a novel approach to efficiently estimate Monte Carlo Rademacher Averages, a powerful tool from statistical learning theory to compute sharp data-dependent approximation bounds. Then, we study the sample complexity of centrality maximization using the VC-dimension, a key concept from statistical learning theory. We show that the number of random samples required to compute high-quality approximations scales with finer characteristics of the graph, such as its vertex diameter, or of the centrality of interest, significantly improving looser bounds derived from standard techniques. We apply CentRA to analyze large real-world networks, showing that it significantly outperforms the state-of-the-art approximation algorithm in terms of number of samples, running times, and accuracy.|图的 k 个最中心节点集的识别，或中心性最大化，是网络分析中的一个关键任务，其应用范围广泛，从寻找社会和生物网络中的社区，到了解哪些种子节点对传播图中的信息是重要的。由于中心性度量的精确计算并不适用于现代规模的网络，最实际的解决方案是采用严格的、但可有效计算的随机近似。在这项工作中，我们提出的 CentRA，第一个算法的基础上逐步采样计算高质量的近似集 k 最中心节点。CentRA 基于一种有效估计 Monte Carlo Rademacher 平均值的新方法，这是一种来自统计学习理论的强大工具，用于计算与数据相关的近似界限。然后，利用统计学习理论中的一个关键概念—— VC 维，研究了中心性最大化的样本复杂度问题。我们表明，随机样本的数量需要计算高质量的近似尺度与更细的图的特征，如其顶点直径，或感兴趣的中心性，显着改善松散的界限源自标准技术。我们使用 CentRA 来分析大型现实世界的网络，结果显示，它在样本数量、运行时间和准确性方面都明显优于最先进的近似演算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Centrality+Maximization+with+Rademacher+Averages)|0|
|[Learning from Positive and Unlabeled Multi-Instance Bags in Anomaly Detection](https://doi.org/10.1145/3580305.3599409)|Lorenzo Perini, Vincent Vercruyssen, Jesse Davis||In the multi-instance learning (MIL) setting instances are grouped together into bags. Labels are provided only for the bags and not on the level of individual instances. A positive bag label means that at least one instance inside the bag is positive, while a negative bag label restricts all the instances in the bag to be negative. MIL data naturally arises in many contexts, such as anomaly detection, where labels are rare and costly, and one often ends up annotating the label for sets of instances. Moreover, in many real-world anomaly detection problems, only positive labels are collected because they usually represent critical events. Such a setting, where only positive labels are provided along with unlabeled data, is called Positive and Unlabeled (PU) learning. Despite being useful for several use cases, there is no work dedicated to learning from positive and unlabeled data in a multi-instance setting for anomaly detection. Therefore, we propose the first method that learns from PU bags in anomaly detection. Our method uses an autoencoder as an underlying anomaly detector. We alter the autoencoder's objective function and propose a new loss that allows it to learn from positive and unlabeled bags of instances. We theoretically analyze this method. Experimentally, we evaluate our method on 30 datasets and show that it performs better than multiple baselines adapted to work in our setting.|在多实例学习(MIL)中，设置实例被分组成袋。标签只提供袋子，而不是在个别情况下的水平。正袋标签表示袋内至少有一个实例为正，而负袋标签则限制袋内所有实例为负。MIL 数据自然而然地出现在许多情况下，比如异常检测，在这些情况下，标签是罕见且昂贵的，人们通常最终会为一组实例标注标签。此外，在许多现实世界的异常检测问题中，只收集正面的标签，因为它们通常代表关键事件。这种只提供正面标签和未标记数据的设置称为正面和未标记(PU)学习。尽管对于几个用例来说是有用的，但是在多实例环境中，没有工作致力于从积极的和未标记的数据中学习异常检测。因此，我们提出了第一种从异常检测中的聚氨酯袋中学习的方法。我们的方法使用自动编码器作为底层异常检测器。我们改变自动编码器的目标功能，并提出一个新的损失，让它学习积极和未标记的实例袋。对这种方法进行了理论分析。实验中，我们在30个数据集上评估了我们的方法，结果表明它比适合我们设置的多个基线表现得更好。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+from+Positive+and+Unlabeled+Multi-Instance+Bags+in+Anomaly+Detection)|0|
|[Generalizable Low-Resource Activity Recognition with Diverse and Discriminative Representation Learning](https://doi.org/10.1145/3580305.3599360)|Xin Qin, Jindong Wang, Shuo Ma, Wang Lu, Yongchun Zhu, Xing Xie, Yiqiang Chen|Beijing Key Lab. of Mobile Com., CAS; Microsoft Research Asia|Human activity recognition (HAR) is a time series classification task that focuses on identifying the motion patterns from human sensor readings. Adequate data is essential but a major bottleneck for training a generalizable HAR model, which assists customization and optimization of online web applications. However, it is costly in time and economy to collect large-scale labeled data in reality, i.e., the low-resource challenge. Meanwhile, data collected from different persons have distribution shifts due to different living habits, body shapes, age groups, etc. The low-resource and distribution shift challenges are detrimental to HAR when applying the trained model to new unseen subjects. In this paper, we propose a novel approach called Diverse and Discriminative representation Learning (DDLearn) for generalizable low-resource HAR. DDLearn simultaneously considers diversity and discrimination learning. With the constructed self-supervised learning task, DDLearn enlarges the data diversity and explores the latent activity properties. Then, we propose a diversity preservation module to preserve the diversity of learned features by enlarging the distribution divergence between the original and augmented domains. Meanwhile, DDLearn also enhances semantic discrimination by learning discriminative representations with supervised contrastive learning. Extensive experiments on three public HAR datasets demonstrate that our method significantly outperforms state-of-art methods by an average accuracy improvement of 9.5% under the low-resource distribution shift scenarios, while being a generic, explainable, and flexible framework.|人类活动识别(HAR)是一个时间序列分类任务，重点是从人类传感器读数识别运动模式。充足的数据是必不可少的，但也是培训一个可推广的 HAR 模型的主要瓶颈，该模型有助于在线 Web 应用程序的定制和优化。然而，在现实生活中收集大规模的标记数据在时间和经济上是昂贵的，也就是说，低资源的挑战。同时，由于生活习惯、体型、年龄等因素的不同，从不同人群收集到的数据也有分布变化。低资源和分配转移的挑战是有害的 HAR 时，应用训练模型的新的看不见的主题。针对可推广的低资源 HAR 问题，提出了一种新的多元鉴别表示学习方法。DDLearning 同时考虑多样性和歧视性学习。通过构建自监督学习任务，扩大了数据的多样性，探索了潜在活动特性。然后，我们提出了一个多样性保持模块，通过扩大原始域和扩展域之间的分布差异来保持学习特征的多样性。同时，通过有监督的对比学习来学习判别表征，DDLearning 也增强了语义识别能力。在三个公共 HAR 数据集上的大量实验表明，在低资源分布转移情景下，我们的方法显着优于最先进的方法，平均准确度提高了9.5% ，同时是一个通用的，可解释的和灵活的框架。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generalizable+Low-Resource+Activity+Recognition+with+Diverse+and+Discriminative+Representation+Learning)|0|
|[3D-IDS: Doubly Disentangled Dynamic Intrusion Detection](https://doi.org/10.1145/3580305.3599238)|Chenyang Qiu, Yingsheng Geng, Junrui Lu, Kaida Chen, Shitong Zhu, Ya Su, Guoshun Nan, Can Zhang, Junsong Fu, Qimei Cui, Xiaofeng Tao|Beijing University of Posts and Telecommunications|Network-based intrusion detection system (NIDS) monitors network traffic for malicious activities, forming the frontline defense against increasing attacks over information infrastructures. Although promising, our quantitative analysis shows that existing methods perform inconsistently in declaring various unknown attacks (e.g., 9% and 35% F1 respectively for two distinct unknown threats for an SVM-based method) or detecting diverse known attacks (e.g., 31% F1 for the Backdoor and 93% F1 for DDoS by a GCN-based state-of-the-art method), and reveals that the underlying cause is entangled distributions of flow features. This motivates us to propose 3D-IDS, a novel method that aims to tackle the above issues through two-step feature disentanglements and a dynamic graph diffusion scheme. Specifically, we first disentangle traffic features by a non-parameterized optimization based on mutual information, automatically differentiating tens and hundreds of complex features of various attacks. Such differentiated features will be fed into a memory model to generate representations, which are further disentangled to highlight the attack-specific features. Finally, we use a novel graph diffusion method that dynamically fuses the network topology for spatial-temporal aggregation in evolving data streams. By doing so, we can effectively identify various attacks in encrypted traffics, including unknown threats and known ones that are not easily detected. Experiments show the superiority of our 3D-IDS. We also demonstrate that our two-step feature disentanglements benefit the explainability of NIDS.|基于网络的入侵预防系统(nIDS)监控网络流量以防范恶意活动，从而形成一道防线，抵御对信息基础设施日益增多的攻击。尽管有希望，但是我们的定量分析表明，现有的方法在声明各种未知攻击(例如，基于 SVM 的方法的两种不同的未知威胁分别为9% 和35% F1)或检测不同的已知攻击(例如，通过基于 GCN 的最新技术的方法，后门为31% F1，DDoS 为93% F1) ，并且揭示了潜在的原因是流特征的纠缠分布。这促使我们提出了三维入侵检测系统(3D-IDS) ，这是一种通过两步特征分离和动态图扩散来解决上述问题的新方法。具体来说，我们首先通过一个基于互信息的非参数化优化来解析流量特征，自动区分出数十个和数百个各种攻击的复杂特征。这些差异化的特征将被输入到一个记忆模型中以生成表征，这些表征将被进一步分离以突出攻击特定的特征。最后，我们使用一种新的图形扩散方法，在不断演化的数据流中动态地融合时空聚合的网络拓扑。通过这样做，我们可以有效地识别加密流量中的各种攻击，包括未知威胁和不容易检测到的已知威胁。实验表明了我们的3D 入侵检测系统的优越性。我们还演示了我们的两步特征分离有利于 NIDS 的可解释性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=3D-IDS:+Doubly+Disentangled+Dynamic+Intrusion+Detection)|0|
|[Reconstructing Graph Diffusion History from a Single Snapshot](https://doi.org/10.1145/3580305.3599488)|Ruizhong Qiu, Dingsu Wang, Lei Ying, H. Vincent Poor, Yifang Zhang, Hanghang Tong|University of Illinois Urbana-Champaign; University of Michigan; Princeton University; Cai Digital Transformation Institute|Diffusion on graphs is ubiquitous with numerous high-impact applications. In these applications, complete diffusion histories play an essential role in terms of identifying dynamical patterns, reflecting on precaution actions, and forecasting intervention effects. Despite their importance, complete diffusion histories are rarely available and are highly challenging to reconstruct due to ill-posedness, explosive search space, and scarcity of training data. To date, few methods exist for diffusion history reconstruction. They are exclusively based on the maximum likelihood estimation (MLE) formulation and require to know true diffusion parameters. In this paper, we study an even harder problem, namely reconstructing Diffusion history from A single SnapsHot} (DASH), where we seek to reconstruct the history from only the final snapshot without knowing true diffusion parameters. We start with theoretical analyses that reveal a fundamental limitation of the MLE formulation. We prove: (a) estimation error of diffusion parameters is unavoidable due to NP-hardness of diffusion parameter estimation, and (b) the MLE formulation is sensitive to estimation error of diffusion parameters. To overcome the inherent limitation of the MLE formulation, we propose a novel barycenter formulation: finding the barycenter of the posterior distribution of histories, which is provably stable against the estimation error of diffusion parameters. We further develop an effective solver named DIffusion hiTting Times with Optimal proposal (DITTO) by reducing the problem to estimating posterior expected hitting times via the Metropolis--Hastings Markov chain Monte Carlo method (M--H MCMC) and employing an unsupervised graph neural network to learn an optimal proposal to accelerate the convergence of M--H MCMC. We conduct extensive experiments to demonstrate the efficacy of the proposed method.|图上的扩散是普遍存在的许多高影响应用。在这些应用中，完整的扩散历史在识别动态模式、反映预防措施和预测干预效果方面发挥着至关重要的作用。尽管它们的重要性，完整的扩散历史很少可用，并且由于不适定性，爆炸搜索空间和缺乏训练数据，重建是非常具有挑战性的。迄今为止，弥散历史重建的方法很少。它们完全基于最大似然估计(MLE)公式，需要知道真实的扩散参数。在本文中，我们研究了一个更加困难的问题，即从一个单一的 SnapsHot }(DASH)重建扩散历史，其中我们寻求只从最终的快照重建历史，而不知道真正的扩散参数。我们从理论分析开始，揭示了极大似然估计公式的基本局限性。我们证明: (a)由于扩散参数估计的 NP 难度，扩散参数的估计误差是不可避免的; (b)扩散参数估计误差对最大似然估计公式是敏感的。为了克服极大似然估计公式的固有局限性，我们提出了一种新的重心公式: 找到历史后验概率的重心，这种公式可以证明对抗扩散参数的估计误差是稳定的。通过使用 Metropolis-Hastings 马尔科夫蒙特卡洛方法(M-H MCMC)来减少估计后期预期命中时间的问题，并使用一个无监督的图形神经网络来学习一个加速 M-H MCMC 收敛的最优方案，我们进一步开发了一个有效的解决方案——扩散命中时间与最优方案(dITTO)。我们进行了广泛的实验，以证明所提出的方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reconstructing+Graph+Diffusion+History+from+a+Single+Snapshot)|0|
|[FedPseudo: Privacy-Preserving Pseudo Value-Based Deep Learning Models for Federated Survival Analysis](https://doi.org/10.1145/3580305.3599348)|Md. Mahmudur Rahman, Sanjay Purushotham|University of Maryland, Baltimore County|Survival analysis, aka time-to-event analysis, has a wide-ranging impact on patient care. Federated Survival Analysis (FSA) is an emerging Federated Learning (FL) paradigm for performing survival analysis on distributed decentralized data available at multiple medical institutions. FSA enables individual medical institutions, referred to as clients, to improve their survival predictions while ensuring privacy. However, FSA faces challenges due to non-linear and non-IID data distributions among clients, as well as bias caused by censoring. Although recent studies have adapted Cox Proportional Hazards (CoxPH) survival models for FSA, a systematic exploration of these challenges is currently lacking. In this paper, we address these critical challenges by introducing FedPseudo, a pseudo value-based deep learning framework for FSA. FedPseudo uses deep learning models to learn robust representations from non-linear survival data, leverages the power of pseudo values to handle non-uniform censoring, and employs FL algorithms such as FedAvg to learn model parameters. We propose a novel and simple approach for estimating pseudo values for FSA. We provide theoretical proof that the estimated pseudo values, referred to as Federated Pseudo Values, are consistent. Moreover, our empirical results demonstrate that they can be computed faster than traditional methods of deriving pseudo values. To ensure and enhance the privacy of both the estimated pseudo values and the shared model parameters, we systematically investigate the application of differential privacy (DP) on both the federated pseudo values and local model updates. Furthermore, we adapt V -Usable Information metric to quantify the informativeness of a client's data for training a survival model and utilize this metric to show the advantages of participating in FSA. We conducted extensive experiments on synthetic and real-world survival datasets to demonstrate that our FedPseudo framework achieves better performance than other FSA approaches and performs similarly to the best centrally trained deep survival model. Moreover, FedPseudo consistently achieves superior results across different censoring settings.|生存分析，又称时间-事件分析，对患者护理有广泛的影响。联邦生存分析(FSA)是一种新兴的联邦学习(FL)范式，用于对多个医疗机构提供的分布式分散数据进行生存分析。FSA 使个体医疗机构，也就是客户，能够在确保隐私的同时提高他们的生存预测。然而，FSA 面临的挑战是客户之间的非线性和非 IID 数据分布，以及审查造成的偏见。尽管最近的研究已经将 Cox 比例风险(CoxPH)生存模型应用于 FSA，但目前还缺乏对这些挑战的系统性探索。在本文中，我们通过引入 FSA 的伪基于值的深度学习框架 FedPseudo 来解决这些关键挑战。FedPseude 利用深度学习模型从非线性生存数据中学习鲁棒表示，利用伪值的能力处理非均匀审查，并使用 FL 算法如 FedAvg 学习模型参数。我们提出了一种新颖而简单的方法来估计 FSA 的伪值。我们提供了理论证明，估计的伪值，称为联邦伪值，是一致的。此外，我们的实验结果表明，它们可以比传统的方法得到更快的伪值。为了保证和增强估计的伪值和共享模型参数的私密性，我们系统地研究了差分隐私(DP)在联邦伪值和局部模型更新中的应用。此外，我们采用 V-Usable 信息度量来量化客户数据的信息量，以训练生存模型，并利用这个度量来显示参与 FSA 的优势。我们在合成和真实世界的生存数据集上进行了广泛的实验，以证明我们的 Fed赝框架比其他 FSA 方法获得了更好的性能，并且表现类似于最好的集中训练的深度生存模型。此外，FedPsedo 在不同的审查设置中始终获得优越的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedPseudo:+Privacy-Preserving+Pseudo+Value-Based+Deep+Learning+Models+for+Federated+Survival+Analysis)|0|
|[Robustness Certification for Structured Prediction with General Inputs via Safe Region Modeling in the Semimetric Output Space](https://doi.org/10.1145/3580305.3599493)|Huaqing Shao, Lanjun Wang, Junchi Yan||Many real-world machine learning problems involve structured prediction beyond categorical labels. However, most existing robustness certification works are devoted to the classification case. It remains open for robustness certification for more general outputs. In this paper, we propose a novel framework of robustness certification for structured prediction problems, where the output space is modeled as a semimetric space with a distance function that satisfies non-negativity and symmetry but not necessarily the triangle inequality. We further develop our tailored certification methods for binary, numerical, and hybrid inputs in structured prediction. Experiment results show that our method achieves tighter robustness guarantees than the SOTA structured certification baseline for numerical inputs (for which it only supports) with ℓ 2 norm perturbation when outputs are measured by intersection over union (IoU) similarity, total variation distance, and perceptual distance. Moreover, we achieve good robustness certification for binary inputs with ℓ 0 norm perturbation and hybrid inputs with corresponding perturbation when outputs are measured by Manhattan distance.|许多现实世界的机器学习问题涉及超越分类标签的结构化预测。然而，现有的鲁棒性认证工作大多致力于分类情况。它仍然开放的健壮性认证更一般的产出。在这篇文章中，我们提出了一个新的结构化预测问题的鲁棒性认证框架，其中输出空间被建模为一个具有距离函数的半度空间，它满足非负性和对称性，但不一定是三角不等式。我们进一步发展我们的二进制，数字和结构化预测混合输入量身定制的认证方法。实验结果表明，该方法比 SOTA 结构化证明基线对数值输入(它只支持)具有12范数扰动时，输出通过交集超过联合(IoU)的相似性，总变化距离和感知距离测量更严格的鲁棒性保证。此外，当输出用曼哈顿距离测量时，我们实现了对具有 l0范数扰动的二元输入和具有相应扰动的混合输入的鲁棒性验证。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robustness+Certification+for+Structured+Prediction+with+General+Inputs+via+Safe+Region+Modeling+in+the+Semimetric+Output+Space)|0|
|[CARL-G: Clustering-Accelerated Representation Learning on Graphs](https://doi.org/10.1145/3580305.3599268)|William Shiao, Uday Singh Saini, Yozen Liu, Tong Zhao, Neil Shah, Evangelos E. Papalexakis|Snap Inc.; University of California, Riverside|Self-supervised learning on graphs has made large strides in achieving great performance in various downstream tasks. However, many state-of-the-art methods suffer from a number of impediments, which prevent them from realizing their full potential. For instance, contrastive methods typically require negative sampling, which is often computationally costly. While non-contrastive methods avoid this expensive step, most existing methods either rely on overly complex architectures or dataset-specific augmentations. In this paper, we ask: Can we borrow from classical unsupervised machine learning literature in order to overcome those obstacles? Guided by our key insight that the goal of distance-based clustering closely resembles that of contrastive learning: both attempt to pull representations of similar items together and dissimilar items apart. As a result, we propose CARL-G - a novel clustering-based framework for graph representation learning that uses a loss inspired by Cluster Validation Indices (CVIs), i.e., internal measures of cluster quality (no ground truth required). CARL-G is adaptable to different clustering methods and CVIs, and we show that with the right choice of clustering method and CVI, CARL-G outperforms node classification baselines on 4/5 datasets with up to a 79x training speedup compared to the best-performing baseline. CARL-G also performs at par or better than baselines in node clustering and similarity search tasks, training up to 1,500x faster than the best-performing baseline. Finally, we also provide theoretical foundations for the use of CVI-inspired losses in graph representation learning.|图上的自我监督学习在各种下游任务中取得了巨大的进展。然而，许多最先进的方法都存在一些障碍，这些障碍使它们无法充分发挥其潜力。例如，对比方法通常需要负采样，这往往是计算成本高昂。虽然非对比方法避免了这一代价高昂的步骤，但是大多数现有方法要么依赖于过于复杂的体系结构，要么依赖于特定于数据集的扩展。在本文中，我们会问: 我们能否借鉴古典非监督式学习文学来克服这些障碍？基于距离聚类的目标与对比学习的目标非常相似: 两者都试图将相似项目的表示放在一起，而将不同项目的表示分开。因此，我们提出了 CARL-G-一种新的基于聚类的图表示学习框架，其使用由聚类验证指数(CVI)启发的损失，即集群质量的内部测量(不需要地面真相)。CARL-G 适应不同的聚类方法和 CVI，我们表明，如果正确选择聚类方法和 CVI，CARL-G 在4/5数据集上优于节点分类基线，与最佳基线相比，训练加速高达79倍。CARL-G 在节点群集和最近邻搜索任务方面的表现也达到或优于基线，训练速度比最佳基线快1500倍。最后，我们还为 CVI 启发的损失在图表示学习中的应用提供了理论基础。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CARL-G:+Clustering-Accelerated+Representation+Learning+on+Graphs)|0|
|[One-shot Joint Extraction, Registration and Segmentation of Neuroimaging Data](https://doi.org/10.1145/3580305.3599452)|Yao Su, Zhentian Qian, Lei Ma, Lifang He, Xiangnan Kong|Worcester Polytechnic Institute; Lehigh University|Brain extraction, registration and segmentation are indispensable preprocessing steps in neuroimaging studies. The aim is to extract the brain from raw imaging scans (i.e., extraction step), align it with a target brain image (i.e., registration step) and label the anatomical brain regions (i.e., segmentation step). Conventional studies typically focus on developing separate methods for the extraction, registration and segmentation tasks in a supervised setting. The performance of these methods is largely contingent on the quantity of training samples and the extent of visual inspections carried out by experts for error correction. Nevertheless, collecting voxel-level labels and performing manual quality control on high-dimensional neuroimages (e.g., 3D MRI) are expensive and time-consuming in many medical studies. In this paper, we study the problem of one-shot joint extraction, registration and segmentation in neuroimaging data, which exploits only one labeled template image (a.k.a. atlas) and a few unlabeled raw images for training. We propose a unified end-to-end framework, called JERS, to jointly optimize the extraction, registration and segmentation tasks, allowing feedback among them. Specifically, we use a group of extraction, registration and segmentation modules to learn the extraction mask, transformation and segmentation mask, where modules are interconnected and mutually reinforced by self-supervision. Empirical results on real-world datasets demonstrate that our proposed method performs exceptionally in the extraction, registration and segmentation tasks. Our code and data can be found at https://github.com/Anonymous4545/JERS|脑提取、配准和分割是神经影像学研究中不可缺少的预处理步骤。目的是从原始成像扫描(即提取步骤)中提取大脑，将其与目标大脑图像(即注册步骤)对齐，并标记解剖大脑区域(即分割步骤)。传统的研究通常侧重于在监督环境下为提取、配准和分割任务开发单独的方法。这些方法的性能在很大程度上取决于训练样本的数量和专家为纠正错误而进行的目视检查的程度。然而，在许多医学研究中，收集体素级标签和对高维神经图像(例如3D MRI)进行手动质量控制是昂贵和耗时的。本文研究了神经影像数据的一次性关节提取、配准和分割问题，该方法只利用一个标记的模板图像(又称地图集)和少量未标记的原始图像进行训练。我们提出了一个统一的端到端框架，称为 JERS，以联合优化提取，注册和分割任务，允许它们之间的反馈。具体来说，我们使用一组抽取、配准和分割模块来学习抽取掩模、变换和分割掩模，这些模块通过自我监督相互联系、相互增强。对实际数据集的实验结果表明，该方法在提取、配准和分割任务中表现优异。我们的代码和数据可以在 https://github.com/anonymous4545/jers 找到|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=One-shot+Joint+Extraction,+Registration+and+Segmentation+of+Neuroimaging+Data)|0|
|[Learning Autoregressive Model in LSM-Tree based Store](https://doi.org/10.1145/3580305.3599405)|Yunxiang Su, Wenxuan Ma, Shaoxu Song|Tsinghua University|Database-native machine learning operators are highly desired for efficient I/O and computation costs. While most existing machine learning algorithms assume the time series data fully available and readily ordered by timestamps, it is not the case in practice. Commodity time series databases store the data in pages with possibly overlapping time ranges, known as LSM-Tree based storage. Data points in a page could be incomplete, owing to either missing values or out-of-order arrivals, which may be inserted by the imputed or delayed points in the following pages. Likewise, data points in a page could also be updated by others in another page, for dirty data repairing or re-transmission. A straightforward idea is thus to first merge and order the data points by timestamps, and then apply the existing learning algorithms. It is not only costly in I/O but also prevents pre-computation of model learning. In this paper, we propose to offline learn the AR models locally in each page on incomplete data, and online aggregate the stored models in different pages with the consideration of the aforesaid inserted and updated data points. Remarkably, the proposed method has been deployed and included as a function in an open source time series database, Apache IoTDB. Extensive experiments in the system demonstrate that our proposal LSMAR shows up to one order-of-magnitude improvement in learning time cost. It needs only about 10s of milliseconds for learning over 1 million data points.|数据库本地机器学习操作符对于高效的 I/O 和计算成本是非常重要的。虽然大多数现有的机器学习算法都假设时间序列数据完全可用并且按时间戳排序，但实际情况并非如此。商品时间序列数据库将数据存储在可能有重叠时间范围的页面中，称为基于 LSM-Tree 的存储。页面中的数据点可能是不完整的，这是由于缺少值或到达顺序不正确造成的，这些数据点可能是由以下页面中的估算点或延迟点插入的。同样，页中的数据点也可以由另一页中的其他数据点更新，以便进行脏数据修复或重新传输。因此，一个简单的想法是首先根据时间戳对数据点进行合并和排序，然后应用现有的学习算法。它不仅在 I/O 代价昂贵，而且也阻碍了模型学习的预计算。本文提出在不完全数据的每个页面上离线学习 AR 模型，并考虑上述插入和更新的数据点，在线聚合不同页面上存储的 AR 模型。值得注意的是，提出的方法已经被部署，并作为一个函数包含在开放源码时间序列数据库 Apache IoTDB 中。在系统中的大量实验表明，我们的建议 LSMAR 显示了一个数量级的学习时间成本的改善。它只需要大约10毫秒就可以学习超过100万个数据点。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Autoregressive+Model+in+LSM-Tree+based+Store)|0|
|[Enhance Diffusion to Improve Robust Generalization](https://doi.org/10.1145/3580305.3599333)|Jianhui Sun, Sanchit Sinha, Aidong Zhang|University of Virginia|Deep neural networks are susceptible to human imperceptible adversarial perturbations. One of the strongest defense mechanisms is \emph{Adversarial Training} (AT). In this paper, we aim to address two predominant problems in AT. First, there is still little consensus on how to set hyperparameters with a performance guarantee for AT research, and customized settings impede a fair comparison between different model designs in AT research. Second, the robustly trained neural networks struggle to generalize well and suffer from tremendous overfitting. This paper focuses on the primary AT framework - Projected Gradient Descent Adversarial Training (PGD-AT). We approximate the dynamic of PGD-AT by a continuous-time Stochastic Differential Equation (SDE), and show that the diffusion term of this SDE determines the robust generalization. An immediate implication of this theoretical finding is that robust generalization is positively correlated with the ratio between learning rate and batch size. We further propose a novel approach, \emph{Diffusion Enhanced Adversarial Training} (DEAT), to manipulate the diffusion term to improve robust generalization with virtually no extra computational burden. We theoretically show that DEAT obtains a tighter generalization bound than PGD-AT. Our empirical investigation is extensive and firmly attests that DEAT universally outperforms PGD-AT by a significant margin.|深层神经网络容易受到人类无法察觉的对抗性扰动的影响。最强大的防御机制之一就是防御训练。在本文中，我们的目标是解决两个主要的问题在 AT。首先，关于如何为 AT 研究设置具有性能保证的超参数仍然没有多少共识，定制的设置妨碍了 AT 研究中不同模型设计之间的公平比较。其次，经过严格训练的神经网络难以很好地推广，并遭受了极大的过度拟合。这篇文章主要关注的是初级 AT 框架——梯度下降法对抗训练(pgd-AT)。我们用一个连续时间随机微分方程(SDE)来近似 PGD-AT 的动态，并表明这个 SDE 的扩散项决定了鲁棒性的推广。这一理论发现的一个直接含义是，鲁棒推广与学习率和批量大小之间的比率正相关。我们进一步提出了一种新的方法，即扩散增强对抗训练(DEAT) ，通过操纵扩散项来提高鲁棒推广能力，而且几乎没有额外的计算负担。从理论上证明了 DEAT 比 PGD-AT 具有更紧的推广界。我们的实证研究是广泛和坚定地证明，DEAT 普遍优于 PGD-AT 的显着差距。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhance+Diffusion+to+Improve+Robust+Generalization)|0|
|[ShapleyFL: Robust Federated Learning Based on Shapley Value](https://doi.org/10.1145/3580305.3599500)|Qiheng Sun, Xiang Li, Jiayao Zhang, Li Xiong, Weiran Liu, Jinfei Liu, Zhan Qin, Kui Ren||Federated Learning (FL) allows clients to form a consortium to train a global model under the orchestration of a central server while keeping data on the local client without sharing it, thus mitigating data privacy issues. However, training a robust global model is challenging since the local data is invisible to the server. The local data of clients are naturally heterogeneous, while some clients can use corrupted data or send malicious updates to interfere with the training process artificially. Meanwhile, communication and computation costs are inevitable challenges in designing a practical FL algorithm. In this paper, to improve the robustness of FL, we propose a Shapley value-inspired adaptive weighting mechanism, which regards the FL training as sequential cooperative games and adjusts clients' weights according to their contributions. We also develop a client sampling strategy based on importance sampling, which can reduce the communication cost by optimizing the variance of the global updates according to the weights of clients. Furthermore, to diminish the computation cost of the server, we propose a weight calculation method by estimating differences between the Shapley value of clients. Our experimental results on several real data sets demonstrate the effectiveness of our approaches.|联邦学习(Federated Learning，FL)允许客户端组成一个联盟，在中央服务器的协调下培训全局模型，同时在不共享数据的情况下将数据保存在本地客户端，从而减少数据隐私问题。然而，训练一个健壮的全局模型是具有挑战性的，因为本地数据对于服务器是不可见的。客户端的本地数据自然是异构的，而一些客户端可以使用损坏的数据或发送恶意更新人为地干扰训练过程。同时，在设计实用的 FL 算法时，通信和计算开销是不可避免的挑战。为了提高 FL 的鲁棒性，提出了一种基于 Shapley 值的自适应加权机制，该机制将 FL 训练视为序列合作对策，并根据客户的贡献调整权重。提出了一种基于重要性抽样的客户端抽样策略，该策略根据客户端的权重，通过优化全局更新的方差来降低通信成本。此外，为了降低服务器的计算成本，我们提出了一种通过估计客户端 Shapley 值之间的差异来进行权重计算的方法。我们在几个实际数据集上的实验结果证明了我们方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ShapleyFL:+Robust+Federated+Learning+Based+on+Shapley+Value)|0|
|[Mastering Stock Markets with Efficient Mixture of Diversified Trading Experts](https://doi.org/10.1145/3580305.3599424)|Shuo Sun, Xinrun Wang, Wanqi Xue, Xiaoxuan Lou, Bo An||Quantitative stock investment is a fundamental financial task that highly relies on accurate prediction of market status and profitable investment decision making. Despite recent advances in deep learning (DL) have shown stellar performance on capturing trading opportunities in the stochastic stock market, the performance of existing DL methods is unstable with sensitivity to network initialization and hyperparameter selection. One major limitation of existing works is that investment decisions are made based on one individual neural network predictor with high uncertainty, which is inconsistent with the workflow in real-world trading firms. To tackle this limitation, we propose AlphaMix, a novel three-stage mixture-of-experts (MoE) framework for quantitative investment to mimic the efficient bottom-up hierarchical trading strategy design workflow of successful trading companies. In Stage one, we introduce an efficient ensemble learning method, whose computational and memory costs are significantly lower comparing to traditional ensemble methods, to train multiple groups of trading experts with personalised market understanding and trading styles. In Stage two, we collect diversified investment suggestions through building a pool of trading experts utilizing hyperparameter level and initialization level diversity of neural networks for post hoc ensemble construction. In Stage three, we design three different mechanisms, namely as-needed router, with-replacement selection and integrated expert soup, to dynamically pick experts from the expert pool, which takes the responsibility of a portfolio manager. Through extensive experiments on US and Chinese stock markets, we demonstrate that AlphaMix significantly outperforms many state-of-the-art baselines in terms of 7 popular financial criteria.|股票定量投资是一项基础性的金融工作，它高度依赖于对市场状况的准确预测和有利可图的投资决策。尽管近年来深度学习(DL)在捕捉随机股票市场交易机会方面取得了显著的进展，但现有的深度学习方法由于对网络初始化和超参数选择的敏感性而表现出不稳定性。现有工作的一个主要局限性是，投资决策是基于一个具有高不确定性的个体神经网络预测器，这与现实世界中交易公司的工作流程不一致。为了解决这一局限性，我们提出了一种新的三阶段专家混合(MoE)量化投资框架 AlphaMix，以模拟成功交易公司有效的自下而上的层次化交易策略设计流程。在第一阶段，我们引入了一种有效的集成学习方法，它的计算和存储成本明显低于传统的集成方法，以培训具有个性化市场理解和交易风格的多组交易专家。在第二阶段，我们利用神经网络的超参数水平和初始化水平多样性，建立一个交易专家库，收集多元化的投资建议，用于事后集成构建。在第三阶段，我们设计了三种不同的机制，即随需应变的路由器、带替换的选择和集成的专家汤，来动态地从专家池中挑选专家，这需要一个投资组合经理的责任。通过在美国和中国股市的大量实验，我们证明 AlphaMix 在7个流行的金融标准方面明显优于许多最先进的基准。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mastering+Stock+Markets+with+Efficient+Mixture+of+Diversified+Trading+Experts)|0|
|[Joint Pre-training and Local Re-training: Transferable Representation Learning on Multi-source Knowledge Graphs](https://doi.org/10.1145/3580305.3599397)|Zequn Sun, Jiacheng Huang, Jinghao Lin, Xiaozhou Xu, Qijin Chen, Wei Hu|Alibaba Group; Nanjing University|In this paper, we present the ``joint pre-training and local re-training'' framework for learning and applying multi-source knowledge graph (KG) embeddings. We are motivated by the fact that different KGs contain complementary information to improve KG embeddings and downstream tasks. We pre-train a large teacher KG embedding model over linked multi-source KGs and distill knowledge to train a student model for a task-specific KG. To enable knowledge transfer across different KGs, we use entity alignment to build a linked subgraph for connecting the pre-trained KGs and the target KG. The linked subgraph is re-trained for three-level knowledge distillation from the teacher to the student, i.e., feature knowledge distillation, network knowledge distillation, and prediction knowledge distillation, to generate more expressive embeddings. The teacher model can be reused for different target KGs and tasks without having to train from scratch. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our framework.|本文提出了一种用于学习和应用多源知识图(KG)嵌入的“联合预训练和局部再训练”框架。我们的动机是，不同的幼稚园包含互补的信息，以改善幼稚园的嵌入和下游的任务。通过对一个大型教师幼儿园嵌入模型的预训练，提取知识，对一个任务特定的幼儿园进行学生模型的训练。为了实现不同幼稚园之间的知识转移，我们使用实体对齐来建立连接子图，以连接预先训练的幼稚园和目标幼稚园。通过对连通子图的重新训练，实现了从教师到学生的三级知识提取，即特征知识提取、网络知识提取和预测知识提取，从而生成更具表达性的嵌入。教师模型可以重用于不同的目标幼儿园和任务，而无需从头开始培训。我们进行了广泛的实验，以证明我们的框架的有效性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Joint+Pre-training+and+Local+Re-training:+Transferable+Representation+Learning+on+Multi-source+Knowledge+Graphs)|0|
|[PERT-GNN: Latency Prediction for Microservice-based Cloud-Native Applications via Graph Neural Networks](https://doi.org/10.1145/3580305.3599465)|Da Sun Handason Tam, Yang Liu, Huanle Xu, Siyue Xie, Wing Cheong Lau|Shanghai University; University of Macau; The Chinese University of Hong Kong|Cloud-native applications using microservice architectures are rapidly replacing traditional monolithic applications. To meet end-to-end QoS guarantees and enhance user experience, each component microservice must be provisioned with sufficient resources to handle incoming API calls. Accurately predicting the latency of microservices-based applications is critical for optimizing resource allocation, which turns out to be extremely challenging due to the complex dependencies between microservices and the inherent stochasticity. To tackle this problem, various predictors have been designed based on the Microservice Call Graph. However, Microservice Call Graphs do not take into account the API-specific information, cannot capture important temporal dependencies, and cannot scale to large-scale applications. In this paper, we propose PERT-GNN, a generic graph neural network based framework to predict the end-to-end latency for microservice applications. PERT-GNN characterizes the interactions or dependency of component microservices observed from prior execution traces of the application using the Program Evaluation and Review Technique (PERT). We then construct a graph neural network based on the generated PERT Graphs, and formulate the latency prediction task as a supervised graph regression problem using the graph transformer method. PERT-GNN can capture the complex temporal causality of different microservice traces, thereby producing more accurate latency predictions for various applications. Evaluations based on datasets generated from common benchmarks and large-scale Alibaba microservice traces show that PERT-GNN can outperform other models by a large margin. In particular, PERT-GNN is able to predict the latency of microservice applications with less than 12% mean absolute percentage error.|使用微服务架构的本地云应用程序正在迅速取代传统的单片机应用程序。为了满足端到端的 QoS 保证和增强用户体验，必须为每个组件微服务提供足够的资源来处理传入的 API 调用。准确预测基于微服务的应用程序的延迟对于优化资源分配至关重要，但由于微服务与固有的随机性之间的复杂依赖关系，资源分配极具挑战性。为了解决这个问题，基于微服务调用图设计了各种预测器。但是，Microservice Call Graphs 没有考虑特定于 API 的信息，不能捕获重要的时间依赖关系，并且不能扩展到大规模应用程序。在本文中，我们提出了 PERT-GNN，这是一个基于通用图形神经网络的框架，用于预测微服务应用的端到端延迟。PERT-GNN 使用程序评估和审查技术(Program Value and Review Technology，PERT)描述从应用程序的先前执行跟踪中观察到的组件微服务的交互或依赖性。然后基于生成的 PERT 图构造一个图形神经网络，并利用图形转换方法将延迟预测任务表达为一个有监督的图形回归问题。PERT-GNN 可以捕获不同微服务跟踪的复杂时间因果关系，从而为各种应用程序产生更准确的延迟预测。基于通用基准测试和大规模阿里巴巴微服务跟踪产生的数据集的评估表明，PERT-GNN 的表现大大优于其他模型。特别是 PERT-GNN 能够预测微服务应用程序的延迟，平均绝对误差小于12% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PERT-GNN:+Latency+Prediction+for+Microservice-based+Cloud-Native+Applications+via+Graph+Neural+Networks)|0|
|[ExplainableFold: Understanding AlphaFold Prediction with Explainable AI](https://doi.org/10.1145/3580305.3599337)|Juntao Tan, Yongfeng Zhang|Rutgers University|This paper presents ExplainableFold, an explainable AI framework for protein structure prediction. Despite the success of AI-based methods such as AlphaFold in this field, the underlying reasons for their predictions remain unclear due to the black-box nature of deep learning models. To address this, we propose a counterfactual learning framework inspired by biological principles to generate counterfactual explanations for protein structure prediction, enabling a dry-lab experimentation approach. Our experimental results demonstrate the ability of ExplainableFold to generate high-quality explanations for AlphaFold's predictions, providing near-experimental understanding of the effects of amino acids on 3D protein structure. This framework has the potential to facilitate a deeper understanding of protein structures.|本文提出了一个可解释的人工智能框架，用于蛋白质结构预测。尽管 AlphaFold 等基于人工智能的方法在这一领域取得了成功，但由于深度学习模型的黑盒子特性，他们预测的潜在原因仍不清楚。为了解决这个问题，我们提出了一个受生物学原理启发的反事实学习框架，以产生对蛋白质结构预测的反事实解释，从而实现了干实验室实验方法。我们的实验结果证明了 ExplainableFold 对 AlphaFold 的预测产生高质量解释的能力，为氨基酸对3D 蛋白质结构的影响提供了近乎实验性的理解。这个框架有可能促进对蛋白质结构的更深入的理解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ExplainableFold:+Understanding+AlphaFold+Prediction+with+Explainable+AI)|0|
|[Feature-based Learning for Diverse and Privacy-Preserving Counterfactual Explanations](https://doi.org/10.1145/3580305.3599343)|Vy Vo, Trung Le, Van Nguyen, He Zhao, Edwin V. Bonilla, Gholamreza Haffari, Dinh Q. Phung||Interpretable machine learning seeks to understand the reasoning process of complex black-box systems that are long notorious for lack of explainability. One flourishing approach is through counterfactual explanations, which provide suggestions on what a user can do to alter an outcome. Not only must a counterfactual example counter the original prediction from the black-box classifier but it should also satisfy various constraints for practical applications. Diversity is one of the critical constraints that however remains less discussed. While diverse counterfactuals are ideal, it is computationally challenging to simultaneously address some other constraints. Furthermore, there is a growing privacy concern over the released counterfactual data. To this end, we propose a feature-based learning framework that effectively handles the counterfactual constraints and contributes itself to the limited pool of private explanation models. We demonstrate the flexibility and effectiveness of our method in generating diverse counterfactuals of actionability and plausibility. Our counterfactual engine is more efficient than counterparts of the same capacity while yielding the lowest re-identification risks.|可解释的机器学习试图理解长期以来因缺乏可解释性而臭名昭著的复杂黑盒系统的推理过程。一个蓬勃发展的方法是通过反事实的解释，提供建议的用户可以做什么来改变一个结果。一个反事实的例子不仅要对抗黑盒分类器的原始预测，而且还要满足实际应用的各种约束条件。多样性是一个关键的制约因素，但仍然很少讨论。虽然不同的反事实是理想的，但同时处理其他一些约束在计算上是具有挑战性的。此外，公开的反事实数据引发了越来越多的隐私问题。为此，我们提出了一个基于特征的学习框架，有效地处理反事实约束，并为有限的私人解释模型库做出贡献。我们证明了我们的方法的灵活性和有效性，在产生不同的反事实的可行性和合理性。我们的反事实引擎是更有效率的同类产品的能力，同时产生最低的重新识别风险。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Feature-based+Learning+for+Diverse+and+Privacy-Preserving+Counterfactual+Explanations)|0|
|[Adversaries with Limited Information in the Friedkin-Johnsen Model](https://doi.org/10.1145/3580305.3599255)|Sijing Tu, Stefan Neumann, Aristides Gionis|KTH Royal Institute of Technology|In recent years, online social networks have been the target of adversaries who seek to introduce discord into societies, to undermine democracies and to destabilize communities. Often the goal is not to favor a certain side of a conflict but to increase disagreement and polarization. To get a mathematical understanding of such attacks, researchers use opinion-formation models from sociology, such as the Friedkin--Johnsen model, and formally study how much discord the adversary can produce when altering the opinions for only a small set of users. In this line of work, it is commonly assumed that the adversary has full knowledge about the network topology and the opinions of all users. However, the latter assumption is often unrealistic in practice, where user opinions are not available or simply difficult to estimate accurately. To address this concern, we raise the following question: Can an attacker sow discord in a social network, even when only the network topology is known? We answer this question affirmatively. We present approximation algorithms for detecting a small set of users who are highly influential for the disagreement and polarization in the network. We show that when the adversary radicalizes these users and if the initial disagreement/polarization in the network is not very high, then our method gives a constant-factor approximation on the setting when the user opinions are known. To find the set of influential users, we provide a novel approximation algorithm for a variant of MaxCut in graphs with positive and negative edge weights. We experimentally evaluate our methods, which have access only to the network topology, and we find that they have similar performance as methods that have access to the network topology and all user opinions. We further present an NP-hardness proof, which was an open question by Chen and Racz [IEEE Trans. Netw. Sci. Eng., 2021].|近年来，在线社交网络一直是敌人的目标，他们试图将不和谐引入社会，破坏民主，破坏社区稳定。通常情况下，目标不是支持冲突的某一方，而是增加分歧和两极分化。为了从数学上理解这种攻击，研究人员使用了社会学中的观点形成模型，比如 Friedkin-Johnsen 模型，并正式研究了当只为一小部分用户改变观点时，对手可以产生多少不和谐。在这方面的工作中，通常假设对手完全了解网络拓扑和所有用户的意见。然而，后一种假设在实践中往往是不现实的，因为用户的意见是不可用的，或者仅仅是难以准确估计。为了解决这个问题，我们提出以下问题: 攻击者能否在社交网络中播种不和谐因素，即使只知道网络拓扑？我们肯定地回答这个问题。我们提出了一种近似算法来检测少量的用户，这些用户对网络中的分歧和极化有很大的影响。我们证明了当对手激化这些用户时，如果网络中最初的分歧/极化不是很高，那么我们的方法在用户意见已知的情况下给出了一个常数因子近似的设置。为了找到一组有影响力的用户，我们提供了一个新的近似演算法，对于一个具有正负边权值的图的变体 MaxCut。我们通过实验来评估我们的方法，这些方法只能访问网络拓扑，我们发现它们的性能与那些能访问网络拓扑和所有用户意见的方法相似。我们进一步提出了一个 NP 硬性证明，这是陈和 Racz [ IEEE Trans ]提出的一个开放性问题。网络。科学。工程师，2021]。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adversaries+with+Limited+Information+in+the+Friedkin-Johnsen+Model)|0|
|[Pattern Expansion and Consolidation on Evolving Graphs for Continual Traffic Prediction](https://doi.org/10.1145/3580305.3599463)|Binwu Wang, Yudong Zhang, Xu Wang, Pengkun Wang, Zhengyang Zhou, Lei Bai, Yang Wang|Suzhou Institute for Advanced Research, University of Science and Technology of China; Shanghai AI Laboratory; University of Science and Technology of China|Recently, spatiotemporal graph convolutional networks are becoming popular in the field of traffic flow prediction and significantly improve prediction accuracy. However, the majority of existing traffic flow prediction models are tailored to static traffic networks and fail to model the continuous evolution and expansion of traffic networks. In this work, we move to investigate the challenge of traffic flow prediction on an expanding traffic network. And we propose an efficient and effective continual learning framework to achieve continuous traffic flow prediction without the access to historical graph data, namely Pattern Expansion and Consolidation based on Pattern Matching based (PECPM). Specifically, we first design a pattern bank based on pattern matching to store representative patterns of the road network. With the expansion of the road network, the model configured with such a bank module can achieve continuous traffic prediction by effectively managing patterns stored in the bank. The core idea is to continuously update new patterns while consolidating learned ones. Specifically, we design a pattern expansion mechanism that can detect evolved and new patterns from the updated network, then these unknown patterns are expanded into the pattern bank to adapt to the updated road network. Additionally, we propose a pattern consolidation mechanism that includes both a bank preservation mechanism and a pattern traceability mechanism. This can effectively consolidate the learned patterns in the bank without requiring access to detailed historical graph data. We construct experiments on real-world traffic datasets to demonstrate the competitive performance, superior efficiency, and strong generalization ability of PECPM.|近年来，时空图卷积网络逐渐成为交通流预测领域的研究热点，并显著提高了预测精度。然而，现有的交通流预测模型大多是针对静态交通网络的，不能模拟交通网络的连续演化和扩展。在这项工作中，我们着手研究的挑战，交通流量预测在一个不断扩大的交通网络。我们提出了一个有效的连续学习框架，即基于模式匹配的模式扩展和合并(pecPM) ，以实现连续的交通流量预测，而无需使用历史图形数据。具体来说，我们首先设计一个基于模式匹配的模式库来存储具有代表性的道路网模式。随着道路网络的扩展，配置了这种银行模块的模型可以通过有效地管理存储在银行中的模式来实现连续的交通预测。其核心思想是不断更新新的模式，同时巩固已学习的模式。具体来说，我们设计了一种模式扩展机制，可以从更新后的路网中检测出进化的和新的模式，然后将这些未知的模式扩展到模式库中，以适应更新后的路网。此外，我们还提出了一种模式整合机制，该机制包括银行保存机制和模式跟踪机制。这可以有效地巩固银行中学到的模式，而不需要访问详细的历史图表数据。通过在实际交通数据集上进行实验，验证了 PECPM 的竞争性能、优越的效率和较强的泛化能力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pattern+Expansion+and+Consolidation+on+Evolving+Graphs+for+Continual+Traffic+Prediction)|0|
|[Financial Default Prediction via Motif-preserving Graph Neural Network with Curriculum Learning](https://doi.org/10.1145/3580305.3599351)|Daixin Wang, Zhiqiang Zhang, Yeyu Zhao, Kai Huang, Yulin Kang, Jun Zhou||User financial default prediction plays a critical role in credit risk forecasting and management. It aims at predicting the probability that the user will fail to make the repayments in the future. Previous methods mainly extract a set of user individual features regarding his own profiles and behaviors and build a binary-classification model to make default predictions. However, these methods cannot get satisfied results, especially for users with limited information. Although recent efforts suggest that default prediction can be improved by social relations, they fail to capture the higher-order topology structure at the level of small subgraph patterns. In this paper, we fill in this gap by proposing a motif-preserving Graph Neural Network with curriculum learning (MotifGNN) to jointly learn the lower-order structures from the original graph and higher-order structures from multi-view motif-based graphs for financial default prediction. Specifically, to solve the problem of weak connectivity in motif-based graphs, we design the motif-based gating mechanism. It utilizes the information learned from the original graph with good connectivity to strengthen the learning of the higher-order structure. And considering that the motif patterns of different samples are highly unbalanced, we propose a curriculum learning mechanism on the whole learning process to more focus on the samples with uncommon motif distributions. Extensive experiments on one public dataset and two industrial datasets all demonstrate the effectiveness of our proposed method.|用户金融违约预测在信用风险预测和管理中起着至关重要的作用。它的目的是预测用户未来偿还贷款的可能性。以往的方法主要是根据用户的个人资料和行为提取用户的一系列个性特征，建立二元分类模型进行默认预测。然而，这些方法都不能得到满意的结果，特别是对于信息量有限的用户。虽然最近的研究表明，缺省预测可以通过社会关系来改进，但是它们未能在小子图模式层次上捕捉到高阶拓扑结构。本文提出了一种基于课程学习的保主题图神经网络(MotifGNN)来联合学习原始图中的低阶结构和基于多视图的高阶结构，用于金融违约预测。具体来说，为了解决基于图模的图的弱连通性问题，我们设计了基于图模的门控机制。它利用从原始图中学到的连通性好的信息来加强对高阶结构的学习。同时考虑到不同样本的母题模式具有高度的不平衡性，我们提出了一种基于整个学习过程的课程学习机制，更多地关注母题分布不均匀的样本。在一个公共数据集和两个工业数据集上的大量实验都证明了该方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Financial+Default+Prediction+via+Motif-preserving+Graph+Neural+Network+with+Curriculum+Learning)|0|
|[Accelerating Antimicrobial Peptide Discovery with Latent Structure](https://doi.org/10.1145/3580305.3599249)|Danqing Wang, Zeyu Wen, Fei Ye, Lei Li, Hao Zhou||Antimicrobial peptides (AMPs) are promising therapeutic approaches against drug-resistant pathogens. Recently, deep generative models are used to discover new AMPs. However, previous studies mainly focus on peptide sequence attributes and do not consider crucial structure information. In this paper, we propose a latent sequence-structure model for designing AMPs (LSSAMP). LSSAMP exploits multi-scale vector quantization in the latent space to represent secondary structures (e.g. alpha helix and beta sheet). By sampling in the latent space, LSSAMP can simultaneously generate peptides with ideal sequence attributes and secondary structures. Experimental results show that the peptides generated by LSSAMP have a high probability of antimicrobial activity. Our wet laboratory experiments verified that two of the 21 candidates exhibit strong antimicrobial activity. The code is released at https://github.com/dqwang122/LSSAMP.|抗微生物肽是对抗耐药性病原体的有希望的治疗方法。最近，深生成模型被用来发现新的 AMP。然而，以往的研究主要集中在氨基酸测序属性，而没有考虑关键的结构信息。在本文中，我们提出了一个潜在的序列结构模型来设计 AMP (LSSAMP)。LSSAMP 利用潜在空间中的多尺度向量量化来表示二级结构(例如 alpha 螺旋和 beta 板)。通过在潜伏空间采样，LSSAMP 可以同时产生具有理想序列属性和二级结构的多肽。实验结果表明，LSSAMP 产生的多肽具有很高的抗菌活性。我们的湿实验室实验证实，21个候选人中的两个表现出强大的抗菌活性。密码在 https://github.com/dqwang122/lssamp 发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Accelerating+Antimicrobial+Peptide+Discovery+with+Latent+Structure)|0|
|[Efficient and Effective Edge-wise Graph Representation Learning](https://doi.org/10.1145/3580305.3599321)|Hewen Wang, Renchi Yang, Keke Huang, Xiaokui Xiao||Graph representation learning (GRL) is a powerful tool for graph analysis, which has gained massive attention from both academia and industry due to its superior performance in various real-world applications. However, the majority of existing works for GRL are dedicated to node-based tasks and thus focus on producing node representations. Despite such methods can be used to derive edge representations by regarding edges as nodes, they suffer from sub-par result utility in practical edge-wise applications, such as financial fraud detection and review spam combating, due to neglecting the unique properties of edges and their inherent drawbacks. Moreover, to our knowledge, there is a paucity of research devoted to edge representation learning. These methods either require high computational costs in sampling random walks or yield severely compromised representation quality because of falling short of capturing high-order information between edges. To address these challenges, we present TER and AER, which generate high-quality edge representation vectors based on the graph structure surrounding edges and edge attributes, respectively. In particular, TER can accurately encode high-order proximities of edges into low-dimensional vectors in a practically efficient and theoretically sound way, while AER augments edge attributes through a carefully-designed feature aggregation scheme. Our extensive experimental study demonstrates that the combined edge representations of TER and AER can achieve significantly superior performance in terms of edge classification on 8 real-life datasets, while being up to one order of magnitude faster than 16 baselines on large graphs.|图表示学习(GRL)是一种强有力的图分析工具，由于其在各种实际应用中的优越性能，受到了学术界和工业界的广泛关注。然而，GRL 现有的大部分工作都致力于基于节点的任务，因此侧重于生成节点表示。尽管这些方法可以通过将边缘视为节点来获得边缘表示，但由于忽视了边缘的独特性质及其固有的缺点，它们在实际的边缘应用(如金融欺诈检测和审查垃圾邮件打击)中的结果效用低于平均值。此外，据我们所知，有关边缘表示学习的研究很少。这些方法要么需要较高的计算代价来采样随机游动，要么由于不能捕获边之间的高阶信息而严重影响表示质量。为了解决这些问题，我们提出了 TER 和 AER，它们分别基于图的边缘结构和边缘属性生成高质量的边缘表示向量。特别是，TER 可以准确地将高阶接近边编码成低维向量在一个实际有效和理论上合理的方式，而 AER 增强边缘属性通过精心设计的特征聚合方案。我们广泛的实验研究表明，在8个实际数据集上，TER 和 AER 的结合边缘表示可以在边缘分类方面取得明显优越的性能，同时比大型图表上的16个基线快一个数量级。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+and+Effective+Edge-wise+Graph+Representation+Learning)|0|
|[PROSE: Graph Structure Learning via Progressive Strategy](https://doi.org/10.1145/3580305.3599476)|Huizhao Wang, Yao Fu, Tao Yu, Linghui Hu, Weihao Jiang, Shiliang Pu||Graph Neural Networks (GNNs) have been a powerful tool to acquire high-quality node representations dealing with graphs, which strongly depends on a promising graph structure. In the real world scenarios, it is inevitable to introduce noises in graph topology. To prevent GNNs from the disturbance of irrelevant edges or missing edges, graph structure learning is proposed and has attracted considerable attentions in recent years. In this paper, we argue that current graph structure learning methods still pay no regard to the status of nodes and just judge all of their connections simultaneously using a monotonous standard, which will lead to indeterminacy and instability in the optimization process. We designate these methods as status-unaware models. To demonstrate the rationality of our point of view, we conduct exploratory experiments on publicly available datasets, and discover some exciting observations. Afterwards, we propose a new model named Graph Structure Learning via Progressive Strategy (PROSE) according to the observations, which uses a progressive strategy to acquire ideal graph structure in a status-aware way. Concretely, PROSE consists of progressive structure splitting module (PSS) and progressive structure refining module (PSR) to modify node connections according to their global potency, and we also introduce horizontal position encoding and vertical position encoding in order to capture fruitful graph topology information ignored by previous methods. On several widely-used graph datasets, we conduct extensive experiments to demonstrate the effectiveness of our model, and the source code 1 https://github.com/tigerbunny2023/PROSE is provided.|图形神经网络(GNN)是获得高质量图形节点表示的有力工具，它对有前途的图结构有很大的依赖性。在真实场景中，图的拓扑结构中不可避免地会引入噪声。为了防止 GNN 受到无关边或缺边的干扰，图结构学习被提出，近年来引起了人们的广泛关注。本文认为，目前的图结构学习方法仍然不考虑节点的状态，只是利用单调的标准同时判断它们之间的所有连接，这将导致优化过程中的不确定性和不稳定性。我们将这些方法指定为不知道状态的模型。为了证明我们观点的合理性，我们对公开可用的数据集进行了探索性实验，并发现了一些令人兴奋的观察结果。然后，根据观察结果，提出了一种基于渐进策略的图结构学习(PROSE)模型，该模型采用渐进策略以状态感知的方式获取理想的图结构。具体来说，PROSE 由渐进结构分裂模块(PSS)和渐进结构细化模块(PSR)组成，根据其全局效能修改节点连接，同时引入了水平位置编码和垂直位置编码，以获取前人忽略的丰富的图拓扑信息。在几个广泛使用的图形数据集上，我们进行了广泛的实验来证明我们的模型的有效性，并提供了源代码1 https://github.com/tigerbunny2023/prose。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PROSE:+Graph+Structure+Learning+via+Progressive+Strategy)|0|
|[Empower Post-hoc Graph Explanations with Information Bottleneck: A Pre-training and Fine-tuning Perspective](https://doi.org/10.1145/3580305.3599330)|Jihong Wang, Minnan Luo, Jundong Li, Yun Lin, Yushun Dong, Jin Song Dong, Qinghua Zheng||Researchers recently investigated to explain Graph Neural Networks (GNNs) on the access to a task-specific GNN, which may hinder their wide applications in practice. Specifically, task-specific explanation methods are incapable of explaining pretrained GNNs whose downstream tasks are usually inaccessible, not to mention giving explanations for the transferable knowledge in pretrained GNNs. Additionally, task-specific methods only consider target models' output in the label space, which are coarse-grained and insufficient to reflect the model's internal logic. To address these limitations, we consider a two-stage explanation strategy, i.e., explainers are first pretrained in a task-agnostic fashion in the representation space and then further fine-tuned in the task-specific label space and representation space jointly if downstream tasks are accessible. The two-stage explanation strategy endows post-hoc graph explanations with the applicability to pretrained GNNs where downstream tasks are inaccessible and the capacity to explain the transferable knowledge in the pretrained GNNs. Moreover, as the two-stage explanation strategy explains the GNNs in the representation space, the fine-grained information in the representation space also empowers the explanations. Furthermore, to achieve a trade-off between the fidelity and intelligibility of explanations, we propose an explanation framework based on the Information Bottleneck principle, named Explainable Graph Information Bottleneck (EGIB). EGIB subsumes the task-specific explanation and task-agnostic explanation into a unified framework. To optimize EGIB objective, we derive a tractable bound and adopt a simple yet effective explanation generation architecture. Based on the unified framework, we further theoretically prove that task-agnostic explanation is a relaxed sufficient condition of task-specific explanation, which indicates the transferability of task-agnostic explanations. Extensive experimental results demonstrate the effectiveness of our proposed explanation method.|研究人员最近调查解释图形神经网络(GNN)的访问任务特定的 GNN，这可能会阻碍其在实践中的广泛应用。具体而言，针对具体任务的解释方法无法解释预先训练的 GNN，因为它们的下游任务通常是无法访问的，更不用说对预先训练的 GNN 中的可转移知识作出解释了。此外，特定于任务的方法只考虑目标模型在标签空间中的输出，这种方法粒度粗糙，不足以反映模型的内部逻辑。为了解决这些局限性，我们考虑了一个两阶段的解释策略，即解释者首先在表示空间中以任务不可知的方式进行预训练，然后在任务特定的标签空间和表示空间中进一步微调，如果下游任务是可访问的。两阶段解释战略赋予事后图解对预先训练的国民导航系统的适用性，因为下游任务无法获得，并且有能力解释预先训练的国民导航系统中的可转移知识。此外，当两阶段解释策略解释表征空间中的 GNN 时，表征空间中的细粒度信息也增强了解释的能力。此外，为了在解释的真实性和可理解性之间取得平衡，我们提出了一个基于信息瓶颈原理的解释框架，命名为可解释图形信息瓶颈(EGIB)。EGIB 将任务特定的解释和任务不可知的解释包含在一个统一的框架中。为了优化 EGIB 目标，我们导出了一个易处理的界限，并采用了一个简单而有效的解释生成体系结构。在统一框架的基础上，进一步从理论上证明了任务不可知解释是任务特定解释的一个宽松的充分条件，表明了任务不可知解释的可迁移性。大量的实验结果证明了我们提出的解释方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Empower+Post-hoc+Graph+Explanations+with+Information+Bottleneck:+A+Pre-training+and+Fine-tuning+Perspective)|0|
|[WHEN: A Wavelet-DTW Hybrid Attention Network for Heterogeneous Time Series Analysis](https://doi.org/10.1145/3580305.3599549)|Jingyuan Wang, Chen Yang, Xiaohan Jiang, Junjie Wu||Given its broad applications, time series analysis has gained substantial research attention but remains a very challenging task. Recent years have witnessed the great success of deep learning methods, eg., CNN and RNN, in time series classification and forecasting, but heterogeneity as the very nature of time series has not yet been addressed adequately and remains the performance "treadstone." In this light, we argue that the intra-sequence non-stationarity and inter-sequence asynchronism are two types of heterogeneities widely existed in multiple times series, and propose a hybrid attention network called WHEN as deep learning solution. WHEN features in two attention mechanisms in two different modules. In the WaveAtt module, we propose a novel data-dependent wavelet function and integrate it into the BiLSTM network as the wavelet attention, for the purpose of analyzing dynamic frequency components in nonstationary time series. In the DTWAtt module, we transform the dynamic time warping (DTW) technique into the form as the DTW attention, where all input sequences are synchronized with a universal parameter sequence to overcome the time distortion problem in multiple time series. WHEN with the hybrid attentions is then formulated as task-dependent neural network for either classification or forecasting tasks. Extensive experiments on 30 UEA datasets and 3 real-world datasets with rich competitive baselines demonstrate the excellent performance of our model. The ability of WHEN in dealing with time series heterogeneities is also elaborately explored via specially designed analysis.|时间序列分析由于其广泛的应用，已经引起了人们的广泛关注，但仍然是一个非常具有挑战性的课题。近年来，CNN 和 RNN 等深度学习方法在时间序列分类和预测方面取得了巨大的成功，但作为时间序列本质的异质性尚未得到充分解决，仍然是其表现的“绊脚石”基于此，本文认为序列内的非平稳性和序列间的异步性是广泛存在于多时间序列中的两种异质性，并提出了一种称为“当”的混合注意网络作为深度学习的解决方案。在两个不同模块的两个注意机制中的特征。在 WaveAtt 模块中，我们提出了一种新的数据相关小波函数，并将其作为小波注意力集成到 BiLSTM 网络中，用于分析非平稳时间序列的动态频率成分。在 DTWAtt 模块中，我们将动态时间规整(dtW)技术转化为 DTW 注意力，其中所有输入序列与一个通用参数序列同步，以克服多个时间序列中的时间失真问题。当与混合注意力，然后公式化为任务相关的神经网络分类或预测任务。在30个 UEA 数据集和3个具有丰富竞争基线的真实世界数据集上的大量实验证明了该模型的优越性能。通过特别设计的分析方法，详细探讨了当前时间序列处理异质性的能力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=WHEN:+A+Wavelet-DTW+Hybrid+Attention+Network+for+Heterogeneous+Time+Series+Analysis)|0|
|[Federated Few-shot Learning](https://doi.org/10.1145/3580305.3599347)|Song Wang, Xingbo Fu, Kaize Ding, Chen Chen, Huiyuan Chen, Jundong Li|School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China; Shenzhen Institute of Artificial Intelligence and Robotics for Society, China|We are interested in developing a unified machine learning framework for effectively training machine learning models from many small data sources such as mobile devices. This is a commonly encountered situation in mobile computing scenarios, where data is scarce and distributed while the tasks are distinct. In this paper, we propose a federated few-shot learning (FedFSL) framework to learn a few-...|我们有兴趣开发一个统一的机器学习框架来有效地训练机器学习模型从许多小数据源，如移动设备。这是移动计算场景中经常遇到的情况，其中数据稀缺且分布式，而任务是不同的。本文提出了一种联邦少镜头学习(FedFSL)框架来学习少镜头学习算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Federated+Few-shot+Learning)|0|
|[Contrastive Meta-Learning for Few-shot Node Classification](https://doi.org/10.1145/3580305.3599288)|Song Wang, Zhen Tan, Huan Liu, Jundong Li|University of Virginia; Arizona State University|Few-shot node classification, which aims to predict labels for nodes on graphs with only limited labeled nodes as references, is of great significance in real-world graph mining tasks. Particularly, in this paper, we refer to the task of classifying nodes in classes with a few labeled nodes as the few-shot node classification problem. To tackle such a label shortage issue, existing works generally leverage the meta-learning framework, which utilizes a number of episodes to extract transferable knowledge from classes with abundant labeled nodes and generalizes the knowledge to other classes with limited labeled nodes. In essence, the primary aim of few-shot node classification is to learn node embeddings that are generalizable across different classes. To accomplish this, the GNN encoder must be able to distinguish node embeddings between different classes, while also aligning embeddings for nodes in the same class. Thus, in this work, we propose to consider both the intra-class and inter-class generalizability of the model. We create a novel contrastive meta-learning framework on graphs, named COSMIC, with two key designs. First, we propose to enhance the intra-class generalizability by involving a contrastive two-step optimization in each episode to explicitly align node embeddings in the same classes. Second, we strengthen the inter-class generalizability by generating hard node classes via a novel similarity-sensitive mix-up strategy. Extensive experiments on few-shot node classification datasets verify the superiority of our framework over state-of-the-art baselines. Our code is provided at https://github.com/SongW-SW/COSMIC.|少镜头节点分类是一种仅以有限标记节点为参考的图上节点标记预测方法，在现实图挖掘任务中具有重要意义。特别地，本文将在有少量标记节点的类中对节点进行分类的任务称为少镜头节点分类问题。为了解决这样的标签短缺问题，现有的工作通常利用元学习框架，该框架利用一个系列长度从有大量标签节点的类中提取可转移的知识，并将知识推广到有限标签节点的其他类中。本质上，少镜头节点分类的主要目的是学习可以在不同类之间推广的节点嵌入。为了实现这一点，GNN 编码器必须能够区分不同类之间的节点嵌入，同时对同一类中的节点的嵌入进行对齐。因此，在这项工作中，我们建议同时考虑类内和类间的泛化模型。我们创建了一个新的对比元学习框架，命名为 COSMIC，有两个关键的设计。首先，我们提出通过在每一集中包含一个对比的两步优化来增强类内的泛化能力，以显式地对同一类中的节点嵌入进行对齐。其次，通过一种新的相似敏感混合策略生成硬节点类，增强了类间的泛化能力。在少镜头节点分类数据集上的大量实验验证了该框架相对于最先进基线的优越性。我们的代码是 https://github.com/songw-sw/cosmic 提供的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contrastive+Meta-Learning+for+Few-shot+Node+Classification)|0|
|[An Observed Value Consistent Diffusion Model for Imputing Missing Values in Multivariate Time Series](https://doi.org/10.1145/3580305.3599257)|Xu Wang, Hongbo Zhang, Pengkun Wang, Yudong Zhang, Binwu Wang, Zhengyang Zhou, Yang Wang|University of Science and Technology of China|Missing values, which are common in multivariate time series, is most important obstacle towards the utilization and interpretation of those data. Great efforts have been employed on how to accurately impute missing values in multivariate time series, and existing works either use deep learning networks to achieve deterministic imputations or aim at generating different plausible imputations by sampling multiple noises from a same distribution and then denoising them. However, these models either fall short of modeling the uncertainties of imputations due to their deterministic nature or perform poorly in terms of interpretability and imputation accuracy due to their ignorance of the correlations between the latent representations of both observed and missing values which are parts of samples from a same distribution. To this end, in this paper, we explicitly take the correlations between observed and missing values into account, and theoretically re-derive the Evidence Lower BOund (ELBO) of conditional diffusion model in the scenario of multivariate time series imputation. Based on the newly derived ELBO, we further propose a novel multivariate imputation diffusion model (MIDM) which is equipped with novel noise sampling, adding and denoising mechanisms for multivariate time series imputation, and the series of newly designed technologies jointly ensure the involving of the consistency between observed and missing values. Extensive experiments on both the tasks of multivariate time series imputation and forecasting witness the superiority of our proposed MIDM model on generating conditional estimations.|在多变量时间序列中常见的缺失值是利用和解释这些数据的最大障碍。如何准确地估计多变量时间序列中的缺失值，已有的工作要么使用深度学习网络来实现确定性估计，要么通过从同一分布中采样多个噪声，然后对它们进行去噪来产生不同的似是而非的估计。然而，由于这些模型的确定性，它们要么无法对插补的不确定性进行建模，要么由于它们忽视了同一分布样本的部分观测值和缺失值的潜在表示之间的相关性，从而在可解释性和插补精度方面表现不佳。为此，本文明确地考虑了观测值与缺失值之间的相关性，并从理论上重新推导了多元时间序列插值情形下条件扩散模型的证据下界(ELBO)。在此基础上，进一步提出了一种新的多变量插补扩散模型(MIDM) ，该模型具有新的噪声采样、多变量时间序列插补的添加和去噪机制，以及一系列新设计的技术共同保证了观测值与缺失值之间的一致性。对多变量时间序列插值和预测任务的大量实验证明了我们提出的 MIDM 模型在生成条件估计方面的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Observed+Value+Consistent+Diffusion+Model+for+Imputing+Missing+Values+in+Multivariate+Time+Series)|0|
|[Automated 3D Pre-Training for Molecular Property Prediction](https://doi.org/10.1145/3580305.3599252)|Xu Wang, Huan Zhao, WeiWei Tu, Quanming Yao|Tsinghua University; 4Paradigm Inc.|Molecular property prediction is an important problem in drug discovery and materials science. As geometric structures have been demonstrated necessary for molecular property prediction, 3D information has been combined with various graph learning methods to boost prediction performance. However, obtaining the geometric structure of molecules is not feasible in many real-world applications due to the high computational cost. In this work, we propose a novel 3D pre-training framework (dubbed 3D PGT), which pre-trains a model on 3D molecular graphs, and then fine-tunes it on molecular graphs without 3D structures. Based on fact that bond length, bond angle, and dihedral angle are three basic geometric descriptors corresponding to a complete molecular 3D conformer, we first develop a multi-task generative pre-train framework based on these three attributes. Next, to automatically fuse these three generative tasks, we design a surrogate metric using the \textit{total energy} to search for weight distribution of the three pretext task since total energy corresponding to the quality of 3D conformer.Extensive experiments on 2D molecular graphs are conducted to demonstrate the accuracy, efficiency and generalization ability of the proposed 3D PGT compared to various pre-training baselines.|分子性质预测是药物发现和材料科学中的一个重要问题。由于几何结构已被证明是分子性质预测的必要条件，因此将三维信息与各种图形学习方法相结合以提高预测性能。然而，由于分子几何结构的计算成本较高，在许多实际应用中难以获得分子的几何结构。在这项工作中，我们提出了一个新的3D 预训练框架(称为3D PGT) ，它在3D 分子图上预训练模型，然后在没有3D 结构的分子图上进行微调。基于键长、键角和二面角是对应于完整分子三维构象的三个基本几何描述符这一事实，我们首先基于这三个属性开发了一个多任务生成预训练框架。接下来，为了自动融合这三个生成任务，我们设计了一个替代度量，使用文本{总能量}搜索三个托辞任务的权重分布，因为总能量对应于三维构象的质量。通过对二维分子图的大量实验，验证了所提出的三维 PGT 与各种预训练基线相比的准确性、效率和泛化能力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automated+3D+Pre-Training+for+Molecular+Property+Prediction)|0|
|[Rapid Image Labeling via Neuro-Symbolic Learning](https://doi.org/10.1145/3580305.3599485)|Yifeng Wang, Zhi Tu, Yiwen Xiang, Shiyuan Zhou, Xiyuan Chen, Bingxuan Li, Tianyi Zhang|University of Toronto; Purdue University; Chongqing University; The Hong Kong Polytechnic University|The success of Computer Vision (CV) relies heavily on manually annotated data. However, it is prohibitively expensive to annotate images in key domains such as healthcare, where data labeling requires significant domain expertise and cannot be easily delegated to crowd workers. To address this challenge, we propose a neuro-symbolic approach called Rapid, which infers image labeling rules from a small amount of labeled data provided by domain experts and automatically labels unannotated data using the rules. Specifically, Rapid combines pre-trained CV models and inductive logic learning to infer the logic-based labeling rules. Rapid achieves a labeling accuracy of 83.33% to 88.33% on four image labeling tasks with only 12 to 39 labeled samples. In particular, Rapid significantly outperforms finetuned CV models in two highly specialized tasks. These results demonstrate the effectiveness of Rapid in learning from small data and its capability to generalize among different tasks. Code and our dataset are publicly available at https://github.com/Neural-Symbolic-Image-Labeling/|计算机视觉(CV)的成功在很大程度上依赖于手工注释的数据。然而，在医疗保健等关键领域对图像进行注释的成本高得令人望而却步，因为在这些领域，数据标签需要大量的领域专业知识，而且不能轻易地委托给群体工作者。为了应对这一挑战，我们提出了一种称为 Rapid 的神经符号方法，它从领域专家提供的少量标记数据中推断出图像标记规则，并使用这些规则自动标记未加注释的数据。具体来说，Rapid 将预先训练好的 CV 模型和归纳逻辑学习相结合，推导出基于逻辑的标记规则。在仅有12 ~ 39个标记样本的情况下，在4个图像标记任务中，该算法的标记准确率分别为83.33% ~ 88.33% 。特别是，在两个高度专业化的任务中，Rapid 的表现明显优于微调 CV 模型。这些结果表明，在小数据学习快速的有效性和它的能力之间的推广不同的任务。代码和我们的数据集在 https://github.com/neural-symbolic-image-labeling/是公开的|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rapid+Image+Labeling+via+Neuro-Symbolic+Learning)|0|
|[Learning to Schedule in Diffusion Probabilistic Models](https://doi.org/10.1145/3580305.3599412)|Yunke Wang, Xiyu Wang, AnhDung Dinh, Bo Du, Charles Xu||Recently, the field of generative models has seen a significant advancement with the introduction of Diffusion Probabilistic Models (DPMs). The Denoising Diffusion Implicit Model (DDIM) was designed to reduce computational time by skipping a number of steps in the inference process of DPMs. However, the hand-crafted sampling schedule in DDIM, which relies on human expertise, has its limitations in considering all relevant factors in the sampling process. Additionally, the assumption that all instances should have the same schedule is not always valid. To address these problems, this paper proposes a method that leverages reinforcement learning to automatically search for an optimal sampling schedule for DPMs. This is achieved by a policy network that predicts the next step to visit based on the current state of the noisy image. The optimization of the policy network is accomplished using an episodic actor-critic framework, which incorporates reinforcement learning. Empirical results demonstrate the superiority of our approach over various datasets with different timesteps. We also observe that the trained sampling schedule has a strong generalization ability across different DPM baselines.|近年来，随着扩散概率模型(DPM)的引入，生成模型领域取得了长足的进展。设计了去噪扩散隐式模型(DDIM) ，通过跳过 DPM 推理过程中的一些步骤来减少计算时间。然而，依赖于人类专业知识的 DDIM 手工制作的抽样时间表在考虑抽样过程中的所有相关因素方面存在局限性。此外，假设所有实例应该具有相同的调度并不总是有效的。为了解决这些问题，本文提出了一种方法，利用强化学习自动搜索最佳的采样时间表。这是通过一个策略网络实现的，该策略网络根据噪声图像的当前状态预测下一步访问。政策网络的优化是通过一个情节性的行为者-评论家框架来完成的，这个框架包含了强化学习。实验结果表明，我们的方法优于不同时间步长的各种数据集。我们还观察到训练后的抽样调度在不同的 DPM 基线上具有很强的泛化能力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Schedule+in+Diffusion+Probabilistic+Models)|0|
|[A Message Passing Neural Network Space for Better Capturing Data-dependent Receptive Fields](https://doi.org/10.1145/3580305.3599243)|Zhili Wang, Shimin Di, Lei Chen|HKUST (GZ); HKUST|Recently, the message passing neural network (MPNN) has attracted a lot of attention, which learns node representations based on the receptive field of the given node. Despite its success in many graph-related tasks, recent studies find that conventional MPNNs are incapable of handling variant receptive fields required in different graphs, and thereby some upgraded MPNNs have been developed. However, these methods are limited to designing a common solution for different graphs, which fails to capture the impact of different graph properties on the receptive fields. To alleviate such issues, we propose a novel MPNN space for data-dependent receptive fields (MpnnDRF), which enables us to dynamically design suitable MPNNs to capture the receptive field for the given graph. More concretely, we systemically investigate the capability of existing designs and propose several key design dimensions to improve them. Then, to fully explore the proposed designs and useful designs in existing works, we propose a novel search space to incorporate them and formulate a search framework. In the empirical study, the proposed MpnnDRF shows very strong robustness against the increased receptive field, which allows MpnnDRF to learn node representations based on a larger perceptual field. Therefore, MpnnDRF consistently achieves outstanding performance on benchmark node and graph classification tasks.|近年来，消息传递神经网络(MPNN)受到了广泛的关注，它根据给定节点的接收域来学习节点的表示。尽管在许多与图形相关的任务中取得了成功，但最近的研究发现，传统的 MPNN 不能处理不同图形所需要的不同接受域，因此一些升级的 MPNN 已经被开发出来。然而，这些方法仅限于为不同的图设计一个通用的解决方案，无法捕捉到不同的图属性对接收字段的影响。为了解决这些问题，我们提出了一种新的数据相关接收域的 MPNN 空间(MpnnDRF) ，它使我们能够动态设计合适的 MPNN 来捕获给定图的接收域。更具体地说，我们系统地调查了现有设计的能力，并提出了几个关键的设计维度来改进它们。然后，为了充分发掘已有作品中提出的设计方案和有用的设计方案，我们提出了一种新的搜索空间，将它们结合起来，形成一个搜索框架。在实验研究中，提出的 MpnnDRF 对增加的接收场具有很强的鲁棒性，这使得 MpnnDRF 能够基于更大的感知场来学习节点表示。因此，MpnnDRF 始终如一地在基准节点和图分类任务上取得出色的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Message+Passing+Neural+Network+Space+for+Better+Capturing+Data-dependent+Receptive+Fields)|0|
|[To Aggregate or Not? Learning with Separate Noisy Labels](https://doi.org/10.1145/3580305.3599522)|Jiaheng Wei, Zhaowei Zhu, Tianyi Luo, Ehsan Amid, Abhishek Kumar, Yang Liu|University of California, Santa Cruz; Google Research, Brain Team; Amazon Search Science and AI|The rawly collected training data often comes with separate noisy labels collected from multiple imperfect annotators (e.g., via crowdsourcing). A typical way of using these separate labels is to first aggregate them into one and apply standard training methods. The literature has also studied extensively on effective aggregation approaches. This paper revisits this choice and aims to provide an answer to the question of whether one should aggregate separate noisy labels into single ones or use them separately as given. We theoretically analyze the performance of both approaches under the empirical risk minimization framework for a number of popular loss functions, including the ones designed specifically for the problem of learning with noisy labels. Our theorems conclude that label separation is preferred over label aggregation when the noise rates are high, or the number of labelers/annotations is insufficient. Extensive empirical results validate our conclusions.|原始收集的训练数据通常伴随着从多个不完美的注释者(例如，通过众包)收集的单独的噪音标签。使用这些单独标签的一个典型方法是首先将它们聚合成一个标签，然后应用标准的培训方法。文献还广泛研究了有效的聚合方法。本文重新审视这一选择，旨在回答是否应该将分离的噪声标签合并为单个标签，还是按照给定的方式分别使用这些标签的问题。我们从理论上分析了这两种方法在经验风险最小化框架下对一些流行的损失函数的性能，包括那些专门为带噪声标签的学习问题而设计的函数。我们的定理得出结论，当噪声率较高或标签/注释数量不足时，标签分离优于标签聚合。大量的实证结果验证了我们的结论。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=To+Aggregate+or+Not?+Learning+with+Separate+Noisy+Labels)|0|
|[Granger Causal Chain Discovery for Sepsis-Associated Derangements via Continuous-Time Hawkes Processes](https://doi.org/10.1145/3580305.3599369)|Song Wei, Yao Xie, Christopher S. Josef, Rishikesan Kamaleswaran||Modern health care systems are conducting continuous, automated surveillance of the electronic medical record (EMR) to identify adverse events with increasing frequency; however, many events such as sepsis do not have elucidated prodromes (i.e., event chains) that can be used to identify and intercept the adverse event early in its course. Clinically relevant and interpretable results require a framework that can (i) infer temporal interactions across multiple patient features found in EMR data (e.g., Labs, vital signs, etc.) and (ii) identify patterns that precede and are specific to an impending adverse event (e.g., sepsis). In this work, we propose a linear multivariate Hawkes process model, coupled with ReLU link function, to recover a Granger Causal (GC) graph with both exciting and inhibiting effects. We develop a scalable two-phase gradient-based method to obtain a maximum surrogate-likelihood estimator, which is shown to be effective via extensive numerical simulation. Our method is subsequently extended to a data set of patients admitted to Grady hospital system in Atlanta, GA, USA, where the estimated GC graph identifies several highly interpretable GC chains that precede sepsis. The code is available at https://github.com/SongWei-GT/two-phase-MHP.|现代医疗系统正在对电子病历(EMR)进行连续的自动监测，以识别频率越来越高的不良事件; 然而，许多事件如败血症没有明确的前兆(即事件链) ，可用于在其过程的早期识别和拦截不良事件。临床相关和可解释的结果需要一个框架，该框架可以(i)推断在 EMR 数据(例如，实验室，生命体征等)中发现的多个患者特征之间的时间相互作用，以及(ii)识别在即将发生的不良事件(例如败血症)之前和特定的模式。在这项工作中，我们提出了一个线性多元 Hawkes 过程模型，结合 ReLU 链接函数，恢复了格兰杰因果(GC)图的兴奋和抑制效应。我们发展了一个可伸缩的两相梯度方法来获得最大替代似然估计，通过大量的数值模拟证明了该方法的有效性。我们的方法随后扩展到美国佐治亚州亚特兰大市格雷迪医院系统的患者数据集，其中估计的 GC 图鉴定了脓毒症之前的几个高度可解释的 GC 链。密码可在 https://github.com/songwei-gt/two-phase-mhp 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Granger+Causal+Chain+Discovery+for+Sepsis-Associated+Derangements+via+Continuous-Time+Hawkes+Processes)|0|
|[Deep Bayesian Active Learning for Accelerating Stochastic Simulation](https://doi.org/10.1145/3580305.3599300)|Dongxia Wu, Ruijia Niu, Matteo Chinazzi, Alessandro Vespignani, YiAn Ma, Rose Yu|Northeastern University; University of California, San Diego|Stochastic simulations such as large-scale, spatiotemporal, age-structured epidemic models are computationally expensive at fine-grained resolution. While deep surrogate models can speed up the simulations, doing so for stochastic simulations and with active learning approaches is an underexplored area. We propose Interactive Neural Process (INP), a deep Bayesian active learning framework for learning deep surrogate models to accelerate stochastic simulations. INP consists of two components, a spatiotemporal surrogate model built upon Neural Process (NP) family and an acquisition function for active learning. For surrogate modeling, we develop Spatiotemporal Neural Process (STNP) to mimic the simulator dynamics. For active learning, we propose a novel acquisition function, Latent Information Gain (LIG), calculated in the latent space of NP based models. We perform a theoretical analysis and demonstrate that LIG reduces sample complexity compared with random sampling in high dimensions. We also conduct empirical studies on two complex spatiotemporal simulators for reaction diffusion and infectious disease. The results demonstrate that STNP outperforms the baselines in the offline learning setting and LIG achieves the state-of-the-art for Bayesian active learning.|随机模拟，例如大规模的、时空的、年龄结构的流行病模型，在细粒度分辨率下计算量很大。虽然深度代理模型可以加速模拟，但是对于随机模拟和主动学习方法来说，这是一个尚未开发的领域。我们提出了交互式神经过程(INP) ，一个深贝叶斯主动学习框架学习深代理模型，以加速随机模拟。INP 由两部分组成，一个建立在神经过程(NP)家族基础上的时空替代模型和一个主动学习的获取函数。对于代理建模，我们开发了时空神经过程(STNP)来模拟模拟器的动力学。对于主动学习，我们提出了一种新的获取函数，潜在信息增益(LIG) ，计算在潜在空间的 NP 基模型。我们进行了理论分析，证明了与高维随机采样相比，LIG 降低了采样复杂度。我们还对反应扩散和传染病的两个复杂的时空模拟器进行了实证研究。结果表明，STNP 算法在离线学习环境中优于基线算法，LIG 算法在贝叶斯主动学习中达到了最高水平。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Bayesian+Active+Learning+for+Accelerating+Stochastic+Simulation)|0|
|[Self-Adaptive Perturbation Radii for Adversarial Training](https://doi.org/10.1145/3580305.3599495)|Huimin Wu, Wanli Shi, Chenkang Zhang, Bin Gu|NUIST; Mohamed bin Zayed University of Artificial Intelligence; Nanjing University of Information Science & Technology|Adversarial training has been shown to be the most popular and effective technique to protect models from imperceptible adversarial samples. Despite its success, it also accompanies the significant performance degeneration to clean data. To achieve a good performance on both clean and adversarial samples, the main effort is searching for an adaptive perturbation radius for each training sample, which essentially suffers from a conflict between exact searching and computational overhead. To address this conflict, in this paper, firstly we show the superiority of adaptive perturbation radii intuitively and theoretically regarding the accuracy and robustness respectively. Then we propose our novel self-adaptive adjustment framework for perturbation radii without tedious searching. We also discuss this framework on both deep neural networks (DNNs) and kernel support vector machines (SVMs). Finally, extensive experimental results show that our framework can improve not only natural generalization performance but also adversarial robustness. It is also competitive with existing searching strategies in terms of running time.|对抗性训练已被证明是最流行和有效的技术，以保护模型从不易察觉的对抗性样本。尽管它取得了成功，但在清理数据方面也伴随着显著的性能退化。为了在干净样本和对手样本上获得良好的性能，主要的工作是为每个训练样本搜索一个自适应扰动半径，这实质上遭受了精确搜索和计算开销之间的冲突。针对这一矛盾，本文首先从理论上和直观上分别论证了自适应摄动半径在精度和鲁棒性方面的优越性。然后，我们提出了新的自适应调整框架扰动半径没有繁琐的搜索。我们还讨论了深度神经网络(DNN)和核支持向量机(SVM)的框架。最后，大量的实验结果表明，该框架不仅提高了自然泛化性能，而且增强了对抗鲁棒性。在运行时间方面，它也与现有的搜索策略具有竞争力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Adaptive+Perturbation+Radii+for+Adversarial+Training)|0|
|[DECOR: Degree-Corrected Social Graph Refinement for Fake News Detection](https://doi.org/10.1145/3580305.3599298)|Jiaying Wu, Bryan Hooi|National University of Singapore|Recent efforts in fake news detection have witnessed a surge of interest in using graph neural networks (GNNs) to exploit rich social context. Existing studies generally leverage fixed graph structures, assuming that the graphs accurately represent the related social engagements. However, edge noise remains a critical challenge in real-world graphs, as training on suboptimal structures can severely limit the expressiveness of GNNs. Despite initial efforts in graph structure learning (GSL), prior works often leverage node features to update edge weights, resulting in heavy computational costs that hinder the methods' applicability to large-scale social graphs. In this work, we approach the fake news detection problem with a novel aspect of social graph refinement. We find that the degrees of news article nodes exhibit distinctive patterns, which are indicative of news veracity. Guided by this, we propose DECOR, a novel application of Degree-Corrected Stochastic Blockmodels to the fake news detection problem. Specifically, we encapsulate our empirical observations into a lightweight social graph refinement component that iteratively updates the edge weights via a learnable degree correction mask, which allows for joint optimization with a GNN-based detector. Extensive experiments on two real-world benchmarks validate the effectiveness and efficiency of DECOR.|最近在假新闻检测方面的努力见证了利用图形神经网络(GNN)来开发丰富的社会背景的兴趣激增。现有的研究通常利用固定的图结构，假设图准确地表示相关的社会活动。然而，边缘噪声仍然是一个关键的挑战，在现实世界的图，因为训练次优结构可以严重限制 GNN 的表达。尽管在图结构学习(GSL)方面做出了初步的努力，但是以前的工作经常利用节点特征来更新边权重，从而导致沉重的计算代价，阻碍了该方法对大规模社会图的适用性。在这项工作中，我们探讨了假新闻检测问题与一个新的方面的社会图精化。我们发现新闻文章节点的程度表现出独特的模式，这表明了新闻的真实性。在此基础上，我们提出了一种基于度修正随机块模型的伪新闻检测方法 DECOR。具体来说，我们将我们的经验观察封装到一个轻量级的社会图精化组件中，该组件通过一个可学习的度修正掩码迭代更新边缘权重，该掩码允许与基于 GNN 的检测器进行联合优化。在两个实际基准上的大量实验验证了 DECOR 的有效性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DECOR:+Degree-Corrected+Social+Graph+Refinement+for+Fake+News+Detection)|0|
|[Certified Edge Unlearning for Graph Neural Networks](https://doi.org/10.1145/3580305.3599271)|Kun Wu, Jie Shen, Yue Ning, Ting Wang, Wendy Hui Wang||The emergence of evolving data privacy policies and regulations has sparked a growing interest in the concept of "machine unlearning", which involves enabling machine learning models to forget specific data instances. In this paper, we specifically focus on edge unlearning in Graph Neural Networks (GNNs), which entails training a new GNN model as if certain specified edges never existed in the original training graph. Unlike conventional unlearning scenarios where data samples are treated as independent entities, edges in graphs exhibit correlation. Failing to carefully account for this data dependency would result in the incomplete removal of the requested data from the model. While retraining the model from scratch by excluding the specific edges can eliminate their influence, this approach incurs a high computational cost. To overcome this challenge, we introduce CEU, a Certified Edge Unlearning framework. CEU expedites the unlearning process by updating the parameters of the pre-trained GNN model in a single step, ensuring that the update removes the influence of the removed edges from the model. We formally prove that CEU offers a rigorous theoretical guarantee under the assumption of convexity on the loss function. Our empirical analysis further demonstrates the effectiveness and efficiency of CEU for both linear and deep GNNs - it achieves significant speedup gains compared to retraining and existing unlearning methods while maintaining comparable model accuracy to retraining from scratch.|不断发展的数据隐私政策和法规的出现，引发了人们对“机器忘却”概念的兴趣。“机器忘却”指的是让机器学习模型忘记特定的数据实例。本文重点研究了图神经网络中的边去除问题，这就需要对一个新的神经网络模型进行训练，就好像原来的训练图中不存在特定的边一样。与传统的取消学习场景中的数据样本被视为独立的实体，图的边表现出相关性。如果不仔细考虑这种数据依赖关系，就会导致从模型中不完全删除请求的数据。尽管通过排除特定边缘来重新训练模型可以消除它们的影响，但是这种方法需要很高的计算成本。为了克服这个挑战，我们引入了 CEU，一个认证边缘学习框架。CEU 通过在单一步骤中更新预先训练的 GNN 模型的参数，确保更新消除了模型中去除边缘的影响，从而加快了忘却过程。我们正式证明了 CEU 在损失函数的凸性假设下提供了严格的理论保证。我们的实证分析进一步证明了 CEU 对于线性和深层 GNN 的有效性和效率——它在保持模型精度的同时，比再训练和现有的去学习方法获得了显著的加速效果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Certified+Edge+Unlearning+for+Graph+Neural+Networks)|0|
|[Recognizing Unseen Objects via Multimodal Intensive Knowledge Graph Propagation](https://doi.org/10.1145/3580305.3599486)|Likang Wu, Zhi Li, Hongke Zhao, Zhefeng Wang, Qi Liu, Baoxing Huai, Nicholas Jing Yuan, Enhong Chen|Huawei Cloud; Tianjin University; Shenzhen International Graduate School, Tsinghua University; University of Science and Technology of China, State Key Laboratory of Cognitive Intelligence|Zero-Shot Learning (ZSL), which aims at automatically recognizing unseen objects, is a promising learning paradigm to understand new real-world knowledge for machines continuously. Recently, the Knowledge Graph (KG) has been proven as an effective scheme for handling the zero-shot task with large-scale and non-attribute data. Prior studies always embed relationships of seen and unseen objects into visual information from existing knowledge graphs to promote the cognitive ability of the unseen data. Actually, real-world knowledge is naturally formed by multimodal facts. Compared with ordinary structural knowledge from a graph perspective, multimodal KG can provide cognitive systems with fine-grained knowledge. For example, the text description and visual content can depict more critical details of a fact than only depending on knowledge triplets. Unfortunately, this multimodal fine-grained knowledge is largely unexploited due to the bottleneck of feature alignment between different modalities. To that end, we propose a multimodal intensive ZSL framework that matches regions of images with corresponding semantic embeddings via a designed dense attention module and self-calibration loss. It makes the semantic transfer process of our ZSL framework learns more differentiated knowledge between entities. Our model also gets rid of the performance limitation of only using rough global features. We conduct extensive experiments and evaluate our model on large-scale real-world data. The experimental results clearly demonstrate the effectiveness of the proposed model in standard zero-shot classification tasks.|零镜头学习(Zero-Shot Learning，ZSL)以自动识别看不见的物体为目标，是机器不断理解新的现实世界知识的一种有前途的学习范式。近年来，知识图(KG)已被证明是处理大规模非属性数据的零射击任务的一种有效方案。以往的研究往往从现有的知识图表中将可见与不可见的对象关系嵌入到可视信息中，以提高不可见数据的认知能力。实际上，现实世界的知识是由多模态事实自然形成的。与普通结构知识相比，多模态 KG 能够为认知系统提供细粒度的知识。例如，文本描述和可视化内容可以描述事实的更多关键细节，而不仅仅依赖于知识三元组。不幸的是，由于不同模式之间特征对齐的瓶颈，这种多模式细粒度知识在很大程度上没有得到开发。为此，我们提出了一个多模态密集型 ZSL 框架，通过设计密集注意模块和自标定损失来匹配图像区域和相应的语义嵌入。它使得我们的 ZSL 框架的语义转换过程学习实体之间更多的区分知识。我们的模型还摆脱了仅使用粗糙的全局特征的性能限制。我们进行了广泛的实验和评估我们的模型对大规模的现实世界的数据。实验结果清楚地表明了该模型在标准零拍分类任务中的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Recognizing+Unseen+Objects+via+Multimodal+Intensive+Knowledge+Graph+Propagation)|0|
|[Towards Reliable Rare Category Analysis on Graphs via Individual Calibration](https://doi.org/10.1145/3580305.3599525)|Longfeng Wu, Bowen Lei, Dongkuan Xu, Dawei Zhou||Rare categories abound in a number of real-world networks and play a pivotal role in a variety of high-stakes applications, including financial fraud detection, network intrusion detection, and rare disease diagnosis. Rare category analysis (RCA) refers to the task of detecting, characterizing, and comprehending the behaviors of minority classes in a highly-imbalanced data distribution. While the vast majority of existing work on RCA has focused on improving the prediction performance, a few fundamental research questions heretofore have received little attention and are less explored: How confident or uncertain is a prediction model in rare category analysis? How can we quantify the uncertainty in the learning process and enable reliable rare category analysis? To answer these questions, we start by investigating miscalibration in existing RCA methods. Empirical results reveal that state-of-the-art RCA methods are mainly over-confident in predicting minority classes and under-confident in predicting majority classes. Motivated by the observation, we propose a novel individual calibration framework, named CALIRARE, for alleviating the unique challenges of RCA, thus enabling reliable rare category analysis. In particular, to quantify the uncertainties in RCA, we develop a node-level uncertainty quantification algorithm to model the overlapping support regions with high uncertainty; to handle the rarity of minority classes in miscalibration calculation, we generalize the distribution-based calibration metric to the instance level and propose the first individual calibration measurement on graphs named Expected Individual Calibration Error (EICE). We perform extensive experimental evaluations on real-world datasets, including rare category characterization and model calibration tasks, which demonstrate the significance of our proposed framework.|稀有类别在现实世界的网络中比比皆是，并且在各种高风险的应用程序中发挥着关键作用，包括金融欺诈检测、网络入侵检测和罕见疾病诊断。稀有类别分析(RCA)是指在一个高度不平衡的数据分布中发现、表征和理解少数类别的行为的任务。尽管现有的大部分 RCA 工作都集中在提高预测性能上，但是一些基础研究问题迄今为止很少受到关注，也很少被探索: 在稀有类别分析中，预测模型的可信度或不确定性如何？我们如何量化学习过程中的不确定性，并使可靠的稀有类别分析成为可能？为了回答这些问题，我们从调查现有 RCA 方法中的误差开始。实证结果表明，目前最先进的 RCA 方法在预测少数民族阶层时主要表现为过度自信，而在预测多数民族阶层时则表现为不自信。受到观察的启发，我们提出了一个新的个体校准框架，称为 CALIRARE，以减轻 RCA 的独特挑战，从而使可靠的罕见类别分析。为了量化 RCA 的不确定性，我们发展了一个节点级的不确定度量化算法，以模拟具有高不确定性的重叠支撑区域，为了处理少数类别在错误校正计算中的罕见情况，我们将基于分布的校正度量推广至实例级别，并提出首个个别校正度量的图形命名为预期个别校正误差(EICE)。我们对真实世界的数据集进行广泛的实验性评估，包括罕见的分类角色塑造和模型校准任务，这些都证明了我们提出的框架的重要性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Reliable+Rare+Category+Analysis+on+Graphs+via+Individual+Calibration)|0|
|[TransformerLight: A Novel Sequence Modeling Based Traffic Signaling Mechanism via Gated Transformer](https://doi.org/10.1145/3580305.3599530)|Qiang Wu, Mingyuan Li, Jun Shen, Linyuan Lü, Bo Du, Ke Zhang||Traffic signal control (TSC) is still one of the most significant and challenging research problems in the transportation field. Reinforcement learning (RL) has achieved great success in TSC but suffers from critically high learning costs in practical applications due to the excessive trial-and-error learning process. Offline RL is a promising method to reduce learning costs whereas the data distribution shift issue is still up in the air. To this end, in this paper, we formulate TSC as a sequence modeling problem with a sequence of Markov decision process described by states, actions, and rewards from the traffic environment. A novel framework, namely TransformerLight, is introduced, which does not aim to fit into value functions by averaging all possible returns, but produces the best possible actions using a gated Transformer. Additionally, the learning process of TransformerLight is much more stable by replacing the residual connections with gated transformer blocks due to a dynamic system perspective. Through numerical experiments on offline datasets, we demonstrate that the TransformerLight model: (1) can build a high-performance adaptive TSC model without dynamic programming; (2) achieves a new state-of-the-art compared to most published offline RL methods so far; and (3) shows a more stable learning process than offline RL and recent Transformer-based methods. The relevant dataset and code are available at Github.|交通信号控制(TSC)仍然是交通领域中最重要、最具挑战性的研究课题之一。强化学习(rL)在 TSC 方面取得了巨大的成功，但由于过多的试错学习过程，实际应用中的学习成本极高。离线 RL 是一种很有前途的降低学习成本的方法，然而数据分布移位问题仍然悬而未决。为此，在本文中，我们将 TSC 作为一个序列建模问题，用一系列马可夫决策过程来描述状态、行为和来自交通环境的回报。引入了一个新的框架，即 TransformerLight，它的目标不是通过平均所有可能的回报来适应值函数，而是使用门限变压器产生最佳可能的操作。另外，从动态系统的角度出发，通过用门控变压器组替换剩余连接，TransformerLight 的学习过程更加稳定。通过对离线数据集的数值实验，我们证明了 TransformerLight 模型: (1)不需要动态编程就可以构建高性能的自适应 TSC 模型; (2)与目前已发表的大多数离线 RL 方法相比，实现了一种新的技术状态; (3)显示了一个比离线 RL 和最近的基于 TransformerLight 的方法更稳定的学习过程。相关的数据集和代码可以在 Github 上找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TransformerLight:+A+Novel+Sequence+Modeling+Based+Traffic+Signaling+Mechanism+via+Gated+Transformer)|0|
|[MedLink: De-Identified Patient Health Record Linkage](https://doi.org/10.1145/3580305.3599427)|Zhenbang Wu, Cao Xiao, Jimeng Sun||A comprehensive patient health history is essential for patient care and healthcare research. However, due to the distributed nature of healthcare services, patient health records are often scattered across multiple systems. Existing record linkage approaches primarily rely on patient identifiers, which have inherent limitations such as privacy invasion and identifier discrepancies. To tackle this problem, we propose linking de-identified patient health records by matching health patterns without strictly relying on sensitive patient identifiers. Our model MedLink solves two challenges faced with the patient linkage task: (1) the challenge of identifying the same patients based on data collected in different timelines as disease progression makes the record matching difficult, and (2) the challenge of identifying distinct health patterns as common medical codes dominate health records and overshadow the more informative low-prevalence codes. To address these challenges, MedLink utilizes bi-directional health prediction to predict future codes forwardly and past codes backwardly, thus accounting for the health progression. MedLink also has a prevalence-aware retrieval design to focus more on the low-prevalence but informative codes during learning. MedLink can be trained end-to-end and is lightweight for efficient inference on large patient databases. We evaluate MedLink against leading baselines on real-world patient datasets, including the critical care dataset MIMIC-III and a large health claims dataset. Results show that MedLink outperforms the best baseline by 4% in top-1 accuracy with only 8% memory cost. Additionally, when combined with existing identifier-based linkage approaches, MedLink can improve their performance by up to 15%.|一个全面的病人健康史是必不可少的病人护理和医疗保健研究。然而，由于医疗服务的分布式特性，患者的健康记录通常分散在多个系统中。现有的记录连接方法主要依赖于患者标识符，这些标识符具有内在的局限性，如隐私侵犯和标识符差异。为了解决这个问题，我们建议通过匹配健康模式而不严格依赖敏感的患者标识符来链接去识别的患者健康记录。我们的模型 MedLink 解决了患者联系任务所面临的两个挑战: (1)根据在不同时间线收集的数据识别相同的患者的挑战使得记录匹配困难，以及(2)将不同的健康模式识别为常见的医疗代码主导健康记录并掩盖更多信息的低流行代码的挑战。为了应对这些挑战，MedLink 利用双向健康预测来向前预测未来的代码，向后预测过去的代码，从而解释健康进展。MedLink 还有一个流行意识检索设计，在学习过程中更多地关注低流行但信息丰富的代码。MedLink 可以进行端到端的训练，并且对于大型患者数据库的高效推理来说是轻量级的。我们根据现实世界患者数据集的主要基线评估 MedLink，包括重症监护数据集 MIMIC-III 和大型健康索赔数据集。结果表明，MedLink 在最高准确率方面比最佳基线高出4% ，而内存成本仅为8% 。此外，当与现有的基于标识符的链接方法结合使用时，MedLink 可以将其性能提高高达15% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MedLink:+De-Identified+Patient+Health+Record+Linkage)|0|
|[A Sequence-to-Sequence Approach with Mixed Pointers to Topic Segmentation and Segment Labeling](https://doi.org/10.1145/3580305.3599245)|Jinxiong Xia, Houfeng Wang||Topic segmentation is the process of dividing a text into semantically coherent segments, and segment labeling involves assigning a topic label to each of these segments. Previous work on this task has included the use of sequence labeling, segment-extraction, and generative models. While these methods have yielded impressive results, existing generative models have struggled to accurately generate strings of segment boundaries, limiting their competitiveness in this area. In this paper, we present a novel Sequence-to-Sequence approach with Mixed Pointers (Seq2Seq-MP). Seq2Seq-MP employs an encoder-decoder architecture with the pointer mechanism to generate both segment boundaries and topics, which allows for a more robust performance than string-generation models and can handle long-range dependencies better than sequence labeling and segment-extraction models. Additionally, we introduce the pairwise type encoding and type-aware relative position encoding to improve the fusion of type and position information, enhancing the interactions between sentences and topics in the encoder and decoder. Our experiments on public datasets show that Seq2Seq-MP outperforms the current state-of-the-art, with up to 2.9% and 4.0% improvements in P k and F 1 , respectively.|主题分割是将文本分割成语义连贯的片段的过程，片段标记涉及到为每个片段分配一个主题标签。此前的工作包括序列标记、片段提取和生成模型的使用。虽然这些方法已经取得了令人印象深刻的成果，但现有的生成模型一直难以准确地生成一系列细分市场的边界，从而限制了它们在这一领域的竞争力。在本文中，我们提出了一种新的混合指针序列到序列的方法(Seq2Seq-MP)。Seq2Seq-MP 采用带有指针机制的编码器-解码器结构来生成段边界和主题，这比字符串生成模型允许更强大的性能，并且比序列标记和段提取模型更好地处理长期依赖。此外，本文还引入了成对类型编码和类型感知的相对位置编码，改善了类型和位置信息的融合，增强了编码器和解码器中句子和话题之间的交互作用。我们在公共数据集上的实验表明，Seq2Seq-MP 优于目前的最新水平，P k 和 F 1分别提高了2.9% 和4.0% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Sequence-to-Sequence+Approach+with+Mixed+Pointers+to+Topic+Segmentation+and+Segment+Labeling)|0|
|[Graph Contrastive Learning with Generative Adversarial Network](https://doi.org/10.1145/3580305.3599370)|Cheng Wu, Chaokun Wang, Jingcao Xu, Ziyang Liu, Kai Zheng, Xiaowei Wang, Yang Song, Kun Gai||Graph Neural Networks (GNNs) have demonstrated promising results on exploiting node representations for many downstream tasks through supervised end-to-end training. To deal with the widespread label scarcity issue in real-world applications, Graph Contrastive Learning (GCL) is leveraged to train GNNs with limited or even no labels by maximizing the mutual information between nodes in its augmented views generated from the original graph. However, the distribution of graphs remains unconsidered in view generation, resulting in the ignorance of unseen edges in most existing literature, which is empirically shown to be able to improve GCL's performance in our experiments. To this end, we propose to incorporate graph generative adversarial networks (GANs) to learn the distribution of views for GCL, in order to i) automatically capture the characteristic of graphs for augmentations, and ii) jointly train the graph GAN model and the GCL model. Specifically, we present GACN, a novel Generative Adversarial Contrastive learning Network for graph representation learning. GACN develops a view generator and a view discriminator to generate augmented views automatically in an adversarial style. Then, GACN leverages these views to train a GNN encoder with two carefully designed self-supervised learning losses, including the graph contrastive loss and the Bayesian personalized ranking Loss. Furthermore, we design an optimization framework to train all GACN modules jointly. Extensive experiments on seven real-world datasets show that GACN is able to generate high-quality augmented views for GCL and is superior to twelve state-of-the-art baseline methods. Noticeably, our proposed GACN surprisingly discovers that the generated views in data augmentation finally conform to the well-known preferential attachment rule in online networks.|图神经网络(GNN)通过监督端到端的训练，在许多下游任务的节点表示方面取得了令人满意的结果。为了解决实际应用中广泛存在的标签稀缺问题，利用图形对比学习(Graph Contrative Learning，GCL) ，通过最大化原始图生成的增强视图中节点之间的相互信息来训练有限甚至没有标签的 GNN。然而，图的分布在视图生成中仍然没有得到考虑，导致大多数现有文献中忽略了看不见的边，这在我们的实验中被经验证明能够改善 GCL 的性能。为此，我们建议结合图形生成对抗网络(GAN)来学习 GCL 的视图分布，以便 i)自动捕获图形的特征进行增强，ii)联合训练图形 GAN 模型和 GCL 模型。特别地，我们提出了 GACN，一种新的图表示学习的生成对抗性对比学习网络。GACN 开发了视图生成器和视图鉴别器，以对抗的方式自动生成增强视图。然后，GACN 利用这些视图训练一个 GNN 编码器，该编码器具有两个精心设计的自监督学习损失，包括图的对比度损失和贝叶斯个性化排序损失。此外，我们还设计了一个优化框架来联合训练所有的 GACN 模块。在7个真实世界数据集上的大量实验表明，GACN 能够为 GCL 生成高质量的增强视图，并且优于十二种最先进的基线方法。值得注意的是，我们提出的 GACN 令人惊讶地发现，在数据增强中生成的视图最终符合在线网络中众所周知的优先依附规则。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Contrastive+Learning+with+Generative+Adversarial+Network)|0|
|[A Causality Inspired Framework for Model Interpretation](https://doi.org/10.1145/3580305.3599240)|Chenwang Wu, Xiting Wang, Defu Lian, Xing Xie, Enhong Chen||This paper introduces a unified causal lens for understanding representative model interpretation methods. We show that their explanation scores align with the concept of average treatment effect in causal inference, which allows us to evaluate their relative strengths and limitations from a unified causal perspective. Based on our observations, we outline the major challenges in applying causal inference to model interpretation, including identifying common causes that can be generalized across instances and ensuring that explanations provide a complete causal explanation of model predictions. We then present CIMI, a Causality-Inspired Model Interpreter, which addresses these challenges. Our experiments show that CIMI provides more faithful and generalizable explanations with improved sampling efficiency, making it particularly suitable for larger pretrained models.|本文介绍了一个统一的因果透镜来理解代表性的模型解释方法。我们表明，他们的解释分数与因果推理中的平均治疗效果的概念一致，这使我们能够从一个统一的因果观点来评估他们的相对优势和局限性。基于我们的观察，我们概述了将因果推理应用于模型解释的主要挑战，包括确定可以在实例间推广的共同原因，并确保解释提供模型预测的完整的因果解释。然后我们介绍 CIMI，一个受因果关系启发的模型解释器，它解决了这些挑战。我们的实验表明，CIMI 提供了更加忠实和一般化的解释与改进的抽样效率，使其特别适合较大的预训练模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Causality+Inspired+Framework+for+Model+Interpretation)|0|
|[Imputation-based Time-Series Anomaly Detection with Conditional Weight-Incremental Diffusion Models](https://doi.org/10.1145/3580305.3599391)|Chunjing Xiao, Zehua Gou, Wenxin Tai, Kunpeng Zhang, Fan Zhou||Existing anomaly detection models for time series are primarily trained with normal-point-dominant data and would become ineffective when anomalous points intensively occur in certain episodes. To solve this problem, we propose a new approach, called DiffAD, from the perspective of time series imputation. Unlike previous prediction- and reconstruction-based methods that adopt either partial or complete data as observed values for estimation, DiffAD uses a density ratio-based strategy to select normal observations flexibly that can easily adapt to the anomaly concentration scenarios. To alleviate the model bias problem in the presence of anomaly concentration, we design a new denoising diffusion-based imputation method to enhance the imputation performance of missing values with conditional weight-incremental diffusion, which can preserve the information of observed values and substantially improves data generation quality for stable anomaly detection. Besides, we customize a multi-scale state space model to capture the long-term dependencies across episodes with different anomaly patterns. Extensive experimental results on real-world datasets show that DiffAD performs better than state-of-the-art benchmarks.|现有的时间序列异常检测模型主要使用正常点占主导地位的数据进行训练，当某些时间段出现大量异常点时，模型将失效。为了解决这个问题，我们从时间序列插补的角度提出了一种新的方法，称为区分 AD。与以往的基于预测和重建的方法不同，它们采用部分或完整的数据作为观测值进行估计，迪夫阿德采用基于密度比的策略灵活地选择正态观测值，可以很容易地适应异常浓度场景。为了减轻异常集中情况下的模型偏差问题，我们设计了一种新的基于扩散的去噪插补方法，通过条件加权增量扩散来提高缺失值的插补性能，这种方法可以保存观测值的信息，并大大提高稳定异常检测的数据生成质量。此外，我们定制了一个多尺度状态空间模型来捕获具有不同异常模式的事件之间的长期依赖关系。在真实世界数据集上的大量实验结果表明，DiffAD 比最先进的基准测试表现得更好。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Imputation-based+Time-Series+Anomaly+Detection+with+Conditional+Weight-Incremental+Diffusion+Models)|0|
|[Spatial Heterophily Aware Graph Neural Networks](https://doi.org/10.1145/3580305.3599510)|Congxi Xiao, Jingbo Zhou, Jizhou Huang, Tong Xu, Hui Xiong|The Hong Kong University of Science and Technology (Guangzhou); Baidu Research; University of Science and Technology of China; Baidu Inc.|Graph Neural Networks (GNNs) have been broadly applied in many urban applications upon formulating a city as an urban graph whose nodes are urban objects like regions or points of interest. Recently, a few enhanced GNN architectures have been developed to tackle heterophily graphs where connected nodes are dissimilar. However, urban graphs usually can be observed to possess a unique spatial heterophily property; that is, the dissimilarity of neighbors at different spatial distances can exhibit great diversity. This property has not been explored, while it often exists. To this end, in this paper, we propose a metric, named Spatial Diversity Score, to quantitatively measure the spatial heterophily and show how it can influence the performance of GNNs. Indeed, our experimental investigation clearly shows that existing heterophilic GNNs are still deficient in handling the urban graph with high spatial diversity score. This, in turn, may degrade their effectiveness in urban applications. Along this line, we propose a Spatial Heterophily Aware Graph Neural Network (SHGNN), to tackle the spatial diversity of heterophily of urban graphs. Based on the key observation that spatially close neighbors on the urban graph present a more similar mode of difference to the central node, we first design a rotation-scaling spatial aggregation module, whose core idea is to properly group the spatially close neighbors and separately process each group with less diversity inside. Then, a heterophily-sensitive spatial interaction module is designed to adaptively capture the commonality and diverse dissimilarity in different spatial groups. Extensive experiments on three real-world urban datasets demonstrate the superiority of our SHGNN over several its competitors.|图形神经网络(GNN)作为一种城市图形，其节点是城市对象，如区域或兴趣点，在许多城市应用中得到了广泛的应用。最近，一些改进的 GNN 体系结构被开发用于处理连接节点不同的异构图。然而，通常可以观察到城市图具有独特的空间异质性，即不同空间距离上邻居的不同可以表现出很大的差异性。虽然这个属性常常存在，但是它还没有被探索过。为此，本文提出了一种空间多样性评价指标——空间多样性评价指标，用于定量评价空间异质性对 GNN 性能的影响。事实上，我们的实验研究清楚地表明，现有的异质性 GNN 在处理高空间多样性得分的城市图中仍然存在缺陷。这反过来可能会降低它们在城市应用中的效率。在此基础上，提出了一种基于空间异构感知的图形神经网络(SHGNN) ，以解决城市图形异构性的空间多样性问题。基于对城市图上空间相近邻居与中心节点的差异模式更为相似的关键观察，首先设计了一个旋转缩放空间聚合模块，其核心思想是对空间相近邻居进行适当的分组，分别处理每个内部差异较小的群体。然后，设计异构敏感的空间交互模块，自适应地捕捉不同空间群体的共性和差异性。在三个真实世界的城市数据集上的大量实验证明了我们的 SHGNN 相对于它的几个竞争对手的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Spatial+Heterophily+Aware+Graph+Neural+Networks)|0|
|[A Dual-Agent Scheduler for Distributed Deep Learning Jobs on Public Cloud via Reinforcement Learning](https://doi.org/10.1145/3580305.3599241)|Mingzhe Xing, Hangyu Mao, Shenglin Yin, Lichen Pan, Zhengchao Zhang, Zhen Xiao, Jieyi Long|Peking University; Theta Labs, Inc.; Sensetime Research; ByteDance|Public cloud GPU clusters are becoming emerging platforms for training distributed deep learning jobs. Under this training paradigm, the job scheduler is a crucial component to improve user experiences, i.e., reducing training fees and job completion time, which can also save power costs for service providers. However, the scheduling problem is known to be NP-hard. Most existing work divides it into two easier sub-tasks, i.e., ordering task and placement task, which are responsible for deciding the scheduling orders of jobs and placement orders of GPU machines, respectively. Due to the superior adaptation ability, learning-based policies can generally perform better than traditional heuristic-based methods. Nevertheless, there are still two main challenges that have not been well-solved. First, most learning-based methods only focus on ordering or placement policy independently, while ignoring their cooperation. Second, the unbalanced machine performances and resource contention impose huge overhead and uncertainty on job duration, but rarely be considered in existing work. To tackle these issues, this paper presents a dual-agent scheduler framework abstracted from the two sub-tasks to jointly learn the ordering and placement policies and make better-informed scheduling decisions. Specifically, we design an ordering agent with a scalable squeeze-and-communicate strategy for better cooperation; for the placement agent, we propose a novel Random Walk Gaussian Process to learn the performance similarities of GPU machines while being aware of the uncertain performance fluctuation. Finally, the dual-agent is jointly optimized with multi-agent reinforcement learning. Extensive experiments conducted on the real-world production cluster trace demonstrate the superiority of our model.|公共云 GPU 集群正在成为培训分布式深度学习工作的新兴平台。在这种培训模式下，作业调度器是提高用户体验的重要组成部分，即减少培训费用和作业完成时间，这也可以为服务提供商节省电力成本。然而，调度问题被认为是 NP 难的。大多数已有的工作将其划分为两个较简单的子任务，即排序任务和放置任务，分别负责确定工件的排序顺序和 GPU 机器的放置顺序。由于优越的自适应能力，基于学习的策略通常能够比传统的基于启发式的方法表现得更好。尽管如此，仍有两个主要挑战没有得到很好的解决。首先，大多数基于学习的方法只关注独立的排序或布局策略，而忽略了它们之间的协作。其次，不平衡的机器性能和资源竞争对作业持续时间造成了巨大的开销和不确定性，但在现有的工作中很少被考虑。为了解决这些问题，本文提出了一个从两个子任务中抽象出来的双智能体调度器框架，以便联合学习排序和布局策略，并做出更好的知情调度决策。具体来说，我们设计了一个具有可扩展的挤压和通信策略的订购代理，为了更好的合作，我们提出了一个新的随机游走高斯过程来学习 GPU 机器的性能相似性，同时意识到不确定的性能波动。最后，双智能体与多智能体强化学习联合优化。在实际生产过程中进行的大量实验证明了该模型的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Dual-Agent+Scheduler+for+Distributed+Deep+Learning+Jobs+on+Public+Cloud+via+Reinforcement+Learning)|0|
|[How does the Memorization of Neural Networks Impact Adversarial Robust Models?](https://doi.org/10.1145/3580305.3599381)|Han Xu, Xiaorui Liu, Wentao Wang, Zitao Liu, Anil K. Jain, Jiliang Tang|Jinan University; North Carilina State University; Michigan State University|Recent studies suggest that "memorization" is one necessary factor for overparameterized deep neural networks (DNNs) to achieve optimal performance. Specifically, the perfectly fitted DNNs can memorize the labels of many atypical samples, generalize their memorization to correctly classify test atypical samples and enjoy better test performance. While, DNNs which are optimized via adversarial training algorithms can also achieve perfect training performance by memorizing the labels of atypical samples, as well as the adversarially perturbed atypical samples. However, adversarially trained models always suffer from poor generalization, with both relatively low clean accuracy and robustness on the test set. In this work, we study the effect of memorization in adversarial trained DNNs and disclose two important findings: (a) Memorizing atypical samples is only effective to improve DNN's accuracy on clean atypical samples, but hardly improve their adversarial robustness and (b) Memorizing certain atypical samples will even hurt the DNN's performance on typical samples. Based on these two findings, we propose Benign Adversarial Training (BAT) which can facilitate adversarial training to avoid fitting "harmful" atypical samples and fit as more "benign" atypical samples as possible. In our experiments, we validate the effectiveness of BAT, and show that it can achieve better clean accuracy vs. robustness trade-off than baseline methods, in benchmark datasets for image classification.|最近的研究表明，“记忆”是过参数化深层神经网络(DNN)获得最佳性能的一个必要因素。具体来说，完全拟合的 DNN 可以记忆许多非典型样本的标签，概括其记忆，以正确分类测试非典型样本，并享有更好的测试性能。同时，通过对抗性训练算法优化的 DNN 也可以通过记忆非典型样本的标签以及对抗性扰动的非典型样本来获得完美的训练性能。然而，对抗训练的模型总是缺乏一般性，在测试集上具有相对较低的清洁精度和鲁棒性。在这项工作中，我们研究了记忆在对抗性训练的 DNN 中的作用，并揭示了两个重要的发现: (a)记忆非典型样本只能有效地提高 DNN 对干净的非典型样本的准确性，但很难提高它们的对抗性鲁棒性和(b)记忆某些非典型样本甚至会损害 DNN 在典型样本上的表现。基于这两个发现，我们提出良性对抗训练(BAT) ，它可以促进对抗训练，以避免拟合“有害”的非典型样本，并拟合尽可能多的“良性”非典型样本。在实验中，我们验证了 BAT 算法的有效性，并表明在基准数据集的图像分类中，BAT 算法能够取得比基准方法更好的清除精度和鲁棒性权衡。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+does+the+Memorization+of+Neural+Networks+Impact+Adversarial+Robust+Models?)|0|
|[Internal Logical Induction for Pixel-Symbolic Reinforcement Learning](https://doi.org/10.1145/3580305.3599393)|Jiacheng Xu, Chao Chen, Fuxiang Zhang, Lei Yuan, Zongzhang Zhang, Yang Yu||Reinforcement Learning (RL) has experienced rapid advancements in recent years. The widely studied RL algorithms mainly focus on a single input form, such as pixel-based image input or symbolic vector input. These two forms have different characteristics and, in many scenarios, will appear together, while few RL algorithms have studied the problems with mixed input types. Specifically, in the scenario where both pixel and symbolic inputs are available, symbolic input usually offers abstract features with specific semantics, which is more conducive to the agent's focus. Conversely, pixel input provides more comprehensive information, enabling the agent to make well-informed decisions. Tailoring the processing approach based on the properties of these two input types can contribute to solving the problem more effectively. To tackle the above issue, we propose an Internal Logical Induction (ILI) framework that integrates deep RL and rule learning into one system. ILI utilizes the deep RL algorithm to process the pixel input and the rule learning algorithm to induce propositional logic knowledge from symbolic input. To efficiently combine these two mechanisms, we further adopt a reward shaping technique by treating valuable knowledge as intrinsic rewards for the RL procedure. Experimental results demonstrate that the ILI framework outperforms baseline approaches in RL problems with pixel-symbolic input, and its inductive knowledge exhibits transferability advantages when pixel input semantics change.|强化学习近年发展迅速。被广泛研究的 RL 算法主要集中在单一的输入形式，如基于像素的图像输入或符号矢量输入。这两种形式具有不同的特点，在许多场景中会同时出现，而很少有 RL 算法研究混合输入类型的问题。具体来说，在像素和符号输入都可用的场景中，符号输入通常提供具有特定语义的抽象特征，这更有利于代理的关注。相反，像素输入提供了更全面的信息，使代理能够做出明智的决策。根据这两种输入类型的特性对处理方法进行裁剪，有助于更有效地解决问题。为了解决上述问题，我们提出了一个内部逻辑归纳(ILI)框架，该框架将深度 RL 和规则学习集成到一个系统中。ILI 利用深度 RL 算法处理像素输入，并利用规则学习算法从符号输入中归纳出命题逻辑知识。为了有效地结合这两种机制，我们进一步采用奖励形成技术，将有价值的知识作为 RL 程序的内在奖励。实验结果表明，在具有像素-符号输入的 RL 问题中，ILI 框架的性能优于基线方法，并且当像素输入语义发生变化时，其归纳知识具有可转移性优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Internal+Logical+Induction+for+Pixel-Symbolic+Reinforcement+Learning)|0|
|[Node Classification Beyond Homophily: Towards a General Solution](https://doi.org/10.1145/3580305.3599446)|Zhe Xu, Yuzhong Chen, Qinghai Zhou, Yuhang Wu, Menghai Pan, Hao Yang, Hanghang Tong|VISA Research; Visa Research; University of Illinois, Urbana Champaign; Arizona State University; University of Illinois at Urbana-Champaign|Graph neural networks (GNNs) have become core building blocks behind a myriad of graph learning tasks. The vast majority of the existing GNNs are built upon, either implicitly or explicitly, the homophily assumption, which is not always true and could heavily degrade the performance of learning tasks. In response, GNNs tailored for heterophilic graphs have been developed. However, most of the existing works are designed for the specific GNN models to address heterophily, which lacks generality. In this paper, we study the problem from the structure learning perspective and propose a family of general solutions named ALT. It can work hand in hand with most of the existing GNNs to decently handle graphs with either low or high homophily. The core of our method is learning to (1) decompose a given graph into two components, (2) extract complementary graph signals from these two components, and (3) adaptively merge the graph signals for node classification. Moreover, analysis based on graph signal processing shows that our framework can empower a broad range of existing GNNs to have adaptive filter characteristics and further modulate the input graph signals, which is critical for handling complex homophilic/heterophilic patterns. The proposed ALT brings significant and consistent performance improvement in node classification for a wide range of GNNs over a variety of real-world datasets.|图形神经网络(GNN)已经成为无数图形学习任务背后的核心构件。现有的 GNN 绝大多数都是建立在隐式或显式的同调假设之上的，这种假设并不总是正确的，而且会严重降低学习任务的性能。作为回应，为异质图量身定制的 GNN 已经被开发出来。然而，现有的大多数工作都是针对特定的 GNN 模型进行异构化处理，缺乏通用性。本文从结构学习的角度对该问题进行了研究，提出了一系列通用的解决方案，即 ALT。它可以与大多数现有的 GNN 一起工作，合理地处理具有低同调或高同调的图。该方法的核心是学习(1)将给定的图分解为两个分量，(2)从这两个分量中提取互补的图信号，(3)自适应地合并图信号进行节点分类。此外，基于图形信号处理的分析表明，我们的框架可以赋予广泛的现有 GNN 具有自适应滤波特性，并进一步调制输入图形信号，这对于处理复杂的亲同/亲异模式至关重要。提出的 ALT 算法可以显著提高现实世界中各种数据集上广泛的 GNN 的节点分类性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Node+Classification+Beyond+Homophily:+Towards+a+General+Solution)|0|
|[CriticalFL: A Critical Learning Periods Augmented Client Selection Framework for Efficient Federated Learning](https://doi.org/10.1145/3580305.3599293)|Gang Yan, Hao Wang, Xu Yuan, Jian Li||Federated learning (FL) is a distributed optimization paradigm that learns from data samples distributed across a number of clients. Adaptive client selection that is cognizant of the training progress of clients has become a major trend to improve FL efficiency but not yet well-understood. Most existing FL methods such as FedAvg and its state-of-the-art variants implicitly assume that all learning phases during the FL training process are equally important. Unfortunately, this assumption has been revealed to be invalid due to recent findings on critical learning periods (CLP), in which small gradient errors may lead to an irrecoverable deficiency on final test accuracy. In this paper, we develop CriticalFL, a CLP augmented FL framework to reveal that adaptively augmenting exiting FL methods with CLP, the resultant performance is significantly improved when the client selection is guided by the discovered CLP. Experiments based on various machine learning models and datasets validate that the proposed CriticalFL framework consistently achieves an improved model accuracy while maintains better communication efficiency as compared to state-of-the-art methods, demonstrating a promising and easily adopted method for tackling the heterogeneity of FL training.|联邦学习(FL)是一种分布式优化范式，它从分布在多个客户端的数据样本中学习。了解客户培训进展的适应性客户选择已成为提高外语教学效率的主要趋势，但尚未得到充分认识。大多数现有的外语学习方法，如 FedAvg 及其最先进的变体，都隐含地假定外语学习过程中的所有学习阶段都同样重要。不幸的是，由于关键学习期(CLP)的最新发现，这一假设已被证明是无效的，在 CLP 中，小的梯度误差可能导致最终测试准确性的不可恢复的缺陷。本文通过开发一个 CLP 增强 FL 框架 CriticalFL，揭示了用 CLP 自适应地增强已有的 FL 方法，当客户选择由发现的 CLP 指导时，所得到的性能显著提高。基于各种机器学习模型和数据集的实验验证了所提出的 CriticalFL 框架在保持较高通信效率的同时，始终保持了较高的模型精度，为解决 FL 训练的异质性问题提供了一种有前途且易于采用的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CriticalFL:+A+Critical+Learning+Periods+Augmented+Client+Selection+Framework+for+Efficient+Federated+Learning)|0|
|[Fragility Index: A New Approach for Binary Classification](https://doi.org/10.1145/3580305.3599356)|Chen Yang, Ziqiang Zhang, Bo Cao, Zheng Cui, Bin Hu, Tong Li, Daniel Zhuoyu Long, Jin Qi, Feng Wang, Ruohan Zhan||In binary classification problems, many performance metrics evaluate the probability that some error exceeds a threshold. Nevertheless, they focus more on the probability and fail to capture the magnitude of the error, which evaluates how large this error exceeds the threshold. Capturing the magnitude of error is desired in many applications. For example, in detecting disease and predicting credit default, the magnitude of error illustrates the confidence in making the wrong prediction. We propose a novel metric, the Fragility Index (FI), to evaluate the performance of binary classifiers by capturing the magnitude of the error. FI alleviates the risk of misclassification by penalizing the large error greatly, which is seldom considered by standard metrics. Moreover, to strengthen the generalization ability and handle unseen samples, we adopt the framework of distributionally robust optimization and robust satisficing, which allows us to derive and control the maximum degree of fragility of the classifier when the distribution of samples shifts. We show that FI can be easily calculated and optimized for common probabilistic distance measures. Experiments with real datasets demonstrate the new insights brought by FI and the advantages of classifiers selected under FI, which always improve the robustness and reduce the risk of large errors as compared to classifiers selected by alternative metrics.|在二进制分类问题中，许多性能指标评估某些错误超过阈值的可能性。尽管如此，他们更多地关注概率，而不能捕捉错误的大小，从而评估这个错误超出阈值的程度。在许多应用程序中，需要捕获错误的大小。例如，在检测疾病和预测信用违约时，错误的大小说明了做出错误预测的信心。我们提出了一种新的度量方法，脆弱性指数(FI) ，通过捕获错误的大小来评估二进制分类器的性能。FI 通过惩罚较大的错误来降低错误分类的风险，而标准度量很少考虑这一点。此外，为了增强分类器的泛化能力和处理不可见样本，我们采用了分布鲁棒优化和鲁棒满足的框架，使我们能够推导和控制样本分布发生变化时分类器的最大脆弱度。结果表明，对于一般的概率距离测度，FI 可以很容易地计算和优化。实际数据集的实验结果表明，相对于其他度量方法选择的分类器，FI 分类器具有更好的鲁棒性和更小的误差风险。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fragility+Index:+A+New+Approach+for+Binary+Classification)|0|
|[IDToolkit: A Toolkit for Benchmarking and Developing Inverse Design Algorithms in Nanophotonics](https://doi.org/10.1145/3580305.3599385)|JiaQi Yang, Yucheng Xu, JiaLei Shen, KeBin Fan, DeChuan Zhan, Yang Yang|Nanjing University of Science and Technology; Nanjing University|Aiding humans with scientific designs is one of the most exciting of artificial intelligence (AI) and machine learning (ML), due to their potential for the discovery of new drugs, design of new materials and chemical compounds, etc. However, scientific design typically requires complex domain knowledge that is not familiar to AI researchers. Further, scientific studies involve professional skills to perform experiments and evaluations. These obstacles prevent AI researchers from developing specialized methods for scientific designs. To take a step towards easy-to-understand and reproducible research of scientific design, we propose a benchmark for the inverse design of nanophotonic devices, which can be verified computationally and accurately. Specifically, we implemented three different nanophotonic design problems, namely a radiative cooler, a selective emitter for thermophotovoltaics, and structural color filters, all of which are different in design parameter spaces, complexity, and design targets. The benchmark environments are implemented with an open-source simulator. We further implemented 10 different inverse design algorithms and compared them in a reproducible and fair framework. The results revealed the strengths and weaknesses of existing methods, which shed light on several future directions for developing more efficient inverse design algorithms. Our benchmark can also serve as the starting point for more challenging scientific design problems. The code of IDToolkit is available at https://github.com/ThyrixYang/IDToolkit.|帮助人类进行科学设计是人工智能(AI)和机器学习(ML)中最令人兴奋的领域之一，因为它们具有发现新药、设计新材料和化合物等方面的潜力。然而，科学设计通常需要复杂的领域知识，这是人工智能研究人员不熟悉的。此外，科学研究还包括进行实验和评估的专业技能。这些障碍阻碍了人工智能研究人员为科学设计开发专门方法。为了向科学设计的易于理解和可重复性研究迈进一步，我们提出了一个纳米光子器件逆向设计的基准，它可以被计算和精确地验证。具体来说，我们实现了三种不同的纳米光子设计问题，即辐射制冷器、热光伏电池的选择性发射器和结构色滤波器，所有这些问题在设计参数空间、复杂性和设计目标上都是不同的。基准测试环境是通过开源模拟器实现的。我们进一步实现了10种不同的逆向设计算法，并在一个可重复和公平的框架中对它们进行了比较。研究结果揭示了现有逆设计方法的优缺点，为开发更有效的逆设计算法指明了方向。我们的基准也可以作为更具挑战性的科学设计问题的起点。IDToolkit 的代码可在 https://github.com/thyrixyang/IDToolkit 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IDToolkit:+A+Toolkit+for+Benchmarking+and+Developing+Inverse+Design+Algorithms+in+Nanophotonics)|0|
|[MAPLE: Semi-Supervised Learning with Multi-Alignment and Pseudo-Learning](https://doi.org/10.1145/3580305.3599423)|Juncheng Yang, Chao Li, Zuchao Li, Wei Yu, Bo Du, Shijun Li|JD Health International Inc.; School of Computer Science, Wuhan University; National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University|Data augmentation has undoubtedly enabled a significant leap forward in training a high-accuracy deep network. Besides the commonly used augmentation to target data, e.g., random cropping, flipping, and rotation, recent works have been dedicated to mining generalized knowledge by using multiple sources. However, along with plentiful data comes the huge data distribution gap between the target and different sources (hybrid shift). To mitigate this problem, existing methods tend to manually annotate more data. Unlike previous methods, this paper focuses on the study of learning deep models by gathering knowledge from multiple sources in a labor-free fashion and further proposes the "Multi-Alignment and Pseudo-Learning'' method, dubbed MAPLE. MAPLE constructs the multi-alignment module, which consists of multiple discriminators to align different data distributions via an adversarial process. In addition, a novel semi-supervised learning (SSL) manner is introduced to further facilitate the utility of our MAPLE. Extensive evaluations conducted on four benchmarks show the effectiveness of the proposed MAPLE, which achieves state-of-the-art performance outperforming existing methods by an obvious margin.|毫无疑问，数据增强使得在训练高精度深度网络方面取得了重大飞跃。除了常用的目标数据增强，如随机裁剪，翻转和旋转，最近的工作已致力于挖掘广义知识使用多个来源。然而，伴随着大量的数据而来的是目标和不同来源(混合移位)之间巨大的数据分布差距。为了缓解这个问题，现有的方法倾向于手动注释更多的数据。与以往的方法不同，本文重点研究了基于多源知识的深度学习方法，并进一步提出了“多对齐伪学习”方法，即 MAPLE 方法。MAPLE 构造了多对齐模块，该模块由多个鉴别器组成，通过对抗过程对齐不同的数据分布。此外，还引入了一种新的半监督学习(SSL)方式，以进一步促进我们的 MAPLE 的实用性。对四个基准进行的广泛评估表明了所提出的 MAPLE 的有效性，它实现了最先进的性能，明显优于现有的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MAPLE:+Semi-Supervised+Learning+with+Multi-Alignment+and+Pseudo-Learning)|0|
|[EXTRACT and REFINE: Finding a Support Subgraph Set for Graph Representation](https://doi.org/10.1145/3580305.3599339)|Kuo Yang, Zhengyang Zhou, Wei Sun, Pengkun Wang, Xu Wang, Yang Wang|Suzhou Institute for Advanced Research, University of Science and Technology of China; University of Science and Technology of China|Subgraph learning has received considerable attention in its capacity of interpreting important structural information for predictions. Existing subgraph learning usually exploits statistics on predefined structures e.g., node degrees, occurrence frequency, to extract subgraphs, or refine the contents via only capturing label-relevant information with node-level sampling. Given diverse subgraph patterns, and mutual independence with local correlations on graphs, current solutions on subgraph learning still have two limitations in extraction and refinement stages. 1) The universality of extracting substructure patterns across domains is still lacking, 2) node-level sampling in refinement will distort the original local topology and none explicit guidance eliminating redundant information contribute to inefficiency issue. In this paper, we propose a unified subgraph learning scheme, Poly-Pivot Graph Neural Network (P2GNN) where we designate the centric node of each subgraph as the pivot. In the extraction stage, we present a general subgraph extraction principle, i.e., Local; Asymmetry between the centric and affiliated nodes. To this end, we asymmetrically model the similarity between each pair of nodes with random walk and quantify mutual affiliations in Affinity Propagation architecture, to extract subgraph structures. In the refinement, we devise a subgraph-level exclusion regularization to squash the target-independent information by considering mutual relations across subgraphs, cooperatively preserving a support set of subgraphs and facilitating the refinement process for graph representation. Empirical experiments on diverse web and biological graphs reveal 1.1%~7.3% improvements against best baselines, and visualized case studies prove the universality and interpretability of our P2GNN.|子图学习在解释用于预测的重要结构信息方面受到了相当的重视。现有的子图学习通常利用预定义结构(如节点度、出现频率)的统计信息来提取子图，或者通过节点级抽样仅捕获与标签相关的信息来细化内容。鉴于子图模式的多样性，以及子图之间的相互独立性和局部相关性，目前子图学习的解决方案在提取和精化阶段仍然存在两个局限性。1)提取域间子结构模式的通用性仍然不足; 2)精化过程中的节点级抽样会扭曲原有的局部拓扑结构，而且没有明确的指导消除冗余信息，从而导致效率低下的问题。本文提出了一种统一的子图学习方案——多支点图神经网络(P2GNN)。在子图抽取阶段，我们提出了一个通用的子图抽取原则，即中心节点和附属节点之间的局部不对称性。为此，在亲和传播体系结构中，我们不对称地建立了随机游走的每对节点之间的相似性模型，量化了相互关联，从而提取出子图结构。在细化过程中，我们通过考虑子图之间的相互关系，协同保持子图的支持集，以及促进图表示的细化过程，设计了子图级排斥正则化来压缩与目标无关的信息。在不同的网络和生物图表上进行的实验表明，与最佳基线相比，P2GNN 有1.1% ~ 7.3% 的改善，可视化的案例研究证明了 P2GNN 的通用性和可解释性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EXTRACT+and+REFINE:+Finding+a+Support+Subgraph+Set+for+Graph+Representation)|0|
|[κHGCN: Tree-likeness Modeling via Continuous and Discrete Curvature Learning](https://doi.org/10.1145/3580305.3599532)|Menglin Yang, Min Zhou, Lujia Pan, Irwin King|Huawei Technologies Co., Ltd., Shenzhen, China; Huawei Technologies Co., Ltd., Hong Kong, China; The Chinese University of Hong Kong, Hong Kong, China|The prevalence of tree-like structures, encompassing hierarchical structures and power law distributions, exists extensively in real-world applications, including recommendation systems, ecosystems, financial networks, social networks, etc. Recently, the exploitation of hyperbolic space for tree-likeness modeling has garnered considerable attention owing to its exponential growth volume. Compared to the flat Euclidean space, the curved hyperbolic space provides a more amenable and embeddable room, especially for datasets exhibiting implicit tree-like architectures. However, the intricate nature of real-world tree-like data presents a considerable challenge, as it frequently displays a heterogeneous composition of tree-like, flat, and circular regions. The direct embedding of such heterogeneous structures into a homogeneous embedding space (i.e., hyperbolic space) inevitably leads to heavy distortions. To mitigate the aforementioned shortage, this study endeavors to explore the curvature between discrete structure and continuous learning space, aiming at encoding the message conveyed by the network topology in the learning process, thereby improving tree-likeness modeling. To the end, a curvature-aware hyperbolic graph convolutional neural network, κHGCN, is proposed, which utilizes the curvature to guide message passing and improve long-range propagation. Extensive experiments on node classification and link prediction tasks verify the superiority of the proposal as it consistently outperforms various competitive models by a large margin.|树状结构的普遍存在，包括层次结构和幂律分布，广泛存在于现实世界的应用，包括推荐系统，生态系统，金融网络，社会网络等。最近，利用双曲空间进行树形建模已经引起了相当大的关注，因为它的指数增长很大。与平坦的欧几里得空间相比，弯曲的双曲空间提供了一个更易于操作和嵌入的空间，特别是对于显示隐式树状结构的数据集。然而，真实世界中树状数据的复杂性质提出了一个相当大的挑战，因为它经常显示树状、平坦和圆形区域的异构组合。将这种异质结构直接嵌入到同质嵌入空间(即双曲空间)中，不可避免地会导致严重的失真。为了解决上述问题，本研究致力于探索离散结构与连续学习空间之间的曲率关系，旨在对学习过程中网络拓扑传递的信息进行编码，从而改进树状模型。最后，提出了一个曲率感知的双曲图卷积神经网络 κHGCN，它利用曲率来引导信息传递和改善远程传播。通过对节点分类和链路预测任务的大量实验，验证了该方案的优越性，因为它始终大大优于各种竞争模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=κHGCN:+Tree-likeness+Modeling+via+Continuous+and+Discrete+Curvature+Learning)|0|
|[Specify Robust Causal Representation from Mixed Observations](https://doi.org/10.1145/3580305.3599512)|Mengyue Yang, Xinyu Cai, Furui Liu, Weinan Zhang, Jun Wang||Learning representations purely from observations concerns the problem of learning a low-dimensional, compact representation which is beneficial to prediction models. Under the hypothesis that the intrinsic latent factors follow some casual generative models, we argue that by learning a causal representation, which is the minimal sufficient causes of the whole system, we can improve the robustness and generalization performance of machine learning models. In this paper, we develop a learning method to learn such representation from observational data by regularizing the learning procedure with mutual information measures, according to the hypothetical factored causal graph. We theoretically and empirically show that the models trained with the learned causal representations are more robust under adversarial attacks and distribution shifts compared with baselines. The supplementary materials are available at https://github.com/ymy $4323460 / \mathrm{CaRI} /$.|纯粹从观测中学习表征涉及到学习低维紧凑表征的问题，这对预测模型是有益的。在内在潜在因素遵循一些随机生成模型的假设下，我们认为通过学习一个因果表示，这是整个系统的最小充分原因，我们可以提高机器学习模型的鲁棒性和泛化性能。本文提出了一种从观测数据中学习这种表示的学习方法，该方法根据假设的因果图，利用互信息测度来规范学习过程。我们从理论和经验上证明了与基线相比，学习因果表征训练的模型在对抗性攻击和分布偏移下更加稳健。补充材料 https://github.com/ymy  $4323460/mathrm { CaRI }/$。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Specify+Robust+Causal+Representation+from+Mixed+Observations)|0|
|[Counterfactual Learning on Heterogeneous Graphs with Greedy Perturbation](https://doi.org/10.1145/3580305.3599289)|Qiang Yang, Changsheng Ma, Qiannan Zhang, Xin Gao, Chuxu Zhang, Xiangliang Zhang||Due to the growing importance of using graph neural networks in high-stakes applications, there is a pressing need to interpret the predicted results of these models. Existing methods for explanation have mainly focused on generating sub-graphs comprising important edges for a specific prediction. However, these methods face two issues. Firstly, they lack counterfactual validity as removing the subgraph may not affect the prediction, and generating plausible counterfactual examples has not been adequately explored. Secondly, they cannot be extended to heterogeneous graphs as the complex information involved in such graphs increases the difficulty of generating interpretations. This paper proposes a novel counterfactual learning method, named CF-HGExplainer, for heterogeneous graphs. The method incorporates a semantic-aware attentive pooling strategy for the heterogeneous graph classifier and designs a heterogeneous decision boundaries extraction module to find the common logic for similar graphs based on the extracted graph embeddings from the classifier. Additionally, we propose to greedily perturb nodes and edges based on the distribution of node features and edge plausibility to train a neural network for heterogeneous edge weight learning. Extensive experiments on two public academic datasets demonstrate the effectiveness of CF-HGExplainer compared to state-of-the-art methods on the graph classification task and graph interpretation task.|由于图形神经网络在高风险应用中的重要性日益增加，迫切需要解释这些模型的预测结果。现有的解释方法主要集中在为特定的预测生成包含重要边的子图。然而，这些方法面临两个问题。首先，它们缺乏反事实效度，因为删除子图可能不会影响预测，生成似是而非的反事实例子也没有得到充分的探讨。其次，它们不能扩展到异构图，因为这类图所包含的复杂信息增加了生成解释的难度。针对异构图，提出了一种新的反事实学习方法 CF-HG Explainer。该方法结合了异构图分类器的语义感知注意池策略，设计了异构决策边界提取模块，基于从分类器中提取的图嵌入，找到相似图的公共逻辑。此外，我们提出了基于节点特征分布和边缘可信度对节点和边缘进行贪婪的扰动，以训练一个神经网络用于异构边缘权值学习。通过对两个公共学术数据集的大量实验，证明了 CF-HG Explainer 方法在图形分类任务和图形解释任务中的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Counterfactual+Learning+on+Heterogeneous+Graphs+with+Greedy+Perturbation)|0|
|[LightPath: Lightweight and Scalable Path Representation Learning](https://doi.org/10.1145/3580305.3599415)|Sean Bin Yang, Jilin Hu, Chenjuan Guo, Bin Yang, Christian S. Jensen||Movement paths are used widely in intelligent transportation and smart city applications. To serve such applications, path representation learning aims to provide compact representations of paths that enable efficient and accurate operations when used for different downstream tasks such as path ranking and travel cost estimation. In many cases, it is attractive that the path representation learning is lightweight and scalable; in resource-limited environments and under green computing limitations, it is essential. Yet, existing path representation learning studies focus on accuracy and pay at most secondary attention to resource consumption and scalability. We propose a lightweight and scalable path representation learning framework, termed LightPath, that aims to reduce resource consumption and achieve scalability without affecting accuracy, thus enabling broader applicability. More specifically, we first propose a sparse auto-encoder that ensures that the framework achieves good scalability with respect to path length. Next, we propose a relational reasoning framework to enable faster training of more robust sparse path encoders. We also propose global-local knowledge distillation to further reduce the size and improve the performance of sparse path encoders. Finally, we report extensive experiments on two real-world datasets to offer insight into the efficiency, scalability, and effectiveness of the proposed framework.|运动路径在智能交通和智能城市中有着广泛的应用。为了满足这些应用，路径表示学习的目的是提供紧凑的路径表示，当用于不同的下游任务(如路径排序和旅行成本估算)时，能够实现高效和准确的操作。在许多情况下，路径表示学习是轻量级和可扩展的，在资源有限的环境和绿色计算的限制下，它是非常有吸引力的。然而，现有的路径表示学习研究关注的是准确性，而对资源消耗和可扩展性的关注最少。我们提出了一个轻量级和可扩展的路径表示学习框架，称为 LightPath，其目的是在不影响精度的情况下减少资源消耗和实现可扩展性，从而实现更广泛的适用性。更具体地说，我们首先提出了一种稀疏自动编码器，它可以确保框架在路径长度方面实现良好的可伸缩性。接下来，我们提出了一个关系推理框架，使更健壮的稀疏路径编码器更快的训练。提出了全局-局部知识提取方法，进一步减小了稀疏路径编码器的体积，提高了编码器的性能。最后，我们在两个真实世界的数据集上进行了广泛的实验，以深入了解所提出的框架的效率、可伸缩性和有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LightPath:+Lightweight+and+Scalable+Path+Representation+Learning)|0|
|[Test Accuracy vs. Generalization Gap: Model Selection in NLP without Accessing Training or Testing Data](https://doi.org/10.1145/3580305.3599518)|Yaoqing Yang, Ryan Theisen, Liam Hodgkinson, Joseph E. Gonzalez, Kannan Ramchandran, Charles H. Martin, Michael W. Mahoney||Selecting suitable architecture parameters and training hyperparameters is essential for enhancing machine learning (ML) model performance. Several recent empirical studies conduct large-scale correlational analysis on neural networks (NNs) to search for effective generalization metrics that can guide this type of model selection. Effective metrics are typically expected to correlate strongly with test performance. In this paper, we expand on prior analyses by examining generalization-metric-based model selection with the following objectives: (i) focusing on natural language processing (NLP) tasks, as prior work primarily concentrates on computer vision (CV) tasks; (ii) considering metrics that directly predict test error instead of the generalization gap; (iii) exploring metrics that do not need access to data to compute. From these objectives, we are able to provide the first model selection results on large pretrained Transformers from Huggingface using generalization metrics. Our analyses consider (I) hundreds of Transformers trained in different settings, in which we systematically vary the amount of data, the model size and the optimization hyperparameters, (II) a total of 51 pretrained Transformers from eight families of Huggingface NLP models, including GPT2, BERT, etc., and (III) a total of 28 existing and novel generalization metrics. Despite their niche status, we find that metrics derived from the heavy-tail (HT) perspective are particularly useful in NLP tasks, exhibiting stronger correlations than other, more popular metrics. To further examine these metrics, we extend prior formulations relying on power law (PL) spectral distributions to exponential (EXP) and exponentially-truncated power law (E-TPL) families.|选择合适的结构参数和训练超参数是提高机器学习(ML)模型性能的关键。最近的一些实证研究对神经网络(NN)进行了大规模的相关分析，以寻找能够指导这种类型的模型选择的有效的泛化度量。有效的度量通常被认为与测试性能密切相关。在本文中，我们通过检查基于概括度量的模型选择，扩展了先前的分析，其目标如下: (i)关注自然语言处理(NLP)任务，因为先前的工作主要集中在计算机视觉(CV)任务; (ii)考虑直接预测测试错误的度量，而不是概括差距; (iii)探索不需要访问数据进行计算的度量。从这些目标，我们能够提供第一个模型选择结果从 Huggingface 预先训练的大型变压器使用通用指标。我们的分析考虑(I)在不同环境中训练的数百个变压器，其中我们系统地改变数据量，模型大小和优化超参数，(II)来自八个 Huggingface NLP 模型家族的总共51个预训练的变压器，包括 GPT2，BERT 等，和(III)总共28个现有的和新的通用指标。尽管它们处于小生境地位，我们发现从重尾(HT)角度得出的度量在 NLP 任务中特别有用，表现出比其他更流行的度量更强的相关性。为了进一步检验这些指标，我们将先前依赖于幂律(PL)谱分布的公式推广到指数(EXP)和指数截断幂律(E-TPL)族。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Test+Accuracy+vs.+Generalization+Gap:+Model+Selection+in+NLP+without+Accessing+Training+or+Testing+Data)|0|
|[DCdetector: Dual Attention Contrastive Representation Learning for Time Series Anomaly Detection](https://doi.org/10.1145/3580305.3599295)|Yiyuan Yang, Chaoli Zhang, Tian Zhou, Qingsong Wen, Liang Sun|Alibaba Group; University of Oxford|Time series anomaly detection is critical for a wide range of applications. It aims to identify deviant samples from the normal sample distribution in time series. The most fundamental challenge for this task is to learn a representation map that enables effective discrimination of anomalies. Reconstruction-based methods still dominate, but the representation learning with anomalies might hurt the performance with its large abnormal loss. On the other hand, contrastive learning aims to find a representation that can clearly distinguish any instance from the others, which can bring a more natural and promising representation for time series anomaly detection. In this paper, we propose DCdetector, a multi-scale dual attention contrastive representation learning model. DCdetector utilizes a novel dual attention asymmetric design to create the permutated environment and pure contrastive loss to guide the learning process, thus learning a permutation invariant representation with superior discrimination abilities. Extensive experiments show that DCdetector achieves state-of-the-art results on multiple time series anomaly detection benchmark datasets. Code is publicly available at https://github.com/DAMO-DI-ML/KDD2023-DCdetector.|时间序列异常检测对于广泛的应用是至关重要的。其目的是从时间序列的正态样本分布中识别出偏差样本。这项任务最基本的挑战是学习一种能够有效区分异常的表示图。基于重构的方法仍然占主导地位，但异常情况下的表示学习会因为大量的异常损失而影响性能。另一方面，对比学习的目的是找到一种能够清晰区分任何实例和其他实例的表征，这种表征能够为时间序列异常检测带来更自然、更有前途的表征。本文提出了一种多尺度双注意对比表征学习模型—— DC 检测器。DC 检测器利用一种新颖的双注意不对称设计来创建置换环境和纯对比度损失来指导学习过程，从而学习具有优越鉴别能力的置换不变表示。大量实验表明，dc 检测器在多个时间序列异常检测基准数据集上取得了最先进的结果。代码可在 https://github.com/damo-di-ml/kdd2023-dcdetector 公开查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DCdetector:+Dual+Attention+Contrastive+Representation+Learning+for+Time+Series+Anomaly+Detection)|0|
|[Improving the Expressiveness of K-hop Message-Passing GNNs by Injecting Contextualized Substructure Information](https://doi.org/10.1145/3580305.3599390)|Tianjun Yao, Yingxu Wang, Kun Zhang, Shangsong Liang||Graph neural networks (GNNs) have become the de facto standard for representational learning in graphs, and have achieved state-of-the-art performance in many graph-related tasks; however, it has been shown that the expressive power of standard GNNs are equivalent maximally to 1-dimensional Weisfeiler-Lehman (1-WL) Test. Recently, there is a line of works aiming to enhance the expressive power of graph neural networks. One line of such works aim at developing K-hop message-passing GNNs where node representation is updated by aggregating information from not only direct neighbors but all neighbors within K-hop of the node. Another line of works leverages subgraph information to enhance the expressive power which is proven to be strictly more powerful than 1-WL test. In this work, we discuss the limitation of K-hop message-passing GNNs and propose substructure encoding function to uplift the expressive power of any K-hop message-passing GNN. We further inject contextualized substructure information to enhance the expressiveness of K-hop message-passing GNNs. Our method is provably more powerful than previous works on K-hop graph neural networks and 1-WL subgraph GNNs, which is a specific type of subgraph based GNN models, and not less powerful than 3-WL. Empirically, our proposed method set new state-of-the-art performance or achieves comparable performance for a variety of datasets. Our code is available at https://github.com/tianyao-aka/Expresive_K_hop_GNNs.|图形神经网络(GNN)已经成为图形表征学习的行业标准，在许多与图形相关的任务中取得了最先进的表现，然而，已经证明标准 GNN 的表达能力最大程度上等同于一维韦斯费勒-雷曼(1-WL)测试。最近，有一系列的工作旨在提高图神经网络的表达能力。其中一种工作是开发 K 跳消息传递 GNN，通过聚集节点的直接邻居和 K 跳内所有邻居的信息来更新节点表示。另一种方法是利用子图信息来增强表达能力，这种方法被证明比1-WL 测试更有效。本文讨论了 K 跳消息传递 GNN 的局限性，提出了子结构编码函数来提高任何 K 跳消息传递 GNN 的表达能力。我们进一步注入上下文化的子结构信息来增强 K-hop 消息传递 GNN 的表达能力。我们的方法被证明是更强大的比先前的工作在 K 跳图神经网络和1-WL 子图 GNN，这是一个特定类型的子图基于 GNN 模型，并不比3-WL 弱。实际上，我们提出的方法为各种数据集设置了新的最先进的性能或者实现了可比较的性能。我们的代码可以在 https://github.com/tianyao-aka/expresive_k_hop_gnns 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+the+Expressiveness+of+K-hop+Message-Passing+GNNs+by+Injecting+Contextualized+Substructure+Information)|0|
|[PAT: Geometry-Aware Hard-Label Black-Box Adversarial Attacks on Text](https://doi.org/10.1145/3580305.3599461)|Muchao Ye, Jinghui Chen, Chenglin Miao, Han Liu, Ting Wang, Fenglong Ma||Despite a plethora of prior explorations, conducting text adversarial attacks in practical settings is still challenging with the following constraints: black box -- the inner structure of the victim model is unknown; hard label -- the attacker only has access to the top-1 prediction results; and semantic preservation - the perturbation needs to preserve the original semantics. In this paper, we present PAT, a novel adversarial attack method employed under all these constraints. Specifically, PAT explicitly models the adversarial and non-adversarial prototypes and incorporates them to measure semantic changes for replacement selection in the hard-label black-box setting to generate high-quality samples. In each iteration, PAT finds original words that can be replaced back and selects better candidate words for perturbed positions in a geometry-aware manner guided by this estimation, which maximally improves the perturbation construction and minimally impacts the original semantics. Extensive evaluation with benchmark datasets and state-of-the-art models shows that PAT outperforms existing text adversarial attacks in terms of both attack effectiveness and semantic preservation. Moreover, we validate the efficacy of PAT against industry-leading natural language processing platforms in real-world settings.|尽管之前已经进行了大量的研究，但是在实际环境中进行文本对抗性攻击仍然面临以下约束: 黑匣子——受害者模型的内部结构是未知的; 硬标签——攻击者只能访问排名前一的预测结果; 语义保留——扰动需要保留原始语义。在本文中，我们提出了一种新的对抗性攻击方法 PAT，在所有这些约束条件下使用。具体来说，PAT 显式地建立了对抗原型和非对抗原型的模型，并将它们合并在一起，以测量硬标签黑盒设置中替换选择的语义变化，从而生成高质量的样本。在每次迭代中，PAT 都能找到可以替换回来的原始单词，并在这种估计的指导下，以几何感知的方式为扰动位置选择更好的候选单词，从而最大限度地改进了扰动结构，并最小限度地影响了原始语义。使用基准数据集和最先进的模型进行的广泛评估表明，PAT 在攻击效果和语义保护方面都优于现有的文本对抗攻击。此外，我们还验证了 PAT 在现实环境中针对业界领先的自然语言处理平台的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PAT:+Geometry-Aware+Hard-Label+Black-Box+Adversarial+Attacks+on+Text)|0|
|[Less is More: SlimG for Accurate, Robust, and Interpretable Graph Mining](https://doi.org/10.1145/3580305.3599413)|Jaemin Yoo, MengChieh Lee, Shubhranshu Shekhar, Christos Faloutsos||How can we solve semi-supervised node classification in various graphs possibly with noisy features and structures? Graph neural networks (GNNs) have succeeded in many graph mining tasks, but their generalizability to various graph scenarios is limited due to the difficulty of training, hyperparameter tuning, and the selection of a model itself. Einstein said that we should "make everything as simple as possible, but not simpler." We rephrase it into the careful simplicity principle: a carefully-designed simple model can surpass sophisticated ones in real-world graphs. Based on the principle, we propose SlimG for semi-supervised node classification, which exhibits four desirable properties: It is (a) accurate, winning or tying on 10 out of 13 real-world datasets; (b) robust, being the only one that handles all scenarios of graph data (homophily, heterophily, random structure, noisy features, etc.); (c) fast and scalable, showing up to 18 times faster training in million-scale graphs; and (d) interpretable, thanks to the linearity and sparsity. We explain the success of SlimG through a systematic study of the designs of existing GNNs, sanity checks, and comprehensive ablation studies.|如何解决具有噪声特征和结构的各种图的半监督节点分类问题？图神经网络(GNN)已经成功地完成了许多图挖掘任务，但由于训练、超参数调整和模型本身的选择等方面的困难，其对各种图场景的推广能力受到了限制。爱因斯坦说过，我们应该“让一切尽可能简单，但不是更简单。”我们将其重新表述为精心设计的简单原则: 一个精心设计的简单模型可以超越现实世界中复杂的模型。基于这个原理，我们提出了 SlimG 用于半监督节点分类，它展示了四个理想的特性: (a)准确，在13个真实世界数据集中的10个数据集上获胜或连接; (b)鲁棒，是唯一一个处理所有图形数据场景(同质性，异质性，随机结构，噪声特征等)的; (c)快速和可扩展，在百万尺度的图中显示高达18倍的更快的训练; 和(d)可解释，由于线性和稀疏。我们通过系统研究现有 GNN 的设计、健康检查和全面的消融研究来解释 SlimG 的成功。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Less+is+More:+SlimG+for+Accurate,+Robust,+and+Interpretable+Graph+Mining)|0|
|[FLAMES2Graph: An Interpretable Federated Multivariate Time Series Classification Framework](https://doi.org/10.1145/3580305.3599354)|Raneen Younis, Zahra Ahmadi, Abdul Hakmeh, Marco Fisichella||Increasing privacy concerns have led to decentralized and federated machine learning techniques that allow individual clients to consult and train models collaboratively without sharing private information. Some of these applications, such as medical and healthcare, require the final decisions to be interpretable. One common form of data in these applications is multivariate time series, where deep neural networks, especially convolutional neural networks based approaches, have established excellent performance in their classification tasks. However, promising results and performance of deep learning models are a black box, and their decisions cannot always be guaranteed and trusted. While several approaches address the interpretability of deep learning models for multivariate time series data in a centralized environment, less effort has been made in a federated setting. In this work, we introduce FLAMES2Graph, a new horizontal federated learning framework designed to interpret the deep learning decisions of each client. FLAMES2Graph extracts and visualizes those input subsequences that are highly activated by a convolutional neural network. Besides, an evolution graph is created to capture the temporal dependencies between the extracted distinct subsequences. The federated learning clients only share this temporal evolution graph with the centralized server instead of trained model weights to create a global evolution graph. Our extensive experiments on various datasets from well-known multivariate benchmarks indicate that the FLAMES2Graph framework significantly outperforms other state-of-the-art federated methods while keeping privacy and augmenting network decision interpretation.|越来越多的隐私问题导致了分散的和联合的机器学习技术，这种技术允许个体客户在不共享私人信息的情况下协作地咨询和培训模型。其中一些应用程序，如医疗和医疗保健，要求最终决定是可解释的。在这些应用中，一种常见的数据形式是多变量时间序列，其中深层神经网络，特别是基于卷积神经网络的方法，在其分类任务中建立了优异的性能。然而，有希望的结果和性能的深度学习模型是一个黑盒子，他们的决定并不总是能够得到保证和信任。虽然有几种方法解决了在集中环境中多变量时间序列数据的深度学习模型的可解释性问题，但在联邦环境中所做的工作较少。在这项工作中，我们介绍了 FLAMES2Graph，一个新的水平联邦学习框架，旨在解释每个客户端的深度学习决策。图提取并可视化那些被卷积神经网络高度激活的输入子序列。此外，还创建了一个演化图来捕获提取的不同子序列之间的时间依赖关系。联邦学习客户端只与中央服务器共享这个时间演化图，而不是通过训练模型权重来创建全局演化图。我们对来自著名多变量基准的各种数据集进行的广泛实验表明，FLAMES2Graph 框架在保持隐私和增强网络决策解释的同时显着优于其他最先进的联邦方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FLAMES2Graph:+An+Interpretable+Federated+Multivariate+Time+Series+Classification+Framework)|0|
|[Towards Variance Reduction for Reinforcement Learning of Industrial Decision-making Tasks: A Bi-Critic based Demand-Constraint Decoupling Approach](https://doi.org/10.1145/3580305.3599527)|Jianyong Yuan, Jiayi Zhang, Zinuo Cai, Junchi Yan||Learning to plan and schedule receives increasing attention due to its efficiency in problem-solving and potential to outperform heuristics. In particular, actor-critic-based reinforcement learning (RL) has been widely adopted for uncertain environments. Yet one standing challenge for applying RL to real-world industrial decision-making problems is the high variance during training. Existing efforts design novel value functions to alleviate the issue but still suffer. In this paper, we address this issue from the perspective of adjusting the actor-critic paradigm. We start by making an observation ignored in many industrial problems---the environmental dynamics for an agent consist of two parts physically independent of each other: the exogenous task demand over time and the hard constraint for action. And we theoretically show that decoupling these two effects in the actor-critic technique would reduce variance. Accordingly, we propose to decouple and model them separately in the state transition of the Markov decision process (MDP). In the demand-encoding process, the temporal task demand, e.g., the passengers for elevator scheduling is encoded followed by a critic for scoring. While in the constraint-encoding process, an actor-critic module is adopted for action embedding, and the two critics are then used for a revised advantaged function calculation. Experimental results show that our method can adaptively handle different dynamic planning and scheduling tasks and outperform recent learning-based models and traditional heuristic algorithms.|学习计划和时间表越来越受到重视，因为它在解决问题的效率和潜力超越启发式。特别是，基于演员-评论家的强化学习(RL)已经被广泛应用于不确定环境中。然而，将 RL 应用于现实世界工业决策问题的一个长期挑战是培训过程中的高变异性。现有的努力设计新的价值函数来缓解这一问题，但仍然受到影响。在本文中，我们从调整行为者-批评范式的角度来探讨这个问题。我们从一个在许多工业问题中被忽视的观察开始——-一个主体的环境动力学由两个物理上相互独立的部分组成: 随着时间的推移外生的任务需求和行动的硬约束。从理论上证明了在行为者-批评技术中将这两种效应解耦可以减少方差。因此，我们建议在马可夫决策过程的状态转换(mDP)中分别对它们进行解耦和建模。在需求编码过程中，时间任务需求(如电梯调度中的乘客)被编码，然后是评分批判。在约束编码过程中，采用行为者-批评者模型进行行为嵌入，然后利用这两个批评者模型进行修正的有利函数计算。实验结果表明，该方法能够自适应地处理不同的动态规划和调度任务，优于现有的基于学习的模型和传统的启发式算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Variance+Reduction+for+Reinforcement+Learning+of+Industrial+Decision-making+Tasks:+A+Bi-Critic+based+Demand-Constraint+Decoupling+Approach)|0|
|[Spatio-temporal Diffusion Point Processes](https://doi.org/10.1145/3580305.3599511)|Yuan Yuan, Jingtao Ding, Chenyang Shao, Depeng Jin, Yong Li|Department of Electronic Engineering, Tsinghua University|Spatio-temporal point process (STPP) is a stochastic collection of events accompanied with time and space. Due to computational complexities, existing solutions for STPPs compromise with conditional independence between time and space, which consider the temporal and spatial distributions separately. The failure to model the joint distribution leads to limited capacities in characterizing the spatio-temporal entangled interactions given past events. In this work, we propose a novel parameterization framework for STPPs, which leverages diffusion models to learn complex spatio-temporal joint distributions. We decompose the learning of the target joint distribution into multiple steps, where each step can be faithfully described by a Gaussian distribution. To enhance the learning of each step, an elaborated spatio-temporal co-attention module is proposed to capture the interdependence between the event time and space adaptively. For the first time, we break the restrictions on spatio-temporal dependencies in existing solutions, and enable a flexible and accurate modeling paradigm for STPPs. Extensive experiments from a wide range of fields, such as epidemiology, seismology, crime, and urban mobility, demonstrate that our framework outperforms the state-of-the-art baselines remarkably, with an average improvement of over 50%. Further in-depth analyses validate its ability to capture spatio-temporal interactions, which can learn adaptively for different scenarios. The datasets and source code are available online: https://github.com/tsinghua-fib-lab/Spatio-temporal-Diffusion-Point-Processes.|时空点过程(STPP)是伴随时间和空间的随机事件集合。由于计算的复杂性，现有的解决方案折衷于时间和空间之间的条件独立，分别考虑时间和空间分布。联合分布模型的失败导致在描述过去事件的时空纠缠相互作用方面能力有限。在这项工作中，我们为 STPP 提出了一个新的参量化框架，它利用扩散模型来学习复杂的时空联合分布。我们将目标联合分布的学习分解为多个步骤，每个步骤都可以由一个正态分布忠实地描述。为了提高每个步骤的学习效率，提出了一个详细的时空共注意模块来自适应地捕捉事件时间和空间之间的相互依赖关系。我们首次打破了现有解决方案中对时空依赖性的限制，为 STPP 提供了一个灵活、准确的建模范式。来自流行病学、地震学、犯罪学和城市流动性等广泛领域的大量实验表明，我们的框架明显优于最先进的基线，平均改善超过50% 。进一步的深入分析验证了其捕获时空交互的能力，可以自适应地学习不同的场景。数据集和源代码可以在线获得:  https://github.com/tsinghua-fib-lab/spatio-temporal-diffusion-point-processes。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Spatio-temporal+Diffusion+Point+Processes)|0|
|[Hyperbolic Graph Topic Modeling Network with Continuously Updated Topic Tree](https://doi.org/10.1145/3580305.3599384)|Delvin Ce Zhang, Rex Ying, Hady W. Lauw||Connectivity across documents often exhibits a hierarchical network structure. Hyperbolic Graph Neural Networks (HGNNs) have shown promise in preserving network hierarchy. However, they do not model the notion of topics, thus document representations lack semantic interpretability. On the other hand, a corpus of documents usually has high variability in degrees of topic specificity. For example, some documents contain general content (e.g., sports), while others focus on specific themes (e.g., basketball and swimming). Topic models indeed model latent topics for semantic interpretability, but most assume a flat topic structure and ignore such semantic hierarchy. Given these two challenges, we propose a Hyperbolic Graph Topic Modeling Network to integrate both network hierarchy across linked documents and semantic hierarchy within texts into a unified HGNN framework. Specifically, we construct a two-layer document graph. Intra- and cross-layer encoding captures network hierarchy. We design a topic tree for text decoding to preserve semantic hierarchy and learn interpretable topics. Supervised and unsupervised experiments verify the effectiveness of our model.|文档之间的连接性通常表现为分层的网络结构。双曲图神经网络(HGNNs)在保持网络层次结构方面显示出了巨大的潜力。然而，它们没有对主题的概念进行建模，因此文档表示缺乏语义可解释性。另一方面，一个文档语料库通常在主题特异性程度上具有很高的可变性。例如，一些文档包含一般内容(如体育) ，而另一些文档关注特定的主题(如篮球和游泳)。话题模型确实为语义可解释性建立了潜在话题模型，但大多数模型采用平面话题结构，忽略了这种语义层次结构。鉴于这两个挑战，我们提出了一个双曲图主题建模网络，将跨链接文档的网络层次结构和文本中的语义层次结构集成到一个统一的 HGNN 框架中。具体来说，我们构造了一个两层文档图。层内和层间编码捕获网络层次结构。我们设计了一个文本解码的主题树，以保留语义层次，学习可解释的主题。有监督和无监督实验验证了模型的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hyperbolic+Graph+Topic+Modeling+Network+with+Continuously+Updated+Topic+Tree)|0|
|[Quantifying Node Importance over Network Structural Stability](https://doi.org/10.1145/3580305.3599480)|Fan Zhang, Qingyuan Linghu, Jiadong Xie, Kai Wang, Xuemin Lin, Wenjie Zhang|Guangzhou University; Chinese University of Hong Kong; Shanghai Jiao Tong University; University of New South Wales|Quantifying node importance on engagement dynamics is critical to support network stability. We can motivate or retain the users in a social platform according to their importance s.t. the network is more sustainable. Existing studies validate that the coreness of a node is the "best practice" on network topology to estimate the engagement of the node. In this paper, the importance of a node is the effect on the engagement of other nodes when its engagement is strengthened or weakened. Specifically, the importance of a node is quantified via two novel concepts: the anchor power to measure the engagement effect of node strengthening (i.e., the overall coreness gain) and the collapse power to measure the engagement effect of node weakening (i.e., the overall coreness loss). We find the computation of the two concepts can be naturally integrated into a shell component-based framework, and propose a unified static algorithm to compute both the anchored and collapsed followers. For evolving networks, efficient maintenance techniques are designed to update the follower sets of each node, which is faster than redoing the static algorithm by around 3 orders of magnitude. Extensive experiments on real-life data demonstrate the effectiveness of our model and the efficiency of our algorithms.|网络参与动态中节点重要性的量化是保证网络稳定性的关键。我们可以根据用户的重要性在社交平台上激励或留住他们。网络更具可持续性。现有的研究证实，节点的核心是评估节点参与程度的“最佳网络拓扑”。在本文中，节点的重要性是指当节点的参与程度加强或减弱时对其他节点参与程度的影响。具体来说，节点的重要性是通过两个新概念来量化的: 锚权衡量节点强化的接合效应(即整体核心增益)和崩溃权衡量节点弱化的接合效应(即整体核心损失)。我们发现这两个概念的计算可以自然地集成到一个基于壳组件的框架中，并提出了一个统一的静态算法来计算锚定和折叠跟随者。对于不断发展的网络，有效的维护技术被设计用于更新每个节点的跟随者集，这比重做静态算法快约3数量级。对实际数据的大量实验证明了模型的有效性和算法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Quantifying+Node+Importance+over+Network+Structural+Stability)|0|
|[HiMacMic: Hierarchical Multi-Agent Deep Reinforcement Learning with Dynamic Asynchronous Macro Strategy](https://doi.org/10.1145/3580305.3599379)|Hancheng Zhang, Guozheng Li, Chi Harold Liu, Guoren Wang, Jian Tang||Multi-agent deep reinforcement learning (MADRL) has been widely used in many scenarios such as robotics and game AI. However, existing methods mainly focus on the optimization of agents' micro policies without considering the macro strategy. As a result, they cannot perform well in complex or sparse reward scenarios like the StarCraft Multi-Agent Challenge (SMAC) and Google Research Football (GRF). To this end, we propose a hierarchical MADRL framework called "HiMacMic" with dynamic asynchronous macro strategy. Spatially, HiMacMic determines a critical position by using a positional heat map. Temporally, the macro strategy dynamically decides its deadline and updates it asynchronously among agents. We validate HiMacMic in four widely used benchmarks, namely: Overcooked, GRF, SMAC and SMAC-v2 with nine chosen scenarios. Results show that HiMacMic not only converges faster and achieves higher results than ten existing approaches, but also shows its adaptability to different environment settings.|多智能体深度强化学习(madrL)已被广泛应用于机器人和游戏人工智能等许多场景中。然而，现有的方法主要集中在代理人的微观政策的优化，而没有考虑宏观战略。因此，他们不能在复杂或稀疏的奖励情况下表现良好，如星际争霸多代理挑战(SMAC)和谷歌研究足球(GRF)。为此，我们提出了一个具有动态异步宏策略的层次化 MADRL 框架“ HiMacMic”。在空间上，HiMacMic 通过使用位置热图确定关键位置。在时间上，宏策略动态地确定其截止日期，并在代理之间异步地更新它。我们在四个广泛使用的基准测试中验证了 HiMacMic，这四个基准测试是: 过度烹饪、 GRF、 SMAC 和 SMAC-v2，共选择了九个场景。结果表明，HiMacMic 方法不仅收敛速度快，比现有的十种方法取得了更好的效果，而且能够适应不同的环境条件。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HiMacMic:+Hierarchical+Multi-Agent+Deep+Reinforcement+Learning+with+Dynamic+Asynchronous+Macro+Strategy)|0|
|[A Study of Situational Reasoning for Traffic Understanding](https://doi.org/10.1145/3580305.3599246)|Jiarui Zhang, Filip Ilievski, Kaixin Ma, Aravinda Kollaa, Jonathan Francis, Alessandro Oltramari|Bosch; USC/ISI; CMU|Intelligent Traffic Monitoring (ITMo) technologies hold the potential for improving road safety/security and for enabling smart city infrastructure. Understanding traffic situations requires a complex fusion of perceptual information with domain-specific and causal commonsense knowledge. Whereas prior work has provided benchmarks and methods for traffic monitoring, it remains unclear whether models can effectively align these information sources and reason in novel scenarios. To address this assessment gap, we devise three novel text-based tasks for situational reasoning in the traffic domain: i) BDD-QA, which evaluates the ability of Language Models (LMs) to perform situational decision-making, ii) TV-QA, which assesses LMs' abilities to reason about complex event causality, and iii) HDT-QA, which evaluates the ability of models to solve human driving exams. We adopt four knowledge-enhanced methods that have shown generalization capability across language reasoning tasks in prior work, based on natural language inference, commonsense knowledge-graph self-supervision, multi-QA joint training, and dense retrieval of domain information. We associate each method with a relevant knowledge source, including knowledge graphs, relevant benchmarks, and driving manuals. In extensive experiments, we benchmark various knowledge-aware methods against the three datasets, under zero-shot evaluation; we provide in-depth analyses of model performance on data partitions and examine model predictions categorically, to yield useful insights on traffic understanding, given different background knowledge and reasoning strategies.|智能交通监控(ITMo)技术具有改善道路安全/安保和启用智能城市基础设施的潜力。理解交通情况需要将感知信息与特定领域和因果常识知识进行复杂的融合。虽然以前的工作已经为交通监控提供了基准和方法，但模型是否能够在新的场景中有效地调整这些信息来源和推理仍然不清楚。为了解决这一评估差距，我们设计了三个新颖的基于文本的任务，用于交通领域的情景推理: 1) BDD-QA，评估语言模型(LM)执行情景决策的能力; 2) TV-QA，评估 LM 对复杂事件因果关系的推理能力; 3) HDT-QA，评估模型解决人类驾驶考试的能力。基于自然语言推理、常识知识图自我监督、多 QA 联合训练和领域信息的密集检索，采用了四种已经在前人工作中显示出跨语言推理任务泛化能力的知识增强方法。我们将每种方法与相关的知识源联系起来，包括知识图表、相关基准和驱动手册。在广泛的实验中，我们基准的各种知识感知的方法对三个数据集，在零拍评估; 我们提供深入的数据分区模型性能分析和检查模型预测分类，以产生有用的见解交通理解，给予不同的背景知识和推理策略。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Study+of+Situational+Reasoning+for+Traffic+Understanding)|0|
|[MixupExplainer: Generalizing Explanations for Graph Neural Networks with Data Augmentation](https://doi.org/10.1145/3580305.3599435)|Jiaxing Zhang, Dongsheng Luo, Hua Wei|NJIT; Florida International University; New Jersey Institute of Technology|Graph Neural Networks (GNNs) have received increasing attention due to their ability to learn from graph-structured data. However, their predictions are often not interpretable. Post-hoc instance-level explanation methods have been proposed to understand GNN predictions. These methods seek to discover substructures that explain the prediction behavior of a trained GNN. In this paper, we shed light on the existence of the distribution shifting issue in existing methods, which affects explanation quality, particularly in applications on real-life datasets with tight decision boundaries. To address this issue, we introduce a generalized Graph Information Bottleneck (GIB) form that includes a label-independent graph variable, which is equivalent to the vanilla GIB. Driven by the generalized GIB, we propose a graph mixup method, MixupExplainer, with a theoretical guarantee to resolve the distribution shifting issue. We conduct extensive experiments on both synthetic and real-world datasets to validate the effectiveness of our proposed mixup approach over existing approaches. We also provide a detailed analysis of how our proposed approach alleviates the distribution shifting issue.|图形神经网络(GNN)由于具有从图形结构数据中学习的能力而受到越来越多的关注。然而，他们的预测往往是无法解释的。事后实例级解释方法已被提出来理解 GNN 预测。这些方法寻求发现子结构，解释训练 GNN 的预测行为。本文揭示了现有方法中存在的分布移位问题，这种问题会影响解释质量，特别是在决策边界较窄的实际数据集上的应用。为了解决这个问题，我们引入了一个通用的图形信息瓶颈(GIB)表单，其中包含一个与标签无关的图形变量，它等价于普通的 GIB。在广义 GIB 的驱动下，本文提出了一种图的混合方法—— MixupExplainer，从理论上保证了分布移位问题的解决。我们在合成和真实世界的数据集上进行了广泛的实验，以验证我们提出的混合方法对现有方法的有效性。我们还提供了一个详细的分析，我们提出的方法如何减轻分配转移的问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MixupExplainer:+Generalizing+Explanations+for+Graph+Neural+Networks+with+Data+Augmentation)|0|
|[Warpformer: A Multi-scale Modeling Approach for Irregular Clinical Time Series](https://doi.org/10.1145/3580305.3599543)|Jiawen Zhang, Shun Zheng, Wei Cao, Jiang Bian, Jia Li|The Hong Kong University of Science and Technology (Guangzhou); Microsoft Research Asia|Irregularly sampled multivariate time series are ubiquitous in various fields, particularly in healthcare, and exhibit two key characteristics: intra-series irregularity and inter-series discrepancy. Intra-series irregularity refers to the fact that time-series signals are often recorded at irregular intervals, while inter-series discrepancy refers to the significant variability in sampling rates among diverse series. However, recent advances in irregular time series have primarily focused on addressing intra-series irregularity, overlooking the issue of inter-series discrepancy. To bridge this gap, we present Warpformer, a novel approach that fully considers these two characteristics. In a nutshell, Warpformer has several crucial designs, including a specific input representation that explicitly characterizes both intra-series irregularity and inter-series discrepancy, a warping module that adaptively unifies irregular time series in a given scale, and a customized attention module for representation learning. Additionally, we stack multiple warping and attention modules to learn at different scales, producing multi-scale representations that balance coarse-grained and fine-grained signals for downstream tasks. We conduct extensive experiments on widely used datasets and a new large-scale benchmark built from clinical databases. The results demonstrate the superiority of Warpformer over existing state-of-the-art approaches.|不规则采样的多变量时间序列在各个领域，特别是在医疗保健领域普遍存在，并表现出两个关键特征: 序列内部的不规则性和序列间的差异性。序列内不规则性是指时间序列信号经常以不规则的间隔记录，而序列间差异是指不同序列间采样率的显著变异性。然而，近年来不规则时间序列研究的进展主要集中在解决序列内部的不规则性，而忽视了序列间差异的问题。为了弥补这一差距，我们提出了 Warpformer，一个新颖的方法，充分考虑到这两个特点。简而言之，Warpformer 有几个关键的设计，包括一个明确描述序列内不规则性和序列间差异性的特定输入表示，一个自适应地统一给定尺度的不规则时间序列的翘曲模块，以及一个用于表示学习的定制注意模块。此外，我们堆叠多个扭曲和注意力模块，以学习在不同的尺度，产生多尺度表示，平衡粗粒度和细粒度信号的下游任务。我们对广泛使用的数据集进行了广泛的实验，并从临床数据库中建立了一个新的大规模基准。研究结果显示，Warpformer 比现有最先进的方法更具优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Warpformer:+A+Multi-scale+Modeling+Approach+for+Irregular+Clinical+Time+Series)|0|
|[Navigating Alignment for Non-identical Client Class Sets: A Label Name-Anchored Federated Learning Framework](https://doi.org/10.1145/3580305.3599443)|Jiayun Zhang, Xiyuan Zhang, Xinyang Zhang, Dezhi Hong, Rajesh K. Gupta, Jingbo Shang||Traditional federated classification methods, even those designed for non-IID clients, assume that each client annotates its local data with respect to the same universal class set. In this paper, we focus on a more general yet practical setting, non-identical client class sets, where clients focus on their own (different or even non-overlapping) class sets and seek a global model that works for the union of these classes. If one views classification as finding the best match between representations produced by data/label encoder, such heterogeneity in client class sets poses a new significant challenge-local encoders at different clients may operate in different and even independent latent spaces, making it hard to aggregate at the server. We propose a novel framework, FedAlign 1 , to align the latent spaces across clients from both label and data perspectives. From a label perspective, we leverage the expressive natural language class names as a common ground for label encoders to anchor class representations and guide the data encoder learning across clients. From a data perspective, during local training, we regard the global class representations as anchors and leverage the data points that are close/far enough to the anchors of locally-unaware classes to align the data encoders across clients. Our theoretical analysis of the generalization performance and extensive experiments on four real-world datasets of different tasks confirm that FedAlign outperforms various state-of-the-art (non-IID) federated classification methods.|传统的联邦分类方法，甚至是那些为非 IID 客户机设计的方法，都假定每个客户机针对相同的通用类集注释其本地数据。在本文中，我们将重点放在一个更通用但实用的设置，非相同的客户端类集，其中客户端关注他们自己的(不同的，甚至不重叠的)类集，并寻求一个全局模型，为这些类的联合工作。如果将分类视为在数据/标签编码器产生的表示之间找到最佳匹配，那么客户端类集中的这种异质性将带来一个新的重大挑战——不同客户端的本地编码器可能在不同甚至独立的潜在空间中运行，从而很难在服务器上进行聚合。我们提出了一个新的框架，FedAlign 1，从标签和数据的角度对齐客户端之间的潜在空间。从标签的角度来看，我们利用具有表达能力的自然语言类名作为标签编码器的共同基础，来锚定类表示并指导跨客户端的数据编码器学习。从数据的角度来看，在本地培训期间，我们将全局类表示视为锚，并利用与本地无意识类的锚足够近/足够远的数据点来跨客户端对齐数据编码器。我们对四个不同任务的真实世界数据集的泛化性能进行了理论分析和广泛的实验，证实了 FedAlign 优于各种最先进的(非 IID)联邦分类方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Navigating+Alignment+for+Non-identical+Client+Class+Sets:+A+Label+Name-Anchored+Federated+Learning+Framework)|0|
|[DyTed: Disentangled Representation Learning for Discrete-time Dynamic Graph](https://doi.org/10.1145/3580305.3599319)|Kaike Zhang, Qi Cao, Gaolin Fang, Bingbing Xu, Hongjian Zou, Huawei Shen, Xueqi Cheng|Tencent Inc.; Institute of Computing Technology, Chinese Academy of Sciences|Unsupervised representation learning for dynamic graphs has attracted a lot of research attention in recent years. Compared with static graph, the dynamic graph is a comprehensive embodiment of both the intrinsic stable characteristics of nodes and the time-related dynamic preference. However, existing methods generally mix these two types of information into a single representation space, which may lead to poor explanation, less robustness, and a limited ability when applied to different downstream tasks. To solve the above problems, in this paper, we propose a novel disenTangled representation learning framework for discrete-time Dynamic graphs, namely DyTed. We specially design a temporal-clips contrastive learning task together with a structure contrastive learning to effectively identify the time-invariant and time-varying representations respectively. To further enhance the disentanglement of these two types of representation, we propose a disentanglement-aware discriminator under an adversarial learning framework from the perspective of information theory. Extensive experiments on Tencent and five commonly used public datasets demonstrate that DyTed, as a general framework that can be applied to existing methods, achieves state-of-the-art performance on various downstream tasks, as well as be more robust against noise.|动态图的无监督表示学习是近年来研究的热点。与静态图相比，动态图综合体现了节点固有的稳定特性和与时间相关的动态偏好。然而，现有的方法通常将这两种类型的信息混合到一个单一的表示空间中，这可能导致在应用于不同的下游任务时解释性较差、鲁棒性较差和能力有限。为了解决上述问题，本文提出了一种新的离散时间动态图的离散表示学习框架 DyTed。我们特别设计了一个时间片段对比学习任务和一个结构对比学习来有效地识别时变和时不变的表示。为了进一步加强这两种表征的分离，我们从信息论的角度提出了一种基于对抗学习框架的分离识别器。在腾讯和五个常用的公共数据集上进行的大量实验表明，DyTed 作为一个可以应用于现有方法的通用框架，在各种下游任务上实现了最先进的性能，并且对噪音有更强的抵抗能力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DyTed:+Disentangled+Representation+Learning+for+Discrete-time+Dynamic+Graph)|0|
|[Rumor Detection with Diverse Counterfactual Evidence](https://doi.org/10.1145/3580305.3599494)|Kaiwei Zhang, Junchi Yu, Haichao Shi, Jian Liang, XiaoYu Zhang|MAISCRIPAC, Institute of Automation, Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences|The growth in social media has exacerbated the threat of fake news to individuals and communities. This draws increasing attention to developing efficient and timely rumor detection methods. The prevailing approaches resort to graph neural networks (GNNs) to exploit the post-propagation patterns of the rumor-spreading process. However, these methods lack inherent interpretation of rumor detection due to the black-box nature of GNNs. Moreover, these methods suffer from less robust results as they employ all the propagation patterns for rumor detection. In this paper, we address the above issues with the proposed Diverse Counterfactual Evidence framework for Rumor Detection (DCE-RD). Our intuition is to exploit the diverse counterfactual evidence of an event graph to serve as multi-view interpretations, which are further aggregated for robust rumor detection results. Specifically, our method first designs a subgraph generation strategy to efficiently generate different subgraphs of the event graph. We constrain the removal of these subgraphs to cause the change in rumor detection results. Thus, these subgraphs naturally serve as counterfactual evidence for rumor detection. To achieve multi-view interpretation, we design a diversity loss inspired by Determinantal Point Processes (DPP) to encourage diversity among the counterfactual evidence. A GNN-based rumor detection model further aggregates the diverse counterfactual evidence discovered by the proposed DCE-RD to achieve interpretable and robust rumor detection results. Extensive experiments on two real-world datasets show the superior performance of our method. Our code is available at https://github.com/Vicinity111/DCE-RD.|社交媒体的增长加剧了假新闻对个人和社区的威胁。这使得人们越来越关注开发有效和及时的谣言检测方法。流行的方法借助于图形神经网络(GNN)来利用谣言传播过程的后传播模式。然而，由于 GNN 的黑盒特性，这些方法缺乏对谣言检测的内在解释。此外，这些方法的鲁棒性较差，因为它们使用所有的传播模式来检测谣言。在本文中，我们提出了一个用于谣言检测的多元反事实证据框架(DCE-RD)来解决上述问题。我们的直觉是利用一个事件图的不同的反事实证据作为多视图的解释，这些解释被进一步聚合以获得可靠的谣言检测结果。具体地说，我们的方法首先设计一个子图生成策略，以有效地生成事件图的不同子图。我们约束这些子图的删除，以引起谣言检测结果的变化。因此，这些子图自然作为反事实证据的谣言检测。为了实现多视角解释，我们设计了一个多样性损失的决定点过程(DPP)的启发，以鼓励多样性的反事实证据。一个基于 GNN 的谣言检测模型进一步聚合了由 DCE-RD 发现的不同的反事实证据，以获得可解释和鲁棒的谣言检测结果。在两个实际数据集上的大量实验表明了该方法的优越性能。我们的代码可以在 https://github.com/vicinity111/dce-rd 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rumor+Detection+with+Diverse+Counterfactual+Evidence)|0|
|[Local Boosting for Weakly-Supervised Learning](https://doi.org/10.1145/3580305.3599417)|Rongzhi Zhang, Yue Yu, Jiaming Shen, Xiquan Cui, Chao Zhang|Georgia Institute of Technology|Boosting is a commonly used technique to enhance the performance of a set of base models by combining them into a strong ensemble model. Though widely adopted, boosting is typically used in supervised learning where the data is labeled accurately. However, in weakly supervised learning, where most of the data is labeled through weak and noisy sources, it remains nontrivial to design effective boosting approaches. In this work, we show that the standard implementation of the convex combination of base learners can hardly work due to the presence of noisy labels. Instead, we propose $\textit{LocalBoost}$, a novel framework for weakly-supervised boosting. LocalBoost iteratively boosts the ensemble model from two dimensions, i.e., intra-source and inter-source. The intra-source boosting introduces locality to the base learners and enables each base learner to focus on a particular feature regime by training new base learners on granularity-varying error regions. For the inter-source boosting, we leverage a conditional function to indicate the weak source where the sample is more likely to appear. To account for the weak labels, we further design an estimate-then-modify approach to compute the model weights. Experiments on seven datasets show that our method significantly outperforms vanilla boosting methods and other weakly-supervised methods.|提升是一种常用的技术，通过将一组基本模型组合成一个强集成模型来提高它们的性能。虽然被广泛采用，但是在数据被精确标记的监督式学习中，通常使用的是增强技术。然而，在弱监督式学习中，大多数数据是通过弱噪声源标记的，因此设计有效的增强方法仍然不是一件轻而易举的事情。在这项工作中，我们表明，由于存在噪音标签，基础学习者凸组合的标准实现很难奏效。相反，我们提出了 $textit { LocalBoost } $，一个弱监督加密的新框架。LocalBoost 迭代地从两个维度(即源内和源间)增强集成模型。内部源增强将局部性引入到基础学习者中，通过在粒度变化的错误区域上培训新的基础学习者，使每个基础学习者能够关注特定的特征体系。对于源间增强，我们利用一个条件函数来指示示例更可能出现的弱源。为了解决弱标签的问题，我们进一步设计了一种先估计后修正的方法来计算模型权重。在七个数据集上的实验表明，我们的方法明显优于香草提升方法和其他弱监督方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Local+Boosting+for+Weakly-Supervised+Learning)|0|
|[Capacity Constrained Influence Maximization in Social Networks](https://doi.org/10.1145/3580305.3599267)|Shiqi Zhang, Yiqian Huang, Jiachen Sun, Wenqing Lin, Xiaokui Xiao, Bo Tang|Southern University of Science and Technology; Tencent; National University of Singapore|Influence maximization (IM) aims to identify a small number of influential individuals to maximize the information spread and finds applications in various fields. It was first introduced in the context of viral marketing, where a company pays a few influencers to promote the product. However, apart from the cost factor, the capacity of individuals to consume content poses challenges for implementing IM in real-world scenarios. For example, players on online gaming platforms can only interact with a limited number of friends. In addition, we observe that in these scenarios, (i) the initial adopters of promotion are likely to be the friends of influencers rather than the influencers themselves, and (ii) existing IM solutions produce sub-par results with high computational demands. Motivated by these observations, we propose a new IM variant called capacity constrained influence maximization (CIM), which aims to select a limited number of influential friends for each initial adopter such that the promotion can reach more users. To solve CIM effectively, we design two greedy algorithms, MG-Greedy and RR-Greedy, ensuring the $1/2$-approximation ratio. To improve the efficiency, we devise the scalable implementation named RR-OPIM+ with $(1/2-\epsilon)$-approximation and near-linear running time. We extensively evaluate the performance of 9 approaches on 6 real-world networks, and our solutions outperform all competitors in terms of result quality and running time. Additionally, we deploy RR-OPIM+ to online game scenarios, which improves the baseline considerably.|影响最大化(IM)的目标是识别少量有影响力的个体，使信息传播最大化，并在各个领域得到应用。它最初是在病毒式营销的背景下引入的，一家公司付钱给一些有影响力的人来推广产品。然而，除了成本因素之外，个人消费内容的能力对在现实世界中实现 IM 构成了挑战。例如，在线游戏平台上的玩家只能与有限数量的朋友互动。此外，我们观察到，在这些情况下，(i)推广的最初采用者可能是影响者的朋友，而不是影响者本身，以及(ii)现有的 IM 解决方案产生低于平均水平的结果具有高计算需求。基于这些观察结果，我们提出了一个新的 IM 变量，称为容量约束影响最大化(CIM) ，其目的是为每个初始采用者选择有限数量的有影响力的朋友，使推广可以达到更多的用户。为了有效地解决 CIM 问题，我们设计了两个贪婪算法: MG 贪婪算法和 RR 贪婪算法，保证了 $1/2 $- 逼近比。为了提高效率，我们设计了具有 $(1/2-epsilon) $- 近似和近线性运行时间的可扩展实现 RR-OPIM + 。我们广泛评估了9种方法在6个真实网络上的性能，我们的解决方案在结果质量和运行时间方面优于所有竞争对手。此外，我们将 RR-OPIM + 部署到在线游戏场景中，这大大改善了基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Capacity+Constrained+Influence+Maximization+in+Social+Networks)|0|
|[AdaProp: Learning Adaptive Propagation for Graph Neural Network based Knowledge Graph Reasoning](https://doi.org/10.1145/3580305.3599404)|Yongqi Zhang, Zhanke Zhou, Quanming Yao, Xiaowen Chu, Bo Han||Due to the popularity of Graph Neural Networks (GNNs), various GNN-based methods have been designed to reason on knowledge graphs (KGs). An important design component of GNN-based KG reasoning methods is called the propagation path, which contains a set of involved entities in each propagation step. Existing methods use hand-designed propagation paths, ignoring the correlation between the entities and the query relation. In addition, the number of involved entities will explosively grow at larger propagation steps. In this work, we are motivated to learn an adaptive propagation path in order to filter out irrelevant entities while preserving promising targets. First, we design an incremental sampling mechanism where the nearby targets and layer-wise connections can be preserved with linear complexity. Second, we design a learning-based sampling distribution to identify the semantically related entities. Extensive experiments show that our method is powerful, efficient and semantic-aware. The code is available at https://github.com/LARS-research/AdaProp.|由于图神经网络(GNN)的普及，各种基于 GNN 的方法被设计用于知识图的推理。基于 GNN 的 KG 推理方法的一个重要设计组成部分称为传播路径，它在每个传播步骤中包含一组相关的实体。现有方法使用手工设计的传播路径，忽略了实体和查询关系之间的相关性。此外，涉及的实体数量将在更大的传播步骤中爆炸性增长。在这项工作中，我们被激励去学习一个自适应的传播路径，以滤除不相关的实体，同时保留有希望的目标。首先，我们设计了一个增量式采样机制，使得附近的目标和层状连接可以保持线性复杂度。其次，我们设计了一个基于学习的抽样分布来识别语义相关的实体。广泛的实验表明，该方法是强大的，有效的和语义感知。密码可在 https://github.com/lars-research/adaprop 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AdaProp:+Learning+Adaptive+Propagation+for+Graph+Neural+Network+based+Knowledge+Graph+Reasoning)|0|
|[Weakly Supervised Multi-Label Classification of Full-Text Scientific Papers](https://doi.org/10.1145/3580305.3599544)|Yu Zhang, Bowen Jin, Xiusi Chen, Yanzhen Shen, Yunyi Zhang, Yu Meng, Jiawei Han|University of California, Los Angeles; University of Illinois at Urbana-Champaign|Instead of relying on human-annotated training samples to build a classifier, weakly supervised scientific paper classification aims to classify papers only using category descriptions (e.g., category names, category-indicative keywords). Existing studies on weakly supervised paper classification are less concerned with two challenges: (1) Papers should be classified into not only coarse-grained research topics but also fine-grained themes, and potentially into multiple themes, given a large and fine-grained label space; and (2) full text should be utilized to complement the paper title and abstract for classification. Moreover, instead of viewing the entire paper as a long linear sequence, one should exploit the structural information such as citation links across papers and the hierarchy of sections and paragraphs in each paper. To tackle these challenges, in this study, we propose FUTEX, a framework that uses the cross-paper network structure and the in-paper hierarchy structure to classify full-text scientific papers under weak supervision. A network-aware contrastive fine-tuning module and a hierarchy-aware aggregation module are designed to leverage the two types of structural signals, respectively. Experiments on two benchmark datasets demonstrate that FUTEX significantly outperforms competitive baselines and is on par with fully supervised classifiers that use 1,000 to 60,000 ground-truth training samples.|弱监督的科学论文分类不依赖于人工注释的训练样本来建立分类器，而是旨在仅仅使用类别描述(例如，类别名称，类别指示性关键词)来对论文进行分类。现有的弱监督论文分类研究较少关注两个挑战: (1)论文不仅应该分为粗粒度的研究课题，而且还应该分为细粒度的主题，并可能分为多个主题，给定一个大而细粒度的标签空间; (2)应该利用全文来补充论文标题和分类摘要。此外，我们不应该把整篇论文看成一个长长的线性序列，而是应该利用结构信息，例如论文之间的引文链接以及每篇论文中章节和段落的层次结构。为了应对这些挑战，本研究提出了 FUTEX 框架，该框架利用跨论文网络结构和论文层次结构在弱监督下对全文科技论文进行分类。设计了网络感知的对比微调模块和层次感知的聚合模块，分别利用这两种结构信号。在两个基准数据集上的实验表明，FUTEX 的性能明显优于竞争性基线，并且与使用1,000至60,000个地面真相训练样本的全监督分类器相当。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Weakly+Supervised+Multi-Label+Classification+of+Full-Text+Scientific+Papers)|0|
|[DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock Trend Forecasting](https://doi.org/10.1145/3580305.3599315)|Lifan Zhao, Shuming Kong, Yanyan Shen|Shanghai Jiao Tong University|Stock trend forecasting is a fundamental task of quantitative investment where precise predictions of price trends are indispensable. As an online service, stock data continuously arrive over time. It is practical and efficient to incrementally update the forecast model with the latest data which may reveal some new patterns recurring in the future stock market. However, incremental learning for stock trend forecasting still remains under-explored due to the challenge of distribution shifts (a.k.a. concept drifts). With the stock market dynamically evolving, the distribution of future data can slightly or significantly differ from incremental data, hindering the effectiveness of incremental updates. To address this challenge, we propose DoubleAdapt, an end-to-end framework with two adapters, which can effectively adapt the data and the model to mitigate the effects of distribution shifts. Our key insight is to automatically learn how to adapt stock data into a locally stationary distribution in favor of profitable updates. Complemented by data adaptation, we can confidently adapt the model parameters under mitigated distribution shifts. We cast each incremental learning task as a meta-learning task and automatically optimize the adapters for desirable data adaptation and parameter initialization. Experiments on real-world stock datasets demonstrate that DoubleAdapt achieves state-of-the-art predictive performance and shows considerable efficiency.|股票趋势预测是量化投资的一项基本任务，对价格趋势的精确预测是必不可少的。作为一种在线服务，股票数据随着时间的推移不断到达。利用最新数据对预测模型进行增量更新，可以揭示未来股票市场中出现的一些新的模式，是一种切实有效的方法。然而，由于分布变化的挑战(也称为概念漂移) ，股市趋势预测的在线机机器学习仍未得到充分探索。随着股票市场的动态演化，未来数据的分布可能会与增量数据略有或显著不同，从而阻碍了增量更新的有效性。为了应对这一挑战，我们提出了 DoubleAdapt，一个带有两个适配器的端到端框架，它可以有效地适应数据和模型，以减轻分布转移的影响。我们的关键洞察力是自动学习如何将股票数据调整到一个本地平稳的分布中，以支持有利可图的更新。在数据自适应的补充下，我们可以在分布偏移减小的情况下自信地调整模型参数。我们将每个在线机机器学习任务转换为元学习任务，并自动优化适配器以适应需要的数据调整和参数初始化。在真实股票数据集上的实验表明，DoubleAdapt 实现了最先进的预测性能，并显示出相当高的效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DoubleAdapt:+A+Meta-learning+Approach+to+Incremental+Learning+for+Stock+Trend+Forecasting)|0|
|[Spatial Clustering Regression of Count Value Data via Bayesian Mixture of Finite Mixtures](https://doi.org/10.1145/3580305.3599509)|Peng Zhao, HouCheng Yang, Dipak K. Dey, Guanyu Hu||Investigating relationships between response variables and covariates in areas such as environmental science, geoscience, and public health is an important endeavor. Based on a Bayesian mixture of finite mixtures model, we present a novel spatially clustered coefficients regression model for count value data. The proposed method detects the spatial homogeneity of the Poisson regression coefficients. A Markov random field constrained mixture of finite mixtures prior provides a regularized estimator of the number of clusters of regression coefficients with geographical neighborhood information. As a by-product, we also provide the theoretical properties of our proposed method when the Markov random field is exchangeable. An efficient Markov chain Monte Carlo algorithm is developed by using the multivariate log gamma distribution as a base distribution. Simulation studies are carried out to examine the empirical performance of the proposed method. Additionally, we analyze Georgia's premature death data as an illustration of the effectiveness of our approach. The supplementary materials are provided on GitHub at https://github.com/pengzhaostat/MLG_MFM.|在环境科学、地球科学和公共卫生等领域，研究响应变量和协变量之间的关系是一项重要的工作。在有限混合贝叶斯混合模型的基础上，提出了一种新的计数值数据空间聚类系数回归模型。该方法检测泊松回归系数的空间均匀性。先前的有限混合的马尔可夫网络约束混合提供了带有地理邻域信息的回归系数簇数目的正则化估计。作为一个副产品，我们还提供了我们建议的方法的理论特性，当马尔可夫网络是可交换的。利用多变量对数马尔科夫蒙特卡洛作为基本分布，开发了一种有效的伽玛分布分析算法。通过仿真研究验证了该方法的经验性能。此外，我们还分析了乔治亚州的过早死亡数据，以说明我们的方法的有效性。补充材料可在 https://GitHub.com/pengzhaostat/mlg_mfm 的 gitHub 上找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Spatial+Clustering+Regression+of+Count+Value+Data+via+Bayesian+Mixture+of+Finite+Mixtures)|0|
|[Skill Disentanglement for Imitation Learning from Suboptimal Demonstrations](https://doi.org/10.1145/3580305.3599506)|Tianxiang Zhao, Wenchao Yu, Suhang Wang, Lu Wang, Xiang Zhang, Yuncong Chen, Yanchi Liu, Wei Cheng, Haifeng Chen|NEC-Labs America; East China Normal University; the Pennsylvania State University|Imitation learning has achieved great success in many sequential decision-making tasks, in which a neural agent is learned by imitating collected human demonstrations. However, existing algorithms typically require a large number of high-quality demonstrations that are difficult and expensive to collect. Usually, a trade-off needs to be made between demonstration quality and quantity in practice. Targeting this problem, in this work we consider the imitation of sub-optimal demonstrations, with both a small clean demonstration set and a large noisy set. Some pioneering works have been proposed, but they suffer from many limitations, e.g., assuming a demonstration to be of the same optimality throughout time steps and failing to provide any interpretation w.r.t knowledge learned from the noisy set. Addressing these problems, we propose {\method} by evaluating and imitating at the sub-demonstration level, encoding action primitives of varying quality into different skills. Concretely, {\method} consists of a high-level controller to discover skills and a skill-conditioned module to capture action-taking policies, and is trained following a two-phase pipeline by first discovering skills with all demonstrations and then adapting the controller to only the clean set. A mutual-information-based regularization and a dynamic sub-demonstration optimality estimator are designed to promote disentanglement in the skill space. Extensive experiments are conducted over two gym environments and a real-world healthcare dataset to demonstrate the superiority of {\method} in learning from sub-optimal demonstrations and its improved interpretability by examining learned skills.|模仿学习在许多顺序决策任务中取得了巨大的成功，其中通过模仿收集的人类演示来学习神经代理。然而，现有的算法通常需要大量高质量的演示，这些演示很难收集并且代价高昂。通常，在实践中需要在演示的质量和数量之间进行权衡。针对这个问题，本文考虑了同时具有小型清洁演示集和大型噪声演示集的次优演示的模拟问题。一些开创性的工作已经被提出，但他们受到许多限制，例如，假设一个演示是相同的最佳整个时间步骤和未能提供任何解释 W.r.t 知识从嘈杂的集合。针对这些问题，我们提出了{方法} ，通过评估和子演示级模拟，编码不同质量的动作原语到不同的技能。具体来说，{ method }由一个发现技能的高级控制器和一个捕获行动策略的技能条件模块组成，并按照两阶段流水线进行训练，首先发现所有演示的技能，然后调整控制器使其仅适用于清洁集合。设计了一个基于互信息的正则化算法和一个动态子演示最优估计器来促进技能空间的解缠。在两个健身环境和一个真实世界的医疗保健数据集上进行了广泛的实验，以证明{ method }在从次优演示中学习方面的优越性，以及通过检查学习技能改进的可解释性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Skill+Disentanglement+for+Imitation+Learning+from+Suboptimal+Demonstrations)|0|
|[GraphGLOW: Universal and Generalizable Structure Learning for Graph Neural Networks](https://doi.org/10.1145/3580305.3599373)|Wentao Zhao, Qitian Wu, Chenxiao Yang, Junchi Yan|Shanghai Jiao Tong University|Graph structure learning is a well-established problem that aims at optimizing graph structures adaptive to specific graph datasets to help message passing neural networks (i.e., GNNs) to yield effective and robust node embeddings. However, the common limitation of existing models lies in the underlying \textit{closed-world assumption}: the testing graph is the same as the training graph. This premise requires independently training the structure learning model from scratch for each graph dataset, which leads to prohibitive computation costs and potential risks for serious over-fitting. To mitigate these issues, this paper explores a new direction that moves forward to learn a universal structure learning model that can generalize across graph datasets in an open world. We first introduce the mathematical definition of this novel problem setting, and describe the model formulation from a probabilistic data-generative aspect. Then we devise a general framework that coordinates a single graph-shared structure learner and multiple graph-specific GNNs to capture the generalizable patterns of optimal message-passing topology across datasets. The well-trained structure learner can directly produce adaptive structures for unseen target graphs without any fine-tuning. Across diverse datasets and various challenging cross-graph generalization protocols, our experiments show that even without training on target graphs, the proposed model i) significantly outperforms expressive GNNs trained on input (non-optimized) topology, and ii) surprisingly performs on par with state-of-the-art models that independently optimize adaptive structures for specific target graphs, with notably orders-of-magnitude acceleration for training on the target graph.|图结构学习是一个公认的问题，旨在优化图结构自适应特定的图数据集，以帮助信息传递神经网络(即 GNN)产生有效和鲁棒的节点嵌入。然而，现有模型的普遍局限性在于其基本的文本{封闭世界假设} : 测试图与训练图相同。这一前提要求从头开始独立训练每个图形数据集的结构学习模型，从而导致严重过拟合的计算成本和潜在风险。为了缓解这些问题，本文探索了一个新的方向，向前推进学习一个通用的结构学习模型，可以推广跨图形数据集在一个开放的世界。我们首先介绍了这种新的问题设置的数学定义，并从概率数据生成的角度描述了模型公式。然后我们设计了一个通用的框架来协调一个单一的图共享结构学习器和多个特定于图的 GNN 来捕获跨数据集的最佳消息传递拓扑的可推广模式。训练有素的结构学习者可以直接为看不见的目标图生成自适应结构，而不需要进行任何微调。在不同的数据集和各种具有挑战性的跨图泛化协议中，我们的实验表明，即使没有对目标图进行训练，所提出的模型 i)显着优于在输入(未优化)拓扑上训练的表达性 GNN，并且 ii)令人惊讶地表现得与独立优化特定目标图的自适应结构的最新模型相当，特别是在目标图上进行训练的数量级加速。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphGLOW:+Universal+and+Generalizable+Structure+Learning+for+Graph+Neural+Networks)|0|
|[Generative Causal Interpretation Model for Spatio-Temporal Representation Learning](https://doi.org/10.1145/3580305.3599363)|Yu Zhao, Pan Deng, Junting Liu, Xiaofeng Jia, Jianwei Zhang||Learning, interpreting, and predicting from complex and high-dimensional spatio-temporal data is a natural ability of humans and other intelligent agents, and one of the most important and difficult challenges of AI. Although objects may present different observed phenomena under different situations, their causal mechanism and generation rules are stable and invariant. Different from most existing studies that focus on dynamic correlation, we explore the latent causal structure and mechanism of causal descriptors in the spatio-temporal dimension at the microscopic level, thus revealing the generation principle of observation. In this paper, we regard the causal mechanism as a spatio-temporal causal process modulated by non-stationary exogenous variables. To this end, we propose a theoretically-grounded Generative Causal Interpretation Model (GCIM), which infers explanatory-capable microscopic causal descriptors from observational data via spatio-temporal causal representations. The core of GCIM is to estimate the prior distribution of causal descriptors by using the spatio-temporal causal structure and transition process under the constraints of identifiable conditions, thus extending the Variational Auto Encoder (VAE). Furthermore, our method is able to automatically capture domain information from observations to model non-stationarity. We further analyze the model identifiability, showing that the proposed model learned from observations recovers the true one up to a certain degree. Experiments on synthetic and real-world datasets show that GCIM can successfully identify latent causal descriptors and structures, and accurately predict future data.|从复杂和高维的时空数据中学习、解释和预测是人类和其他智能体的一种天赋能力，也是人工智能面临的最重要和最困难的挑战之一。虽然物体在不同的情况下可能呈现不同的观察现象，但其因果机制和生成规则是稳定和不变的。与大多数以动态相关为研究重点的研究不同，我们在微观层面上探讨了因果描述符在时空维度上的潜在因果结构和机制，从而揭示了观察的生成原理。本文认为因果机制是由非平稳外生变量调节的时空因果过程。为此，我们提出了一个基于理论的生成性因果解释模型(GCIM) ，该模型通过时空因果表征从观测数据中推导出具有解释能力的微观因果描述符。GCIM 的核心是在可识别条件的约束下，利用时空因果结构和转换过程来估计因果描述符的先验分布，从而扩展了变分自动编码器(VAE)。此外，我们的方法能够自动捕获领域信息从观测到模型的非平稳性。进一步分析了模型的可辨识性，结果表明，从观测数据中学习得到的模型在一定程度上恢复了真实模型。在合成和真实数据集上的实验表明，GCIM 能够成功地识别潜在的因果描述符和结构，并准确地预测未来数据。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+Causal+Interpretation+Model+for+Spatio-Temporal+Representation+Learning)|0|
|[Maintaining the Status Quo: Capturing Invariant Relations for OOD Spatiotemporal Learning](https://doi.org/10.1145/3580305.3599421)|Zhengyang Zhou, Qihe Huang, Kuo Yang, Kun Wang, Xu Wang, Yudong Zhang, Yuxuan Liang, Yang Wang||Spatiotemporal (ST) learning has become a crucial technique for urban digitalization. Due to expansions and dynamics of cities, current spatiotemporal models are inclined to suffer distribution shifts between training and testing sets, leading to the OOD delimma. However, few studies focus on such OOD problem in temporal regressions, let alone spatiotemporal learning. Spatiotemporal data usually reveals segment-level heterogeneity within periodicity and complex spatial dependencies, posing challenges to invariance extraction. In this paper, we find that ST relations make sense for generalization and devise a Causal ST learning framework, CauSTG, which enables invariant relation transferred to OOD scenarios. Specifically, we take temporal steps as environments, and transform spatial-temporal relations into learnable parameters. To tackle heterogeneity in periodicity, we partition temporal steps into sub-environments by identifying distinctive trend patterns, enabling re-organized samples trained separately. To extract invariance within ST observations, we propose a spatiotemporal consistency learner and a hierarchical invariance explorer to jointly filter out stable relations. Our spatiotemporal learner quantifies bi-directional spatial consistency and extracts disentangled seasonal-trend patterns via trainable parameters. Further, the hierarchical invariance explorer constructs variation-based filter to achieve both local and global invariances. Experiments reveal that CauSTG can increase at most 10.26% performance against best baselines, and visualized invariant relations can well interpret the physical rationales. The appendix and codes can be available in our Github repository.|时空学习已成为城市数字化的关键技术。由于城市的扩张和动态性，目前的时空模型倾向于在训练集和测试集之间发生分布转移，导致面向对象设计的困境。然而，很少有研究关注时间回归中的面向对象问题，更不用说时空学习了。时空数据通常揭示了周期性和复杂空间依赖性中的分段级异质性，对不变性提取提出了挑战。在本文中，我们发现 ST 关系对于推广是有意义的，并且设计了一个因果 ST 学习框架 CauSTG，它能够将不变关系转移到面向对象的场景中。具体来说，我们以时间步骤作为环境，将时空关系转换为可学习的参数。为了解决周期性的异质性问题，我们通过识别不同的趋势模式，将时间步骤划分为子环境，使重组样本能够分别训练。为了提取 ST 观测值中的不变性，我们提出了一种时空一致性学习器和分层不变性探索器来联合滤除稳定关系。我们的时空学习者量化双向空间一致性，并通过可训练的参数提取分离的季节趋势模式。此外，分层不变性浏览器构造基于变异的滤波器来实现局部和全局不变性。实验表明，对于最佳基线，CauSTG 最多可以提高10.26% 的性能，可视化不变关系可以很好地解释物理原理。附录和代码可以在我们的 Github 存储库中找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Maintaining+the+Status+Quo:+Capturing+Invariant+Relations+for+OOD+Spatiotemporal+Learning)|0|
|[Dual-view Molecular Pre-training](https://doi.org/10.1145/3580305.3599317)|Jinhua Zhu, Yingce Xia, Lijun Wu, Shufang Xie, Wengang Zhou, Tao Qin, Houqiang Li, TieYan Liu||Molecular pre-training, which is about to learn an effective representation for molecules on large amount of data, has attracted substantial attention in cheminformatics and bioinformatics. A molecule can be viewed as either a graph (where atoms are connected by bonds) or a SMILES sequence (where depth-first-search is applied to the molecular graph with specific rules). The Transformer and graph neural networks (GNN) are two representative methods to deal with the sequential data and the graphic data, which can globally and locally model the molecules respectively and are supposed to be complementary. In this work, we propose to leverage both representations and design a new pre-training algorithm, dual-view molecule pre-training (briefly, DVMP), that can effectively combine the strengths of both types of molecule representations. DVMP has a Transformer branch and a GNN branch, and the two branches are pre-trained to maintain the semantic consistency of molecules. After pre-training, we can use either the Transformer branch (this one is recommended according to empirical results), the GNN branch, or both for downstream tasks. DVMP is tested on 11 molecular property prediction tasks and outperforms strong baselines. Furthermore, we test DVMP on three retrosynthesis tasks and it achieves state-of-the-art results. Our code is released at https://github.com/microsoft/DVMP.|分子预训练即将在大量的数据中学习分子的有效表示，已经引起了化学信息学和生物信息学的广泛关注。一个分子可以被看作是一个图(其中原子通过键连接)或一个 SMILES 序列(其中深度优先搜索应用于具体规则的分子图)。变压器神经网络和图形神经网络是处理序列数据和图形数据的两种有代表性的方法，它们可以分别对分子进行全局和局部建模，具有互补性。在这项工作中，我们建议利用这两种表示和设计一个新的预训练算法，双视图分子预训练(简称 DVMP) ，可以有效地结合两种类型的分子表示的优势。DVMP 具有一个 Transformer 分支和一个 GNN 分支，这两个分支被预先训练以保持分子的语义一致性。在预训练之后，我们可以使用 Transformer 分支(根据经验结果推荐使用这个分支)、 GNN 分支，或者两者都用于下游任务。DVMP 在11个分子性质预测任务上进行了测试，其性能优于强基线。此外，我们在三个回溯合成任务上进行了 DVMP 测试，并取得了最先进的结果。我们的代码在 https://github.com/microsoft/dvmp 发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dual-view+Molecular+Pre-training)|0|
|[On Structural Expressive Power of Graph Transformers](https://doi.org/10.1145/3580305.3599451)|Wenhao Zhu, Tianyu Wen, Guojie Song, Liang Wang, Bo Zheng|Peking University; Alibaba Group|Graph Transformer has recently received wide attention in the research community with its outstanding performance, yet its structural expressive power has not been well analyzed. Inspired by the connections between Weisfeiler-Lehman (WL) graph isomorphism test and graph neural network (GNN), we introduce \textbf{SEG-WL test} (\textbf{S}tructural \textbf{E}ncoding enhanced \textbf{G}lobal \textbf{W}eisfeiler-\textbf{L}ehman test), a generalized graph isomorphism test algorithm as a powerful theoretical tool for exploring the structural discriminative power of graph Transformers. We theoretically prove that the SEG-WL test is an expressivity upper bound on a wide range of graph Transformers, and the representational power of SEG-WL test can be approximated by a simple Transformer network arbitrarily under certain conditions. With the SEG-WL test, we show how graph Transformers' expressive power is determined by the design of structural encodings, and present conditions that make the expressivity of graph Transformers beyond WL test and GNNs. Moreover, motivated by the popular shortest path distance encoding, we follow the theory-oriented principles and develop a provably stronger structural encoding method, Shortest Path Induced Subgraph (\textit{SPIS}) encoding. Our theoretical findings provide a novel and practical paradigm for investigating the expressive power of graph Transformers, and extensive synthetic and real-world experiments empirically verify the strengths of our proposed methods.|近年来，图形变换器以其优异的性能受到了研究界的广泛关注，但其结构表达能力一直没有得到很好的分析。受 Weisfeiler-Lehman (WL)图同构检验和图神经网络(GNN)之间联系的启发，我们引入了一种广义图同构检验算法 textbf { SEG-WL test }(textbf { S }结构 textbf { E } ncoding 增强 textbf { G } global textbf { W } eisfeiler-textbf { L } ehman test) ，作为探索图变换器结构判别能力的有力理论工具。从理论上证明了 SEG-WL 测试是一个广泛的图形变压器的表示上界，并且 SEG-WL 测试的表示功率可以在一定条件下用一个简单的变压器网络任意逼近。通过 SEG-WL 测试，我们展示了图形变换器的表达能力是如何通过结构编码的设计来决定的，并提出了使图形变换器的表达能力超越 WL 测试和 GNN 的条件。此外，在目前流行的最短路径距离编码的推动下，本文遵循理论导向的原则，提出了一种可证明性更强的结构化编码方法——最短路径诱导子图(text tit { SPIS })编码。我们的理论研究结果为研究图形变换器的表达能力提供了一个新颖而实用的范式，广泛的综合实验和现实世界的实验验证了我们提出的方法的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Structural+Expressive+Power+of+Graph+Transformers)|0|
|[WinGNN: Dynamic Graph Neural Networks with Random Gradient Aggregation Window](https://doi.org/10.1145/3580305.3599551)|Yifan Zhu, Fangpeng Cong, Dan Zhang, Wenwen Gong, Qika Lin, Wenzheng Feng, Yuxiao Dong, Jie Tang||Modeling the dynamics into graph neural networks (GNNs) contributes to the understanding of evolution in dynamic graphs, which helps optimize temporal-spatial representations for real-world dynamic network problems. Empirically, dynamic GNN embedding requires additional temporal encoders, which inevitably introduces additional learning parameters to make dynamic GNNs oversized and inefficient. Furthermore, previous dynamic GNN models are under the same fixed temporal term, which causes the short-temporal optimum. To address these issues, we propose the WinGNN framework to model dynamic graphs, which is realized by a simple GNN model with the meta-learning strategy and a novel mechanism of random gradient aggregation. WinGNN calculates the frame-wise loss of the current snapshot and passes the loss gradient to the next to model graph dynamics without temporal encoders. Then it introduces the randomized sliding-window to acquire the window-aware gradienton consecutive snapshots, and the calculated two types of gradient are aggregated to update the GNN, thereby reducing the parameter size and improving the robustness. Experiments on six public datasets show the advantage of our WinGNN compared with existing baselines, where it has reached the optimum in twenty-two out of twenty-four performance metrics.|将动态过程建模为图形神经网络(GNN)有助于理解动态图的演化过程，从而有助于优化现实世界动态网络问题的时空表示。经验表明，动态 GNN 嵌入需要额外的时间编码器，这就不可避免地引入了额外的学习参数来使动态 GNN 过大和效率低下。此外，以往的动态 GNN 模型在相同的固定时间项下，导致短时最优。为了解决这些问题，我们提出了一个 WinGNN 框架来建立动态图模型，该框架通过一个简单的 GNN 模型，采用元学习策略和一种新的随机梯度聚集机制来实现。WinGNN 计算当前快照的帧损失，并将损失梯度传递给不使用时态编码器的下一个模型图动态。然后引入随机滑动窗口获取连续快照的窗口感知梯度，并将计算得到的两种梯度集合起来更新 GNN，从而减小参数大小，提高鲁棒性。在六个公共数据集上的实验显示了我们的 WinGNN 相对于现有基线的优势，在24个性能指标中有22个达到了最优。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=WinGNN:+Dynamic+Graph+Neural+Networks+with+Random+Gradient+Aggregation+Window)|0|
|[DyGen: Learning from Noisy Labels via Dynamics-Enhanced Generative Modeling](https://doi.org/10.1145/3580305.3599318)|Yuchen Zhuang, Yue Yu, Lingkai Kong, Xiang Chen, Chao Zhang|Georgia Institute of Technology; Adobe Research|Learning from noisy labels is a challenge that arises in many real-world applications where training data can contain incorrect or corrupted labels. When fine-tuning language models with noisy labels, models can easily overfit the label noise, leading to decreased performance. Most existing methods for learning from noisy labels use static input features for denoising, but these methods are limited by the information they can provide on true label distributions and can result in biased or incorrect predictions. In this work, we propose the Dynamics-Enhanced Generative Model (DyGen), which uses dynamic patterns in the embedding space during the fine-tuning process of language models to improve noisy label predictions. DyGen uses the variational auto-encoding framework to infer the posterior distributions of true labels from noisy labels and training dynamics. Additionally, a co-regularization mechanism is used to minimize the impact of potentially noisy labels and priors. DyGen demonstrates an average accuracy improvement of 3.10% on two synthetic noise datasets and 1.48% on three real-world noise datasets compared to the previous state-of-the-art. Extensive experiments and analyses show the effectiveness of each component in DyGen. Our code is available for reproducibility on GitHub.|从嘈杂的标签中学习是一个挑战，它出现在许多实际应用中，其中训练数据可能包含不正确或损坏的标签。当对带有噪声标签的语言模型进行微调时，模型很容易过分适应标签噪声，从而导致性能下降。大多数现有的从噪声标签中学习的方法使用静态输入特征进行去噪，但是这些方法受到它们能够提供的关于真实标签分布的信息的限制，并且可能导致有偏或不正确的预测。在这项工作中，我们提出了动态增强生成模型(dyGen) ，它在语言模型的微调过程中使用嵌入空间中的动态模式来改善有噪声的标签预测。DyGen 利用变分自动编码框架从噪声标签和训练动力学中推断真实标签的后验分布。此外，共正则化机制被用来最小化潜在的噪声标签和先验的影响。DyGen 在两个合成噪声数据集和三个实际噪声数据集上的平均准确率比之前的最新技术提高了3.10% 和1.48% 。大量的实验和分析表明 DyGen 中各组分的有效性。我们的代码可以在 GitHub 上重现。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DyGen:+Learning+from+Noisy+Labels+via+Dynamics-Enhanced+Generative+Modeling)|0|
|[Learning to Solve Grouped 2D Bin Packing Problems in the Manufacturing Industry](https://doi.org/10.1145/3580305.3599860)|Wenxuan Ao, Guozhen Zhang, Yong Li, Depeng Jin||The two-dimensional bin packing problem (2DBP) is a critical optimization problem in the furniture production and glass cutting industries, where the objective is to cut smaller-sized items from a minimum number of large standard-sized raw materials. In practice, factories manufacture hundreds of customer orders (sets of items) every day, and to relieve pressure in management, a common practice is to group the orders into batches for production, ensuring that items from one order are in the same batch instead of scattered across the production line. In this work, we formulate this problem as the grouped 2D bin packing problem, a bi-level problem where the upper level partitions orders into groups and the lower level solves 2DBP for items in each group. The main challenges are (1) the coupled optimization of upper and lower levels and (2) the high computational efficiency required for practical application. To tackle these challenges, we propose an iteration-based hierarchical reinforcement learning framework, which can learn to solve the optimization problem in a data-driven way and provide fast online performance after offline training. Extensive experiments demonstrate that our method not only achieves the best performance compared to all baselines but is also robust to changes in dataset distribution and problem constraints. Finally, we deployed our method in the ARROW Home factory in China, resulting in a 4.1% reduction in raw material costs. We have released the source code and datasets to facilitate future research.|二维集装优化是家具生产和玻璃切割行业的一个关键最佳化问题，其目标是从最少数量的大型标准尺寸原材料中切割出较小尺寸的物品。在实践中，工厂每天生产数百个客户订单(成套项目) ，为了缓解管理压力，通常的做法是将订单分批生产，确保一个订单的项目是在同一批，而不是分散在生产线上。在这项工作中，我们将这个问题表述为分组的二维集装优化，这是一个双层次的问题，其中上层次的分区被分成组，下层次的分区为每个组中的项目解决2dBP 问题。主要的挑战是(1)上层和下层的耦合优化和(2)实际应用所需的高计算效率。为了应对这些挑战，我们提出了一个基于迭代的分层强化学习框架，它可以学习以数据驱动的方式解决最佳化问题，并在离线培训后提供快速的在线性能。大量的实验表明，与所有基线相比，该方法不仅获得了最佳的性能，而且对数据集分布和问题约束的变化具有鲁棒性。最后，我们在中国的 ARROW Home 工厂部署了我们的方法，使原材料成本降低了4.1% 。我们已经发布了源代码和数据集，以方便未来的研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Solve+Grouped+2D+Bin+Packing+Problems+in+the+Manufacturing+Industry)|0|
|[Fusing Multimodal Signals on Hyper-complex Space for Extreme Abstractive Text Summarization (TL;DR) of Scientific Contents](https://doi.org/10.1145/3580305.3599830)|Yash Kumar Atri, Vikram Goyal, Tanmoy Chakraborty|IIIT Delhi|The realm of scientific text summarization has experienced remarkable progress due to the availability of annotated brief summaries and ample data. However, the utilization of multiple input modalities, such as videos and audio, has yet to be thoroughly explored. At present, scientific multimodal-input-based text summarization systems tend to employ longer target summaries like abstracts, leading to an underwhelming performance in the task of text summarization. In this paper, we deal with a novel task of extreme abstractive text summarization (aka TL;DR generation) by leveraging multiple input modalities. To this end, we introduce mTLDR, a first-of-its-kind dataset for the aforementioned task, comprising videos, audio, and text, along with both author-composed summaries and expert-annotated summaries. The mTLDR dataset accompanies a total of 4,182 instances collected from various academic conference proceedings, such as ICLR, ACL, and CVPR. Subsequently, we present mTLDRgen, an encoder-decoder-based model that employs a novel dual-fused hyper-complex Transformer combined with a Wasserstein Riemannian Encoder Transformer, to dexterously capture the intricacies between different modalities in a hyper-complex latent geometric space. The hyper-complex Transformer captures the intrinsic properties between the modalities, while the Wasserstein Riemannian Encoder Transformer captures the latent structure of the modalities in the latent space geometry, thereby enabling the model to produce diverse sentences. mTLDRgen outperforms 20 baselines on mTLDR as well as another non-scientific dataset (How2) across three Rouge-based evaluation measures. Furthermore, based on the qualitative metrics, BERTScore and FEQA, and human evaluations, we demonstrate that the summaries generated by mTLDRgen are fluent and congruent to the original source material.|科学文本摘要领域由于有了简短的注释摘要和充足的数据，已经取得了显著的进展。然而，利用多种输入模式，如视频和音频，还有待深入探讨。目前，基于多模态输入的科学文摘系统往往使用较长的目标摘要，如摘要，导致在文本摘要的任务中表现不佳。在本文中，我们处理了一个新的任务，极端抽象的文本摘要(又名 TL; DR 生成)利用多种输入模式。为此，我们介绍了 mTLDR，这是用于上述任务的首个同类数据集，包括视频，音频和文本，以及作者撰写的摘要和专家注释的摘要。MTLDR 数据集包含从各种学术会议记录(如 ICLR、 ACL 和 CVPR)收集的总共4,182个实例。随后，我们提出了一个基于编码器-解码器的模型 mTLDRgen，该模型采用了一种新型的双融合超复杂变压器结合 Wasserstein 黎曼编码器变压器，灵巧地捕捉超复杂潜在几何空间中不同模式之间的复杂性。超复杂的变压器捕捉模式之间的内在属性，而 Wasserstein 黎曼编码变压器捕捉潜在的空间几何模式的潜在结构，从而使模型能够产生不同的句子。MTLDRgen 在三个基于 Rouge 的评估指标上优于 mTLDR 的20个基线以及另一个非科学数据集(How2)。此外，基于定性指标、 BERTScore 和 FEQA 以及人工评估，我们证明 mTLDRgen 生成的摘要与原始资料是流畅和一致的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fusing+Multimodal+Signals+on+Hyper-complex+Space+for+Extreme+Abstractive+Text+Summarization+(TL;DR)+of+Scientific+Contents)|0|
|[Web-Scale Academic Name Disambiguation: The WhoIsWho Benchmark, Leaderboard, and Toolkit](https://doi.org/10.1145/3580305.3599930)|Bo Chen, Jing Zhang, Fanjin Zhang, Tianyi Han, Yuqing Cheng, Xiaoyan Li, Yuxiao Dong, Jie Tang|Renmin University of China; Tsinghua University; Zhipu.AI|Name disambiguation -- a fundamental problem in online academic systems -- is now facing greater challenges with the increasing growth of research papers. For example, on AMiner, an online academic search platform, about 10% of names own more than 100 authors. Such real-world hard cases cannot be fully addressed by existing research efforts, because of the small-scale or low-quality datasets that they use to build algorithms. The development of effective algorithms is further hampered by a variety of tasks and evaluation protocols designed on top of diverse datasets. To this end, we present WhoIsWho owning, a large-scale benchmark with over 1,000,000 papers built using an interactive annotation process, a regular leaderboard with comprehensive tasks, and an easy-to-use toolkit encapsulating the entire pipeline as well as the most powerful features and baseline models for tackling the tasks. Our developed strong baseline has already been deployed online in the AMiner system to enable daily arXiv paper assignments. The documentation and regular leaderboards are publicly available at http://whoiswho.biendata.xyz/.|名称消歧是在线学术系统中的一个基本问题，随着研究论文的增加，名称消歧正面临着更大的挑战。例如，在在线学术搜索平台 AMiner 上，大约10% 的名字拥有100多位作者。现有的研究工作无法完全解决这些现实世界中的难题，因为他们使用小规模或低质量的数据集来构建算法。在不同数据集上设计的各种任务和评估协议进一步阻碍了有效算法的开发。为此，我们展示了 whoIsWho Owning，这是一个大规模的基准测试，使用交互式注释过程构建了超过100万篇论文，一个包含全面任务的常规排行榜，以及一个封装了整个流水线以及最强大的功能和处理任务的基线模型的易于使用的工具包。我们开发的强大的基线已经部署在 AMiner 系统的网上，以支持每日 arXiv 文件分配。文件和常规排行榜可在 http://whoiswho.biendata.xyz/公开查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Web-Scale+Academic+Name+Disambiguation:+The+WhoIsWho+Benchmark,+Leaderboard,+and+Toolkit)|0|
|[FS-REAL: Towards Real-World Cross-Device Federated Learning](https://doi.org/10.1145/3580305.3599829)|Daoyuan Chen, Dawei Gao, Yuexiang Xie, Xuchen Pan, Zitao Li, Yaliang Li, Bolin Ding, Jingren Zhou|Alibaba Group|Federated Learning (FL) aims to train high-quality models in collaboration with distributed clients while not uploading their local data, which attracts increasing attention in both academia and industry. However, there is still a considerable gap between the flourishing FL research and real-world scenarios, mainly caused by the characteristics of heterogeneous devices and its scales. Most existing works conduct evaluations with homogeneous devices, which are mismatched with the diversity and variability of heterogeneous devices in real-world scenarios. Moreover, it is challenging to conduct research and development at scale with heterogeneous devices due to limited resources and complex software stacks. These two key factors are important yet underexplored in FL research as they directly impact the FL training dynamics and final performance, making the effectiveness and usability of FL algorithms unclear. To bridge the gap, in this paper, we propose an efficient and scalable prototyping system for real-world cross-device FL, FS-Real. It supports heterogeneous device runtime, contains parallelism and robustness enhanced FL server, and provides implementations and extensibility for advanced FL utility features such as personalization, communication compression and asynchronous aggregation. To demonstrate the usability and efficiency of FS-Real, we conduct extensive experiments with various device distributions, quantify and analyze the effect of the heterogeneous device and various scales, and further provide insights and open discussions about real-world FL scenarios. Our system is released to help to pave the way for further real-world FL research and broad applications involving diverse devices and scales.|联邦学习(Federated Learning，FL)旨在与分布式客户协作培训高质量的模型，同时不上传他们的本地数据，这引起了学术界和业界越来越多的关注。然而，由于异构器件及其规模的特点，目前外语研究的蓬勃发展与实际情景之间仍然存在很大的差距。现有的大多数工作都是使用同质设备进行评估，这与现实场景中异质设备的多样性和可变性不匹配。此外，由于资源有限和软件堆栈复杂，使用异构设备进行大规模的研究和开发具有挑战性。这两个关键因素直接影响外语学习的动态性和最终的性能，使得外语学习算法的有效性和可用性尚不清楚，因此在外语学习研究中这两个关键因素是重要的，但尚未得到充分的研究。为了弥补这一差距，本文提出了一个高效、可扩展的实际跨设备 FL、 FS-Real 原型系统。它支持异构设备运行时，包含并行性和鲁棒性增强的 FL 服务器，并为高级 FL 实用功能(如个性化、通信压缩和异步聚合)提供实现和可扩展性。为了证明 FS-Real 的可用性和效率，我们对不同的设备分布进行了广泛的实验，量化和分析了异构设备和不同尺度的影响，并进一步提供了关于真实世界 FL 场景的见解和公开讨论。我们的系统发布，以帮助为进一步的现实世界的外语研究和广泛的应用涉及不同的设备和规模铺平道路。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FS-REAL:+Towards+Real-World+Cross-Device+Federated+Learning)|0|
|[A Data-driven Region Generation Framework for Spatiotemporal Transportation Service Management](https://doi.org/10.1145/3580305.3599760)|Liyue Chen, Jiangyi Fang, Zhe Yu, Yongxin Tong, Shaosheng Cao, Leye Wang|Peking University; DiDi Chuxing; Beihang University; Huazhong University of Science and Technology|MAUP (modifiable areal unit problem) is a fundamental problem for spatial data management and analysis. As an instantiation of MAUP in online transportation platforms, region generation (i.e., specifying the areal unit for service operations) is the first and vital step for supporting spatiotemporal transportation services such as ride-sharing and freight transport. Most existing region generation methods are manually specified (e.g., fixed-size grids), suffering from poor spatial semantic meaning and inflexibility to meet service operation requirements. In this paper, we propose RegionGen, a data-driven region generation framework that can specify regions with key characteristics (e.g., good spatial semantic meaning and predictability) by modeling region generation as a multi-objective optimization problem. First, to obtain good spatial semantic meaning, RegionGen segments the whole city into atomic spatial elements based on road networks and obstacles (e.g., rivers). Then, it clusters the atomic spatial elements into regions by maximizing various operation characteristics, which is formulated as a multi-objective optimization problem. For this optimization problem, we propose a multi-objective co-optimization algorithm. Extensive experiments verify that RegionGen can generate more suitable regions than traditional methods for spatiotemporal service management.|MAUP (可调整地区单元问题)是空间数据管理和分析的一个基本问题。作为在线交通平台 MAUP 的一个实例，区域生成(即指定服务运营的面积单元)是支持时空交通服务(如拼车和货运)的第一步，也是至关重要的一步。大多数现有的区域生成方法都是手动指定的(例如，固定大小的网格) ，这些方法存在空间语义不足和不能满足服务操作需求的缺陷。在本文中，我们提出了 RegionGen，这是一个数据驱动的区域生成框架，通过将区域生成建模为一个多目标最佳化问题，可以指定具有关键特征的区域(例如，良好的空间语义和可预测性)。首先，为了获得良好的空间语义，RegionGen 根据道路网络和障碍物(如河流)将整个城市划分为原子空间元素。然后，它通过最大化各种操作特性将原子空间元素聚类到各个区域，这是一个多目标最佳化问题。针对这个最佳化问题，我们提出了一个多目标协同优化算法。大量的实验证明，与传统的时空服务管理方法相比，RegionGen 可以生成更多合适的区域。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Data-driven+Region+Generation+Framework+for+Spatiotemporal+Transportation+Service+Management)|0|
|[Binary Classifier Evaluation on Unlabeled Segments using Inverse Distance Weighting with Distance Learning](https://doi.org/10.1145/3580305.3599781)|Xu Chen, Katerina Marazopoulou, Wesley Lee, Christine Agarwal, Jason Sukumaran, Aude Hofleitner||Binary classification models are ubiquitous, and reliably measuring their performance is critical for their proper usage. Ideally, the performance of supervised models is measured using high-quality labeled datasets that are sufficiently large and representative of the population. However, obtaining labels for all segments of the population can be difficult, and model performance typically varies across different segments of the population (e.g., in different countries). In this work, we present a novel methodology to estimate the performance of a binary classifier in segments of the population where labels are unavailable. The main idea is that if two segments are "similar,'' then the performance of the classifier in these two segments would also be "similar.'' Specifically, we define a way to measure similarity between segments, and propose a statistical model that describes the performance of the model in unlabeled segments as a function of the performance in labeled segments. With extensive numerical experiments on synthetic and real-world datasets, we demonstrate that the proposed method substantially improves over existing methods in both estimation accuracy and computational efficiency. We also showcase the application of our method on the Instagram Adult Classifier to improve the geographic coverage and usability of the model.|二进制分类模型是无处不在的，可靠地度量它们的性能对于它们的正确使用至关重要。理想情况下，监督模型的性能是通过使用高质量的标记数据集来衡量的，这些数据集是足够大和代表人口的。然而，获得所有人群的标签可能是困难的，模型的性能通常因人群的不同部分而异(例如，在不同的国家)。在这项工作中，我们提出了一个新的方法来估计一个二进制分类器的性能在部分人口的标签是不可用的。其主要思想是，如果两个段是“相似的”，那么分类器在这两个段中的性能也将是“相似的”具体来说，我们定义了一种测量片段之间相似性的方法，并提出了一个统计模型来描述模型在未标记片段中的性能作为标记片段性能的函数。通过在合成数据集和真实数据集上的大量数值实验，我们证明了该方法在估计精度和计算效率方面都比现有方法有很大的提高。我们还展示了我们的方法在 Instagram 成人分类器上的应用，以提高模型的地理覆盖率和可用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Binary+Classifier+Evaluation+on+Unlabeled+Segments+using+Inverse+Distance+Weighting+with+Distance+Learning)|0|
|[Evolve Path Tracer: Early Detection of Malicious Addresses in Cryptocurrency](https://doi.org/10.1145/3580305.3599817)|Ling Cheng, Feida Zhu, Yong Wang, Ruicheng Liang, Huiwen Liu|Hefei University of Technology; Singapore Management University|With the ever-increasing boom of Cryptocurrency, detecting fraudulent behaviors and associated malicious addresses draws significant research effort. However, most existing studies still rely on the full history features or full-fledged address transaction networks, thus cannot meet the requirements of early malicious address detection, which is urgent but seldom discussed by existing studies. To detect fraud behaviors of malicious addresses in the early stage, we present Evolve Path Tracer, which consists of Evolve Path Encoder LSTM, Evolve Path Graph GCN, and Hierarchical Survival Predictor. Specifically, in addition to the general address features, we propose asset transfer paths and corresponding path graphs to characterize early transaction patterns. Further, since the transaction patterns are changing rapidly during the early stage, we propose Evolve Path Encoder LSTM and Evolve Path Graph GCN to encode asset transfer path and path graph under an evolving structure setting. Hierarchical Survival Predictor then predicts addresses' labels with nice scalability and faster prediction speed. We investigate the effectiveness and versatility of Evolve Path Tracer on three real-world illicit bitcoin datasets. Our experimental results demonstrate that Evolve Path Tracer outperforms the state-of-the-art methods. Extensive scalability experiments demonstrate the model's adaptivity under a dynamic prediction setting.|随着加密货币的不断繁荣，检测欺诈行为和相关的恶意地址需要大量的研究工作。然而，现有的研究大多依赖于完整的历史特征或成熟的地址交易网络，不能满足早期恶意地址检测的要求，这是现有研究中迫切但很少讨论的问题。为了检测恶意地址早期的欺诈行为，提出了由进化路径编码器 LSTM、进化路径图 GCN 和分层生存预测器组成的进化路径跟踪器。具体来说，除了一般的地址特征之外，我们还提出了资产转移路径和相应的路径图来描述早期交易模式。进一步，由于交易模式在早期阶段变化迅速，我们提出了演化路径编码器 LSTM 和演化路径图 GCN 来编码资产转移路径和路径图在一个演化的结构设置。层次生存预测器预测地址标签具有良好的可伸缩性和更快的预测速度。我们研究了演化路径跟踪在三个真实世界非法比特币数据集上的有效性和通用性。实验结果表明，演化路径跟踪方法的性能优于最先进的方法。大量的可扩展性实验证明了该模型在动态预测环境下的自适应性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Evolve+Path+Tracer:+Early+Detection+of+Malicious+Addresses+in+Cryptocurrency)|0|
|[Conditional Neural ODE Processes for Individual Disease Progression Forecasting: A Case Study on COVID-19](https://doi.org/10.1145/3580305.3599792)|Ting Dang, Jing Han, Tong Xia, Erika Bondareva, Chloë SiegeleBrown, Jagmohan Chauhan, Andreas Grammenos, Dimitris Spathis, Pietro Cicuta, Cecilia Mascolo||Time series forecasting, as one of the fundamental machine learning areas, has attracted tremendous attentions over recent years. The solutions have evolved from statistical machine learning (ML) methods to deep learning techniques. One emerging sub-field of time series forecasting is individual disease progression forecasting, e.g., predicting individuals' disease development over a few days (e.g., deteriorating trends, recovery speed) based on few past observations. Despite the promises in the existing ML techniques, a variety of unique challenges emerge for disease progression forecasting, such as irregularly-sampled time series, data sparsity, and individual heterogeneity in disease progression. To tackle these challenges, we propose novel Conditional Neural Ordinary Differential Equations Processes (CNDPs), and validate it in a COVID-19 disease progression forecasting task using audio data. CNDPs allow for irregularly-sampled time series modelling, enable accurate forecasting with sparse past observations, and achieve individual-level progression forecasting. CNDPs show strong performance with an Unweighted Average Recall (UAR) of 78.1%, outperforming a variety of commonly used Recurrent Neural Networks based models. With the proposed label-enhancing mechanism (i.e., including the initial health status as input) and the customised individual-level loss, CNDPs further boost the performance reaching a UAR of 93.6%. Additional analysis also reveals the model's capability in tracking individual-specific recovery trend, implying the potential usage of the model for remote disease progression monitoring. In general, CNDPs pave new pathways for time series forecasting, and provide considerable advantages for disease progression monitoring.|时间序列预测作为机器学习的基础领域之一，近年来受到了广泛的关注。解决方案已经从统计机器学习(ML)方法发展到深度学习技术。时间序列预测的一个新兴子领域是个体疾病进展预测，例如，根据过去的一些观察，预测个体在几天内的疾病发展(例如，恶化趋势，恢复速度)。尽管现有的机器学习技术有希望，但疾病进展预测仍面临各种独特的挑战，例如不规则采样的时间序列、数据稀疏和疾病进展中的个体异质性。为了应对这些挑战，我们提出了新的条件神经常微分方程过程(CNDP) ，并使用音频数据在2019冠状病毒疾病疾病进展预测任务中验证它。国家数据中心可以进行不规则采样的时间序列建模，能够在过去很少观测的情况下进行准确的预测，并实现个人层面的进展预测。CNDP 表现出强大的性能，未加权平均召回率(UAR)为78.1% ，优于各种常用的基于回归神经网络的模型。透过建议的标签强化机制(即包括最初的健康状况作为输入)和个别层面的损失，「建筑噪音感应强的地方」的表现进一步提升至93.6% 。额外的分析还揭示了该模型跟踪个体特异性恢复趋势的能力，这意味着该模型可能用于远程疾病进展监测。一般而言，CNDP 为时间序列预测铺平了新的道路，为疾病进展监测提供了可观的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Conditional+Neural+ODE+Processes+for+Individual+Disease+Progression+Forecasting:+A+Case+Study+on+COVID-19)|0|
|[Time-to-Event Modeling with Hypernetwork based Hawkes Process](https://doi.org/10.1145/3580305.3599912)|Manisha Dubey, P. K. Srijith, Maunendra Sankar Desarkar|IIT Hyderabad, Kandi, India|Many real-world applications are associated with collection of events with timestamps, known as time-to-event data. Earthquake occurrences, social networks, and user activity logs can be represented as a sequence of discrete events observed in continuous time. Temporal point process serves as an essential tool for modeling such time-to-event data in continuous time space. Despite having massive amounts of event sequence data from various domains like social media, healthcare etc., real world application of temporal point process faces two major challenges: 1) it is not generalizable to predict events from unseen event sequences in dynamic environment 2) they are not capable of thriving in continually evolving environment with minimal supervision while retaining previously learnt knowledge. To tackle these issues, we propose HyperHawkes, a hypernetwork based temporal point process framework which is capable of modeling time of event occurrence for unseen sequences and consequently, zero-shot learning for time-to-event modeling. We also develop a hypernetwork based continually learning temporal point process for continuous modeling of time-to-event sequences with minimal forgetting. HyperHawkes augments the temporal point process with zero-shot modeling and continual learning capabilities. We demonstrate the application of the proposed framework through our experiments on real-world datasets. Our results show the efficacy of the proposed approach in terms of predicting future events under zero-shot regime for unseen event sequences. We also show that the proposed model is able to learn the time-to-event sequences continually while retaining information from previous event sequences, mitigating catastrophic forgetting in neural temporal point process.|许多实际应用程序与具有时间戳的事件集合相关联，称为时间到事件数据。地震发生、社交网络和用户活动日志可以表示为在连续时间内观察到的一系列离散事件。时间点过程是在连续时间空间中对时间-事件数据进行建模的重要工具。尽管有大量来自不同领域的事件序列数据，如社交媒体、医疗保健等，但时间点过程在现实世界中的应用面临两大挑战: 1)在动态环境中无法从看不见的事件序列中预测事件2)它们不能在持续发展的环境中蓬勃发展，同时保留以前学到的知识。为了解决这些问题，我们提出了基于超网络的时间点过程框架 HyperHawkes，它能够对未知序列的事件发生时间进行建模，从而对时间-事件建模进行零点学习。我们还开发了一个基于超网络的连续学习时间点过程的时间到事件序列的连续建模与最小遗忘。HyperHawkes 通过零拍建模和持续学习能力增强了时间点过程。通过在真实数据集上的实验，验证了该框架的应用。我们的研究结果显示了提出的方法在预测未来事件在未知事件序列的零拍摄制度方面的有效性。我们还表明，该模型能够不断地学习时间-事件序列，同时保留以前事件序列的信息，减轻了神经时间点过程中的灾难性遗忘。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Time-to-Event+Modeling+with+Hypernetwork+based+Hawkes+Process)|0|
|[Discovering Novel Biological Traits From Images Using Phylogeny-Guided Neural Networks](https://doi.org/10.1145/3580305.3599808)|Mohannad Elhamod, Mridul Khurana, Harish Babu Manogaran, Josef C. Uyeda, Meghan A. Balk, Wasila M. Dahdul, Yasin Bakis, Henry L. Bart Jr., Paula M. Mabee, Hilmar Lapp, James P. Balhoff, Caleb Charpentier, David Carlyn, WeiLun Chao, Charles V. Stewart, Daniel I. Rubenstein, Tanya Y. BergerWolf, Anuj Karpatne|Duke University; Tulane University; Virginia Tech; University of North Carolina at Chapel Hill; Princeton University; The Ohio State University; Battelle; University of California, Irvine; Rensselaer Polytechnic Institute|Discovering evolutionary traits that are heritable across species on the tree of life (also referred to as a phylogenetic tree) is of great interest to biologists to understand how organisms diversify and evolve. However, the measurement of traits is often a subjective and labor-intensive process, making trait discovery a highly label-scarce problem. We present a novel approach for discovering evolutionary traits directly from images without relying on trait labels. Our proposed approach, Phylo-NN, encodes the image of an organism into a sequence of quantized feature vectors -- or codes -- where different segments of the sequence capture evolutionary signals at varying ancestry levels in the phylogeny. We demonstrate the effectiveness of our approach in producing biologically meaningful results in a number of downstream tasks including species image generation and species-to-species image translation, using fish species as a target example.|在生命之树(也称为系统发生树)上发现可以跨物种遗传的进化特征，对于生物学家理解有机体如何多样化和进化具有重大意义。然而，特征的测量往往是一个主观的和劳动密集型的过程，使特征发现一个高度标签稀缺的问题。我们提出了一种新的方法，直接从图像中发现进化特征，而不依赖于特征标签。我们提出的方法，Phylo-NN，将一个生物体的图像编码成一系列量化的特征向量——或编码——其中序列的不同片段捕获系统发育中不同祖先水平的进化信号。我们证明了我们的方法在许多下游任务中产生具有生物学意义的结果的有效性，包括物种图像生成和物种之间的图像翻译，使用鱼种作为目标例子。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Discovering+Novel+Biological+Traits+From+Images+Using+Phylogeny-Guided+Neural+Networks)|0|
|[Demystifying Fraudulent Transactions and Illicit Nodes in the Bitcoin Network for Financial Forensics](https://doi.org/10.1145/3580305.3599803)|Youssef Elmougy, Ling Liu|Georgia Institute of Technology|Blockchain provides the unique and accountable channel for financial forensics by mining its open and immutable transaction data. A recent surge has been witnessed by training machine learning models with cryptocurrency transaction data for anomaly detection, such as money laundering and other fraudulent activities. This paper presents a holistic applied data science approach to fraud detection in the Bitcoin network with two original contributions. First, we contribute the Elliptic++ dataset, which extends the Elliptic transaction dataset to include over 822k Bitcoin wallet addresses (nodes), each with 56 features, and 1.27M temporal interactions. This enables both the detection of fraudulent transactions and the detection of illicit addresses (actors) in the Bitcoin network by leveraging four types of graph data: (i) the transaction-to-transaction graph, representing the money flow in the Bitcoin network, (ii) the address-to-address interaction graph, capturing the types of transaction flows between Bitcoin addresses, (iii) the address-transaction graph, representing the bi-directional money flow between addresses and transactions (BTC flow from input address to one or more transactions and BTC flow from a transaction to one or more output addresses), and (iv) the user entity graph, capturing clusters of Bitcoin addresses representing unique Bitcoin users. Second, we perform fraud detection tasks on all four graphs by using diverse machine learning algorithms. We show that adding enhanced features from the address-to-address and the address-transaction graphs not only assists in effectively detecting both illicit transactions and illicit addresses, but also assists in gaining in-depth understanding of the root cause of money laundering vulnerabilities in cryptocurrency transactions and the strategies for fraud detection and prevention. Released at github.com/git-disl/EllipticPlusPlus.|区块链通过挖掘其公开和不可变的交易数据，为金融取证提供了独特和可靠的渠道。最近，利用加密货币交易数据对机器学习模型进行训练的异常检测激增，例如洗钱和其它欺诈活动。本文提出了一种比特币网络欺诈检测的整体应用数据科学方法。首先，我们贡献了椭圆 + + 数据集，它扩展了椭圆交易数据集，包括超过822k 比特币钱包地址(节点) ，每个节点具有56个特征和1.27 M 时间交互。利用四种图形数据，既可侦测欺诈交易，又可侦测比特币网络中的非法地址(参与者) : (i)代表比特币网络资金流的交易至交易图; (ii)地址至地址互动图，捕捉比特币地址之间的交易流; (iii)地址交易图，捕捉地址与交易之间的双向资金流(输入地址至一项或多项交易的比特币流，以及一项交易至一项或多项输出地址的比特币流) ; 以及(iv)用户实体图，捕捉代表独特比特币用户的比特币地址群。其次，我们使用不同的机器学习算法在所有四个图上执行欺诈检测任务。我们发现，加入地址对地址和地址交易图表的增强功能，不但有助有效侦测非法交易和非法地址，而且有助深入了解加密货币交易中出现洗钱漏洞的根本原因，以及侦测和防止欺诈的策略。Github.com/git-disl/ellipticplusplus 释放。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Demystifying+Fraudulent+Transactions+and+Illicit+Nodes+in+the+Bitcoin+Network+for+Financial+Forensics)|0|
|[RecruitPro: A Pretrained Language Model with Skill-Aware Prompt Learning for Intelligent Recruitment](https://doi.org/10.1145/3580305.3599894)|Chuyu Fang, Chuan Qin, Qi Zhang, Kaichun Yao, Jingshuai Zhang, Hengshu Zhu, Fuzhen Zhuang, Hui Xiong||Recent years have witnessed the rapid development of machine-learning-based intelligent recruitment services. Along this line, a large number of emerging models have been proposed, achieving remarkable performance in various tasks, such as person-job fit, job classification and salary prediction. However, existing studies are usually domain/task specific, which significantly hinders the adaptation of models for different industries/tasks with limited training data. To this end, in this paper, we propose a novel skill-aware prompt-based pretraining framework, namely RecruitPro, which is capable of learning unified representations on the recruitment data and adapting for various downstream tasks of intelligent recruitment services. To be specific, we first present a contextualized embedding model that is pretrained on a large-scale recruitment dataset. Then, we construct 13 downstream benchmark tasks that are representative in the recruitment process. Along this line, we propose a skill-aware prompt learning module to enhance the adaptability of the pretrained model on downstream tasks. This module includes a skill-related prompt, which is designed to explore key semantic information (i.e., skills) from recruitment text, and a task-related prompt, which is designed to bridge the gap between the pretrained model and different downstream tasks. Moreover, we propose a strategy for extracting potential skills to further improve the performance of our skill-aware prompt learning module. Finally, extensive experiments have clearly demonstrated the effectiveness of RecruitPro. In addition, a case study has been presented to discuss the privacy preserving issue of our RecruitPro.|近年来，基于机器学习的智能招聘服务迅速发展。沿着这条路线，大量新兴的模型被提出，在人与岗位匹配、岗位分类和薪酬预测等各种任务中都取得了显著的成绩。然而，现有的研究通常是针对特定领域/任务的，这显著地阻碍了模型在有限的培训数据下适应不同的行业/任务。为此，本文提出了一种新颖的基于技能感知的、基于提示的预培训框架，它能够学习招聘数据的统一表示，并适应智能招聘服务的各种下游任务。具体来说，我们首先提出了一个上下文嵌入模型，预先训练的大规模招聘数据集。然后，我们构建了13个在招聘过程中具有代表性的下游基准任务。在此基础上，我们提出了一个技能感知的快速学习模块，以增强预训练模型对下游任务的适应性。本单元包括一个与技能相关的提示，旨在探索招聘文本中的关键语义信息(即技能) ，以及一个与任务相关的提示，旨在弥合预先训练的模型和不同的下游任务之间的差距。此外，我们还提出了一个提取潜在技能的策略，以进一步提高我们的技能感知快速学习模块的性能。最后，广泛的实验清楚地证明了该方法的有效性。此外，我们还提出了一个案例研究，以讨论保护隐私的问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RecruitPro:+A+Pretrained+Language+Model+with+Skill-Aware+Prompt+Learning+for+Intelligent+Recruitment)|0|
|[A Lightweight, Efficient and Explainable-by-Design Convolutional Neural Network for Internet Traffic Classification](https://doi.org/10.1145/3580305.3599762)|Kevin Fauvel, Fuxing Chen, Dario Rossi|Huawei Technologies Co., Ltd|Traffic classification, i.e. the identification of the type of applications flowing in a network, is a strategic task for numerous activities (e.g., intrusion detection, routing). This task faces some critical challenges that current deep learning approaches do not address. The design of current approaches do not take into consideration the fact that networking hardware (e.g., routers) often runs with limited computational resources. Further, they do not meet the need for faithful explainability highlighted by regulatory bodies. Finally, these traffic classifiers are evaluated on small datasets which fail to reflect the diversity of applications in real-world settings. Therefore, this paper introduces a new Lightweight, Efficient and eXplainable-by-design convolutional neural network (LEXNet) for Internet traffic classification, which relies on a new residual block (for lightweight and efficiency purposes) and prototype layer (for explainability). Based on a commercial-grade dataset, our evaluation shows that LEXNet succeeds to maintain the same accuracy as the best performing state-of-the-art neural network, while providing the additional features previously mentioned. Moreover, we illustrate the explainability feature of our approach, which stems from the communication of detected application prototypes to the end-user, and we highlight the faithfulness of LEXNet explanations through a comparison with post hoc methods.|流量分类，即识别在网络中流动的应用程序的类型，是许多活动(例如，入侵检测，路由)的战略任务。这项任务面临一些当前深度学习方法无法解决的关键挑战。当前方法的设计没有考虑到这样一个事实，即网络硬件(例如路由器)往往在有限的计算资源下运行。此外，它们不符合监管机构强调的忠实可解释性的需要。最后，对这些流量分类器在小数据集上进行评估，这些数据集不能反映现实环境中应用程序的多样性。因此，本文介绍了一个新的轻量级、高效和可解释的设计互联网流量分类卷积神经网络(LEXNet) ，它依赖于一个新的剩余块(用于轻量级和高效目的)和原型层(用于可解释性)。基于一个商业级数据集，我们的评估表明，LEXNet 成功地保持了与最佳性能的最先进的神经网络相同的准确性，同时提供了前面提到的额外功能。此外，我们说明了该方法的可解释性特征，这源于检测到的应用程序原型与终端用户的通信，并通过与事后方法的比较，突出了 LEXNet 解释的可信性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Lightweight,+Efficient+and+Explainable-by-Design+Convolutional+Neural+Network+for+Internet+Traffic+Classification)|0|
|[ILRoute: A Graph-based Imitation Learning Method to Unveil Riders' Routing Strategies in Food Delivery Service](https://doi.org/10.1145/3580305.3599844)|Tao Feng, Huan Yan, Huandong Wang, Wenzhen Huang, Yuyang Han, Hongsen Liao, Jinghua Hao, Yong Li|Tsinghua University|Pick-up and delivery (PD) services such as online food ordering are playing an increasingly important role in serving people's daily demands. Accurate PD route prediction (PDRP) is important for service providers to efficiently schedule riders to improve service quality. It is crucial to model the decision-making process behind the route choice of riders for PDRP. Recent years have witnessed the success of utilizing imitation learning (IL) to model user decision-making process. Therefore, we propose to deploy an IL framework to solve the PDRP problem. However, there still exist three main challenges: (1) the rider's route decision is affected by multi-source and heterogeneous features and the complex relationships among these features make it hard to explore how they influence the rider's route decision-making; (2) the large route decision-making space make it easy to explore and predict unreasonable routes; (3) the rider's personalized preference is important in modeling the route decision-making process but cannot be fully explored. To tackle the above challenges, we propose ILRoute, a Graph-based imitation learning method for PDRP. ILRoute utilizes a multi-graph neural network (multi-GNN) to extract the multi-source and heterogeneous features and model their complex relationships. To address the large route decision-making space, ILRoute introduces a mobility regularity-aware constraint as prior route choice knowledge to reduce the exploration route decision-making space. To model the personalized preferences of the rider, ILRoute utilizes a personalized constraint mechanism to enhance the personalization of the rider's route decision-making process. Offline experiments conducted on three real-world datasets and online comparisons demonstrate the superiority of our proposed model.|提货和送货(PD)服务，如在线订购食品正在发挥越来越重要的作用，以满足人们的日常需求。准确的局部放电路径预测(PDRP)是服务提供商有效调度乘客以提高服务质量的重要手段。对 PDRP 中乘客路径选择背后的决策过程进行建模至关重要。近年来，利用模仿学习对用户决策过程进行建模已经取得了一定的成功。因此，我们建议部署一个 IL 框架来解决 PDRP 问题。然而，目前仍然存在三个主要的挑战: (1)驾驶员的路径决策受到多源异质特征的影响，这些特征之间的复杂关系使得很难探索它们是如何影响驾驶员的路径决策的; (2)大的路径决策空间使得很容易探索和预测不合理的路径; (3)驾驶员的个性化偏好在路径决策过程建模中起着重要作用，但不能充分探索。为了解决上述问题，我们提出了一种基于图的 PDRP 仿真学习方法 ILRoute。ILRoute 利用多图神经网络(multi-GNN)提取多源异构特征，建立其复杂关系模型。为了解决大规模路径决策空间问题，ILRoute 引入了移动规则感知约束作为先验路径选择知识，以减少探测路径决策空间。为了建立驾驶员个性化偏好模型，ILRoute 利用个性化约束机制来增强驾驶员路径决策过程的个性化。在三个实际数据集上进行的离线实验和在线比较证明了该模型的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ILRoute:+A+Graph-based+Imitation+Learning+Method+to+Unveil+Riders'+Routing+Strategies+in+Food+Delivery+Service)|0|
|[Influence Maximization with Fairness at Scale](https://doi.org/10.1145/3580305.3599847)|Yuting Feng, Ankitkumar Patel, Bogdan Cautis, Hossein Vahabi|University Paris-Saclay; University of California – Berkeley|In this paper, we revisit the problem of influence maximization with fairness, which aims to select k influential nodes to maximise the spread of information in a network, while ensuring that selected sensitive user attributes are fairly affected, i.e., are proportionally similar between the original network and the affected users. Recent studies on this problem focused only on extremely small networks, hence the challenge remains on how to achieve a scalable solution, applicable to networks with millions or billions of nodes. We propose an approach that is based on learning node representations for fair spread from diffusion cascades, instead of the social connectivity s.t. we can deal with very large graphs. We propose two data-driven approaches: (a) fairness-based participant sampling (FPS), and (b) fairness as context (FAC). Spread related user features, such as the probability of diffusing information to others, are derived from the historical information cascades, using a deep neural network. The extracted features are then used in selecting influencers that maximize the influence spread, while being also fair with respect to the chosen sensitive attributes. In FPS, fairness and cascade length information are considered independently in the decision-making process, while FAC considers these information facets jointly and considers correlations between them. The proposed algorithms are generic and represent the first policy-driven solutions that can be applied to arbitrary sets of sensitive attributes at scale. We evaluate the performance of our solutions on a real-world public dataset (Sina Weibo) and on a hybrid real-synthethic dataset (Digg), which exhibit all the facets that we exploit, namely diffusion network, diffusion traces, and user profiles. These experiments show that our methods outperform the state-the-art solutions in terms of spread, fairness, and scalability.|本文重新讨论了公平影响最大化问题，其目的是选择 k 个有影响的节点以使网络中的信息传播最大化，同时确保选择的敏感用户属性受到相当大的影响，即原始网络与受影响用户之间的比例相似。最近关于这个问题的研究只集中在极小的网络上，因此挑战仍然是如何实现一个可伸缩的解决方案，适用于拥有数百万或数十亿个节点的网络。我们提出了一种基于学习节点表示的扩散级联公平扩散方法，代替了社会连通性方法，我们可以处理非常大的图。我们提出两种数据驱动的方法: (a)基于公平的参与者抽样(FPS)和(b)作为上下文的公平(FAC)。传播相关的用户特征，如向他人传播信息的概率，是从历史信息级联，使用深度神经网络推导出来的。然后将提取的特征用于选择影响者，使影响扩散最大化，同时对所选择的敏感属性也是公平的。在 FPS 中，公平性和级联长度信息在决策过程中被独立地考虑，而 FAC 则联合考虑这些信息方面，并考虑它们之间的相关性。提出的算法是通用的，代表了第一个策略驱动的解决方案，可以应用于任意集的敏感属性在规模。我们在一个真实世界的公共数据集(新浪微博)和一个混合的真实合成数据集(Digg)上评估我们的解决方案的性能，这些数据集展示了我们利用的所有方面，即扩散网络、扩散轨迹和用户配置文件。这些实验表明，我们的方法在扩展性、公平性和可伸缩性方面优于最先进的解决方案。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Influence+Maximization+with+Fairness+at+Scale)|0|
|[Yggdrasil Decision Forests: A Fast and Extensible Decision Forests Library](https://doi.org/10.1145/3580305.3599933)|Mathieu GuillameBert, Sebastian Bruch, Richard Stotz, Jan Pfeifer|Pinecone; Google|Yggdrasil Decision Forests is a library for the training, serving and interpretation of decision forest models, targeted both at research and production work, implemented in C++, and available in C++, command line interface, Python (under the name TensorFlow Decision Forests), JavaScript, and Go. The library has been developed organically since 2018 following a set of four design principles applicable to machine learning libraries and frameworks: simplicity of use, safety of use, modularity and high-level abstraction, and integration with other machine learning libraries. In this paper, we describe those principles in detail and present how they have been used to guide the design of the library. We then showcase the use of our library on a set of classical machine learning problems. Finally, we report a benchmark comparing our library to related solutions.|Yggdrasil Decision Forest 是一个用于决策森林模型的培训、服务和解释的库，针对研究和生产工作，用 C + + 实现，可用于 C + + 、命令行界面、 Python (名为 TensorFlow Decision Forest)、 JavaScript 和 Go。自2018年以来，该图书馆按照适用于机器学习图书馆和框架的四条设计原则进行了有机开发: 简单易用、安全使用、模块化和高级抽象，以及与其他机器学习图书馆的集成。在本文中，我们详细地描述了这些原则，并介绍了如何使用这些原则来指导图书馆的设计。然后，我们展示了我们的库在一组经典的机器学习问题上的应用。最后，我们报告了一个比较我们的库和相关解决方案的基准。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Yggdrasil+Decision+Forests:+A+Fast+and+Extensible+Decision+Forests+Library)|0|
|[Towards Equitable Assignment: Data-Driven Delivery Zone Partition at Last-mile Logistics](https://doi.org/10.1145/3580305.3599915)|Baoshen Guo, Shuai Wang, Haotian Wang, Yunhuai Liu, Fanshuo Kong, Desheng Zhang, Tian He||The popularity of online e-commerce has promoted the rapid development of last-mile logistics in recent years. In last-mile services, to ensure delivery efficiency and enhance user experience, the delivery zone is proposed to perform delivery task assignment, which is a fundamental part of last-mile delivery. Each courier is responsible for one delivery zone. Couriers will collect orders belonging to their delivery zones from the delivery station and deliver orders to customers. Existing delivery zone partition practices in last-mile logistics consist of manual experience-based and static optimization-based methods, which perform order amount balancing among different zone but suffer from dissatisfaction and inefficiency because of two limitations: (i) using order amount is not always a good balancing metric considering deliveries' various difficulties (e.g., residence or industrial park, with or without elevators); (ii) less considering couriers' familiarity and preference behaviors. To generate delivery zone partition with equitable workload assignment, in this paper, we propose E-partition, a data-driven delivery zone partition framework to achieve equitable workload assignment in last-mile logistics. We first design a learning-based workload prediction model to estimate service time given a partition plan that consists of unseen courier-zone matching scenarios. Then, a delivery zone partition algorithm is proposed to iterative optimize couriers' core-AOI (i.e., area of interest) generation and AOI assignment process. Extensive offline experimental results show that our model outperforms baselines in working time prediction and workload balancing performances. Real-world deployment results at JD Logistics also verify the effectiveness of equitable-assignment aware delivery zone partition, with a 2.2% increase in service on-time rate compared to state-of-practice partition solutions.|近年来，在线电子商务的普及促进了最后一公里物流的快速发展。在最后一英里服务中，为了确保交付效率和提高用户体验，提出了交付区域来执行交付任务分配，这是最后一英里服务的基本组成部分。每个快递员负责一个送货区。快递员将从送货站收集属于其送货区域的订单，并向客户发送订单。现有的最后一英里物流配送区划分实践包括基于人工经验和基于静态优化的方法，这些方法在不同区域之间执行订单数量的平衡，但由于两个限制而遭受不满和效率低下: (i)考虑到配送的各种困难(例如，住宅或工业园区，有或没有电梯) ，使用订单数量并不总是一个很好的平衡指标; (ii)较少考虑快递员的熟悉度和偏好行为。为了生成具有公平工作量分配的配送区域划分，本文提出了基于数据驱动的配送区域划分框架 E 划分，以实现最后一英里物流的公平工作量分配。我们首先设计了一个基于学习的工作负载预测模型来估计服务时间，该模型给出了一个由不可见的信使区匹配场景组成的分区计划。然后，提出了一种交付区域划分算法，用于迭代优化信使的核心 AOI (即感兴趣区域)生成和 AOI 分配过程。大量离线实验结果表明，该模型在工作时间预测和工作负载平衡方面优于基线模型。JD Logistics 的实际部署结果也验证了公平分配感知交付区划分的有效性，与实践状态分区解决方案相比，服务准时率提高了2.2% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Equitable+Assignment:+Data-Driven+Delivery+Zone+Partition+at+Last-mile+Logistics)|0|
|[An Interpretable, Flexible, and Interactive Probabilistic Framework for Melody Generation](https://doi.org/10.1145/3580305.3599772)|Stephen Hahn, Rico Zhu, Simon Mak, Cynthia Rudin, Yue Jiang||The fast-growing demand for algorithmic music generation is found throughout entertainment, art, education, etc. Unfortunately, most recent models are practically impossible to interpret or musically fine-tune, as they use deep neural networks with thousands of parameters. We introduce an interpretable, flexible, and interactive model, SchenkComposer, for melody generation that empowers users to be creative in all aspects of the music generation pipeline and allows them to learn from the process. We divide the task of melody generation into steps based on the process that a human composer using music-theoretical domain knowledge might use. First, the model determines phrase structure based on form analysis and identifies an appropriate number of measures. Using concepts from Schenkerian analysis, the model then finds a fitting harmonic rhythm, middleground harmonic progression, foreground rhythm, and melody in a hierarchical, scaffolded approach using a probabilistic context-free grammar based on musical contours. By incorporating theories of musical form and harmonic structure, our model produces music with long-term structural coherence. In extensive human experiments, we find that music generated with our approach successfully passes a Turing test in human experiments while current state-of-the-art approaches fail, and we further demonstrate superior performance and preference for our melodies compared to existing melody generation methods. Additionally, we developed and deployed a public website for SchenkComposer, and conducted preliminary user surveys. Through analysis, we show the strong viability and enjoyability of SchenkComposer.|在娱乐、艺术、教育等领域，对算法生成音乐的需求正在快速增长。不幸的是，大多数最近的模型实际上是不可能解释或音乐微调，因为他们使用深层神经网络与数千参数。我们介绍了一个可解释的，灵活的，交互式的模型，申克作曲家，为旋律生成，使用户在音乐生成管道的各个方面都具有创造性，并允许他们从过程中学习。我们根据人类作曲家使用音乐理论领域知识可能使用的过程，将旋律生成的任务分为几个步骤。首先，该模型基于形式分析确定短语结构，并识别出适当数量的度量。使用申克分析的概念，模型然后找到一个合适的和声节奏，中间和声进展，前景节奏和旋律，在一个分层的，脚手架式的方法，使用基于音乐轮廓的概率上下文无关文法。该模型融合了音乐形式和和声结构理论，产生了具有长期结构连贯性的音乐。在大量的人体实验中，我们发现用我们的方法生成的音乐成功地通过了人体实验中的图灵测试，而目前最先进的方法失败了，我们进一步证明了与现有旋律生成方法相比，我们对旋律的优越性能和偏好。此外，我们还为 SchenkComposer 开发和部署了一个公共网站，并进行了初步的用户调查。通过分析，我们发现申克作曲家具有很强的生存力和欣赏力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Interpretable,+Flexible,+and+Interactive+Probabilistic+Framework+for+Melody+Generation)|0|
|[Efficient Continuous Space Policy Optimization for High-frequency Trading](https://doi.org/10.1145/3580305.3599813)|Li Han, Nan Ding, Guoxuan Wang, Dawei Cheng, Yuqi Liang||High-frequency trading is an extraordinarily intricate financial task, which is normally treated as a near real-time sequential decision problem. Compared with the traditional two-phase approach, forecasting equity's trend and then weighting them by combinatorial optimization, deep reinforcement learning (DRL) methods have shown advances in reward chasing with optimal policies. However, existing DRL-based methods either leverage portfolio optimization on low-frequency scenarios or only support a very limited number of assets with discrete action space, facing significant computing efficiency challenges. Therefore, we propose an efficient DRL-based policy optimization (DRPO) method for high-frequency trading. In particular, we model the portfolio management task with Markov Decision Process by directly inferring the equity weights in the action space guided by maximum accumulated returns. To reduce agents' interaction complexity without reducing interpretation, we detach the environment into the "static'' market states and "dynamic'' portfolio weight states. Then, we design an efficient reward expectation calculation algorithm via probabilistic dynamic programming, which enables our agents directly collect feedback away from trajectory sampling-based morass. To the best of our knowledge, this is the first work that solves the high-frequency portfolio optimization problem by devising an efficient continuous space policy optimization algorithm in the DRL framework. Through extensive experiments on the real-world data from Dow Jones, Coinbase and SSE exchanges, we show that our proposed DRPO significantly outperforms state-of-the-art benchmark methods. The results demonstrate the practical applicability and effectiveness of the proposed method.|高频交易是一项极其复杂的金融任务，通常被视为一个近乎实时的连续决策问题。与传统的两阶段方法相比，预测股票的趋势，然后用组合优化加权，深度强化学习(DRL)方法在最优政策的报酬追逐方面显示出了进步。然而，现有的基于 DRL 的方法要么在低频场景下利用投资组合优化，要么只支持数量非常有限的具有离散操作空间的资产，面临着巨大的计算效率挑战。因此，我们提出了一种有效的基于 DRL 的高频交易策略优化(DRPO)方法。特别是，我们通过直接推断以最大累积回报为指导的行动空间中的权重，以马可夫决策过程为投资组合管理任务建模。为了在不减少解释的情况下降低代理的交互复杂性，我们将环境分离为“静态”市场状态和“动态”投资组合权重状态。然后，利用概率动态规划方法设计了一种高效的报酬期望计算算法，使得我们的代理能够直接从基于轨迹抽样的困境中收集反馈信息。据我们所知，这是第一个通过在 DRL 框架内设计一个有效的连续空间政策优化算法来解决高频组合最佳化问题的工作。通过对道琼斯、 Coinbase 和 SSE 交易所的实际数据进行广泛的实验，我们发现我们提出的 DRPO 方法明显优于最先进的基准测试方法。计算结果表明了该方法的实用性和有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Continuous+Space+Policy+Optimization+for+High-frequency+Trading)|0|
|[Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering](https://doi.org/10.1145/3580305.3599819)|Xinyue Hu, Lin Gu, Qiyuan An, Mengliang Zhang, Liangchen Liu, Kazuma Kobayashi, Tatsuya Harada, Ronald M. Summers, Yingying Zhu||To contribute to automating the medical vision-language model, we propose a novel Chest-Xray Difference Visual Question Answering (VQA) task. Given a pair of main and reference images, this task attempts to answer several questions on both diseases and, more importantly, the differences between them. This is consistent with the radiologist's diagnosis practice that compares the current image with the reference before concluding the report. We collect a new dataset, namely MIMIC-Diff-VQA, including 700,703 QA pairs from 164,324 pairs of main and reference images. Compared to existing medical VQA datasets, our questions are tailored to the Assessment-Diagnosis-Intervention-Evaluation treatment procedure used by clinical professionals. Meanwhile, we also propose a novel expert knowledge-aware graph representation learning model to address this task. The proposed baseline model leverages expert knowledge such as anatomical structure prior, semantic, and spatial knowledge to construct a multi-relationship graph, representing the image differences between two images for the image difference VQA task. The dataset and code can be found at https://github.com/Holipori/MIMIC-Diff-VQA. We believe this work would further push forward the medical vision language model.|为了有助于医学视觉语言模型的自动化，我们提出了一个新颖的胸部 X 射线差分视觉问题回答(VQA)任务。给出一对主要和参考图像，这项任务试图回答这两种疾病的几个问题，更重要的是，它们之间的差异。这与放射科医生的诊断实践是一致的，即在结束报告之前将当前图像与参考文献进行比较。我们收集了一个新的数据集，即 MIMIC-Diff-VQA，包括来自164,324对主要和参考图像的700,703个 QA 对。与现有的医疗 VQA 数据集相比，我们的问题是根据临床专业人员使用的评估-诊断-干预-评估治疗程序而定制的。同时，提出了一种新的专家知识感知的图表示学习模型来解决这一问题。该基线模型利用解剖结构先验知识、语义知识和空间知识构造多关系图，表示两幅图像之间的图像差异，用于图像差异 VQA 任务。数据集和代码可以在 https://github.com/holipori/mimic-diff-vqa 找到。我们相信这项工作将进一步推动医学视觉语言模型的发展。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Expert+Knowledge-Aware+Image+Difference+Graph+Representation+Learning+for+Difference-Aware+Medical+Visual+Question+Answering)|0|
|[Graph Learning in Physical-informed Mesh-reduced Space for Real-world Dynamic Systems](https://doi.org/10.1145/3580305.3599835)|Yeping Hu, Bo Lei, Victor M. Castillo|Lawrence Livermore National Laboratory|Recent machine learning approaches have demonstrated their ability to extract information from data that could be translated into knowledge about the underlying dynamic systems. However, these learning-based models suffer from scalability issues when training on high-dimensional and high-resolution simulation data generated for real-world applications. In this work, we aim to tackle this challenge by deliberately prioritizing certain aspects of dynamic systems, while allocating relatively less attention and computational resources to others. Specifically, we concentrate on improving the predictive accuracy of crucial properties or regions that significantly impact these dynamic systems, while comparatively reducing emphasis on the remaining aspects. By employing graph learning schemes and custom-designed modules, we have developed a two-stage prediction model that incorporates prior knowledge of the systems. This approach enables us to place a heightened emphasis on the region of interest (ROI) during the learning process where the model operates in a reduced-dimensional mesh space, resulting in reduced computational costs while preserving crucial physical properties. To test and evaluate our method, we utilized two simulation datasets: lid-driven cavity and cylinder flow. The results show that even under reduced operational space, our method still achieves desirable performance on accuracy and generalizability of both prediction and physical consistency over region of interest.|最近的机器学习方法已经证明了它们从数据中提取信息的能力，这些信息可以转化为关于底层动态系统的知识。然而，这些基于学习的模型在训练为实际应用程序生成的高维和高分辨率仿真数据时遇到了可伸缩性问题。在这项工作中，我们的目标是解决这一挑战，有意识地优先考虑动态系统的某些方面，同时分配相对较少的注意力和计算资源给其他人。具体来说，我们集中于提高对这些动态系统有重大影响的关键属性或区域的预测准确性，同时相对减少对其余方面的重视。通过使用图学习方案和定制设计的模块，我们开发了一个两阶段预测模型，其中包含了系统的先验知识。这种方法使我们在学习过程中更加重视感兴趣区域(ROI) ，在这个过程中，模型在一个降维的网格空间中运行，从而降低了计算成本，同时保留了关键的物理特性。为了验证和评价我们的方法，我们使用了两个模拟数据集: 盖驱动腔和圆柱流动。结果表明，即使在较小的运算空间下，该方法仍然能够在预测精度和泛化能力方面达到理想的性能，并且在感兴趣区域上的物理一致性方面也能够达到理想的效果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Learning+in+Physical-informed+Mesh-reduced+Space+for+Real-world+Dynamic+Systems)|0|
|[Multimodal Indoor Localisation in Parkinson's Disease for Detecting Medication Use: Observational Pilot Study in a Free-Living Setting](https://doi.org/10.1145/3580305.3599872)|Ferdian Jovan, Catherine Morgan, Ryan McConville, Emma L. Tonkin, Ian Craddock, Alan L. Whone||Parkinson's disease (PD) is a slowly progressive, debilitating neurodegenerative disease which causes motor symptoms including gait dysfunction. Motor fluctuations are alterations between periods with a positive response to levodopa therapy ("on") and periods marked by re-emergency of PD symptoms ("off") as the response to medication wears off. These fluctuations often affect gait speed and they increase in their disabling impact as PD progresses. To improve the effectiveness of current indoor localisation methods, a transformer-based approach utilising dual modalities which provide complementary views of movement, Received Signal Strength Indicator (RSSI) and accelerometer data from wearable devices, is proposed. A sub-objective aims to evaluate whether indoor localisation, including its in-home gait speed features (i.e. the time taken to walk between rooms), could be used to evaluate motor fluctuations by detecting whether the person with PD is taking levodopa medications or withholding them. To properly evaluate our proposed method, we use a free-living dataset where the movements and mobility are greatly varied and unstructured as expected in real-world conditions. 24 participants lived in pairs (consisting of one person with PD, one control) for five days in a smart home with various sensors. Our evaluation on the resulting dataset demonstrates that our proposed network outperforms other methods for indoor localisation. The sub-objective evaluation shows that precise room-level localisation predictions, transformed into in-home gait speed features, produce accurate predictions on whether the PD participant is taking or withholding their medications.|帕金森病(PD)是一种缓慢进展、使人衰弱的神经退行性疾病，会引起包括步态功能障碍在内的运动症状。运动波动是指对左旋多巴疗法有积极反应的时期(“开”)和对药物反应逐渐消退时 PD 症状再次出现的时期(“关”)之间的变化。这些波动常常影响步速，并且随着 PD 的发展，其致残影响会增加。为了提高现有室内定位方法的有效性，提出了一种基于变压器的方法，该方法利用双重模式提供互补的运动视图，接收信号强度指示器(RSSI)和来自可穿戴设备的加速度计数据。一个子目标旨在评估是否可以通过检测帕金森病患者是否正在服用左旋多巴药物或不服用这些药物来评估运动波动，包括室内定位，包括室内步态速度特征(即在房间之间步行所需的时间)。为了正确地评估我们提出的方法，我们使用一个自由生活的数据集，其中的运动和流动性是非常不同的和非结构化的，如预期的现实世界条件。24名参与者(包括一名帕金森氏综合症患者和一名对照者)在一个装有各种传感器的智能家庭中生活了五天。我们对最终数据集的评估表明，我们提出的网络优于其他室内定位方法。亚客观评估显示，精确的房间水平定位预测，转化为家庭步态速度特征，产生准确的预测 PD 参与者是否正在服用或扣留他们的药物。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multimodal+Indoor+Localisation+in+Parkinson's+Disease+for+Detecting+Medication+Use:+Observational+Pilot+Study+in+a+Free-Living+Setting)|0|
|[Ball Trajectory Inference from Multi-Agent Sports Contexts Using Set Transformer and Hierarchical Bi-LSTM](https://doi.org/10.1145/3580305.3599779)|Hyunsung Kim, HanJun Choi, Chang Jo Kim, Jinsung Yoon, SangKi Ko|Kangwon National University; Fitogether Inc.|As artificial intelligence spreads out to numerous fields, the application of AI to sports analytics is also in the spotlight. However, one of the major challenges is the difficulty of automated acquisition of continuous movement data during sports matches. In particular, it is a conundrum to reliably track a tiny ball on a wide soccer pitch with obstacles such as occlusion and imitations. Tackling the problem, this paper proposes an inference framework of ball trajectory from player trajectories as a cost-efficient alternative to ball tracking. We combine Set Transformers to get permutation-invariant and equivariant representations of the multi-agent contexts with a hierarchical architecture that intermediately predicts the player ball possession to support the final trajectory inference. Also, we introduce the reality loss term and postprocessing to secure the estimated trajectories to be physically realistic. The experimental results show that our model provides natural and accurate trajectories as well as admissible player ball possession at the same time. Lastly, we suggest several practical applications of our framework including missing trajectory imputation, semi-automated pass annotation, automated zoom-in for match broadcasting, and calculating possession-wise running performance metrics.|随着人工智能在各个领域的广泛应用，人工智能在体育分析中的应用也越来越受到人们的关注。然而，其中一个主要的挑战是在体育比赛中自动获取连续运动数据的困难。特别是，在一个有遮挡和模仿等障碍的宽阔足球场上可靠地跟踪一个小球是一个难题。针对这一问题，本文提出了一种球员轨迹推理框架，作为一种性价比高的球轨迹推理方法。我们将集合变换器结合起来，得到了多智能体上下文的置换不变性和等变性表示，并采用层次结构中间预测球员的控球时间，以支持最终的轨迹推断。同时，我们引入实际损失项和后处理，以确保估计的轨迹是物理现实的。实验结果表明，我们的模型提供了自然和准确的轨迹，以及允许球员拥有球的同时。最后，我们提出了该框架的几个实际应用，包括缺失轨迹插补、半自动传球注释、比赛广播的自动放大以及计算占有方式的运行性能指标。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ball+Trajectory+Inference+from+Multi-Agent+Sports+Contexts+Using+Set+Transformer+and+Hierarchical+Bi-LSTM)|0|
|[Neural Insights for Digital Marketing Content Design](https://doi.org/10.1145/3580305.3599875)|Fanjie Kong, Yuan Li, Houssam Nassif, Tanner Fiez, Ricardo Henao, Shreya Chakrabarti|Duke University; ; Amazon.com, Inc.|In digital marketing, experimenting with new website content is one of the key levers to improve customer engagement. However, creating successful marketing content is a manual and time-consuming process that lacks clear guiding principles. This paper seeks to close the loop between content creation and online experimentation by offering marketers AI-driven actionable insights based on historical data to improve their creative process. We present a neural-network-based system that scores and extracts insights from a marketing content design, namely, a multimodal neural network predicts the attractiveness of marketing contents, and a post-hoc attribution method generates actionable insights for marketers to improve their content in specific marketing locations. Our insights not only point out the advantages and drawbacks of a given current content, but also provide design recommendations based on historical data. We show that our scoring model and insights work well both quantitatively and qualitatively.|在数字营销中，尝试新的网站内容是提高客户参与度的关键手段之一。然而，创建成功的营销内容是一个手工和耗时的过程，缺乏明确的指导原则。本文试图通过为营销人员提供基于历史数据的人工智能驱动的可操作的见解来改进他们的创造过程，从而结束内容创造和在线实验之间的循环。我们提出了一个基于神经网络的系统，从营销内容设计中得分和提取见解，即多模态神经网络预测营销内容的吸引力，事后归因方法为营销人员产生可操作的见解，以改善他们的内容在特定的营销地点。我们的见解不仅指出了给定当前内容的优缺点，而且还提供了基于历史数据的设计建议。我们证明了我们的评分模型和洞察力在数量和质量上都运行良好。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Insights+for+Digital+Marketing+Content+Design)|0|
|[Revisiting Hate Speech Benchmarks: From Data Curation to System Deployment](https://doi.org/10.1145/3580305.3599896)|Atharva Kulkarni, Sarah Masud, Vikram Goyal, Tanmoy Chakraborty|Carnegie Mellon University; IIIT-Delhi; IIT Delhi|Social media is awash with hateful content, much of which is often veiled with linguistic and topical diversity. The benchmark datasets used for hate speech detection do not account for such divagation as they are predominantly compiled using hate lexicons. However, capturing hate signals becomes challenging in neutrally-seeded malicious content. Thus, designing models and datasets that mimic the real-world variability of hate warrants further investigation. To this end, we present GOTHate, a large-scale code-mixed crowdsourced dataset of around 51k posts for hate speech detection from Twitter. GOTHate is neutrally seeded, encompassing different languages and topics. We conduct detailed comparisons of GOTHate with the existing hate speech datasets, highlighting its novelty. We benchmark it with 10 recent baselines. Our extensive empirical and benchmarking experiments suggest that GOTHate is hard to classify in a text-only setup. Thus, we investigate how adding endogenous signals enhances the hate speech detection task. We augment GOTHate with the user's timeline information and ego network, bringing the overall data source closer to the real-world setup for understanding hateful content. Our proposed solution HEN-mBERT is a modular, multilingual, mixture-of-experts model that enriches the linguistic subspace with latent endogenous signals from history, topology, and exemplars. HEN-mBERT transcends the best baseline by 2.5% and 5% in overall macro-F1 and hate class F1, respectively. Inspired by our experiments, in partnership with Wipro AI, we are developing a semi-automated pipeline to detect hateful content as a part of their mission to tackle online harm.|社交媒体充斥着令人憎恶的内容，其中许多内容往往被语言和话题的多样性所掩盖。用于检测仇恨言论的基准数据集没有考虑到这种分歧，因为它们主要是使用仇恨词典编纂的。然而，在中立种子的恶意内容中捕获仇恨信号变得具有挑战性。因此，设计模拟真实世界仇恨变化的模型和数据集需要进一步研究。为此，我们提出 GOTHate，一个大规模的代码混合众包数据集约51k 帖子仇恨言论检测从 Twitter。GOTHate 是中性的种子，包含不同的语言和主题。我们进行详细的比较 GOTHate 与现有的仇恨言论数据集，突出其新颖性。我们以最近的10个基线作为基准。我们广泛的经验和基准测试实验表明，GOTHate 很难分类在一个只有文本的设置。因此，我们研究如何增加内源信号增强仇恨语音检测任务。我们增加 GOTHate 与用户的时间线信息和自我网络，使整体数据源更接近现实世界的设置理解可恶的内容。我们提出的解决方案 HEN-mBERT 是一个模块化的、多语言的、专家混合的模型，它用来自历史、拓扑和范例的潜在内源信号丰富了语言子空间。HEN-mBERT 在整个宏 F1和仇恨类 F1中分别超过最佳基线的2.5% 和5% 。受我们实验的启发，与 Wipro AI 合作，我们正在开发一个半自动化的管道来检测仇恨内容，作为他们处理网络危害任务的一部分。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+Hate+Speech+Benchmarks:+From+Data+Curation+to+System+Deployment)|0|
|[Towards Suicide Prevention from Bipolar Disorder with Temporal Symptom-Aware Multitask Learning](https://doi.org/10.1145/3580305.3599917)|Daeun Lee, Sejung Son, Hyolim Jeon, Seungbae Kim, Jinyoung Han|Sungkyunkwan University; University of South Florida|Bipolar disorder (BD) is closely associated with an increased risk of suicide. However, while the prior work has revealed valuable insight into understanding the behavior of BD patients on social media, little attention has been paid to developing a model that can predict the future suicidality of a BD patient. Therefore, this study proposes a multi-task learning model for predicting the future suicidality of BD patients by jointly learning current symptoms. We build a novel BD dataset clinically validated by psychiatrists, including 14 years of posts on bipolar-related subreddits written by 818 BD patients, along with the annotations of future suicidality and BD symptoms. We also suggest a temporal symptom-aware attention mechanism to determine which symptoms are the most influential for predicting future suicidality over time through a sequence of BD posts. Our experiments demonstrate that the proposed model outperforms the state-of-the-art models in both BD symptom identification and future suicidality prediction tasks. In addition, the proposed temporal symptom-aware attention provides interpretable attention weights, helping clinicians to apprehend BD patients more comprehensively and to provide timely intervention by tracking mental state progression.|躁郁症(BD)与自杀风险增加密切相关。然而，尽管先前的工作已经揭示了了解 BD 患者在社交媒体上的行为的有价值的洞察力，但是很少有人注意开发一个能够预测 BD 患者未来自杀行为的模型。因此，本研究提出了一个多任务学习模型，通过联合学习当前症状来预测 BD 患者未来的自杀行为。我们建立了一个由精神科医生临床验证的新型 BD 数据集，包括818名 BD 患者撰写的14年双相相关子版块的帖子，以及未来自杀和 BD 症状的注释。我们还提出了一个时间症状意识的注意机制，以确定哪些症状是最有影响的预测未来自杀随着时间的推移，通过一系列的 BD 职位。我们的实验表明，所提出的模型在 BD 症状识别和未来自杀预测任务方面都优于最先进的模型。此外，提出的时间症状意识注意提供了可解释的注意力权重，帮助临床医生更全面地理解 BD 患者，并提供及时的干预，跟踪精神状态的进展。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Suicide+Prevention+from+Bipolar+Disorder+with+Temporal+Symptom-Aware+Multitask+Learning)|0|
|[Learning Slow and Fast System Dynamics via Automatic Separation of Time Scales](https://doi.org/10.1145/3580305.3599858)|Ruikun Li, Huandong Wang, Yong Li|Tsinghua University|Learning the underlying slow and fast dynamics of a system is instrumental for many practical applications related to the system. However, existing approaches are limited in discovering the appropriate time scale to separate the slow and fast variables and effectively learning their dynamics based on correct-dimensional representation vectors. In this paper, we introduce a framework that effectively learns slow and fast system dynamics in an integrated manner. We propose a novel intrinsic dimensionality (ID) driven learning method based on a time-lagged autoencoder framework to identify appropriate time scales to separate slow and fast variables and their IDs simultaneously. Further, we propose an integrated framework to concurrently learn the system's slow and fast dynamics, which is able to integrate prior knowledge of time scale and IDs and model the complex coupled slow and fast variables. Extensive experimental results on two representative dynamical systems show that our proposed framework is able to efficiently learn slow and fast system dynamics. Specifically, the long-time prediction performance is able to be improved by 36% on average compared with four representative baselines based on our proposed framework. Furthermore, our proposed system is able to extract interpretable slow and fast dynamics highly correlated with the known slow and fast variables in the dynamical systems. Our codes and datasets are open-sourced at: https://github.com/tsinghua-fib-lab/SlowFastSeparation.|学习一个系统的潜在的缓慢和快速的动态对于许多与该系统相关的实际应用是有帮助的。然而，现有的方法在寻找合适的时间尺度来分离慢变量和快变量以及基于正确的维数表示向量有效地学习它们的动力学方面受到了限制。在本文中，我们介绍了一个框架，有效地学习慢速和快速的系统动力学在一个集成的方式。提出了一种基于时滞自动编码器框架的内禀维度(ID)驱动学习方法，通过识别合适的时间尺度来同时分离慢变量和快变量及其 ID。进一步，我们提出了一个集成的框架来同时学习系统的慢速和快速动态，它能够集成时间尺度和 ID 的先验知识，并建模复杂耦合的慢速和快速变量。两个典型动力系统的大量实验结果表明，我们提出的框架能够有效地学习慢速和快速的系统动力学。具体来说，基于我们提出的框架，与四个具有代表性的基线相比，长时间预测性能平均提高了36% 。此外，我们提出的系统能够提取可解释的慢速和快速动态与已知的动态系统中的慢速和快速变量高度相关。我们的代码和数据集是开源的:  https://github.com/tsinghua-fib-lab/slowfastseparation。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Slow+and+Fast+System+Dynamics+via+Automatic+Separation+of+Time+Scales)|0|
|[Diga: Guided Diffusion Model for Graph Recovery in Anti-Money Laundering](https://doi.org/10.1145/3580305.3599806)|Xujia Li, Yuan Li, Xueying Mo, Hebing Xiao, Yanyan Shen, Lei Chen||With the upsurge of online banking, mobile payment, and virtual currency, new money-laundering crimes easily conceal in the enormous transaction volume. The traditional rule-based methods with large amounts of alerting thresholds are already incapable of handling the fast-changing transaction networks. Recently, the DL models represented by the graph neural networks (GNNs) show the potential to capture money-laundering modes with high accuracy. However, most related works are still far from practical deployment in the industry. Based on our practice at WeBank, there are three major challenges: Firstly, supervised learning is infeasible facing the extraordinarily large-scale but imbalanced data, with hundreds of millions of active accounts but only thousands of anomalies. Secondly, the real-world transactions form a sparse network with millions of isolated user groups, which overflows the expressive ability of current node-level GNNs. Thirdly, the explanation for each suspicious account is mandatory by the government for double check, which conflicts with the black-box nature of most DL models. Therefore, we proposed Diga, the first work to apply the diffusion probabilistic model to a graph anomaly detection problem with three novel techniques: the biased K-hop PageRank, the semi-supervised guided diffusion and the novel weight-sharing GNN layer. The effectiveness and efficiency of Diga are verified via intensive experiments on both industrial and public datasets.|随着网上银行、移动支付和虚拟货币的兴起，新的洗钱犯罪很容易隐藏在巨大的交易量中。具有大量警报阈值的传统基于规则的方法已经不能处理快速变化的事务网络。最近，以图形神经网络(GNN)为代表的 DL 模型显示了高精度捕获洗钱模式的潜力。然而，大多数相关工作还远远没有实际应用到行业中。根据我们在 WeBank 的实践，有三个主要的挑战: 第一，面对异常庞大但不平衡的数据，监督式学习是不可行的，有数以亿计的活跃账户，但只有数以千计的异常。其次，现实世界中的事务形成了一个由数百万个孤立的用户组组成的稀疏网络，这使得当前节点级 GNN 的表达能力泛滥。第三，政府对每个可疑账户的解释都是强制性的，这与大多数 DL 模型的黑箱性质相冲突。因此，我们提出了 Diga，这是第一个将扩散概率模型应用于图形异常检测问题的工作，它采用了三种新技术: 有偏的 K-hop pageRank、半监督引导扩散和新的权重分享 GNN 层。通过对工业数据集和公共数据集的深入实验，验证了 Diga 的有效性和高效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Diga:+Guided+Diffusion+Model+for+Graph+Recovery+in+Anti-Money+Laundering)|0|
|[HardSATGEN: Understanding the Difficulty of Hard SAT Formula Generation and A Strong Structure-Hardness-Aware Baseline](https://doi.org/10.1145/3580305.3599837)|Yang Li, Xinyan Chen, Wenxuan Guo, Xijun Li, Wanqian Luo, Junhua Huang, HuiLing Zhen, Mingxuan Yuan, Junchi Yan||Industrial SAT formula generation is a critical yet challenging task. Existing SAT generation approaches can hardly simultaneously capture the global structural properties and maintain plausible computational hardness. We first present an in-depth analysis for the limitation of previous learning methods in reproducing the computational hardness of original instances, which may stem from the inherent homogeneity in their adopted split-merge procedure. On top of the observations that industrial formulae exhibit clear community structure and oversplit substructures lead to the difficulty in semantic formation of logical structures, we propose HardSATGEN, which introduces a fine-grained control mechanism to the neural split-merge paradigm for SAT formula generation to better recover the structural and computational properties of the industrial benchmarks. Experiments including evaluations on private and practical corporate testbed show the superiority of HardSATGEN being the only method to successfully augment formulae maintaining similar computational hardness and capturing the global structural properties simultaneously. Compared to the best previous methods, the average performance gains achieve 38.5 statistics, 88.4 effectiveness of guiding solver tuning by our generated instances. Source code is available at http://github.com/Thinklab-SJTU/HardSATGEN|工业 SAT 公式的生成是一项关键而富有挑战性的任务。现有的 SAT 生成方法很难同时捕获全局结构性质和保持合理的计算难度。我们首先深入分析了以往学习方法在再现原始实例的计算难度方面存在的局限性，这可能是由于它们所采用的拆分合并过程固有的同质性所致。针对工业公式表现出明显的群体结构和子结构过度分裂导致逻辑结构语义形成困难的现象，提出了 HardSATGEN 算法，该算法在 SAT 公式生成的神经分裂合并范式中引入了细粒度控制机制，以更好地恢复工业基准的结构和计算特性。实验包括在私营企业和实际企业的试验台上进行的评估表明，HardSATGEN 是唯一能够成功地增强计算公式并同时获取整体结构特性的方法，具有优越性。与以前最好的方法相比，平均性能增益达到了38.5个统计数据，通过生成的实例指导解决方案调优的有效性达到了88.4个。源代码可在 http://github.com/thinklab-sjtu/hardsatgen 下载|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HardSATGEN:+Understanding+the+Difficulty+of+Hard+SAT+Formula+Generation+and+A+Strong+Structure-Hardness-Aware+Baseline)|0|
|[Learning Joint Relational Co-evolution in Spatial-Temporal Knowledge Graph for SMEs Supply Chain Prediction](https://doi.org/10.1145/3580305.3599855)|Youru Li, Zhenfeng Zhu, Xiaobo Guo, Linxun Chen, Zhouyin Wang, Yinmeng Wang, Bing Han, Yao Zhao|Beijing Jiaotong University; MYBank, Ant Group|To effectively explore the supply chain relationships among Small and Medium-sized Enterprises (SMEs), some remarkable progress in such a relation modeling problem, especially knowledge graph-based methods have been witnessed during these years. As a typical link prediction task, supply chain prediction can usually predict the unknown future relationship facts between SMEs by utilizing the historical semantic connections between entities in knowledge graphs (KGs). However, it is still a great challenge for existing models as seldom of them can consider both temporal dependency and cooperative correlation of the connectivity pattern along the timeline synergistically. Accordingly, we propose a novel framework to learn joint relational co-evolution in Spatial-Temporal Knowledge Graphs (STKG). Specifically, on the base of the constructed large-scale financial STKG, a multi-view relational sequences mining method is proposed to reveal the semantic information from ontological concepts. Furthermore, a relational co-evolution learning module is also developed to capture the regularity of evolving connectivity patterns from the spatial-temporal view. Meanwhile, a multiple random subspace representation learning layer is also designed to improve both compatibility and complementarity during knowledge aggregation. Experimental results on large-scale SMEs supply chain prediction tasks from four real-world industries in China have illustrated the effectiveness of the proposed model.|为了有效地探索中小企业之间的供应链关系，近年来在这类关系建模问题，特别是基于知识图的方法方面取得了显著的进展。供应链预测作为一种典型的环节预测任务，通常利用知识图中实体之间的历史语义联系来预测未知的中小企业未来关系事实。然而，对于现有的模型来说，这仍然是一个巨大的挑战，因为很少有模型能够同时考虑连通模式在时间轴上的时间依赖性和协同相关性。因此，我们提出了一个新的框架来学习联合关系协同进化的时空知识图(STKG)。在构建大规模金融 STKG 的基础上，提出了一种多视图关系序列挖掘方法，以揭示本体概念的语义信息。此外，还开发了一个关系协同进化学习模块，从时空视角捕捉连通模式演化的规律性。同时，设计了一个多随机子空间表示学习层，以提高知识集成过程中的兼容性和互补性。通过对我国四个实际行业大规模中小企业供应链预测任务的实验结果表明了该模型的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Joint+Relational+Co-evolution+in+Spatial-Temporal+Knowledge+Graph+for+SMEs+Supply+Chain+Prediction)|0|
|[Analysis of COVID-19 Offensive Tweets and Their Targets](https://doi.org/10.1145/3580305.3599773)|Song Liao, Ebuka Okpala, Long Cheng, Mingqi Li, Nishant Vishwamitra, Hongxin Hu, Feng Luo, Matthew Costello||During the global COVID-19 pandemic, people utilized social media platforms, especially Twitter, to spread and express opinions about the pandemic. Such discussions also drove the rise in COVID-related offensive speech. In this work, focusing on Twitter, we present a comprehensive analysis of COVID-related offensive tweets and their targets. We collected a COVID-19 dataset with over 747 million tweets for 30 months and fine-tuned a BERT classifier to detect offensive tweets. Our offensive tweets analysis shows that the ebb and flow of COVID-related offensive tweets potentially reflect events in the physical world. We then studied the targets of these offensive tweets. There was a large number of offensive tweets with abusive words, which could negatively affect the targeted groups or individuals. We also conducted a user network analysis, and found that offensive users interact more with other offensive users and that the pandemic had a lasting impact on some offensive users. Our study offers novel insights into the persistence and evolution of COVID-related offensive tweets during the pandemic|在全球2019冠状病毒疾病大流行期间，人们利用社交媒体平台，特别是推特，来传播和表达关于大流行的意见。这样的讨论也推动了 COVID 相关攻击性言论的增加。在这项工作中，以 Twitter 为重点，我们提出了一个综合分析 COVID 相关的攻击性推文及其目标。我们收集了一个超过7.47亿条推文的2019冠状病毒疾病数据集，历时30个月，并对 BERT 分类器进行了微调，以检测攻击性推文。我们的攻击性推文分析表明，与新冠病毒相关的攻击性推文的起伏可能反映了现实世界中的事件。然后我们研究了这些攻击性推文的目标。大量带有侮辱性词语的攻击性推文可能对目标群体或个人产生负面影响。我们还进行了用户网络分析，发现攻击性用户与其他攻击性用户互动更多，这一大流行病对一些攻击性用户产生了持久影响。我们的研究为大流行期间与新冠病毒相关的攻击性推文的持续性和演变提供了新的见解|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Analysis+of+COVID-19+Offensive+Tweets+and+Their+Targets)|0|
|[Balancing Approach for Causal Inference at Scale](https://doi.org/10.1145/3580305.3599778)|Sicheng Lin, Meng Xu, Xi Zhang, ShihKang Chao, YingKai Huang, Xiaolin Shi|Realtor.com; Snap Inc.; University of Missouri|With the modern software and online platforms to collect massive amount of data, there is an increasing demand of applying causal inference methods at large scale when randomized experimentation is not viable. Weighting methods that directly incorporate covariate balancing have recently gained popularity for estimating causal effects in observational studies. These methods reduce the manual efforts required by researchers to iterate between propensity score modeling and balance checking until a satisfied covariate balance result. However, conventional solvers for determining weights lack the scalability to apply such methods on large scale datasets in companies like Snap Inc. To address the limitations and improve computational efficiency, in this paper we present scalable algorithms, DistEB and DistMS, for two balancing approaches: entropy balancing and MicroSynth. The solvers have linear time complexity and can be conveniently implemented in distributed computing frameworks such as Spark, Hive, etc. We study the properties of balancing approaches at different scales up to 1 million treated units and 487 covariates. We find that with larger sample size, both bias and variance in the causal effect estimation are significantly reduced. The results emphasize the importance of applying balancing approaches on large scale datasets. We combine the balancing approach with a synthetic control framework and deploy an end-to-end system for causal impact estimation at Snap Inc.|随着现代软件和在线平台对海量数据的收集，在随机实验不可行的情况下，大规模应用因果推理方法的需求越来越大。加权方法，直接纳入协变量平衡，最近得到了普遍的估计因果效应的观察研究。这些方法减少了研究人员在倾向评分建模和平衡检查之间进行迭代所需的人工努力，直到得到满意的协变量平衡结果。然而，用于确定权重的传统解决方案缺乏可伸缩性，无法在 Snap Inc. 等公司的大规模数据集上应用这种方法。为了解决这些局限性和提高计算效率，本文提出了两种可扩展的算法: distEB 和 distMS，分别用于熵平衡和 MicroSynth。求解器具有线性时间复杂性，可以方便地在分布式计算框架(如 Spark、 Hive 等)中实现。我们研究在不同尺度上多达100万个治疗单位和487个协变量的平衡方法的特性。我们发现，随着样本量的增加，因果效应估计的偏差和方差都显著减少。结果强调了在大规模数据集上应用平衡方法的重要性。我们将平衡方法与综合控制框架相结合，并在 Snap 公司部署了一个端到端的因果影响估计系统。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Balancing+Approach+for+Causal+Inference+at+Scale)|0|
|[Uncertainty-Aware Probabilistic Travel Time Prediction for On-Demand Ride-Hailing at DiDi](https://doi.org/10.1145/3580305.3599925)|Hao Liu, Wenzhao Jiang, Shui Liu, Xi Chen||Travel Time Estimation (TTE) aims to accurately forecast the expected trip duration from an origin to a destination. As one of the world's largest ride-hailing platforms, DiDi answers billions of TTE queries per day. The quality of TTE directly decides the customer's experience and the effectiveness of passenger-to-driver matching. However, existing studies mainly regard TTE as a deterministic regression problem and focus on improving the prediction accuracy of a single label, which overlooks the travel time uncertainty induced by various dynamic contextual factors. To this end, in this paper, we propose a probabilistic framework, ProbTTE, for uncertainty-aware travel time prediction. Specifically, the framework first transforms the single-label regression task to a multi-class classification problem to estimate the implicit travel time distribution. Moreover, we propose an adaptive local label-smoothing scheme to capture the ordinal inter-class relationship among soft travel time labels. Furthermore, we construct a route-wise log-normal distribution regularizer to absorb prior knowledge from large-scale historical trip data. By explicitly considering the travel uncertainty, the proposed approach not only improves the TTE accuracy but also provides additional travel time information to benefit downstream tasks in ride-hailing. Extensive experiments on real-world datasets demonstrate the superiority of the proposed framework compared with state-of-the-art travel time prediction algorithms. In addition, ProbTTE has been deployed in production at DiDi in late 2022 to empower various order dispatching services, and improves passenger and driver experiences significantly.|旅行时间估计(TTE)的目的是准确预测从出发地到目的地的预期旅行时间。作为全球最大的叫车平台之一，滴滴每天回答数十亿条 TTE 查询。TTE 的质量直接决定了客户的体验和乘客与驾驶员匹配的有效性。然而，现有的研究主要将 TTE 视为一个确定性回归问题，侧重于提高单个标签的预测精度，忽视了各种动态背景因素引起的行程时间不确定性。为此，本文提出了一种基于概率框架的不确定感知行程时间预测方法—— PROTTE。具体来说，该框架首先将单标签回归任务转换为多类分类问题，以估计隐含的行程时间分布。此外，我们提出了一个自适应局部标签平滑方案来捕捉软旅行时间标签之间的有序类间关系。此外，我们还构造了一个路径对数正态分布规则化器，以吸收大规模历史行程数据中的先验知识。通过明确考虑出行不确定性，提出的方法不仅提高了 TTE 的精度，而且提供了额外的出行时间信息，有利于下游任务的叫车。在实际数据集上的大量实验表明，该框架与目前最先进的行程时间预测算法相比具有优越性。此外，PROTTE 已于2022年底在滴滴投入生产，以增强各种订单调度服务的能力，并显著改善乘客和司机的体验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Uncertainty-Aware+Probabilistic+Travel+Time+Prediction+for+On-Demand+Ride-Hailing+at+DiDi)|0|
|[WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences](https://doi.org/10.1145/3580305.3599931)|Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, Jie Tang|Beihang University; Tsinghua University; Zhipu.AI|We present WebGLM, a web-enhanced question-answering system based on the General Language Model (GLM). Its goal is to augment a pre-trained large language model (LLM) with web search and retrieval capabilities while being efficient for real-world deployments. To achieve this, we develop WebGLM with strategies for the LLM-augmented retriever, bootstrapped generator, and human preference-aware scorer. Specifically, we identify and address the limitations of WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency, and cost-effectiveness advantages. In addition, we propose systematic criteria for evaluating web-enhanced QA systems. We conduct multi-dimensional human evaluation and quantitative ablation studies, which suggest the outperformance of the proposed WebGLM designs over existing systems. WebGLM with the 10-billion-parameter GLM (10B) is shown to perform better than the similar-sized WebGPT (13B) and even comparably to WebGPT (175B) in human evaluation. The code, demo, and data are at \url{https://github.com/THUDM/WebGLM}.|本文介绍了基于通用语言模型(GLM)的 Web 增强型问答系统 WebGLM。它的目标是增强预先训练的大型语言模型(LLM) ，该模型具有 Web 搜索和检索功能，同时对于现实世界的部署非常有效。为了实现这一点，我们使用 LLM 增强检索器、引导生成器和人类偏好感知记分器的策略来开发 WebGLM。具体来说，我们确定并解决了 WebGPT (OpenAI)的局限性，通过它，WebGLM 具有准确性、效率和成本效益方面的优势。此外，我们提出了评估网络增强 QA 系统的系统标准。我们进行了多维人体评估和定量消融研究，这表明所提出的 WebGLM 设计优于现有系统。具有100亿参数 GLM (10B)的 WebGLM 显示出比类似大小的 WebGPT (13B)更好的性能，甚至在人类评估中与 WebGPT (175B)相当。代码、演示和数据位于 url { https://github.com/thudm/webglm }。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=WebGLM:+Towards+An+Efficient+Web-Enhanced+Question+Answering+System+with+Human+Preferences)|0|
|[Impact-Oriented Contextual Scholar Profiling using Self-Citation Graphs](https://doi.org/10.1145/3580305.3599845)|Yuankai Luo, Lei Shi, Mufan Xu, Yuwen Ji, Fengli Xiao, Chunming Hu, Zhiguang Shan|Beihang University; State Information Center|Quantitatively profiling a scholar's scientific impact is important to modern research society. Current practices with bibliometric indicators (e.g., h-index), lists, and networks perform well at scholar ranking, but do not provide structured context for scholar-centric, analytical tasks such as profile reasoning and understanding. This work presents GeneticFlow (GF), a suite of novel graph-based scholar profiles that fulfill three essential requirements: structured-context, scholar-centric, and evolution-rich. We propose a framework to compute GF over large-scale academic data sources with millions of scholars. The framework encompasses a new unsupervised advisor-advisee detection algorithm, a well-engineered citation type classifier using interpretable features, and a fine-tuned graph neural network (GNN) model. Evaluations are conducted on the real-world task of scientific award inference. Experiment outcomes show that the F1 score of best GF profile significantly outperforms alternative methods of impact indicators and bibliometric networks in all the 6 computer science fields considered. Moreover, the core GF profiles, with 63.6%-66.5% nodes and 12.5%-29.9% edges of the full profile, still significantly outrun existing methods in 5 out of 6 fields studied. Visualization of GF profiling result also reveals human explainable patterns for high-impact scholars.|定量描述学者的科学影响力对于现代研究社会具有重要意义。目前使用文献计量指标(例如 h 索引)、列表和网络的实践在学者排名方面表现良好，但是没有为以学者为中心的分析任务(例如概况推理和理解)提供结构化的上下文。这项工作介绍了 GeneticFlow (GF) ，一套新颖的基于图形的学者档案，它满足三个基本要求: 结构化上下文、学者中心和进化丰富。我们提出了一个框架来计算广义函数在大规模的学术数据来源与数百万学者。该框架包括一个新的无监督导师-顾问检测算法，一个使用可解释特征的精心设计的引文类型分类器，以及一个微调图形神经网络(GNN)模型。对科学奖项推理的现实任务进行评价。实验结果表明，在所有6个计算机科学领域中，最佳 GF 配置文件的 F1得分显著优于影响指标和文献计量网络的替代方法。此外，核心 GF 配置文件，具有63.6% -66.5% 的节点和12.5% -29.9% 的边缘的完整配置文件，仍然显着超过现有的方法在6个研究领域中的5个。GF 分析结果的可视化也为高影响力学者提供了人类可解释的模式。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Impact-Oriented+Contextual+Scholar+Profiling+using+Self-Citation+Graphs)|0|
|[A Look into Causal Effects under Entangled Treatment in Graphs: Investigating the Impact of Contact on MRSA Infection](https://doi.org/10.1145/3580305.3599763)|Jing Ma, Chen Chen, Anil Vullikanti, Ritwick Mishra, Gregory Madden, Daniel Borrajo, Jundong Li||Methicillin-resistant Staphylococcus aureus (MRSA) is a type of bacteria resistant to certain antibiotics, making it difficult to prevent MRSA infections. Among decades of efforts to conquer infectious diseases caused by MRSA, many studies have been proposed to estimate the causal effects of close contact (treatment) on MRSA infection (outcome) from observational data. In this problem, the treatment assignment mechanism plays a key role as it determines the patterns of missing counterfactuals -- the fundamental challenge of causal effect estimation. Most existing observational studies for causal effect learning assume that the treatment is assigned individually for each unit. However, on many occasions, the treatments are pairwisely assigned for units that are connected in graphs, i.e., the treatments of different units are entangled. Neglecting the entangled treatments can impede the causal effect estimation. In this paper, we study the problem of causal effect estimation with treatment entangled in a graph. Despite a few explorations for entangled treatments, this problem still remains challenging due to the following challenges: (1) the entanglement brings difficulties in modeling and leveraging the unknown treatment assignment mechanism; (2) there may exist hidden confounders which lead to confounding biases in causal effect estimation; (3) the observational data is often time-varying. To tackle these challenges, we propose a novel method NEAT, which explicitly leverages the graph structure to model the treatment assignment mechanism, and mitigates confounding biases based on the treatment assignment modeling. We also extend our method into a dynamic setting to handle time-varying observational data. Experiments on both synthetic datasets and a real-world MRSA dataset validate the effectiveness of the proposed method, and provide insights for future applications.|耐药性金黄葡萄球菌(MRSA)是一种对某些抗生素有抗药性的细菌，因此难以预防感染耐甲氧西林金黄色葡萄球菌。数十年来，人们一直致力克服由耐药性金黄葡萄球菌引起的传染病，其中不少研究是根据观察数据估计密切接触(治疗)对耐药性金黄葡萄球菌感染(结果)的影响。在这个问题中，处理分配机制起着关键作用，因为它决定了缺失反事实的模式——因果效应估计的根本挑战。大多数现有的因果效应学习的观察性研究假设治疗是为每个单位单独指定的。然而，在许多情况下，对于图中连接的单元，处理是成对分配的，也就是说，不同单元的处理是纠缠在一起的。忽略纠缠处理会阻碍因果效应的估计。本文研究了处理纠缠于图中的因果效应估计问题。尽管对纠缠治疗进行了一些探索，但由于以下挑战，这个问题仍然具有挑战性: (1)纠缠给建模和利用未知的治疗分配机制带来困难; (2)可能存在隐藏的混杂因素，导致因果效应估计中的混杂偏差; (3)观察数据往往是时变的。为了解决这些问题，我们提出了一种新的方法 NEAT，它显式地利用图结构来建模处理分配机制，并基于处理分配模型来缓解混杂偏差。我们还将该方法推广到一个动态设置来处理时变的观测数据。在合成数据集和实际 MRSA 数据集上的实验验证了该方法的有效性，为今后的应用提供了参考。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Look+into+Causal+Effects+under+Entangled+Treatment+in+Graphs:+Investigating+the+Impact+of+Contact+on+MRSA+Infection)|0|
|[SAInf: Stay Area Inference of Vehicles using Surveillance Camera Records](https://doi.org/10.1145/3580305.3599952)|Zhipeng Ma, Chuishi Meng, Huimin Ren, Sijie Ruan, Jie Bao, Xiaoting Wang, Tianrui Li, Yu Zheng||Stay area detection is one of the most important applications in trajectory data mining, which is helpful to understand human's behavior intentions. Traditional stay area detection methods are based on GPS data with relatively high sampling rate. However, because of privacy issues, accessing GPS data can be difficult in most real-world applications. Fortunately, traffic surveillance cameras have been widely deployed in urban area, and it provides us a novel way of acquiring vehicles' trajectories. All the vehicles that traverse by can be recognized and recorded in a passive way. However, the trajectory data collected in this way is extremely coarse, because the surveillance cameras are only deployed in important locations, such as crossroads. This coarse trajectory introduces two challenges for the stay area detection problem, i.e., whether and where the stay event occurs. In this paper, we design a two-stage method to solve the stay area detection problem with coarse trajectories. It first detects the stay event between a surveillance camera record pair, then uses a layer-by-layer stay area identification algorithm to infer the exact stay area. Extensive experiments based on real-world data were used to evaluate the performance of the proposed framework. Results demonstrate the proposed framework SAInf achieved a 58% performance improvement compared with SOTA methods.|停留区域检测是弹道数据挖掘的重要应用之一，它有助于理解人类的行为意图。传统的停留区检测方法是基于 GPS 数据，采样率较高。然而，由于隐私问题，在大多数实际应用中访问 GPS 数据可能很困难。幸运的是，交通监控摄像机已经广泛应用于城市地区，它为我们提供了一种获取车辆轨迹的新方法。所有经过的车辆都可以被动识别和记录。然而，这种方式收集的弹道数据非常粗糙，因为监控摄像头只部署在重要地点，如十字路口。这种粗轨迹对停留区域检测问题提出了两个挑战，即停留事件是否发生以及在何处发生。本文设计了一种两阶段的方法来解决具有粗轨迹的停留区域检测问题。该算法首先检测监控摄像机记录对之间的停留事件，然后利用分层停留区域识别算法推断准确的停留区域。基于真实数据的大量实验被用来评估所提出的框架的性能。结果表明，与 SOTA 方法相比，提出的框架 SAInf 实现了58% 的性能改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SAInf:+Stay+Area+Inference+of+Vehicles+using+Surveillance+Camera+Records)|0|
|[Detecting Vulnerable Nodes in Urban Infrastructure Interdependent Network](https://doi.org/10.1145/3580305.3599804)|Jinzhu Mao, Liu Cao, Chen Gao, Huandong Wang, Hangyu Fan, Depeng Jin, Yong Li||Understanding and characterizing the vulnerability of urban infrastructures, which refers to the engineering facilities essential for the regular running of cities and that exist naturally in the form of networks, is of great value to us. Potential applications include protecting fragile facilities and designing robust topologies, etc. Due to the strong correlation between different topological characteristics and infrastructure vulnerability and their complicated evolution mechanisms, some heuristic and machine-assisted analysis fall short in addressing such a scenario. In this paper, we model the interdependent network as a heterogeneous graph and propose a system based on graph neural network with reinforcement learning, which can be trained on real-world data, to characterize the vulnerability of the city system accurately. The presented system leverages deep learning techniques to understand and analyze the heterogeneous graph, which enables us to capture the risk of cascade failure and discover vulnerable infrastructures of cities. Extensive experiments with various requests demonstrate not only the expressive power of our system but also transferring ability and necessity of the specific components.|城市基础设施的脆弱性是指城市正常运行所必需的、以网络形式自然存在的工程设施，理解和描述城市基础设施的脆弱性对我们具有重要价值。潜在的应用包括保护脆弱的设施和设计健壮的拓扑等。由于不同的拓扑结构特征和基础设施脆弱性之间存在很强的相关性，而且它们的演化机制复杂，因此一些启发式和机器辅助分析在处理这种情况时显得力不从心。本文将相互依赖的网络建模为一个异构图形，并提出了一个基于图形神经网络和强化学习的系统，该系统可以根据现实世界的数据进行训练，以准确描述城市系统的脆弱性。该系统利用深度学习技术来理解和分析异构图，使我们能够捕捉级联故障的风险，发现城市脆弱的基础设施。各种要求的大量实验不仅证明了我们系统的表达能力，而且证明了具体组件的转换能力和必要性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+Vulnerable+Nodes+in+Urban+Infrastructure+Interdependent+Network)|0|
|[DRL4Route: A Deep Reinforcement Learning Framework for Pick-up and Delivery Route Prediction](https://doi.org/10.1145/3580305.3599811)|Xiaowei Mao, Haomin Wen, Hengrui Zhang, Huaiyu Wan, Lixia Wu, Jianbin Zheng, Haoyuan Hu, Youfang Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DRL4Route:+A+Deep+Reinforcement+Learning+Framework+for+Pick-up+and+Delivery+Route+Prediction)|0|
|[Deep Offline Reinforcement Learning for Real-world Treatment Optimization Applications](https://doi.org/10.1145/3580305.3599800)|Mila Nambiar, Supriyo Ghosh, Priscilla Ong, Yu En Chan, Yong Mong Bee, Pavitra Krishnaswamy|Singapore General Hospital; Institute for Infocomm Research (I2R|There is increasing interest in data-driven approaches for dynamically choosing optimal treatment strategies in many chronic disease management and critical care applications. Reinforcement learning methods are well-suited to this sequential decision-making problem, but must be trained and evaluated exclusively on retrospective medical record datasets as direct online exploration is unsafe and infeasible. Despite this requirement, the vast majority of dynamic treatment optimization studies use off-policy RL methods (e.g., Double Deep Q Networks (DDQN) or its variants) that are known to perform poorly in purely offline settings. Recent advances in offline RL, such as Conservative Q-Learning (CQL), offer a suitable alternative. But there remain challenges in adapting these approaches to real-world applications where suboptimal examples dominate the retrospective dataset and strict safety constraints need to be satisfied. In this work, we introduce a practical transition sampling approach to address action imbalance during offline RL training, and an intuitive heuristic to enforce hard constraints during policy execution. We provide theoretical analyses to show that our proposed approach would improve over CQL. We perform extensive experiments on two real-world tasks for diabetes and sepsis treatment optimization to compare performance of the proposed approach against prominent off-policy and offline RL baselines (DDQN and CQL). Across a range of principled and clinically relevant metrics, we show that our proposed approach enables substantial improvements in expected health outcomes and in consistency with relevant practice and safety guidelines.|在许多慢性疾病管理和重症监护应用中，动态选择最佳治疗策略的数据驱动方法越来越引起人们的兴趣。强化学习方法非常适合于这种连续的决策问题，但必须经过培训，并且只能在回顾性医疗记录数据集上进行评估，因为直接在线探索是不安全和不可行的。尽管有这个要求，绝大多数动态治疗优化研究使用非策略 RL 方法(例如，双深 Q 网络(DDQN)或其变体) ，已知在纯脱机设置中表现不佳。离线 RL 的最新进展，如保守 Q 学习(CQL) ，提供了一个合适的替代方案。但是，在将这些方法应用于实际应用中仍然存在挑战，因为在实际应用中，次优示例占据了回顾性数据集的主导地位，并且需要满足严格的安全约束。在这项工作中，我们介绍了一个实用的转换抽样方法来解决离线 RL 训练中的动作不平衡问题，以及一个直观的启发式算法来加强策略执行中的硬约束。我们提供的理论分析表明，我们提出的方法将改善 CQL。我们在糖尿病和脓毒症治疗优化的两个现实世界任务上进行了广泛的实验，以比较所提出的方法对突出的非策略和离线 RL 基线(DDQN 和 CQL)的性能。通过一系列原则性和临床相关指标，我们表明我们提出的方法能够大大改善预期的健康结果，并与相关的实践和安全指南保持一致。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Offline+Reinforcement+Learning+for+Real-world+Treatment+Optimization+Applications)|0|
|[Rewiring Police Officer Training Networks to Reduce Forecasted Use of Force](https://doi.org/10.1145/3580305.3599899)|Ritika Pandey, Jeremy G. Carter, James Hill, George O. Mohler||Research has shown that police officer involved shootings, misconduct and excessive use of force complaints exhibit network effects, where officers are at greater risk of being involved in these incidents when they socialize with officers who have a history of use of force and misconduct. In this work, we first construct a network survival model for the time-to-event of use of force incidents involving new police trainees. The model includes network effects of the diffusion of risk from field training officer (FTO) to trainee. We then introduce a network rewiring algorithm to maximize the expected time to use of force events upon completion of field training. We study several versions of the algorithm, including constraints that encourage demographic diversity of FTOs. Using data from Indianapolis, we show that rewiring the network can increase the expected time (in days) of a recruit's first use of force incident by 8%. We then discuss the potential benefits and challenges associated with implementing such an algorithm in practice.|研究表明，涉及枪击、不当行为和过度使用武力投诉的警官具有网络效应，当警官与有使用武力和不当行为历史的警官交往时，他们卷入这些事件的风险更大。在本研究中，我们首先建构一个网路生存模式，以适应警队新学员在使用武力事件发生时的情形。该模型考虑了风险从现场培训人员(FTO)向受训人员扩散的网络效应。然后，我们介绍了一个网络重新布线算法，以最大限度地利用现场训练完成后的力量事件的预期时间。我们研究了该算法的几个版本，包括鼓励 FTO 的人口多样性的约束条件。使用来自印第安纳波利斯的数据，我们表明，重新布线网络可以增加预期的时间(在天)新兵的第一次使用武力事件8% 。然后，我们讨论在实践中实现这种算法的潜在好处和挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rewiring+Police+Officer+Training+Networks+to+Reduce+Forecasted+Use+of+Force)|0|
|[Extreme Multi-Label Classification for Ad Targeting using Factorization Machines](https://doi.org/10.1145/3580305.3599822)|Martin Pavlovski, Srinath Ravindran, Djordje Gligorijevic, Shubham Agrawal, Ivan Stojkovic, Nelson SeguraNunez, Jelena Gligorijevic||Applications involving Extreme Multi-Label Classification (XMLC) face several practical challenges with respect to scale, model size and prediction latency, while maintaining satisfactory predictive accuracy. In this paper, we propose a Multi-Label Factorization Machine (MLFM) model, which addresses some of the challenges in XMLC problems. We use behavioral ad targeting as a case study to illustrate the benefits of the MLFM model. Predicting user qualifications for targeting segments plays a major role in both personalization and real-time bidding. Considering the large number of segments and the prediction time requirements of real-world production systems, building scalable models is often difficult and computationally burdensome. To cope with these challenges, we (1) reformulate the problem of assigning users to segments as a multi-label classification (XMLC) problem, and (2) leverage the benefits of the conventional FM model and generalize its capacity to joint prediction across a large number of targeting segments. We have shown that the MLFM model is both effective and computationally efficient compared to several baseline models on publicly available datasets in addition to the targeting use case.|涉及极端多标签分类(XMLC)的应用程序在规模、模型大小和预测延迟方面面临一些实际挑战，同时保持令人满意的预测准确性。在本文中，我们提出了一个多标签分解机(MLFM)模型，它解决了 XMLC 问题中的一些挑战。我们使用行为广告定位作为一个案例研究来说明 MLFM 模型的好处。预测目标市场的用户资格在个性化和实时投标中起着重要作用。考虑到现实生产系统的大量分段和预测时间要求，建立可扩展模型往往比较困难，计算量也比较大。为了应对这些挑战，我们(1)将用户分配到片段的问题重新表述为一个多标签分类(XMLC)问题，(2)利用传统 FM 模型的好处，并将其能力推广到跨大量目标片段的联合预测。我们已经表明，MLFM 模型是有效的和计算效率相比，在公开可用的数据集上除了目标用例的几个基线模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Extreme+Multi-Label+Classification+for+Ad+Targeting+using+Factorization+Machines)|0|
|[un-xPass: Measuring Soccer Player's Creativity](https://doi.org/10.1145/3580305.3599924)|Pieter Robberechts, Maaike Van Roy, Jesse Davis||Creativity is highly valued in soccer players. It contributes to exciting and unpredictable play, which can help teams to overcome defensive strategies and create scoring opportunities. Consequently, evaluating the creative abilities of players is an important aspect of the player recruitment process. However, there is currently no clear way to measure creativity in soccer. It is not captured by the typical result-based performance indicators, as being creative entails going beyond just doing something useful, to accomplishing something useful but in a unique or atypical way. Therefore in this paper, we define a novel metric to quantify the level of creativity involved in a player's passes. Our Creative Decision Rating (CDR) utilizes machine learning techniques to assess two important factors: the originality of a pass, and its value in terms of increasing the team's chances of scoring a goal. We validated our metric on StatsBomb 360 contextual event stream data of the 2021/22 English Premier League season and show through a number of use cases that it provides another angle on a player's skill, complementing existing player evaluation metrics. Overall, our metric provides a concise method for capturing and quantifying the creativity of soccer players and could have important implications for player recruitment and talent development in the sport.|足球运动员非常重视创造力。它有助于激动人心和不可预测的比赛，这可以帮助球队克服防守策略和创造得分机会。因此，评估球员的创造能力是球员招募过程中的一个重要方面。然而，目前还没有明确的方法来衡量创造力在足球。典型的基于结果的业绩指标没有体现这一点，因为创造性意味着超越仅仅做一些有用的事情，而是以一种独特或非典型的方式完成一些有用的事情。因此，在本文中，我们定义了一个新的度量，以量化创造力水平涉及到球员的传球。我们的创造性决策评分(CDR)利用机器学习技术来评估两个重要因素: 传球的原创性，以及它在增加球队进球机会方面的价值。我们在2021/22英超联赛赛季的 StatsBomb 360上下文事件流数据上验证了我们的指标，并通过大量的用例显示它提供了一个球员技能的另一个角度，补充了现有的球员评估指标。总的来说，我们的度量方法提供了一个简明的方法来捕捉和量化的创造力的足球运动员，可以有重要的影响球员招聘和人才发展的体育。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=un-xPass:+Measuring+Soccer+Player's+Creativity)|0|
|[QTNet: Theory-based Queue Length Prediction for Urban Traffic](https://doi.org/10.1145/3580305.3599890)|Ryu Shirakami, Toshiya Kitahara, Koh Takeuchi, Hisashi Kashima||Smart traffic management is the cornerstone of Intelligent Transport Systems (ITS). To achieve smooth travel in urban road networks, ITS provide software-based traffic management based on traffic forecasts. Recently, spatial-temporal graph neural networks (STGNNs) have achieved significant improvements in traffic forecasting by taking into account spatial and temporal dependencies in traffic data. However, in spite of being an indispensable statistic in traffic management in urban areas, the length of congestion queues has not been a prediction target. In addition, existing methods have not considered the use of multimodal traffic data for forecasting. Moreover, given the significant impact of ITS on the real world, black-box predictions with less explainability are unreliable. In this paper, we propose aQueueing-theory-based Neural Network (QTNet), which combines data-driven STGNN methods with queueing-theory-based domain knowledge of traffic engineering in order to achieve accurate and explainable predictions. In our queue length prediction experiments using a real-world dataset collected in urban areas of Tokyo, QTNet outperformed the baseline methods including the state-of-the-art STGNNs by 12.6% in RMSE and 9.9% MAE, and particularly for severe congestion, by 8.1% and 8.4%.|智能交通管理是智能交通系统(ITS)的基石。为了实现城市道路网络的畅通，ITS 提供了基于交通预测的软件交通管理。近年来，时空图形神经网络(STGNN)通过考虑交通数据的时空依赖性，在交通预测方面取得了显著的进展。然而，尽管拥挤排队长度是城市交通管理中不可缺少的统计数据，但它并不是一个预测目标。此外，现有的方法还没有考虑使用多模式交通数据进行预测。此外，考虑到 ITS 对现实世界的重大影响，解释性较差的黑箱预测是不可靠的。本文提出了一种基于排队论的神经网络(QTNet) ，它将数据驱动的 STGNN 方法与基于排队论的交通工程领域知识相结合，以实现准确、可解释的预测。在我们使用在东京市区收集的真实世界数据集进行的队列长度预测实验中，QTNet 在 RMSE 和9.9% MAE 中优于包括最先进的 STGNN 在内的基线方法12.6% ，特别是对于严重的拥堵，分别为8.1% 和8.4% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=QTNet:+Theory-based+Queue+Length+Prediction+for+Urban+Traffic)|0|
|[Deep Transfer Learning for City-scale Cellular Traffic Generation through Urban Knowledge Graph](https://doi.org/10.1145/3580305.3599801)|Shiyuan Zhang, Tong Li, Shuodi Hui, Guangyu Li, Yanping Liang, Li Yu, Depeng Jin, Yong Li||The problem of cellular traffic generation in cities without historical traffic data is critical and urgently needs to be solved to assist 5G base station deployments in mobile networks. In this paper, we propose ADAPTIVE, a deep transfer learning framework for city-scale cellular traffic generation through the urban knowledge graph. ADAPTIVE leverages historical data from other cities that have deployed 5G networks to assist cities that are newly deploying 5G networks through deep transfer learning. Specifically, ADAPTIVE can align the representations of base stations in the target city and source city while considering the environmental factors of cities, spatial and environmental contextual relations between base stations, and traffic temporal patterns at base stations. We next design a feature-enhanced generative adversarial network, which is trained based on the historical traffic data and representations of base stations in the source city. By feeding the aligned target city's base station representations into the trained model, we can then obtain the generated traffic data for the target city. Extensive experiments on real-world cellular traffic datasets show that ADAPTIVE generally outperforms state-of-the-art baselines by more than 40% in terms of Jensen-Shannon divergence and root-mean-square error. Also, ADAPTIVE has strong robustness based on the results of various cross-city experiments. ADAPTIVE has been successfully deployed on the 'Jiutian' Artificial Intelligence Platform of China Mobile to support cellular traffic generation and assist in the construction and operation of mobile networks.|在没有历史交通数据的城市中，手机业务的产生问题是一个亟待解决的问题，以协助5G 基站在移动网络中的部署。本文提出了一种基于城市知识图的城市蜂窝交通生成深度迁移学习框架 ADAPTIVE。ADAPTIVE 利用其他已经部署5G 网络的城市的历史数据，通过深度传输学习来帮助新部署5G 网络的城市。具体来说，ADAPTIVE 可以在考虑城市环境因素、基站之间的空间和环境关系以及基站交通时间模式的同时，对目标城市和源城市的基站表示进行调整。接下来，我们设计了一个特征增强的生成对抗网络，该网络基于历史交通数据和源城市基站的表示进行训练。通过将对齐后的目标城市的基站表示输入到训练后的模型中，我们就可以获得目标城市生成的交通数据。在现实世界的手机流量数据集上进行的大量实验表明，就 Jensen-Shannon 的差异和均方根差而言，ADAPTIVE 通常比最先进的基线表现好40% 以上。同时，基于各种跨城市实验的结果，自适应算法具有很强的鲁棒性。ADAPTIVE 已成功应用于中国移动的“九天”人工智能平台，支持移动通信业务的生成，并协助移动网络的建设和运营。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Transfer+Learning+for+City-scale+Cellular+Traffic+Generation+through+Urban+Knowledge+Graph)|0|
|[Hierarchical Reinforcement Learning for Dynamic Autonomous Vehicle Navigation at Intelligent Intersections](https://doi.org/10.1145/3580305.3599839)|Qian Sun, Le Zhang, Huan Yu, Weijia Zhang, Yu Mei, Hui Xiong||Recent years have witnessed the rapid development of the Cooperative Vehicle Infrastructure System (CVIS), where road infrastructures such as traffic lights (TL) and autonomous vehicles (AVs) can share information among each other and work collaboratively to provide safer and more comfortable transportation experience to human beings. While many efforts have been made to develop efficient and sustainable CVIS solutions, existing approaches on urban intersections heavily rely on domain knowledge and physical assumptions, preventing them from being practically applied. To this end, this paper proposes NavTL, a learning-based framework to jointly control traffic signal plans and autonomous vehicle rerouting in mixed traffic scenarios where human-driven vehicles and AVs co-exist. The objective is to improve travel efficiency and reduce total travel time by minimizing congestion at the intersections while guiding AVs to avoid the temporally congested roads. Specifically, we design a graph-enhanced multi-agent decentralized bi-directional hierarchical reinforcement learning framework by regarding TLs as manager agents and AVs as worker agents. At lower temporal resolution timesteps, each manager sets a goal for the workers within its controlled region. Simultaneously, managers learn to take the signal actions based on the observation from the environment as well as an intention information extracted from its workers. At higher temporal resolution timesteps, each worker makes rerouting decisions along its way to the destination based on its observation from the environment, an intention-enhanced manager state representation, and a goal from its present manager. Finally, extensive experiments on one synthetic and two real-world network-level datasets demonstrate the effectiveness of our proposed framework in terms of improving travel efficiency.|近年来，协同车辆基础设施系统(CVIS)得到了迅速发展，其中道路基础设施，如交通信号灯(TL)和自动驾驶汽车(AVs) ，可以相互分享信息，协同工作，为人类提供更安全、更舒适的交通体验。虽然已经作出了许多努力来制定有效和可持续的城市交叉口信息系统解决方案，但现有的城市交叉口方法严重依赖于领域知识和物理假设，使它们无法得到实际应用。为此，本文提出了基于学习的导航定位框架(NavTL) ，以便在人驾车辆和自动驾驶车辆共存的混合交通情况下，联合控制交通信号规划和无人机重路由。该方法的目的是通过引导 AVs 避开临时拥堵的道路，最大限度地减少交叉口的拥堵，从而提高出行效率，减少总出行时间。具体来说，我们设计了一个图形增强的多代理分散双向层次强化学习框架，将 TL 视为管理者代理，AVs 视为工作者代理。在较低的时间解析度下，每个管理者都为其控制区域内的工人设定一个目标。同时，管理者学会根据从环境中观察到的信号以及从员工身上提取的意图信息来采取信号行动。在更高时间解析度的时间步骤中，每个工作人员根据对环境的观察、意图增强的管理者状态表示以及当前管理者的目标，在前往目的地的途中做出重新路由决策。最后，在一个合成的和两个真实世界的网络级数据集上进行了广泛的实验，证明了我们提出的框架在提高出行效率方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Reinforcement+Learning+for+Dynamic+Autonomous+Vehicle+Navigation+at+Intelligent+Intersections)|0|
|[TrustGeo: Uncertainty-Aware Dynamic Graph Learning for Trustworthy IP Geolocation](https://doi.org/10.1145/3580305.3599920)|Wenxin Tai, Bin Chen, Fan Zhou, Ting Zhong, Goce Trajcevski, Yong Wang, Kai Chen||The rising popularity of online social network services has attracted a lot of research focusing on mining various user patterns. Among them, accurate IP geolocation is essential for a plethora of location-aware applications. However, despite extensive research efforts and significant advances, the "accurate and reliable'' desideratum is yet to be achieved at a higher quality level. This work presents a graph neural network (GNN)-based model, called TrustGeo, for trustworthy street-level IP geolocation. A distinct and important aspect of TrustGeo is the incorporation of sources of uncertainty in the learning process. The results of our extensive experimental evaluations on three real-world datasets demonstrate the superiority of our framework in significantly improving the accuracy and trustworthiness of street-level IP geolocation. Our code and datasets are available at https://github.com/ICDM-UESTC/TrustGeo.|在线社交网络服务的日益普及引起了人们对挖掘各种用户模式的研究。其中，精确的 IP 地理定位对于大量的位置感知应用程序是必不可少的。然而，尽管广泛的研究努力和重大进展，“准确和可靠”的期望尚未实现在更高的质量水平。提出了一种基于图神经网络(GNN)的可信街道级 IP 地理定位模型 TrustGeo。TrustGeo 的一个独特而重要的方面是在学习过程中纳入不确定性的来源。对三个实际数据集的广泛实验评估结果表明，该框架在显著提高街道级 IP 地理定位的准确性和可信度方面具有优越性。我们的代码和数据集 https://github.com/icdm-uestc/trustgeo 可用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TrustGeo:+Uncertainty-Aware+Dynamic+Graph+Learning+for+Trustworthy+IP+Geolocation)|0|
|[Automatic Music Playlist Generation via Simulation-based Reinforcement Learning](https://doi.org/10.1145/3580305.3599777)|Federico Tomasi, Joseph Cauteruccio, Surya Kanoria, Kamil Ciosek, Matteo Rinaldi, Zhenwen Dai||Personalization of playlists is a common feature in music streaming services, but conventional techniques, such as collaborative filtering, rely on explicit assumptions regarding content quality to learn how to make recommendations. Such assumptions often result in misalignment between offline model objectives and online user satisfaction metrics. In this paper, we present a reinforcement learning framework that solves for such limitations by directly optimizing for user satisfaction metrics via the use of a simulated playlist-generation environment. Using this simulator we develop and train a modified Deep Q-Network, the action head DQN (AH-DQN), in a manner that addresses the challenges imposed by the large state and action space of our RL formulation. The resulting policy is capable of making recommendations from large and dynamic sets of candidate items with the expectation of maximizing consumption metrics. We analyze and evaluate agents offline via simulations that use environment models trained on both public and proprietary streaming datasets. We show how these agents lead to better user-satisfaction metrics compared to baseline methods during online A/B tests. Finally, we demonstrate that performance assessments produced from our simulator are strongly correlated with observed online metric results.|个性化播放列表是音乐流媒体服务的一个共同特征，但是传统的技术，如协同过滤，依赖于对内容质量的明确假设来学习如何提出建议。这样的假设经常导致离线模型目标和在线用户满意度指标之间的不一致。在本文中，我们提出了一个强化学习框架，通过使用模拟播放列表生成环境直接优化用户满意度指标，解决了这些限制。使用这个模拟器，我们开发和训练了一个改进的深 Q 网络，行动头 DQN (AH-DQN) ，以一种方式来解决我们的 RL 公式的大的状态和行动空间所带来的挑战。生成的策略能够从大型的、动态的候选项集中提出建议，并期望最大化消费指标。我们通过使用在公共和私有流数据集上训练的环境模型的模拟来分析和评估离线代理。我们展示了在线 A/B 测试期间，与基线方法相比，这些代理如何导致更好的用户满意度指标。最后，我们证明了我们的模拟器产生的性能评估与观察到的在线度量结果密切相关。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automatic+Music+Playlist+Generation+via+Simulation-based+Reinforcement+Learning)|0|
|[Stabilising Job Survival Analysis for Disability Employment Services in Unseen Environments](https://doi.org/10.1145/3580305.3599908)|Ha Xuan Tran, Thuc Duy Le, Jiuyong Li, Lin Liu, Xiaomei Li, Jixue Liu, Tony Waters||In Disability Employment Services (DES), an emerging problem is to make job survival analysis stable in unseen environments without prior knowledge of these environments. Existing survival analysis methods cannot adequately solve this problem since they assume that distribution of unseen data is similar to that observed during training. However, this assumption can be violated in practice where unanticipated events such as COVID19 and inflation can change the work and life patterns of people with disability. Models trained before the COVID19 pandemic may make unreliable job survival predictions in COVID19 or inflation situations. It is also costly and time consuming to frequently re-train and deploy the models. This paper proposes a stable survival analysis method for the DES sector without requiring prior knowledge of deployment environments. Latent representations are learned to capture non-linear relationships between relevant features and job survival time. Two reweighting stages are developed to remove censoring and conditional spurious correlations between irrelevant features and the survival outcome. The case study of Australian workers with disability shows that our method can make stable risk predictions. It can also help workers with disability determine the most effective skills for improvement to increase their job survival time. Further evaluations with public datasets show the promising stable performance of our method in other applications.|在残疾人就业服务(DES) ，一个新出现的问题是使工作生存分析稳定在看不见的环境中，而事先不知道这些环境。现有的生存分析方法不能充分解决这一问题，因为它们假定未见数据的分布与训练期间观察到的分布相似。然而，在诸如 COVID19和通货膨胀等意外事件可能改变残疾人的工作和生活模式的实践中，这一假设可能被违反。在 COVID19大流行之前培训的模型可能在 COVID19或通货膨胀情况下作出不可靠的工作生存预测。频繁地重新训练和部署模型也是昂贵和耗时的。本文提出了一个稳定的生存分析方法的 DES 扇区，无需事先知道部署环境。学习潜在表征来捕捉相关特征和作业生存时间之间的非线性关系。两个重新加权阶段的发展，以消除审查和条件假相关之间的无关特征和生存结果。澳大利亚残疾工人的案例研究表明，我们的方法可以作出稳定的风险预测。它还可以帮助残疾工人确定最有效的改进技能，以增加他们的工作生存时间。对公共数据集的进一步评估表明，我们的方法在其他应用中具有良好的稳定性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Stabilising+Job+Survival+Analysis+for+Disability+Employment+Services+in+Unseen+Environments)|0|
|[Fair Multilingual Vandalism Detection System for Wikipedia](https://doi.org/10.1145/3580305.3599823)|Mykola Trokhymovych, Muniza Aslam, AiJou Chou, Ricardo BaezaYates, Diego SáezTrumper|Wikimedia Foundation; Pompeu Fabra University; EAI, Northeastern University|This paper presents a novel design of the system aimed at supporting the Wikipedia community in addressing vandalism on the platform. To achieve this, we collected a massive dataset of 47 languages, and applied advanced filtering and feature engineering techniques, including multilingual masked language modeling to build the training dataset from human-generated data. The performance of the system was evaluated through comparison with the one used in production in Wikipedia, known as ORES. Our research results in a significant increase in the number of languages covered, making Wikipedia patrolling more efficient to a wider range of communities. Furthermore, our model outperforms ORES, ensuring that the results provided are not only more accurate but also less biased against certain groups of contributors.|本文提出了一个新颖的系统设计，旨在支持维基百科社区解决平台上的破坏行为。为了实现这一目标，我们收集了47种语言的大规模数据集，并应用先进的过滤和特征工程技术，包括多语言掩蔽语言建模，从人工生成的数据中建立训练数据集。该系统的性能是通过与维基百科生产中使用的系统(即 ORES)进行比较来评估的。我们的研究结果显示，维基百科覆盖的语言数量显著增加，使得维基百科对更广泛的社区巡逻更有效率。此外，我们的模型优于 ORES，确保所提供的结果不仅更准确，而且对某些贡献者群体的偏见也更少。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fair+Multilingual+Vandalism+Detection+System+for+Wikipedia)|0|
|[Auto-Validate by-History: Auto-Program Data Quality Constraints to Validate Recurring Data Pipelines](https://doi.org/10.1145/3580305.3599776)|Dezhan Tu, Yeye He, Weiwei Cui, Song Ge, Haidong Zhang, Shi Han, Dongmei Zhang, Surajit Chaudhuri||Data pipelines are widely employed in modern enterprises to power a variety of Machine-Learning (ML) and Business-Intelligence (BI) applications. Crucially, these pipelines are \emph{recurring} (e.g., daily or hourly) in production settings to keep data updated so that ML models can be re-trained regularly, and BI dashboards refreshed frequently. However, data quality (DQ) issues can often creep into recurring pipelines because of upstream schema and data drift over time. As modern enterprises operate thousands of recurring pipelines, today data engineers have to spend substantial efforts to \emph{manually} monitor and resolve DQ issues, as part of their DataOps and MLOps practices. Given the high human cost of managing large-scale pipeline operations, it is imperative that we can \emph{automate} as much as possible. In this work, we propose Auto-Validate-by-History (AVH) that can automatically detect DQ issues in recurring pipelines, leveraging rich statistics from historical executions. We formalize this as an optimization problem, and develop constant-factor approximation algorithms with provable precision guarantees. Extensive evaluations using 2000 production data pipelines at Microsoft demonstrate the effectiveness and efficiency of AVH.|数据管道在现代企业中被广泛应用，为各种机器学习(ML)和商业智能(BI)应用提供动力。至关重要的是，这些管道在生产设置中是重复出现的(例如，每天或每小时) ，以保持数据更新，这样机器学习模型可以定期重新训练，BI 仪表板可以频繁刷新。然而，由于上游模式和数据随时间的变化，数据质量(DQ)问题通常会潜伏到重复出现的管道中。随着现代企业运行数以千计的重复管道，今天的数据工程师不得不花费大量的精力来强化{手动}监视和解决 DQ 问题，作为他们的 DataOps 和 MLOps 实践的一部分。鉴于管理大规模流水线操作的高人力成本，我们必须尽可能地强调{自动化}。在这项工作中，我们提出了基于历史的自动验证(Auto-Valide-by-History，AVH) ，它可以利用历史执行中的丰富统计信息，自动检测重复管道中的 DQ 问题。我们将其形式化为一个最佳化问题，并开发具有可证明的精度保证的常因子近似算法。使用微软2000个生产数据管道进行的广泛评估证明了 AVH 的有效性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Auto-Validate+by-History:+Auto-Program+Data+Quality+Constraints+to+Validate+Recurring+Data+Pipelines)|0|
|[The Missing Indicator Method: From Low to High Dimensions](https://doi.org/10.1145/3580305.3599911)|Mike Van Ness, Tomas M. Bosschieter, Roberto HalpinGregorio, Madeleine Udell|Stanford University; Cornell University|Missing data is common in applied data science, particularly for tabular data sets found in healthcare, social sciences, and natural sciences. Most supervised learning methods only work on complete data, thus requiring preprocessing such as missing value imputation to work on incomplete data sets. However, imputation alone does not encode useful information about the missing values themselves. For data sets with informative missing patterns, the Missing Indicator Method (MIM), which adds indicator variables to indicate the missing pattern, can be used in conjunction with imputation to improve model performance. While commonly used in data science, MIM is surprisingly understudied from an empirical and especially theoretical perspective. In this paper, we show empirically and theoretically that MIM improves performance for informative missing values, and we prove that MIM does not hurt linear models asymptotically for uninformative missing values. Additionally, we find that for high-dimensional data sets with many uninformative indicators, MIM can induce model overfitting and thus test performance. To address this issue, we introduce Selective MIM (SMIM), a novel MIM extension that adds missing indicators only for features that have informative missing patterns. We show empirically that SMIM performs at least as well as MIM in general, and improves MIM for high-dimensional data. Lastly, to demonstrate the utility of MIM on real-world data science tasks, we demonstrate the effectiveness of MIM and SMIM on clinical tasks generated from the MIMIC-III database of electronic health records.|缺少数据在应用数据科学中很常见，特别是在医疗保健、社会科学和自然科学中发现的表格数据集。大多数监督式学习方法只能处理完整的数据，因此需要对不完整的数据集进行预处理，例如缺少值插补。然而，单独的插补并不能编码关于缺失值本身的有用信息。对于具有信息缺失模式的数据集，缺失指示器方法(MIM)添加指示变量来指示缺失模式，可以与插补结合使用来提高模型性能。虽然常用于数据科学，但令人惊讶的是，从经验和特别是理论的角度来研究 MIM。本文从经验和理论上证明了 MIM 改善了信息缺失值的性能，并且证明了对于非信息缺失值，MIM 不会对线性模型造成渐近破坏。此外，我们发现对于含有许多非信息性指标的高维数据集，MIM 会导致模型过拟合，从而测试性能。为了解决这个问题，我们引入了选择性 MIM (Selective MIM，SMIM) ，这是一个新颖的 MIM 扩展，它只为具有信息缺失模式的特性添加缺失指示符。我们的经验表明，SMIM 的性能至少与 MIM 相当，并且对高维数据的 MIM 进行了改进。最后，为了证明 MIM 在现实数据科学任务中的有效性，我们证明了 MIM 和 SMIM 在由 MIMIC-III 电子健康记录数据库生成的临床任务中的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Missing+Indicator+Method:+From+Low+to+High+Dimensions)|0|
|[Interdependent Causal Networks for Root Cause Localization](https://doi.org/10.1145/3580305.3599849)|Dongjie Wang, Zhengzhang Chen, Jingchao Ni, Liang Tong, Zheng Wang, Yanjie Fu, Haifeng Chen||The goal of root cause analysis is to identify the underlying causes of system problems by discovering and analyzing the causal structure from system monitoring data. It is indispensable for maintaining the stability and robustness of large-scale complex systems. Existing methods mainly focus on the construction of a single effective isolated causal network, whereas many real-world systems are complex and exhibit interdependent structures (i.e., multiple networks of a system are interconnected by cross-network links). In interdependent networks, the malfunctioning effects of problematic system entities can propagate to other networks or different levels of system entities. Consequently, ignoring the interdependency results in suboptimal root cause analysis outcomes. In this paper, we propose REASON, a novel framework that enables the automatic discovery of both intra-level (i.e., within-network) and inter-level (i.e., across-network) causal relationships for root cause localization. REASON consists of Topological Causal Discovery (TCD) and Individual Causal Discovery (ICD). The TCD component aims to model the fault propagation in order to trace back to the root causes. To achieve this, we propose novel hierarchical graph neural networks to construct interdependent causal networks by modeling both intra-level and inter-level non-linear causal relations. Based on the learned interdependent causal networks, we then leverage random walk with restarts to model the network propagation of a system fault. The ICD component focuses on capturing abrupt change patterns of a single system entity. This component examines the temporal patterns of each entity's metric data (i.e., time series), and estimates its likelihood of being a root cause based on the Extreme Value theory. Combining the topological and individual causal scores, the top K system entities are identified as root causes. Extensive experiments on three real-world datasets validate the effectiveness of the proposed framework.|根本原因分析的目标是从系统监察数据中发现和分析系统问题的成因结构，从而找出系统问题的根本原因。这对于维持大型复杂系统的稳定性和鲁棒性是必不可少的。现有的方法主要集中在构造一个单一的有效的孤立的因果网络，而现实世界中的许多系统是复杂的并且表现出相互依赖的结构(即，一个系统的多个网络通过交叉网络链接相互连接)。在相互依赖的网络中，有问题的系统实体的故障效应可以传播到其他网络或不同层次的系统实体。因此，忽视相互依赖会导致不理想的根本原因分析结果。在本文中，我们提出了一个新的框架 REASON，它能够自动发现根源定位的层内(即网络内)和层间(即跨网络)因果关系。REASON 包括拓扑因果发现(TCD)和个体因果发现(ICD)。TCD 组件旨在建立故障传播模型，以便追溯到根本原因。为了实现这一目标，我们提出了一种新的分层图形神经网络，通过建立层内和层间的非线性因果关系来构造相互依赖的因果网络。基于所学习的相互依赖因果网络，利用随机游走和重启技术，建立了系统故障的网络传播模型。ICD 组件的重点是捕获单个系统实体的突变模式。该组件检查每个实体的度量数据(即时间序列)的时间模式，并基于极值理论估计其成为根本原因的可能性。结合拓扑和个体因果得分，最高 K 系统实体被确定为根本原因。在三个实际数据集上的大量实验验证了该框架的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interdependent+Causal+Networks+for+Root+Cause+Localization)|0|
|[Learning to Discover Various Simpson's Paradoxes](https://doi.org/10.1145/3580305.3599859)|Jingwei Wang, Jianshan He, Weidi Xu, Ruopeng Li, Wei Chu||Simpson's paradox is a well-known statistical phenomenon that has captured the attention of statisticians, mathematicians, and philosophers for more than a century. The paradox often confuses people when it appears in data, and ignoring it may lead to incorrect decisions. Recent studies have found many examples of Simpson's paradox in social data and proposed a few methods to detect the paradox automatically. However, these methods suffer from many limitations, such as being only suitable for categorical variables or one specific paradox. To address these problems, we develop a learning-based approach to discover various Simpson's paradoxes. Firstly, we propose a framework from a statistical perspective that unifies multiple variants of Simpson's paradox currently known. Secondly, we present a novel loss function, Multi-group Pearson Correlation Coefficient (MPCC), to calculate the association strength of two variables of multiple subgroups. Then, we design a neural network model, coined SimNet, to automatically disaggregate data into multiple subgroups by optimizing the MPCC loss. Experiments on various datasets demonstrate that SimNet can discover various Simpson's paradoxes caused by discrete and continuous variables, even hidden variables. The code is available at https://github.com/ant-research/Learning-to-Discover-Various-Simpson-Paradoxes.|辛普森悖论是一个众所周知的统计学现象，一个多世纪以来一直受到统计学家、数学家和哲学家的关注。当这个悖论出现在数据中时，人们常常感到困惑，忽略它可能会导致错误的决策。近年来的研究发现，社会数据中存在着大量的辛普森悖论实例，并提出了一些自动检测该悖论的方法。然而，这些方法存在许多局限性，如只适用于范畴变量或一个特定的悖论。为了解决这些问题，我们开发了一个基于学习的方法来发现各种辛普森悖论。首先，我们从统计学的角度提出了一个框架，统一了目前已知的辛普森悖论的多个变体。其次，我们提出了一个新的损失函数，多组皮尔逊相关系数(MPCC) ，计算多个子群的两个变量的关联强度。然后，我们设计了一个神经网络模型，称为 SimNet，通过优化 MPCC 损失，自动将数据分解为多个子群。在各种数据集上的实验表明，SimNet 能够发现由离散变量、连续变量甚至隐变量引起的各种辛普森悖论。密码可在 https://github.com/ant-research/learning-to-discover-various-simpson-paradoxes 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Discover+Various+Simpson's+Paradoxes)|0|
|[Removing Camouflage and Revealing Collusion: Leveraging Gang-crime Pattern in Fraudster Detection](https://doi.org/10.1145/3580305.3599895)|Lewen Wang, Haozhe Zhao, Cunguang Feng, Weiqing Liu, Congrui Huang, Marco Santoni, Manuel Cristofaro, Paola Jafrancesco, Jiang Bian||As one of the major threats to the healthy development of various online platforms, fraud has become increasingly committed in the form of gangs since collusive fraudulent activities are much easier to obtain illicit benefits with lower exposure risk. To detect fraudsters in a gang, spatio-temporal graph neural network models have been widely applied to detect both temporal and spatial collusive patterns. However, a closer peek into real-world records of fraudsters can reveal that fraud gangs usually conduct community-level camouflage, specified by two types, i.e., temporal and spatial camouflage. Such camouflage can disguise gangs as benign communities by concealing collusive patterns and thus deceiving many existing graph neural network models. In the meantime, many existing graph neural network models suffer from the challenge of extreme sample imbalance caused by rare fraudsters hidden among massive users. To handle all these challenges, in this paper, we propose a generative adversarial network framework, named Adversarial Camouflage Detector, to detect fraudsters. Concretely, this ACD framework consists of four modules, in charge of community division, camouflage identification, fraudster detection, and camouflage generation, respectively. The first three modules form up a discriminator that uses spatio-temporal graph neural networks as the foundation model and enhance fraudster detection by amplifying the gangs' collusive patterns through automatically identifying and removing camouflage. Meanwhile, the camouflage generation module plays as the generator role that generates fraudsters samples by competing against the discriminator to alleviate the challenge of sample imbalance and increase the model robustness. The experimental result shows that our proposed method outperforms other methods on real-world datasets.|作为对各种在线平台健康发展的主要威胁之一，欺诈越来越多地以团伙的形式进行，因为串通欺诈活动更容易获得非法利益，风险也更低。时空图形神经网络模型被广泛应用于团伙欺诈行为的时间和空间模式检测。然而，更近距离地观察欺诈者的真实记录可以发现，欺诈团伙通常进行社区级别的伪装，具体分为两种类型，即时间和空间伪装。这种伪装可以通过隐藏共谋模式，从而欺骗现有的许多图形神经网络模型，将帮派伪装成良性社区。与此同时，许多现有的图形神经网络模型都面临着隐藏在大量用户中的少数欺诈者造成的极端样本不平衡的挑战。为了应对这些挑战，本文提出了一个生成式对抗性网络框架，称为对抗性伪装检测器，用于检测欺诈者。具体而言，该 ACD 框架由四个模块组成，分别负责社区划分、伪装识别、欺诈者识别和伪装生成。前三个模块构成了以时空图形神经网络为基础模型的识别器，通过自动识别和去伪装放大犯罪团伙的合谋模式，提高了对欺诈者的识别能力。同时，伪装生成模块充当伪装样本生成器的角色，通过与鉴别器的竞争来生成欺诈样本，以减轻样本不平衡的挑战，提高模型的鲁棒性。实验结果表明，该方法在实际数据集上的性能优于其他方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Removing+Camouflage+and+Revealing+Collusion:+Leveraging+Gang-crime+Pattern+in+Fraudster+Detection)|0|
|[ShuttleSet: A Human-Annotated Stroke-Level Singles Dataset for Badminton Tactical Analysis](https://doi.org/10.1145/3580305.3599906)|WeiYao Wang, YungChang Huang, TsiUi Ik, WenChih Peng|National Ying Ming Chiao Tung University|With the recent progress in sports analytics, deep learning approaches have demonstrated the effectiveness of mining insights into players' tactics for improving performance quality and fan engagement. This is attributed to the availability of public ground-truth datasets. While there are a few available datasets for turn-based sports for action detection, these datasets severely lack structured source data and stroke-level records since these require high-cost labeling efforts from domain experts and are hard to detect using automatic techniques. Consequently, the development of artificial intelligence approaches is significantly hindered when existing models are applied to more challenging structured turn-based sequences. In this paper, we present ShuttleSet, the largest publicly-available badminton singles dataset with annotated stroke-level records. It contains 104 sets, 3,685 rallies, and 36,492 strokes in 44 matches between 2018 and 2021 with 27 top-ranking men's singles and women's singles players. ShuttleSet is manually annotated with a computer-aided labeling tool to increase the labeling efficiency and effectiveness of selecting the shot type with a choice of 18 distinct classes, the corresponding hitting locations, and the locations of both players at each stroke. In the experiments, we provide multiple benchmarks (i.e., stroke influence, stroke forecasting, and movement forecasting) with baselines to illustrate the practicability of using ShuttleSet for turn-based analytics, which is expected to stimulate both academic and sports communities. Over the past two years, a visualization platform has been deployed to illustrate the variability of analysis cases from ShuttleSet for coaches to delve into players' tactical preferences with human-interactive interfaces, which was also used by national badminton teams during multiple international high-ranking matches.|随着体育分析的最新进展，深度学习方法已经证明了挖掘运动员战术洞察力对于提高表现质量和球迷参与度的有效性。这归因于公共地面真相数据集的可用性。虽然有一些可用于动作检测的基于回合的运动数据集，但这些数据集严重缺乏结构化源数据和中风水平记录，因为这些数据需要领域专家的高成本标记努力，并且难以使用自动技术检测。因此，当现有模型被应用于更具挑战性的结构化回合序列时，人工智能方法的发展受到严重阻碍。在本文中，我们提出了梭子集，最大的公开可用的羽毛球单打数据集与注释中风水平记录。在2018年至2021年的44场比赛中，共有104盘，3685次拉力赛和36492次击球，其中包括27名顶级男单和女单选手。ShuttleSet 使用计算机辅助标签工具手动注释，以提高标签效率和有效性，选择18个不同类别的击球类型，相应的击球位置，以及每次击球时两名球员的位置。在实验中，我们提供了多个基准(即中风影响，中风预测和运动预测)和基线，以说明使用 ShuttleSet 进行基于回合的分析的实用性，这有望刺激学术和体育社区。在过去的两年中，一个可视化平台已经被部署来说明 ShuttleSet 分析案例的变化，以便教练用人机交互界面深入研究运动员的战术偏好，这也被国家羽毛球队在多个国际高排名比赛中使用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ShuttleSet:+A+Human-Annotated+Stroke-Level+Singles+Dataset+for+Badminton+Tactical+Analysis)|0|
|[VRDU: A Benchmark for Visually-rich Document Understanding](https://doi.org/10.1145/3580305.3599929)|Zilong Wang, Yichao Zhou, Wei Wei, ChenYu Lee, Sandeep Tata||Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry. Although recent multi-modal language models have achieved impressive results, we find that existing benchmarks do not reflect the complexity of real documents seen in industry. In this work, we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding (VRDU). VRDU contains two datasets that represent several challenges: rich schema including diverse data types as well as hierarchical entities, complex templates including tables and multi-column layouts, and diversity of different layouts (templates) within a single document type. We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results. We report the performance of strong baselines and offer three observations: (1) generalizing to new document templates is still very challenging, (2) few-shot performance has a lot of headroom, and (3) models struggle with hierarchical fields such as line-items in an invoice. We plan to open source the benchmark and the evaluation toolkit. We hope this helps the community make progress on these challenging tasks in extracting structured data from visually rich documents.|理解可视化丰富的业务文档以提取结构化数据和实现业务工作流的自动化已经受到学术界和业界的关注。虽然最近的多模态语言模型已经取得了令人印象深刻的成果，但是我们发现现有的基准并不能反映工业中真实文档的复杂性。在这项工作中，我们确定了一个更全面的基准测试所需的数据，并提出了一个我们称为“可视化丰富文档理解(VRDU)”的基准测试。VRDU 包含两个数据集，代表了几个挑战: 丰富的模式包括不同的数据类型以及层次实体，复杂的模板包括表和多列布局，以及单个文档类型中不同布局(模板)的多样性。我们设计了少镜头和常规实验设置以及一个精心设计的匹配算法来评估提取结果。我们报告了强基线的性能，并提供了三点意见: (1)推广到新的文档模板仍然是非常具有挑战性的，(2)少镜头的性能有很大的空间，(3)模型与层次化领域，如发票中的行项目斗争。我们计划开放基准测试和评估工具包的源代码。我们希望这有助于社区在从视觉丰富的文档中提取结构化数据这些具有挑战性的任务上取得进展。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=VRDU:+A+Benchmark+for+Visually-rich+Document+Understanding)|0|
|[DNet: Distributional Network for Distributional Individualized Treatment Effects](https://doi.org/10.1145/3580305.3599809)|Guojun Wu, Ge Song, Xiaoxiang Lv, Shikai Luo, Chengchun Shi, Hongtu Zhu||There is a growing interest in developing methods to estimate individualized treatment effects (ITEs) for various real-world applications, such as e-commerce and public health. This paper presents a novel architecture, called DNet, to infer distributional ITEs. DNet can learn the entire outcome distribution for each treatment, whereas most existing methods primarily focus on the conditional average treatment effect and ignore the conditional variance around its expectation. Additionally, our method excels in settings with heavy-tailed outcomes and outperforms state-of-the-art methods in extensive experiments on benchmark and real-world datasets. DNet has also been successfully deployed in a widely used mobile app with millions of daily active users.|人们越来越感兴趣的是开发方法来估计个体化治疗效果(ITE)的各种现实世界的应用，如电子商务和公共卫生。本文提出了一种新的体系结构，称为 DNet，用于推断分布式 ITE。DNet 可以了解每个治疗的整个结果分布，而大多数现有的方法主要集中在条件平均治疗效果，而忽略了围绕其期望的条件方差。此外，我们的方法在具有重尾结果的设置中表现出色，并且在基准和真实世界数据集的广泛实验中优于最先进的方法。DNet 还成功地部署在一个广泛使用的移动应用程序中，每天有数百万活跃用户。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DNet:+Distributional+Network+for+Distributional+Individualized+Treatment+Effects)|0|
|[A Predict-Then-Optimize Couriers Allocation Framework for Emergency Last-mile Logistics](https://doi.org/10.1145/3580305.3599766)|Kaiwen Xia, Li Lin, Shuai Wang, Haotian Wang, Desheng Zhang, Tian He||In recent years, emergency last-mile logistics (ELML) have played an essential role in urban emergencies. The efficient allocation of couriers in ELML is of practical significance to ensure the supply of essential materials, especially in public health emergencies (PHEs). However, couriers allocation becomes challenging due to the instability of demand, dynamic supply comprehension, and the evolutional delivery environment for ELML caused by PHEs. While existing work has delved into couriers allocation, the impact of PHEs on demand-supply-delivery has yet to be considered. In this work, we design PTOCA, a Predict-Then-Optimize Couriers Allocation framework. Specifically, in the prediction stage, we design a resource-aware prediction module that performs spatio-temporal modeling of unstable demand characteristics using a variational graph GRU encoder and builds a task-resource regressor to predict demand accurately. In the optimization stage, firstly, the priority ranking module solves the matching of delivery resources under demand-supply imbalance. Then the multi-factor task allocation module is used to model the dynamic evolutional environment and reasonably assign the delivery tasks of couriers. We evaluate PTOCA using real-world data covering 170 delivery zones, more than 10,000 couriers, and 100 million delivery tasks. The data is collected from JD Logistics, one of the largest logistics service companies. Extensive experimental results show that our method outperforms the baseline in task delivery rate and on-time delivery rate.|近年来，应急最后一英里物流(ELML)在城市突发事件中发挥了重要作用。快递员的有效配置对于确保基本物资的供应，特别是在突发公共卫生事件中的物资供应具有重要的现实意义。然而，由于需求的不稳定性、供应理解的动态性以及由高等教育引起的 ELML 的演化交付环境，信使分配变得具有挑战性。虽然现有的工作已经深入探讨了送货人的分配问题，但尚未考虑到公共住房对需求-供应-交付的影响。在这项工作中，我们设计了一个预测然后优化的信使分配框架 PTOCA。具体来说，在预测阶段，我们设计了一个资源感知的预测模块，该模块使用变分图 GRU 编码器对不稳定的需求特征进行时空建模，并构建了一个任务资源回归器来准确预测需求。在优化阶段，优先级排序模块首先解决供需不平衡条件下的配送资源匹配问题;。然后利用多因素任务分配模块对动态演化环境进行建模，合理分配送货员的配送任务。我们评估 PTOCA 使用真实世界的数据，涵盖170个送货区，超过10,000名快递员，和1亿个送货任务。这些数据是从最大的物流服务公司之一的京东物流收集的。大量实验结果表明，该方法在任务交付率和准时交付率方面均优于基准。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Predict-Then-Optimize+Couriers+Allocation+Framework+for+Emergency+Last-mile+Logistics)|0|
|[Graph-Aware Language Model Pre-Training on a Large Graph Corpus Can Help Multiple Graph Applications](https://doi.org/10.1145/3580305.3599833)|Han Xie, Da Zheng, Jun Ma, Houyu Zhang, Vassilis N. Ioannidis, Xiang Song, Qing Ping, Sheng Wang, Carl Yang, Yi Xu, Belinda Zeng, Trishul Chilimbi|Amazon Search AI Palo Alto, CA, USA.; Amazon Search AI Santa Clara, CA, USA.; Amazon Scholar Seattle, WA, USA.; Walgreens AI Lab Bellevue, WA, USA.; Emory University Atlanta, GA, USA.; Amazon AWS AI Santa Clara, CA, USA.; Amazon Search AI Seattle, WA, USA.|Model pre-training on large text corpora has been demonstrated effective for various downstream applications in the NLP domain. In the graph mining domain, a similar analogy can be drawn for pre-training graph models on large graphs in the hope of benefiting downstream graph applications, which has also been explored by several recent studies. However, no existing study has ever investigated the pre-training of text plus graph models on large heterogeneous graphs with abundant textual information (a.k.a. large graph corpora) and then fine-tuning the model on different related downstream applications with different graph schemas. To address this problem, we propose a framework of graph-aware language model pre-training (GaLM) on a large graph corpus, which incorporates large language models and graph neural networks, and a variety of fine-tuning methods on downstream applications. We conduct extensive experiments on Amazon's real internal datasets and large public datasets. Comprehensive empirical results and in-depth analysis demonstrate the effectiveness of our proposed methods along with lessons learned.|大文本语料库的模型预训练已被证明在 NLP 领域的各种下游应用中是有效的。在图挖掘领域，可以对大图上的预训练图模型进行类似的类比，以期有利于下游图的应用，最近的一些研究也对此进行了探索。然而，目前还没有研究针对具有丰富文本信息的大型异构图(即大型图语料库)进行文本加图模型的预训练，然后针对不同图模式下的不同相关下游应用对模型进行微调。针对这一问题，本文提出了一种基于大规模图语料库的图感知语言模型预训练(GaLM)框架，该框架结合了大规模语言模型和图神经网络，并针对下游应用提出了多种微调方法。我们对亚马逊的真实内部数据集和大型公共数据集进行了广泛的实验。综合的实证结果和深入的分析证明了我们提出的方法的有效性以及所吸取的教训。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph-Aware+Language+Model+Pre-Training+on+a+Large+Graph+Corpus+Can+Help+Multiple+Graph+Applications)|0|
|[NEON: Living Needs Prediction System in Meituan](https://doi.org/10.1145/3580305.3599874)|Xiaochong Lan, Chen Gao, Shiqi Wen, Xiuqi Chen, Yingge Che, Han Zhang, Huazhou Wei, Hengliang Luo, Yong Li||Living needs refer to the various needs in human's daily lives for survival and well-being, including food, housing, entertainment, etc. On life service platforms that connect users to service providers, such as Meituan, the problem of living needs prediction is fundamental as it helps understand users and boost various downstream applications such as personalized recommendation. However, the problem has not been well explored and is faced with two critical challenges. First, the needs are naturally connected to specific locations and times, suffering from complex impacts from the spatiotemporal context. Second, there is a significant gap between users' actual living needs and their historical records on the platform. To address these two challenges, we design a system of living NEeds predictiON named NEON, consisting of three phases: feature mining, feature fusion, and multi-task prediction. In the feature mining phase, we carefully extract individual-level user features for spatiotemporal modeling, and aggregated-level behavioral features for enriching data, which serve as the basis for addressing two challenges, respectively. Further, in the feature fusion phase, we propose a neural network that effectively fuses two parts of features into the user representation. Moreover, we design a multi-task prediction phase, where the auxiliary task of needs-meeting way prediction can enhance the modeling of spatiotemporal context. Extensive offline evaluations verify that our NEON system can effectively predict users' living needs. Furthermore, we deploy NEON into Meituan's algorithm engine and evaluate how it enhances the three downstream prediction applications, via large-scale online A/B testing.|生活需要是指人类在日常生活中对生存和幸福的各种需要，包括食物、住房、娱乐等。在连接用户与服务提供者(如美团)的生活服务平台上，生活需求预测是一个基本问题，因为它有助于了解用户，推动各种下游应用程序(如个性化推荐)的发展。然而，这个问题还没有得到很好的探讨，并面临着两个关键的挑战。首先，需求自然地与特定的地点和时间相联系，受到时空背景的复杂影响。其次，用户的实际生活需求与他们在平台上的历史记录之间存在显著差距。为了解决这两个问题，我们设计了一个生活需求预测系统 NEON，该系统由特征挖掘、特征融合和多任务预测三个阶段组成。在特征挖掘阶段，我们仔细地提取了用于时空建模的个体级用户特征，以及用于丰富数据的聚合级行为特征，分别作为解决这两个挑战的基础。此外，在特征融合阶段，我们提出了一个神经网络，有效地融合两个部分的特征到用户表示。此外，我们设计了一个多任务预测阶段，其中需求满足方式预测的辅助任务可以增强时空上下文的建模。广泛的离线评估验证了我们的 NEON 系统能够有效地预测用户的生活需求。此外，我们将 NEON 部署到美团的算法引擎中，并通过大规模的在线 A/B 测试，评估它如何增强三个下游预测应用程序。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NEON:+Living+Needs+Prediction+System+in+Meituan)|0|
|[AlerTiger: Deep Learning for AI Model Health Monitoring at LinkedIn](https://doi.org/10.1145/3580305.3599802)|Zhentao Xu, Ruoying Wang, Girish Balaji, Manas Bundele, XiaoFei Liu, Leo Liu, Tie Wang|LinkedIn Corporation|Data-driven companies use AI models extensively to develop products and intelligent business solutions, making the health of these models crucial for business success. Model monitoring and alerting in industries pose unique challenges, including a lack of clear model health metrics definition, label sparsity, and fast model iterations that result in short-lived models and features. As a product, there are also requirements for scalability, generalizability, and explainability. To tackle these challenges, we propose AlerTiger, a deep-learning-based MLOps model monitoring system that helps AI teams across the company monitor their AI models' health by detecting anomalies in models' input features and output score over time. The system consists of four major steps: model statistics generation, deep-learning-based anomaly detection, anomaly post-processing, and user alerting. Our solution generates three categories of statistics to indicate AI model health, offers a two-stage deep anomaly detection solution to address label sparsity and attain the generalizability of monitoring new models, and provides holistic reports for actionable alerts. This approach has been deployed to most of LinkedIn's production AI models for over a year and has identified several model issues that later led to significant business metric gains after fixing.|数据驱动的公司广泛使用人工智能模型来开发产品和智能商业解决方案，这使得这些模型的健康对商业成功至关重要。行业中的模型监控和警报提出了独特的挑战，包括缺乏清晰的模型健康度量定义、标签稀疏性以及导致短寿命模型和特性的快速模型迭代。作为一个产品，还需要可伸缩性、通用性和可解释性。为了应对这些挑战，我们提出了 AlerTiger，这是一个基于深度学习的 MLOps 模型监控系统，通过检测模型的输入特征和输出得分随时间的变化，帮助公司的 AI 团队监控他们 AI 模型的健康状况。该系统包括四个主要步骤: 模型统计生成、基于深度学习的异常检测、异常后处理和用户提醒。我们的解决方案生成三类统计数据以显示人工智能模型的健康状况，提供一个两阶段的深度异常检测解决方案以解决标签稀疏问题和实现监测新模型的普遍性，并为可采取行动的警报提供整体报告。这种方法已经应用于 LinkedIn 的大多数生产 AI 模型超过一年，并确定了几个模型问题，后来导致显着的业务指标收益修复后。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AlerTiger:+Deep+Learning+for+AI+Model+Health+Monitoring+at+LinkedIn)|0|
|[Assisting Clinical Decisions for Scarcely Available Treatment via Disentangled Latent Representation](https://doi.org/10.1145/3580305.3599774)|Bing Xue, Ahmed Sameh Said, Ziqi Xu, Hanyang Liu, Neel Shah, Hanqing Yang, Philip R. O. Payne, Chenyang Lu|Washington University in St. Louis|Extracorporeal membrane oxygenation (ECMO) is an essential life-supporting modality for COVID-19 patients who are refractory to conventional therapies. However, the proper treatment decision has been the subject of significant debate and it remains controversial about who benefits from this scarcely available and technically complex treatment option. To support clinical decisions, it is a critical need to predict the treatment need and the potential treatment and no-treatment responses. Targeting this clinical challenge, we propose Treatment Variational AutoEncoder (TVAE), a novel approach for individualized treatment analysis. TVAE is specifically designed to address the modeling challenges like ECMO with strong treatment selection bias and scarce treatment cases. TVAE conceptualizes the treatment decision as a multi-scale problem. We model a patient's potential treatment assignment and the factual and counterfactual outcomes as part of their intrinsic characteristics that can be represented by a deep latent variable model. The factual and counterfactual prediction errors are alleviated via a reconstruction regularization scheme together with semi-supervision, and the selection bias and the scarcity of treatment cases are mitigated by the disentangled and distribution-matched latent space and the label-balancing generative strategy. We evaluate TVAE on two real-world COVID-19 datasets: an international dataset collected from 1651 hospitals across 63 countries, and a institutional dataset collected from 15 hospitals. The results show that TVAE outperforms state-of-the-art treatment effect models in predicting both the propensity scores and factual outcomes on heterogeneous COVID-19 datasets. Additional experiments also show TVAE outperforms the best existing models in individual treatment effect estimation on the synthesized IHDP benchmark dataset.|对于那些对常规治疗无效的体外膜氧合2019冠状病毒疾病患者来说，体外膜肺氧合(ECMO)是一种重要的生命支持方式。然而，适当的治疗决定一直是重大辩论的主题，关于谁能从这种几乎无法获得和技术上复杂的治疗选择中受益，仍然存在争议。为了支持临床决策，预测治疗需求以及潜在的治疗和不治疗反应是至关重要的。针对这一临床挑战，我们提出了治疗变异自动编码器(TVAE) ，一种个体化治疗分析的新方法。TVAE 是专门设计来解决建模的挑战，如 ECMO 与强大的治疗选择偏倚和稀缺的治疗案例。TVAE 将治疗决策概念化为一个多尺度问题。我们建模一个病人的潜在治疗分配和事实和反事实的结果作为其内在特征的一部分，可以用一个深潜变量模型来表示。通过重构正则化方案和半监督策略，减轻了事实和反事实预测误差，并通过解纠缠和分布匹配潜在空间以及标签平衡生成策略，减轻了选择偏差和治疗案例稀缺性。我们通过两个真实世界的2019冠状病毒疾病数据集来评估 TVAE: 一个是从63个国家的1651家医院收集的国际数据集，另一个是从15家医院收集的机构数据集。结果显示，TVAE 在预测异质2019冠状病毒疾病数据集的倾向得分和实际结果方面优于最先进的治疗效果模型。额外的实验还表明，TVAE 在综合 IHDP 基准数据集的个体治疗效果评估方面优于现有的最佳模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Assisting+Clinical+Decisions+for+Scarcely+Available+Treatment+via+Disentangled+Latent+Representation)|0|
|[Contextual Self-attentive Temporal Point Process for Physical Decommissioning Prediction of Cloud Assets](https://doi.org/10.1145/3580305.3599794)|Fangkai Yang, Jue Zhang, Lu Wang, Bo Qiao, Di Weng, Xiaoting Qin, Gregory Weber, Durgesh Nandini Das, Srinivasan Rakhunathan, Ranganathan Srikanth, Qingwei Lin, Dongmei Zhang||As cloud computing continues to expand globally, the need for effective management of decommissioned cloud assets in data centers becomes increasingly important. This work focuses on predicting the physical decommissioning date of cloud assets as a crucial component in reverse cloud supply chain management and data center warehouse operation. The decommissioning process is modeled as a contextual self-attentive temporal point process, which incorporates contextual information to model sequences with parallel events and provides more accurate predictions with more seen historical data. We conducted extensive offline and online experiments in 20 sampled data centers. The results show that the proposed methodology achieves the best performance compared with baselines and improves remarkable 94% prediction accuracy in online experiments. This modeling methodology can be extended to other domains with similar workflow-like processes.|随着云计算在全球范围内的不断扩展，对数据中心中退役的云资产进行有效管理的必要性变得越来越重要。作为逆向云供应链管理和数据中心仓库运营的重要组成部分，云资产的物理退役日期预测是本文的研究重点。退役过程被建模为一个上下文自我关注的时间点过程，它将上下文信息整合到具有并行事件的序列模型中，并使用更多的历史数据提供更准确的预测。我们在20个样本数据中心进行了大量的离线和在线实验。实验结果表明，与基线相比，该方法取得了较好的预测效果，在线实验的预测精度显著提高了94% 。这种建模方法可以扩展到具有类似工作流过程的其他领域。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contextual+Self-attentive+Temporal+Point+Process+for+Physical+Decommissioning+Prediction+of+Cloud+Assets)|0|
|[From Labels to Decisions: A Mapping-Aware Annotator Model](https://doi.org/10.1145/3580305.3599828)|Evan Yao, Jagdish Ramakrishnan, Xu Chen, VietAn Nguyen, Udi Weinsberg||Online platforms regularly rely on human annotators to make real-time operational decisions for tasks such as content moderation. While crowdsourcing models have been proposed for aggregating noisy labels, they do not generalize well when annotators produce a labels in a large space, e.g., generated from complex review trees. We study a novel crowdsourcing setting with D possible operational decisions or outcomes, but annotators produce labels in a larger space of size L > D which are mapped to decisions through a known mapping function. For content moderation, such labels can correspond to violation reasons (e.g. nudity, violence), while the space of decisions is binary: remove the content or keep it up. In this setting, it is more important to make the right decision rather than estimating the correct underlying label. Existing methods typically separate out the labels to decisions mapping from the modeling of annotators, leading to sub-optimal statistical inference efficiency and excessive computation complexity. We propose a novel confusion matrix model for each annotator that leverages this mapping. Our model is parameterized in a hierarchical manner with both population parameters shared across annotators to model shared confusions and individual parameters to admit heterogeneity among annotators. With extensive numerical experiments, we demonstrate that the proposed model substantially improves accuracy over existing methods and scales well for moderate and large L. In a real-world application on content moderation at Meta, the proposed method offers a 13% improvement in AUC over prior methods, including Meta's existing model in production.|在线平台通常依赖人工注释器为内容审核等任务做出实时操作决策。虽然众包模型被提议用于聚合噪音标签，但是当注释者在一个大空间中生成一个标签(例如，从复杂的评论树生成)时，它们不能很好地推广。我们研究了一个新的众包设置与 D 可能的操作决策或结果，但注释者产生的标签在一个更大的空间大小 L > D 映射到决策通过一个已知的映射函数。对于内容审查，这样的标签可以对应违规原因(如裸体，暴力) ，而空间的决定是二元的: 删除内容或保持它。在这种情况下，更重要的是做出正确的决定，而不是估计正确的基础标签。现有的方法通常将标签和决策映射从注释器的建模中分离出来，导致推论统计学效率不够理想，计算复杂度过高。我们为每个注释者提供一个新的混淆矩阵模型来利用这个映射。我们的模型以分层的方式参数化，在注释者之间共享群体参数来模拟共享的混淆，以及个体参数来承认注释者之间的异质性。通过大量的数值实验，我们证明了所提出的模型比现有的方法和尺度有很大的提高精度，适用于中等大小的 L。在 Meta 内容审核的实际应用中，提出的方法比以前的方法(包括 Meta 现有的生产模型)提供了13% 的 AUC 改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Labels+to+Decisions:+A+Mapping-Aware+Annotator+Model)|0|
|[M3PT: A Multi-Modal Model for POI Tagging](https://doi.org/10.1145/3580305.3599862)|Jingsong Yang, Guanzhou Han, Deqing Yang, Jingping Liu, Yanghua Xiao, Xiang Xu, Baohua Wu, Shenghua Ni|Alibaba Group; East China University of Science and Technology; Fudan University|POI tagging aims to annotate a point of interest (POI) with some informative tags, which facilitates many services related to POIs, including search, recommendation, and so on. Most of the existing solutions neglect the significance of POI images and seldom fuse the textual and visual features of POIs, resulting in suboptimal tagging performance. In this paper, we propose a novel Multi-Modal Model for POI Tagging, namely M3PT, which achieves enhanced POI tagging through fusing the target POI's textual and visual features, and the precise matching between the multi-modal representations. Specifically, we first devise a domain-adaptive image encoder (DIE) to obtain the image embeddings aligned to their gold tags' semantics. Then, in M3PT's text-image fusion module (TIF), the textual and visual representations are fully fused into the POIs' content embeddings for the subsequent matching. In addition, we adopt a contrastive learning strategy to further bridge the gap between the representations of different modalities. To evaluate the tagging models' performance, we have constructed two high-quality POI tagging datasets from the real-world business scenario of Ali Fliggy. Upon the datasets, we conducted the extensive experiments to demonstrate our model's advantage over the baselines of uni-modality and multi-modality, and verify the effectiveness of important components in M3PT, including DIE, TIF and the contrastive learning strategy.|POI 标记的目的是用一些信息性标记来注释兴趣点(POI) ，这有利于许多与 POI 相关的服务，包括搜索、推荐等。现有的解决方案大多忽略了 POI 图像的重要性，很少融合 POI 图像的文本特征和视觉特征，导致标注性能不理想。本文提出了一种新的多模态 POI 标注模型 M3PT，通过融合目标 POI 的文本特征和视觉特征，以及多模态表示之间的精确匹配，实现了 POI 标注的增强。具体来说，我们首先设计一个域自适应图像编码器(DIE)来获得与其金标记语义对齐的图像嵌入。然后，在 M3PT 的文本-图像融合模块(TIF)中，将文本和可视化表示完全融合到 POI 的内容嵌入中，进行后续匹配。此外，我们采用对比学习策略，进一步缩小不同模式表征之间的差距。为了评估标签模型的性能，我们从 Ali Fliggy 的实际业务场景中构建了两个高质量的 POI 标签数据集。在数据集上，我们进行了广泛的实验，以验证我们的模型在单模态和多模态基线上的优势，并验证了 M3PT 中的重要组件，包括 DIE、 TIF 和对比学习策略的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=M3PT:+A+Multi-Modal+Model+for+POI+Tagging)|0|
|[Self-supervised Classification of Clinical Multivariate Time Series using Time Series Dynamics](https://doi.org/10.1145/3580305.3599954)|Yakir Yehuda, Daniel Freedman, Kira Radinsky||To improve the accuracy of clinical multivariate time series (MTS) classification (such as EEG and ECG) by a novel self-supervised paradigm that directly captures the dynamics between the different time series learned together to optimize the classification task. Labels in clinical datasets are very often insufficient. One way to address this challenge is leveraging self-supervision. This paradigm attempts to identify a supervisory signal inherent within a dataset to serve as a surrogate label. We present a novel form of self-supervision: dynamics of clinical MTS. Unlike other self-supervision methods, such as masking, that are intuitive but still heuristic, we suggest to learn a representation justified by Koopman theory. The latter was shown useful for representing clinical time series and can be used as a form of surrogate task to improve the clinical MTS classification. In the ECG task, we show that our proposed framework achieved higher sensitivity and specificity than the state-of-the-art (SOTA) baseline over numerous common diagnoses. For EEG abnormality classification, our proposed framework also achieved higher sensitivity and specificity than the SOTA baseline. All results are statistically significant. Our technique yields reliable clinical diagnosis in an empirical study employing signals from thousands of patients in multiple clinical tasks employing two types of clinical-grade sensors (ECG and EEG) as compared to the state-of-the-art machine learning. Leveraging time-series-dynamics self-supervision can help mitigate the lack of labels in clinical datasets used for training machine learning algorithms and significantly improve their performance. Specifically, the ECG system presented in this work is being trialed in hospitals, used by top cardiologists for patient diagnosis and treatment. We believe that the deployment of such cutting-edge technology will significantly improve the accuracy and speed of cardiac assessments.|为了提高临床多变量时间序列(MTS)分类(如 EEG 和 ECG)的准确性，通过一种新的自我监督范式，直接捕获共同学习的不同时间序列之间的动态，以优化分类任务。临床数据集中的标签通常是不够的。应对这一挑战的一种方法是利用自我监督。这个范例试图识别数据集内部固有的监督信号，以作为代理标签。我们提出了一种新的自我监督形式: 临床 MTS 的动力学。不像其他自我监督的方法，如掩蔽，这是直观的，但仍然是启发式的，我们建议学习一个表示证明库普曼理论。后者显示有用的代表临床时间序列，可作为一种形式的替代任务，以改善临床 MTS 分类。在心电图任务中，我们展示了我们提出的框架在许多常见诊断中取得了比最先进的(SOTA)基线更高的灵敏度和特异度。对于脑电图异常分类，我们提出的框架也获得了比 SOTA 基线更高的灵敏度和特异度。所有的结果都有统计学意义。与最先进的机器学习相比，我们的技术在一项实验研究中产生了可靠的临床诊断，该研究使用了来自数千名患者的信号，采用了两种临床等级传感器(ECG 和 EEG)进行多种临床任务。利用时间序列动力学自我监控可以帮助减少用于训练机器学习算法的临床数据集中缺乏标签的情况，并显著提高其性能。具体来说，在这项工作中提出的心电图系统正在医院进行试验，由顶级心脏病专家用于病人的诊断和治疗。我们相信，这种尖端技术的部署将大大提高心脏评估的准确性和速度。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-supervised+Classification+of+Clinical+Multivariate+Time+Series+using+Time+Series+Dynamics)|0|
|[DGI: An Easy and Efficient Framework for GNN Model Evaluation](https://doi.org/10.1145/3580305.3599805)|Peiqi Yin, Xiao Yan, Jinjing Zhou, Qiang Fu, Zhenkun Cai, James Cheng, Bo Tang, Minjie Wang||While many systems have been developed to train graph neural networks (GNNs), efficient model evaluation, which computes node embedding according to a given model, remains to be addressed. For instance, using the widely adopted node-wise approach, model evaluation can account for over 90% of the time in the end-to-end training process due to neighbor explosion, which means that a node accesses its multi-hop neighbors. The layer-wise approach avoids neighbor explosion by conducting computation layer by layer in GNN models. However, layer-wise model evaluation takes considerable implementation efforts because users need to manually decompose the GNN model into layers, and different implementations are required for GNN models with different structures. In this paper, we present DGI -a framework for easy and efficient GNN model evaluation, which automatically translates the training code of a GNN model for layer-wise evaluation to minimize user effort. DGI is general for different GNN models and evaluation requests (e.g., computing embedding for all or some of the nodes), and supports out-of-core execution on large graphs that cannot fit in CPU memory. Under the hood, DGI traces the computation graph of GNN model, partitions the computation graph into layers that are suitable for layer-wise evaluation according to tailored rules, and executes each layer efficiently by reordering the computation tasks and managing device memory consumption. Experiment results show that DGI matches hand-written implementations of layer-wise evaluation in efficiency and consistently outperforms node-wise evaluation across different datasets and hardware settings, and the speedup can be over 1,000x.|虽然已经开发了许多系统来训练图形神经网络(GNN) ，但是根据给定的模型计算节点嵌入的有效模型评估仍然有待解决。例如，使用广泛采用的节点分析方法，模型评估可以在端到端训练过程中占用90% 以上的时间，由于邻居爆炸，这意味着一个节点访问它的多跳邻居。在 GNN 模型中，采用分层方法逐层进行计算，避免了相邻爆炸。然而，层次模型评估需要大量的实现工作，因为用户需要手动地将 GNN 模型分解成层，并且具有不同结构的 GNN 模型需要不同的实现。本文提出了一个简单有效的 GNN 模型评估框架 DGI，它自动翻译 GNN 模型的训练代码进行分层评估，以最小化用户的工作量。DGI 通用于不同的 GNN 模型和求值请求(例如，对所有或部分节点进行计算嵌入) ，并支持在 CPU 内存无法容纳的大型图表上执行核外操作。DGI 通过对 GNN 模型的计算图进行跟踪，根据裁剪规则将计算图划分为适合分层评估的层次，并通过重新排序计算任务和管理设备内存消耗来有效地执行每一层。实验结果表明，DGI 在效率上与手写的分层评估实现相匹配，并且在不同数据集和硬件设置下的性能一致地优于节点评估，加速比可达1000倍以上。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DGI:+An+Easy+and+Efficient+Framework+for+GNN+Model+Evaluation)|0|
|[Learning Multivariate Hawkes Process via Graph Recurrent Neural Network](https://doi.org/10.1145/3580305.3599857)|Kanghoon Yoon, Youngjun Im, Jingyu Choi, Taehwan Jeong, Jinkyoo Park||This paper presents a novel approach for modeling and predicting patterns of events in time-series learning, named graph recurrent temporal point process (GRTPP). Prior research has focused on using deep learning techniques, such as recurrent neural networks (RNNs) or attention-based sequential data embedding, on modeling the time-varying intensity of events. However, these models were typically limited to modeling a single intensity function capturing the event occurrence of all event types simultaneously. GRTPP addresses this issue by encoding multivariate event sequences into a sequence of graphs, where each node contains information about the event occurrence and time. The sequence of graphs is then embedded into node embeddings for each event type, taking into account the relationships between the event types. By integrating the estimated intensity functions, GRTPP predicts the event type and the timing of the next event. The proposed GRTPP model offers improved effectiveness and explainability compared to previous models, as demonstrated through empirical evaluations on five real-world datasets and the actual credit card transaction dataset. The code is available at https://github.com/im0j/GRTPP https://github.com/im0j/GRTPP.|本文提出了一种新的时间序列学习中事件模式的建模和预测方法——图回归时间点过程(GRTPP)。先前的研究主要集中在使用深度学习技术，如递归神经网络(RNN)或基于注意力的序列数据嵌入，对事件的时变强度进行建模。然而，这些模型通常仅限于建模一个单一的强度函数，捕获同时发生的所有事件类型。GRTPP 通过将多变量事件序列编码为图序列来解决这个问题，其中每个节点包含关于事件发生和时间的信息。然后，考虑到事件类型之间的关系，将图序列嵌入到每个事件类型的节点嵌入中。通过积分估计的强度函数，GRTPP 预测下一个事件的类型和时间。通过对五个实际数据集和实际信用卡交易数据集的实证评估，所提出的 GRTPP 模型与以前的模型相比，提高了有效性和可解释性。密码可在 https://github.com/im0j/grtpp  https://github.com/im0j/grtpp 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Multivariate+Hawkes+Process+via+Graph+Recurrent+Neural+Network)|0|
|[Generating Synergistic Formulaic Alpha Collections via Reinforcement Learning](https://doi.org/10.1145/3580305.3599831)|Shuo Yu, Hongyan Xue, Xiang Ao, Feiyang Pan, Jia He, Dandan Tu, Qing He|Institute of Computing Technology, Chinese Academy of Sciences; Huawei EI Innovation Lab|In the field of quantitative trading, it is common practice to transform raw historical stock data into indicative signals for the market trend. Such signals are called alpha factors. Alphas in formula forms are more interpretable and thus favored by practitioners concerned with risk. In practice, a set of formulaic alphas is often used together for better modeling precision, so we need to find synergistic formulaic alpha sets that work well together. However, most traditional alpha generators mine alphas one by one separately, overlooking the fact that the alphas would be combined later. In this paper, we propose a new alpha-mining framework that prioritizes mining a synergistic set of alphas, i.e., it directly uses the performance of the downstream combination model to optimize the alpha generator. Our framework also leverages the strong exploratory capabilities of reinforcement learning~(RL) to better explore the vast search space of formulaic alphas. The contribution to the combination models' performance is assigned to be the return used in the RL process, driving the alpha generator to find better alphas that improve upon the current set. Experimental evaluations on real-world stock market data demonstrate both the effectiveness and the efficiency of our framework for stock trend forecasting. The investment simulation results show that our framework is able to achieve higher returns compared to previous approaches.|在定量交易领域，将原始历史股票数据转化为市场趋势的指示性信号是一种常见的做法。这种信号被称为阿尔法因子。公式形式的阿尔法更易于解释，因此受到关注风险的从业人员的青睐。在实践中，一组公式化 alpha 通常一起使用以获得更好的建模精度，因此我们需要找到协同的公式化 alpha 集，以便能够很好地协同工作。然而，大多数传统的 alpha 生成器都是分别挖掘一个又一个的 alpha，忽略了以后会合并 alpha 的事实。在本文中，我们提出了一个新的阿尔法挖掘框架，优先挖掘一个协同的阿尔法集，即它直接使用下游组合模型的性能来优化阿尔法生成器。我们的框架还利用了强化学习 ~ (RL)强大的探索能力来更好地探索公式化 alpha 的广阔搜索空间。对组合模型性能的贡献被指定为 RL 过程中使用的返回值，从而驱动 alpha 生成器在当前集合的基础上寻找更好的 alpha。通过对实际股票市场数据的实验评估，验证了本文提出的股票趋势预测框架的有效性和有效性。投资模拟结果表明，与以往的方法相比，我们的框架能够获得更高的收益。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generating+Synergistic+Formulaic+Alpha+Collections+via+Reinforcement+Learning)|0|
|[LibAUC: A Deep Learning Library for X-Risk Optimization](https://doi.org/10.1145/3580305.3599861)|Zhuoning Yuan, Dixian Zhu, ZiHao Qiu, Gang Li, Xuanhui Wang, Tianbao Yang|University of Iowa; Texas AM University; Google Research; Nanjing University|This paper introduces the award-winning deep learning (DL) library called LibAUC for implementing state-of-the-art algorithms towards optimizing a family of risk functions named X-risks. X-risks refer to a family of compositional functions in which the loss function of each data point is defined in a way that contrasts the data point with a large number of others. They have broad applications in AI for solving classical and emerging problems, including but not limited to classification for imbalanced data (CID), learning to rank (LTR), and contrastive learning of representations (CLR). The motivation of developing LibAUC is to address the convergence issues of existing libraries for solving these problems. In particular, existing libraries may not converge or require very large mini-batch sizes in order to attain good performance for these problems, due to the usage of the standard mini-batch technique in the empirical risk minimization (ERM) framework. Our library is for deep X-risk optimization (DXO) that has achieved great success in solving a variety of tasks for CID, LTR and CLR. The contributions of this paper include: (1) It introduces a new mini-batch based pipeline for implementing DXO algorithms, which differs from existing DL pipeline in the design of controlled data samplers and dynamic mini-batch losses; (2) It provides extensive benchmarking experiments for ablation studies and comparison with existing libraries. The LibAUC library features scalable performance for millions of items to be contrasted, faster and better convergence than existing libraries for optimizing X-risks, seamless PyTorch deployment and versatile APIs for various loss optimization. Our library is available to the open source community at https://github.com/Optimization-AI/LibAUC, to facilitate further academic research and industrial applications.|本文介绍了获奖的深度学习(DL)库 LibAUC，该库用于实现最先进的算法，以优化一系列名为 X-risk 的风险函数。X 风险是指一系列复合函数，其中每个数据点的损失函数的定义方式是将该数据点与大量其他数据点进行对比。它们在人工智能中广泛应用于解决经典问题和新兴问题，包括但不限于不平衡数据分类(CID)、学习排序(LTR)和对比表征学习(CLR)。发展 LibAUC 的动机是解决现有图书馆的融合问题，以解决这些问题。特别是，由于在经验风险最小化(ERM)框架中使用了标准的迷你批处理技术，现有的库可能不会收敛或需要非常大的迷你批处理大小来获得良好的性能。我们的库是用于深度 X 风险优化(DXO)的，它在解决 CID、 LTR 和 CLR 的各种任务方面取得了巨大的成功。本文的主要贡献包括: (1)介绍了一种新的基于小批量的 DXO 算法实现流水线，该流水线不同于现有的 DL 流水线，在设计受控数据采样器和动态小批量损失方面有所不同; (2)为烧蚀研究提供了广泛的基准实验，并与现有的数据库进行了比较。LibAUC 库具有可扩展性能，可以对数百万个项目进行对比，比现有库更快、更好地收敛，可以优化 X 风险，可以无缝地部署 PyTorch，还可以使用多种 API 进行各种损失优化。我们的图书馆可供开放源码社区 https://github.com/optimization-ai/libauc 使用，以促进进一步的学术研究和工业应用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LibAUC:+A+Deep+Learning+Library+for+X-Risk+Optimization)|0|
|[Towards a Generic Framework for Mechanism-guided Deep Learning for Manufacturing Applications](https://doi.org/10.1145/3580305.3599913)|Hanbo Zhang, Jiangxin Li, Shen Liang, Peng Wang, Themis Palpanas, Chen Wang, Wei Wang, Haoxuan Zhou, Jianwei Song, Wen Lu||Manufacturing data analytics tasks are traditionally undertaken with Mechanism Models (MMs), which are domain-specific mathematical equations modeling the underlying physical or chemical processes of the tasks. Recently, Deep Learning (DL) has been increasingly applied to manufacturing. MMs and DL have their individual pros and cons, motivating the development of Mechanism-guided Deep Learning Models (MDLMs) that combine the two. Existing MDLMs are often tailored to specific tasks or types of MMs, and can fail to effectively 1) utilize interconnections of multiple input examples, 2) adaptively self-correct prediction errors with error bounding, and 3) ensemble multiple MMs. In this work, we propose a generic, task-agnostic MDLM framework that can embed one or more MMs in deep networks, and address the 3 aforementioned issues. We present 2 diverse use cases where we experimentally demonstrate the effectiveness and efficiency of our models.|制造业数据分析任务传统上是用机制模型(MM)进行的，它是特定领域的数学方程，模拟任务的基本物理或化学过程。近年来，深度学习(DL)在制造业中的应用越来越广泛。MM 和 DL 各有利弊，促进了机制引导的深度学习模型(MDLM)的发展，将两者结合起来。现有的 MDLM 通常针对特定的任务或 MM 类型进行定制，并且可能无法有效地1)利用多个输入示例的互连，2)具有误差边界的自适应自校正预测错误，以及3)集成多个 MM。在这项工作中，我们提出了一个通用的，任务无关的 MDLM 框架，可以嵌入一个或多个深层网络 MM，并解决上述3个问题。我们提出了2个不同的用例，在这些用例中我们实验性地证明了我们的模型的有效性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+a+Generic+Framework+for+Mechanism-guided+Deep+Learning+for+Manufacturing+Applications)|0|
|[GLM-Dialog: Noise-tolerant Pre-training for Knowledge-grounded Dialogue Generation](https://doi.org/10.1145/3580305.3599832)|Jing Zhang, Xiaokang Zhang, Daniel ZhangLi, Jifan Yu, Zijun Yao, Zeyao Ma, Yiqi Xu, Haohua Wang, Xiaohan Zhang, Nianyi Lin, Sunrui Lu, Juanzi Li, Jie Tang|Renmin University of China; ZHIPU.AI; Beijing University of Posts and Telecommunications; Tsinghua University|We present GLM-Dialog, a large-scale language model (LLM) with 10B parameters capable of knowledge-grounded conversation in Chinese using a search engine to access the Internet knowledge. GLM-Dialog offers a series of applicable techniques for exploiting various external knowledge including both helpful and noisy knowledge, enabling the creation of robust knowledge-grounded dialogue LLMs with limited proper datasets. To evaluate the GLM-Dialog more fairly, we also propose a novel evaluation method to allow humans to converse with multiple deployed bots simultaneously and compare their performance implicitly instead of explicitly rating using multidimensional metrics.Comprehensive evaluations from automatic to human perspective demonstrate the advantages of GLM-Dialog comparing with existing open source Chinese dialogue models. We release both the model checkpoint and source code, and also deploy it as a WeChat application to interact with users. We offer our evaluation platform online in an effort to prompt the development of open source models and reliable dialogue evaluation systems. The additional easy-to-use toolkit that consists of short text entity linking, query generation, and helpful knowledge classification is also released to enable diverse applications. All the source code is available on Github.|本文提出了一种大规模语言模型 GLM-Dialog，该模型具有10B 参数，能够通过搜索引擎访问互联网上的知识，实现基于知识的汉语会话。GLM-Dialog 提供了一系列适用的技术来开发各种外部知识，包括有用的和有噪音的知识，使得能够创建具有有限适当数据集的健壮的基于知识的对话 LLM。为了更公平地评估 GLM-Dialog，我们还提出了一种新的评估方法，允许人类同时与多个部署的机器人进行对话，并隐式地比较它们的性能，而不是使用多维度度量进行明确的评估。从自动化到人性化的综合评价体现了 GLM-Dialog 与现有开源中文对话模型相比的优势。我们同时发布了模型检查点和源代码，并将其作为微信应用程序部署，以便与用户进行交互。我们提供在线评估平台，以促进开放源码模型和可靠的对话评估系统的开发。还发布了其他易于使用的工具包，包括短文本实体链接、查询生成和有用的知识分类，以支持不同的应用程序。所有的源代码都可以在 Github 上找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GLM-Dialog:+Noise-tolerant+Pre-training+for+Knowledge-grounded+Dialogue+Generation)|0|
|[Robust Multimodal Failure Detection for Microservice Systems](https://doi.org/10.1145/3580305.3599902)|Chenyu Zhao, Minghua Ma, Zhenyu Zhong, Shenglin Zhang, Zhiyuan Tan, Xiao Xiong, LuLu Yu, Jiayi Feng, Yongqian Sun, Yuzhi Zhang, Dan Pei, Qingwei Lin, Dongmei Zhang|Microsoft; Nankai University; Tsinghua University|Proactive failure detection of instances is vitally essential to microservice systems because an instance failure can propagate to the whole system and degrade the system's performance. Over the years, many single-modal (i.e., metrics, logs, or traces) data-based nomaly detection methods have been proposed. However, they tend to miss a large number of failures and generate numerous false alarms because they ignore the correlation of multimodal data. In this work, we propose AnoFusion, an unsupervised failure detection approach, to proactively detect instance failures through multimodal data for microservice systems. It applies a Graph Transformer Network (GTN) to learn the correlation of the heterogeneous multimodal data and integrates a Graph Attention Network (GAT) with Gated Recurrent Unit (GRU) to address the challenges introduced by dynamically changing multimodal data. We evaluate the performance of AnoFusion through two datasets, demonstrating that it achieves the F1-score of 0.857 and 0.922, respectively, outperforming the state-of-the-art failure detection approaches.|实例的主动故障检测对于微服务系统至关重要，因为实例故障可以传播到整个系统并降低系统的性能。多年来，已经提出了许多基于单模态(即度量、日志或跟踪)数据的异常检测方法。然而，由于它们忽略了多模态数据的相关性，它们往往会错过大量的故障并产生大量的虚警。在这项工作中，我们提出了一种无监督的故障检测方法 AnoFusion，主动检测实例故障通过多模态数据的微服务系统。它利用图形转换网络(GTN)来学习异构多模态数据的相关性，并将图形注意网络(GAT)与门限回归单元(GRU)相结合，以解决多模态数据动态变化所带来的挑战。我们通过两个数据集评估 AnoFusion 的性能，证明其分别达到0.857和0.922的 F1评分，优于最先进的故障检测方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Multimodal+Failure+Detection+for+Microservice+Systems)|0|
|[CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X](https://doi.org/10.1145/3580305.3599790)|Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, Teng Su, Zhilin Yang, Jie Tang||Large pre-trained code generation models, such as OpenAI Codex, can generate syntax-and function-correct code, making the coding of programmers more productive. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 8 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to increase coding efficiency for 83.4% of its users. Finally, CodeGeeX is publicly accessible since Sep. 2022, we open-sourced its code, model weights, API, extensions, and HumanEval-X at https://github.com/THUDM/CodeGeeX.|大型预先训练好的代码生成模型(如 OpenAI Codex)可以生成语法和函数正确的代码，从而提高编程人员的工作效率。在本文中，我们介绍了 CodeGeeX，这是一个具有130亿个代码生成参数的多语言模型。截至2022年6月，CodeGeeX 预先接受了23种编程语言的8,500亿个令牌的培训。我们的大量实验表明，CodeGeeX 在 HumanEval-X 上的代码生成和翻译任务都优于同等规模的多语言代码模型。基于 HumanEval (仅用于 Python) ，我们开发了 HumanEval-X 基准，通过用 C + + 、 Java、 JavaScript 和 Go 手工编写解决方案来评估多语言模型。此外，我们在 Visual Studio Code、 JetBrains 和 Cloud Studio 上构建基于 CodeGeeX 的扩展，每周为成千上万的活跃用户生成80亿令牌。我们的用户研究表明，CodeGeeX 可以帮助83.4% 的用户提高编码效率。最后，CodeGeeX 自2022年9月开始向公众开放，我们开放了它的代码、模型权重、 API、扩展和 HumanEval-X 的 https://github.com/thudm/CodeGeeX。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CodeGeeX:+A+Pre-Trained+Model+for+Code+Generation+with+Multilingual+Benchmarking+on+HumanEval-X)|0|
|[MIDLG: Mutual Information based Dual Level GNN for Transaction Fraud Complaint Verification](https://doi.org/10.1145/3580305.3599865)|Wen Zheng, Bingbing Xu, Emiao Lu, Yang Li, Qi Cao, Xuan Zong, Huawei Shen|Institute of Computing Technology, Chinese Academy of Sciences; Institute of Computing Technology, University of Chinese Academy of Sciences; Wechat Pay, Tencent|"Transaction fraud" complaint verification, i.e., verifying whether a transaction corresponding to a complaint is fraudulent, is particularly critical to prevent economic loss. Compared with traditional fraud pre-transaction detection, complaint verification puts forward higher requirements: 1)an individual tends to exhibit different identities in different complaints, e.g., complainant or respondent, requiring the model to capture identity-related representations corresponding to the complaint; 2)the fraud ways evolve frequently to confront detection, requiring the model to perform stably under different fraud ways. Previous methods mainly focused on fraud pre-transaction detection, utilizing the historical information of users or conduct message passing based GNNs on relationship networks. However, they rarely consider capturing various identity-related representations and ignore the evolution of fraud ways, leading to failure in complaint verification. To address the above challenges, we propose the mutual information based dual level graph neural network, namely MIDLG, which defines a complaint as a super-node consisting of involved individuals, and characterizes the individual over node-level and super-node-level. Furthermore, the mutual information minimization objective is proposed based on "complaint verification-causal graph" to decouple the model prediction from relying on specific fraud ways, and thus achieve stability. MIDLG achieves SOTA results through extensive experiments in complaint verification on WeChat Finance, one online payment service serving more than 600 million users in China.|“交易欺诈”投诉核实，即核实与投诉相应的交易是否属于欺诈，对于防止经济损失尤为重要。与传统的交易前欺诈检测相比，投诉验证提出了更高的要求: 1)在不同的投诉中，个体倾向于表现出不同的身份，如投诉人或被投诉人，要求模型捕捉与投诉相应的身份相关表征; 2)欺诈方式频繁演变以面对检测，要求模型在不同的欺诈方式下表现稳定。以往的方法主要集中在欺诈的交易前检测，利用用户的历史信息或进行信息传递的 GNN 的关系网络。然而，他们很少考虑捕获各种与身份相关的表示，并忽视欺诈方式的演变，导致投诉验证失败。为了应对上述挑战，我们提出了基于互信息的双层图形神经网络，即 MIDLG，它将投诉定义为一个由相关个体组成的超级节点，并在节点级和超级节点级表征个体。在此基础上，提出了基于“投诉验证-因果图”的互信息最小化目标，使模型预测与特定的欺诈方式解耦，从而实现稳定性。MIDLG 通过在微信金融上进行广泛的投诉验证实验，取得了 SOTA 结果。微信金融是一家为中国6亿多用户服务的在线支付服务公司。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MIDLG:+Mutual+Information+based+Dual+Level+GNN+for+Transaction+Fraud+Complaint+Verification)|0|
|[Road Planning for Slums via Deep Reinforcement Learning](https://doi.org/10.1145/3580305.3599901)|Yu Zheng, Hongyuan Su, Jingtao Ding, Depeng Jin, Yong Li|Department of Electronic Engineering, BNRist, Tsinghua University|Millions of slum dwellers suffer from poor accessibility to urban services due to inadequate road infrastructure within slums, and road planning for slums is critical to the sustainable development of cities. Existing re-blocking or heuristic methods are either time-consuming which cannot generalize to different slums, or yield sub-optimal road plans in terms of accessibility and construction costs. In this paper, we present a deep reinforcement learning based approach to automatically layout roads for slums. We propose a generic graph model to capture the topological structure of a slum, and devise a novel graph neural network to select locations for the planned roads. Through masked policy optimization, our model can generate road plans that connect places in a slum at minimal construction costs. Extensive experiments on real-world slums in different countries verify the effectiveness of our model, which can significantly improve accessibility by 14.3% against existing baseline methods. Further investigations on transferring across different tasks demonstrate that our model can master road planning skills in simple scenarios and adapt them to much more complicated ones, indicating the potential of applying our model in real-world slum upgrading.|由于贫民窟内道路基础设施不足，数百万贫民窟居民难以获得城市服务，贫民窟道路规划对于城市的可持续发展至关重要。现有的重新封锁或启发式方法要么耗费时间，无法推广到不同的贫民窟，要么在可达性和建设成本方面产生次优的道路规划。在这篇文章中，我们提出了一个基于深度强化学习的方法来自动规划贫民窟的道路。我们提出了一个通用的图模型来捕捉贫民窟的拓扑结构，并设计了一个新颖的图神经网络来选择规划道路的位置。通过隐蔽的政策优化，我们的模型可以生成道路计划，以最低的建设成本连接贫民窟的地方。在不同国家对现实世界中的贫民窟进行的大量实验验证了我们模型的有效性，与现有的基准方法相比，该模型可以显著提高14.3% 的可达性。关于跨不同任务转移的进一步调查表明，我们的模型能够在简单的情景中掌握道路规划技能，并使之适应更为复杂的情景，这表明我们的模型在现实世界贫民窟改造中的应用潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Road+Planning+for+Slums+via+Deep+Reinforcement+Learning)|0|
|[AI Explainability 360 Toolkit for Time-Series and Industrial Use Cases](https://doi.org/10.1145/3580305.3599182)|Giridhar Ganapavarapu, Sumanta Mukherjee, Natalia Martinez Gil, Kanthi K. Sarpatwar, Amaresh Rajasekharan, Amit Dhurandhar, Vijay Arya, Roman Vaculín|IBM Research, Bengaluru, India; IBM Research, Gurugram, India; IBM Research, Yorktown Heights, NY, USA; IBM Software, Poughkeepsie, NY, USA|With the growing adoption of AI, trust and explainability have become critical which has attracted a lot of research attention over the past decade and has led to the development of many popular AI explainability libraries such as AIX360, Alibi, OmniXAI, etc. Despite that, applying explainability techniques in practice often poses challenges such as lack of consistency between explainers, semantically incorrect explanations, or scalability. Furthermore, one of the key modalities that has been less explored, both from the algorithmic and practice point of view, is time-series. Several application domains involve time-series including Industry 4.0, asset monitoring, supply chain or finance to name a few. The AIX360 library (https://github.com/Trusted-AI/AIX360) has been incubated by the Linux Foundation AI & Data open-source projects and it has gained significant popularity: its public GitHub repository has over 1.3K stars and has been broadly adopted in the academic and applied settings. Motivated by industrial applications, large scale client projects and deployments in software products in the areas of IoT, asset management or supply chain, the AIX360 library has been recently expanded significantly to address the above challenges. AIX360 now includes new techniques including support for time-series modality introducing time series based explainers such as TS-LIME, TS Saliency explainer, TS-ICE and TS-SHAP. It also introduces improvements in generating model agnostic, consistent, diverse, and scalable explanations, and new algorithms for tabular data. In this hands-on tutorial, we provide an overview of the library with the focus on the latest additions, time series explainers and use cases such as forecasting, time series anomaly detection or classification, and hands-on demonstrations based on industrial use-cases selected to demonstrate practical challenges and how they are addressed. The audience will be able to evaluate different types of explanations with a focus on practical aspects motivated by real deployments.|随着人工智能的日益普及，信任和可解释性已经成为一个关键问题，近十年来引起了人们的广泛关注，并导致了 AIX360、 Alibi、 OmniXAI 等许多流行的人工智能可解释性库的发展。尽管如此，在实践中应用可解释性技术往往会带来诸如解释者之间缺乏一致性、语义不正确的解释或可伸缩性等挑战。此外，从算法和实践的角度来看，时间序列是较少被探索的关键模式之一。有几个应用领域涉及时间序列，包括行业4.0、资产监控、供应链或金融等等。AIx360库( https://GitHub.com/trusted-AI/AIX360)是由 Linux 基金会的人工智能和数据开源项目孵化出来的，它已经变得非常受欢迎: 它的公共 GitHub 库已经超过1.3 k 星级，已经被学术界和应用界广泛采用。受到工业应用、大规模客户项目以及物联网、资产管理或供应链领域软件产品部署的推动，AIX360库最近得到了显著扩展，以应对上述挑战。AIX360现在包括新的技术，包括支持时间序列模态引入基于时间序列的解释器，如 TS-LIME，TS 显著性解释器，TS-ICE 和 TS-SHAP。它还介绍了在生成模型不可知性、一致性、多样性和可扩展性解释方面的改进，以及表格数据的新算法。在这个实践教程中，我们提供了一个图书馆的概述，重点是最新的补充，时间序列解释和用例，如预测，时间序列异常检测或分类，以及基于工业用例的实践演示，选择了展示实际挑战和如何解决这些挑战。听众将能够评估不同类型的解释，重点放在由实际部署引起的实际方面。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AI+Explainability+360+Toolkit+for+Time-Series+and+Industrial+Use+Cases)|0|
|[Hands-on Tutorial: "Explanations in AI: Methods, Stakeholders and Pitfalls"](https://doi.org/10.1145/3580305.3599181)|Mia C. Mayer, Muhammad Bilal Zafar, Luca Franceschi, Huzefa Rangwala|Amazon Web Services, Arlington, VA, USA; Amazon Web Services, Berlin, Germany; Amazon Web Services, Seattle, WA, USA|While using vast amounts of training data and sophisticated models has enhanced the predictive performance of Machine Learning (ML) and Artificial Intelligence (AI) solutions, it has also led to an increased difficulty in comprehending their predictions. The ability to explain predictions is often one of the primary desiderata for adopting AI and ML solutions [6, 13]. The desire for explainability has led to a rapidly growing body of literature on explainable AI (XAI) and has also resulted in the development of hundreds of XAI methods targeting different domains (e.g., finance, healthcare), applications (e.g., model debugging, actionable recourse), data modalities (e.g., tabular data, images), models (e.g., transformers, convolutional neural networks) and stakeholders (e.g., end-users, regulatory authorities, data scientists). The goal of this tutorial is to present a comprehensive overview of the XAI field to the participants. As a hands-on tutorial, we will showcase state-of-the-art methods that can be used for different data modalities and contexts to extract the right abstractions for interpretation. We will also cover common pitfalls when using explanations, e.g., misrepresentation, and lack of robustness of explanations.|虽然使用大量的训练数据和复杂的模型提高了机器学习(ML)和人工智能(AI)解决方案的预测性能，但它也导致理解它们的预测变得更加困难。解释预测的能力通常是采用人工智能和机器学习解决方案的首要条件之一[6,13]。对可解释性的渴望导致了关于可解释 AI (XAI)的文献的迅速增长，并且还导致了针对不同领域(例如，金融，医疗) ，应用(例如，模型调试，可操作的追索权) ，数据模式(例如，表格数据，图像) ，模型(例如，变压器，卷积神经网络)和利益相关者(例如，最终用户，监管当局，数据科学家)的数百种 XAI 方法的发展。本教程的目标是向参与者展示 XAI 字段的全面概述。作为一个实践教程，我们将展示最先进的方法，这些方法可用于不同的数据模式和上下文，以提取正确的抽象进行解释。我们也会介绍使用解释时常见的缺陷，例如不正当手法引诱，以及缺乏可靠的解释。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hands-on+Tutorial:+"Explanations+in+AI:+Methods,+Stakeholders+and+Pitfalls")|0|
|[Graph Neural Networks in TensorFlow](https://doi.org/10.1145/3580305.3599177)|Bryan Perozzi, Sami AbuElHaija, Anton Tsitsulin|Electronics, Information, and Bioengineering, Politecnico di Milano, Milan, Italy; Informatics, Universita della Svizzera Italiana, Lugano, Switzerland|In this paper we present Spektral, an open-source Python library for building graph neural networks with TensorFlow and the Keras application programming interface. Spektral implements a large set of methods for deep learning on graphs, including message-passing and pooling operators, as well as utilities for processing graphs and loading popular benchmark datasets. The purpose of this library is to provide the essential building blocks for creating graph neural networks, focusing on the guiding principles of user-friendliness and quick prototyping on which Keras is based. Spektral is, therefore, suitable for absolute beginners and expert deep learning practitioners alike. In this work, we present an overview of Spektral's features and report the performance of the methods implemented by the library in scenarios of node classification, graph classification, and graph regression.|本文介绍了 Spektral，这是一个开源的 Python 库，用于构建具有 TensorFlow 和 Kera 应用程序编程接口的图形神经网络。Spektral 实现了大量用于图表深度学习的方法，包括消息传递和池操作符，以及用于处理图表和加载流行基准数据集的实用程序。这个图书馆的目的是为建立图形神经网络提供基本的组成部分，着重于方便用户和快速原型的指导原则，这是 Keras 的基础。因此，Spektral 适合于绝对初学者和专家深入学习实践者一样。在这项工作中，我们提出了 Spektral 的特点概述，并报告了该库实现的方法在节点分类、图分类和图回归场景中的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Neural+Networks+in+TensorFlow)|0|
|[PyHealth: A Deep Learning Toolkit for Healthcare Applications](https://doi.org/10.1145/3580305.3599178)|Chaoqi Yang, Zhenbang Wu, Patrick Jiang, Zhen Lin, Junyi Gao, Benjamin P. Danek, Jimeng Sun|University of Edinburgh, Health Data Research UK, Edinburgh, United Kingdom; University of Illinois Urbana-Champaign, Urbana, IL, USA|Deep learning (DL) has emerged as a promising tool in healthcare applications. However, the reproducibility of many studies in this field is limited by the lack of accessible code implementations and standard benchmarks. To address the issue, we create PyHealth, a comprehensive library to build, deploy, and validate DL pipelines for healthcare applications. PyHealth supports various data modalities, including electronic health records (EHRs), physiological signals, medical images, and clinical text. It offers various advanced DL models and maintains comprehensive medical knowledge systems. The library is designed to support both DL researchers and clinical data scientists. Upon the time of writing, PyHealth has received 633 stars, 130 forks, and 15k+ downloads in total on GitHub. This tutorial will provide an overview of PyHealth, present different modules, and showcase their functionality through hands-on demos. Participants can follow along and gain hands-on experience on the Google Colab platform during the session.|深度学习(DL)已经成为医疗保健应用中一个很有前途的工具。然而，由于缺乏可访问的代码实现和标准基准，该领域的许多研究的可重复性受到了限制。为了解决这个问题，我们创建了 PyHealth，这是一个用于为医疗保健应用程序构建、部署和验证 DL 管道的综合库。PyHealth 支持各种数据模式，包括电子健康记录(EHR)、生理信号、医学图像和临床文本。它提供各种先进的 DL 模型，并维护全面的医学知识体系。该图书馆旨在支持 DL 研究人员和临床数据科学家。在撰写本文之时，PyHealth 已经在 GitHub 上获得了633颗星、130个 fork 和总共15k + 的下载。本教程将提供 PyHealth 的概述，展示不同的模块，并通过实践演示展示它们的功能。与会者可以跟随并在会议期间在 GoogleColab 平台上获得实践经验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PyHealth:+A+Deep+Learning+Toolkit+for+Healthcare+Applications)|0|
|[GraphStorm an Easy-to-use and Scalable Graph Neural Network Framework: From Beginners to Heroes](https://doi.org/10.1145/3580305.3599179)|Jian Zhang, Da Zheng, Xiang Song, Theodore Vasiloudis, Israt Nisa, Jim Lu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphStorm+an+Easy-to-use+and+Scalable+Graph+Neural+Network+Framework:+From+Beginners+to+Heroes)|0|
|[Towards Next-Generation Intelligent Assistants Leveraging LLM Techniques](https://doi.org/10.1145/3580305.3599572)|Xin Luna Dong, Seungwhan Moon, Yifan Ethan Xu, Kshitiz Malik, Zhou Yu|Columbia University, New York City, NY, USA; Meta Reality Labs, Menlo Park, CA, USA; Meta Reality Labs, Austin, TX, USA; Meta Reality Labs, Redmond, WA, USA|Virtual Intelligent Assistants take user requests in the voice form, perform actions such as setting an alarm, turning on a light, and answering a question, and provide answers or confirmations in the voice form or through other channels such as a screen. Assistants have become prevalent in the past decade, and users have been taking services from assistants like Amazon Alexa, Apple Siri, Google Assistant, and Microsoft Cortana. The emergence of AR/VR devices raised many new challenges for building intelligent assistants. The unique requirements have inspired new research directions such as (a) understanding users' situated multi-modal contexts (e.g. vision, sensor signals) as well as language-oriented conversational contexts, (b) personalizing the assistant services by grounding interactions on growing public and personal knowledge graphs and online search engines, and (c) on- device model inference and training techniques that satisfy strict resource and privacy constraints. In this tutorial, we will provide an in-depth walk-through of techniques in the afore-mentioned areas in the recent literature. We aim to introduce techniques for researchers and practitioners who are building intelligent assistants, and inspire research that will bring us one step closer to realizing the dream of building an all-day accompanying assistant. Additionally, we will highlight the significant role that Large Language Models (LLMs) play in enhancing these strategies, underscoring their potential to reshape the future landscape of intelligent assistance.|虚拟智能助理以语音形式接受用户请求，执行诸如设置闹钟、打开灯光和回答问题等动作，并以语音形式或通过屏幕等其他渠道提供答案或确认。在过去的十年里，助理服务变得越来越普遍，用户使用的服务来自亚马逊的 Alexa、苹果的 Siri、谷歌助理和 Cortana。AR/VR 设备的出现为构建智能助手提出了许多新的挑战。独特的需求激发了新的研究方向，例如(a)理解用户所处的多模态环境(例如视觉，传感器信号)以及面向语言的会话环境，(b)通过基于不断增长的公共和个人知识图表和在线搜索引擎的交互使辅助服务个性化，以及(c)满足严格的资源和隐私约束的设备上模型推断和培训技术。在本教程中，我们将在最近的文献中提供上述领域中的技术的深入演练。我们的目标是向研究人员和从业人员介绍建立智能助手的技术，并激发研究，将使我们更接近实现建立一个全天陪伴助手的梦想。此外，我们将强调大型语言模型(LLM)在增强这些策略方面发挥的重要作用，强调它们重塑未来智能辅助领域的潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Next-Generation+Intelligent+Assistants+Leveraging+LLM+Techniques)|0|
|[XAI for Predictive Maintenance](https://doi.org/10.1145/3580305.3599578)|João Gama, Slawomir Nowaczyk, Sepideh Pashami, Rita P. Ribeiro, Grzegorz J. Nalepa, Bruno Veloso|Univ N Carolina, Dept Elect & Comp Engn, Charlotte, NC 28223 USA; Old Dominion Univ, Dept Engn Technol, Norfolk, VA 23529 USA; Norwegian Univ Sci & Technol, Dept Elect Power Engn, N-7491 Trondheim, Norway; eKare Inc, Fairfax, VA 22031 USA|Over the last two decades, Artificial Intelligence (AI) approaches have been applied to various applications of the smart grid, such as demand response, predictive maintenance, and load forecasting. However, AI is still considered to be a "black-box" due to its lack of explainability and transparency, especially for something like solar photovoltaic (PV) forecasts that involves many parameters. Explainable Artificial Intelligence (XAI) has become an emerging research field in the smart grid domain since it addresses this gap and helps understand why the AI system made a forecast decision. This article presents several use cases of solar PV energy forecasting using XAI tools, such as LIME, SHAP, and ELI5, which can contribute to adopting XAI tools for smart grid applications. Understanding the inner workings of a prediction model based on AI can give insights into the application field. Such insight can provide improvements to the solar PV forecasting models and point out relevant parameters.|在过去的二十年中，人工智能(AI)方法已经被应用到智能电网的各种应用中，如需求响应、预测维护和负荷预测。然而，人工智能仍然被认为是一个“黑盒子”，因为它缺乏解释性和透明度，特别是对于像太阳能光伏(PV)预测这样涉及许多参数。可解释人工智能(XAI)是智能电网领域的一个新兴研究领域，它弥补了这一空白，有助于理解为什么人工智能系统会做出预测决策。本文介绍了使用 XAI 工具(如 LIME、 SHAP 和 ELI5)进行太阳能光伏发电量预测的几个用例，这些工具有助于将 XAI 工具用于智能电网应用。了解基于人工智能的预测模型的内部工作原理，可以深入了解其应用领域。这种洞察力可以为太阳能光伏预测模型提供改进，并指出相关参数。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=XAI+for+Predictive+Maintenance)|0|
|[Distributed Optimization for Big Data Analytics: Beyond Minimization](https://doi.org/10.1145/3580305.3599554)|Hongchang Gao, Xinwen Zhang|Temple University, Philadelphia, PA, USA|The traditional machine learning model can be formulated as an empirical risk minimization problem, which is typically optimized via stochastic gradient descent (SGD). With the emergence of big data, distributed optimization, e.g., distributed SGD, has been attracting increasing attention to facilitate machine learning models for big data analytics. However, existing distributed optimization mainly focuses on the standard empirical risk minimization problem, failing to deal with the emerging machine learning models that are beyond that category. Thus, of particular interest of this tutorial includes the stochastic minimax optimization, stochastic bilevel optimization, and stochastic compositional optimization, which covers a wide range of emerging machine learning models, e.g., model-agnostic meta-learning models, adversarially robust machine learning models, imbalanced data classification models, etc. Since these models have been widely used in big data analytics, it is necessary to provide a comprehensive introduction about the new distributed optimization algorithms designed for these models. Therefore, the goal of this tutorial is to present the state-of-the-art and recent advances in distributed minimax optimization, distributed bilevel optimization, and distributed compositional optimization. In particular, we will introduce the typical applications in each category and discuss the corresponding distributed optimization algorithms in both centralized and decentralized settings. Through this tutorial, the researchers will be exposed to the fundamental algorithmic design and basic convergence theories, and the practitioners will be able to benefit from this tutorial to apply these algorithms to real-world data mining applications.|传统的机器学习模型可以表述为一个经验风险最小化问题，通常通过随机梯度下降(SGD)进行优化。随着大数据的出现，分布式优化(如分布式 SGD)越来越受到人们的关注，以方便大数据分析的机器学习模型的建立。然而，现有的分布式优化主要集中在标准的经验风险最小化问题，未能处理新兴的机器学习模型超出这一范畴。因此，本教程特别感兴趣的包括随机极大极小优化，随机双层优化和随机组合优化，其中涵盖了广泛的新兴机器学习模型，如模型无关元学习模型，对抗鲁棒性机器学习模型，不平衡数据分类模型等。由于这些模型在大数据分析中得到了广泛的应用，因此有必要对为这些模型设计的新的分布式优化算法进行全面的介绍。因此，本教程的目标是介绍分布式极大极小优化、分布式两级优化和分布式组合优化方面的最新进展。特别地，我们将介绍每个类别中的典型应用，并讨论相应的集中和分散环境下的分布式优化算法。通过本教程，研究人员将了解基本的算法设计和基本收敛理论，从业人员将能够从本教程中受益，将这些算法应用到现实世界的数据挖掘应用程序。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distributed+Optimization+for+Big+Data+Analytics:+Beyond+Minimization)|0|
|[Privacy in Advertising: Analytics and Modeling](https://doi.org/10.1145/3580305.3599570)|Badih Ghazi, Ravi Kumar, Pasin Manurangsi|Google, Mountain View, CA, USA; Google, Bangkok, Thailand|Privacy in general, and differential privacy (DP) in particular, have become important topics in data mining and machine learning. Digital advertising is a critical component of the internet and is powered by large-scale data analytics and machine learning models; privacy concerns around these are on the rise. Despite the central importance of private ad analytics and training privacy-preserving ad prediction models, there has been relatively little exposure of this subject to the broader KDD community. In the past three years, the interest in privacy and the interest in online advertising have been steadily growing in KDD. The aim of this tutorial is to provide KDD researchers with an introduction to the problems that arise in private analytics and modeling in advertising, survey recent results, and describe the main research challenges in the space.|一般而言，私隐，特别是差分隐私(DP) ，已成为数据挖掘和机器学习的重要课题。数字广告是互联网的重要组成部分，由大规模的数据分析和机器学习模型提供动力; 围绕这些的隐私问题正在上升。尽管私人广告分析和培训保护隐私的广告预测模型至关重要，但是这个主题在更广泛的 KDD 社区中的曝光率相对较低。在过去的三年中，KDD 对隐私和在线广告的兴趣一直在稳步增长。本教程的目的是为 KDD 研究人员提供一个介绍在私人分析和广告建模中出现的问题，调查最近的结果，并描述在空间中的主要研究挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Privacy+in+Advertising:+Analytics+and+Modeling)|0|
|[Causal Discovery from Temporal Data](https://doi.org/10.1145/3580305.3599552)|Chang Gong, Di Yao, Chuzhe Zhang, Wenbin Li, Jingping Bi, Lun Du, Jin Wang|Institute of Computing Technology, Chinese Academy of Sciences & University of Chinese Academy of Sciences, Beijing, China; Microsoft, Beijing, China; Fudan University, Shanghai, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Megagon Labs, Mountain View, CA, USA|Temporal data representing chronological observations of complex systems can be ubiquitously collected in smart industry, medicine, finance and etc. In the last decade, many tasks have been studied for mining temporal data and offered significant value for various applications. Among these tasks, causal discovery aims to understand the underlying generation mechanism of temporal data and has attracted much research attention. According to whether the data is calibrated, existing causal discovery approaches can be divided into two subtasks, i.e., multivariate time-series causal discovery, and event sequence causal discovery. Previous tutorials or surveys have primarily focused on causal discovery from time-series data and disregarded the second ones. In this tutorial, we elucidate the correlation between the two subtasks and provide a comprehensive review of the existing solutions. Moreover, we offer some potential applications and summarize new perspectives for discovering causal relations from temporal data. We hope the audiences can obtain a systematic overview of this topic and inspire some new ideas for their own research.|代表复杂系统时间观测的时间数据可以在智能工业、医学、金融等领域广泛收集。近十年来，人们对时态数据挖掘进行了大量的研究，为各种应用提供了重要的价值。在这些任务中，因果发现旨在理解时间数据的潜在生成机制，引起了研究者的广泛关注。根据数据是否被校准，现有的因果发现方法可以分为两个子任务，即多变量时间序列因果发现和事件序列因果发现。以前的教程或调查主要侧重于从时间序列数据中发现因果关系，而忽略了第二个。在本教程中，我们阐明了这两个子任务之间的关系，并对现有的解决方案进行了全面的回顾。此外，我们提供了一些潜在的应用，并总结了从时间数据中发现因果关系的新视角。我们希望读者能够对这一课题有一个系统的了解，并为自己的研究提供一些新的思路。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causal+Discovery+from+Temporal+Data)|0|
|[Generative AI meets Responsible AI: Practical Challenges and Opportunities](https://doi.org/10.1145/3580305.3599557)|Krishnaram Kenthapadi, Himabindu Lakkaraju, Nazneen Rajani|Harvard University, Cambridge, MA, USA; Fiddler AI, Palo Alto, CA, USA; Hugging Face, Palo Alto, CA, USA|Generative AI models and applications are being rapidly developed and deployed across a wide spectrum of industries and applications ranging from writing and email assistants to graphic design and art generation to educational assistants to coding to drug discovery. However, there are several ethical and social considerations associated with generative AI models and applications. These concerns include lack of interpretability, bias and discrimination, privacy, lack of model robustness, fake and misleading content, copyright implications, plagiarism, and environmental impact associated with training and inference of generative AI models. In this tutorial, we first motivate the need for adopting responsible AI principles when developing and deploying large language models (LLMs) and other generative AI models, as part of a broader AI model governance and responsible AI framework, from societal, legal, user, and model developer perspectives, and provide a roadmap for thinking about responsible AI for generative AI in practice. We provide a brief technical overview of text and image generation models, and highlight the key responsible AI desiderata associated with these models. We then describe the technical considerations and challenges associated with realizing the above desiderata in practice. We focus on real-world generative AI use cases spanning domains such as media generation, writing assistants, copywriting, code generation, and conversational assistants, present practical solution approaches / guidelines for applying responsible AI techniques effectively, discuss lessons learned from deploying responsible AI approaches for generative AI applications in practice, and highlight the key open research problems. We hope that our tutorial will inform both researchers and practitioners, stimulate further research on responsible AI in the context of generative AI, and pave the way for building more reliable and trustworthy generative AI applications in the future.|生成性人工智能模型和应用程序正在迅速开发和应用于广泛的行业和应用程序，从写作和电子邮件助理到图形设计和艺术生成，从教育助理到编码到药物发现。然而，与生成性人工智能模型和应用相关的一些伦理和社会因素。这些担忧包括缺乏可解释性，偏见和歧视，隐私，缺乏模型的稳健性，虚假和误导内容，版权影响，剽窃，以及与培训和推断生成 AI 模型相关的环境影响。在本教程中，我们首先从社会，法律，用户和模型开发者的角度，激发了在开发和部署大型语言模型(LLM)和其他生成性人工智能模型时采用负责任的人工智能原则的需要，作为更广泛的人工智能模型治理和负责任的人工智能框架的一部分，并提供了一个路线图，用于在实践中思考负责任的人工智能生成性人工智能。我们提供了一个简短的文本和图像生成模型的技术概述，并突出了与这些模型相关的关键负责人工智能急需数据。然后，我们描述了与在实践中实现上述目标相关的技术考虑和挑战。我们关注跨越媒体生成，写作助手，文案，代码生成和会话助手等领域的现实世界生成 AI 用例，提出有效应用负责任 AI 技术的实用解决方案/指南，讨论在实践中为生成 AI 应用部署负责任 AI 方法的经验教训，并强调关键的开放研究问题。我们希望，我们的教程将告知研究人员和从业人员，刺激在生成性人工智能背景下负责任的人工智能的进一步研究，并为未来建立更可靠和可信赖的生成性人工智能应用铺平道路。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+AI+meets+Responsible+AI:+Practical+Challenges+and+Opportunities)|0|
|[Getting an h-Index of 100 in 20 Years or Less!](https://doi.org/10.1145/3580305.3599558)|Eamonn Keogh|University of California, Riverside, Riverside, CA, USA|The title of this tutorial is clickbait! However, a high h-index (relative to the stage of your career), will make you advisor, chair, dean and chancellor happy, and it can help you get a job or get promoted. Moreover, I will show that h-indices are highly susceptible to the Matthew Effect, a high h-index may help you get funding and attract strong collaborators/students, which in turn will likely further increase your h-index! Thus, "kickstarting" your h-index early in your career can pay huge dividends. It goes without saying that optimizing h-index is not the same as optimizing your scientific impact. Note that John Clauser, the 2022 Nobel prize winner in physics has an h-index of just 29. However, a high h-index does mean that you have been prolific at publishing papers, and it also means that the community does read and cite these papers. In this tutorial, I show that beyond pure quality of your research, there are many "tricks" you can do to increase both your paper's chance of acceptance, and increase its number of citations, thus optimizing your h-index. Please note that there are ways to increase your h-index that are clearly disingenuous and bad for the community (citation cartels, ghost authorships etc.). However, I believe that all the ideas presented in this tutorial are legal, moral, and are generally helpful for the entire community.|本教程的标题是“点击诱饵”！然而，高 h 指数(相对于你的职业生涯阶段)会让你的顾问、主席、院长和校长感到高兴，它可以帮助你得到一份工作或得到晋升。此外，我将说明 h 指数高度容易受到马太效应的影响，高 h 指数可以帮助你获得资金和吸引强大的合作者/学生，这反过来可能会进一步增加你的 h 指数！因此，在你职业生涯的早期“启动”你的 h 指数可以带来巨大的回报。不言而喻，优化 h 指数并不等同于优化你的科学影响力。请注意，2022年诺贝尔物理学奖得主约翰•克劳泽(John Clauser)的 h 指数仅为29。然而，高 h 指数确实意味着你在发表论文方面很多产，这也意味着社区确实阅读并引用了这些论文。在本教程中，我将向您展示除了纯粹的研究质量之外，还有许多“技巧”可以用来增加论文被接受的机会，并增加其引用次数，从而优化您的 h-index。请注意，增加 h 索引的方法显然是虚伪的，对社区有害的(引用卡特尔，幽灵作者等)。但是，我相信本教程中提出的所有想法都是合法的、道德的，并且对整个社区都有帮助。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Getting+an+h-Index+of+100+in+20+Years+or+Less!)|0|
|[Uncertainty Quantification in Deep Learning](https://doi.org/10.1145/3580305.3599577)|Lingkai Kong, Harshavardhan Kamarthi, Peng Chen, B. Aditya Prakash, Chao Zhang|; Dibrugarh Univ, Dibrugarh, Assam, India; Swinburne Univ Technol, Dept Comp Sci & Software Engn, Melbourne, Vic, Australia; Google Research, Google, USA; Shenzhen Univ, Coll Math & Stat, Guangdong Key Lab Intelligent Informat Proc, Shenzhen, Peoples R China; Univ Oulu, Ctr Machine Vis & Signal Anal, Oulu, Finland; Deakin Univ, Inst Intelligent Syst Res & Innovat IISRI, Geelong, Vic, Australia; Univ Waterloo, Dept Syst Design Engn, Waterloo, ON, Canada|Uncertainty quantification (UQ) methods play a pivotal role in reducing the impact of uncertainties during both optimization and decision making processes. They have been applied to solve a variety of real-world problems in science and engineering. Bayesian approximation and ensemble learning techniques are two widely-used types of uncertainty quantification (UQ) methods. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning, investigates the application of these methods in reinforcement learning, and highlights fundamental research challenges and directions associated with UQ.|不确定性量化(UQ)方法在优化和决策过程中对减少不确定性的影响起着关键作用。它们已被应用于解决各种现实世界的科学和工程问题。贝叶斯近似和集成学习技术是两种广泛使用的不确定性量化方法。在这方面，研究人员提出了不同的智商测试方法，并检验了它们在各种应用中的表现，例如计算机视觉(例如自动驾驶汽车和目标检测)、图像处理(例如影像复原)、医学图像分析(例如医学图像分类和分割)、自然语言处理(例如文本分类、社交媒体文本和累犯风险评分)、生物信息学等。本研究回顾了深度学习中使用的智商方法的最新进展，调查了这些方法在强化学习中的应用，并强调了与智商相关的基础研究挑战和方向。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Uncertainty+Quantification+in+Deep+Learning)|0|
|[Mining of Real-world Hypergraphs: Patterns, Tools, and Generators](https://doi.org/10.1145/3580305.3599567)|Geon Lee, Jaemin Yoo, Kijung Shin|KAIST, Seoul, South Korea; Carnegie Mellon University, Pittsburgh, PA, USA|ABSTRACTGroup interactions are prevalent in various complex systems (e.g., collaborations of researchers and group discussions on online Q&A sites), and they are commonly modeled as hypergraphs. Hyperedges, which compose a hypergraph, are non-empty subsets of any number of nodes, and thus each hyperedge naturally represents a group interaction among entities. The higher-order nature of hypergraphs brings about unique structural properties that have not been considered in ordinary pairwise graphs. In this tutorial, we offer a comprehensive overview of a new research topic called hypergraph mining. We first present recently revealed structural properties of real-world hypergraphs, including (a) static and dynamic patterns, (b) global and local patterns, and (c) connectivity and overlapping patterns. Together with the patterns, we describe advanced data mining tools used for their discovery. Lastly, we introduce simple yet realistic hypergraph generative models that provide an explanation of the structural properties.|在各种复杂的系统中(例如，研究人员的合作和在线问答网站上的小组讨论) ，小组之间的交互非常普遍，它们通常被建模为超图。组成超图的超边是任意数量节点的非空子集，因此每个超边自然地表示实体之间的群交互。超图的高阶性质带来了普通成对图所没有考虑到的独特的结构性质。在本教程中，我们将提供一个称为超图挖掘的新研究主题的全面概述。我们首先介绍了现实世界超图的结构特征，包括(a)静态和动态模式，(b)全局和局部模式，以及(c)连通性和重叠模式。结合这些模式，我们描述了用于发现这些模式的高级数据挖掘工具。最后，我们介绍了一个简单而实际的超图生成模型，它解释了超图的结构性质。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mining+of+Real-world+Hypergraphs:+Patterns,+Tools,+and+Generators)|0|
|[Knowledge Graph Reasoning and Its Applications](https://doi.org/10.1145/3580305.3599564)|Lihui Liu, Hanghang Tong|University of Illinois at Urbana-Champaign, Urbana, USA|The use of knowledge graphs has gained significant traction in a wide variety of applications, ranging from recommender systems and question answering to fact checking. By leveraging the wealth of information contained within knowledge graphs, it is possible to greatly enhance various downstream tasks, making reasoning over knowledge graphs an area of increasing interest. However, despite its popularity, knowledge graph reasoning remains a challenging problem. The first major challenge of knowledge graph reasoning lies in the nature of knowledge graphs themselves. Most knowledge graphs are incomplete, meaning that they may not capture all the relevant knowledge required for reasoning. As a result, reasoning on incomplete knowledge graphs can be difficult. Additionally, real-world knowledge graphs often evolve over time, which presents an additional challenge. The second challenge of knowledge graph reasoning pertains to the input data. In some KG reasoning applications, users may be unfamiliar with the background knowledge graph, leading to the possibility of asking ambiguous questions that can make KG reasoning tasks more challenging. Moreover, some applications require iterative reasoning, where users ask several related questions in sequence, further increasing the complexity of the task. The third challenge of knowledge graph reasoning concerns the algorithmic aspect. Due to the varied properties of relations in knowledge graphs, such as transitivity, symmetry, and asymmetry, designing an all-round KG reasoning model that fits all these properties can be challenging. Furthermore, most KG reasoning models tend to focus on solving a specific problem, lacking the generalization ability required to apply to other tasks. This tutorial aims to comprehensively review different aspects of knowledge graph reasoning applications and highlight open challenges and future directions. It is intended to benefit researchers and practitioners in the fields of data mining, artificial intelligence, and social science.|知识图表的使用在推荐系统、问题回答和事实核查等广泛的应用中获得了巨大的推动力。通过利用包含在知识图表中的丰富信息，有可能极大地增强各种下游任务，使知识图表的推理成为一个日益感兴趣的领域。然而，尽管知识图推理很流行，它仍然是一个具有挑战性的问题。知识图推理的第一个主要挑战在于知识图本身的性质。大多数知识图表是不完整的，这意味着它们可能无法捕获推理所需的所有相关知识。因此，对不完全知识图的推理是困难的。此外，真实世界的知识图经常随着时间的推移而发展，这带来了额外的挑战。知识图推理的第二个挑战与输入数据有关。在一些 KG 推理应用程序中，用户可能不熟悉背景知识图，导致提出模棱两可的问题的可能性，使 KG 推理任务更具挑战性。此外，一些应用程序需要迭代推理，其中用户按顺序提出几个相关的问题，进一步增加了任务的复杂性。知识图推理的第三个挑战涉及到算法方面。由于知识图中的关系具有传递性、对称性和不对称性等多种属性，因此设计一个能够满足这些属性的全方位 KG 推理模型具有一定的挑战性。此外，大多数 KG 推理模型往往侧重于解决特定问题，缺乏应用于其他任务所需的泛化能力。本教程旨在全面回顾知识图推理应用程序的不同方面，并强调开放的挑战和未来的方向。它旨在使数据挖掘、人工智能和社会科学领域的研究人员和从业人员受益。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Graph+Reasoning+and+Its+Applications)|0|
|[Fast Text Generation with Text-Editing Models](https://doi.org/10.1145/3580305.3599579)|Eric Malmi, Yue Dong, Jonathan Mallinson, Aleksandr Chuklin, Jakub Adámek, Daniil Mirylenka, Felix Stahlberg, Sebastian Krause, Shankar Kumar, Aliaksei Severyn|Google, Zurich, Switzerland; Google, New York, NY, USA; University of California, Riverside, Riverside, CA, USA; Google, Berlin, Germany|Text-editing models have recently become a prominent alternative to seq2seq models for monolingual text-generation tasks such as grammatical error correction, simplification, and style transfer. These tasks share a common trait -- they exhibit a large amount of textual overlap between the source and target texts. Text-editing models take advantage of this observation and learn to generate the output by predicting edit operations applied to the source sequence. In contrast, seq2seq models generate outputs word-by-word from scratch thus making them slow at inference time. Text-editing models provide several benefits over seq2seq models including faster inference speed, higher sample efficiency, and better control and explainability of the outputs. This tutorial provides a comprehensive overview of text-editing models and discusses how they can be used to mitigate hallucination and bias, both pressing challenges in the field of text generation. Finally, we discuss how to optimize latency of large language models via distillation to text-editing models and other means.|文本编辑模型最近已成为 seq2seq 模型的一个突出的替代方案，用于单语文本生成任务，如语法错误纠正、简化和样式转换。这些任务有一个共同的特点——它们在源文本和目标文本之间表现出大量的文本重叠。文本编辑模型利用这种观察，并学习通过预测应用于源序列的编辑操作来生成输出。相比之下，seq2seq 模型从头开始逐字生成输出，从而使它们在推理时变慢。与 seq2seq 模型相比，文本编辑模型提供了几个好处，包括更快的推理速度、更高的采样效率以及更好的输出控制和可解释性。本教程提供了一个文本编辑模型的全面概述，并讨论了如何使用它们来减轻幻觉和偏见，这两个紧迫的挑战在文本生成领域。最后，通过对文本编辑模型和其他方法的提取，讨论了如何优化大型语言模型的延迟。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+Text+Generation+with+Text-Editing+Models)|0|
|[Pretrained Language Representations for Text Understanding: A Weakly-Supervised Perspective](https://doi.org/10.1145/3580305.3599569)|Yu Meng, Jiaxin Huang, Yu Zhang, Yunyi Zhang, Jiawei Han|UIUC, Urbana, USA|Language representations pretrained on general-domain corpora and adapted to downstream task data have achieved enormous success in building natural language understanding (NLU) systems. While the standard supervised fine-tuning of pretrained language models (PLMs) has proven an effective approach for superior NLU performance, it often necessitates a large quantity of costly human-annotated training data. For example, the enormous success of ChatGPT and GPT-4 can be largely credited to their supervised fine-tuning with massive manually-labeled prompt-response training pairs. Unfortunately, obtaining large-scale human annotations is in general infeasible for most practitioners. To broaden the applicability of PLMs to various tasks and settings, weakly-supervised learning offers a promising direction to minimize the annotation requirements for PLM adaptions. In this tutorial, we cover the recent advancements in pretraining language models and adaptation methods for a wide range of NLU tasks. Our tutorial has a particular focus on weakly-supervised approaches that do not require massive human annotations. We will introduce the following topics in this tutorial: (1) pretraining language representation models that serve as the fundamentals for various NLU tasks, (2) extracting entities and hierarchical relations from unlabeled texts, (3) discovering topical structures from massive text corpora for text organization, and (4) understanding documents and sentences with weakly-supervised techniques.|在通用领域语料库上预先训练并适应下游任务数据的语言表示在构建自然语言理解(NLU)系统方面取得了巨大的成功。预训练语言模型(PLM)的标准监督微调已被证明是提高 NLU 性能的有效方法，但它往往需要大量昂贵的人工注释训练数据。例如，ChatGPT 和 GPT-4的巨大成功在很大程度上归功于它们通过大量手工标记的提示-响应训练对进行的监督微调。不幸的是，对于大多数实践者来说，获得大规模的人工注释通常是不可行的。为了扩大 PLM 在各种任务和设置中的适用性，弱监督学习为最小化 PLM 适配的注释需求提供了一个有希望的方向。在本教程中，我们将介绍针对大量 NLU 任务的预训练语言模型和适应方法的最新进展。我们的教程特别关注不需要大量人工注释的弱监督方法。在本教程中，我们将介绍以下主题: (1)作为各种 NLU 任务基础的预训练语言表示模型，(2)从未标记的文本中提取实体和层次关系，(3)从大量文本语料库中发现用于文本组织的主题结构，(4)用弱监督技术理解文档和句子。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pretrained+Language+Representations+for+Text+Understanding:+A+Weakly-Supervised+Perspective)|0|
|[Precision Health in the Age of Large Language Models](https://doi.org/10.1145/3580305.3599568)|Hoifung Poon, Tristan Naumann, Sheng Zhang, Javier González Hernández|Microsoft Research, Redmond, USA; Microsoft Research, Cambridge, United Kingdom|Medicine today is imprecise. Among the top 20 drugs in the U.S., up to 80% of patients are non-responders. The goal of precision health is to provide the right intervention for the right people at the right time. The key to realize this dream is to develop a data-driven, learning system that can instantly incorporate new health information to optimize care delivery and accelerate biomedical discovery. In reality, however, the health ecosystem is mired in overwhelming unstructured data and excruciating manual processing. For example, in cancer, standard of care often fails, and clinical trials are the last hope. Yet less than 3% of patients could find a matching trial, whereas 40% of trial failures simply stem from insufficient recruitment. Discovery is painfully slow as a new drug may take billions of dollars and over a decade to develop. In this tutorial, we will explore how large language models (LLMs) can serve as a universal structuring tool to democratize biomedical knowledge work and usher in an intelligence revolution in precision health. We first review background for precision health and give a broad overview of the AI revolution that culminated in the development of large language models, highlighting key technical innovations and prominent trends such as consolidation of AI methods across modalities. We then give an in-depth review of biomedical LLMs and precision health applications, with a particular focus on scaling real-world evidence generation and drug discovery. To conclude, we discuss key technical challenges (e.g., bias, hallucination, cost), societal ramifications (e.g., privacy, regulation), as well as exciting research frontiers such as prompt programming, knowledge distillation, multi-modal learning, causal discovery.|今天的医学是不精确的。在美国排名前20的药物中，高达80% 的患者无反应。精确健康的目标是在正确的时间为正确的人提供正确的干预。实现这一梦想的关键是开发一个数据驱动的学习系统，该系统可以即时整合新的健康信息，以优化护理服务，加速生物医学的发现。然而，在现实中，健康生态系统陷入了压倒性的非结构化数据和痛苦的手工处理的泥潭。例如，在癌症治疗中，标准治疗常常失败，临床试验是最后的希望。然而，只有不到3% 的患者能够找到匹配的试验，而40% 的试验失败仅仅是由于招募不足。由于新药的研发可能需要数十亿美元和十年以上的时间，因此发现速度非常缓慢。在本教程中，我们将探索如何大语言模型(LLM)可以作为一个通用的结构化工具，民主化生物医学知识工作，并在精确健康的智能革命。我们首先回顾了精确健康的背景，并对人工智能革命进行了广泛的概述，这场革命最终导致了大型语言模型的发展，突出了关键的技术创新和突出的趋势，例如跨模式的人工智能方法的整合。然后，我们给出了生物医学 LLM 和精确健康应用的深入评论，特别是重点放在规模的现实世界的证据生成和药物发现。最后，我们讨论了关键的技术挑战(例如，偏见，幻觉，成本) ，社会后果(例如，隐私，监管) ，以及令人兴奋的研究前沿，如快速编程，知识提取，多模态学习，因果发现。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Precision+Health+in+the+Age+of+Large+Language+Models)|0|
|[Trustworthy Transfer Learning: Transferability and Trustworthiness](https://doi.org/10.1145/3580305.3599576)|Jun Wu, Jingrui He|University of Illinois at Urbana-Champaign, Champaign, IL, USA|Deep transfer learning investigates the transfer of knowledge or information from a source domain to a relevant target domain via deep neural networks. In this tutorial, we dive into understanding deep transfer learning from the perspective of knowledge transferability and trustworthiness (e.g., privacy, robustness, fairness, transparency, etc.). To this end, we provide a comprehensive review of state-of-the-art theoretical analysis and algorithms for deep transfer learning. To be specific, we start by introducing the concepts of transferability and trustworthiness in the context of deep transfer learning. Then we summarize recent theories and algorithms for understanding knowledge transferability from two aspects: (1) IID transferability: the samples within each domain are independent and identically distributed (e.g., individual images), and (2) non-IID transferability: The samples within each domain are interdependent (e.g., connected nodes within a graph). In addition to knowledge transferability, we also review the impact of trustworthiness on deep transfer learning, e.g., whether the transferred knowledge is adversarially robust or algorithmically fair, how to transfer the knowledge under privacy-preserving constraints, etc. Finally, we highlight the open questions and future directions for understanding deep transfer learning in real-world applications. We believe this tutorial can benefit researchers and practitioners by rethinking the trade-off between knowledge transferability and trustworthiness in developing trustworthy transfer learning systems.|深度迁移学习研究的是通过深度神经网络将知识或信息从源领域转移到相关目标领域。在本教程中，我们将从知识可转移性和可信度(例如，隐私、健壮性、公平性、透明度等)的角度深入理解深度迁移学习。为此，我们提供了一个全面的综述国家的最新理论分析和算法的深度迁移学习。具体来说，我们首先在深度迁移学习的背景下介绍可迁移性和可信度的概念。然后，我们从两个方面总结了理解知识可转移性的最新理论和算法: (1) IID 可转移性: 每个领域内的样本是独立和同分布的(例如，单个图像) ，和(2)非 IID 可转移性: 每个领域内的样本是相互依赖的(例如，图中的连接节点)。除了知识可转移性之外，我们还回顾了可信性对深度转移学习的影响，例如，转移的知识是否具有对抗鲁棒性或算法公平性，如何在保护隐私的约束下转移知识等。最后，我们强调了在实际应用中理解深度迁移学习的开放性问题和未来方向。我们相信本教程可以通过重新思考在开发可信赖的转移学习系统中知识可转移性和可信赖性之间的权衡，使研究人员和从业人员受益。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Trustworthy+Transfer+Learning:+Transferability+and+Trustworthiness)|0|
|[Graph Neural Networks: Foundation, Frontiers and Applications](https://doi.org/10.1145/3580305.3599560)|Lingfei Wu, Peng Cui, Jian Pei, Liang Zhao, Xiaojie Guo|Emory University, Atlantic, USA; Pinterest, San Francisco, USA; Tsinghua University, Beijing, USA; IBM T.J. Watson Research Center, Yorktown Height, USA; Duke University, Durham, USA|The field of graph neural networks (GNNs) has seen rapid and incredible strides over the recent years. Graph neural networks, also known as deep learning on graphs, graph representation learning, or geometric deep learning, have become one of the fastest-growing research topics in machine learning, especially deep learning. This wave of research at the intersection of graph theory and deep learning has also influenced other fields of science, including recommendation systems, computer vision, natural language processing, inductive logic programming, program synthesis, software mining, automated planning, cybersecurity, and intelligent transportation. However, as the field rapidly grows, it has been extremely challenging to gain a global perspective of the developments of GNNs. Therefore, we feel the urgency to bridge the above gap and have a comprehensive tutorial on this fast-growing yet challenging topic. This tutorial of Graph Neural Networks (GNNs): Foundation, Frontiers and Applications will cover a broad range of topics in graph neural networks, by reviewing and introducing the fundamental concepts and algorithms of GNNs, new research frontiers of GNNs, and broad and emerging applications with GNNs. In addition, rich tutorial materials will be included and introduced to help the audience gain a systematic understanding by using our recently published book-Graph Neural Networks (GNN): Foundation, Frontiers, and Applications [12], which can easily be accessed at https://graph-neural-networks.github.io/index.html.|近年来，图形神经网络(GNN)领域取得了令人难以置信的快速发展。图形神经网络，又称图形深度学习、图形表示学习或几何深度学习，已成为机器学习，尤其是深度学习中发展最快的研究课题之一。图论和深度学习交叉领域的研究热潮也影响了其他科学领域，包括推荐系统、计算机视觉、自然语言处理、归纳逻辑编程、程序综合、软件挖掘、自动规划、网络安全和智能交通。然而，随着该领域的迅速发展，要从全球的角度了解 GNN 的发展变得极具挑战性。因此，我们感到紧迫的桥梁以上的差距，并有一个全面的教程对这个快速增长但具有挑战性的主题。本教程的图形神经网络(GNNs) : 基础，前沿和应用将涵盖在图形神经网络的广泛的主题，通过审查和介绍 GNNs 的基本概念和算法，GNNs 的新的研究前沿，广泛和新兴的应用与 GNNs。此外，丰富的教学材料将包括和介绍，以帮助观众获得一个系统的理解，使用我们最近出版的书-图形神经网络(GNN) : 基础，前沿，和应用[12] ，这可以很容易地在 https://graph-Neural-Networks.github.io/index.html 访问。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Neural+Networks:+Foundation,+Frontiers+and+Applications)|0|
|[Graph and Geometry Generative Modeling for Drug Discovery](https://doi.org/10.1145/3580305.3599559)|Minkai Xu, Meng Liu, Wengong Jin, Shuiwang Ji, Jure Leskovec, Stefano Ermon|Stanford University, Palo Alto, CA, USA; Broad Institute of MIT & Harvard University, Boston, MA, USA; Texas A&M University, College Station, TX, USA|With the recent progress in geometric deep learning, generative modeling, and the availability of large-scale biological datasets, molecular graph and geometry generative modeling have emerged as a highly promising direction for scientific discovery such as drug design. These generative methods enable efficient chemical space exploration and potential drug candidate generation. However, by representing molecules as 2D graphs or 3D geometries, there exist many both fundamental and challenging problems for modeling the distribution of these irregular and complex relational data. In this tutorial, we will introduce participants to the latest key developments in this field, covering important topics including 2D molecular graph generation, 3D molecular geometry generation, 2D graph to 3D geometry generation, and conditional 3D molecular geometry generation. We further include antibody generation, where we particularly consider large-size antibody molecules. For each topic, we will outline the underlying problem characteristics, summarize key challenges, present unified views of the representative approaches, and highlight future research direction and potential impacts. We anticipate this lecture-style tutorial would attract a broad audience of researchers and practitioners.|近年来，随着几何深度学习、生成建模以及大规模生物数据集的出现，分子图和几何生成建模已经成为药物设计等科学发现的一个非常有前途的方向。这些生成方法使有效的化学空间探索和潜在的候选药物生成成为可能。然而，通过将分子表示为二维图形或三维几何图形，对于这些不规则和复杂的关系数据的分布建模存在许多基本的和具有挑战性的问题。在本教程中，我们将向学员介绍这一领域的最新发展，包括二维分子图生成、三维分子结构生成、二维图到三维几何生成以及条件三维分子结构生成等重要主题。我们进一步包括抗体生成，其中我们特别考虑大型抗体分子。对于每个主题，我们将概述潜在的问题特征，总结关键挑战，提出代表性方法的统一观点，并强调未来的研究方向和潜在影响。我们预计这种讲座式的教学将吸引广大的研究人员和从业人员的观众。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+and+Geometry+Generative+Modeling+for+Drug+Discovery)|0|
|[Data-centric AI: Techniques and Future Perspectives](https://doi.org/10.1145/3580305.3599553)|Daochen Zha, KweiHerng Lai, Fan Yang, Na Zou, Huiji Gao, Xia Hu|Wake Forest University, Winston-Salem, NC, USA; Airbnb, Inc., San Francisco, CA, USA; Rice University, Houston, TX, USA; Texas A&M University, College Station, TX, USA|The role of data in AI has been significantly magnified by the emerging concept of data-centric AI. In contrast to the traditional model-centric paradigm, which focuses on developing more effective models given fixed datasets, data-centric AI emphasizes the systematic engineering of data in building AI systems. However, as a new concept, many critical aspects of data-centric AI remain ambiguous, such as its definitions, associated tasks, algorithms, challenges, and benchmarks. This tutorial aims to review and discuss this emerging field, with a particular focus on the three general data-centric AI goals: training data development, inference data development, and data maintenance. The objective of this tutorial is threefold: (1) to formally categorize the field of data-centric AI using a goal-driven taxonomy and discuss the needs and challenges of each goal, (2) to comprehensively review the state-of-the-art techniques, and (3) to discuss the future perspectives and open research directions to inspire further innovations in this field.|以数据为中心的人工智能概念的出现大大放大了数据在人工智能中的作用。与传统的以模型为中心的模式不同，以数据为中心的人工智能强调在构建人工智能系统时对数据进行系统化处理。然而，作为一个新的概念，以数据为中心的人工智能的许多关键方面仍然模糊不清，例如它的定义、相关的任务、算法、挑战和基准。本教程旨在回顾和讨论这个新兴领域，特别关注三个以数据为中心的人工智能目标: 培训数据开发、推断数据开发和数据维护。本教程的目标有三个: (1)使用目标驱动的分类法正式对以数据为中心的 AI 领域进行分类，并讨论每个目标的需求和挑战，(2)全面回顾最先进的技术，(3)讨论未来的观点和开放的研究方向，以激励该领域的进一步创新。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Data-centric+AI:+Techniques+and+Future+Perspectives)|0|
|[Hyperbolic Graph Neural Networks: A Tutorial on Methods and Applications](https://doi.org/10.1145/3580305.3599562)|Min Zhou, Menglin Yang, Bo Xiong, Hui Xiong, Irwin King|The Chinese University of Hong Kong, Hong Kong, Hong Kong; Hong Kong University of Science and Technology (Guangzhou), Guangzhou, Hong Kong; University of Stuttgart, Stuttgart, Germany; Huawei Cloud, Shenzhen, China|Graph Neural Networks (GNNs) generalize conventional neural networks to graph-structured data and have received considerable attention owing to their impressive performance. In spite of the notable successes, the performance of Euclidean models is inherently bounded and limited by the representation ability of Euclidean geometry, especially when it comes to datasets with highly non-Euclidean latent anatomy. Recently, hyperbolic spaces have emerged as a promising alternative for processing graph data with tree-like structure or power-law distribution and a surge of works on either methods or novel applications have been seen. Unlike Euclidean space, which expands polynomially, hyperbolic space grows exponentially with its radius, making it more suitable for modeling complex real-world data. Hence, it gains natural advantages in abstracting tree-like graphs with a hierarchical organization or power-law distribution. To support the burgeoning interest in Hyperbolic Graph Neural Networks (HGNNs), the primary goal of this tutorial is to give a systematical review of the methods, applications, and challenges in this fast-growing and vibrant area, with the express purpose of being accessible to all audiences.More specifically, we will first give a brief introduction to graph neural networks as well as some preliminary of Riemannian manifold and hyperbolic geometry. We then will comprehensively revisit the technical details of the developed HGNNs, by unifying them into a general framework and summarizing the variants of each component. Besides, we will introduce applications deployed in a variety of fields. Finally, we will discuss several challenges and present the potential solutions to address them, including some initial attempts of our own, which potentially paves the path for the further flourishing of the research community.|图形神经网络(GNN)将传统的神经网络推广到图形结构的数据，由于其令人印象深刻的性能而受到广泛的关注。尽管取得了显著的成功，但欧几里德模型的性能本质上受到欧几里得几何表示能力的限制，尤其是当涉及到具有高度非欧几里德潜在解剖结构的数据集时。近年来，双曲空间已成为处理具有树状结构或幂律分布的图形数据的一种有前途的替代方法。不像欧几里得空间，它是多项式展开的，双曲空间随着半径呈指数增长，这使得它更适合于建模复杂的现实世界数据。因此，它在用分层组织分布或幂律分布抽象树状图方面获得了天然的优势。为了支持对双曲图形神经网络(HGNNs)的兴趣，本教程的主要目标是对这个快速增长和充满活力的领域中的方法、应用和挑战进行系统的回顾，明确的目的是让所有的受众都可以访问。更具体地说，我们将首先简要介绍图形神经网络，以及一些初步的黎曼流形和双曲几何。然后，我们将全面重新审视已开发的 HGNN 的技术细节，将它们统一到一个总体框架中，并总结每个组件的变体。此外，我们还将介绍部署在各个领域的应用程序。最后，我们将讨论几个挑战，并提出解决这些问题的潜在解决方案，包括我们自己的一些初步尝试，这有可能为研究界的进一步繁荣铺平道路。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hyperbolic+Graph+Neural+Networks:+A+Tutorial+on+Methods+and+Applications)|0|
|[Fragile Earth: AI for Climate Sustainability - From Wildfire Disaster Management to Public Health and Beyond](https://doi.org/10.1145/3580305.3599217)|Naoki Abe, Kathleen Buckingham, Yuzhou Chen, Bistra Dilkina, Emre Eftelioglu, Auroop R. Ganguly, Yulia R. Gel, James Hodson, Ramakrishnan Kannan, Huikyo Lee, Jiafu Mao, Rose Yu|Northeastern University, Boston, MA, USA; Amazon, Inc., Seattle, WA, USA; Veritree, Washington, DC, USA; Jet Propulsion Laboratory, Pasadena, CA, USA; University of California, San Diego, La Jolla, CA, USA; Oak Ridge National Laboratory, Oak Ridge, CA, USA; University of Southern California, Los Angeles, CA, USA; IBM Research, Yorktown Heights, NY, USA; Temple University, Philadelphia, PA, USA; AI for Good Foundation, San Francisco, CA, USA; University of Texas at Dallas, Richardson, TX, USA|The Fragile Earth Workshop is a recurring event in ACM's KDD Conference on research in knowledge discovery and data mining that gathers the research community to find and explore how data science can measure and progress climate and social issues, fol- lowing the United Nations Sustainable Development Goals (SDGs) framework.|脆弱地球研讨会是 ACM 关于知识发现和数据挖掘研究的 KDD 会议上的一个经常性活动，该会议聚集了研究界，探索数据科学如何在联合国可持续发展目标(SDGs)框架下衡量和推进气候和社会问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fragile+Earth:+AI+for+Climate+Sustainability+-+From+Wildfire+Disaster+Management+to+Public+Health+and+Beyond)|0|
|[AdKDD 2023](https://doi.org/10.1145/3580305.3599582)|Abraham Bagherjeiran, Nemanja Djuric, KuangChih Lee, Linsey Pang, Vladan Radosavljevic, Suju Rajan|Spotify, New York City, NY, USA; Aurora Innovation, Inc., Pittsburgh, PA, USA; eBay, Inc., San Jose, CA, USA; Walmart, Sunnyvale, CA, USA; Salesforce, San Francisco, CA, USA; Amazon, Palo Alto, CA, USA|The digital advertising field has always had challenging ML problems, learning from petabytes of data that is highly imbalanced, reactivity times in the milliseconds, and more recently compounded with the complex user's path to purchase across devices, across platforms, and even online/real-world behavior. The AdKDD workshop continues to be a forum for researchers in advertising, during and after KDD. Our website which hosts slides and abstracts receives approximately 2,000 monthly visits and 1,800 active users during the KDD 2021. In surveys during AdKDD 2019 and 2020, over 60% agreed that AdKDD is the reason they attended KDD, and over 90% indicated they would attend next year. The 2023 edition is particularly timely because of the increasing application of Graph-based NN and Generative AI models in advertising. Coupled with privacy-preserving initiatives enforced by GDPR, CCPA the future of computational advertising is at an interesting crossroads. For this edition, we plan to solicit papers that span the spectrum of deep user understanding while remaining privacy-preserving. In addition, we will seek papers that discuss fairness in the context of advertising, to what extent does hyper-personalization work, and whether the ad industry as a whole needs to think through more effective business models such as incrementality. We have hosted several academic and industry luminaries as keynote speakers and have found our invited speaker series hosting expert practitioners to be an audience favorite. We will continue fielding a diverse set of keynote speakers and invited talks for this edition as well. As with past editions, we hope to motivate researchers in this space to think not only about the ML aspects but also to spark conversations about the societal impact of online advertising.|数字广告领域一直存在机器学习的挑战性问题，从高度不平衡的千兆字节数据中学习，以毫秒为单位的反应时间，以及最近复杂的用户跨设备、跨平台、甚至在线/现实世界行为的购买路径。AdKDD 研讨会仍然是广告研究人员的论坛，在 KDD 期间和之后。我们的网站提供幻灯片和摘要，在2021年的 KDD 期间，每月大约有2000次访问和1800个活跃用户。在2019年和2020年的调查中，超过60% 的人同意 AdKDD 是他们参加 KDD 的原因，超过90% 的人表示他们明年会参加。2023年的版本是特别及时的，因为越来越多的应用基于图的神经网络和生成人工智能模型在广告。加上 GDPR 实施的保护隐私的措施，CCPA 计算机广告的未来正处于一个有趣的十字路口。对于这个版本，我们计划征集论文，跨越深刻的用户理解范围，同时保留隐私。此外，我们将寻找那些讨论广告背景下的公平性的论文，超个性化在多大程度上起作用，以及广告业作为一个整体是否需要考虑更有效的商业模式，比如增量。我们已经主持了几个学术和行业的杰出人物作为主题演讲者，并发现我们的邀请演讲者系列主持专家从业人员是观众的最爱。我们将继续安排各种各样的主题演讲者，并邀请他们在本届会议上发表演讲。与过去的版本一样，我们希望激励这一领域的研究人员不仅要思考机器学习方面的问题，而且要激发关于在线广告的社会影响的讨论。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AdKDD+2023)|0|
|[Robust NLP for Finance (RobustFin)](https://doi.org/10.1145/3580305.3599211)|Sameena Shah, Xiaodan Zhu, Gerard de Melo, Armineh Nourbakhsh, Xiaomo Liu, Zhiqiang Ma, Charese Smiley, Zhiyu Chen|HPI / University of Potsdam, Potsdam, Germany; Queen's University, Kingston, Canada; Meta, Redmond, USA; JPMorgan AI Research, New York, USA; JPMorgan AI Research, Chicago, USA|Natural language processing (NLP) technologies have been widely applied in business domains such as e-commerce and customer service, but their adoption in the financial sector has been constrained by industry-specific performance standards and regulatory restrictions. This challenge has created new opportunities for core research in related areas. Recent advancements in NLP, such as the advent of large language models, has encouraged adoption in the finance sector. However, compared to other domains, finance has stricter requirements for robustness, explainability, and generalizability. Given this background, we propose to organize the first Robust NLP for Finance (RobustFin) workshop at KDD '23 to encourage the study of and research on robustness and explainability technologies with regard to financial NLP. The goal of the workshop is to extend the applications of NLP in finance, while motivating further research in robust NLP.|自然语言处理(NLP)技术已被广泛应用于商业领域，如电子商务和客户服务，但它们在金融部门的采用受到行业特定的性能标准和监管限制。这一挑战为相关领域的核心研究创造了新的机遇。自然语言处理(NLP)的最新进展，例如大型语言模型的出现，鼓励了金融业的采用。然而，与其他领域相比，金融对健壮性、可解释性和普遍性有更严格的要求。在这种背景下，我们建议在 KDD’23组织第一个健壮的金融自然语言处理(RobustFin)研讨会，以鼓励对金融自然语言处理方面的健壮性和可解释性技术的研究。研讨会的目的是扩大自然语言处理在金融领域的应用，同时激励进一步研究健壮的自然语言处理。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+NLP+for+Finance+(RobustFin))|0|
|[From Innovation to Scale (I2S) - Discuss and Learn How to Successfully Build, Commercialize, and Scale AI Innovations in Challenging Market Conditions](https://doi.org/10.1145/3580305.3599580)|Ankur M. Teredesai, Michael Zeller, Shenghua Bao, Wee Hyong Tok, Linsey Pang|Microsoft, Seattle, WA, USA; Salesforce, San Francisco, CA, USA; Temasek, San Diego, CA, USA; Amazon, Cupertino, CA, USA; CueZen Inc., & University of Washington, Seattle, WA, USA|In recent years, the AI community has witnessed an exciting acceleration in innovation across foundation models, deep learning, new AI applications across numerous verticals, and more. In addition, AI innovations driven by both academic and industry research labs have rapidly been adopted by big tech companies and startups to deliver value-differentiated products and services. For many machine learning researchers looking at commercializing their work, one of the frequently wondered questions is - "How do I kickstart a startup that can commercialize my research innovations?". For many ML practitioners in the KDD community, there is always curiosity on how big tech and startups take AI research and innovations, and scale it to be used by millions of users. This interactive workshop aims to achieve two goals: First, the workshop will bring together invited AI thought leaders working in academia, big tech as well as startups to share their perspective on the next big AI ideas that will change the world, and deliver impact. Second, the workshop will invite startup founders (from both academia and industry) to share their journey of acquiring customers, building a team, pitching for initial funding, and commercializing their research into successful enterprises|近年来，人工智能领域在基础模型、深度学习、新的人工智能应用等领域的创新都有了令人兴奋的加速发展。此外，由学术和行业研究实验室推动的人工智能创新已被大型科技公司和初创企业迅速采用，以提供价值差异化的产品和服务。对于许多机器学习研究人员来说，看着商业化他们的工作，一个经常想知道的问题是-“我如何启动一个初创公司，可以商业化我的研究创新?”.对于 KDD 社区的许多机器学习从业者来说，对于大型科技公司和创业公司如何利用人工智能的研究和创新，并将其扩展到数百万用户来使用总是充满好奇。这个互动研讨会旨在实现两个目标: 首先，研讨会将邀请来自学术界、大型科技公司以及初创企业的人工智能思想领袖参加，分享他们对下一个将改变世界并产生影响的大型人工智能想法的看法。其次，研讨会将邀请创业公司的创始人(来自学术界和行业)分享他们获得客户、建立团队、争取初始资金以及将他们的研究成果商业化成为成功企业的过程|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Innovation+to+Scale+(I2S)+-+Discuss+and+Learn+How+to+Successfully+Build,+Commercialize,+and+Scale+AI+Innovations+in+Challenging+Market+Conditions)|0|
|[Deep Learning on Graphs: Methods and Applications (DLG-KDD2023)](https://doi.org/10.1145/3580305.3599207)|Lingfei Wu, Jian Pei, Jiliang Tang, Yinglong Xia, Xiaojie Guo|Pinterest, San Francisco, USA; Meta AI, Menlo Park, USA; IBM T.J.Watson Research Center, Yorktown Height, USA; Duke University, Durham, USA; Michigan State University, East Lansing, USA|Deep Learning models are at the core of research in Artificial Intelligence research today. A tide in research for deep learning on graphs or graph neural networks. This wave of research at the intersection of graph theory and deep learning has also influenced other fields of science, including computer vision, natural language processing, program synthesis and analysis, financial security, Drug Discovery and so on. However, there are still many challenges regarding a broad range of the topics in deep learning on graphs, from methodologies to applications, and from foundations to the new frontiers of GNNs. This international workshop on "Deep Learning on Graphs: Method and Applications (DLG-KDD'23)" aims to bring together both academic researchers and industrial practitioners from different backgrounds and perspectives to above challenges.|深度学习模型是当今人工智能研究的核心。图或图神经网络深度学习的研究热点。这股图论与深度学习交叉的研究热潮也影响了其他科学领域，包括计算机视觉、自然语言处理、程序综合与分析、金融安全、药物发现等。然而，从方法论到应用，从基础到 GNN 的新前沿，在图形深度学习的广泛课题方面仍然存在许多挑战。这个名为“图表的深度学习: 方法与应用”的国际研讨会旨在将来自不同背景和视角的学术研究人员和工业实践者聚集在一起，共同应对上述挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Learning+on+Graphs:+Methods+and+Applications+(DLG-KDD2023))|0|
|[Scenario-Adaptive Feature Interaction for Click-Through Rate Prediction](https://doi.org/10.1145/3580305.3599936)|Erxue Min, Da Luo, Kangyi Lin, Chunzhen Huang, Yang Liu|The Hong Kong University of Science and Technology; Weixin Open Platform, Tencent; Independent Researcher|Traditional Click-Through Rate (CTR) prediction models are usually trained and deployed in a single scenario. However, large-scale commercial platforms usually contain multiple recommendation scenarios, the traffic characteristics of which may be significantly different. Recent studies have proved that learning a unified model to serve multiple scenarios is effective in improving the overall performance. However, most existing approaches suffer from various limitations respectively, such as insufficient distinction modeling, inefficiency with the increase of scenarios, and lack of interpretability. More importantly, as far as we know, none of existing Multi-Scenario Modeling approaches takes explicit feature interaction into consideration when modeling scenario distinctions, which limits the expressive power of the network and thus impairs the performance. In this paper, we propose a novel Scenario-Adaptive Feature Interaction framework named SATrans, which models scenario discrepancy as the distinction of patterns in feature correlations. Specifically, SATrans is built on a Transformer architecture to learn high-order feature interaction and involves the scenario information in the modeling of self-attention to capture distribution shifts across scenarios. We provide various implementations of our framework to boost the performance, and experiments on both public and industrial datasets show that SATrans 1) significantly outperforms existing state-of-the-art approaches for prediction, 2) is parameter-efficient as the space complexity grows marginally with the increase of scenarios, 3) offers good interpretability in both instance-level and scenario-level. We have deployed the model in WeChat Official Account Platform and have seen more than 2.84% online CTR increase on average in three major scenarios.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scenario-Adaptive+Feature+Interaction+for+Click-Through+Rate+Prediction)|-1|
|[Sequential Learning Algorithms for Contextual Model-Free Influence Maximization](https://doi.org/10.1145/3580305.3599498)|Alexandra Iacob, Bogdan Cautis, Silviu Maniu||The online influence maximization (OIM) problem aims to learn sequentially an optimal policy for selecting seed nodes which maximize the cumulative spread of information (influence) in a diffusion medium, throughout a multi-round diffusion campaign. We consider the sub-class of OIM problems where (i) the reward of a given round of the ongoing campaign consists of only the new activations(not observed at previous rounds), and (ii) the round's context and the historical data from previous rounds can be exploited to learn the best policy. This problem is directly motivated by the real-world scenarios of information diffusion in influencer marketing, where (i) only a target user's first / unique activation is of interest (and this activation will persist as an acquired, latent one throughout the campaign), and (ii) valuable side-information is available to the learning agent. We call this OIM formulation Episodic Contextual Influence Maximization with Persistence (in short, ECIMP). We propose the algorithm LSVI-GT-UCB, which implements the optimism in the face of uncertainty principle for episodic reinforcement learning with linear approximation. The learning agent estimates for each seed node its remaining potential with a Good-Turing estimator, modified by an estimated Q-function. The algorithm is empirically proven to perform better than state-of-the-art methods on two real-world datasets and a synthetically generated one.|在线影响最大化(OIM)问题的目的是依次学习在多轮扩散过程中最大化信息在扩散介质中累积传播(影响)的种子节点选择的最优策略。我们考虑 OIM 问题的子类，其中(i)正在进行的一轮活动的奖励仅包括新的激活(在前几轮中没有观察到) ，以及(ii)可以利用前几轮的背景和历史数据来学习最佳策略。这个问题直接受到影响者营销中信息传播的现实世界场景的激发，其中(i)只有目标用户的第一次/唯一激活是有意义的(并且这种激活将在整个活动中作为获得的潜在的一种持续存在) ，以及(ii)有价值的侧面信息可用于学习代理。我们称这种 OIM 公式为持久性情景影响最大化(简称 ECIMP)。我们提出了 LSVI-gT-UCB 算法，该算法在具有强化学习的情节线性近似的不确定性原理面前实现了乐观。每个种子节点的学习代理用一个 Good-Turing 估计器估计其剩余潜力，并用一个估计的 Q 函数进行修正。经验证明，该算法在两个实际数据集和一个综合生成的数据集上的性能优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sequential+Learning+Algorithms+for+Contextual+Model-Free+Influence+Maximization)|-1|
|[LightToken: A Task and Model-agnostic Lightweight Token Embedding Framework for Pre-trained Language Models](https://doi.org/10.1145/3580305.3599416)|Haoyu Wang, Ruirui Li, Haoming Jiang, Zhengyang Wang, Xianfeng Tang, Bin Bi, Monica Xiao Cheng, Bing Yin, Yaqing Wang, Tuo Zhao, Jing Gao||Pre-trained language models~(PLMs) such as BERT, RoBERTa, and DeBERTa have achieved state-of-the-art performance on various downstream tasks. The enormous sizes of PLMs hinder their deployment in resource-constrained scenarios, e.g., on edge and mobile devices. To address this issue, many model compression approaches have been proposed to reduce the number of model parameters. This paper focuses on compressing the token embedding matrices of PLMs, which typically make up a large proportion~(around 20-30%) of the entire model parameters. Existing efforts to compress token embedding usually require the introduction of customized compression architectures or the optimization of model compression processes for individual downstream tasks, limiting their applicability in both model and task dimensions. To overcome these limitations and adhere to the principle of "one-for-all", we propose a lightweight token embedding framework named LightToken, which is able to produce compressed token embedding in a task and model-agnostic fashion. LightToken is generally compatible with different architectures and applicable to any downstream task. Specifically, through an integration of low-rank approximation, novel residual binary autoencoder, and a new compression loss function, LightToken can significantly improve the model compression ratio. To demonstrate the effectiveness of LightToken, we conduct comprehensive experiments on natural language understanding and question answering tasks. In particular, LightToken improves the state-of-the-art token embedding compression ratio from 5 to 25 and outperforms the existing token embedding compression approaches by 11% and 5% on GLUE and SQuAD v1.1 benchmarks, respectively.|像 BERT、 RoBERTa 和 DeBERTa 这样的预训练语言模型已经在各种下游任务中取得了最先进的性能。PLM 的巨大规模阻碍了它们在资源受限的场景中的部署，例如在边缘和移动设备上。为了解决这个问题，人们提出了许多模型压缩方法来减少模型参数的数量。本文主要研究 PLM 中的令牌嵌入矩阵的压缩问题，这些令牌嵌入矩阵在整个模型参数中占有很大的比例(约20-30%)。现有的压缩令牌嵌入的工作通常需要引入定制的压缩体系结构，或者为单个下游任务优化模型压缩过程，从而限制了它们在模型和任务维度上的适用性。为了克服这些限制并坚持“一个对所有人”的原则，我们提出了一个名为 LightToken 的轻量级令牌嵌入框架，它能够以任务和模型无关的方式产生压缩令牌嵌入。LightToken 通常与不同的体系结构兼容，并适用于任何下游任务。具体来说，通过集成低秩近似、新颖的剩余二进制自动编码器和新的压缩损耗函数，LightToken 可以显著改善模型的压缩比。为了证明 LightToken 的有效性，我们对自然语言理解和问答任务进行了全面的实验。特别值得一提的是，LightToken 将最先进的令牌嵌入压缩比从5提高到了25，并且在 GLUE 和 SQUAD v1.1基准上分别比现有的令牌嵌入压缩方法提高了11% 和5% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LightToken:+A+Task+and+Model-agnostic+Lightweight+Token+Embedding+Framework+for+Pre-trained+Language+Models)|-1|
|[Automatic Temporal Relation in Multi-Task Learning](https://doi.org/10.1145/3580305.3599261)|Menghui Zhou, Po Yang|The University of Sheffield|Multi-task learning with temporal relation is a common prediction method for modelling the evolution of a wide range of systems. Considering the inherent relations between multiple time points, many works apply multi-task learning to jointly analyse all time points, with each time point corresponding to a prediction task. The most difficult challenge is determining how to fully explore and thus exploit the shared valuable temporal information between tasks to improve the generalization performance and robustness of the model. Existing works are classified as temporal smoothness and mean temporal relations. Both approaches, however, utilize a predefined and symmetric task relation structure that is too rigid and insufficient to adequately capture the intricate temporal relations between tasks. Instead, we propose a novel mechanism named Automatic Temporal Relation (AutoTR) for directly and automatically learning the temporal relation from any given dataset. To solve the biconvex objective function, we adopt the alternating optimization and show that the two related sub-optimization problems are amenable to closed-form computation of the proximal operator. To solve the two problems efficiently, the accelerated proximal gradient method is used, which has the fastest convergence rate of any first-order method. We have preprocessed six public real-life datasets and conducted extensive experiments to fully demonstrate the superiority of AutoTR. The results show that AutoTR outperforms several baseline methods on almost all datasets with different training ratios, in terms of overall model performance and every individual task performance. Furthermore, our findings verify that the temporal relation between tasks is asymmetrical, which has not been considered in previous works. The implementation source can be found at https://github.com/menghui-zhou/AutoTR.|具有时间关系的多任务学习是一种常用的模拟系统演化的预测方法。考虑到多个时间点之间的内在联系，许多工作采用多任务学习的方法对所有时间点进行联合分析，每个时间点对应一个预测任务。最困难的挑战是确定如何充分探索并利用任务之间共享的有价值的时间信息，以提高模型的泛化性能和鲁棒性。现有的作品分为时间平滑性和平均时间关系。然而，这两种方法都使用了一种预定义的对称任务关系结构，这种结构过于严格，不足以充分捕获任务之间错综复杂的时间关系。相反，我们提出了一种新的机制称为自动时间关系(AutoTR)直接和自动学习的时间关系从任何给定的数据集。为了求解双凸目标函数，我们采用了交替优化的方法，并且证明了两个相关的子优化问题适合于近似算子的闭式计算。为了有效地解决这两个问题，采用了加速近似梯度法，它的收敛速度是任何一阶方法中最快的。为了充分展示 AutoTR 的优越性，我们对六个公共现实数据集进行了预处理，并进行了广泛的实验。结果表明，AutoTR 在不同训练比例的数据集上，在整体模型性能和各个任务性能方面，优于几乎所有的基线方法。此外，我们的研究结果证实了任务之间的时间关系是不对称的，这是以前的研究没有考虑到的。实施来源可参阅 https://github.com/menghui-zhou/autotr。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automatic+Temporal+Relation+in+Multi-Task+Learning)|-1|
|[Empowering General-purpose User Representation with Full-life Cycle Behavior Modeling](https://doi.org/10.1145/3580305.3599331)|Bei Yang, Jie Gu, Ke Liu, Xiaoxiao Xu, Renjun Xu, Qinghui Sun, Hong Liu||User Modeling plays an essential role in industry. In this field, task-agnostic approaches, which generate general-purpose representation applicable to diverse downstream user cognition tasks, is a promising direction being more valuable and economical than task-specific representation learning. With the rapid development of Internet service platforms, user behaviors have been accumulated continuously. However, existing general-purpose user representation researches have little ability for full-life cycle modeling on extremely long behavior sequences since user registration. In this study, we propose a novel framework called full- Life cycle User Representation Model (LURM) to tackle this challenge. Specifically, LURM consists of two cascaded sub-models: (\romannumeral1) Bag-of-Interests (BoI) encodes user behaviors in any time period into a sparse vector with super-high dimension (\textite. \textitg. , 10^5 ); (\romannumeral2) Self-supervised Multi-anchor Encoder Network (SMEN) maps sequences of BoI features to multiple low-dimensional user representations. Specially, SMEN achieves almost lossless dimensionality reduction, benefiting from a novel multi-anchor module which can learn different aspects of user interests. Experiments on several benchmark datasets show that our approach outperforms state-of-the-art general-purpose representation methods.|用户建模在工业中起着至关重要的作用。在这一领域，任务无关方法产生适用于不同下游用户认知任务的通用表示，是一个比任务特定表示学习更有价值和更经济的方向。随着互联网服务平台的飞速发展，用户行为不断积累。然而，现有的通用用户表示研究对于用户注册后的极长行为序列的全生命周期建模能力不足。在这项研究中，我们提出了一个全生命周期用户表示模型(LURM)的新框架来应对这一挑战。具体来说，LURM 由两个级联子模型组成: 利益包(BoI)将用户在任意时间段的行为编码成一个超高维稀疏向量(textite)。短信。自监督多锚编码器网络(SMN)将 BoI 特征序列映射到多个低维用户表示。特别值得一提的是，采用新颖的多锚模块，可以了解用户兴趣的不同方面，实现了几乎无损的降维。在几个基准数据集上的实验表明，我们的方法优于最先进的通用表示方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Empowering+General-purpose+User+Representation+with+Full-life+Cycle+Behavior+Modeling)|-1|
|[Towards Understanding and Enhancing Robustness of Deep Learning Models against Malicious Unlearning Attacks](https://doi.org/10.1145/3580305.3599526)|Wei Qian, Chenxu Zhao, Wei Le, Meiyi Ma, Mengdi Huai||Given the availability of abundant data, deep learning models have been advanced and become ubiquitous in the past decade. In practice, due to many different reasons (e.g., privacy, usability, and fidelity), individuals also want the trained deep models to forget some specific data. Motivated by this, machine unlearning (also known as selective data forgetting) has been intensively studied, which aims at removing the influence that any particular training sample had on the trained model during the unlearning process. However, people usually employ machine unlearning methods as trusted basic tools and rarely have any doubt about their reliability. In fact, the increasingly critical role of machine unlearning makes deep learning models susceptible to the risk of being maliciously attacked. To well understand the performance of deep learning models in malicious environments, we believe that it is critical to study the robustness of deep learning models to malicious unlearning attacks, which happen during the unlearning process. To bridge this gap, in this paper, we first demonstrate that malicious unlearning attacks pose immense threats to the security of deep learning systems. Specifically, we present a broad class of malicious unlearning attacks wherein maliciously crafted unlearning requests trigger deep learning models to misbehave on target samples in a highly controllable and predictable manner. In addition, to improve the robustness of deep learning models, we also present a general defense mechanism, which aims to identify and unlearn effective malicious unlearning requests based on their gradient influence on the unlearned models. Further, theoretical analyses are conducted to analyze the proposed methods. Extensive experiments on real-world datasets validate the vulnerabilities of deep learning models to malicious unlearning attacks and the effectiveness of the introduced defense mechanism.|由于有大量的数据可用，深度学习模型在过去十年中得到了发展并变得无处不在。在实践中，由于许多不同的原因(例如，隐私、可用性和保真度) ，个人也希望训练有素的深度模型忘记一些特定的数据。基于此，机器学习(也称为选择性数据遗忘)被广泛研究，旨在消除任何特定训练样本在遗忘过程中对训练模型的影响。然而，人们通常使用机器忘却方法作为可信赖的基本工具，很少怀疑它们的可靠性。事实上，机器忘记的作用越来越重要，使得深度学习模型容易受到恶意攻击的风险。为了更好地理解深度学习模型在恶意环境中的性能，我们认为研究深度学习模型对于恶意忘却攻击的鲁棒性是至关重要的。为了弥补这一差距，本文首先论证了恶意忘记攻击对深度学习系统的安全性构成了巨大的威胁。具体来说，我们提出了一类广泛的恶意忘却攻击，其中恶意编写的忘却请求触发深度学习模型，以高度可控和可预测的方式对目标样本进行错误行为。此外，为了提高深度学习模型的鲁棒性，我们还提出了一种通用的防御机制，该机制旨在根据有效的恶意忘却请求对未学习模型的梯度影响来识别和忘却这些恶意忘却请求。进一步，对提出的方法进行了理论分析。在实际数据集上的大量实验验证了深度学习模型对恶意忘却攻击的脆弱性以及所引入的防御机制的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Understanding+and+Enhancing+Robustness+of+Deep+Learning+Models+against+Malicious+Unlearning+Attacks)|-1|
|[Virtual Node Tuning for Few-shot Node Classification](https://doi.org/10.1145/3580305.3599541)|Zhen Tan, Ruocheng Guo, Kaize Ding, Huan Liu|Arizona State University|Few-shot Node Classification (FSNC) is a challenge in graph representation learning where only a few labeled nodes per class are available for training. To tackle this issue, meta-learning has been proposed to transfer structural knowledge from base classes with abundant labels to target novel classes. However, existing solutions become ineffective or inapplicable when base classes have no or limited labeled nodes. To address this challenge, we propose an innovative method dubbed Virtual Node Tuning (VNT). Our approach utilizes a pretrained graph transformer as the encoder and injects virtual nodes as soft prompts in the embedding space, which can be optimized with few-shot labels in novel classes to modulate node embeddings for each specific FSNC task. A unique feature of VNT is that, by incorporating a Graph-based Pseudo Prompt Evolution (GPPE) module, VNT-GPPE can handle scenarios with sparse labels in base classes. Experimental results on four datasets demonstrate the superiority of the proposed approach in addressing FSNC with unlabeled or sparsely labeled base classes, outperforming existing state-of-the-art methods and even fully supervised baselines.|少镜头节点分类(FSNC)是图表示学习中的一个挑战，每个类只有少量标记节点可用于训练。为了解决这个问题，元学习已经被提出来将结构化知识从有大量标签的基类转移到新类中。但是，当基类没有或只有有限的标记节点时，现有的解决方案将变得无效或不适用。为了应对这一挑战，我们提出了一种称为虚拟节点优化(Virtual Node Tuning，VNT)的创新方法。该方法利用预先训练好的图形变换器作为编码器，在嵌入空间中注入虚拟节点作为软提示，并通过新类中的少镜头标签优化，调节节点嵌入，以适应每个特定的 FSNC 任务。VNT 的一个独特特性是，通过合并一个基于图的伪提示演化(PseudoPrompt Evolution，GPPE)模块，VNT-GPPE 可以处理基类中带有稀疏标签的场景。在四个数据集上的实验结果表明，该方法在处理未标记或稀疏标记基类的 FSNC 问题上具有优越性，其性能优于现有的最先进的方法，甚至优于完全监督的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Virtual+Node+Tuning+for+Few-shot+Node+Classification)|-1|
|[Select and Trade: Towards Unified Pair Trading with Hierarchical Reinforcement Learning](https://doi.org/10.1145/3580305.3599951)|Weiguang Han, Boyi Zhang, Qianqian Xie, Min Peng, Yanzhao Lai, Jimin Huang|Southwest Jiaotong University; Chancefocus AMC; Wuhan University|Pair trading is one of the most effective statistical arbitrage strategies which seeks a neutral profit by hedging a pair of selected assets. Existing methods generally decompose the task into two separate steps: pair selection and trading. However, the decoupling of two closely related subtasks can block information propagation and lead to limited overall performance. For pair selection, ignoring the trading performance results in the wrong assets being selected with irrelevant price movements, while the agent trained for trading can overfit to the selected assets without any historical information of other assets. To address it, in this paper, we propose a paradigm for automatic pair trading as a unified task rather than a two-step pipeline. We design a hierarchical reinforcement learning framework to jointly learn and optimize two subtasks. A high-level policy would select two assets from all possible combinations and a low-level policy would then perform a series of trading actions. Experimental results on real-world stock data demonstrate the effectiveness of our method on pair trading compared with both existing pair selection and trading methods.|成对交易是最有效的统计套利策略之一，它通过对选定的资产进行套期保值来寻求中性利润。现有的方法通常将任务分解为两个独立的步骤: 配对选择和交易。然而，两个紧密相关的子任务的解耦会阻碍信息传播，并导致有限的总体性能。对于配对选择，忽略交易绩效会导致错误的资产选择和不相关的价格变动，而受过交易培训的代理人可能在没有任何其他资产历史信息的情况下过度适应选定的资产。为了解决这个问题，本文提出了一个自动配对交易的范例，它是一个统一的任务，而不是一个两步的流水线。我们设计了一个层次化的强化学习框架来联合学习和优化两个子任务。高级策略将从所有可能的组合中选择两种资产，然后低级策略将执行一系列交易操作。对实际股票数据的实验结果表明，与现有的成对选择和成对交易方法相比，本文提出的方法是有效的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Select+and+Trade:+Towards+Unified+Pair+Trading+with+Hierarchical+Reinforcement+Learning)|-1|
