# KDD2024 Paper List

|论文|作者|组织|摘要|翻译|代码|引用数|
|---|---|---|---|---|---|---|
|[On the Convergence of Zeroth-Order Federated Tuning for Large Language Models](https://doi.org/10.1145/3637528.3671865)|Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Yaliang Li, Ying Shen|Sun Yat-sen University & Pazhou Lab, Shenzhen, Guangdong, China; Alibaba Group, Hangzhou, Zhejiang, China; Alibaba Group, Bellevue, Washington, USA; Sun Yat-sen University, Shenzhen, Guangdong, China|The confluence of Federated Learning (FL) and Large Language Models (LLMs) isushering in a new era in privacy-preserving natural language processing.However, the intensive memory requirements for fine-tuning LLMs posesignificant challenges, especially when deploying on clients with limitedcomputational resources. To circumvent this, we explore the novel integrationof Memory-efficient Zeroth-Order Optimization within a federated setting, asynergy we term as FedMeZO. Our study is the first to examine the theoreticalunderpinnings of FedMeZO in the context of LLMs, tackling key questionsregarding the influence of large parameter spaces on optimization behavior, theestablishment of convergence properties, and the identification of criticalparameters for convergence to inform personalized federated strategies. Ourextensive empirical evidence supports the theory, showing that FedMeZO not onlyconverges faster than traditional first-order methods such as FedAvg but alsosignificantly reduces GPU memory usage during training to levels comparable tothose during inference. Moreover, the proposed personalized FL strategy that isbuilt upon the theoretical insights to customize the client-wise learning ratecan effectively accelerate loss reduction. We hope our work can help to bridgetheoretical and practical aspects of federated fine-tuning for LLMs, therebystimulating further advancements and research in this area.|联邦学习(FL)和大语言模型(LLM)的融合开创了保护隐私的自然语言处理的新纪元。然而，微调 LLM 所需的大量内存带来了巨大的挑战，特别是在部署到计算资源有限的客户机上时。为了规避这个问题，我们探索了一种新的集成内存高效的零阶优化在一个联邦设置，我们称之为 FedMeZO 的不协调。我们的研究首次在 LLM 的背景下检验了 FedMeZO 的理论基础，解决了大参数空间对优化行为的影响，建立收敛性质，以及识别收敛的关键参数以通知个性化的联邦策略等关键问题。我们广泛的经验证明支持这一理论，表明 FedmeZO 不仅比传统的一阶方法(如 FedAvg)收敛得更快，而且在训练过程中显著降低了 GPU 内存的使用，与推理过程中的使用水平相当。此外，提出的个性化 FL 策略是建立在理论的洞察力，定制客户明智的学习率，可以有效地加速减少损失。我们希望我们的工作能够有助于联合微调 LLM 的理论和实践方面的桥梁，从而促进该领域的进一步发展和研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+the+Convergence+of+Zeroth-Order+Federated+Tuning+for+Large+Language+Models)|2|
|[LLM4DyG: Can Large Language Models Solve Spatial-Temporal Problems on Dynamic Graphs?](https://doi.org/10.1145/3637528.3671709)|Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Yijian Qin, Wenwu Zhu|DCST, BNRist, Tsinghua University, Beijing, China; DCST, Tsinghua University, Beijing, China|In an era marked by the increasing adoption of Large Language Models (LLMs)for various tasks, there is a growing focus on exploring LLMs' capabilities inhandling web data, particularly graph data. Dynamic graphs, which capturetemporal network evolution patterns, are ubiquitous in real-world web data.Evaluating LLMs' competence in understanding spatial-temporal information ondynamic graphs is essential for their adoption in web applications, whichremains unexplored in the literature. In this paper, we bridge the gap viaproposing to evaluate LLMs' spatial-temporal understanding abilities on dynamicgraphs, to the best of our knowledge, for the first time. Specifically, wepropose the LLM4DyG benchmark, which includes nine specially designed tasksconsidering the capability evaluation of LLMs from both temporal and spatialdimensions. Then, we conduct extensive experiments to analyze the impacts ofdifferent data generators, data statistics, prompting techniques, and LLMs onthe model performance. Finally, we propose Disentangled Spatial-TemporalThoughts (DST2) for LLMs on dynamic graphs to enhance LLMs' spatial-temporalunderstanding abilities. Our main observations are: 1) LLMs have preliminaryspatial-temporal understanding abilities on dynamic graphs, 2) Dynamic graphtasks show increasing difficulties for LLMs as the graph size and densityincrease, while not sensitive to the time span and data generation mechanism,3) the proposed DST2 prompting method can help to improve LLMs'spatial-temporal understanding abilities on dynamic graphs for most tasks. Thedata and codes will be open-sourced at publication time.|在一个为各种任务越来越多地采用大语言模型(LLM)的时代，人们越来越关注探索 LLM 处理 Web 数据(尤其是图形数据)的能力。动态图是现实网络数据中普遍存在的一种捕捉网络演化模式的图形。评估 LLM 理解动态图中时空信息的能力对于它们在 Web 应用程序中的应用至关重要，这在文献中尚未得到探索。本文首次提出了利用动态图来评价 LLM 的时空理解能力。具体来说，我们提出 LLM4DyG 基准，其中包括九个特别设计的任务，考虑到时间和空间维度的 LLM 的能力评估。然后，我们进行了广泛的实验来分析不同的数据生成器、数据统计、提示技术和 LLM 对模型性能的影响。最后，我们提出了动态图上 LLM 的时空分离思想(DST2) ，以提高 LLM 的时空理解能力。我们的主要观察结果是: 1) LLM 对动态图具有初步的时空理解能力; 2)随着图的大小和密度的增加，动态图任务对 LLM 的理解难度增加，而对时间跨度和数据生成机制不敏感; 3)所提出的 DST2提示方法有助于提高 LLM 对大多数任务的动态图的时空理解能力。数据和代码将在出版时公开来源。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LLM4DyG:+Can+Large+Language+Models+Solve+Spatial-Temporal+Problems+on+Dynamic+Graphs?)|2|
|[FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning](https://doi.org/10.1145/3637528.3671573)|Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei Gao, Xuchen Pan, Yuexiang Xie, Yaliang Li, Bolin Ding, Jingren Zhou|Alibaba Group, Beijing, China; Alibaba Group, Bellevue, USA; Alibaba Group, Hangzhou, China|Large language models (LLMs) have demonstrated great capabilities in various natural language understanding and generation tasks. These pre-trained LLMs can be further improved for specific downstream tasks by fine-tuning. However, the adoption of LLM in real-world applications can be hindered by privacy concerns and the resource-intensive nature of model training and fine-tuning. When multiple entities have similar interested tasks but cannot directly share their local data due to privacy regulations, federated learning (FL) is a mainstream solution to leverage the data of different entities. Besides avoiding direct data sharing, FL can also achieve rigorous data privacy protection, model intelligent property protection, and model customization via composition with different techniques. Despite the aforementioned advantages of FL, fine-tuning LLMs in FL settings still lacks adequate support from the existing frameworks and, therefore, faces challenges in optimizing the consumption of significant communication and computational resources, preparing various data for different tasks, and satisfying diverse information protection demands. In this paper, we discuss these challenges and introduce our package FederatedScope-LLM (FS-LLM) as a main contribution, which consists: (1) We build a complete end-to-end benchmarking pipeline under real-world scenarios, automizing the processes of dataset preprocessing, federated fine-tuning execution or simulation, and performance evaluation; (2) We provide comprehensive and off-the-shelf federated parameter-efficient fine-tuning (PEFT) algorithm implementations and versatile programming interfaces for future extension, enhancing the capabilities of LLMs in FL scenarios with low communication and computation costs, even without accessing the full model; (3) We adopt several accelerating and resource-efficient operators, and provide flexible pluggable sub-routines for interdisciplinary study. We conduct extensive and reproducible experiments to show the effectiveness of FS-LLM and benchmark advanced LLMs with PEFT algorithms in FL. We release FS-LLM at https://github.com/alibaba/FederatedScope/tree/llm.|大型语言模型(LLM)在各种自然语言理解和生成任务中表现出了很强的能力。通过微调，这些预先训练好的 LLM 可以针对特定的下游任务进一步改进。然而，在实际应用中采用 LLM 可能会受到隐私问题以及模型训练和微调的资源密集性的阻碍。当多个实体具有相似的感兴趣任务，但由于隐私规则的限制而不能直接共享本地数据时，联邦学习(FL)是利用不同实体数据的主流解决方案。除了避免直接数据共享，FL 还可以实现严格的数据隐私保护、模型智能财产保护和通过不同技术组合的模型定制。尽管 FL 具有上述优势，但 FL 环境中的微调 LLM 仍然缺乏现有框架的足够支持，因此在优化重要通信和计算资源的消耗，为不同任务准备各种数据以及满足不同的信息保护需求方面面临挑战。本文讨论了这些挑战，并介绍了我们的软件包 FederatedScope-LLM (FS-LLM) ，它的主要贡献包括: (1)构建了一个完整的现实场景下端到端基准测试流水线，实现了数据集预处理、联邦微调执行或仿真、性能评估等过程的自动化;(2)我们提供全面和现成的联邦参数高效微调(PEFT)算法实现和多功能的编程接口，为未来的扩展提供支持，提高 LLM 在低通信和计算成本的 FL 场景中的能力，即使不访问完整的模型; (3)我们采用多个加速和资源高效的运算符，并为跨学科研究提供灵活的可插入子例程。我们进行了广泛和可重复的实验，以证明 FS-LLM 和基准高级 LLM 与 PEFT 算法在 FL 的有效性。我们在 https://github.com/alibaba/federatedscope/tree/llm 发布 FS-LLM。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FederatedScope-LLM:+A+Comprehensive+Package+for+Fine-tuning+Large+Language+Models+in+Federated+Learning)|2|
|[Ads Recommendation in a Collapsed and Entangled World](https://doi.org/10.1145/3637528.3671607)|Junwei Pan, Wei Xue, Ximei Wang, Haibin Yu, Xun Liu, Shijie Quan, Xueming Qiu, Dapeng Liu, Lei Xiao, Jie Jiang|Tencent Inc., Shenzhen, China|We present Tencent's ads recommendation system and examine the challenges and practices of learning appropriate recommendation representations. Our study begins by showcasing our approaches to preserving prior knowledge when encoding features of diverse types into embedding representations. We specifically address sequence features, numeric features, and pre-trained embedding features. Subsequently, we delve into two crucial challenges related to feature representation: the dimensional collapse of embeddings and the interest entanglement across different tasks or scenarios. We propose several practical approaches to address these challenges that result in robust and disentangled recommendation representations. We then explore several training techniques to facilitate model optimization, reduce bias, and enhance exploration. Additionally, we introduce three analysis tools that enable us to study feature correlation, dimensional collapse, and interest entanglement. This work builds upon the continuous efforts of Tencent's ads recommendation team over the past decade. It summarizes general design principles and presents a series of readily applicable solutions and analysis tools. The reported performance is based on our online advertising platform, which handles hundreds of billions of requests daily and serves millions of ads to billions of users.|我们将介绍腾讯的广告推荐系统，并探讨学习适当的推荐表达的挑战和实践。我们的研究首先展示了我们在将不同类型的特征编码到嵌入表示中时保留先验知识的方法。我们专门讨论序列特征、数字特征和预先训练的嵌入特征。随后，我们深入研究了与特征表示相关的两个关键挑战: 嵌入的维度崩溃和不同任务或场景之间的兴趣纠缠。我们提出了几个实用的方法来解决这些挑战，导致健壮的和分离的建议表示。然后，我们探讨了几种训练技术，以促进模型优化，减少偏差，并加强探索。此外，我们还介绍了三种分析工具，使我们能够研究特征相关性、维度折叠和利益纠缠。这项工作建立在腾讯广告推荐团队过去十年不断努力的基础上。它总结了一般的设计原则，并提出了一系列容易适用的解决方案和分析工具。报告的性能是基于我们的在线广告平台，该平台每天处理数千亿个请求，为数十亿用户提供数百万个广告。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ads+Recommendation+in+a+Collapsed+and+Entangled+World)|1|
|[EAGER: Two-Stream Generative Recommender with Behavior-Semantic Collaboration](https://doi.org/10.1145/3637528.3671775)|Ye Wang, Jiahao Xun, Minjie Hong, Jieming Zhu, Tao Jin, Wang Lin, Haoyuan Li, Linjun Li, Yan Xia, Zhou Zhao, Zhenhua Dong|Zhejiang University, Hangzhou, Zhejiang, China; Huawei Noah's Ark Lab, Shenzhen, China; Zhejiang University, Hangzhou, China|Generative retrieval has recently emerged as a promising approach to sequential recommendation, framing candidate item retrieval as an autoregressive sequence generation problem. However, existing generative methods typically focus solely on either behavioral or semantic aspects of item information, neglecting their complementary nature and thus resulting in limited effectiveness. To address this limitation, we introduce EAGER, a novel generative recommendation framework that seamlessly integrates both behavioral and semantic information. Specifically, we identify three key challenges in combining these two types of information: a unified generative architecture capable of handling two feature types, ensuring sufficient and independent learning for each type, and fostering subtle interactions that enhance collaborative information utilization. To achieve these goals, we propose (1) a two-stream generation architecture leveraging a shared encoder and two separate decoders to decode behavior tokens and semantic tokens with a confidence-based ranking strategy; (2) a global contrastive task with summary tokens to achieve discriminative decoding for each type of information; and (3) a semantic-guided transfer task designed to implicitly promote cross-interactions through reconstruction and estimation objectives. We validate the effectiveness of EAGER on four public benchmarks, demonstrating its superior performance compared to existing methods. Our source code will be publicly available on PapersWithCode.com.|生成性检索是近年来出现的一种有前途的序列推荐方法，它将候选项检索框架为一个自回归序列生成问题。然而，现有的生成方法通常只关注项目信息的行为方面或语义方面，忽视了它们的互补性，因此效果有限。为了解决这个问题，我们引入了一个新的生成推荐框架 EAGER，它能够无缝地整合行为推荐和语义信息推荐。具体来说，我们确定了将这两种类型的信息结合起来的三个关键挑战: 一个能够处理两种特征类型的统一生成体系结构，确保对每种类型进行充分和独立的学习，以及培养能够提高协作信息利用率的微妙交互。为了实现这些目标，我们提出(1)利用共享编码器和两个单独的解码器的两流生成架构，以基于置信度的排序策略解码行为标记和语义标记; (2)具有摘要标记的全局对比任务，以实现每种类型的信息的区分性解码; 和(3)语义指导的转移任务，旨在通过重建和评估目标隐式促进交叉互动。我们在四个公共基准上验证了 EAGER 算法的有效性，证明了与现有方法相比，EAGER 算法具有更好的性能。我们的源代码将在 paperswithcode.com 上公开。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EAGER:+Two-Stream+Generative+Recommender+with+Behavior-Semantic+Collaboration)|1|
|[Debiased Recommendation with Noisy Feedback](https://doi.org/10.1145/3637528.3671915)|Haoxuan Li, Chunyuan Zheng, Wenjie Wang, Hao Wang, Fuli Feng, XiaoHua Zhou|Peking University, Beijing, China; Zhejiang University, Hangzhou, China; University of Science and Technology of China, Hefei, China; National University of Singapore, Singapore, Singapore; University of California, San Diego, Beijing, China|Ratings of a user to most items in recommender systems are usually missing not at random (MNAR), largely because users are free to choose which items to rate. To achieve unbiased learning of the prediction model under MNAR data, three typical solutions have been proposed, including error-imputation-based (EIB), inverse-propensity-scoring (IPS), and doubly robust (DR) methods. However, these methods ignore an alternative form of bias caused by the inconsistency between the observed ratings and the users' true preferences, also known as noisy feedback or outcome measurement errors (OME), e.g., due to public opinion or low-quality data collection process. In this work, we study intersectional threats to the unbiased learning of the prediction model from data MNAR and OME in the collected data. First, we design OME-EIB, OME-IPS, and OME-DR estimators, which largely extend the existing estimators to combat OME in real-world recommendation scenarios. Next, we theoretically prove the unbiasedness and generalization bound of the proposed estimators. We further propose an alternate denoising training approach to achieve unbiased learning of the prediction model under MNAR data with OME. Extensive experiments are conducted on three real-world datasets and one semi-synthetic dataset to show the effectiveness of our proposed approaches. The code is available at https://github.com/haoxuanli-pku/KDD24-OME-DR.|在推荐系统中，用户对大多数项目的评分通常不是随机丢失的(MNAR) ，主要是因为用户可以自由选择对哪些项目进行评分。为了实现 MNAR 数据下预测模型的无偏学习，提出了三种典型的解决方案，包括基于错误插补(EIB)、逆倾向评分(IPS)和双鲁棒(DR)方法。然而，这些方法忽略了由观察到的评分和用户的真实偏好之间的不一致引起的另一种形式的偏差，也称为噪声反馈或结果测量错误(OME) ，例如由于公众意见或低质量的数据收集过程。在这项工作中，我们研究交叉威胁的预测模型无偏学习的数据 MNAR 和 OME 收集的数据。首先，我们设计了 OME-EIB、 OME-IPS 和 OME-DR 估计器，它们在很大程度上扩展了现有的估计器，以便在实际推荐场景中对抗 OME。接着，我们从理论上证明了所提出的估计量的无偏性和广义界。我们进一步提出了一种交替去噪训练方法，用 OME 实现 MNAR 数据下预测模型的无偏学习。在三个实际数据集和一个半合成数据集上进行了广泛的实验，以证明我们提出的方法的有效性。密码可在 https://github.com/haoxuanli-pku/kdd24-ome-dr 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Debiased+Recommendation+with+Noisy+Feedback)|1|
|[Harm Mitigation in Recommender Systems under User Preference Dynamics](https://doi.org/10.1145/3637528.3671925)|Jerry Chee, Shankar Kalyanaraman, Sindhu Kiranmai Ernala, Udi Weinsberg, Sarah Dean, Stratis Ioannidis|Meta, Menlo Park, CA, USA; Northeastern University, Boston, MA, USA; Cornell University, Ithaca, NY, USA|We consider a recommender system that takes into account the interplaybetween recommendations, the evolution of user interests, and harmful content.We model the impact of recommendations on user behavior, particularly thetendency to consume harmful content. We seek recommendation policies thatestablish a tradeoff between maximizing click-through rate (CTR) and mitigatingharm. We establish conditions under which the user profile dynamics have astationary point, and propose algorithms for finding an optimal recommendationpolicy at stationarity. We experiment on a semi-synthetic movie recommendationsetting initialized with real data and observe that our policies outperformbaselines at simultaneously maximizing CTR and mitigating harm.|我们考虑建议之间的相互作用，用户兴趣的演变和有害内容的推荐系统。我们模拟建议对用户行为的影响，特别是消费有害内容的趋势。我们寻求建议政策，在最大化点进率和减少伤害之间建立平衡。我们建立了用户轮廓动态具有平稳点的条件，并提出了在平稳点寻找最优推荐策略的算法。我们在一个半合成的电影推荐设置上进行了实验，初始化为真实数据，并观察到我们的策略在同时最大化点击率和减少危害方面优于基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Harm+Mitigation+in+Recommender+Systems+under+User+Preference+Dynamics)|1|
|[Face4Rag: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese](https://doi.org/10.1145/3637528.3671656)|Yunqi Xu, Tianchi Cai, Jiyan Jiang, Xierui Song|Ant Group, Shanghai, China; Ant Group, Hangzhou, China; Tsinghua University, Beijing, China|The prevailing issue of factual inconsistency errors in conventional Retrieval Augmented Generation (RAG) motivates the study of Factual Consistency Evaluation (FCE). Despite the various FCE methods proposed earlier, these methods are evaluated on datasets generated by specific Large Language Models (LLMs). Without a comprehensive benchmark, it remains unexplored how these FCE methods perform on other LLMs with different error distributions or even unseen error types, as these methods may fail to detect the error types generated by other LLMs. To fill this gap, in this paper, we propose the first comprehensive FCE benchmark Face4RAG for RAG independent of the underlying LLM. Our benchmark consists of a synthetic dataset built upon a carefully designed typology for factuality inconsistency error and a real-world dataset constructed from six commonly used LLMs, enabling evaluation of FCE methods on specific error types or real-world error distributions. On the proposed benchmark, we discover the failure of existing FCE methods to detect the logical fallacy, which refers to a mismatch of logic structures between the answer and the retrieved reference. To fix this issue, we further propose a new method called L-Face4RAG with two novel designs of logic-preserving answer decomposition and fact-logic FCE. Extensive experiments show L-Face4RAG substantially outperforms previous methods for factual inconsistency detection on a wide range of tasks, notably beyond the RAG task from which it is originally motivated. Both the benchmark and our proposed method are publicly available. https://huggingface.co/datasets/yq27/Face4RAG|传统检索增强生成(RAG)中事实不一致性错误的普遍问题激发了事实一致性评价(FCE)的研究。尽管之前提出了各种 FCE 方法，但是这些方法是在特定的大语言模型(LLM)生成的数据集上进行评估的。由于没有一个全面的基准测试，这些 FCE 方法在其他具有不同错误分布甚至不可见错误类型的 LLM 上的表现仍然是未知的，因为这些方法可能无法检测其他 LLM 产生的错误类型。为了填补这个空白，在本文中，我们提出了独立于底层 LLM 的 RAG 的第一个全面的 FCE 基准 Face4RAG。我们的基准包括一个基于事实不一致性错误类型精心设计的合成数据集，以及一个由六个常用 LLM 构建的现实世界数据集，从而能够对特定错误类型或现实世界错误分布的 FCE 方法进行评估。在所提出的基准上，我们发现现有的 FCE 方法在检测逻辑谬误方面的失败，这是指答案和检索到的引用之间的逻辑结构不匹配。为了解决这一问题，我们进一步提出了一种新的方法，称为 L-Face4RAG 与两个新颖的设计保持逻辑的答案分解和事实逻辑 FCE。大量的实验表明，L-Face4RAG 在广泛的任务范围内大大优于以前的事实不一致性检测方法，特别是在 RAG 任务之外。基准测试和我们提出的方法都是公开的。Https://huggingface.co/datasets/yq27/face4rag|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Face4Rag:+Factual+Consistency+Evaluation+for+Retrieval+Augmented+Generation+in+Chinese)|1|
|[A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys)](https://doi.org/10.1145/3637528.3671474)|Yashar Deldjoo, Zhankui He, Julian McAuley, Anton Korikov, Scott Sanner, Arnau Ramisa, René Vidal, Maheswaran Sathiamoorthy, Atoosa Kasirzadeh, Silvia Milano|University of California, La Jolla, CA, USA; University of Edinburgh, Edinburgh, UK; Bespoke Labs, Santa Clara, CA, USA; Amazon, Palo Alto, CA, USA; University of Exeter and LMU Munich, Munich, Germany; Polytechnic University of Bari, Bari, Italy; University of Toronto, Toronto, ON, Canada|Traditional recommender systems typically use user-item rating histories as their main data source. However, deep generative models now have the capability to model and sample from complex data distributions, including user-item interactions, text, images, and videos, enabling novel recommendation tasks. This comprehensive, multidisciplinary survey connects key advancements in RS using Generative Models (Gen-RecSys), covering: interaction-driven generative models; the use of large language models (LLM) and textual data for natural language recommendation; and the integration of multimodal models for generating and processing images/videos in RS. Our work highlights necessary paradigms for evaluating the impact and harm of Gen-RecSys and identifies open challenges. This survey accompanies a "tutorial" presented at ACM KDD'24, with supporting materials provided at: https://encr.pw/vDhLq.|传统的推荐系统通常使用用户项评分历史作为其主要数据源。然而，深层生成模型现在能够对复杂的数据分布进行建模和采样，包括用户项交互、文本、图像和视频，从而实现新的推荐任务。这个全面的，多学科的调查连接了 RS 使用生成模型(Gen-RecSys)的关键进步，包括: 交互驱动的生成模型; 使用大语言模型(LLM)和文本数据进行自然语言推荐; 以及集成多模态模型用于生成和处理 RS 中的图像/视频。我们的工作突出了评估 Gen-RecSys 的影响和危害的必要范式，并确定了公开的挑战。本调查附有在 ACM kDD’24会议上提出的“教程”，辅助材料提供在以下 https://encr.pw/vdhlq。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Review+of+Modern+Recommender+Systems+Using+Generative+Models+(Gen-RecSys))|1|
|[A Population-to-individual Tuning Framework for Adapting Pretrained LM to On-device User Intent Prediction](https://doi.org/10.1145/3637528.3671984)|Jiahui Gong, Jingtao Ding, Fanjin Meng, Guilong Chen, Hong Chen, Shen Zhao, Haisheng Lu, Yong Li|Honor Device Co., Ltd., Shenzhen, China; Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China|Mobile devices, especially smartphones, can support rich functions and have developed into indispensable tools in daily life. With the rise of generative AI services, smartphones can potentially transform into personalized assistants, anticipating user needs and scheduling services accordingly. Predicting user intents on smartphones, and reflecting anticipated activities based on past interactions and context, remains a pivotal step towards this vision. Existing research predominantly focuses on specific domains, neglecting the challenge of modeling diverse event sequences across dynamic contexts. Leveraging pre-trained language models (PLMs) offers a promising avenue, yet adapting PLMs to on-device user intent prediction presents significant challenges. To address these challenges, we propose PITuning, a Population-to-Individual Tuning framework. PITuning enhances common pattern extraction through dynamic event-to-intent transition modeling and addresses long-tailed preferences via adaptive unlearning strategies. Experimental results on real-world datasets demonstrate PITuning's superior intent prediction performance, highlighting its ability to capture long-tailed preferences and its practicality for on-device prediction scenarios.|移动设备，尤其是智能手机，可以支持丰富的功能，并已发展成为日常生活中不可或缺的工具。随着产生式人工智能服务的兴起，智能手机有可能转变为个性化的助手，预测用户需求并相应地安排服务。在智能手机上预测用户意图，并根据过去的交互和上下文反映预期活动，仍然是实现这一愿景的关键一步。现有的研究主要集中在特定领域，忽视了在动态背景下建模不同事件序列的挑战。利用预先训练的语言模型(PLM)提供了一个有前途的途径，然而使 PLM 适应设备上的用户意图预测提出了重大挑战。为了应对这些挑战，我们提出 PITuning，一个人口到个人的调整框架。PITuning 通过动态事件-意图转换模型增强了公共模式提取，并通过自适应忘却策略解决了长尾偏好。在真实世界数据集上的实验结果表明，PITuning 具有优越的意图预测性能，突出了其捕获长尾偏好的能力以及在设备上预测场景的实用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Population-to-individual+Tuning+Framework+for+Adapting+Pretrained+LM+to+On-device+User+Intent+Prediction)|1|
|[Efficient Exploration of the Rashomon Set of Rule-Set Models](https://doi.org/10.1145/3637528.3671818)|Martino Ciaperoni, Han Xiao, Aristides Gionis|KTH Royal Institute of Technology, Stockholm, Sweden; Aalto University, Espoo, Uusimaa, Finland; The Upright Project, Helsinki, Uusimaa, Finland|Today, as increasingly complex predictive models are developed, simple rule sets remain a crucial tool to obtain interpretable predictions and drive high-stakes decision making. However, a single rule set provides a partial representation of a learning task. An emerging paradigm in interpretable machine learning aims at exploring the Rashomon set of all models exhibiting near-optimal performance. Existing work on Rashomon-set exploration focuses on exhaustive search of the Rashomon set for particular classes of models, which can be a computationally challenging task. On the other hand, exhaustive enumeration leads to redundancy that often is not necessary, and a representative sample or an estimate of the size of the Rashomon set is sufficient for many applications. In this work, we propose, for the first time, efficient methods to explore the Rashomon set of rule set models with or without exhaustive search. Extensive experiments demonstrate the effectiveness of the proposed methods in a variety of scenarios.|今天，随着越来越复杂的预测模型的发展，简单的规则集仍然是获得可解释的预测和推动高风险决策的关键工具。但是，单个规则集提供了学习任务的部分表示。可解释机器学习的一个新兴范式旨在探索所有表现出接近最佳性能的罗生门模型集。现有的罗生门集探索工作侧重于对罗生门集进行详尽的搜索，寻找特定类别的模型，这可能是一项具有计算挑战性的任务。另一方面，穷举导致冗余，往往是不必要的，一个代表性的样本或罗生门集大小的估计是足够的许多应用程序。在这项工作中，我们首次提出了有效的方法来探索罗生门集的规则集模型有或没有穷举搜索。大量的实验证明了该方法在各种场景下的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Exploration+of+the+Rashomon+Set+of+Rule-Set+Models)|1|
|[Learning the Covariance of Treatment Effects Across Many Weak Experiments](https://doi.org/10.1145/3637528.3672034)|Aurélien Bibaut, Winston Chou, Simon Ejdemyr, Nathan Kallus|Netflix, Los Gatos, CA, USA; Netflix & Cornell University, Los Gatos, CA, USA|When primary objectives are insensitive or delayed, experimenters may instead focus on proxy metrics derived from secondary outcomes. For example, technology companies often infer the long-term impacts of product interventions from their effects on short-term user engagement signals. We consider the meta-analysis of many historical experiments to learn the covariance of treatment effects on these outcomes, which can support the construction of such proxies. Even when experiments are plentiful, if treatment effects are weak, the covariance of estimated treatment effects across experiments can be highly biased. We overcome this with techniques inspired by weak instrumental variable analysis. We show that Limited Information Maximum Likelihood (LIML) learns a parameter equivalent to fitting total least squares to a transformation of the scatterplot of treatment effects, and that Jackknife Instrumental Variables Estimation (JIVE) learns another parameter computable from the average of Jackknifed covariance matrices across experiments. We also present a total covariance estimator for the latter estimand under homoskedasticity, which is equivalent to a k-class estimator. We show how these parameters can be used to construct unbiased proxy metrics under various structural models. Lastly, we discuss the real-world application of our methods at Netflix.|当主要目标不敏感或延迟时，实验者可能会转而关注从次要结果衍生出的代理指标。例如，技术公司往往从产品干预对短期用户参与信号的影响中推断出产品干预的长期影响。我们考虑了许多历史实验的荟萃分析，以了解治疗效果对这些结果的协方差，这可以支持构建这样的代理。即使实验数量充足，如果治疗效果较弱，跨实验的估计治疗效果的协方差可能会有很大的偏差。我们用受弱工具变量分析启发的技术克服了这个问题。我们表明，有限信息最大似然(LIML)学习了一个等价于将总最小二乘拟合到治疗效果的散点图变换的参数，并且杰克刀仪器变量估计(JIVE)学习了另一个可以从杰克刀协方差矩阵的平均值跨实验计算的参数。我们还给出了后一种估计在同方差下的总协方差估计，它等价于一个 k 类估计。我们展示了如何使用这些参数在各种结构模型下构建无偏代理度量。最后，我们讨论了我们的方法在 Netflix 上的实际应用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+the+Covariance+of+Treatment+Effects+Across+Many+Weak+Experiments)|1|
|[Compact Decomposition of Irregular Tensors for Data Compression: From Sparse to Dense to High-Order Tensors](https://doi.org/10.1145/3637528.3671846)|Taehyung Kwon, Jihoon Ko, Jinhong Jung, JunGi Jang, Kijung Shin|KAIST, Seoul, Republic of Korea; UIUC, Champaign, IL, USA; Soongsil University, Seoul, Republic of Korea|An irregular tensor is a collection of matrices with different numbers of rows. Real-world data from diverse domains, including medical and stock data, are effectively represented as irregular tensors due to the inherent variations in data length. For their analysis, various tensor decomposition methods (e.g., PARAFAC2) have been devised. While they are expected to be effective in compressing large-scale irregular tensors, akin to regular tensor decomposition methods, our analysis reveals that their compression performance is limited due to the larger number of first mode factor matrices. In this work, we propose accurate and compact decomposition methods for lossy compression of irregular tensors. First, we propose Light-IT, which unifies all first mode factor matrices into a single matrix, dramatically reducing the size of compressed outputs. Second, motivated by the success of Tucker decomposition in regular tensor compression, we extend Light-IT to Light-IT++ to enhance its expressive power and thus reduce compression error. Finally, we generalize both methods to handle irregular tensors of any order and leverage the sparsity of tensors for acceleration. Extensive experiments on 6 real-world datasets demonstrate that our methods are (a) Compact: their compressed output is up to 37× smaller than that of the most concise baseline, (b) Accurate: our methods are up to 5× more accurate, with smaller compressed output, than the most accurate baseline, and (c) Versatile: our methods are effective for sparse, dense, and higher-order tensors.|不规则张量是具有不同行数的矩阵的集合。来自不同领域的真实世界数据，包括医疗和股票数据，由于数据长度的固有变化，有效地表示为不规则张量。对于它们的分析，各种张量分解方法(例如，PARAFAC2)已被设计出来。虽然它们在压缩大规模不规则张量方面是有效的，类似于正则张量分解方法，但我们的分析表明，由于第一模因子矩阵数量较多，它们的压缩性能受到限制。在这项工作中，我们提出了精确和紧凑的分解方法对不规则张量的有损数据压缩。首先，我们提出了光信息技术，它将所有的第一模因子矩阵统一到一个单一的矩阵，大大减少了压缩输出的大小。其次，由于 Tucker 分解在正则张量压缩中的成功，我们将 Light-IT 扩展到 Light-IT + + ，以增强其表达能力，从而减少压缩误差。最后，我们将这两种方法推广到处理任意阶的不规则张量，并利用张量的稀疏性来加速。在6个真实世界数据集上的大量实验表明，我们的方法是(a)紧凑的: 其压缩输出比最简洁的基线小37倍; (b)精确: 我们的方法比最精确的基线精确5倍，压缩输出更小; (c)多功能: 我们的方法对稀疏，密集和高阶张量是有效的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Compact+Decomposition+of+Irregular+Tensors+for+Data+Compression:+From+Sparse+to+Dense+to+High-Order+Tensors)|1|
|[TDNetGen: Empowering Complex Network Resilience Prediction with Generative Augmentation of Topology and Dynamics](https://doi.org/10.1145/3637528.3671934)|Chang Liu, Jingtao Ding, Yiwen Song, Yong Li|Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China; Shenzhen International Graduate School, Tsinghua University, Shenzhen, Guangdong, China|Predicting the resilience of complex networks, which represents the ability to retain fundamental functionality amidst external perturbations or internal failures, plays a critical role in understanding and improving real-world complex systems. Traditional theoretical approaches grounded in nonlinear dynamical systems rely on prior knowledge of network dynamics. On the other hand, data-driven approaches frequently encounter the challenge of insufficient labeled data, a predicament commonly observed in real-world scenarios. In this paper, we introduce a novel resilience prediction framework for complex networks, designed to tackle this issue through generative data augmentation of network topology and dynamics. The core idea is the strategic utilization of the inherent joint distribution present in unlabeled network data, facilitating the learning process of the resilience predictor by illuminating the relationship between network topology and dynamics. Experiment results on three network datasets demonstrate that our proposed framework TDNetGen can achieve high prediction accuracy up to 85%-95%. Furthermore, the framework still demonstrates a pronounced augmentation capability in extreme low-data regimes, thereby underscoring its utility and robustness in enhancing the prediction of network resilience. We have open-sourced our code in the following link, https://github.com/tsinghua-fib-lab/TDNetGen.|预测复杂网络的恢复能力，代表了在外部扰动或内部故障中保持基本功能的能力，在理解和改善现实世界中的复杂系统方面起着关键作用。基于非线性动力系统的传统理论方法依赖于网络动力学的先验知识。另一方面，数据驱动的方法经常遇到标记数据不足的挑战，这是在现实场景中常见的困境。在这篇文章中，我们介绍了一个新的复杂网络弹性预测框架，旨在解决这个问题，通过网络拓扑和动态的生成数据增强。其核心思想是战略性地利用未标记网络数据中固有的联合分布，通过阐明网络拓扑和动态之间的关系，促进弹性预测器的学习过程。在三个网络数据集上的实验结果表明，我们提出的框架 TDNetGen 能够实现高达85% -95% 的预测准确率。此外，该框架在极端低数据情况下仍然显示出明显的增强能力，从而突出了其在加强网络复原力预测方面的效用和稳健性。我们已在以下连结开放我们的程式码供 https://github.com/tsinghua-fib-lab/tdnetgen 使用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TDNetGen:+Empowering+Complex+Network+Resilience+Prediction+with+Generative+Augmentation+of+Topology+and+Dynamics)|1|
|[Scalable Temporal Motif Densest Subnetwork Discovery](https://doi.org/10.1145/3637528.3671889)|Ilie Sarpe, Fabio Vandin, Aristides Gionis|KTH Royal Institute of Technology, Stockholm, Sweden; University of Padova, Padova, Italy|Finding dense subnetworks, with density based on edges or more complex structures, such as subgraphs or k-cliques, is a fundamental algorithmic problem with many applications. While the problem has been studied extensively in static networks, much remains to be explored for temporal networks. In this work we introduce the novel problem of identifying the temporal motif densest subnetwork, i.e., the densest subnetwork with respect to temporal motifs, which are high-order patterns characterizing temporal networks. Identifying temporal motifs is an extremely challenging task, and thus, efficient methods are required. To address this challenge, we design two novel randomized approximation algorithms with rigorous probabilistic guarantees that provide high-quality solutions. We perform extensive experiments showing that our methods outperform baselines. Furthermore, our algorithms scale on networks with up to billions of temporal edges, while baselines cannot handle such large networks. We use our techniques to analyze a financial network and show that our formulation reveals important network structures, such as bursty temporal events and communities of users with similar interests.|寻找基于边或更复杂结构(如子图或 k 团)的密集子网络是许多应用中的一个基本计算问题。虽然这个问题已经在静态网络中得到了广泛的研究，但对于时态网络还有许多有待探索的地方。在本文中，我们介绍了一个新的时间模式最密集子网络的识别问题，即时间模式方面的最密集子网络，这是高阶模式的特点时间网络。识别时间模式是一项极具挑战性的任务，因此，需要有效的方法。为了解决这个问题，我们设计了两种新的随机近似算法，它们具有严格的概率保证，能够提供高质量的解决方案。我们进行了大量的实验，表明我们的方法比基线更有效。此外，我们的算法扩展到具有数十亿时间边缘的网络上，而基线不能处理如此大的网络。我们使用我们的技术来分析一个金融网络，并表明我们的公式揭示了重要的网络结构，如突发的时间事件和具有相似兴趣的用户群体。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scalable+Temporal+Motif+Densest+Subnetwork+Discovery)|1|
|[UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction](https://doi.org/10.1145/3637528.3671662)|Yuan Yuan, Jingtao Ding, Jie Feng, Depeng Jin, Yong Li|Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China|Urban spatio-temporal prediction is crucial for informed decision-making, such as traffic management, resource optimization, and emergence response. Despite remarkable breakthroughs in pretrained natural language models that enable one model to handle diverse tasks, a universal solution for spatio-temporal prediction remains challenging. Existing prediction approaches are typically tailored for specific spatio-temporal scenarios, requiring task-specific model designs and extensive domain-specific training data. In this study, we introduce UniST, a universal model designed for general urban spatio-temporal prediction across a wide range of scenarios. Inspired by large language models, UniST achieves success through: (i) utilizing diverse spatio-temporal data, (ii) effective pre-training to capture complex spatio-temporal relationships, (iii) spatio-temporal knowledge-guided prompts to enhance generalization capabilities. These designs together unlock the potential of building a universal model for various scenarios. Extensive experiments on more than 20 spatio-temporal scenarios demonstrate UniST's efficacy in advancing state-of-the-art performance, especially in few-shot and zero-shot prediction. The datasets and code implementation are released on https://github.com/tsinghua-fib-lab/UniST.|城市时空预测对于交通管理、资源优化和应急响应等知情决策至关重要。尽管在预先训练的自然语言模型中取得了显著的突破，使得一个模型能够处理不同的任务，但时空预测的通用解决方案仍然具有挑战性。现有的预测方法通常针对特定的时空场景，需要特定于任务的模型设计和大量特定于领域的训练数据。在这项研究中，我们介绍了 UniST，一个通用的模型设计的一般城市时空预测在广泛的情景。受到大型语言模型的启发，UniST 通过以下方式取得成功: (i)利用不同的时空数据，(ii)有效的预训练以捕获复杂的时空关系，(iii)时空知识引导的提示以增强泛化能力。这些设计一起释放了为各种场景构建通用模型的潜力。在20多个时空场景上的大量实验证明了 UniST 在提高最先进性能方面的功效，特别是在小镜头和零镜头预测方面。数据集和代码实现在 https://github.com/tsinghua-fib-lab/unist 上发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=UniST:+A+Prompt-Empowered+Universal+Model+for+Urban+Spatio-Temporal+Prediction)|1|
|[UrbanGPT: Spatio-Temporal Large Language Models](https://doi.org/10.1145/3637528.3671578)|Zhonghang Li, Lianghao Xia, Jiabin Tang, Yong Xu, Lei Shi, Long Xia, Dawei Yin, Chao Huang|South China University of Technology & The University of Hong Kong, Guangzhou, China; South China University of Technology, Guangzhou, China; Baidu Inc., Beijing, China; The University of Hong Kong, Hong Kong SAR, China|Spatio-temporal prediction aims to forecast and gain insights into the ever-changing dynamics of urban environments across both time and space. Its purpose is to anticipate future patterns, trends, and events in diverse facets of urban life, including transportation, population movement, and crime rates. Although numerous efforts have been dedicated to developing neural network techniques for accurate predictions on spatio-temporal data, it is important to note that many of these methods heavily depend on having sufficient labeled data to generate precise spatio-temporal representations. Unfortunately, the issue of data scarcity is pervasive in practical urban sensing scenarios. In certain cases, it becomes challenging to collect any labeled data from downstream scenarios, intensifying the problem further. Consequently, it becomes necessary to build a spatio-temporal model that can exhibit strong generalization capabilities across diverse spatio-temporal learning scenarios. Taking inspiration from the remarkable achievements of large language models (LLMs), our objective is to create a spatio-temporal LLM that can exhibit exceptional generalization capabilities across a wide range of downstream urban tasks. To achieve this objective, we present the UrbanGPT, which seamlessly integrates a spatio-temporal dependency encoder with the instruction-tuning paradigm. This integration enables LLMs to comprehend the complex inter-dependencies across time and space, facilitating more comprehensive and accurate predictions under data scarcity. To validate the effectiveness of our approach, we conduct extensive experiments on various public datasets, covering different spatio-temporal prediction tasks. The results consistently demonstrate that our UrbanGPT, with its carefully designed architecture, consistently outperforms state-of-the-art baselines. These findings highlight the potential of building large language models for spatio-temporal learning, particularly in zero-shot scenarios where labeled data is scarce. The code and data are available at: https://github.com/HKUDS/UrbanGPT.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=UrbanGPT:+Spatio-Temporal+Large+Language+Models)|1|
|[Choosing a Proxy Metric from Past Experiments](https://doi.org/10.1145/3637528.3671543)|Nilesh Tripuraneni, Lee Richardson, Alexander D'Amour, Jacopo Soriano, Steve Yadlowsky||In many randomized experiments, the treatment effect of the long-term metric (i.e. the primary outcome of interest) is often difficult or infeasible to measure. Such long-term metrics are often slow to react to changes and sufficiently noisy they are challenging to faithfully estimate in short-horizon experiments. A common alternative is to measure several short-term proxy metrics in the hope they closely track the long-term metric -- so they can be used to effectively guide decision-making in the near-term. We introduce a new statistical framework to both define and construct an optimal proxy metric for use in a homogeneous population of randomized experiments. Our procedure first reduces the construction of an optimal proxy metric in a given experiment to a portfolio optimization problem which depends on the true latent treatment effects and noise level of experiment under consideration. We then denoise the observed treatment effects of the long-term metric and a set of proxies in a historical corpus of randomized experiments to extract estimates of the latent treatment effects for use in the optimization problem. One key insight derived from our approach is that the optimal proxy metric for a given experiment is not apriori fixed; rather it should depend on the sample size (or effective noise level) of the randomized experiment for which it is deployed. To instantiate and evaluate our framework, we employ our methodology in a large corpus of randomized experiments from an industrial recommendation system and construct proxy metrics that perform favorably relative to several baselines.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Choosing+a+Proxy+Metric+from+Past+Experiments)|1|
|[A Review of Graph Neural Networks in Epidemic Modeling](https://doi.org/10.1145/3637528.3671455)|Zewen Liu, Guancheng Wan, B. Aditya Prakash, Max S. Y. Lau, Wei Jin|Emory University, Atlanta, GA, USA; Georgia Institute of Technology, Atlanta, GA, USA|Since the onset of the COVID-19 pandemic, there has been a growing interestin studying epidemiological models. Traditional mechanistic modelsmathematically describe the transmission mechanisms of infectious diseases.However, they often fall short when confronted with the growing challenges oftoday. Consequently, Graph Neural Networks (GNNs) have emerged as aprogressively popular tool in epidemic research. In this paper, we endeavor tofurnish a comprehensive review of GNNs in epidemic tasks and highlightpotential future directions. To accomplish this objective, we introducehierarchical taxonomies for both epidemic tasks and methodologies, offering atrajectory of development within this domain. For epidemic tasks, we establisha taxonomy akin to those typically employed within the epidemic domain. Formethodology, we categorize existing work into Neural Models andHybrid Models. Following this, we perform an exhaustive and systematicexamination of the methodologies, encompassing both the tasks and theirtechnical details. Furthermore, we discuss the limitations of existing methodsfrom diverse perspectives and systematically propose future researchdirections. This survey aims to bridge literature gaps and promote theprogression of this promising field. We hope that it will facilitate synergiesbetween the communities of GNNs and epidemiology, and contribute to theircollective progress.|自2019冠状病毒疾病大流行开始以来，人们对研究流行病学模型的兴趣日益浓厚。传统的机械模型从数学上描述了传染病的传导机制。然而，当他们面对今天日益增长的挑战时，他们往往力不从心。因此，图形神经网络(GNN)已经成为流行病学研究中一种日益流行的工具。在本文中，我们努力提供一个全面的回顾 GNN 在流行病的任务和突出潜在的未来方向。为了实现这一目标，我们引入了流行病任务和方法的层次分类法，提供了这一领域的发展轨迹。对于传染病任务，我们建立了类似于传染病领域中典型使用的分类法。在方法论上，我们将现有的工作分为神经模型和混合模型。接下来，我们对方法论进行了详尽而系统的检查，包括任务和技术细节。此外，从不同角度讨论了现有方法的局限性，并系统地提出了未来的研究方向。这项调查旨在弥补文献差距，促进这个有前途的领域的进展。我们希望这将促进 GNN 和流行病学社区之间的协同作用，并为他们的共同进步做出贡献。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Review+of+Graph+Neural+Networks+in+Epidemic+Modeling)|1|
|[Cross-Domain LifeLong Sequential Modeling for Online Click-Through Rate Prediction](https://doi.org/10.1145/3637528.3671601)|Ruijie Hou, Zhaoyang Yang, Ming Yu, Hongyu Lu, Zhuobin Zheng, Yu Chen, Qinsong Zeng, Ming Chen|Wechat, Tencent, Beijing, China; Wechat, Tencent, Guangzhou, China|Lifelong sequential modeling (LSM) has significantly advanced recommendation systems on social media platforms. Diverging from single-domain LSM, cross-domain LSM involves modeling lifelong behavior sequences from a source domain to a different target domain. In this paper, we propose the Lifelong Cross Network (LCN), a novel approach for cross-domain LSM. LCN features a Cross Representation Production (CRP) module that utilizes contrastive loss to improve the learning of item embeddings, effectively bridging items across domains. This is important for enhancing the retrieval of relevant items in cross-domain lifelong sequences. Furthermore, we propose the Lifelong Attention Pyramid (LAP) module, which contains three cascading attention levels. By adding an intermediate level and integrating the results from all three levels, the LAP module can capture a broad spectrum of user interests and ensure gradient propagation throughout the sequence. The proposed LAP can also achieve remarkable consistency across attention levels, making it possible to further narrow the candidate item pool of the top level. This allows for the use of advanced attention techniques to effectively mitigate the impact of the noise in cross-domain sequences and improve the non-linearity of the representation, all while maintaining computational efficiency. Extensive experiments conducted on both a public dataset and an industrial dataset from the WeChat Channels platform reveal that the LCN outperforms current methods in terms of prediction accuracy and online performance metrics.|终身顺序建模(LSM)在社交媒体平台上拥有非常先进的推荐系统。与单域 LSM 不同，跨域 LSM 涉及从源域到不同目标域的终身行为序列建模。本文提出了一种新的跨域 LSM 方法——终身交叉网络(LCN)。LCN 提供了一个交叉表示生成(CRP)模块，该模块利用对比度损失来改进项目嵌入的学习，有效地跨域连接项目。这对于提高跨域终身序列中相关项目的检索是非常重要的。此外，我们提出了终身注意金字塔(LAP)模块，它包含三个级联注意水平。通过增加一个中间层，并整合来自所有三个层次的结果，LAP 模块可以捕获广泛的用户兴趣，并确保梯度传播整个序列。提出的 LAP 还可以在不同的注意水平上实现显著的一致性，从而有可能进一步缩小最高水平的候选项库。这允许使用先进的注意力技术，以有效地减轻跨域序列中噪声的影响，并改善非线性表示，同时保持计算效率。在公共数据集和来自微信频道平台的工业数据集上进行的大量实验表明，LCN 在预测准确性和在线性能指标方面优于目前的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cross-Domain+LifeLong+Sequential+Modeling+for+Online+Click-Through+Rate+Prediction)|0|
|[Mitigating Pooling Bias in E-commerce Search via False Negative Estimation](https://doi.org/10.1145/3637528.3671630)|Xiaochen Wang, Xiao Xiao, Ruhan Zhang, Xuan Zhang, Taesik Na, Tejaswi Tenneti, Haixun Wang, Fenglong Ma|The Pennsylvania State University, University Park, PA, USA; Instacart, San Francisco, CA, USA|Efficient and accurate product relevance assessment is critical for user experiences and business success. Training a proficient relevance assessment model requires high-quality query-product pairs, often obtained through negative sampling strategies. Unfortunately, current methods introduce pooling bias by mistakenly sampling false negatives, diminishing performance and business impact. To address this, we present Bias-mitigating Hard Negative Sampling (BHNS), a novel negative sampling strategy tailored to identify and adjust for false negatives, building upon our original False Negative Estimation algorithm. Our experiments in the Instacart search setting confirm BHNS as effective for practical e-commerce use. Furthermore, comparative analyses on public dataset showcase its domain-agnostic potential for diverse applications.|高效、准确的产品相关性评估对于用户体验和商业成功至关重要。训练一个熟练的相关性评估模型需要高质量的查询产品对，通常通过负抽样策略获得。不幸的是，目前的方法通过错误地采样错误的否定，减少性能和业务影响引入汇集偏见。为了解决这个问题，我们提出了消除偏差的硬负采样(BHNS) ，一种新的负采样策略，专门用于识别和调整假阴性，建立在我们原来的假阴性估计算法的基础上。我们在 Instacart 搜索设置中的实验证实了 BHNS 对于实际电子商务的使用是有效的。此外，对公共数据集的比较分析表明，其领域不可知的潜力适用于不同的应用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mitigating+Pooling+Bias+in+E-commerce+Search+via+False+Negative+Estimation)|0|
|[Automatic Multi-Task Learning Framework with Neural Architecture Search in Recommendations](https://doi.org/10.1145/3637528.3671715)|Shen Jiang, Guanghui Zhu, Yue Wang, Chunfeng Yuan, Yihua Huang|State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China|Multi-task learning (MTL), which aims to make full use of knowledge contained in multiple tasks to enhance overall performance and efficiency, has been broadly applied in recommendations. The main challenge for MTL models is negative transfer. Existing MTL models, mainly built on the Mixture-of-Experts (MoE) structure, seek enhancements in performance through feature selection and specific expert sharing mode design. However, one expert sharing mode may not be universally applicable due to the complex correlations and diverse demands among various tasks. Additionally, homogeneous expert architectures in such models further limit their performance. To address these issues, in this paper, we propose an innovative automatic MTL framework, AutoMTL, leveraging neural architecture search (NAS) to design optimal expert architectures and sharing modes. The Dual-level Expert Sharing mode and Architecture Navigator (DESAN) search space of AutoMTL can not only efficiently explore expert sharing modes and feature selection schemes but also focus on the architectures of expert subnetworks. Along with this, we introduce an efficient Progressively Discretizing Differentiable Architecture Search (PD-DARTS) algorithm for search space exploration. Extensive experiments demonstrate that AutoMTL can consistently outperform state-of-the-art, human-crafted MTL models. Moreover, the insights obtained from the discovered architectures provide valuable guidance for building new multi-task recommendation models.|多任务学习(Multi-Task Learning，MTL)旨在充分利用多任务中包含的知识，提高整体绩效和效率，已被广泛应用于建议学习中。MTL 模型的主要挑战是负迁移。现有的 MTL 模型主要建立在专家混合(MoE)结构的基础上，通过特征选择和专家共享模式设计来提高性能。然而，由于各种任务之间复杂的相关性和不同的需求，一种专家共享模式并不能普遍适用。此外，这些模型中的同类专家体系结构进一步限制了它们的性能。为了解决这些问题，本文提出了一种创新的自动 MTL 框架 AutoMTL，利用神经结构搜索(NAS)来设计最优的专家结构和共享模式。AutoMTL 的双层专家共享模式和体系结构导航器(DESAN)搜索空间不仅可以有效地探索专家共享模式和特征选择方案，而且可以集中研究专家子网的体系结构。在此基础上，提出了一种高效的逐次离散可微体系结构搜索(PD-DARTS)算法。大量的实验表明，AutoMTL 可以持续优于最先进的，人工制作的 MTL 模型。此外，从所发现的体系结构中获得的见解为构建新的多任务推荐模型提供了有价值的指导。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automatic+Multi-Task+Learning+Framework+with+Neural+Architecture+Search+in+Recommendations)|0|
|[CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation](https://doi.org/10.1145/3637528.3671901)|Junda Wu, ChengChun Chang, Tong Yu, Zhankui He, Jianing Wang, Yupeng Hou, Julian J. McAuley|Adobe Research, San Jose, CA, USA; University of California San Diego, La Jolla, CA, USA; Columbia University, New York, NY, USA|The long-tail recommendation is a challenging task for traditionalrecommender systems, due to data sparsity and data imbalance issues. The recentdevelopment of large language models (LLMs) has shown their abilities incomplex reasoning, which can help to deduce users' preferences based on veryfew previous interactions. However, since most LLM-based systems rely on items'semantic meaning as the sole evidence for reasoning, the collaborativeinformation of user-item interactions is neglected, which can cause the LLM'sreasoning to be misaligned with task-specific collaborative information of thedataset. To further align LLMs' reasoning to task-specific user-iteminteraction knowledge, we introduce collaborative retrieval-augmented LLMs,CoRAL, which directly incorporate collaborative evidence into the prompts.Based on the retrieved user-item interactions, the LLM can analyze shared anddistinct preferences among users, and summarize the patterns indicating whichtypes of users would be attracted by certain items. The retrieved collaborativeevidence prompts the LLM to align its reasoning with the user-item interactionpatterns in the dataset. However, since the capacity of the input prompt islimited, finding the minimally-sufficient collaborative information forrecommendation tasks can be challenging. We propose to find the optimalinteraction set through a sequential decision-making process and develop aretrieval policy learned through a reinforcement learning (RL) framework,CoRAL. Our experimental results show that CoRAL can significantly improve LLMs'reasoning abilities on specific recommendation tasks. Our analysis also revealsthat CoRAL can more efficiently explore collaborative information throughreinforcement learning.|由于数据稀疏和数据不平衡的问题，长尾推荐对于传统的推荐系统来说是一项具有挑战性的任务。大型语言模型(LLM)的最新发展已经显示出它们在复杂推理方面的能力，这种能力可以帮助推断用户基于极少数以前的交互的偏好。然而，由于大多数基于 LLM 的系统依赖于项目的语义作为推理的唯一证据，用户-项目交互的协作信息被忽视，这可能导致 LLM 的推理与数据集的特定任务的协作信息不一致。为了进一步将 LLM 的推理与特定于任务的用户项目交互知识结合起来，我们引入了协作检索增强 LLM，CoRAL，它直接将协作证据合并到提示中。基于检索到的用户-项目交互，LLM 可以分析用户之间的共享和不同偏好，并总结模式，指出哪些类型的用户会被某些项目吸引。检索到的协作证据提示 LLM 使其推理与数据集中的用户项交互模式保持一致。然而，由于输入提示的能力是有限的，找到最低限度-足够的协作信息的推荐任务可能是具有挑战性的。我们建议通过一个连续的决策过程来寻找最佳的交互集合，并通过一个强化学习(RL)框架 CoRAL 来发展检索策略。实验结果表明，CoRAL 可以显著提高 LLM 对特定推荐任务的推理能力。我们的分析还显示，通过强化学习，CoRAL 可以更有效地探索协作信息。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CoRAL:+Collaborative+Retrieval-Augmented+Large+Language+Models+Improve+Long-tail+Recommendation)|0|
|[Text Matching Indexers in Taobao Search](https://doi.org/10.1145/3637528.3671654)|Sen Li, Fuyu Lv, Ruqing Zhang, Dan Ou, Zhixuan Zhang, Maarten de Rijke|University of Amsterdam, Amsterdam, Netherlands; CAS Key Lab of Network Data Science and Technology, ICT, CAS, Beijing, China; Alibaba Group, Hangzhou, China|Product search is an important service on Taobao, the largest e-commerce platform in China. Through this service, users can easily find products relevant to their specific needs. Coping with billion-size query loads, Taobao product search has traditionally relied on classical term-based retrieval models due to their powerful and interpretable indexes. In essence, efficient retrieval hinges on the proper storage of the inverted index. Recent successes involve reducing the size (pruning) of the inverted index but the construction and deployment of lossless static index pruning in practical product search still pose non-trivial challenges. In this work, we introduce a novel SM art INDexing (SMIND) solution in Taobao product search. SMIND is designed to reduce information loss during the static pruning process by incorporating user search preferences. Specifically, we first construct "user-query-item'' hypergraphs for four different search preferences, namely purchase, click, exposure, and relevance. Then, we develop an efficient TermRank algorithm applied to these hypergraphs, to preserve relevant items based on specific user preferences during the pruning of the inverted indexer. Our approach offers fresh insights into the field of product search, emphasizing that term dependencies in user search preferences go beyond mere text relevance. Moreover, to address the vocabulary mismatch problem inherent in term-based models, we also incorporate an multi-granularity semantic retrieval model to facilitate semantic matching. Empirical results from both offline evaluation and online A/B tests showcase the superiority of SMIND over state-of-the-art methods, especially in commerce metrics with significant improvements of 1.34% in Pay Order Count and 1.50% in Gross Merchandise Value. Besides, SMIND effectively mitigates the Matthew effect of user queries and has been in service for hundreds of millions of daily users since November 2022.|产品搜索是中国最大的电子商务平台淘宝上的一项重要服务。通过这项服务，用户可以很容易地找到与他们的具体需求相关的产品。为了应对数十亿大小的查询负载，淘宝产品搜索传统上依赖于传统的基于词汇的检索模型，因为它们的索引功能强大且易于解释。实质上，有效的检索取决于适当存储倒排索引。最近的成功包括减少了反向索引的大小(修剪) ，但是在实际的产品搜索中构建和部署无损静态索引修剪仍然带来了不小的挑战。在本文中，我们介绍了一个新颖的 SM 艺术索引(SMIND)解决方案在淘宝产品搜索。SMIND 的目的是通过合并用户搜索偏好，减少静态修剪过程中的信息损失。具体来说，我们首先为四种不同的搜索偏好(即购买、点击、曝光和相关性)构建“用户查询项目”超图。然后，我们开发了一个有效的 TermRank 算法应用于这些超图，以保留相关的项目基于特定的用户喜好在反向索引器的修剪过程中。我们的方法为产品搜索领域提供了新的视角，强调用户搜索偏好中的术语依赖性不仅仅是文本相关性。此外，为了解决基于词汇的模型所固有的词汇不匹配问题，我们还引入了一个多粒度的语义检索模型来促进语义匹配。线下评估和在线 A/B 测试的实证结果表明，SMIND 相对于最先进的方法具有优势，尤其是在商业指标方面，支付订单计数和商品总价值分别显著提高了1.34% 和1.50% 。此外，SMIND 有效地减轻了用户查询的“马太效应”，自2022年11月以来已为数亿日常用户提供服务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Text+Matching+Indexers+in+Taobao+Search)|0|
|[Unified Low-rank Compression Framework for Click-through Rate Prediction](https://doi.org/10.1145/3637528.3671520)|Hao Yu, Minghao Fu, Jiandong Ding, Yusheng Zhou, Jianxin Wu|Researcher, Shanghai, China; Nanjing University, Nanjing, Jiangsu, China|Deep Click-Through Rate (CTR) prediction models play an important role in modern industrial recommendation scenarios. However, high memory overhead and computational costs limit their deployment in resource-constrained environments. Low-rank approximation is an effective method for computer vision and natural language processing models, but its application in compressing CTR prediction models has been less explored. Due to the limited memory and computing resources, compression of CTR prediction models often confronts three fundamental challenges, i.e., (1). How to reduce the model sizes to adapt to edge devices? (2). How to speed up CTR prediction model inference? (3). How to retain the capabilities of original models after compression? Previous low-rank compression research mostly uses tensor decomposition, which can achieve a high parameter compression ratio, but brings in AUC degradation and additional computing overhead. To address these challenges, we propose a unified low-rank decomposition framework for compressing CTR prediction models. We find that even with the most classic matrix decomposition SVD method, our framework can achieve better performance than the original model. To further improve the effectiveness of our framework, we locally compress the output features instead of compressing the model weights. Our unified low-rank compression framework can be applied to embedding tables and MLP layers in various CTR prediction models. Extensive experiments on two academic datasets and one real industrial benchmark demonstrate that, with 3--5× model size reduction, our compressed models can achieve both faster inference and higher AUC than the uncompressed original models. Our code is at https://github.com/yuhao318/Atomic_Feature_Mimicking.|深点进率(ctrl)预测模型在现代工业推荐方案中扮演着重要角色。但是，较高的内存开销和计算成本限制了它们在资源受限环境中的部署。低秩近似是计算机视觉和自然语言处理模型的一种有效方法，但其在压缩 CTR 预测模型方面的应用研究较少。由于有限的内存和计算资源，CTR 预测模型的压缩往往面临三个基本挑战，即(1)。如何减小模型尺寸以适应边缘设备？(2).如何加快 CTR 预测模型的推导？(3).如何保留原始模型压缩后的能力？先前的低秩压缩研究大多使用张量分解，这可以实现高参数的压缩比，但是带来了 AUC 降解和额外的计算开销。为了应对这些挑战，我们提出了一个统一的低秩分解框架来压缩 CTR 预测模型。我们发现，即使使用最经典的矩阵分解奇异值分解方法，我们的框架也能取得比原始模型更好的性能。为了进一步提高框架的有效性，我们对输出特征进行了局部压缩，而不是对模型权重进行压缩。我们统一的低秩压缩框架可以应用于各种 CTR 预测模型中的表和 MLP 层的嵌入。在两个学术数据集和一个实际工业基准上的大量实验表明，与未压缩的原始模型相比，通过3-5 × 模型尺寸的缩减，我们的压缩模型可以实现更快的推理和更高的 AUC。我们的代码是 https://github.com/yuhao318/atomic_feature_mimicking。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unified+Low-rank+Compression+Framework+for+Click-through+Rate+Prediction)|0|
|[Optimizing Smartphone App Usage Prediction: A Click-Through Rate Ranking Approach](https://doi.org/10.1145/3637528.3671567)|Yuqi Zhang, Meiying Kang, Xiucheng Li, Yu Qiu, Zhijun Li|Soochow University, Suzhou, China; Independent, Chengdu, China; Harbin Institute of Technology, Harbin, China; Harbin Institute of Technology, Shenzhen, China|Over the past decade, smartphones have become indispensable personal mobile devices, experiencing a remarkable surge in software apps. These apps empower users to seamlessly connect with various internet services, such as social communication and online shopping. Accurately predicting smartphone app usage can effectively improve user experience and optimize resource utilization. However, existing models often treat app usage prediction as a classification problem, which suffers from issues of app usage imbalance and out-of-distribution (OOD) during deployment. To address these challenges, this paper proposes a novel click-through rate (CTR) ranking-based method for predicting app usage. By transforming the classification problem into a CTR problem, we can eliminate the negative impact of the app usage imbalance issue. To address the OOD issue during deployment, we generate the app click sequence and three types of discriminative features, which enable generalization on unseen apps. The app click sequence and the three types of features serve as inputs for training a CTR estimation model in the cloud, and the trained model is then deployed on the user's smartphone to predict the CTR for each installed app. The decision-making process involves ranking these CTR values and selecting the app with the highest CTR as the final prediction. Our method has been extensively tested with large-scale app usage data. The results demonstrate that our approach is able to outperform state-of-the-art methods, with improvements over 4.93% in top-3 accuracy and 6.64% in top-5 accuracy. It achieves approximately twice the accuracy in predicting apps with low usage frequencies in comparison to baseline methods. Our method has been successfully deployed on the app recommendation system of a leading smartphone manufacturer.|在过去十年里，智能手机已经成为不可或缺的个人移动设备，软件应用程序出现了惊人的增长。这些应用程序使用户能够无缝连接各种互联网服务，如社会交流和网上购物。准确预测智能手机应用程序的使用情况可以有效地改善用户体验和优化资源利用。然而，现有的模型往往将应用程序使用预测视为一个分类问题，在部署过程中存在应用程序使用不平衡和分布不均衡(OOD)的问题。为了应对这些挑战，本文提出了一种新的基于点进率排名(ctrr)的方法来预测应用程序的使用情况。通过将分类问题转化为 CTR 问题，我们可以消除应用程序使用不平衡问题的负面影响。为了解决部署过程中的 OOD 问题，我们生成了应用程序的点击序列和三种类型的区分特性，这些特性可以对看不见的应用程序进行泛化。应用程序点击序列和三种类型的功能作为输入，用于在云中训练一个点击率估计模型，然后将训练好的模型部署到用户的智能手机上，以预测每个安装的应用程序的点击率。决策过程包括对这些点击率值进行排序，并选择点击率最高的应用程序作为最终预测。我们的方法已经通过大规模的应用程序使用数据进行了广泛的测试。结果表明，我们的方法能够优于国家的最先进的方法，提高了4.93% 以上的前3名的准确性和6.64% 以上的前5名的准确性。与基线方法相比，它在预测使用频率较低的应用程序方面达到了大约两倍的准确性。我们的方法已成功应用于一家领先的智能手机制造商的应用程序推荐系统。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+Smartphone+App+Usage+Prediction:+A+Click-Through+Rate+Ranking+Approach)|0|
|[Relevance Meets Diversity: A User-Centric Framework for Knowledge Exploration Through Recommendations](https://doi.org/10.1145/3637528.3671949)|Erica Coppolillo, Giuseppe Manco, Aristides Gionis|ICAR-CNR, Rende, Italy; Division of Theoretical Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; Department of Computer Science, University of Calabria & ICAR-CNR, Rende, Italy|Providing recommendations that are both relevant and diverse is a key consideration of modern recommender systems. Optimizing both of these measures presents a fundamental trade-off, as higher diversity typically comes at the cost of relevance, resulting in lower user engagement. Existing recommendation algorithms try to resolve this trade-off by combining the two measures, relevance and diversity, into one aim and then seeking recommendations that optimize the combined objective, for a given number of items. Traditional approaches, however, do not consider the user interaction with the suggested items. In this paper, we put the user at the central stage, and build on the interplay between relevance, diversity, and user behavior. In contrast to applications where the goal is solely to maximize engagement, we focus on scenarios aiming at maximizing the total amount of knowledge encountered by the user. We use diversity as a surrogate for the amount of knowledge obtained by the user while interacting with the system, and we seek to maximize diversity. We propose a probabilistic user-behavior model in which users keep interacting with the recommender system as long as they receive relevant suggestions, but they may stop if the relevance of the recommended items drops. Thus, for a recommender system to achieve a high-diversity measure, it will need to produce recommendations that are both relevant and diverse. Finally, we propose a novel recommendation strategy that combines relevance and diversity by a copula function. We conduct an extensive evaluation of the proposed methodology over multiple datasets, and we show that our strategy outperforms several state-of-the-art competitors. Our implementation is publicly available at https://github.com/EricaCoppolillo/EXPLORE.|提供相关和多样化的建议是现代推荐系统的一个关键考虑因素。优化这两个措施提出了一个基本的权衡，因为更高的多样性通常以相关性为代价，导致用户参与度降低。现有的推荐算法试图通过将相关性和多样性这两个衡量标准结合为一个目标来解决这种权衡，然后寻求建议，优化合并目标，以满足给定数量的项目。但是，传统方法不考虑用户与建议项的交互。在本文中，我们把用户放在中心阶段，并建立在相关性，多样性和用户行为之间的相互作用。与目标仅仅是最大化参与的应用程序不同，我们关注的场景旨在最大化用户遇到的知识总量。我们使用多样性作为用户在与系统交互时获得的知识量的替代指标，并寻求最大化多样性。我们提出了一个概率用户行为模型，在这个模型中，用户只要收到相关的建议，就会与推荐系统保持互动，但是如果推荐项目的相关性下降，他们可能会停止。因此，推荐系统要实现高度多样化的措施，就需要提出相关且多样化的建议。最后，我们提出了一个新的推荐策略，通过一个 Copula 函数结合相关性和多样性。我们在多个数据集上对提出的方法进行了广泛的评估，我们表明我们的策略优于几个最先进的竞争对手。我们的实施 https://github.com/ericacoppolillo/explore 公开发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Relevance+Meets+Diversity:+A+User-Centric+Framework+for+Knowledge+Exploration+Through+Recommendations)|0|
|[Understanding the Ranking Loss for Recommendation with Sparse User Feedback](https://doi.org/10.1145/3637528.3671565)|Zhutian Lin, Junwei Pan, Shangyu Zhang, Ximei Wang, Xi Xiao, Shudong Huang, Lei Xiao, Jie Jiang|Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Shenzhen International Graduate School, Tsinghua University, Shenzhen, Guangdong, China; Tencent Inc., Shenzhen, China|Click-through rate (CTR) prediction is a crucial area of research in online advertising. While binary cross entropy (BCE) has been widely used as the optimization objective for treating CTR prediction as a binary classification problem, recent advancements have shown that combining BCE loss with an auxiliary ranking loss can significantly improve performance. However, the full effectiveness of this combination loss is not yet fully understood. In this paper, we uncover a new challenge associated with the BCE loss in scenarios where positive feedback is sparse: the issue of gradient vanishing for negative samples. We introduce a novel perspective on the effectiveness of the auxiliary ranking loss in CTR prediction: it generates larger gradients on negative samples, thereby mitigating the optimization difficulties when using the BCE loss only and resulting in improved classification ability. To validate our perspective, we conduct theoretical analysis and extensive empirical evaluations on public datasets. Additionally, we successfully integrate the ranking loss into Tencent's online advertising system, achieving notable lifts of 0.70% and 1.26% in Gross Merchandise Value (GMV) for two main scenarios. The code is openly accessible at: https://github.com/SkylerLinn/Understanding-the-Ranking-Loss.|点进率预测是在线广告研究的一个关键领域。二进制交叉熵(BCE)已被广泛用作将 CTR 预测作为二进制分类问题处理的优化目标，但最近的研究表明，将 BCE 损失与辅助排序损失相结合可以显著提高性能。然而，这种组合损失的全部有效性尚未完全了解。在本文中，我们揭示了一个新的挑战与 BCE 损失相关的情况下，正反馈是稀疏的: 问题的梯度消失的负样本。我们介绍了一种新的视角辅助排序损失在 CTR 预测中的有效性: 它在负样本上产生较大的梯度，从而减少了优化时仅使用 BCE 损失的困难，并导致分类能力的提高。为了验证我们的观点，我们对公共数据集进行了理论分析和广泛的实证评估。此外，我们成功地将排名损失纳入腾讯的在线广告系统，在两个主要情况下，商品总值(GMV)分别显著提高了0.70% 和1.26% 。该守则可在以下 https://github.com/skylerlinn/understanding-The-ranking-loss 公开查阅:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+the+Ranking+Loss+for+Recommendation+with+Sparse+User+Feedback)|0|
|[Multi-Task Neural Linear Bandit for Exploration in Recommender Systems](https://doi.org/10.1145/3637528.3671649)|Yi Su, Haokai Lu, Yuening Li, Liang Liu, Shuchao Bi, Ed H. Chi, Minmin Chen|Google Deepmind, Mountain View, CA, USA; Google, Mountain View, CA, USA|Exposure bias and its induced feedback loop effect are well-known problems in recommender systems. Exploration is believed to be the key to break such feedback loops. While classical contextual bandit algorithms such as Upper-Confidence-Bound and Thompson Sampling have been successful in addressing the exploration-exploitation trade-off in the single-task settings with one clear reward signal, modern recommender systems often leverage multiple rich sources of feedback such as clicks, likes, dislikes, shares, satisfaction survey responses, and employ multi-task learning in practice. It is unclear how one can incorporate exploration in the multi-task setup with different objectives. In this paper, we study an efficient bandit algorithm tailored to multi-task recommender systems, named Multi-task Neural Linear Bandit (mtNLB). In particular, we investigate efficient feature embeddings in the multi-task setups that could be used as contextual features in the Neural Linear Bandit, a contextual bandit algorithm that nicely combines the representation power from DNN and simplicity in uncertainty calculation from linear models. We further study cost-effective approximations of the uncertainty estimate and principled ways to incorporate uncertainty into the multi-task scoring of items. To showcase the efficacy of our proposed method, we conduct live experiments on a large-scale commercial recommendation platform that serves billions of users. We evaluate the quality of the uncertainty estimate and demonstrate its ability to improve exploration across the different dimensions of the reward signals in comparison to baseline approaches.|在推荐系统中，曝光偏差及其诱导反馈回路效应是一个众所周知的问题。勘探被认为是打破这种反馈循环的关键。尽管传统的情境强盗算法如 Upper-Confidence-Bound 和 Thompson Sampling 已经成功地解决了单任务环境中的探索-开发权衡问题，但是现代推荐系统往往利用多种丰富的反馈来源，如点击，喜欢，不喜欢，分享，满意度调查反馈，并在实践中采用多任务学习。目前还不清楚如何将探索结合到具有不同目标的多任务设置中。本文研究了一种适用于多任务推荐系统的高效盗贼算法——多任务神经网络线性盗贼(mtNLB)。特别地，我们研究了在多任务设置中有效的特征嵌入，这些特征可以作为神经网络线性匪徒算法中的上下文特征，这种上下文匪徒算法很好地结合了 DNN 的表示能力和线性模型的不确定性计算的简单性。我们进一步研究了不确定性估计的成本效益近似，以及将不确定性纳入多任务项目评分的原则方法。为了展示我们提出的方法的有效性，我们在一个服务于数十亿用户的大规模商业推荐平台上进行了现场实验。我们评估不确定性估计的质量，并证明其能力，以改善探索的不同维度的奖励信号相比，基线方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Task+Neural+Linear+Bandit+for+Exploration+in+Recommender+Systems)|0|
|[Enhancing Pre-Ranking Performance: Tackling Intermediary Challenges in Multi-Stage Cascading Recommendation Systems](https://doi.org/10.1145/3637528.3671580)|Jianping Wei, Yujie Zhou, Zhengwei Wu, Ziqi Liu|Ant Group, HangZhou, China|Large-scale search engines and recommendation systems utilize a three-stage cascading architecture-recall, pre-ranking, and ranking-to deliver relevant results within stringent latency limits. The pre-ranking stage is crucial for filtering a large number of recalled items into a manageable set for the ranking stage, greatly affecting the system's performance. Pre-ranking faces two intermediary challenges: Sample Selection Bias (SSB) arises when training is based on ranking stage feedback but the evaluation is on a broader recall dataset. Also, compared to the ranking stage, simpler pre-rank models may perform worse and less consistently. Traditional methods to tackle SSB issues include using all recall results and treating unexposed portions as negatives for training, which can be costly and noisy. To boost performance and consistency, some pre-ranking feature interaction enhancers don't fully fix consistency issues, while methods like knowledge distillation in ranking models ignore exposure bias. Our proposed framework targets these issues with three integral modules: Sample Selection, Domain Adaptation, and Unbiased Distillation. Sample Selection filters recall results to mitigate SSB and compute costs. Domain Adaptation enhances model robustness by assigning pseudo-labels to unexposed samples. Unbiased Distillation uses exposure-independent scores from Domain Adaptation to implement unbiased distillation for the pre-ranking model. The framework focuses on optimizing pre-ranking while maintaining training efficiency. We introduce new metrics for pre-ranking evaluation, while experiments confirm the effectiveness of our framework. Our framework is also deployed in real industrial systems.|大型搜索引擎和推荐系统利用三阶段级联架构——召回、预先排序和排序——在严格的延迟限制内交付相关结果。预排序阶段对于将大量被召回的项目过滤到一个可管理的集合中以进行排序至关重要，这极大地影响了系统的性能。预排序面临两个中间挑战: 样本选择偏差(SSB)出现时，训练是基于排序阶段的反馈，但评估是在一个更广泛的召回数据集。此外，与排名阶段相比，更简单的预排名模型可能表现得更差，更不一致。解决 SSB 问题的传统方法包括使用所有的召回结果，并将未暴露的部分作为培训的负面因素，这可能是昂贵和嘈杂的。为了提高性能和一致性，一些预排序特征交互增强器不能完全解决一致性问题，而排序模型中的知识提取等方法忽略了暴露偏差。我们提出的框架通过三个整体模块来解决这些问题: 样本选择、领域适应和无偏提取。示例选择过滤器召回结果以减少 SSB 和计算成本。领域自适应通过为未暴露的样本分配伪标签来增强模型的鲁棒性。无偏蒸馏使用领域适应的暴露无关分数实现预排序模型的无偏蒸馏。该框架的重点是优化预排序，同时保持培训效率。在实验的基础上，我们引入了新的指标来进行预排序评价，并验证了该框架的有效性。我们的框架也部署在真实的工业系统中。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Pre-Ranking+Performance:+Tackling+Intermediary+Challenges+in+Multi-Stage+Cascading+Recommendation+Systems)|0|
|[Explicit and Implicit Modeling via Dual-Path Transformer for Behavior Set-informed Sequential Recommendation](https://doi.org/10.1145/3637528.3671755)|Ming Chen, Weike Pan, Zhong Ming|; Shenzhen University & Shenzhen Technology University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China|Sequential recommendation (SR) and multi-behavior sequential recommendation (MBSR) both come from real-world scenarios. Compared with SR, MBSR takes into account the dependencies of different behaviors. We find that most existing works on MBSR are studied in the context of e-commerce scenarios. In terms of the data format of the behavior types, we observe that the conventional label-formatted data carries limited information and is inadequate for scenarios like social media. With this observation, we introducebehavior set and extend MBSR to behavior set-informed sequential recommendation (BSSR). In BSSR, behavior dependencies become more complex and personalized, and user interest arousal may lack explicit contextual associations. To delve into the dynamics inhered within a behavior set and adaptively tailor recommendation lists upon its variability, we propose a novel solution called Explicit and Implicit modeling via Dual-Path Transformer (EIDP) for BSSR. Our EIDP adopts a dual-path architecture, distinguishing between explicit modeling path (EMP) and implicit modeling path (IMP) based on whether to directly incorporate the behavior representations. EMP features the personalized behavior set-wise transition pattern extractor (PBS-TPE) as its core component. It couples behavioral representations with both the items and positions to explore intra-behavior dynamics within a behavior set at a fine granularity. IMP utilizes light multi-head self-attention blocks (L-MSAB) as encoders under specific behavior types. The obtained multi-view representations are then aggregated by cross-behavior attention fusion (CBAF), using the behavior set of the next time step as a guidance to extract collaborative semantics at the behavioral level. Extensive experiments on two real-world datasets demonstrate the effectiveness of our EIDP. We release the implementation code at: https://github.com/OshiNoCSMA/EIDP.|序贯推荐(SR)和多行为序贯推荐(MBSR)都来自于现实场景。与 SR 相比，MBSR 考虑了不同行为的依赖性。我们发现现有的大多数 MBSR 的研究工作都是在电子商务环境下进行的。就行为类型的数据格式而言，我们观察到传统的标签格式的数据携带有限的信息，对于像社交媒体这样的场景来说是不够的。在此基础上，我们引入了行为集合，并将 MBSR 扩展到行为集合知情序列推荐(BSSR)。在 BSSR，行为依赖变得更加复杂和个性化，用户兴趣唤起可能缺乏明确的上下文关联。为了深入研究行为集内部的动态性，并根据其可变性自适应调整推荐列表，我们提出了一种新的解决方案，即通过双路径转换器(EIDP)对 BSSR 进行显式和隐式建模。我们的 EIDP 采用双路径结构，根据是否直接合并行为表示，区分显式建模路径(EMP)和隐式建模路径(IMP)。EMP 以个性化行为集合过渡模式提取器(PBS-TPE)为核心组件。它将行为表示与项目和位置耦合起来，以便在一个细粒度的行为集内探索行为内动态。IMP 利用轻型多头自我注意块(L-MSAB)作为特定行为类型下的编码器。然后通过交叉行为注意融合(CBAF)对所获得的多视图表示进行聚合，利用下一个时间步骤的行为集作为指导，从行为层面提取协作语义。在两个实际数据集上的大量实验证明了我们的 EIDP 算法的有效性。我们在以下 https://github.com/oshinocsma/eidp 发布实现代码:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explicit+and+Implicit+Modeling+via+Dual-Path+Transformer+for+Behavior+Set-informed+Sequential+Recommendation)|0|
|[Disentangled Multi-interest Representation Learning for Sequential Recommendation](https://doi.org/10.1145/3637528.3671800)|Yingpeng Du, Ziyan Wang, Zhu Sun, Yining Ma, Hongzhi Liu, Jie Zhang|Singapore University of Technology and Design, Singapore, Singapore; Nanyang Technological University, Singapore, Singapore; Peking University, Beijing, China|Recently, much effort has been devoted to modeling users' multi-interests (aka multi-faceted preferences) based on their behaviors, aiming to accurately capture users' complex preferences. Existing methods attempt to model each interest of users through a distinct representation, but these multi-interest representations easily collapse into similar ones due to a lack of effective guidance. In this paper, we propose a generic multi-interest method for sequential recommendation, achieving disentangled representation learning of diverse interests technically and theoretically. To alleviate the collapse issue of multi-interests, we propose to conduct item partition guided by their likelihood of being co-purchased in a global view. It can encourage items in each group to focus on a discriminated interest, thus achieving effective disentangled learning of multi-interests. Specifically, we first prove the theoretical connection between item partition and spectral clustering, demonstrating its effectiveness in alleviating item-level and facet-level collapse issues that hinder existing disentangled methods. To efficiently optimize this problem, we then propose a Markov Random Field (MRF)-based method that samples small-scale sub-graphs from two separate MRFs, thus it can be approximated with a cross-entropy loss and optimized through contrastive learning. Finally, we perform multi-task learning to seamlessly align item partition learning with multi-interest modeling for more accurate recommendation. Experiments on three real-world datasets show that our method significantly outperforms state-of-the-art methods and can flexibly integrate with existing multi-interest models as a plugin to enhance their performances.|近年来，人们致力于根据用户的行为建立用户的多重兴趣(即多方面偏好)模型，以准确捕捉用户的复杂偏好。现有的方法试图通过一个不同的表示来对用户的每个兴趣进行建模，但是由于缺乏有效的指导，这些多兴趣表示很容易崩溃成为相似的表示。本文提出了一种通用的多兴趣序列推荐方法，从理论和技术上实现了不同兴趣的分离表示学习。为了缓解多重利益的崩溃问题，我们提出在全局视角下，以多重利益共同购买的可能性为指导进行项目分割。它可以鼓励项目在每个组集中在一个歧视性的兴趣，从而实现有效的多兴趣分离学习。具体来说，我们首先证明了项目划分和 SVD 之间的理论联系，证明了它在缓解阻碍现有分离方法的项目级和面级崩溃问题方面的有效性。为了有效地优化这个问题，我们提出了一个基于马尔可夫网络(MRF)的方法，从两个不同的 MRF 中抽取小规模子图，因此它可以用交叉熵损失近似，并通过对比学习进行优化。最后，我们进行多任务学习，使项目划分学习与多兴趣模型无缝对齐，以获得更准确的推荐。在三个实际数据集上的实验表明，该方法的性能明显优于最新的方法，并且可以灵活地与现有的多兴趣模型集成，作为一个插件来提高它们的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Disentangled+Multi-interest+Representation+Learning+for+Sequential+Recommendation)|0|
|[Continual Collaborative Distillation for Recommender System](https://doi.org/10.1145/3637528.3671924)|Gyuseok Lee, SeongKu Kang, Wonbin Kweon, Hwanjo Yu|University of Illinois Urbana-Champaign, Champaign, Illinois, USA; Pohang University of Science and Technology, Pohang, Gyeongsangbuk-do, Republic of Korea|Knowledge distillation (KD) has emerged as a promising technique foraddressing the computational challenges associated with deploying large-scalerecommender systems. KD transfers the knowledge of a massive teacher system toa compact student model, to reduce the huge computational burdens for inferencewhile retaining high accuracy. The existing KD studies primarily focus onone-time distillation in static environments, leaving a substantial gap intheir applicability to real-world scenarios dealing with continuously incomingusers, items, and their interactions. In this work, we delve into a systematicapproach to operating the teacher-student KD in a non-stationary data stream.Our goal is to enable efficient deployment through a compact student, whichpreserves the high performance of the massive teacher, while effectivelyadapting to continuously incoming data. We propose Continual CollaborativeDistillation (CCD) framework, where both the teacher and the studentcontinually and collaboratively evolve along the data stream. CCD facilitatesthe student in effectively adapting to new data, while also enabling theteacher to fully leverage accumulated knowledge. We validate the effectivenessof CCD through extensive quantitative, ablative, and exploratory experiments ontwo real-world datasets. We expect this research direction to contribute tonarrowing the gap between existing KD studies and practical applications,thereby enhancing the applicability of KD in real-world systems.|知识精馏(KD)已经成为解决部署大规模推荐系统所面临的计算挑战的一种有前途的技术。KD 将大规模教师系统的知识转移到一个紧凑的学生模型，以减少推理的巨大计算负担，同时保持高精度。现有的 KD 研究主要集中在静态环境中的一次性提取，在处理不断进入的用户、项目及其交互的实际场景的适用性方面留下了很大的空白。在这项工作中，我们深入探讨了一个系统的方法来运行师生知识发展在一个非平稳的数据流。我们的目标是通过一个紧凑的学生实现有效的部署，这样可以保持大规模教师的高性能，同时有效地适应不断传入的数据。我们提出了持续协作蒸馏(CCD)框架，在这个框架中，教师和学生沿着数据流不断地、协作地发展。CCD 帮助学生有效地适应新的数据，同时也使教师能够充分利用积累的知识。我们通过在两个实际数据集上进行广泛的定量、消融和探索性实验，验证了 CCD 的有效性。我们期望这一研究方向能够缩小现有 KD 研究与实际应用之间的差距，从而提高 KD 在现实世界系统中的适用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Continual+Collaborative+Distillation+for+Recommender+System)|0|
|[Mitigating Negative Transfer in Cross-Domain Recommendation via Knowledge Transferability Enhancement](https://doi.org/10.1145/3637528.3671799)|Zijian Song, Wenhan Zhang, Lifang Deng, Jiandong Zhang, Zhihua Wu, Kaigui Bian, Bin Cui|; Lazada Group, Beijing, China|Cross-Domain Recommendation (CDR) is a promising technique to alleviate data sparsity by transferring knowledge across domains. However, the negative transfer issue in the presence of numerous domains has received limited attention. Most existing methods transfer all information from source domains to the target domain without distinction. This introduces harmful noise and irrelevant features, resulting in suboptimal performance. Although some methods decompose user features into domain-specific and domain-shared components, they fail to consider other causes of negative transfer. Worse still, we argue that simple feature decomposition is insufficient for multi-domain scenarios. To bridge this gap, we propose TrineCDR, the TRIple-level kNowledge transferability Enhanced model for multi-target CDR. Unlike previous methods, TrineCDR captures single domain and targeted cross-domain embeddings to serve multi-domain recommendation. For the latter, we identify three fundamental causes of negative transfer, ranging from micro to macro perspectives, and correspondingly enhance knowledge transferability at three different levels: the feature level, the interaction level, and the domain level. Through these efforts, TrineCDR effectively filters out noise and irrelevant information from source domains, leading to more comprehensive and accurate representations in the target domain. We extensively evaluate the proposed model on real-world datasets, sampled from Amazon and Douban, under both dual-target and multi-target scenarios. The experimental results demonstrate the superiority of TrineCDR over state-of-the-art cross-domain recommendation methods.|跨域推荐(CDR)是一种通过跨域传输知识来缓解数据稀疏性的有前途的技术。然而，在众多领域存在的负迁移问题受到的关注有限。大多数现有的方法不加区别地将所有信息从源域传输到目标域。这会引入有害的噪音和不相关的特征，导致性能不理想。尽管有些方法将用户特性分解为特定于领域和共享领域的组件，但它们没有考虑到负迁移的其他原因。更糟糕的是，我们认为简单的特征分解对于多领域场景是不够的。为了弥补这一差距，我们提出了 TrineCDR，一种针对多目标 CDR 的 TRIple-level 知识可转移性增强模型。与以前的方法不同，TrineCDR 捕获单个域和目标跨域嵌入，以服务于多域推荐。对于后者，我们从微观到宏观的角度找出了负迁移的三个基本原因，并相应地在三个不同的层面上提高了知识的可迁移性: 特征层面、交互层面和领域层面。通过这些努力，TrineCDR 有效地过滤掉源域中的噪声和不相关信息，从而在目标域中实现更全面、更准确的表示。在双目标和多目标情景下，我们对亚马逊和豆瓣采样的真实世界数据集上提出的模型进行了广泛的评估。实验结果表明 TrineCDR 方法优于目前最先进的跨域推荐方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mitigating+Negative+Transfer+in+Cross-Domain+Recommendation+via+Knowledge+Transferability+Enhancement)|0|
|[Controllable Multi-Behavior Recommendation for In-Game Skins with Large Sequential Model](https://doi.org/10.1145/3637528.3671572)|Yanjie Gou, Yuanzhou Yao, Zhao Zhang, Yiqing Wu, Yi Hu, Fuzhen Zhuang, Jiangming Liu, Yongjun Xu|; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; School of Information Science and Engineering, Yunnan University, Kunming, China; Common Data Platform, Tencent, Shenzhen, China; Institute of Artificial Intelligence, Beihang University & Zhongguancun Laboratory, Beijing, China|Online games often house virtual shops where players can acquire character skins. Our task is centered on tailoring skin recommendations across diverse scenarios by analyzing historical interactions such as clicks, usage, and purchases. Traditional multi-behavior recommendation models employed for this task are limited. They either only predict skins based on a single type of behavior or merely recommend skins for target behavior type/task. These models lack the ability to control predictions of skins that are associated with different scenarios and behaviors. To overcome these limitations, we utilize the pretraining capabilities of Large Sequential Models (LSMs) coupled with a novel stimulus prompt mechanism and build a controllable multi-behavior recommendation (CMBR) model. In our approach, the pretraining ability is used to encapsulate users' multi-behavioral sequences into the representation of users' general interests. Subsequently, our designed stimulus prompt mechanism stimulates the model to extract scenario-related interests, thus generating potential skin purchases (or clicks and other interactions) for users. To the best of our knowledge, this is the first work to provide controlled multi-behavior recommendations, and also the first to apply the pretraining capabilities of LSMs in game domain. Through offline experiments and online A/B tests, we validate our method significantly outperforms baseline models, exhibiting about a tenfold improvement on various metrics during the offline test.|在线游戏通常会有虚拟商店，玩家可以在那里获得角色皮肤。我们的任务是通过分析点击、使用和购买等历史交互，跨不同场景裁剪皮肤推荐。传统的用于此任务的多行为推荐模型是有限的。它们要么只根据单一类型的行为预测皮肤，要么只为目标行为类型/任务推荐皮肤。这些模型缺乏控制与不同场景和行为相关联的皮肤预测的能力。为了克服这些局限性，我们利用大序列模型(LSM)的预训练能力，结合一种新颖的刺激提示机制，建立了一个可控的多行为推荐(CMBR)模型。该方法利用预训练能力将用户的多行为序列封装成用户兴趣的表示。随后，我们设计的刺激提示机制刺激模型以提取场景相关的兴趣，从而为用户产生潜在的皮肤购买(或点击和其他交互)。据我们所知，这是第一个提供可控的多行为建议的工作，也是第一个应用在游戏领域的 LSM 的预训练能力。通过离线实验和在线 A/B 测试，我们验证了我们的方法明显优于基线模型，在离线测试期间在各种指标上表现出大约10倍的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Controllable+Multi-Behavior+Recommendation+for+In-Game+Skins+with+Large+Sequential+Model)|0|
|[Multi-objective Learning to Rank by Model Distillation](https://doi.org/10.1145/3637528.3671597)|Jie Tang, Huiji Gao, Liwei He, Sanjeev Katariya|Airbnb, San Francisco, CA, USA|In online marketplaces, search ranking's objective is not only to purchase or conversion (primary objective), but to also the purchase outcomes(secondary objectives), e.g. order cancellation(or return), review rating, customer service inquiries, platform long term growth. Multi-objective learning to rank has been widely studied to balance primary and secondary objectives. But traditional approaches in industry face some challenges including expensive parameter tuning leads to sub-optimal solution, suffering from imbalanced data sparsity issue, and being not compatible with ad-hoc objective. In this paper, we propose a distillation-based ranking solution for multi-objective ranking, which optimizes the end-to-end ranking system at Airbnb across multiple ranking models on different objectives along with various considerations to optimize training and serving efficiency to meet industry standards. We found it performs much better than traditional approaches, it doesn't only significantly increases primary objective by a large margin but also meet secondary objectives constraints and improve model stability. We also demonstrated the proposed system could be further simplified by model self-distillation. Besides this, we did additional simulations to show that this approach could also help us efficiently inject ad-hoc non-differentiable business objective into the ranking system while enabling us to balance our optimization objectives.|在在线市场中，搜索排名的目标不仅仅是购买或转换(主要目标) ，还包括购买结果(次要目标) ，例如取消订单(或返回) ，评论等级，客户服务查询，平台长期增长。多目标排序学习已被广泛研究，以平衡小学和中学的目标。但是，传统的方法在工业上面临着一些挑战，包括参数调整费用昂贵、数据稀疏性不平衡、与特定目标不兼容等问题。针对多目标排序问题，提出了一种基于精馏的排序方法，该方法在不同目标的多个排序模型上对 Airbnb 的端到端排序系统进行优化，同时考虑各种因素，优化培训和服务效率，以满足行业标准。我们发现它比传统的方法有更好的性能，它不仅大幅度增加了主要目标，而且满足次要目标的约束，提高了模型的稳定性。我们还证明了模型自蒸馏可以进一步简化所提出的体系。除此之外，我们做了额外的模拟，以表明这种方法也可以帮助我们有效地注入临时不可微的业务目标到排名系统，同时使我们能够平衡我们的优化目标。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-objective+Learning+to+Rank+by+Model+Distillation)|0|
|[Unified Dual-Intent Translation for Joint Modeling of Search and Recommendation](https://doi.org/10.1145/3637528.3671519)|Yuting Zhang, Yiqing Wu, Ruidong Han, Ying Sun, Yongchun Zhu, Xiang Li, Wei Lin, Fuzhen Zhuang, Zhulin An, Yongjun Xu|; Institute of Artificial Intelligence, Beihang University & Zhongguancun Laboratory, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Meituan, Beijing, China|Recommendation systems, which assist users in discovering their preferred items among numerous options, have served billions of users across various online platforms. Intuitively, users' interactions with items are highly driven by their unchanging inherent intents (e.g., always preferring high-quality items) and changing demand intents (e.g., wanting a T-shirt in summer but a down jacket in winter). However, both types of intents are implicitly expressed in recommendation scenario, posing challenges in leveraging them for accurate intent-aware recommendations. Fortunately, in search scenario, often found alongside recommendation on the same online platform, users express their demand intents explicitly through their query words. Intuitively, in both scenarios, a user shares the same inherent intent and his/her interactions may be influenced by the same demand intent. It is therefore feasible to utilize the interaction data from both scenarios to reinforce the dual intents for joint intent-aware modeling. But the joint modeling should deal with two problems: (1) accurately modeling users' implicit demand intents in recommendation; (2) modeling the relation between the dual intents and the interactive items. To address these problems, we propose a novel model named Unified Dual-Intents Translation for joint modeling of Search and Recommendation (UDITSR). To accurately simulate users' demand intents in recommendation, we utilize real queries from search data as supervision information to guide its generation. To explicitly model the relation among the triplet , we propose a dual-intent translation propagation mechanism to learn the triplet in the same semantic space via embedding translations. Extensive experiments demonstrate that UDITSR outperforms SOTA baselines both in search and recommendation tasks. Moreover, our model has been deployed online on Meituan Waimai platform, leading to an average improvement in GMV (Gross Merchandise Value) of 1.46% and CTR(Click-Through Rate) of 0.77% over one month.|推荐系统帮助用户在众多选项中发现他们喜欢的项目，已经在各种在线平台上为数十亿用户服务。直觉上，用户与物品的互动高度受到他们不变的内在意图(例如，总是喜欢高质量的物品)和不断变化的需求意图(例如，夏天想要一件 T 恤，冬天想要一件羽绒服)的驱动。然而，这两种类型的意图都隐式地在推荐场景中表达，在利用它们获得准确的意图感知建议方面提出了挑战。幸运的是，在搜索场景中，用户通过他们的查询词明确地表达他们的需求意图，这种情况经常在同一个在线平台的推荐旁边发现。直观地说，在这两种情况下，用户共享相同的内在意图，他/她的交互可能受到相同的需求意图的影响。因此，利用来自两个场景的交互数据来加强联合意图感知建模的双重意图是可行的。但是联合建模需要解决两个问题: (1)准确地建立推荐中用户隐含需求意图的模型; (2)建立双重意图与交互项目之间的关系模型。针对这些问题，本文提出了一种新的联合建模模型——统一双意图翻译模型(UDITSR)。为了准确地模拟推荐中用户的需求意图，我们利用搜索数据中的实际查询作为监督信息来指导推荐的生成。为了明确地模拟三联体之间的关系，我们提出了一种双意图翻译传播机制，通过嵌入翻译来学习同一语义空间中的三联体。大量的实验表明，UDITSR 在搜索和推荐任务中都优于 SOTA 基线。此外，我们的模型已经在 Waimai 的美团平台上使用，导致一个月内平均商品总值(GMV)提高了1.46% ，点击率(点进率)提高了0.77% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unified+Dual-Intent+Translation+for+Joint+Modeling+of+Search+and+Recommendation)|0|
|[Shopping Trajectory Representation Learning with Pre-training for E-commerce Customer Understanding and Recommendation](https://doi.org/10.1145/3637528.3671747)|Yankai Chen, QuocTuan Truong, Xin Shen, Jin Li, Irwin King|Amazon, Seattle, WA, USA; The Chinese University of Hong Kong, Hong Kong, China|Understanding customer behavior is crucial for improving service quality in large-scale E-commerce. This paper proposes C-STAR, a new framework that learns compact representations from customer shopping journeys, with good versatility to fuel multiple downstream customer-centric tasks. We define the notion of shopping trajectory that encompasses customer interactions at the level of product categories, capturing the overall flow of their browsing and purchase activities. C-STAR excels at modeling both inter-trajectory distribution similarity-the structural similarities between different trajectories, and intra-trajectory semantic correlation-the semantic relationships within individual ones. This coarse-to-fine approach ensures informative trajectory embeddings for representing customers. To enhance embedding quality, we introduce a pre-training strategy that captures two intrinsic properties within the pre-training data. Extensive evaluation on large-scale industrial and public datasets demonstrates the effectiveness of C-STAR across three diverse customer-centric tasks. These tasks empower customer profiling and recommendation services for enhancing personalized shopping experiences on our E-commerce platform.|了解顾客行为是提高大规模电子商务服务质量的关键。本文提出了一个新的框架 C-STAR，它从顾客购物旅程中学习紧凑的表示，具有良好的通用性，可以为多个下游顾客中心任务提供支持。我们定义了购物轨迹的概念，它包含了产品类别层面的客户交互，捕捉了他们浏览和购买活动的总体流程。C-STAR 在建立轨迹间分布相似性(不同轨迹之间的结构相似性)和轨迹内语义相关性(各个轨迹之间的语义关系)两方面都表现出色。这种从粗到细的方法确保为代表客户嵌入信息轨迹。为了提高嵌入质量，我们引入了一种预训练策略，捕获预训练数据的两个内在属性。对大规模工业和公共数据集的广泛评估证明了 C-STAR 在三种不同的以客户为中心的任务中的有效性。这些任务授权客户档案和推荐服务，以提高我们的电子商务平台上的个性化购物体验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Shopping+Trajectory+Representation+Learning+with+Pre-training+for+E-commerce+Customer+Understanding+and+Recommendation)|0|
|[DIET: Customized Slimming for Incompatible Networks in Sequential Recommendation](https://doi.org/10.1145/3637528.3671669)|Kairui Fu, Shengyu Zhang, Zheqi Lv, Jingyuan Chen, Jiwei Li|Zhejiang University & Shanghai Institute for Advanced Study of Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China|Due to the continuously improving capabilities of mobile edges, recommendersystems start to deploy models on edges to alleviate network congestion causedby frequent mobile requests. Several studies have leveraged the proximity ofedge-side to real-time data, fine-tuning them to create edge-specific models.Despite their significant progress, these methods require substantial on-edgecomputational resources and frequent network transfers to keep the model up todate. The former may disrupt other processes on the edge to acquirecomputational resources, while the latter consumes network bandwidth, leadingto a decrease in user satisfaction. In response to these challenges, we proposea customizeD slImming framework for incompatiblE neTworks(DIET). DIET deploysthe same generic backbone (potentially incompatible for a specific edge) to alldevices. To minimize frequent bandwidth usage and storage consumption inpersonalization, DIET tailors specific subnets for each edge based on its pastinteractions, learning to generate slimming subnets(diets) within incompatiblenetworks for efficient transfer. It also takes the inter-layer relationshipsinto account, empirically reducing inference time while obtaining more suitablediets. We further explore the repeated modules within networks and propose amore storage-efficient framework, DIETING, which utilizes a single layer ofparameters to represent the entire network, achieving comparably excellentperformance. The experiments across four state-of-the-art datasets and twowidely used models demonstrate the superior accuracy in recommendation andefficiency in transmission and storage of our framework.|由于移动边缘功能的不断改进，推荐系统开始在边缘部署模型，以缓解频繁的移动请求造成的拥塞控制。一些研究已经利用边缘接近实时数据，微调它们以创建边缘特定的模型。尽管这些方法取得了显著的进展，但它们需要大量的边缘计算资源和频繁的网络传输来保持模型的最新性。前者可能破坏边缘的其他进程以获取计算资源，而后者消耗网络带宽，导致用户满意度下降。为了应对这些挑战，我们提出了针对不兼容网络(DIET)的定制减肥框架。DIET 为所有设备部署相同的通用主干网(对于特定的边缘可能不兼容)。为了尽量减少频繁的带宽使用和个性化存储消耗，DIET 根据其过去的相互作用为每个边裁剪特定的子网，学习在不兼容的网络中生成减肥子网(饮食)以便有效地传输。它还考虑到层间关系，通过实验减少推断时间，同时获得更合适的饮食。我们进一步探讨了网络中的重复模块，并提出了一种存储效率更高的框架—— DIETING，它利用一个单一的参数层来表示整个网络，取得了较好的性能。通过四个最先进的数据集和两个广泛使用的模型进行的实验表明，我们的框架在传输和存储方面具有优越的推荐准确性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DIET:+Customized+Slimming+for+Incompatible+Networks+in+Sequential+Recommendation)|0|
|[Large Language Models meet Collaborative Filtering: An Efficient All-round LLM-based Recommender System](https://doi.org/10.1145/3637528.3671931)|Sein Kim, Hongseok Kang, Seungyoon Choi, Donghyun Kim, MinChul Yang, Chanyoung Park|KAIST, Daejeon, Republic of Korea; NAVER Corporation, Seongnam, Republic of Korea|Collaborative filtering recommender systems (CF-RecSys) have shown successiveresults in enhancing the user experience on social media and e-commerceplatforms. However, as CF-RecSys struggles under cold scenarios with sparseuser-item interactions, recent strategies have focused on leveraging modalityinformation of user/items (e.g., text or images) based on pre-trained modalityencoders and Large Language Models (LLMs). Despite their effectiveness undercold scenarios, we observe that they underperform simple traditionalcollaborative filtering models under warm scenarios due to the lack ofcollaborative knowledge. In this work, we propose an efficient All-roundLLM-based Recommender system, called A-LLMRec, that excels not only in the coldscenario but also in the warm scenario. Our main idea is to enable an LLM todirectly leverage the collaborative knowledge contained in a pre-trainedstate-of-the-art CF-RecSys so that the emergent ability of the LLM as well asthe high-quality user/item embeddings that are already trained by thestate-of-the-art CF-RecSys can be jointly exploited. This approach yields twoadvantages: (1) model-agnostic, allowing for integration with various existingCF-RecSys, and (2) efficiency, eliminating the extensive fine-tuning typicallyrequired for LLM-based recommenders. Our extensive experiments on variousreal-world datasets demonstrate the superiority of A-LLMRec in variousscenarios, including cold/warm, few-shot, cold user, and cross-domainscenarios. Beyond the recommendation task, we also show the potential ofA-LLMRec in generating natural language outputs based on the understanding ofthe collaborative knowledge by performing a favorite genre prediction task. Ourcode is available at https://github.com/ghdtjr/A-LLMRec .|协同过滤推荐系统(CF-recsys)在增强社交媒体和电子商务平台的用户体验方面取得了成功。然而，由于 CF-RecSys 在冷场景下与稀疏用户-项目交互的斗争，最近的策略集中在基于预先训练的 modalityencoders 和 Large Language Model (LLM)利用用户/项目(例如文本或图像)的模态信息。尽管它们在低温情景下有效，但是我们观察到，由于缺乏协作知识，它们在温暖情景下表现不如传统的简单协作过滤模型。在这项工作中，我们提出了一个高效的基于全局 LLM 的推荐系统，称为 A-LLmrec，它不仅在冷场景中表现出色，而且在暖场景中也表现出色。我们的主要想法是使 LLM 能够直接利用包含在预先培训的最先进的 CF-RecSys 中的协作知识，以便 LLM 的应急能力以及已经由最先进的 CF-RecSys 培训的高质量用户/项目嵌入能够被共同利用。这种方法有两个优点: (1)模型无关，允许与各种现有的 CF-RecSys 集成; (2)效率，消除了基于 LLM 的推荐程序通常需要的广泛的微调。我们在各种真实世界数据集上的广泛实验证明了 A-LLMRec 在各种场景下的优越性，包括冷/温、少拍摄、冷用户和跨域场景。除了推荐任务，我们还展示了 A-LLMRec 的潜力，通过执行一个喜欢的体裁预测任务，基于对协作知识的理解来生成自然语言输出。我们的代码可以在 https://github.com/ghdtjr/a-llmrec 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+meet+Collaborative+Filtering:+An+Efficient+All-round+LLM-based+Recommender+System)|0|
|[Probabilistic Attention for Sequential Recommendation](https://doi.org/10.1145/3637528.3671733)|Yuli Liu, Christian Walder, Lexing Xie, Yiqun Liu|; Australian National University & Data61 CSIRO, Canberra, Australia; Google Research, Brain Team, Montreal, Canada|Sequential Recommendation (SR) navigates users' dynamic preferences through modeling their historical interactions. The incorporation of the popular Transformer framework, which captures long relationships through pairwise dot products, has notably benefited SR. However, prevailing research in this domain faces three significant challenges: (i) Existing studies directly adopt the primary component of Transformer (i.e., the self-attention mechanism), without a clear explanation or tailored definition for its specific role in SR; (ii) The predominant focus on pairwise computations overlooks the global context or relative prevalence of item pairs within the overall sequence; (iii) Transformer primarily pursues relevance-dominated relationships, neglecting another essential objective in recommendation, i.e., diversity. In response, this work introduces a fresh perspective to elucidate the attention mechanism in SR. Here, attention is defined as dependency interactions among items, quantitatively determined under a global probabilistic model by observing the probabilities of corresponding item subsets. This viewpoint offers a precise and context-specific definition of attention, leading to the design of a distinctive attention mechanism tailored for SR. Specifically, we transmute the well-formulated global, repulsive interactions in Determinantal Point Processes (DPPs) to effectively model dependency interactions. Guided by the repulsive interactions, a theoretically and practically feasible DPP kernel is designed, enabling our attention mechanism to directly consider category/topic distribution for enhancing diversity. Consequently, the Probabilistic Attention mechanism (PAtt) for sequential recommendation is developed. Experimental results demonstrate the excellent scalability and adaptability of our attention mechanism, which significantly improves recommendation performance in terms of both relevance and diversity.|顺序推荐(SR)通过建模用户的历史交互来导航用户的动态偏好。通过成对点产品捕获长关系的流行的 Transformer 框架的结合，使 SR 受益匪浅。然而，这个领域的主流研究面临三个重大挑战: (i)现有的研究直接采用 Transformer 的主要组成部分(即自我注意机制) ，没有明确的解释或量身定制的定义其在 SR 中的具体作用; (ii)成对计算的主要重点忽视了整体序列中项目对的全局上下文或相对流行程度; (iii) Transformer 主要追求相关性主导的关系，忽视了推荐中的另一个基本目标，即多样性。作为回应，本文引入了一个新的视角来阐明注意机制。这里，注意被定义为项目之间的依赖交互作用，在全局概率模型下通过观察相应项目子集的概率来定量确定。这一观点提供了一个精确的和具体的上下文关注的定义，导致了一个独特的注意机制的设计专门为 SR。具体来说，我们转换了确定性点过程(DPP)中的良好制定的全局排斥交互作用，以有效地建模依赖交互作用。在排斥交互作用的指导下，设计了一个理论上和实际上可行的 DPP 核，使我们的注意机制能够直接考虑类别/主题分布，从而增强多样性。为此，提出了序贯推荐的概率注意机制。实验结果表明，该注意机制具有良好的可扩展性和适应性，在相关性和多样性方面显著提高了推荐性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Probabilistic+Attention+for+Sequential+Recommendation)|0|
|[Diffusion-Based Cloud-Edge-Device Collaborative Learning for Next POI Recommendations](https://doi.org/10.1145/3637528.3671743)|Jing Long, Guanhua Ye, Tong Chen, Yang Wang, Meng Wang, Hongzhi Yin|The University of Queensland, Brisbane, Australia; Beijing University of Posts and Telecommunications, BeiJing, China; Hefei University of Technology, Hefei, China|The rapid expansion of Location-Based Social Networks (LBSNs) has highlightedthe importance of effective next Point-of-Interest (POI) recommendations, whichleverage historical check-in data to predict users' next POIs to visit.Traditional centralized deep neural networks (DNNs) offer impressive POIrecommendation performance but face challenges due to privacy concerns andlimited timeliness. In response, on-device POI recommendations have beenintroduced, utilizing federated learning (FL) and decentralized approaches toensure privacy and recommendation timeliness. However, these methods oftensuffer from computational strain on devices and struggle to adapt to new usersand regions. This paper introduces a novel collaborative learning framework,Diffusion-Based Cloud-Edge-Device Collaborative Learning for Next POIRecommendations (DCPR), leveraging the diffusion model known for its successacross various domains. DCPR operates with a cloud-edge-device architecture tooffer region-specific and highly personalized POI recommendations whilereducing on-device computational burdens. DCPR minimizes on-devicecomputational demands through a unique blend of global and local learningprocesses. Our evaluation with two real-world datasets demonstrates DCPR'ssuperior performance in recommendation accuracy, efficiency, and adaptabilityto new users and regions, marking a significant step forward in on-device POIrecommendation technology.|基于位置的社交网络(LBSNs)的快速扩张突出了有效的下一个兴趣点(POI)建议的重要性，这些建议利用历史签入数据来预测用户下一个访问的 POI。传统的集中式深层神经网络(DNN)提供了令人印象深刻的 POI 推荐性能，但由于隐私问题和有限的时间面临挑战。作为回应，在设备上的 POI 推荐已经被引入，利用联邦学习(FL)和分散的方法来确保隐私和推荐的及时性。然而，这些方法经常受到设备计算压力的影响，难以适应新的用户和地区。本文介绍了一个新的合作学习框架，基于扩散的云端设备合作学习，用于下一个 POI 建议(DCPR) ，利用扩散模型在各个领域的成功。DCPR 采用云端设备架构，可以提供区域特定的高度个性化的 POI 建议，同时减少设备上的计算负担。DCPR 通过独特的全球和本地学习过程的混合，最大限度地减少了设备上的计算需求。我们用两个真实世界的数据集进行的评估表明，DCPR 在推荐准确性、效率以及对新用户和地区的适应性方面具有优越的性能，这标志着在设备 POI 推荐技术方面向前迈进了一大步。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Diffusion-Based+Cloud-Edge-Device+Collaborative+Learning+for+Next+POI+Recommendations)|0|
|[Certified Robustness on Visual Graph Matching via Searching Optimal Smoothing Range](https://doi.org/10.1145/3637528.3671852)|Huaqing Shao, Lanjun Wang, Yongwei Wang, Qibing Ren, Junchi Yan|SNMC, Tianjin University, Tianjin, China; SIAS and College of Computer Science, Zhejiang University, Hangzhou, China; Department of CSE and MoE Key Lab of AI, Shanghai Jiao Tong University, Shanghai, China; Department of CSE, Shanghai Jiao Tong University, Shanghai, China; School of AI and Department of CSE, Shanghai Jiao Tong University, Shanghai, China|Deep visual graph matching (GM) is a challenging combinatorial task that involves finding a permutation matrix that indicates the correspondence between keypoints from a pair of images. Like many learning systems, empirical studies have shown that visual GM is susceptible to adversarial attacks, with reliability issues in downstream applications. To the best of our knowledge, certifying robustness for deep visual GM remains an open challenge with two main difficulties: how to handle the paired inputs together with the heavily non-linear permutation output space (especially at large scale), and how to balance the trade-off between certified robustness and matching performance. Inspired by the randomized smoothing (RS) technique, we propose the Certified Robustness based on the Optimal Smoothing Range Search (CR-OSRS) technique to fulfill the robustness guarantee for deep visual GM. First, unlike conventional RS methods that use isotropic Gaussian distributions for smoothing, we build the smoothed model with paired joint Gaussian distributions, which capture the structural information among keypoints, and mitigate the performance degradation caused by smoothing. For the vast space of the permutation output, we devise a similarity-based partitioning method that can lower the computational complexity and certification difficulty. We then derive a stringent robustness guarantee that links the certified space of inputs to their corresponding fixed outputs. Second, we design a global optimization method to search for optimal joint Gaussian distributions and facilitate a larger certified space and better performance. Third, we apply data augmentation and a similarity-based regularizer in training to enhance smoothed model performance. Lastly, for the high-dimensional and multivariable nature of the certified space, we propose two methods (sampling and marginal radii) to evaluate it. Experimental results on public benchmarks show that our method achieves state-of-the-art certified robustness.|深度视觉图形匹配(GM)是一项具有挑战性的组合任务，包括从一对图像中找到一个指示关键点之间对应关系的置换矩阵。与许多学习系统一样，经验研究表明，视觉 GM 易受敌对攻击，在下游应用中存在可靠性问题。据我们所知，深度视觉 GM 的鲁棒性认证仍然是一个公开的挑战，有两个主要的困难: 如何处理配对输入和严重的非线性排列输出空间(特别是在大规模) ，以及如何平衡之间的权衡认证鲁棒性和匹配性能。受随机平滑(RS)技术的启发，我们提出了基于最优平滑范围搜索(CR-OSRS)技术的认证鲁棒性，以实现深层视觉 GM 的鲁棒性保证。首先，不同于传统的 RS 方法使用各向同性高斯分布进行平滑，我们建立了平滑模型与配对联合高斯分布，捕捉关键点之间的结构信息，并减轻平滑造成的性能下降。针对排列输出的巨大空间，提出了一种基于相似度的划分方法，降低了计算复杂度和认证难度。然后，我们推导出一个严格的鲁棒性保证，将经过验证的输入空间与它们相应的固定输出联系起来。其次，我们设计了一个全局优化方法来寻找最佳的联合高斯分布，使得更大的认证空间和更好的性能。第三，在训练中应用数据增强和基于相似性的正则化方法来提高平滑模型的性能。最后，针对证明空间的高维性和多变量性，提出了两种评价方法(抽样法和边缘半径法)。对公共基准测试的实验结果表明，该方法具有较好的鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Certified+Robustness+on+Visual+Graph+Matching+via+Searching+Optimal+Smoothing+Range)|0|
|[Pre-Training with Transferable Attention for Addressing Market Shifts in Cross-Market Sequential Recommendation](https://doi.org/10.1145/3637528.3671698)|Chen Wang, Ziwei Fan, Liangwei Yang, Mingdai Yang, Xiaolong Liu, Zhiwei Liu, Philip S. Yu|University of Illinois Chicago, Chicago, IL, USA; The University of Chicago, Chicago, IL, USA; Tsinghua University, Beijing, China; Amazon, Santa Clara, CA, USA; Salesforce AI Research, Palo Alto, CA, USA|Cross-market recommendation (CMR) involves selling the same set of items across multiple nations or regions within a transfer learning framework. However, CMR's distinctive characteristics, including limited data sharing due to privacy policies, absence of user overlap, and a shared item set between markets present challenges for traditional recommendation methods. Moreover, CMR experiences market shifts, leading to differences in item popularity and user preferences among different markets. This study focuses on cross-market sequential recommendation (CMSR) and proposes the Cross-market Attention Transferring with Sequential Recommendation (CAT-SR) framework to address these challenges and market shifts. CAT-SR incorporates a pre-training strategy emphasizing item-item correlation, selective self-attention transferring for effective transfer learning, and query and key adapters for market-specific user preferences. Experimental results on real-world cross-market datasets demonstrate the superiority of CAT-SR, and ablation studies validate the benefits of its components across different geographical continents. CAT-SR offers a robust and adaptable solution for cross-market sequential recommendation. The code is available at https://github.com/ChenMetanoia/CATSR-KDD/.|跨市场推荐(CMR)涉及在一个迁移学习框架内在多个国家或地区销售同一套产品。然而，CMR 的显著特点，包括由于隐私政策导致的有限数据共享、用户重叠的缺失以及市场之间的共享项目集，对传统的推荐方法提出了挑战。此外，CMR 经历了市场变化，导致不同市场之间的产品流行度和用户偏好的差异。本研究以跨市场序贯推荐(CMSR)为研究对象，提出了基于序贯推荐的跨市场注意力转移(CAT-SR)框架，以解决这些挑战和市场转移问题。CAT-SR 包括一个强调项目-项目相关性的预训练策略，选择性自我注意转移以有效转移学习，以及针对特定市场用户偏好的查询和关键适配器。在现实世界跨市场数据集上的实验结果证明了 CAT-SR 的优越性，并且消融研究验证了其组件在不同地理大陆上的优势。CAT-SR 为跨市场连续推荐提供了一个健壮的、适应性强的解决方案。密码可在 https://github.com/chenmetanoia/catsr-kdd/查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pre-Training+with+Transferable+Attention+for+Addressing+Market+Shifts+in+Cross-Market+Sequential+Recommendation)|0|
|[Dataset Regeneration for Sequential Recommendation](https://doi.org/10.1145/3637528.3671841)|Mingjia Yin, Hao Wang, Wei Guo, Yong Liu, Suojuan Zhang, Sirui Zhao, Defu Lian, Enhong Chen|; Huawei Singapore Research Center, Singapore, Singapore|The sequential recommender (SR) system is a crucial component of modern recommender systems, as it aims to capture the evolving preferences of users. Significant efforts have been made to enhance the capabilities of SR systems. These methods typically follow the model-centric paradigm, which involves developing effective models based on fixed datasets. However, this approach often overlooks potential quality issues and flaws inherent in the data. Driven by the potential of data-centric AI, we propose a novel data-centric paradigm for developing an ideal training dataset using a model-agnostic dataset regeneration framework called DR4SR. This framework enables the regeneration of a dataset with exceptional cross-architecture generalizability. Additionally, we introduce the DR4SR+ framework, which incorporates a model-aware dataset personalizer to tailor the regenerated dataset specifically for a target model. To demonstrate the effectiveness of the data-centric paradigm, we integrate our framework with various model-centric methods and observe significant performance improvements across four widely adopted datasets. Furthermore, we conduct in-depth analyses to explore the potential of the data-centric paradigm and provide valuable insights. The code can be found at https://github.com/USTC-StarTeam/DR4SR.|顺序推荐(SR)系统是现代推荐系统的重要组成部分，因为它旨在捕获用户不断变化的偏好。为提高 SR 系统的性能已经做出了重大努力。这些方法通常遵循以模型为中心的范式，其中包括基于固定数据集开发有效的模型。然而，这种方法常常忽略数据中潜在的质量问题和固有缺陷。在以数据为中心的人工智能的潜力驱动下，我们提出了一种新的以数据为中心的范式，用于开发一个理想的训练数据集，使用一个模型无关的数据集再生框架 DR4SR。这个框架使得数据集的再生具有异常的跨架构通用性。此外，我们还介绍了 DR4SR + 框架，该框架结合了一个模型感知的数据集个性化工具，可以专门为目标模型定制重新生成的数据集。为了证明以数据为中心的范式的有效性，我们将我们的框架与各种以模型为中心的方法集成在一起，并观察四个广泛采用的数据集的显著性能改进。此外，我们进行深入的分析，以探索潜在的数据为中心的范式，并提供有价值的见解。密码可以在 https://github.com/ustc-starteam/dr4sr 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dataset+Regeneration+for+Sequential+Recommendation)|0|
|[GPFedRec: Graph-Guided Personalization for Federated Recommendation](https://doi.org/10.1145/3637528.3671702)|Chunxu Zhang, Guodong Long, Tianyi Zhou, Zijian Zhang, Peng Yan, Bo Yang|; Computer Science and UMIACS, University of Maryland, Maryland, USA|The federated recommendation system is an emerging AI service architecture that provides recommendation services in a privacy-preserving manner. Using user-relation graphs to enhance federated recommendations is a promising topic. However, it is still an open challenge to construct the user-relation graph while preserving data locality-based privacy protection in federated settings. Inspired by a simple motivation, similar users share a similar vision (embeddings) to the same item set, this paper proposes a novel Graph-guided Personalization for Federated Recommendation (GPFedRec). The proposed method constructs a user-relation graph from user-specific personalized item embeddings at the server without accessing the users' interaction records. The personalized item embedding is locally fine-tuned on each device, and then a user-relation graph will be constructed by measuring the similarity among client-specific item embeddings. Without accessing users' historical interactions, we embody the data locality-based privacy protection of vanilla federated learning. Furthermore, a graph-guided aggregation mechanism is designed to leverage the user-relation graph and federated optimization framework simultaneously. Extensive experiments on five benchmark datasets demonstrate GPFedRec's superior performance. The in-depth study validates that GPFedRec can generally improve existing federated recommendation methods as a plugin while keeping user privacy safe. Code is available https://github.com/Zhangcx19/GPFedRec|联邦推荐系统是一种新兴的人工智能服务架构，它以保护隐私的方式提供推荐服务。使用用户关系图来增强联邦推荐是一个很有前途的课题。然而，在联邦环境中保护基于数据位置的隐私保护的同时构建用户关系图仍然是一个公开的挑战。受简单动机的启发，相似用户对同一条目集有着相似的愿景(嵌入) ，本文提出了一种新的基于图的联邦推荐个性化(GPFedRec)方法。该方法在不访问用户交互记录的情况下，通过在服务器上嵌入用户特定的个性化项目来构造用户关系图。在每个设备上对个性化项目嵌入进行局部微调，然后通过测量客户特定项目嵌入之间的相似度来构造用户关系图。在不访问用户历史交互的情况下，体现了基于数据位置的普通联邦学习的隐私保护。此外，设计了一种图引导的聚合机制，同时利用用户关系图和联邦优化框架。在五个基准数据集上的大量实验证明了 GPFedRec 的优越性能。深入的研究证实，GPFedRec 可以作为一个插件改进现有的联邦推荐方法，同时保护用户隐私安全。密码 https://github.com/zhangcx19/gpfedrec|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GPFedRec:+Graph-Guided+Personalization+for+Federated+Recommendation)|0|
|[GradCraft: Elevating Multi-task Recommendations through Holistic Gradient Crafting](https://doi.org/10.1145/3637528.3671585)|Yimeng Bai, Yang Zhang, Fuli Feng, Jing Lu, Xiaoxue Zang, Chenyi Lei, Yang Song|University of Science and Technology of China, Hefei, China; Kuaishou Technology, Beijing, China; University of Science and Technology of China & USTC Beijing Research Institute, Hefei, China|Recommender systems require the simultaneous optimization of multiple objectives to accurately model user interests, necessitating the application of multi-task learning methods. However, existing multi-task learning methods in recommendations overlook the specific characteristics of recommendation scenarios, falling short in achieving proper gradient balance. To address this challenge, we set the target of multi-task learning as attaining the appropriate magnitude balance and the global direction balance, and propose an innovative methodology named GradCraft in response. GradCraft dynamically adjusts gradient magnitudes to align with the maximum gradient norm, mitigating interference from gradient magnitudes for subsequent manipulation. It then employs projections to eliminate gradient conflicts in directions while considering all conflicting tasks simultaneously, theoretically guaranteeing the global resolution of direction conflicts. GradCraft ensures the concurrent achievement of appropriate magnitude balance and global direction balance, aligning with the inherent characteristics of recommendation scenarios. Both offline and online experiments attest to the efficacy of GradCraft in enhancing multi-task performance in recommendations. The source code for GradCraft can be accessed at https://github.com/baiyimeng/GradCraft.|推荐系统需要同时对多个目标进行优化，以准确地建立用户兴趣模型，这就需要应用多任务学习方法。然而，现有的多任务推荐学习方法忽视了推荐场景的特殊性，未能实现适当的梯度平衡。为了应对这一挑战，我们将多任务学习的目标设定为实现适当的量级平衡和全局方向平衡，并提出了一种名为“毕业设计”的创新方法。梯度工艺动态调整梯度大小，以符合最大梯度范数，减少干扰梯度大小，以便随后的操作。然后利用预测消除方向梯度冲突，同时考虑所有冲突任务，从理论上保证了方向冲突的全局解决。Gracraft 确保同时实现适当的规模平衡和全球方向平衡，与建议方案的固有特点保持一致。线下和线上的实验都证明了 GradCraft 在提高推荐中的多任务表现方面的有效性。你可透过 https://github.com/baiyimeng/GradCraft 查阅葛拉夫特的源代码。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GradCraft:+Elevating+Multi-task+Recommendations+through+Holistic+Gradient+Crafting)|0|
|[NudgeRank: Digital Algorithmic Nudging for Personalized Health](https://doi.org/10.1145/3637528.3671562)|Jodi Chiam, Aloysius Lim, Ankur Teredesai|CueZen, Inc. & University of Washington, Seattle, WA, USA; CueZen, Inc., Singapore, Singapore|In this paper we describe NudgeRankTM, an innovative digital algorithmic nudging system designed to foster positive health behaviors on a population-wide scale. Utilizing a novel combination of Graph Neural Networks augmented with an extensible Knowledge Graph, this Recommender System is operational in production, delivering personalized and context-aware nudges to over 1.1 million care recipients daily. This enterprise deployment marks one of the largest AI-driven health behavior change initiatives, accommodating diverse health conditions and wearable devices. Rigorous evaluation reveals statistically significant improvements in health outcomes, including a 6.17% increase in daily steps and 7.61% more exercise minutes. Moreover, user engagement and program enrollment surged, with a 13.1% open rate compared to baseline systems' 4%. Demonstrating scalability and reliability, NudgeRankTM operates efficiently on commodity compute resources while maintaining automation and observability standards essential for production systems.|在本文中，我们描述了 NudgeRankTM，一个创新的数字算法推动系统，旨在培养积极的健康行为在全人口范围内。利用图形神经网络与可扩展的知识图表相结合的新颖组合，这个推荐系统在生产中运作，每天向超过110万名护理接受者提供个性化和上下文感知的推动。这个企业部署标志着一个最大的人工智能驱动的健康行为改变倡议，适应不同的健康条件和可穿戴设备。严格的评估显示，健康结果在统计学上有显著的改善，包括每日步数增加6.17% ，运动时间增加7.61% 。此外，用户参与度和程序注册率也大幅上升，开放率为13.1% ，而基准系统的开放率为4% 。NudgeRankTM 展示了可伸缩性和可靠性，它可以有效地运行商品计算资源，同时维护生产系统必不可少的自动化和可观测性标准。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NudgeRank:+Digital+Algorithmic+Nudging+for+Personalized+Health)|0|
|[Achieving a Better Tradeoff in Multi-stage Recommender Systems through Personalization](https://doi.org/10.1145/3637528.3671593)|Ariel Evnine, Stratis Ioannidis, Dimitris Kalimeris, Shankar Kalyanaraman, Weiwei Li, Israel Nir, Wei Sun, Udi Weinsberg|Northeastern University, Boston, MA, USA; Meta, Menlo Park, CA, USA|Recommender systems in social media websites provide value to their communities by recommending engaging content and meaningful connections. Scaling high-quality recommendations to billions of users in real-time requires sophisticated ranking models operating on a vast number of potential items to recommend, becoming prohibitively expensive computationally. A common technique "funnels'' these items through progressively complex models ("multi-stage''), each ranking fewer items but at higher computational cost for greater accuracy. This architecture introduces a trade-off between the cost of ranking items and providing users with the best recommendations. A key observation we make in this paper is that, all else equal, ranking more items indeed improves the overall objective but has diminishing returns. Following this observation, we provide a rigorous formulation through the framework of DR-submodularity, and argue that for a certain class of objectives (reward functions), it is possible to improve the trade-off between performance and computational cost in multi-stage ranking systems with strong theoretical guarantees. We show that this class of reward functions that provide this guarantee is large and robust to various noise models. Finally, we describe extensive experimentation of our method on three real-world recommender systems in Facebook, achieving 8.8% reduction in overall compute resources with no significant impact on recommendation quality, compared to a 0.8% quality loss in a non-personalized budget allocation.|社交媒体网站的推荐系统通过推荐参与内容和有意义的联系，为社区提供价值。实时将高质量推荐扩展到数十亿用户需要对大量潜在项目进行复杂的排名模型，计算成本高得令人望而却步。一种常见的技术是通过逐步复杂的模型(“多阶段”)“漏斗”这些项目，每个项目的排名较少，但计算成本较高，以获得更高的准确性。这种体系结构在对项目排序的成本和为用户提供最佳建议之间进行了权衡。我们在本文中的一个关键观察结果是，在其他条件相同的情况下，对更多项目进行排序确实会提高整体目标，但具有报酬递减。在此基础上，我们通过 DR- 子模块化的框架提供了一个严格的公式，并认为对于一定类型的目标(奖励函数) ，在具有强理论保证的多阶段排序系统中，可以改善性能和计算成本之间的权衡。我们证明了这类提供这种保证的奖励函数对各种噪声模型是大的和鲁棒的。最后，我们描述了我们的方法在 Facebook 的三个现实世界推荐系统上的广泛实验，实现了总体计算资源减少8.8% ，对推荐质量没有显着影响，而非个性化预算分配中的质量损失为0.8% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Achieving+a+Better+Tradeoff+in+Multi-stage+Recommender+Systems+through+Personalization)|0|
|[Residual Multi-Task Learner for Applied Ranking](https://doi.org/10.1145/3637528.3671523)|Cong Fu, Kun Wang, Jiahua Wu, Yizhou Chen, Guangda Huzhang, Yabo Ni, Anxiang Zeng, Zhiming Zhou|Nanyang Technological University, Singapore, Singapore; Shopee Pte. Ltd., Singapore, Singapore; SCSE, Nanyang Technological University, Singapore, Singapore; ECONCS, Shanghai University of Finance and Economics, Shanghai, China; Shopee Pte. Ltd., Shanghai, China|Modern e-commerce platforms rely heavily on modeling diverse user feedback to provide personalized services. Consequently, multi-task learning has become an integral part of their ranking systems. However, existing multi-task learning methods encounter two main challenges: some lack explicit modeling of task relationships, resulting in inferior performance, while others have limited applicability due to being computationally intensive, having scalability issues, or relying on strong assumptions. To address these limitations and better fit our real-world scenario, pre-rank in Shopee Search, we introduce in this paper ResFlow, a lightweight multi-task learning framework that enables efficient cross-task information sharing via residual connections between corresponding layers of task networks. Extensive experiments on datasets from various scenarios and modalities demonstrate its superior performance and adaptability over state-of-the-art methods. The online A/B tests in Shopee Search showcase its practical value in large-scale industrial applications, evidenced by a 1.29% increase in OPU (order-per-user) without additional system latency. ResFlow is now fully deployed in the pre-rank module of Shopee Search. To facilitate efficient online deployment, we propose a novel offline metric Weighted Recall@K, which aligns well with our online metric OPU, addressing the longstanding online-offline metric misalignment issue. Besides, we propose to fuse scores from the multiple tasks additively when ranking items, which outperforms traditional multiplicative fusion.|现代电子商务平台在很大程度上依赖于建模不同的用户反馈来提供个性化服务。因此，多任务学习已成为其排名系统的一个组成部分。然而，现有的多任务学习方法遇到了两个主要的挑战: 一些方法缺乏对任务关系的明确建模，导致性能较差，而另一些方法由于计算密集、存在可伸缩性问题或依赖于强大的假设而适用性有限。为了解决这些局限性，并更好地适应我们的现实世界场景，在 Shopee 搜索中的预排序，本文介绍了 ResFlow，一个轻量级的多任务学习框架，使得有效的跨任务信息共享通过相应的任务网络层之间的剩余连接。对来自不同场景和模式的数据集进行的大量实验表明，它比最先进的方法具有更好的性能和适应性。Shopee Search 的在线 A/B 测试展示了其在大规模工业应用中的实用价值，在不增加系统延迟的情况下，OPU (每用户订单)增长了1.29% 。ResFlow 现在完全部署在 Shopee Search 的 pre-rank 模块中。为了促进有效的在线部署，我们提出了一种新的离线度量加权召回@K，它与我们的在线度量 OPU 很好地一致，解决了长期存在的在线-离线度量失调问题。此外，本文还提出了在项目排序时对多个任务的得分进行累加融合，这种融合方法的性能优于传统的乘法融合方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Residual+Multi-Task+Learner+for+Applied+Ranking)|0|
|[Multi-task Conditional Attention Network for Conversion Prediction in Logistics Advertising](https://doi.org/10.1145/3637528.3671549)|Baoshen Guo, Xining Song, Shuai Wang, Wei Gong, Tian He, Xue Liu|Southeast University & JD Logistics, Nanjing, China; JD Logistics, Beijing, China; University of Science and Technology of China, Hefei, China; Southeast University, Nanjing, China; McGill University, Montréal, Canada|Logistics advertising is an emerging task in online-to-offline logistics systems, where logistics companies expand parcel shipping services to new users through advertisements on shopping websites. Compared to existing online e-commerce advertising, logistics advertising has two significant new characteristics: (i) the complex factors in logistics advertising considering both users' offline logistics preference and online purchasing profiles; and (ii) data sparsity and mutual relations among multiple steps due to longer advertising conversion processes. To address these challenges, we design MCAC, a Multi-task Conditional Attention network-based logistics advertising Conversion prediction framework, which consists of (i) an offline shipping preference extraction model to extract the user's offline logistics preference from historical shipping records, and (ii) a multi-task conditional attention-based conversion rate prediction module to model mutual relations among multiple steps in logistics advertising conversion processes. We evaluate and deploy MCAC on one of the largest e-commerce platforms in China for logistics advertising. Extensive offline experiments show that our method outperforms state-of-the-art baselines in various metrics. Moreover, the conversion rate prediction results of large-scale online A/B testing show that MCAC achieves a 15.22% improvement compared to existing industrial practices, which demonstrates the effectiveness of the proposed framework.|物流广告是线上到线下物流系统中的一项新兴任务，物流公司通过在购物网站上投放广告，向新用户提供包裹运输服务。与现有的在线电子商务广告相比，物流广告具有两个重要的新特点: (1)考虑用户线下物流偏好和线上购买概况的物流广告复杂因素; (2)由于广告转换过程较长，数据稀疏和多步骤之间的相互关系。针对这些挑战，我们设计了基于多任务条件注意网络的物流广告转化预测框架 MCAC，该框架包括: (1)离线运输偏好提取模型，从历史运输记录中提取用户的离线物流偏好; (2)基于多任务条件注意的转化率预测模型，模拟物流广告转化过程中多个步骤之间的相互关系。我们评估和部署 MCAC 在中国最大的电子商务平台之一的物流广告。大量的离线实验表明，我们的方法在各种指标上都优于最先进的基线。此外，大规模在线 A/B 测试的转换率预测结果表明，与现有工业实践相比，MCAC 的转换率提高了15.22% ，证明了该框架的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-task+Conditional+Attention+Network+for+Conversion+Prediction+in+Logistics+Advertising)|0|
|[Learning to Rank for Maps at Airbnb](https://doi.org/10.1145/3637528.3671648)|Malay Haldar, Hongwei Zhang, Kedar Bellare, Sherry Chen, Soumyadip Banerjee, Xiaotang Wang, Mustafa Abdool, Huiji Gao, Pavan Tapadia, Liwei He, Sanjeev Katariya|Airbnb, Inc., San Francisco, WA, USA; Airbnb, Inc., San Francisco, CA, USA|As a two-sided marketplace, Airbnb brings together hosts who own listings for rent with prospective guests from around the globe. Results from a guest's search for listings are displayed primarily through two interfaces: (1) as a list of rectangular cards that contain on them the listing image, price, rating, and other details, referred to as list-results (2) as oval pins on a map showing the listing price, called map-results. Both these interfaces, since their inception, have used the same ranking algorithm that orders listings by their booking probabilities and selects the top listings for display. But some of the basic assumptions underlying ranking, built for a world where search results are presented as lists, simply break down for maps. This paper describes how we rebuilt ranking for maps by revising the mathematical foundations of how users interact with search results. Our iterative and experiment-driven approach led us through a path full of twists and turns, ending in a unified theory for the two interfaces. Our journey shows how assumptions taken for granted when designing machine learning algorithms may not apply equally across all user interfaces, and how they can be adapted. The net impact was one of the largest improvements in user experience for Airbnb which we discuss as a series of experimental validations.|作为一个双向的市场，Airbnb 将拥有房源的房东和来自世界各地的潜在客人聚集在一起。客人对列表的搜索结果主要通过两个界面显示: (1)作为一个矩形卡片列表，其中包含列表图像、价格、评级和其他细节，称为列表结果(2)作为椭圆形针在地图上显示列表价格，称为地图结果。这两个界面从一开始就使用相同的排名算法，根据预订概率对列表进行排序，并选择最上面的列表进行显示。但是，一些基本的排名假设，建立在一个世界里，搜索结果显示为列表，只是分解为地图。本文描述了我们如何通过修改用户与搜索结果交互的数学基础来重建地图的排名。我们的迭代和实验驱动的方法带领我们走过了一条充满曲折的道路，最终形成了两个接口的统一理论。我们的旅程表明，当设计机器学习算法时，假设是理所当然的，可能不会平等地适用于所有用户界面，以及它们是如何适应的。净影响是 Airbnb 用户体验的最大改进之一，我们将其作为一系列实验验证进行讨论。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Rank+for+Maps+at+Airbnb)|0|
|[Deep Bag-of-Words Model: An Efficient and Interpretable Relevance Architecture for Chinese E-Commerce](https://doi.org/10.1145/3637528.3671559)|Zhe Lin, Jiwei Tan, Dan Ou, Xi Chen, Shaowei Yao, Bo Zheng|Alibaba Group, HangZhou, China|Text relevance or text matching of query and product is an essential technique for the e-commerce search system to ensure that the displayed products can match the intent of the query. Many studies focus on improving the performance of the relevance model in search system. Recently, pre-trained language models like BERT have achieved promising performance on the text relevance task. While these models perform well on the offline test dataset, there are still obstacles to deploy the pre-trained language model to the online system as their high latency. The two-tower model is extensively employed in industrial scenarios, owing to its ability to harmonize performance with computational efficiency. Regrettably, such models present an opaque ''black box'' nature, which prevents developers from making special optimizations. In this paper, we raise deep Bag-o f-Words (DeepBoW) model, an efficient and interpretable relevance architecture for Chinese e-commerce. Our approach proposes to encode the query and the product into the sparse BoW representation, which is a set of word-weight pairs. The weight means the important or the relevant score between the corresponding word and the raw text. The relevance score is measured by the accumulation of the matched word between the sparse BoW representation of the query and the product. Compared to popular dense distributed representation that usually suffers from the drawback of black-box, the most advantage of the proposed representation model is highly explainable and interventionable, which is a superior advantage to the deployment and operation of online search engines. Moreover, the online efficiency of the proposed model is even better than the most efficient inner product form of dense representation. The proposed model is experimented on three different datasets for learning the sparse BoW representations, including the human-annotation set, the search-log set and the click-through set. Then the models are evaluated by experienced human annotators. Both the auto metrics and the online evaluations show our DeepBoW model achieves competitive performance while the online inference is much more efficient than the other models. Our DeepBoW model has already deployed to the biggest Chinese e-commerce search engine Taobao and served the entire search traffic for over 6 months.|查询与产品的文本相关性或文本匹配是电子商务搜索系统中保证所显示的产品与查询意图相匹配的关键技术。许多研究集中在提高搜索系统中相关性模型的性能。近年来，像 BERT 这样的预训练语言模型在文本相关性任务中取得了良好的效果。尽管这些模型在离线测试数据集上表现良好，但仍然存在将预先训练的语言模型部署到在线系统的障碍，因为它们具有较高的延迟。由于双塔模型能够协调性能和计算效率，因此在工业场景中得到了广泛的应用。遗憾的是，这样的模型呈现出一种不透明的“黑盒”特性，这阻止了开发人员进行特殊的优化。本文提出了一种面向中国电子商务的高效、易解释的关联结构——深层 Bag-o-Words 模型。我们的方法建议将查询和产品编码成稀疏的 BW 表示，这是一组字权重对。加权表示相应单词和原始文本之间的重要或相关得分。相关性得分是通过查询的稀疏弓形表示和产品之间匹配词的累积来衡量的。该模型的最大优点是可解释性强、可干预性强，优于在线搜索引擎的部署和运行。此外，该模型的在线效率甚至优于最有效的密集表示内积形式。该模型在三个不同的数据集上进行了实验，包括人工注释集、搜索日志集和点击通过集。然后由经验丰富的人工注释者对模型进行评估。自动度量和在线评估都表明，我们的 DeepBW 模型达到了竞争性能，而在线推理是更有效的比其他模型。我们的 DeepBow 模式已经部署到中国最大的电子商务搜索引擎淘宝，并服务于整个搜索流量超过6个月。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Bag-of-Words+Model:+An+Efficient+and+Interpretable+Relevance+Architecture+for+Chinese+E-Commerce)|0|
|[GRAM: Generative Retrieval Augmented Matching of Data Schemas in the Context of Data Security](https://doi.org/10.1145/3637528.3671602)|Xuanqing Liu, Runhui Wang, Yang Song, Luyang Kong|Amazon Web Services, Seattle, WA, USA|Schema matching constitutes a pivotal phase in the data ingestion process forcontemporary database systems. Its objective is to discern pairwisesimilarities between two sets of attributes, each associated with a distinctdata table. This challenge emerges at the initial stages of data analytics,such as when incorporating a third-party table into existing databases toinform business insights. Given its significance in the realm of databasesystems, schema matching has been under investigation since the 2000s. Thisstudy revisits this foundational problem within the context of large languagemodels. Adhering to increasingly stringent data security policies, our focuslies on the zero-shot and few-shot scenarios: the model should analyze only aminimal amount of customer data to execute the matching task, contrasting withthe conventional approach of scrutinizing the entire data table. We emphasizethat the zero-shot or few-shot assumption is imperative to safeguard theidentity and privacy of customer data, even at the potential cost of accuracy.The capability to accurately match attributes under such stringent requirementsdistinguishes our work from previous literature in this domain.|模式匹配是现代数据库系统数据摄取过程中的一个关键阶段。它的目标是识别两组属性之间的成对相似性，每组属性都与一个不同的数据表相关联。这一挑战出现在数据分析的初始阶段，例如将第三方表合并到现有数据库中以提供业务见解。鉴于模式匹配在数据库领域的重要性，自2000年以来，模式匹配一直在研究之中。本研究在大型语言模型的背景下重新审视这个基本问题。坚持越来越严格的数据安全策略，我们的重点是零射击和少射击场景: 模型应该只分析最小数量的客户数据来执行匹配任务，与审查整个数据表的传统方法形成对比。我们强调，零拍摄或少拍摄假设是必要的，以保护客户数据的身份和隐私，即使在潜在的准确性成本。在如此严格的要求下精确匹配属性的能力使我们的工作区别于这个领域以前的文献。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GRAM:+Generative+Retrieval+Augmented+Matching+of+Data+Schemas+in+the+Context+of+Data+Security)|0|
|[Non-autoregressive Generative Models for Reranking Recommendation](https://doi.org/10.1145/3637528.3671645)|Yuxin Ren, Qiya Yang, Yichun Wu, Wei Xu, Yalong Wang, Zhiqiang Zhang|Kuaishou Technology, Beijing, China; Tsinghua University, Beijing, China; Peking University, Beijing, China|Contemporary recommendation systems are designed to meet users' needs by delivering tailored lists of items that align with their specific demands or interests. In a multi-stage recommendation system, reranking plays a crucial role by modeling the intra-list correlations among items. The key challenge of reranking lies in the exploration of optimal sequences within the combinatorial space of permutations. Recent research proposes a generator-evaluator learning paradigm, where the generator generates multiple feasible sequences and the evaluator picks out the best sequence based on the estimated listwise score. The generator is of vital importance, and generative models are well-suited for the generator function. Current generative models employ an autoregressive strategy for sequence generation. However, deploying autoregressive models in real-time industrial systems is challenging. Firstly, the generator can only generate the target items one by one and hence suffers from slow inference. Secondly, the discrepancy between training and inference brings an error accumulation. Lastly, the left-to-right generation overlooks information from succeeding items, leading to suboptimal performance. To address these issues, we propose a Non-AutoRegressive generative model for reranking Recommendation (NAR4Rec) designed to enhance efficiency and effectiveness. To tackle challenges such as sparse training samples and dynamic candidates, we introduce a matching model. Considering the diverse nature of user feedback, we employ a sequence-level unlikelihood training objective to differentiate feasible sequences from unfeasible ones. Additionally, to overcome the lack of dependency modeling in non-autoregressive models regarding target items, we introduce contrastive decoding to capture correlations among these items. Extensive offline experiments validate the superior performance of NAR4Rec over state-of-the-art reranking methods. Online A/B tests reveal that NAR4Rec significantly enhances the user experience. Furthermore, NAR4Rec has been fully deployed in a popular video app Kuaishou with over 300 million daily active users.|当代的推荐系统通过提供符合用户特定需求或兴趣的量身定制的项目列表来满足用户的需求。在多阶段推荐系统中，重新排序通过建立项目之间的列表内相关性起着至关重要的作用。重新排序的关键挑战在于在排列的组合空间中探索最优序列。最近的研究提出了生成器-评估器学习范式，其中生成器生成多个可行序列，评估器根据估计的列表分数挑选出最佳序列。生成器是至关重要的，生成模型非常适合于生成器函数。当前的生成模型采用自回归策略进行序列生成。然而，在实时工业系统中部署自回归模型是具有挑战性的。首先，生成器只能生成一个个目标项，因此存在推理速度慢的问题。其次，训练与推理的差异带来了错误的积累。最后，从左到右的生成会忽略来自后续项的信息，从而导致性能不理想。为了解决这些问题，我们提出了一个非自动回归的重新排名建议(NAR4rec)生成模型，旨在提高效率和效力。为了解决稀疏训练样本和动态候选人等问题，我们引入了一个匹配模型。考虑到用户反馈的多样性，我们采用序列级不似然训练目标来区分可行序列和不可行序列。此外，为了克服非自回归模型中缺乏对目标项的依赖建模，我们引入对比解码来捕获这些项之间的相关性。大量的离线实验验证了 NAR4Rec 优于最先进的重新排序方法的性能。在线 A/B 测试显示 NAR4Rec 显著提高了用户体验。此外，NAR4Rec 已经完全部署在一个流行的视频应用快手中，每天有超过3亿的活跃用户。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Non-autoregressive+Generative+Models+for+Reranking+Recommendation)|0|
|[Chaining Text-to-Image and Large Language Model: A Novel Approach for Generating Personalized e-commerce Banners](https://doi.org/10.1145/3637528.3671636)|Shanu Vashishtha, Abhinav Prakash, Lalitesh Morishetti, Kaushiki Nag, Yokila Arora, Sushant Kumar, Kannan Achan|Walmart Global Tech, Sunnyvale, CA, USA|Text-to-image models such as stable diffusion have opened a plethora ofopportunities for generating art. Recent literature has surveyed the use oftext-to-image models for enhancing the work of many creative artists. Manye-commerce platforms employ a manual process to generate the banners, which istime-consuming and has limitations of scalability. In this work, we demonstratethe use of text-to-image models for generating personalized web banners withdynamic content for online shoppers based on their interactions. The novelty inthis approach lies in converting users' interaction data to meaningful promptswithout human intervention. To this end, we utilize a large language model(LLM) to systematically extract a tuple of attributes from itemmeta-information. The attributes are then passed to a text-to-image model viaprompt engineering to generate images for the banner. Our results show that theproposed approach can create high-quality personalized banners for users.|文本到图像的模型，例如稳定的扩散，已经为艺术的生成提供了大量的机会。最近的文献调查了文本到图像模型的使用，以提高许多创造性的艺术家的工作。许多电子商务平台使用手工过程来生成横幅，这非常耗时并且具有可伸缩性的限制。在这项工作中，我们演示了使用文本到图像的模型来生成个性化的网络横幅与动态内容的在线购物者基于他们的交互。这种方法的新颖之处在于无需人工干预就能将用户的交互数据转换为有意义的提示。为此，我们利用大型语言模型(LLM)系统地从 itemmeta 信息中提取属性元组。然后通过提示工程将属性传递给文本到图像模型，以生成横幅的图像。结果表明，该方法可以为用户创建高质量的个性化横幅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Chaining+Text-to-Image+and+Large+Language+Model:+A+Novel+Approach+for+Generating+Personalized+e-commerce+Banners)|0|
|[LiMAML: Personalization of Deep Recommender Models via Meta Learning](https://doi.org/10.1145/3637528.3671599)|Ruofan Wang, Prakruthi Prabhakar, Gaurav Srivastava, Tianqi Wang, Zeinab S. Jalali, Varun Bharill, Yunbo Ouyang, Aastha Nigam, Divya Venugopalan, Aman Gupta, Fedor Borisyuk, S. Sathiya Keerthi, Ajith Muralidharan|LinkedIn Corporation, Sunnyvale, CA, USA; Aliveo AI Corp, Sunnyvale, CA, USA|In the realm of recommender systems, the ubiquitous adoption of deep neural networks has emerged as a dominant paradigm for modeling diverse business objectives. As user bases continue to expand, the necessity of personalization and frequent model updates have assumed paramount significance to ensure the delivery of relevant and refreshed experiences to a diverse array of members. In this work, we introduce an innovative meta-learning solution tailored to the personalization of models for individual members and other entities, coupled with the frequent updates based on the latest user interaction signals. Specifically, we leverage the Model-Agnostic Meta Learning (MAML) algorithm to adapt per-task sub-networks using recent user interaction data. Given the near infeasibility of productionizing original MAML-based models in online recommendation systems, we propose an efficient strategy to operationalize meta-learned sub-networks in production, which involves transforming them into fixed-sized vectors, termed meta embeddings, thereby enabling the seamless deployment of models with hundreds of billions of parameters for online serving. Through extensive experimentation on production data drawn from various applications at LinkedIn, we demonstrate that the proposed solution consistently outperforms the best performing baseline models of those applications, including strong baselines such as using wide-and-deep ID based personalization approach. Our approach has enabled the deployment of a range of highly personalized AI models across diverse LinkedIn applications, leading to substantial improvements in business metrics as well as refreshed experience for our members.|在推荐系统领域，深层神经网络的普遍采用已经成为建模不同业务目标的主要范例。随着用户基础的不断扩大，个性化和频繁更新模型的必要性对于确保向各类成员提供相关的最新经验具有至关重要的意义。在这项工作中，我们介绍了一个创新的元学习解决方案，专为个人成员和其他实体的模型个性化，加上基于最新用户交互信号的频繁更新。具体来说，我们利用模型不可知元学习(MAML)算法来使用最近的用户交互数据来适应每个任务的子网络。鉴于在在线推荐系统中生产原始的基于 MAML 的模型几乎是不可行的，我们提出了一个有效的策略来操作生产中的元学习子网络，包括将它们转换成固定大小的向量，称为元嵌入，从而能够无缝部署具有数千亿参数的在线服务模型。通过对来自 LinkedIn 各种应用程序的生产数据进行广泛的实验，我们证明了所提出的解决方案始终优于这些应用程序的最佳性能基线模型，包括强大的基线，例如使用基于广泛和深度 ID 的个性化方法。我们的方法使得一系列高度个性化的人工智能模型能够在不同的 LinkedIn 应用程序中部署，从而大大改善了业务指标，并为我们的会员带来了全新的体验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LiMAML:+Personalization+of+Deep+Recommender+Models+via+Meta+Learning)|0|
|[Enhancing Asymmetric Web Search through Question-Answer Generation and Ranking](https://doi.org/10.1145/3637528.3671517)|Dezhi Ye, Jie Liu, Jiabin Fan, Bowen Tian, Tianhua Zhou, Xiang Chen, Jin Ma|Tencent PCG, Beijing, China|This paper addresses the challenge of the semantic gap between user queries and web content, commonly referred to as asymmetric text matching, within the domain of web search. By leveraging BERT for reading comprehension, current algorithms enable significant advancements in query understanding, but still encounter limitations in effectively resolving the asymmetrical ranking problem due to model comprehension and summarization constraints. To tackle this issue, we propose the QAGR (Question-Answer Generation and Ranking) method, comprising an offline module called QAGeneration and an online module called QARanking. The QAGeneration module utilizes large language models (LLMs) to generate high-quality question-answering pairs for each web page. This process involves two steps: generating question-answer pairs and performing verification to eliminate irrelevant questions, resulting in high-quality questions associated with their respective documents. The QARanking module combines and ranks the generated questions and web page content. To ensure efficient online inference, we design the QARanking model as a homogeneous dual-tower model, incorporating query intent to drive score fusion while balancing keyword matching and asymmetric matching. Additionally, we conduct a preliminary screening of questions for each document, selecting only the top-N relevant questions for further relevance calculation. Empirical results demonstrate the substantial performance improvement of our proposed method in web search. We achieve over 8.7% relative offline relevance improvement and over 8.5% online engagement gain compared to the state-of-the-art web search system. Furthermore, we deploy QAGR to online web search engines and share our deployment experience, including production considerations and ablation experiments. This research contributes to advancing the field of asymmetric web search and provides valuable insights for enhancing search engine performance.|本文讨论了在网络搜索领域中，用户查询和网页内容之间的语义差异(通常称为非对称文本匹配)所带来的挑战。通过利用 BERT 的阅读理解，当前的算法在查询理解方面取得了显著的进步，但由于模型理解和摘要约束，在有效解决非对称排序问题方面仍然存在局限性。为了解决这个问题，我们提出了 QAGR (问答生成和排序)方法，包括一个称为 QAGeneration 的离线模块和一个称为 QARanking 的在线模块。QAGeneration 模块利用大语言模型(LLM)为每个网页生成高质量的问答对。这个过程包括两个步骤: 生成问题-答案对和进行验证，以消除不相关的问题，从而产生与各自文件相关的高质量问题。QARanking 模块将生成的问题和网页内容进行组合和排序。为了保证在线推理的有效性，我们将 QARanking 模型设计成一个均匀的双塔模型，在平衡关键字匹配和非对称匹配的同时，结合查询意图驱动得分融合。此外，我们对每个文档的问题进行初步筛选，只选择排名前 N 位的相关问题进行进一步的相关性计算。实验结果表明，本文提出的方法在网络搜索中性能得到了显著提高。与最先进的网络搜索系统相比，我们实现了超过8.7% 的相对离线相关性改善和超过8.5% 的在线参与收益。此外，我们部署 QAGR 到在线网络搜索引擎和分享我们的部署经验，包括生产考虑和烧蚀实验。本文的研究有助于推进非对称网络搜索领域的发展，为提高搜索引擎性能提供了有价值的见解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Asymmetric+Web+Search+through+Question-Answer+Generation+and+Ranking)|0|
|[Unsupervised Ranking Ensemble Model for Recommendation](https://doi.org/10.1145/3637528.3671598)|Wenhui Yu, Bingqi Liu, Bin Xia, Xiaoxiao Xu, Ying Chen, Yongchang Li, Lantao Hu|Kuaishou Technology, Beijing, China|When visiting an online platform, a user generates various actions, such as clicks, long views, likes, comments, etc. To capture user preferences in these aspects, we learn these objectives and return multiple rankings of candidate items for each user. We need to aggregate them into one to truncate the candidate set, and ranking ensemble model is proposed for this task. However, there is a critical issue: though we input abundant information, what model learns depends on the supervision. Unfortunately, the existing supervision is poorly designed, leading to serious information loss issue. To address this issue, we designed an unsupervised loss to compel the ranking ensemble model to learn all information of input rankings, including sequential and numerical information. (1) For sequential information, we design a distance measure between two rankings, and train the ensemble ranking to have similar order with all input rankings by minimizing the distance. (2) For numerical information, we design a decoder to reconstruct values of original rankings from the hidden layer of the model, to guarantee that the model captures as much input information as possible. Our unsupervised loss is compatible with all ranking ensemble models. We optimize several widely-used structures to propose unsupervised ranking ensemble models. We devise comprehensive experiments on two real-world datasets to demonstrate the effectiveness of the proposed models. We also apply our model in a short video platform with billions of users, and achieve significant improvement.|当访问在线平台时，用户生成各种各样的动作，如点击、长视图、喜欢、评论等。为了捕获这些方面的用户偏好，我们学习这些目标，并返回每个用户的候选项的多个排名。为了截断候选集，我们需要将它们聚合成一个集合，并提出了排序集成模型。然而，有一个关键的问题: 虽然我们输入了大量的信息，但是模型学到了什么取决于监督。然而，现有的监管体系设计不当，导致了严重的信息流失问题。为了解决这个问题，我们设计了一个无监督的损失，以迫使排名集成模型学习所有的输入排名信息，包括顺序和数字信息。(1)对于序列信息，我们设计了两个排名之间的距离度量，并通过最小化距离训练集合排名与所有输入排名具有相似的顺序。(2)对于数值信息，我们设计了一个解码器，从模型的隐层重建原始排名值，以保证模型捕获尽可能多的输入信息。我们的无监督损失与所有等级集合模型兼容。我们优化了几个广泛使用的结构，提出了无监督排序集成模型。我们设计了两个实际数据集的综合实验来验证所提出模型的有效性。在一个拥有数十亿用户的短视频平台上应用了该模型，并取得了显著的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unsupervised+Ranking+Ensemble+Model+for+Recommendation)|0|
|[Adapting Job Recommendations to User Preference Drift with Behavioral-Semantic Fusion Learning](https://doi.org/10.1145/3637528.3671759)|Xiao Han, Chen Zhu, Xiao Hu, Chuan Qin, Xiangyu Zhao, Hengshu Zhu|Career Science Lab, BOSS Zhipin, Beijing, China; City University of Hong Kong, Hong Kong, China; Career Science Lab, BOSS Zhipin, University of Science and Technology of China, Beijing, China|Job recommender systems are crucial for aligning job opportunities with job-seekers in online job-seeking. However, users tend to adjust their job preferences to secure employment opportunities continually, which limits the performance of job recommendations. The inherent frequency of preference drift poses a challenge to promptly and precisely capture user preferences. To address this issue, we propose a novel session-based framework, BISTRO, to timely model user preference through fusion learning of semantic and behavioral information. Specifically, BISTRO is composed of three stages: 1) coarse-grained semantic clustering, 2) fine-grained job preference extraction, and 3) personalized top-k job recommendation. Initially, BISTRO segments the user interaction sequence into sessions and leverages session-based semantic clustering to achieve broad identification of person-job matching. Subsequently, we design a hypergraph wavelet learning method to capture the nuanced job preference drift. To mitigate the effect of noise in interactions caused by frequent preference drift, we innovatively propose an adaptive wavelet filtering technique to remove noisy interaction. Finally, a recurrent neural network is utilized to analyze session-based interaction for inferring personalized preferences. Extensive experiments on three real-world offline recruitment datasets demonstrate the significant performances of our framework. Significantly, BISTRO also excels in online experiments, affirming its effectiveness in live recruitment settings. This dual success underscores the robustness and adaptability of BISTRO. The source code is available at https://github.com/Applied-Machine-Learning-Lab/BISTRO.|在网上求职中，职位推荐系统对于将求职机会与求职者联系起来至关重要。然而，用户倾向于不断调整自己的工作偏好以获得就业机会，这就限制了工作推荐的效果。偏好漂移的固有频率对及时、准确地捕捉用户偏好提出了挑战。为了解决这个问题，我们提出了一个新的基于会话的框架 BISTRO，通过语义和行为信息的融合学习来及时建模用户偏好。具体来说，BISTRO 由三个阶段组成: 1)粗粒度语义聚类，2)细粒度工作偏好提取，3)个性化的 top-k 工作推荐。最初，BISTRO 将用户交互序列分割成会话，并利用基于会话的语义聚类实现人-工匹配的广泛识别。随后，我们设计了一种超图小波学习方法来捕捉细微的工作偏好漂移。为了消除频繁偏好漂移引起的相互作用中的噪声影响，本文创新性地提出了一种自适应小波滤波技术来去除噪声相互作用。最后，一个递归神经网络被用来分析基于会话的交互来推断个性化的偏好。在三个真实世界的离线招聘数据集上的大量实验证明了我们的框架的显著性能。值得注意的是，BISTRO 还擅长在线实验，确认其在现场招聘设置的有效性。这种双重成功突出了 BISTRO 的稳健性和适应性。源代码可在 https://github.com/applied-machine-learning-lab/bistro 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adapting+Job+Recommendations+to+User+Preference+Drift+with+Behavioral-Semantic+Fusion+Learning)|0|
|[Learning to Bid the Interest Rate in Online Unsecured Personal Loans](https://doi.org/10.1145/3637528.3671584)|Dong Jun Jee, Seung Jung Jin, JiHoon Yoo, Byunggyu Ahn|PFC Technologies, Seoul, Republic of Korea|The unsecured personal loan (UPL) market is a multi-billion dollar market where numerous financial institutions compete. Due to the development of online banking, loan applicants start to compare numerous loan products. They aim for high loan limits and low interest rates. Since loan applicants have a desired loan amount, institutions instead focus on adjusting interest rates. Despite the importance of determining optimal interest strategies, institutions have traditionally relied on heuristic methods by human experts to set interest rates. This is done by adding a target return on assets (ROA) to the applicant's expected default probability predicted by a credit scoring system (CSS) such as the FICO score. We conceptualize the UPL market dynamics as a repeated auction scenario, where loan applicants (akin to sellers) seek the lowest interest rates, while financial institutions (akin to bidders) aim to maximize profits through higher interest rates. To the best of our knowledge, this is the first time anyone has approached the UPL market through the viewpoint of a repeated auction. While there are several research done in learning to bid in repeated auctions, those works cannot be directly applied to the UPL market due to the lack of any feedback about other bidders' strategies and the need to satisfy the bidder's target loan volume and profit variance. We present an algorithm named AutoInterest, which is a modification of the dual gradient descent algorithm. In addition, we provide a framework to evaluate interest rate bidding strategies on a benchmark dataset and the credit bureau dataset of actual loan applicants in South Korea. We evaluate AutoInterest on this framework and show higher cumulative profit compared to other common online algorithms and the current fixed strategy used by real institutions.|无担保个人贷款(UPL)市场是一个数十亿美元的市场，众多金融机构在其中展开竞争。由于网上银行的发展，贷款申请者开始比较众多的贷款产品。他们的目标是高贷款限额和低利率。由于贷款申请人有一个理想的贷款数额，机构反而把重点放在调整利率。尽管确定最优利率策略很重要，但机构历来依赖人类专家的启发式方法来设定利率。这是通过将目标资产收益率(ROA)添加到由信用评分系统(CSS)(例如 FICO 评分)预测的申请者的预期违约概率来完成的。我们将 UPL 市场动态概念化为一个重复的拍卖场景，其中贷款申请者(类似于卖方)寻求最低的利率，而金融机构(类似于投标者)旨在通过更高的利率实现利润最大化。据我们所知，这是第一次有人通过重复拍卖的观点进入 UPL 市场。虽然在重复拍卖中学习投标已经有了一些研究，但是由于缺乏对其他投标者策略的反馈，以及需要满足投标者的目标贷款量和利润差异，这些工作不能直接应用于 UPL 市场。我们提出了一个名为“自动兴趣”的算法，它是对双梯度下降法算法的修改。此外，我们还提供了一个基于基准数据集和韩国实际贷款申请者信用局数据集评估利率投标策略的框架。我们在这个框架下评估了自动收益，并显示了比其他常见的在线算法和当前实际机构使用的固定策略更高的累积利润。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Bid+the+Interest+Rate+in+Online+Unsecured+Personal+Loans)|0|
|[A Hierarchical and Disentangling Interest Learning Framework for Unbiased and True News Recommendation](https://doi.org/10.1145/3637528.3671944)|Shoujin Wang, Wentao Wang, Xiuzhen Zhang, Yan Wang, Huan Liu, Fang Chen|Macquarie University, Sydney, Australia; RMIT University, Melbourne, Australia; University of Technology Sydney, Sydney, Australia; Arizona State University, Tempe, USA|In the era of information explosion, news recommender systems are crucial for users to effectively and efficiently discover their interested news. However, most of the existing news recommender systems face two major issues, hampering recommendation quality. Firstly, they often oversimplify users' reading interests, neglecting their hierarchical nature, spanning from high-level event (e.g., US Election) related interests to low-level news article-specifc interests. Secondly, existing work often assumes a simplistic context, disregarding the prevalence of fake news and political bias under the real-world context. This oversight leads to recommendations of biased or fake news, posing risks to individuals and society. To this end, this paper addresses these gaps by introducing a novel framework, the Hierarchical and Disentangling Interest learning framework (HDInt). HDInt incorporates a hierarchical interest learning module and a disentangling interest learning module. The former captures users' high- and low-level interests, enhancing next-news recommendation accuracy. The latter effectively separates polarity and veracity information from news contents and model them more specifcally, promoting fairness- and truth-aware reading interest learning for unbiased and true news recommendations. Extensive experiments on two real-world datasets demonstrate HDInt's superiority over state-of-the-art news recommender systems in delivering accurate, unbiased, and true news recommendations.|在信息爆炸时代，新闻推荐系统对于用户有效发现感兴趣的新闻至关重要。然而，大多数现有的新闻推荐系统面临两个主要问题，阻碍了推荐质量。首先，他们往往过分简化用户的阅读兴趣，忽视了他们的等级性质，从高水平的事件(如美国大选)相关兴趣低水平的新闻文章的具体兴趣。其次，现有作品往往假设一个简单化的语境，忽视了现实语境下假新闻和政治偏见的盛行。这种疏忽导致了有偏见或假新闻的建议，给个人和社会带来风险。为此，本文提出了一种新的兴趣学习框架——分层分离式兴趣学习框架(HDInt)。HDInt 集成了分层兴趣学习模块和分离兴趣学习模块。前者捕捉用户的高层次和低层次兴趣，提高下一新闻推荐的准确性。后者有效地将极性和准确性信息从新闻内容中分离出来，并对其进行更具体的建模，促进公平和真实意识的阅读兴趣学习，以获得无偏见和真实的新闻推荐。在两个真实世界数据集上的大量实验表明，HDInt 在提供准确、公正和真实的新闻推荐方面优于最先进的新闻推荐系统。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Hierarchical+and+Disentangling+Interest+Learning+Framework+for+Unbiased+and+True+News+Recommendation)|0|
|[Warming Up Cold-Start CTR Prediction by Learning Item-Specific Feature Interactions](https://doi.org/10.1145/3637528.3671784)|Yaqing Wang, Hongming Piao, Daxiang Dong, Quanming Yao, Jingbo Zhou|Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Computer Science, City University of Hong Kong, Hong Kong, Hong Kong; Baidu Research, Baidu Inc., Beijing, China; Baidu AI Cloud, Baidu Inc., Beijing, China|In recommendation systems, new items are continuously introduced, initially lacking interaction records but gradually accumulating them over time. Accurately predicting the click-through rate (CTR) for these items is crucial for enhancing both revenue and user experience. While existing methods focus on enhancing item ID embeddings for new items within general CTR models, they tend to adopt a global feature interaction approach, often overshadowing new items with sparse data by those with abundant interactions. Addressing this, our work introduces EmerG, a novel approach that warms up cold-start CTR prediction by learning item-specific feature interaction patterns. EmerG utilizes hypernetworks to generate an item-specific feature graph based on item characteristics, which is then processed by a Graph Neural Network (GNN). This GNN is specially tailored to provably capture feature interactions at any order through a customized message passing mechanism. We further design a meta learning strategy that optimizes parameters of hypernetworks and GNN across various item CTR prediction tasks, while only adjusting a minimal set of item-specific parameters within each task. This strategy effectively reduces the risk of overfitting when dealing with limited data. Extensive experiments on benchmark datasets validate that EmerG consistently performs the best given no, a few and sufficient instances of new items.|在推荐系统中，不断引入新的项目，最初缺乏交互记录，但随着时间的推移逐渐积累。准确地预测这些项目的点进率对于提高收入和用户体验至关重要。虽然现有的方法侧重于在一般的 CTR 模型中增强新项目的项目 ID 嵌入，但它们倾向于采用全局特征交互方法，往往使那些具有丰富交互的数据稀疏的新项目黯然失色。为了解决这个问题，我们的工作介绍了 EmerG，这是一种通过学习项目特定的特征交互模式来预测冷启动 CTR 的新方法。EmerG 利用超网络生成基于项目特征的项目特征图，然后通过图神经网络(GNN)对其进行处理。这个 GNN 是专门定制的，可以通过定制的消息传递机制以任何顺序捕获特性交互。我们进一步设计了一个元学习策略，在不同的项目 CTR 预测任务中优化超级网络和 GNN 的参数，同时在每个任务中只调整最小的项目特定参数集。这种策略有效地降低了处理有限数据时过度拟合的风险。在基准数据集上的大量实验验证了 EmerG 始终如一地表现出最好的特性——没有、少数和充分的新项目实例。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Warming+Up+Cold-Start+CTR+Prediction+by+Learning+Item-Specific+Feature+Interactions)|0|
|[Improving Multi-modal Recommender Systems by Denoising and Aligning Multi-modal Content and User Feedback](https://doi.org/10.1145/3637528.3671703)|Guipeng Xv, Xinyu Li, Ruobing Xie, Chen Lin, Chong Liu, Feng Xia, Zhanhui Kang, Leyu Lin|Tencent, Beijing, China; School of Informatics, Xiamen University, Xiamen, Fujian, China|Multi-modal recommender systems (MRSs) are pivotal in diverse online web platforms and have garnered considerable attention in recent years. However, previous studies overlook the challenges of (1)noisy multi-modal content, (2) noisy user feedback, and (3) aligning multi-modal content and user feedback. To tackle these challenges, we propose Denoising and Aligning Multi-modal Recommender System (DA-MRS). To mitigate noise in multi-modal content, DA-MRS first constructs item-item graphs determined by consistent content similarity across modalities. To denoise user feedback, DA-MRS associates the probability of observed feedback with multi-modal content and devises a denoised BPR loss. Furthermore, DA-MRS implements Alignment guided by User preference to enhance task-specific item representation and Alignment guided by graded Item relations to provide finer-grained alignment. Extensive experiments verify that DA-MRS is a plug-and-play framework and achieves significant and consistent improvements across various datasets, backbone models, and noisy scenarios.|多模态推荐系统(MRS)是各种在线网络平台中的关键技术，近年来得到了广泛的关注。然而，以往的研究忽略了以下挑战: (1)噪声多模态内容，(2)噪声用户反馈，和(3)调整多模态内容和用户反馈。为了应对这些挑战，我们提出了去噪和对齐多模态推荐系统(DA-MRS)。为了减轻多模态内容中的噪声，DA-MRS 首先构建由不同模态间一致的内容相似性确定的项目-项目图。为了去除用户反馈的噪声，DA-MRS 将观测反馈的概率与多模态内容联系起来，设计了一种去除 BPR 损失的方法。此外，DA-MRS 还实现了用户偏好引导的对齐，增强了任务特定项目的表示和分级项目关系引导的对齐，提供了更细粒度的对齐。大量的实验验证了 DA-MRS 是一个即插即用的框架，并在各种数据集、骨干模型和噪声场景中实现了显著和一致的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Multi-modal+Recommender+Systems+by+Denoising+and+Aligning+Multi-modal+Content+and+User+Feedback)|0|
|[DDCDR: A Disentangle-based Distillation Framework for Cross-Domain Recommendation](https://doi.org/10.1145/3637528.3671605)|Zhicheng An, Zhexu Gu, Li Yu, Ke Tu, Zhengwei Wu, Binbin Hu, Zhiqiang Zhang, Lihong Gu, Jinjie Gu|Ant Group, Hangzhou, Zhejiang, China|Modern recommendation platforms frequently encompass multiple domains to cater to the varied preferences of users. Recently, cross-domain learning has gained traction as a significant paradigm within the context of recommendation systems, enabling the leveraging of rich information from a well-endowed source domain to enhance a target domain, often limited by inadequate data resources. A primary concern in cross-domain recommendation is the mitigation of negative transfer-ensuring the selective transference of pertinent knowledge from the source (domain-shared knowledge) while maintaining the integrity of domain-unique insights within the target domain (domain-specific knowledge). In this paper, we propose a novel Disentangle-based Distillation Framework for Cross-Domain Recommendation (DDCDR), designed to operate at the representational level and rooted in the established teacher-student knowledge distillation paradigm. Our methodology begins with the development of a cross-domain teacher model, trained adversarially alongside a domain discriminator. This is followed by the creation of a target domain-specific student model. By employing the trained domain discriminator, we successfully segregate domain-shared from domain-specific representations. The teacher model guides the learning of domain-shared features, while domain-specific features are enhanced via contrastive learning methods. Experiments conducted on both public datasets and an industrial dataset demonstrate DDCDR achieves a new state-of-the-art performance. The implementation within Ant Group's platform further confirms its online efficacy, manifesting relative improvements of 0.33% and 0.45% in Unique Visitor Click-Through Rate (UVCTR) across two distinct recommendation scenarios, compared to baseline performances.|现代推荐平台经常包含多个域，以满足用户的不同偏好。最近，跨领域学习作为推荐系统范围内的一个重要范式已经获得了广泛的关注，使得能够利用来自资源丰富的源领域的丰富信息来增强目标领域，而这往往受到数据资源不足的限制。跨领域推荐的主要关注点是减轻负面转移-确保从源(领域共享知识)选择性转移相关知识，同时保持目标领域(领域特定知识)内领域独特见解的完整性。本文提出了一种新的基于分离角度的跨域推荐精馏框架(DDCDR) ，该框架基于已建立的师生知识精馏范式，设计在表示层次上进行操作。我们的方法开始于开发一个跨领域的教师模型，与领域鉴别器一起进行对抗性的培训。然后创建特定于目标领域的学生模型。通过使用训练有素的领域鉴别器，我们成功地将领域共享与领域特定的表示隔离开来。教师模型指导领域共享特征的学习，而领域特定特征通过对比学习方法得到增强。在公共数据集和工业数据集上进行的实验表明，DDCDR 实现了一种新的最先进的性能。蚂蚁集团平台的实施进一步证实了其在线功效，与基线表现相比，在两个不同的推荐场景中，Unique Visitor 点进率(UVCTR)的相对改善率分别为0.33% 和0.45% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DDCDR:+A+Disentangle-based+Distillation+Framework+for+Cross-Domain+Recommendation)|0|
|[Rankability-enhanced Revenue Uplift Modeling Framework for Online Marketing](https://doi.org/10.1145/3637528.3671516)|Bowei He, Yunpeng Weng, Xing Tang, Ziqiang Cui, Zexu Sun, Liang Chen, Xiuqiang He, Chen Ma|Renmin University of China, Beijing, China; FiT, Tencent, Shenzhen, China; City University of Hong Kong, Hong Kong, Hong Kong|Uplift modeling has been widely employed in online marketing by predicting the response difference between the treatment and control groups, so as to identify the sensitive individuals toward interventions like coupons or discounts. Compared with traditional conversion uplift modeling,revenue uplift modeling exhibits higher potential due to its direct connection with the corporate income. However, previous works can hardly handle the continuous long-tail response distribution in revenue uplift modeling. Moreover, they have neglected to optimize the uplift ranking among different individuals, which is actually the core of uplift modeling. To address such issues, in this paper, we first utilize the zero-inflated lognormal (ZILN) loss to regress the responses and customize the corresponding modeling network, which can be adapted to different existing uplift models. Then, we study the ranking-related uplift modeling error from the theoretical perspective and propose two tighter error bounds as the additional loss terms to the conventional response regression loss. Finally, we directly model the uplift ranking error for the entire population with a listwise uplift ranking loss. The experiment results on offline public and industrial datasets validate the effectiveness of our method for revenue uplift modeling. Furthermore, we conduct large-scale experiments on a prominent online fintech marketing platform, Tencent FiT, which further demonstrates the superiority of our method in real-world applications.|提升模型通过预测治疗组与对照组之间的反应差异，以识别对优惠券或折扣等干预措施敏感的个体，在网络营销中得到了广泛的应用。与传统的转换提升模型相比，收入提升模型由于与企业收入直接相关，因此具有更大的潜力。然而，以往的工作难以处理连续的长尾响应分布的收入提升模型。而且，他们忽略了优化不同个体之间的提升排序，这实际上是提升模型的核心。为了解决这些问题，本文首先利用零膨胀对数正态(ZILN)损失对响应进行回归，并定制相应的模型网络，以适应不同的现有抬升模型。然后，从理论角度研究了与排名相关的提升模型误差，提出了两个更严格的误差界作为常规响应回归损失的附加损失项。最后，我们直接模型的提升排名误差的整个人口与列表提升排名损失。在离线公共数据集和工业数据集上的实验结果验证了该方法对收入提升模型的有效性。此外，我们在一个著名的网上金融科技营销平台腾讯 FiT 上进行了大规模的实验，这进一步证明了我们的方法在实际应用中的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rankability-enhanced+Revenue+Uplift+Modeling+Framework+for+Online+Marketing)|0|
|[Personalized Product Assortment with Real-time 3D Perception and Bayesian Payoff Estimation](https://doi.org/10.1145/3637528.3671518)|Porter Jenkins, Michael Selander, J. Stockton Jenkins, Andrew Merrill, Kyle Armstrong|Delicious AI, Lehi, UT, USA; Department of Computer Science, Brigham Young University, Provo, UT, USA|Product assortment selection is a critical challenge facing physical retailers. Effectively aligning inventory with the preferences of shoppers can increase sales and decrease out-of-stocks. However, in real-world settings the problem is challenging due to the combinatorial explosion of product assortment possibilities. Consumer preferences are typically heterogeneous across space and time, making inventory-preference alignment challenging. Additionally, existing strategies rely on syndicated data, which tends to be aggregated, low resolution, and suffer from high latency. To solve these challenges, we introduce a real-time recommendation system, which we call EdgeRec3D. Our system utilizes recent advances in 3D computer vision for perception and automatic, fine grained sales estimation. These perceptual components run on the edge of the network and facilitate real-time reward signals. Additionally, we develop a Bayesian payoff model to account for noisy estimates from 3D LIDAR data. We rely on spatial clustering to allow the system to adapt to heterogeneous consumer preferences, and a graph-based candidate generation algorithm to address the combinatorial search problem. We test our system in real-world stores across two, 6-8 week A/B tests with beverage products and demonstrate a 35% and 27% increase in sales respectively. Finally, we monitor the deployed system for a period of 28 weeks with an observational study and show a 9.4% increase in sales.|产品分类选择是实体零售商面临的一个关键挑战。有效地调整库存与购物者的偏好可以增加销售和减少缺货。然而，在现实世界中，这个问题是具有挑战性的，因为产品分类的可能性是组合爆炸的。消费者的偏好在空间和时间上具有典型的异质性，这使得库存偏好的调整具有挑战性。此外，现有的策略依赖于聚合数据，这些数据往往是聚合的、低分辨率的，并且存在高延迟。为了解决这些问题，我们引入了一个实时推荐系统，我们称之为 EdgeRec3D。我们的系统利用三维计算机视觉的最新进展来进行感知和自动、细粒度的销售估算。这些感知成分运行在网络的边缘，便于实时奖励信号。此外，我们开发了贝叶斯支付模型，以考虑噪声估计从三维激光雷达数据。我们依靠空间聚类来使系统能够适应不同的消费者偏好，以及一个基于图的候选人生成算法来解决组合搜索问题。我们测试我们的系统在现实世界的商店两个，6-8周的 A/B 测试与饮料产品，并证明了35% 和27% 的销售分别增长。最后，我们对已部署的系统进行了为期28周的观察性研究监控，结果显示销售额增长了9.4% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalized+Product+Assortment+with+Real-time+3D+Perception+and+Bayesian+Payoff+Estimation)|0|
|[Where Have You Been? A Study of Privacy Risk for Point-of-Interest Recommendation](https://doi.org/10.1145/3637528.3671758)|Kunlin Cai, Jinghuai Zhang, Zhiqing Hong, William Shand, Guang Wang, Desheng Zhang, Jianfeng Chi, Yuan Tian|University of California, Los Angeles, Los Angeles, CA, USA; Meta, New York, NY, USA; Rutgers University, New Brunswick, NJ, USA; Florida State University, Tallahassee, FL, USA|As location-based services (LBS) have grown in popularity, more human mobility data has been collected. The collected data can be used to build machine learning (ML) models for LBS to enhance their performance and improve overall experience for users. However, the convenience comes with the risk of privacy leakage since this type of data might contain sensitive information related to user identities, such as home/work locations. Prior work focuses on protecting mobility data privacy during transmission or prior to release, lacking the privacy risk evaluation of mobility data-based ML models. To better understand and quantify the privacy leakage in mobility data-based ML models, we design a privacy attack suite containing data extraction and membership inference attacks tailored for point-of-interest (POI) recommendation models, one of the most widely used mobility data-based ML models. These attacks in our attack suite assume different adversary knowledge and aim to extract different types of sensitive information from mobility data, providing a holistic privacy risk assessment for POI recommendation models. Our experimental evaluation using two real-world mobility datasets demonstrates that current POI recommendation models are vulnerable to our attacks. We also present unique findings to understand what types of mobility data are more susceptible to privacy attacks. Finally, we evaluate defenses against these attacks and highlight future directions and challenges.|随着基于位置的服务(LBS)越来越流行，越来越多的移动性数据被收集。所收集的数据可以用来为 LBS 建立机器学习(ML)模型，以提高它们的性能和改善用户的整体体验。然而，这种便利伴随着隐私泄露的风险，因为这类数据可能包含与用户身份有关的敏感信息，例如家庭/工作地点。此前的工作重点是保护移动数据在传输过程中或发布之前的隐私，缺乏基于移动数据的机器学习模型的隐私风险评估。为了更好地理解和量化基于移动性数据的机器学习模型中的隐私泄漏，我们设计了一个隐私攻击套件，其中包含针对感兴趣点(POI)推荐模型的数据提取和成员推断攻击，这是最广泛使用的基于移动性数据的机器学习模型之一。我们的攻击套件中的这些攻击假设不同的敌人知识，目的是从移动数据中提取不同类型的敏感信息，为 POI 推荐模型提供一个全面的隐私风险评估。我们的实验评估使用两个真实世界的移动性数据集表明，目前的 POI 推荐模型是脆弱的，我们的攻击。我们还提出了独特的发现，以了解哪些类型的移动数据更容易受到隐私攻击。最后，我们将评估针对这些攻击的防御措施，并强调未来的方向和挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Where+Have+You+Been?+A+Study+of+Privacy+Risk+for+Point-of-Interest+Recommendation)|0|
|[Neural Retrievers are Biased Towards LLM-Generated Content](https://doi.org/10.1145/3637528.3671882)|Sunhao Dai, Yuqi Zhou, Liang Pang, Weihao Liu, Xiaolin Hu, Yong Liu, Xiao Zhang, Gang Wang, Jun Xu|Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; ; Noah's Ark Lab, Huawei, Shenzhen, China|Recently, the emergence of large language models (LLMs) has revolutionized the paradigm of information retrieval (IR) applications, especially in web search, by generating vast amounts of human-like texts on the Internet. As a result, IR systems in the LLM era are facing a new challenge: the indexed documents are now not only written by human beings but also automatically generated by the LLMs. How these LLM-generated documents influence the IR systems is a pressing and still unexplored question. In this work, we conduct a quantitative evaluation of IR models in scenarios where both human-written and LLM-generated texts are involved. Surprisingly, our findings indicate that neural retrieval models tend to rank LLM-generated documents higher. We refer to this category of biases in neural retrievers towards the LLM-generated content as the source bias. Moreover, we discover that this bias is not confined to the first-stage neural retrievers, but extends to the second-stage neural re-rankers. Then, in-depth analyses from the perspective of text compression indicate that LLM-generated texts exhibit more focused semantics with less noise, making it easier for neural retrieval models to semantic match. To mitigate the source bias, we also propose a plug-and-play debiased constraint for the optimization objective, and experimental results show its effectiveness. Finally, we discuss the potential severe concerns stemming from the observed source bias and hope our findings can serve as a critical wake-up call to the IR community and beyond. To facilitate future explorations of IR in the LLM era, the constructed two new benchmarks are available at https://github.com/KID-22/Source-Bias.|最近，大语言模型(LLMs)的出现彻底改变了信息检索(IR)应用的范式，特别是在网络搜索中，通过在互联网上生成大量类似人类的文本。因此，LLM 时代的信息检索系统面临着新的挑战: 索引文档不仅由人工编写，而且由 LLM 自动生成。这些 LLM 生成的文档如何影响 IR 系统是一个紧迫的、尚未探索的问题。在这项工作中，我们进行了定量评估的情况下，国际关系模型的人写和 LLM 生成的文本都涉及。令人惊讶的是，我们的研究结果表明，神经检索模型往往排名 LLM 生成的文档更高。我们将神经检索器对 LLM 生成的内容的这类偏差称为源偏差。此外，我们发现这种偏差并不局限于第一阶段的神经检索，而是延伸到第二阶段的神经重新排序。然后，从文本压缩的角度进行深入分析，结果表明 LLM 生成的文本具有更集中的语义和更少的噪声，使得神经检索模型更容易进行语义匹配。为了减小源偏差，我们还提出了一个即插即用的去偏约束优化目标，实验结果表明其有效性。最后，我们讨论了由观察到的来源偏差引起的潜在的严重关切，并希望我们的发现可以作为对 IR 社区和其他方面的一个关键的警告。为了促进 LLM 时代对信息检索的未来探索，构建了两个新的基准 https://github.com/kid-22/source-bias。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Retrievers+are+Biased+Towards+LLM-Generated+Content)|0|
|[DisCo: Towards Harmonious Disentanglement and Collaboration between Tabular and Semantic Space for Recommendation](https://doi.org/10.1145/3637528.3672008)|Kounianhua Du, Jizheng Chen, Jianghao Lin, Yunjia Xi, Hangyu Wang, Xinyi Dai, Bo Chen, Ruiming Tang, Weinan Zhang|Huawei Noah's Ark Lab, Shenzhen, China; Huawei Noah's Ark Lab, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China|Recommender systems play important roles in various applications such ase-commerce, social media, etc. Conventional recommendation methods usuallymodel the collaborative signals within the tabular representation space.Despite the personalization modeling and the efficiency, the latent semanticdependencies are omitted. Methods that introduce semantics into recommendationthen emerge, injecting knowledge from the semantic representation space wherethe general language understanding are compressed. However, existingsemantic-enhanced recommendation methods focus on aligning the two spaces,during which the representations of the two spaces tend to get close while theunique patterns are discarded and not well explored. In this paper, we proposeDisCo to Disentangle the unique patterns from the two representation spaces andCollaborate the two spaces for recommendation enhancement, where both thespecificity and the consistency of the two spaces are captured. Concretely, wepropose 1) a dual-side attentive network to capture the intra-domain patternsand the inter-domain patterns, 2) a sufficiency constraint to preserve thetask-relevant information of each representation space and filter out thenoise, and 3) a disentanglement constraint to avoid the model from discardingthe unique information. These modules strike a balance between disentanglementand collaboration of the two representation spaces to produce informativepattern vectors, which could serve as extra features and be appended toarbitrary recommendation backbones for enhancement. Experiment results validatethe superiority of our method against different models and the compatibility ofDisCo over different backbones. Various ablation studies and efficiencyanalysis are also conducted to justify each model component.|推荐系统在电子商务、社交媒体等各种应用中发挥着重要作用。传统的推荐方法通常是在表格表示空间中对协作信号进行建模。尽管个性化建模和效率，潜在的语义依赖性被忽略。然后出现将语义引入推荐的方法，从语义表示空间注入知识，在这个空间中一般语言理解被压缩。然而，现有的语义增强推荐方法侧重于对齐这两个空间，在此期间，两个空间的表示趋于接近，而唯一的模式被丢弃，没有得到很好的探索。在本文中，我们提出了 DisCo 从两个表示空间中分离出唯一的模式，并协作两个空间进行推荐增强，同时捕获两个空间的特殊性和一致性。具体来说，我们提出了两种方案: 1)双侧注意网络捕获域内模式和域间模式; 2)充分约束保留每个表示空间的任务相关信息并过滤掉噪声; 3)解缠约束避免模型丢弃唯一信息。这些模块在两个表示空间的分离和协作之间取得了平衡，从而产生了信息模式向量，这些向量可以作为额外的特征，并附加到任意的推荐主干上进行增强。实验结果验证了该方法对不同模型的优越性以及 DisCo 在不同骨架上的兼容性。各种消融研究和效率分析也进行了验证每个模型组件。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DisCo:+Towards+Harmonious+Disentanglement+and+Collaboration+between+Tabular+and+Semantic+Space+for+Recommendation)|0|
|[Label Shift Correction via Bidirectional Marginal Distribution Matching](https://doi.org/10.1145/3637528.3671867)|Ruidong Fan, Xiao Ouyang, Hong Tao, Chenping Hou|National University of Defense Technology, Changsha, Hunan, China|Due to the timeliness and uncertainty of data acquisition, label shift, which assumes that the source (training) and target (test) label distributions differ, occurs with the changing environment and reduces the generalization ability of traditional models. To correct the label shift, existing methods estimate the true label distribution by prediction of target data from a source classifier, which results in high variance, especially with large label shift. In this paper, we tackle this problem by proposing a novel approach termed as Label Shift Correction via Bidirectional Marginal Distribution Matching (BMDM). Our approach matchs the label and feature marginal distributions simultaneously to ensure the stability of estimated class proportions. We prove theoretically that there is a unique optimal solution, i.e., true target label distribution, for our approach under mild conditions, and an efficient optimization strategy is also proposed. On this basis, in multi-shot scenario where label distribution changes continuously, we extend BMDM by designing a new distribution matching mechanism and constructing a regularization term that constrains the direction of label distribution change. Extensive experimental results validate the effectiveness of our approach over existing state-of-the-arts methods.|由于数据采集的及时性和不确定性，假设源(训练)和目标(测试)标签分布不同的标签偏移随着环境的变化而发生，降低了传统模型的泛化能力。为了校正标签偏移，现有的方法通过预测源分类器的目标数据来估计真实的标签分布，这导致了很大的方差，尤其是标签偏移。在本文中，我们提出了一种新的方法来解决这个问题，这种方法被称为双向边缘分布匹配的标签偏移校正(bMDM)。我们的方法同时匹配标签和特征的边际分布，以确保估计的类比例的稳定性。从理论上证明了该方法在温和条件下存在唯一的最优解，即真实目标标签分布，并提出了一种有效的优化策略。在此基础上，在标签分布不断变化的多镜头场景下，通过设计一种新的分布匹配机制，构造一个约束标签分布变化方向的正则项，对 BMDM 进行了扩展。大量的实验结果验证了我们的方法比现有的最先进的方法更有效。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Label+Shift+Correction+via+Bidirectional+Marginal+Distribution+Matching)|0|
|[On (Normalised) Discounted Cumulative Gain as an Off-Policy Evaluation Metric for Top-n Recommendation](https://doi.org/10.1145/3637528.3671687)|Olivier Jeunen, Ivan Potapov, Aleksei Ustimenko|ShareChat, London, United Kingdom; ShareChat, Edinburgh, United Kingdom|Approaches to recommendation are typically evaluated in one of two ways: (1)via a (simulated) online experiment, often seen as the gold standard, or (2)via some offline evaluation procedure, where the goal is to approximate theoutcome of an online experiment. Several offline evaluation metrics have beenadopted in the literature, inspired by ranking metrics prevalent in the fieldof Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is onesuch metric that has seen widespread adoption in empirical studies, and higher(n)DCG values have been used to present new methods as the state-of-the-art intop-n recommendation for many years. Our work takes a critical look at this approach, and investigates when we canexpect such metrics to approximate the gold standard outcome of an onlineexperiment. We formally present the assumptions that are necessary to considerDCG an unbiased estimator of online reward and provide a derivation for thismetric from first principles, highlighting where we deviate from itstraditional uses in IR. Importantly, we show that normalising the metricrenders it inconsistent, in that even when DCG is unbiased, ranking competingmethods by their normalised DCG can invert their relative order. Through acorrelation analysis between off- and on-line experiments conducted on alarge-scale recommendation platform, we show that our unbiased DCG estimatesstrongly correlate with online reward, even when some of the metric's inherentassumptions are violated. This statement no longer holds for its normalisedvariant, suggesting that nDCG's practical utility may be limited.|推荐方法通常以两种方式之一进行评估: (1)通过(模拟)在线实验，通常被视为黄金标准，或者(2)通过一些离线评估程序，其目标是近似在线实验的结果。文献中已经采用了一些线下评估指标，这些指标的灵感来自于信息检索领域中流行的排名指标。(标准化)贴现累积增益(nDCG)是在实证研究中得到广泛采用的一种度量标准，较高的(n) DCG 值已被用于表示新方法作为最先进的推荐方法多年。我们的工作对这种方法进行了批判性的研究，并且调查了我们什么时候可以期望这些指标接近在线实验的黄金标准结果。我们正式提出的假设是必要的，认为 DCG 是一个公正的估计在线奖励，并提供了从第一原则这一度量的推导，突出了我们偏离其传统用途在 IR。重要的是，我们表明，规范化的度量呈现它不一致，即使当 DCG 是无偏见的，排名竞争方法的规范化 DCG 可以颠倒他们的相对顺序。通过在大规模推荐平台上进行的离线和在线实验之间的相关性分析，我们表明我们的无偏 DCG 估计与在线奖励强烈相关，即使一些度量的固有假设被违背。这种说法不再适用于它的正常化变体，表明 nDCG 的实际用途可能是有限的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+(Normalised)+Discounted+Cumulative+Gain+as+an+Off-Policy+Evaluation+Metric+for+Top-n+Recommendation)|0|
|[FairMatch: Promoting Partial Label Learning by Unlabeled Samples](https://doi.org/10.1145/3637528.3671685)|Jiahao Jiang, Yuheng Jia, Hui Liu, Junhui Hou|College of Software Engineering, Southeast University, Nanjing, China; School of Computer Science and Engineering, Southeast University, Nanjing, China; School of Computing & Information Sciences, Saint Francis University, HongKong, China; Department of Computer Science, City University of Hong Kong, HongKong, China|This paper studies the semi-supervised partial label learning (SSPLL) problem, which aims to improve the partial label learning (PLL) by leveraging unlabeled samples. Both the existing SSPLL methods and the semi-supervised learning methods exploit the information in unlabeled samples by selecting high-confidence unlabeled samples as the pseudo labels based on the maximum value of the model output. However, the scarcity of labeled samples and the ambiguity from partial labels skew this strategy towards an unfair selection of high-confidence samples on each class, most notably during the initial phases of training, resulting in slower training and performance degradation. In this paper, we propose a novel method FairMatch, which adopts a learning state aware self-adaptive threshold for selecting the same number of high-confidence samples on each class, and uses augmentation consistency to incorporate the unlabeled samples to promote PLL. In addition, we adopt the candidate label disambiguation to utilize the partial labeled samples and mix up the partial labeled samples and the selected high-confidence unlabeled samples to prevent the model from overfitting on partial label samples. FairMatch can achieve maximum accuracy improvements of 9.53%, 4.9%, and 16.45% on CIFAR-10, CIFAR-100, and CIFAR-100H, respectively. The codes can be found at https://github.com/jhjiangSEU/FairMatch.|本文研究了半监督部分标记学习(SSPLL)问题，旨在利用未标记样本改进部分标记学习(PLL)。现有的 SSPLL 方法和半监督学习方法都是根据模型输出的最大值，选择高置信度的未标记样本作为伪标签，从而利用未标记样本的信息。然而，标记样本的稀缺性和部分标记的模糊性使该策略倾向于在每个类别上不公平地选择高置信度样本，最显着的是在训练的初始阶段，导致训练较慢和性能下降。本文提出了一种新的 FairMatch 方法，该方法采用一种学习状态感知的自适应阈值来选择每类中相同数量的高置信度样本，并使用增强一致性来合并未标记的样本来提升锁相环。此外，我们采用候选标签消歧的方法，利用部分标签样本，混合部分标签样本和选择的高置信度未标签样本，以防止模型对部分标签样本的过度拟合。FairMatch 可以分别在 CIFAR-10，CIFAR-100和 CIFAR-100H 上实现9.53% ，4.9% 和16.45% 的最大准确性改进。密码可以在 https://github.com/jhjiangseu/fairmatch 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FairMatch:+Promoting+Partial+Label+Learning+by+Unlabeled+Samples)|0|
|[Privileged Knowledge State Distillation for Reinforcement Learning-based Educational Path Recommendation](https://doi.org/10.1145/3637528.3671872)|Qingyao Li, Wei Xia, Li'ang Yin, Jiarui Jin, Yong Yu|Huawei Noah's Ark Lab, Shenzhen, China; Shanghai Jiao Tong University, Shanghai, China|Educational recommendation seeks to suggest knowledge concepts that match a learner's ability, thus facilitating a personalized learning experience. In recent years, reinforcement learning (RL) methods have achieved considerable results by taking the encoding of the learner's exercise log as the state and employing an RL-based agent to make suitable recommendations. However, these approaches suffer from handling the diverse and dynamic learner's knowledge states. In this paper, we introduce the privileged feature distillation technique and propose the P rivileged K nowledge S tate D istillation (PKSD ) framework, allowing the RL agent to leverage the "actual'' knowledge state as privileged information in the state encoding to help tailor recommendations to meet individual needs. Concretely, our PKSD takes the privileged knowledge states together with the representations of the exercise log for the state representations during training. And through distillation, we transfer the ability to adapt to learners to aknowledge state adapter. During inference, theknowledge state adapter would serve as the estimated privileged knowledge states instead of the real one since it is not accessible. Considering that there are strong connections among the knowledge concepts in education, we further propose to collaborate the graph structure learning for concepts into our PKSD framework. This new approach is termed GEPKSD (Graph-Enhanced PKSD). As our method is model-agnostic, we evaluate PKSD and GEPKSD by integrating them with five different RL bases on four public simulators, respectively. Our results verify that PKSD can consistently improve the recommendation performance with various RL methods, and our GEPKSD could further enhance the effectiveness of PKSD in all the simulations.|教育推荐旨在建议符合学习者能力的知识概念，从而促进个性化的学习体验。近年来，以学习者运动日志的编码为状态，并使用基于强化学习的代理来提出合适的建议，这些方法已经取得了相当大的成果。然而，这些方法在处理多样化和动态学习者的知识状态时存在缺陷。本文介绍了特权特征提取技术，提出了 P 特权 K 知识 S 状态 D 提取(PKSD)框架，允许 RL 代理利用“实际”知识状态作为状态编码中的特权信息，帮助定制推荐以满足个体需求。具体地说，我们的 PKSD 将特权知识状态与训练日志的表示一起提取，用于训练过程中的状态表示。并通过升华，将学习者的适应能力转化为知识状态适应能力。在推理过程中，知识状态适配器将作为估计的特权知识状态，而不是实际知识状态，因为它是不可访问的。考虑到教育中知识概念之间的紧密联系，我们进一步提出将概念的图形结构学习协同到 PKSD 框架中。这种新的方法被称为 GEPKSD (图形增强 PKSD)。由于我们的方法是模型无关的，所以我们分别在四个公共模拟器上将 PKSD 和 GEPKSD 与五个不同的 RL 基地集成在一起进行评估。实验结果表明，PKSD 能够持续改善各种 RL 方法的推荐性能，而 GEPKSD 能够进一步提高 PKSD 在所有仿真中的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Privileged+Knowledge+State+Distillation+for+Reinforcement+Learning-based+Educational+Path+Recommendation)|0|
|[Toward Structure Fairness in Dynamic Graph Embedding: A Trend-aware Dual Debiasing Approach](https://doi.org/10.1145/3637528.3671848)|Yicong Li, Yu Yang, Jiannong Cao, Shuaiqi Liu, Haoran Tang, Guandong Xu|The Hong Kong Polytechnic University, Hong Kong, Hong Kong; The Education University of Hong Kong & University of Technology Sydney, Hong Kong, Hong Kong|Recent studies successfully learned static graph embeddings that arestructurally fair by preventing the effectiveness disparity of high- andlow-degree vertex groups in downstream graph mining tasks. However, achievingstructure fairness in dynamic graph embedding remains an open problem.Neglecting degree changes in dynamic graphs will significantly impair embeddingeffectiveness without notably improving structure fairness. This is because theembedding performance of high-degree and low-to-high-degree vertices willsignificantly drop close to the generally poorer embedding performance of mostslightly changed vertices in the long-tail part of the power-law distribution.We first identify biased structural evolutions in a dynamic graph based on theevolving trend of vertex degree and then propose FairDGE, the firststructurally Fair Dynamic Graph Embedding algorithm. FairDGE learns biasedstructural evolutions by jointly embedding the connection changes amongvertices and the long-short-term evolutionary trend of vertex degrees.Furthermore, a novel dual debiasing approach is devised to encode fairembeddings contrastively, customizing debiasing strategies for different biasedstructural evolutions. This innovative debiasing strategy breaks theeffectiveness bottleneck of embeddings without notable fairness loss. Extensiveexperiments demonstrate that FairDGE achieves simultaneous improvement in theeffectiveness and fairness of embeddings.|最近的研究成功地学习了静态图嵌入，通过防止有效性差异的高度和低度顶点组在下游图挖掘任务。然而，在动态图嵌入中实现结构公平性仍然是一个悬而未决的问题。忽略动态图的度变化会显著降低嵌入效率，而不能显著提高结构的公平性。这是因为在幂律分布的长尾部分，高度顶点和低到高度顶点的嵌入性能会明显下降，接近于最微小变化顶点的嵌入性能普遍较差。我们首先根据顶点度的演化趋势在动态图中识别有偏的结构演化，然后提出 FairDGE，第一个结构公平的动态图嵌入算法。FairDGE 通过联合嵌入顶点之间的联系变化和顶点度的长期短期演化趋势来学习有偏的结构演化。此外，设计了一种新的双重消偏方法来对比编码公平层合，定制消偏策略以适应不同的有偏结构演化。这种创新的去偏策略打破了嵌入的有效性瓶颈，没有明显的公平性损失。大量实验表明，FairDGE 算法同时提高了嵌入的有效性和公平性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Toward+Structure+Fairness+in+Dynamic+Graph+Embedding:+A+Trend-aware+Dual+Debiasing+Approach)|0|
|[Bridging Items and Language: A Transition Paradigm for Large Language Model-Based Recommendation](https://doi.org/10.1145/3637528.3671884)|Xinyu Lin, Wenjie Wang, Yongqi Li, Fuli Feng, SeeKiong Ng, TatSeng Chua|University of Science and Technology of China, Hefei, China; National University of Singapore, Singapore, Singapore; The Hong Kong Polytechnic University, Hong Kong SAR, China|Harnessing Large Language Models (LLMs) for recommendation is rapidly emerging, which relies on two fundamental steps to bridge the recommendation item space and the language space: 1) item indexing utilizes identifiers to represent items in the language space, and 2) generation grounding associates LLMs' generated token sequences to in-corpus items. However, previous methods exhibit inherent limitations in the two steps. Existing ID-based identifiers (e.g., numeric IDs) and description-based identifiers (e.g., titles) either lose semantics or lack adequate distinctiveness. Moreover, prior generation grounding methods might generate invalid identifiers, thus misaligning with in-corpus items. To address these issues, we propose a novel Transition paradigm for LLM-based Recommender (named TransRec) to bridge items and language. Specifically, TransRec presents multi-facet identifiers, which simultaneously incorporate ID, title, and attribute for item indexing to pursue both distinctiveness and semantics. Additionally, we introduce a specialized data structure for TransRec to ensure generating valid identifiers only and utilize substring indexing to encourage LLMs to generate from any position of identifiers. Lastly, TransRec presents an aggregated grounding module to leverage generated multi-facet identifiers to rank in-corpus items efficiently. We instantiate TransRec on two backbone models, BART-large and LLaMA-7B. Extensive results on three real-world datasets under diverse settings validate the superiority of TransRec.|利用大型语言模型(LLM)进行推荐正在迅速兴起，这依赖于两个基本步骤来连接推荐项空间和语言空间: 1)项索引利用标识符来表示语言空间中的项目，2)生成基础将 LLM 生成的令牌序列与语料库中的项目相关联。然而，以前的方法在这两个步骤中表现出固有的局限性。现有的基于 ID 的标识符(例如，数字 ID)和基于描述的标识符(例如，标题)要么失去语义，要么缺乏足够的区别性。此外，上一代接地方法可能会产生无效的标识符，从而与语料库中的项目不一致。为了解决这些问题，我们提出了一种新的基于 LLM 的传输范式(称为 TransRec) ，以连接项目和语言。具体来说，TransRec 提供了多方面标识符，这些标识符同时合并 ID、 title 和属性用于项目索引，以实现独特性和语义。此外，我们还为 TransRec 引入了专门的数据结构，以确保只生成有效的标识符，并利用子字符串索引鼓励 LLM 从标识符的任何位置生成标识符。最后，TransRec 提出了一个聚合的接地模块，利用生成的多方面标识符对语料库中的项目进行有效排序。我们在两个骨干模型上实例化 TransRec，BART-large 和 LLaMA-7B。在三个不同设置的真实世界数据集上的广泛结果验证了 TransRec 的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bridging+Items+and+Language:+A+Transition+Paradigm+for+Large+Language+Model-Based+Recommendation)|0|
|[BadSampler:  Harnessing the Power of Catastrophic Forgetting to Poison Byzantine-robust Federated Learning](https://doi.org/10.1145/3637528.3671879)|Yi Liu, Cong Wang, Xingliang Yuan|The University of Melbourne, Melbourne, Australia; City University of Hong Kong, Hong Kong, China|Federated Learning (FL) is susceptible to poisoning attacks, wherein compromised clients manipulate the global model by modifying local datasets or sending manipulated model updates. Experienced defenders can readily detect and mitigate the poisoning effects of malicious behaviors using Byzantine-robust aggregation rules. However, the exploration of poisoning attacks in scenarios where such behaviors are absent remains largely unexplored for Byzantine-robust FL. This paper addresses the challenging problem of poisoning Byzantine-robust FL by introducing catastrophic forgetting. To fill this gap, we first formally define generalization error and establish its connection to catastrophic forgetting, paving the way for the development of a clean-label data poisoning attack named BadSampler. This attack leverages only clean-label data (i.e., without poisoned data) to poison Byzantine-robust FL and requires the adversary to selectively sample training data with high loss to feed model training and maximize the model's generalization error. We formulate the attack as an optimization problem and present two elegant adversarial sampling strategies, Top-k sampling, and meta-sampling, to approximately solve it. Additionally, our formal error upper bound and time complexity analysis demonstrate that our design can preserve attack utility with high efficiency. Extensive evaluations on two real-world datasets illustrate the effectiveness and performance of our proposed attacks.|联邦学习(FL)容易受到中毒攻击，其中受损的客户端通过修改本地数据集或发送受控模型更新来操纵全局模型。经验丰富的防御者可以很容易地使用拜占庭稳健的聚合规则检测和减轻恶意行为的毒害效应。然而，在没有这种行为的情况下，对中毒攻击的探索对于拜占庭-鲁棒 FL 来说仍然很大程度上是未知的。本文通过引入灾难遗忘，解决了对拜占庭-鲁棒 FL 中毒的挑战性问题。为了填补这一空白，我们首先正式定义了泛化误差，并建立了它与灾难性遗忘之间的联系，为名为 BadSampler 的清洁标签数据中毒攻击的开发铺平了道路。这种攻击只利用干净的标签数据(即，没有中毒的数据)来毒害拜占庭-鲁棒的 FL，并要求对手有选择地采样高损失的训练数据来提供模型训练，并最大限度地提高模型的泛化误差。我们把这种攻击作为一种最佳化问题，并提出了两种优雅的对抗性抽样策略: Top-k 抽样和 meta 抽样，来近似地解决这个问题。此外，我们的形式误差上限和时间复杂度分析表明，我们的设计可以保持攻击效用的高效率。对两个真实世界数据集的广泛评估说明了我们提出的攻击的有效性和性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BadSampler:++Harnessing+the+Power+of+Catastrophic+Forgetting+to+Poison+Byzantine-robust+Federated+Learning)|0|
|[Dataset Condensation for Time Series Classification via Dual Domain Matching](https://doi.org/10.1145/3637528.3671675)|Zhanyu Liu, Ke Hao, Guanjie Zheng, Yanwei Yu|Ocean University of China, Qingdao, China; Shanghai Jiao Tong University, Shanghai, China|Time series data has been demonstrated to be crucial in various researchfields. The management of large quantities of time series data presentschallenges in terms of deep learning tasks, particularly for training a deepneural network. Recently, a technique named Dataset Condensation hasemerged as a solution to this problem. This technique generates a smallersynthetic dataset that has comparable performance to the full real dataset indownstream tasks such as classification. However, previous methods areprimarily designed for image and graph datasets, and directly adapting them tothe time series dataset leads to suboptimal performance due to their inabilityto effectively leverage the rich information inherent in time series data,particularly in the frequency domain. In this paper, we propose a novelframework named Dataset Condensation forTime SeriesClassification via Dual Domain Matching (CondTSC)which focuses on the time series classification dataset condensation task.Different from previous methods, our proposed framework aims to generate acondensed dataset that matches the surrogate objectives in both the time andfrequency domains. Specifically, CondTSC incorporates multi-view dataaugmentation, dual domain training, and dual surrogate objectives to enhancethe dataset condensation process in the time and frequency domains. Throughextensive experiments, we demonstrate the effectiveness of our proposedframework, which outperforms other baselines and learns a condensed syntheticdataset that exhibits desirable characteristics such as conforming to thedistribution of the original data.|时间序列数据已被证明在各个研究领域都是至关重要的。大量时间序列数据的管理在深度学习任务方面面临挑战，特别是在训练深度神经网络方面。最近，一种名为“数据集压缩”的技术被用来解决这个问题。该技术生成一个较小的合成数据集，其性能与完整的实际数据集的下游任务(如分类)具有可比性。然而，以前的方法主要是为图像和图形数据集设计的，直接将它们适应于时间序列数据集会导致次优性能，因为它们无法有效地利用时间序列数据中固有的丰富信息，特别是在频率域。本文提出了一种基于双域匹配的时间序列分类数据集压缩框架(CondTSC) ，该框架主要针对时间序列分类数据集压缩任务。与以往的方法不同，我们提出的框架旨在生成在时间和频率领域匹配替代目标的浓缩数据集。具体来说，CondTSC 结合了多视图数据增强、双域训练和双代理目标来增强数据集在时间和频率域的缩聚过程。通过大量的实验，我们证明了我们提出的框架的有效性，它的性能优于其他基线，并学习了一个浓缩的合成数据集，这个数据集展示了令人满意的特征，例如符合原始数据的分布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dataset+Condensation+for+Time+Series+Classification+via+Dual+Domain+Matching)|0|
|[Self-Supervised Denoising through Independent Cascade Graph Augmentation for Robust Social Recommendation](https://doi.org/10.1145/3637528.3671958)|Youchen Sun, Zhu Sun, Yingpeng Du, Jie Zhang, Yew Soon Ong|; ASTAR Centre for Frontier AI Research & Nanyang Technological University, Singapore, Singapore; Nanyang Technological University, Singapore, Singapore|Social Recommendation (SR) typically exploits neighborhood influence in the social network to enhance user preference modeling. However, users' intricate social behaviors may introduce noisy social connections for user modeling and harm the models' robustness. Existing solutions to alleviate social noise either filter out the noisy connections or generate new potential social connections. Due to the absence of labels, the former approaches may retain uncertain connections for user preference modeling while the latter methods may introduce additional social noise. Through data analysis, we discover that (1) social noise likely comes from the connected users with low preference similarity; and (2) Opinion Leaders (OLs) play a pivotal role in influence dissemination, surpassing high-similarity neighbors, regardless of their preference similarity with trusting peers. Guided by these observations, we propose a novel Self-Supervised Denoising approach through Independent Cascade Graph Augmentation, for more robust SR. Specifically, we employ the independent cascade diffusion model to generate an augmented graph view, which traverses the social graph and activates the edges in sequence to simulate the cascading influence spread. To steer the augmentation towards a denoised social graph, we (1) introduce a hierarchical contrastive loss to prioritize the activation of OLs first, followed by high-similarity neighbors, while weakening the low-similarity neighbors; and (2) integrate an information bottleneck based contrastive loss, aiming to minimize mutual information between original and augmented graphs yet preserve sufficient information for improved SR. Experiments conducted on two public datasets demonstrate that our model outperforms the state-of-the-art while also exhibiting higher robustness to different extents of social noise.|社交推荐(SR)通常利用社交网络中的邻域影响来增强用户偏好建模。然而，用户错综复杂的社会行为可能为用户建模引入噪声社会关系，损害模型的鲁棒性。现有的减轻社会噪音的解决方案要么过滤掉噪音连接，要么产生新的潜在社会连接。由于标签的缺失，前一种方法在用户偏好建模时可能会保留不确定的联系，而后一种方法可能会引入额外的社会噪声。通过数据分析，我们发现: (1)社交噪声可能来自偏好相似度较低的关联用户; (2)意见领袖(OLs)在影响力传播中发挥着关键作用，超过了高相似度的邻居，而不管他们与信任同伴的偏好相似度如何。在这些观测结果的指导下，我们提出了一种新的自我监督去噪方法，通过独立级联图增强，更健壮的 SR。具体来说，我们采用独立的级联扩散模型来生成一个扩展图视图，该视图横穿社会图并依次激活边界，以模拟级联影响的传播。为了将扩展引向去噪的社会图，我们(1)引入分层对比损失来优先激活 OLs，然后是高相似性邻居，同时弱化低相似性邻居; (2)整合基于对比损失的信息瓶颈，旨在最小化原始图和扩展图之间的相互信息，同时保留足够的信息以改善 SR。在两个公共数据集上进行的实验表明，我们的模型优于最先进的水平，同时也表现出对不同程度的社会噪声更高的鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Supervised+Denoising+through+Independent+Cascade+Graph+Augmentation+for+Robust+Social+Recommendation)|0|
|[Revisiting Local PageRank Estimation on Undirected Graphs: Simple and Optimal](https://doi.org/10.1145/3637528.3671820)|Hanzhi Wang|Renmin University of China, Beijing, China|We propose a simple and optimal algorithm, BackMC, for local PageRank estimation in undirected graphs: given an arbitrary target node t in an undirected graph G comprising n nodes and m edges, BackMC accurately estimates the PageRank score of node t while assuring a small relative error and a high success probability. The worst-case computational complexity of BackMC is upper bounded by O(1/dmin ⋅ min(dt, m1/2)), where dmin denotes the minimum degree of G, and dt denotes the degree of t, respectively. Compared to the previously best upper bound of O(log n ⋅ min(dt, m1/2)) (VLDB '23), which is derived from a significantly more complex algorithm and analysis, our BackMC improves the computational complexity for this problem by a factor of Θ(log n/dmin) with a much simpler algorithm. Furthermore, we establish a matching lower bound of Ω(1/dmin ⋅ min(dt, m1/2)) for any algorithm that attempts to solve the problem of local PageRank estimation, demonstrating the theoretical optimality of our BackMC. We conduct extensive experiments on various large-scale real-world and synthetic graphs, where BackMC consistently shows superior performance.|针对无向图的局部 PageRank 估计问题，提出了一种简单而优化的 BackMC 算法: 在包含 n 个节点和 m 条边的无向图 G 中，给定一个任意目标节点 t，BackMC 在保证较小的相对误差和较高的成功概率的情况下，精确地估计节点 t 的 PageRank 得分。最坏情况下 BackMC 的计算复杂度上界为 O (1/dmin ≥ min (dt，m1/2)) ，其中 dmin 表示 G 的最小度，dt 表示 t 的最小度。相比之前的最佳上界 O (log n  (dt，m1/2))(VLDB’23) ，它是由一个更加复杂的算法和分析得到的，我们的 BackMC 用一个更加简单的算法提高了 Θ (log n/dmin)的一个因子，从而提高了这个问题的计算复杂度。进一步，我们建立了任何试图解决局部 PageRank 估计问题的算法的匹配下界 Ω (1/dmin min (dt，m1/2)) ，证明了我们的 BackMC 算法的理论最优性。我们在各种大规模的真实世界和合成图上进行广泛的实验，其中 BackMC 始终显示出优越的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+Local+PageRank+Estimation+on+Undirected+Graphs:+Simple+and+Optimal)|0|
|[Performative Debias with Fair-exposure Optimization Driven by Strategic Agents in Recommender Systems](https://doi.org/10.1145/3637528.3671786)|Zhichen Xiang, Hongke Zhao, Chuang Zhao, Ming He, Jianping Fan|; AI Lab at Lenovo Research, Beijing, China; The Hong Kong University of Science and Technology, Hong Kong, China|Data bias, e.g., popularity impairs the dynamics of two-sided markets within recommender systems. This overshadows the less visible but potentially intriguing long-tail items that could capture user interest. Despite the abundance of research surrounding this issue, it still poses challenges and remains a hot topic in academic circles. Along this line, in this paper, we developed a re-ranking approach in dynamic settings with fair-exposure optimization driven by strategic agents. Designed for the producer side, the execution of agents assumes content creators can modify item features based on strategic incentives to maximize their exposure. This iterative process entails an end-to-end optimization, employing differentiable ranking operators that simultaneously target accuracy and fairness. Joint objectives ensure the performance of recommendations while enhancing the visibility of tail items. We also leveraged the performativity nature of predictions to illustrate how strategic learning influences content creators to shift towards fairness efficiently, thereby incentivizing features of tail items. Through comprehensive experiments on both public and industrial datasets, we have substantiated the effectiveness and dominance of the proposed method especially on unveiling the potential of tail items.|数据偏差，例如，受欢迎程度会损害推荐系统中双边市场的动态性。这掩盖了不太可见但可能引起用户兴趣的长尾项目。尽管围绕这一问题进行了大量的研究，但它仍然是学术界面临的挑战和热点问题。沿着这条路线，本文开发了一种在动态环境下的重新排序方法，其中公平曝光优化是由战略代理驱动的。代理的执行是为生产者设计的，它假定内容创造者可以基于战略激励来修改项目特征，以最大限度地提高他们的曝光率。这个迭代过程需要一个端到端的优化，使用可微排名运算符，同时目标的准确性和公平性。联合目标确保建议的执行，同时提高尾部项目的可见性。我们还利用预测的执行性质来说明战略学习如何影响内容创建者有效地转向公平，从而激励尾部项目的特性。通过对公共数据集和工业数据集的综合实验，验证了该方法的有效性和优越性，特别是在揭示尾部项目的潜力方面。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Performative+Debias+with+Fair-exposure+Optimization+Driven+by+Strategic+Agents+in+Recommender+Systems)|0|
|[Preventing Strategic Behaviors in Collaborative Inference for Vertical Federated Learning](https://doi.org/10.1145/3637528.3671663)|Yidan Xing, Zhenzhe Zheng, Fan Wu|Shanghai Jiao Tong University, Shanghai, China|Vertical federated learning (VFL) is an emerging collaborative machine learning paradigm to facilitate the utilization of private features distributed across multiple parties. During the inference process of VFL, the involved parties need to upload their local embeddings to be aggregated for the final prediction. Despite its remarkable performances, the inference process of the current VFL system is vulnerable to the strategic behavior of involved parties, as they could easily change the uploaded local embeddings to exert direct influences on the prediction result. In a representative case study of federated recommendation, we find the allocation of display opportunities to be severely disrupted due to the parties' preferences in display content. In order to elicit the true local embeddings for VFL system, we propose a distribution-based penalty mechanism to detect and penalize the strategic behaviors in collaborative inference. As the key motivation of our design, we theoretically prove the power of constraining the distribution of uploaded embeddings in preventing the dishonest parties from achieving higher utility. Our mechanism leverages statistical two-sample tests to distinguish whether the distribution of uploaded embeddings is reasonable, and penalize the dishonest party through deactivating her uploaded embeddings. The resulted mechanism could be shown to admit truth-telling to converge to a Bayesian Nash equilibrium asymptotically under mild conditions. The experimental results further demonstrate the effectiveness of the proposed mechanism to reduce the dishonest utility increase of strategic behaviors and promote the truthful uploading of local embeddings in inferences.|垂直联邦学习(VFL)是一种新兴的协作机器学习范式，它有助于利用分布在多个方面的私有特性。在 VFL 的推理过程中，各参与方需要上传自己的局部嵌入信息进行聚合，才能得到最终的预测结果。当前 VFL 系统的推理过程虽然具有显著的性能，但容易受到相关各方策略行为的影响，因为它们很容易改变上传的局部嵌入，从而直接影响预测结果。在联邦推荐的一个典型案例中，我们发现由于各方对显示内容的偏好，显示机会的分配会受到严重干扰。为了在 VFL 系统中实现真正的局部嵌入，我们提出了一种基于分布的惩罚机制来检测和惩罚协同推理中的策略行为。作为我们设计的关键动机，我们从理论上证明了约束上传嵌入分布的力量，防止不诚实的当事人获得更高的效用。我们的机制利用统计双样本检验来判断上传嵌入的分布是否合理，并通过停用不诚实方的上传嵌入来惩罚不诚实方。结果显示，在温和的条件下，该机制能够承认说实话，并渐近地收敛到贝叶斯纳什均衡点。实验结果进一步证明了该机制在减少策略行为的不诚实效用增加和促进推理中局部嵌入的真实上传方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Preventing+Strategic+Behaviors+in+Collaborative+Inference+for+Vertical+Federated+Learning)|0|
|[Extreme Meta-Classification for Large-Scale Zero-Shot Retrieval](https://doi.org/10.1145/3637528.3672046)|Sachin Yadav, Deepak Saini, Anirudh Buvanesh, Bhawna Paliwal, Kunal Dahiya, Siddarth Asokan, Yashoteja Prabhu, Jian Jiao, Manik Varma|Microsoft Research, Bangalore, India; Indian Institute of Technology, Delhi, India; Microsoft, Redmond, WA, USA|We develop accurate and efficient solutions for large-scale retrieval tasks where novel (zero-shot) items can arrive continuously at a rapid pace. Conventional Siamese-style approaches embed both queries and items through a small encoder and retrieve the items lying closest to the query. While this approach allows efficient addition and retrieval of novel items, the small encoder lacks sufficient capacity for the necessary world knowledge in complex retrieval tasks. The extreme classification approaches have addressed this by learning a separate classifier for each item observed in the training set which significantly increases the representation capacity of the model. Such classifiers outperform Siamese approaches on observed items, but cannot be trained for novel items due to data and latency constraints. To bridge these gaps, this paper develops: (1) A new algorithmic framework, EMMETT, which efficiently synthesizes classifiers on-the-fly for novel items, by relying on the readily available classifiers for observed items; (2) A new algorithm, IRENE, which is a simple and effective instance of EMMETT that is specifically suited for large-scale deployments, and (3) A new theoretical framework for analyzing the generalization performance in large-scale zero-shot retrieval which guides our algorithm and training related design decisions. Comprehensive experiments are conducted on a wide range of retrieval tasks which demonstrate that IRENE improves the zero-shot retrieval accuracy by up to 15% points in Recall@10 when added on top of leading encoders. Additionally, on an online A/B test in a large-scale ad retrieval task in a major search engine, IRENE improved the ad click-through rate by 4.2%. Lastly, we validate our design choices through extensive ablative experiments. The source code for IRENE is available at https://aka.ms/irene.|我们开发准确和有效的解决方案，大规模的检索任务，新的(零射击)项目可以连续到达快速的步伐。传统的暹罗式方法通过一个小编码器嵌入查询和项，并检索与查询最接近的项。虽然这种方法可以有效地增加和检索新的项目，小编码器缺乏足够的能力，必要的世界知识在复杂的检索任务。极端分类方法通过为训练集中观察到的每个项目学习一个单独的分类器来解决这个问题，这大大提高了模型的表示能力。这种分类器在观察项目上的表现优于暹罗方法，但是由于数据和延迟限制，不能对新项目进行训练。为了弥补这些差距，本文开发了: (1)一种新的算法框架—— EMMETT，该算法依靠现有的分类器对观察到的项目进行高效的动态综合分类; (2)一种新的算法—— IRENE，它是 EMMETT 的一个简单而有效的实例，特别适合于大规模部署; (3)一种新的理论框架，用于分析大规模零拍检索中的泛化性能，指导我们的算法和训练相关的设计决策。实验结果表明，在前置编码器的基础上加入 IRENE 后，Recall@10的零镜头检索精度提高了15% 。此外，在一个主要搜索引擎的大规模广告检索任务的在线 A/B 测试中，iRENE 将广告点进率提高了4.2% 。最后，我们通过广泛的烧蚀实验验证了我们的设计选择。IRENe 的源代码可在 https://aka.ms/IRENE 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Extreme+Meta-Classification+for+Large-Scale+Zero-Shot+Retrieval)|0|
|[Conversational Dueling Bandits in Generalized Linear Models](https://doi.org/10.1145/3637528.3671892)|Shuhua Yang, Hui Yuan, Xiaoying Zhang, Mengdi Wang, Hong Zhang, Huazheng Wang|University of Science and Technology of China, Hefei, China; ByteDance, Beijing, China; Oregon State University, Corvallis, OR, USA; Princeton University, Princeton, NJ, USA|Conversational recommendation systems elicit user preferences by interacting with users to obtain their feedback on recommended commodities. Such systems utilize a multi-armed bandit framework to learn user preferences in an online manner and have received great success in recent years. However, existing conversational bandit methods have several limitations. First, they only enable users to provide explicit binary feedback on the recommended items or categories, leading to ambiguity in interpretation. In practice, users are usually faced with more than one choice. Relative feedback, known for its informativeness, has gained increasing popularity in recommendation system design. Moreover, current contextual bandit methods mainly work under linear reward assumptions, ignoring practical non-linear reward structures in generalized linear models. Therefore, in this paper, we introduce relative feedback-based conversations into conversational recommendation systems through the integration of dueling bandits in generalized linear models (GLM) and propose a novel conversational dueling bandit algorithm called ConDuel. Theoretical analyses of regret upper bounds and empirical validations on synthetic and real-world data underscore ConDuel's efficacy. We also demonstrate the potential to extend our algorithm to multinomial logit bandits with theoretical and experimental guarantees, which further proves the applicability of the proposed framework.|对话式推荐系统通过与用户进行交互以获得他们对推荐商品的反馈，从而引起用户的偏好。这类系统利用多臂老虎机框架，以在线方式学习用户偏好，近年来取得了巨大成功。然而，现有的会话强盗方法有一些局限性。首先，它们只允许用户对推荐的项目或类别提供明确的二进制反馈，从而导致解释上的歧义。实际上，用户通常面临不止一种选择。相对反馈以信息量大而闻名，在推荐系统设计中越来越受欢迎。此外，目前的情境强盗方法主要在线性报酬假设下工作，忽略了广义线性模型中实际的非线性报酬结构。因此，本文通过广义线性模型(GLM)中对决斗强盗的集成，将基于相对反馈的会话引入到会话推荐系统中，提出了一种新的会话决斗强盗算法 ConDuel。对遗憾上限的理论分析以及对合成数据和现实数据的经验验证强调了 ConDuel 的有效性。在理论和实验的基础上，证明了该算法在多项式 Logit 强盗问题上的可行性，进一步证明了该算法的适用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Conversational+Dueling+Bandits+in+Generalized+Linear+Models)|0|
|[User Welfare Optimization in Recommender Systems with Competing Content Creators](https://doi.org/10.1145/3637528.3672021)|Fan Yao, Yiming Liao, Mingzhe Wu, Chuanhao Li, Yan Zhu, James Yang, Jingzhou Liu, Qifan Wang, Haifeng Xu, Hongning Wang|University of Virginia, Charlottesville, USA; University of Chicago, Chicago, USA; Yale University, New Haven, USA; University of Southern California, Los Angeles, USA; Google, Mountain View, USA; Meta Platforms, Inc., New York, USA; Meta Platforms, Inc., Menlo Park, USA|Driven by the new economic opportunities created by the creator economy, an increasing number of content creators rely on and compete for revenue generated from online content recommendation platforms. This burgeoning competition reshapes the dynamics of content distribution and profoundly impacts long-term user welfare on the platform. However, the absence of a comprehensive picture of global user preference distribution often traps the competition, especially the creators, in states that yield sub-optimal user welfare. To encourage creators to best serve a broad user population with relevant content, it becomes the platform's responsibility to leverage its information advantage regarding user preference distribution to accurately signal creators. In this study, we perform system-side user welfare optimization under a competitive game setting among content creators. We propose an algorithmic solution for the platform, which dynamically computes a sequence of weights for each user based on their satisfaction of the recommended content. These weights are then utilized to design mechanisms that adjust the recommendation policy or the post-recommendation rewards, thereby influencing creators' content production strategies. To validate the effectiveness of our proposed method, we report our findings from a series of experiments, including: 1. a proof-of-concept negative example illustrating how creators' strategies converge towards sub-optimal states without platform intervention; 2. offline experiments employing our proposed intervention mechanisms on diverse datasets; and 3. results from a three-week online experiment conducted on Instagram Reels short-video recommendation platform.|在创作者经济带来的新经济机遇的驱动下，越来越多的内容创作者依赖并竞争在线内容推荐平台产生的收入。这种蓬勃发展的竞争重塑了内容分发的动态，并深刻影响了平台上的长期用户福利。然而，缺乏全球用户偏好分布的全面图像，往往会使竞争，尤其是创造者陷入产生次优用户福利的状态。为了鼓励创作者用相关内容最好地服务于广泛的用户群体，平台有责任利用其在用户偏好分布方面的信息优势来准确地向创作者发出信号。在这项研究中，我们在内容创作者之间的竞争博弈环境下进行系统端用户福利优化。我们提出了一个平台的算法解决方案，该方案根据每个用户对推荐内容的满意度动态计算每个用户的权重序列。然后利用这些权重来设计调整推荐策略或推荐后奖励的机制，从而影响创作者的内容生产策略。为了验证我们提出的方法的有效性，我们报告了一系列的实验结果，包括: 1。一个概念证明的否定例子，说明创造者的策略如何在没有平台干预的情况下收敛到次优状态;。在不同的数据集上使用我们提出的干预机制的离线实验;。在 Instagram Reels 短视频推荐平台上进行了为期三周的在线实验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=User+Welfare+Optimization+in+Recommender+Systems+with+Competing+Content+Creators)|0|
|[Embedding Two-View Knowledge Graphs with Class Inheritance and Structural Similarity](https://doi.org/10.1145/3637528.3671941)|Kyuhwan Yeom, Hyeongjun Yang, Gayeon Park, Myeongheon Jeon, Yunjeong Ko, Byungkook Oh, KyongHo Lee|Computer Science, Yonsei University, Seoul, Republic of Korea; Computer Science and Engineering, Konkuk University, Seoul, Republic of Korea; Artificial Intelligence, Yonsei University, Seoul, Republic of Korea|Numerous large-scale knowledge graphs (KGs) fundamentally represent two-view KGs: an ontology-view KG with abstract classes in ontology and an instance-view KG with specific collections of entities instantiated from ontology classes. Two-view KG embedding aims to jointly learn continuous vector representations of entities and relations in the aforementioned two-view KGs. In essence, an ontology schema exhibits a tree-like structure guided by class hierarchies, which leads classes to form inheritance hierarchies. However, existing two-view KG embedding models neglect those hierarchies, which provides the necessity to reflect class inheritance. On the other hand, KG is constructed based on a pre-defined ontology schema that includes heterogeneous relations between classes. Furthermore, these relations are defined within the scope of those among classes since instances inherit all the properties of their corresponding classes, which reveals structural similarity between two multi-relational networks. Despite the consideration to bridge the gap among two-view KG representations, existing methods ignore the existence of structural similarity between two-view KGs. To address these issues, we propose a novel two-view KG embedding model, CISS, considering Class Inheritance and Structural Similarity between two-view KGs. To deal with class inheritance, we utilize class sets, each of which is composed of sibling classes, to learn fine-grained class representations. In addition, we configure virtual instance-view KG from clustered instances and compare subgraph representations of two-view KGs to enhance structural similarity between them. Experimental results show our superior performance compared to existing models.|许多大规模的知识图(KG)从根本上表示两个视图 KG: 一个本体视图 KG 具有本体中的抽象类，一个实例视图 KG 具有从本体类实例化的实体的特定集合。双视图幼儿园嵌入的目的是联合学习上述两视图幼儿园中实体和关系的连续向量表示。从本质上讲，本体模式表现出一种由类层次结构引导的树状结构，这种结构引导类形成继承层次结构。然而，现有的双视图 KG 嵌入模型忽略了这些层次结构，这就需要反映类继承。另一方面，KG 是基于一个预定义的本体模式构建的，该模式包含类之间的异构关系。此外，这些关系是在类之间的范围内定义的，因为实例继承了相应类的所有属性，这揭示了两个多关系网络之间的结构相似性。尽管现行方法已考虑填补双视角幼稚园表现形式之间的差距，但却忽略了双视角幼稚园之间是否存在结构相似性。为了解决这些问题，我们提出了一个新的双视图幼儿园嵌入模型 CISS，该模型考虑了类继承和双视图幼儿园之间的结构相似性。为了处理类继承，我们利用类集(每个类集都由兄弟类组成)来学习细粒度的类表示。此外，我们从集群实例配置虚拟实例视图幼稚园，并比较两个视图幼稚园的子图表示，以增强它们之间的结构相似性。实验结果表明，我们的性能优于现有的模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Embedding+Two-View+Knowledge+Graphs+with+Class+Inheritance+and+Structural+Similarity)|0|
|[Item-Difficulty-Aware Learning Path Recommendation: From a Real Walking Perspective](https://doi.org/10.1145/3637528.3671947)|Haotian Zhang, Shuanghong Shen, Bihan Xu, Zhenya Huang, Jinze Wu, Jing Sha, Shijin Wang|; iFLYTEK AI Research, Hefei, China; State Key Laboratory of Cognitive Intelligence & iFLYTEK AI Research, Hefei, China|Learning path recommendation aims to provide learners with a reasonable order of items to achieve their learning goals. Intuitively, the learning process on the learning path can be metaphorically likened to walking. Despite extensive efforts in this area, most previous methods mainly focus on the relationship among items but overlook the difficulty of items, which may raise two issues from a real walking perspective: (1) The path may be rough: When learners tread the path without considering item difficulty, it's akin to walking a dark, uneven road, making learning harder and dampening interest. (2) The path may be inefficient: Allowing learners only a few attempts on very challenging items before switching, or persisting with a difficult item despite numerous attempts without mastery, can result in inefficiencies in the learning journey. To conquer the above limitations, we propose a novel method named Difficulty-constrained Learning Path Recommendation (DLPR), which is aware of item difficulty. Specifically, we first explicitly categorize items into learning items and practice items, then construct a hierarchical graph to model and leverage item difficulty adequately. Then we design a Difficulty-driven Hierarchical Reinforcement Learning (DHRL) framework to facilitate learning paths with efficiency and smoothness. Finally, extensive experiments on three different simulators demonstrate our framework achieves state-of-the-art performance.|学习路径推荐的目的是为学习者提供一个合理的项目顺序，以实现他们的学习目标。直观地说，学习过程中的学习路径可以比喻为行走。尽管在这个领域做了大量的努力，以前的大多数方法主要关注项目之间的关系，但是忽略了项目的难度，这可能会从一个真正的行走的角度提出两个问题: (1)路径可能是粗糙的: 当学习者在不考虑项目难度的情况下行走在路径上，这就像走在一条黑暗的、不平坦的路上，使学习更加困难和抑制兴趣。(2)路径可能是低效的: 允许学习者在转换之前只尝试几次非常具有挑战性的项目，或者尽管尝试了很多次但没有掌握，仍然坚持一个困难的项目，可能会导致学习过程中的低效。为了克服上述限制，本文提出了一种新的学习路径推荐方法——难度约束学习路径推荐(DLPR) ，该方法能够识别项目的难度。具体来说，我们首先明确地将项目分类为学习项目和实践项目，然后构建一个层次图来充分地建模和利用项目难度。然后，我们设计了一个难度驱动的层次强化学习(dHRL)框架，以促进学习路径的有效性和顺畅性。最后，在三个不同的模拟器上进行了广泛的实验，证明了我们的框架实现了最先进的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Item-Difficulty-Aware+Learning+Path+Recommendation:+From+a+Real+Walking+Perspective)|0|
|[Optimized Cost Per Click in Online Advertising: A Theoretical Analysis](https://doi.org/10.1145/3637528.3671767)|Kaichen Zhang, Zixuan Yuan, Hui Xiong||In recent years, Optimized Cost Per Click (OCPC) and Optimized Cost Per Mille(OCPM) have emerged as the most widely adopted pricing models in the onlineadvertising industry. However, the existing literature has yet to identify thespecific conditions under which these models outperform traditional pricingmodels like Cost Per Click (CPC) and Cost Per Action (CPA). To fill the gap,this paper builds an economic model that compares OCPC with CPC and CPAtheoretically, which incorporates out-site scenarios and outside options as twokey factors. Our analysis reveals that OCPC can effectively replace CPA bytackling the problem of advertisers strategically manipulating conversionreporting in out-site scenarios where conversions occur outside the advertisingplatform. Furthermore, OCPC exhibits the potential to surpass CPC in platformpayoffs by providing higher advertiser payoffs and consequently attracting moreadvertisers. However, if advertisers have less competitive outside options andconsistently stay in the focal platform, the platform may achieve higherpayoffs using CPC. Our findings deliver valuable insights for onlineadvertising platforms in selecting optimal pricing models, and providerecommendations for further enhancing their payoffs. To the best of ourknowledge, this is the first study to analyze OCPC from an economicperspective. Moreover, our analysis can be applied to the OCPM model as well.|近年来，优化每点击成本(OCPC)和优化每公里成本(OCPM)已经成为在线广告行业最广泛采用的定价模型。然而，现有文献尚未确定这些模型优于传统定价模型如每次点击成本(CPC)和每次行动成本(CPA)的具体条件。为了填补这一空白，本文从理论上建立了一个比较 OCPC 与 CPC 和 CPA 的经济模型，该模型将场外情景和场外选择作为两个关键因素。我们的分析表明，OCPC 可以有效地替代 CPA，解决广告商在转化发生在广告平台之外的外部场景中策略性地操纵转化报告的问题。此外，OCPC 通过提供更高的广告客户收益，从而吸引更多的广告客户，在平台收益方面显示出超过 CPC 的潜力。然而，如果广告商没有那么多竞争性的外部选择，并且一直呆在焦点平台上，那么该平台可能会利用 CPC 获得更高的回报。我们的研究结果为在线广告平台选择最佳定价模型提供了有价值的见解，并为进一步提高其收益提供了建议。据我们所知，这是第一个从经济学角度分析 OCPC 的研究。此外，我们的分析也适用于 OCPM 模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimized+Cost+Per+Click+in+Online+Advertising:+A+Theoretical+Analysis)|0|
|[Counteracting Duration Bias in Video Recommendation via Counterfactual Watch Time](https://doi.org/10.1145/3637528.3671817)|Haiyuan Zhao, Guohao Cai, Jieming Zhu, Zhenhua Dong, Jun Xu, JiRong Wen|Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; Noah's Ark Lab, Huawei, Shenzhen, China; School of Information, Renmin University of China, Beijing, China|In video recommendation, an ongoing effort is to satisfy users' personalizedinformation needs by leveraging their logged watch time. However, watch timeprediction suffers from duration bias, hindering its ability to reflect users'interests accurately. Existing label-correction approaches attempt to uncoveruser interests through grouping and normalizing observed watch time accordingto video duration. Although effective to some extent, we found that theseapproaches regard completely played records (i.e., a user watches the entirevideo) as equally high interest, which deviates from what we observed on realdatasets: users have varied explicit feedback proportion when completelyplaying videos. In this paper, we introduce the counterfactual watch time(CWT),the potential watch time a user would spend on the video if its duration issufficiently long. Analysis shows that the duration bias is caused by thetruncation of CWT due to the video duration limitation, which usually occurs onthose completely played records. Besides, a Counterfactual Watch Model (CWM) isproposed, revealing that CWT equals the time users get the maximum benefit fromvideo recommender systems. Moreover, a cost-based transform function is definedto transform the CWT into the estimation of user interest, and the model can belearned by optimizing a counterfactual likelihood function defined overobserved user watch times. Extensive experiments on three real videorecommendation datasets and online A/B testing demonstrated that CWMeffectively enhanced video recommendation accuracy and counteracted theduration bias.|在视频推荐中，一个持续的努力是通过利用用户的观看时间来满足用户的个性化信息需求。然而，手表时间预测存在持续时间偏差，影响了其准确反映用户兴趣的能力。现有的标签校正方法试图通过根据视频持续时间对观看时间进行分组和标准化来揭示用户的兴趣。虽然在某种程度上有效，但是我们发现这些方法把完全播放的记录(例如，用户观看整个视频)视为同样高的兴趣，这偏离了我们在真实数据集上观察到的: 当完全播放视频时，用户有不同的显式反馈比例。本文介绍了反事实观看时间(CWT) ，即当视频持续时间过长时，用户可能花在视频上的观看时间。分析表明，持续时间偏差是由于视频持续时间受到限制而导致的连续小波变换(CWT)截断所引起的，这种情况通常发生在完全播放的记录上。此外，提出了一种反事实观察模型(CWM) ，揭示了 CWT 等于用户从视频推荐系统中获得最大收益的时间。此外，定义了一个基于代价的转换函数，将连续小波变换转换为用户兴趣的估计，该模型可以通过优化一个反事实似然函数来定义过度观察的用户观察时间。在三个真实视频推荐数据集上的大量实验和在线 A/B 测试表明，CWM 有效地提高了视频推荐的准确性，抵消了持续时间偏差。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Counteracting+Duration+Bias+in+Video+Recommendation+via+Counterfactual+Watch+Time)|0|
|[MMBee: Live Streaming Gift-Sending Recommendations via Multi-Modal Fusion and Behaviour Expansion](https://doi.org/10.1145/3637528.3671511)|Jiaxin Deng, Shiyao Wang, Yuchen Wang, Jiansong Qi, Liqin Zhao, Guorui Zhou, Gaofeng Meng|; Institute of Automation, Beijing, China; KuaiShou Inc., Beijing, China|Live streaming services are becoming increasingly popular due to real-time interactions and entertainment. Viewers can chat and send comments or virtual gifts to express their preferences for the streamers. Accurately modeling the gifting interaction not only enhances users' experience but also increases streamers' revenue. Previous studies on live streaming gifting prediction treat this task as a conventional recommendation problem, and model users' preferences using categorical data and observed historical behaviors. However, it is challenging to precisely describe the real-time content changes in live streaming using limited categorical information. Moreover, due to the sparsity of gifting behaviors, capturing the preferences and intentions of users is quite difficult. In this work, we propose MMBee based on real-time Multi-Modal Fusion and Behaviour Expansion to address these issues. Specifically, we first present a Multi-modal Fusion Module with Learnable Query (MFQ) to perceive the dynamic content of streaming segments and process complex multi-modal interactions, including images, text comments and speech. To alleviate the sparsity issue of gifting behaviors, we present a novel Graph-guided Interest Expansion (GIE) approach that learns both user and streamer representations on large-scale gifting graphs with multi-modal attributes. It consists of two main parts: graph node representations pre-training and metapath-based behavior expansion, all of which help model jump out of the specific historical gifting behaviors for exploration and largely enrich the behavior representations. Comprehensive experiment results show that MMBee achieves significant performance improvements on both public datasets and Kuaishou real-world streaming datasets and the effectiveness has been further validated through online A/B experiments. MMBee has been deployed and is serving hundreds of millions of users at Kuaishou.|由于实时交互和娱乐，流媒体直播服务变得越来越流行。观众可以聊天和发送评论或虚拟礼物来表达他们对主播的喜好。精确建模礼物互动不仅提高了用户的体验，而且增加了主播的收入。以往的流媒体直播礼物预测研究将这一任务视为一个传统的推荐问题，并利用分类数据和观察到的历史行为对用户的偏好进行建模。然而，使用有限的分类信息来精确描述直播流中的实时内容变化是一个挑战。此外，由于送礼行为的稀少性，要捕捉用户的喜好和意图是相当困难的。在这项工作中，我们提出了基于实时多模态融合和行为扩展的 MMBee 来解决这些问题。具体来说，我们首先提出了一种基于可学习查询(Learnable Query，MFQ)的多模态融合模块来感知流媒体片段的动态内容，并处理复杂的多模态交互，包括图像、文本注释和语音。为了缓解送礼行为的稀疏性问题，我们提出了一种新的图引导兴趣扩展(GIE)方法，该方法同时学习用户和流媒体在具有多模态属性的大规模送礼图上的表示。它包括两个主要部分: 图节点表示预训练和基于元路径的行为扩展，所有这些都有助于模型跳出具体的历史馈赠行为去探索，并大大丰富了行为表示。综合实验结果表明，mMBee 在公共数据集和 Kuaishou 真实世界流数据集上都取得了显著的性能改善，并通过在线 A/B 实验进一步验证了其有效性。MMBee 已经部署完毕，目前正在 Kuaishou 为数亿用户服务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MMBee:+Live+Streaming+Gift-Sending+Recommendations+via+Multi-Modal+Fusion+and+Behaviour+Expansion)|0|
|[Contextual Distillation Model for Diversified Recommendation](https://doi.org/10.1145/3637528.3671514)|Fan Li, Xu Si, Shisong Tang, Dingmin Wang, Kunyan Han, Bing Han, Guorui Zhou, Yang Song, Hechang Chen|Jilin University, Changchun, China; University of Oxford, Oxford, United Kingdom; Tsinghua University, Beijing, China; Kuaishou Inc. & Tsinghua University, Beijing, China; University of Science and Technology of China, Hefei, China; Kuaishou Inc., Beijing, China|The diversity of recommendation is equally crucial as accuracy in improvinguser experience. Existing studies, e.g., Determinantal Point Process (DPP) andMaximal Marginal Relevance (MMR), employ a greedy paradigm to iterativelyselect items that optimize both accuracy and diversity. However, prior methodstypically exhibit quadratic complexity, limiting their applications to there-ranking stage and are not applicable to other recommendation stages with alarger pool of candidate items, such as the pre-ranking and ranking stages. Inthis paper, we propose Contextual Distillation Model (CDM), an efficientrecommendation model that addresses diversification, suitable for thedeployment in all stages of industrial recommendation pipelines. Specifically,CDM utilizes the candidate items in the same user request as context to enhancethe diversification of the results. We propose a contrastive context encoderthat employs attention mechanisms to model both positive and negative contexts.For the training of CDM, we compare each target item with its context embeddingand utilize the knowledge distillation framework to learn the win probabilityof each target item under the MMR algorithm, where the teacher is derived fromMMR outputs. During inference, ranking is performed through a linearcombination of the recommendation and student model scores, ensuring bothdiversity and efficiency. We perform offline evaluations on two industrialdatasets and conduct online A/B test of CDM on the short-video platformKuaiShou. The considerable enhancements observed in both recommendation qualityand diversity, as shown by metrics, provide strong superiority for theeffectiveness of CDM.|推荐的多样性与提高用户体验的准确性同样重要。现有的研究，例如行列式点过程(DPP)和最大边际相关性(MMR) ，采用贪婪的范式迭代选择项目，优化准确性和多样性。然而，先验方法通常表现出二次复杂性，将其应用限制在三阶段排名阶段，不适用于其他候选项目较多的推荐阶段，如预先排名阶段和排名阶段。本文提出了一种适用于行业推荐流程各个阶段的高效多样化推荐模型——上下文精馏模型(CDM)。具体来说，CDM 利用同一用户请求中的候选项作为上下文，以增强结果的多样化。我们提出了一个对比语境编码器，它使用注意机制来模拟积极和消极的语境。对于 CDM 的培训，我们将每个目标项目与其上下文嵌入进行比较，并利用知识提取框架在 MMR 算法下学习每个目标项目的获胜概率，其中教师是从 MMR 输出中获得的。在推理过程中，排名是通过推荐和学生模型得分的线性组合来完成的，从而保证了多样性和效率。我们对两个工业数据集进行离线评估，并在快手短视频平台上对 CDM 进行在线 A/B 测试。指标表明，在推荐质量和多样性方面观察到的显著增强为 CDM 的有效性提供了强大的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contextual+Distillation+Model+for+Diversified+Recommendation)|0|
|[MGMatch: Fast Matchmaking with Nonlinear Objective and Constraints via Multimodal Deep Graph Learning](https://doi.org/10.1145/3637528.3671553)|Yu Sun, Kai Wang, Zhipeng Hu, Runze Wu, Yaoxin Wu, Wen Song, Xudong Shen, Tangjie Lv, Changjie Fan|Fuxi AI Lab, NetEase Inc., Hangzhou, Zhejiang, China; Eindhoven University of Technology, Eindhoven, Netherlands; Shandong University, Qingdao, Shandong, China|As a core problem of online games, matchmaking is to assign players into multiple teams to maximize their gaming experience. With the rapid development of game industry, it is increasingly difficulty to explicitly model players' experiences as linear functions. Instead, it is often modeled in a data-driven way by training a neural network. Meanwhile, complex rules must be satisfied to ensure the robustness of matchmaking, which are often described using logical operators. Therefore, matchmaking in practical scenarios is a challenging combinatorial optimization problem with nonlinear objective, linear constraints and logical constraints, which receives much less attention in previous research. In this paper, we propose a novel deep learning method for high-quality matchmaking in real-time. We first cast the problem as standard mixed-integer programming (MIP) by linearizing ReLU networks and logical constraints. Then, based on supervised learning, we design and train a multi-modal graph learning architecture to predict optimal solutions end-to-end from instance data, and solve a surrogate problem to efficiently obtain feasible solutions. Evaluation results on real industry datasets show that our method can deliver near-optimal solutions within 100ms.|作为网络游戏的一个核心问题，匹配是将玩家分配到多个团队中，以最大限度地提高他们的游戏体验。随着游戏产业的快速发展，将玩家的体验明确地建模为线性函数变得越来越困难。相反，它通常是通过训练神经网络以数据驱动的方式建模的。同时，为了保证匹配的鲁棒性，必须满足复杂的规则，这些规则通常用逻辑运算符来描述。因此，实际场景中的匹配问题是一个具有非线性目标、线性约束和逻辑约束的组合优化问题，在以往的研究中受到的关注较少。本文提出了一种新的高质量实时匹配的深度学习方法。首先通过线性化 ReLU 网络和逻辑约束将问题转化为标准的混合整数规划(MIP)问题。然后，在监督式学习的基础上，我们设计并训练了一个多模态图学习架构来从实例数据中预测端到端的最优解，并解决一个代理问题来有效地获得可行的解。对实际工业数据集的评估结果表明，该方法可以在100ms 内提供接近最优的解决方案。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MGMatch:+Fast+Matchmaking+with+Nonlinear+Objective+and+Constraints+via+Multimodal+Deep+Graph+Learning)|0|
|[R-Eval: A Unified Toolkit for Evaluating Domain Knowledge of Retrieval Augmented Large Language Models](https://doi.org/10.1145/3637528.3671564)|Shangqing Tu, Yuanchun Wang, Jifan Yu, Yuyang Xie, Yaran Shi, Xiaozhi Wang, Jing Zhang, Lei Hou, Juanzi Li|BNRist, DCST, Tsinghua University, Beijing, China; SIOE, Beihang University, Beijing, China; SoI, Renmin University of China, Beijing, China; DCST, Tsinghua University, Beijing, China|Large language models have achieved remarkable success on general NLP tasks,but they may fall short for domain-specific problems. Recently, variousRetrieval-Augmented Large Language Models (RALLMs) are proposed to address thisshortcoming. However, existing evaluation tools only provide a few baselinesand evaluate them on various domains without mining the depth of domainknowledge. In this paper, we address the challenges of evaluating RALLMs byintroducing the R-Eval toolkit, a Python toolkit designed to streamline theevaluation of different RAG workflows in conjunction with LLMs. Our toolkit,which supports popular built-in RAG workflows and allows for the incorporationof customized testing data on the specific domain, is designed to beuser-friendly, modular, and extensible. We conduct an evaluation of 21 RALLMsacross three task levels and two representative domains, revealing significantvariations in the effectiveness of RALLMs across different tasks and domains.Our analysis emphasizes the importance of considering both task and domainrequirements when choosing a RAG workflow and LLM combination. We are committedto continuously maintaining our platform at https://github.com/THU-KEG/R-Evalto facilitate both the industry and the researchers.|大型语言模型已经在一般的 NLP 任务上取得了显著的成功，但是它们可能在特定领域的问题上有所不足。最近，各种检索增强大型语言模型(RALLMs)被提出来解决这个问题。然而，现有的评估工具只提供了一些基线，并在不同的领域进行评估，而没有挖掘领域知识的深度。在本文中，我们通过引入 R-Eval 工具包来解决评估 RAG 工作流的挑战，这是一个 Python 工具包，旨在与 LLM 一起简化不同 RAG 工作流的评估。我们的工具包，支持流行的内置 RAG 工作流程，并允许在特定领域集成定制的测试数据，被设计成用户友好的、模块化的和可扩展的。我们在三个任务级别和两个代表性领域对21个 RALLM 进行了评估，揭示了 RALLM 在不同任务和领域的有效性的显著差异。我们的分析强调了在选择 RAG 工作流和 LLM 组合时同时考虑任务和领域需求的重要性。我们致力于不断维护我们的平台， https://github.com/thu-keg/r-evalto 为业界和研究人员提供便利。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=R-Eval:+A+Unified+Toolkit+for+Evaluating+Domain+Knowledge+of+Retrieval+Augmented+Large+Language+Models)|0|
|[ADSNet: Cross-Domain LTV Prediction with an Adaptive Siamese Network in Advertising](https://doi.org/10.1145/3637528.3671612)|Ruize Wang, Hui Xu, Ying Cheng, Qi He, Xing Zhou, Rui Feng, Wei Xu, Lei Huang, Jie Jiang|Tencent Inc., Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; Tencent Inc., Shenzhen, China|Advertising platforms have evolved in estimating Lifetime Value (LTV) to better align with advertisers' true performance metric which considers cumulative sum of purchases a customer contributes over a period. Accurate LTV estimation is crucial for the precision of the advertising system and the effectiveness of advertisements. However, the sparsity of real-world LTV data presents a significant challenge to LTV predictive model(i.e., pLTV), severely limiting the their capabilities. Therefore, we propose to utilize external data, in addition to the internal data of advertising platform, to expand the size of purchase samples and enhance the LTV prediction model of the advertising platform. To tackle the issue of data distribution shift between internal and external platforms, we introduce an Adaptive Difference Siamese Network (ADSNet), which employs cross-domain transfer learning to prevent negative transfer. Specifically, ADSNet is designed to learn information that is beneficial to the target domain. We introduce a gain evaluation strategy to calculate information gain, aiding the model in learning helpful information for the target domain and providing the ability to reject noisy samples, thus avoiding negative transfer. Additionally, we also design a Domain Adaptation Module as a bridge to connect different domains, reduce the distribution distance between them, and enhance the consistency of representation space distribution. We conduct extensive offline experiments and online A/B tests on a real advertising platform. Our proposed ADSNet method outperforms other methods, improving GINI by 2%. The ablation study highlights the importance of the gain evaluation strategy in negative gain sample rejection and improving model performance. Additionally, ADSNet significantly improves long-tail prediction. The online A/B tests confirm ADSNet's efficacy, increasing online LTV by 3.47% and GMV by 3.89%.|广告平台已经发展到估算终身价值(LTV) ，以便更好地与广告商的真实业绩指标保持一致，后者考虑的是消费者在一段时间内贡献的累计购买总额。准确的 LTV 估计对于广告系统的准确性和广告的有效性至关重要。然而，实际 LTV 数据的稀疏性对 LTV 预测模型(即 pLTV)提出了严峻的挑战，严重限制了它们的能力。因此，我们建议利用外部数据，除了广告平台的内部数据外，扩大购买样本的规模，提高广告平台的 LTV 预测模型。为了解决内部平台和外部平台之间数据分布转移的问题，我们引入了自适应差分暹罗网(ADSNet) ，该网络采用跨域传输学习来防止负向传输。具体来说，ADSNet 是为了学习对目标域有益的信息而设计的。我们引入一个增益评估策略来计算信息增益，帮助模型学习目标域的有用信息，并提供拒绝噪声样本的能力，从而避免负迁移。此外，我们还设计了一个领域适应模块作为连接不同领域的桥梁，减少它们之间的分布距离，提高表示空间分布的一致性。我们在一个真实的广告平台上进行广泛的离线实验和在线 A/B 测试。我们提出的 ADSNet 方法优于其他方法，GINI 提高了2% 。烧蚀研究突出了增益评价策略在抑制负增益样本和改善模型性能中的重要性。此外，ADSNet 显著改善了长尾预测。在线 A/B 测试证实了 ADSNet 的有效性，在线 LTV 增加了3.47% ，GMV 增加了3.89% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ADSNet:+Cross-Domain+LTV+Prediction+with+an+Adaptive+Siamese+Network+in+Advertising)|0|
|[Multi-Behavior Collaborative Filtering with Partial Order Graph Convolutional Networks](https://doi.org/10.1145/3637528.3671569)|Yijie Zhang, Yuanchen Bei, Hao Chen, Qijie Shen, Zheng Yuan, Huan Gong, Senzhang Wang, Feiran Huang, Xiao Huang|Alibaba Group, Zhejiang, Hangzhou, China; Zhejiang University, Zhejiang, Hangzhou, China; Jinan University, Guangzhou, China; National University of Defense Technology, Changsha, China; The Hong Kong Polytechnic University, Hong Kong, China; Central South University, Hunan, Changsha, China|Representing information of multiple behaviors in the single graph collaborative filtering (CF) vector has been a long-standing challenge. This is because different behaviors naturally form separate behavior graphs and learn separate CF embeddings. Existing models merge the separate embeddings by appointing the CF embeddings for some behaviors as the primary embedding and utilizing other auxiliaries to enhance the primary embedding. However, this approach often results in the joint embedding performing well on the main tasks but poorly on the auxiliary ones. To address the problem arising from the separate behavior graphs, we propose the concept of Partial Order Recommendation Graphs (POG). POG defines the partial order relation of multiple behaviors and models behavior combinations as weighted edges to merge separate behavior graphs into a joint POG. Theoretical proof verifies that POG can be generalized to any given set of multiple behaviors. Based on POG, we propose the tailored Partial Order Graph Convolutional Networks (POGCN) that convolute neighbors' information while considering the behavior relations between users and items. POGCN also introduces a partial-order BPR sampling strategy for efficient and effective multiple-behavior CF training. POGCN has been successfully deployed on the homepage of Alibaba for two months, providing recommendation services for over one billion users. Extensive offline experiments conducted on three public benchmark datasets demonstrate that POGCN outperforms state-of-the-art multi-behavior baselines across all types of behaviors. Furthermore, online A/B tests confirm the superiority of POGCN in billion-scale recommender systems.|在单一图形协同过滤(CF)向量中表示多种行为的信息一直是一个长期的挑战。这是因为不同的行为自然地形成独立的行为图，并学习独立的 CF 嵌入。现有的模型通过指定一些行为的 CF 嵌入作为主嵌入，并利用其他辅助来增强主嵌入，从而将分离的嵌入进行合并。然而，这种方法往往导致联合嵌入在主要任务上表现良好，但在辅助任务上表现不佳。为了解决分离的行为图所带来的问题，我们提出了偏序推荐图(POG)的概念。POG 将多个行为和模型行为组合的偏序关系定义为加权边，以便将单独的行为图合并到一个联合 POG 中。理论证明了 POG 可以推广到任意给定的一组多行为。在 POG 的基础上，提出了一种考虑用户与项目之间行为关系的卷积邻居信息的剪裁偏序图卷积网络(POGCN)。POGCN 还引入了一种偏序 BPR 抽样策略，用于有效的多行为 CF 训练。POgcn 已成功登入阿里巴巴网页两个月，为超过十亿用户提供推荐服务。在三个公共基准数据集上进行的大量离线实验表明，POGCN 在所有类型的行为上都优于最先进的多行为基准。此外，在线 A/B 测试证实了 POGCN 在亿万规模推荐系统中的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Behavior+Collaborative+Filtering+with+Partial+Order+Graph+Convolutional+Networks)|0|
|[Inductive Modeling for Realtime Cold Start Recommendations](https://doi.org/10.1145/3637528.3671588)|Chandler Zuo, Jonathan Castaldo, Hanqing Zhu, Haoyu Zhang, Ji Liu, Yangpeng Ou, Xiao Kong|Meta, Menlo Park, CA, USA|In recommendation systems, the timely delivery of new content to their relevant audiences is critical for generating a growing and high quality collection of content for all users. The nature of this problem requires retrieval models to be able to make inferences in real time and with high relevance. There are two specific challenges for cold start contents. First, the information loss problem in a standard Two Tower model, due to the limited feature interactions between the user and item towers, is exacerbated for cold start items due to training data sparsity. Second, the huge volume of user-generated content in industry applications today poses a big bottleneck in the end-to-end latency of recommending new content. To overcome the two challenges, we propose a novel architecture, the Item History Model (IHM). IHM directly injects user-interaction information into the item tower to overcome information loss. In addition, IHM incorporates an inductive structure using attention-based pooling to eliminate the need for recurring training, a key bottleneck for the real-timeness. On both public and industry datasets, we demonstrate that IHM can not only outperform baselines in recommending cold start contents, but also achieves SoTA real-timeness in industry applications.|在推荐系统中，及时向相关受众提供新内容对于为所有用户收集越来越多的高质量内容至关重要。这个问题的性质要求检索模型能够实时地进行推理，并且具有高度的相关性。对于冷启动内容有两个具体的挑战。首先，标准双塔模型中的信息丢失问题，由于用户和项目塔之间的特征交互有限，由于训练数据稀疏而加剧了冷启动项目的信息丢失问题。其次，当今行业应用程序中的大量用户生成内容对推荐新内容的端到端延迟造成了巨大的瓶颈。为了克服这两个挑战，我们提出了一个新的体系结构，项目历史模型(IHM)。IHM 直接将用户交互信息注入到项目塔中以克服信息丢失。此外，IHM 采用了一种归纳结构，使用基于注意力的汇集来消除重复训练的需要，这是实时性的一个关键瓶颈。在公共数据集和工业数据集上，我们证明 IHM 不仅在推荐冷启动内容方面优于基线，而且在工业应用中实现了 SoTA 的实时性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Inductive+Modeling+for+Realtime+Cold+Start+Recommendations)|0|
|[Bias and Unfairness in Information Retrieval Systems: New Challenges in the LLM Era](https://doi.org/10.1145/3637528.3671458)|Sunhao Dai, Chen Xu, Shicheng Xu, Liang Pang, Zhenhua Dong, Jun Xu|Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; ; Huawei Noah's Ark Lab, Shenzhen, China|With the rapid advancements of large language models (LLMs), information retrieval (IR) systems, such as search engines and recommender systems, have undergone a significant paradigm shift. This evolution, while heralding new opportunities, introduces emerging challenges, particularly in terms of biases and unfairness, which may threaten the information ecosystem. In this paper, we present a comprehensive survey of existing works on emerging and pressing bias and unfairness issues in IR systems when the integration of LLMs. We first unify bias and unfairness issues as distribution mismatch problems, providing a groundwork for categorizing various mitigation strategies through distribution alignment. Subsequently, we systematically delve into the specific bias and unfairness issues arising from three critical stages of LLMs integration into IR systems: data collection, model development, and result evaluation. In doing so, we meticulously review and analyze recent literature, focusing on the definitions, characteristics, and corresponding mitigation strategies associated with these issues. Finally, we identify and highlight some open problems and challenges for future work, aiming to inspire researchers and stakeholders in the IR field and beyond to better understand and mitigate bias and unfairness issues of IR in this LLM era. We also consistently maintain a GitHub repository for the relevant papers and resources in this rising direction at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey.|随着大型语言模型(LLM)的快速发展，搜索引擎和推荐系统等信息检索(IR)系统经历了一个重大的范式转变。这种演变虽然预示着新的机遇，但也带来了新的挑战，特别是偏见和不公平方面的挑战，可能威胁到信息生态系统。在本文中，我们提出了一个综合调查的现有工作中出现的和紧迫的偏见和不公平问题的国际关系系统时，一体化的 LLM。我们首先将偏见和不公平问题统一为分布不匹配问题，为通过分布对齐来分类各种缓解策略提供了基础。随后，我们系统地研究了 LLM 集成到 IR 系统的三个关键阶段所引起的具体的偏见和不公平问题: 数据收集、模型开发和结果评估。在这样做的时候，我们仔细审查和分析最近的文献，重点是定义，特点和相应的缓解策略与这些问题有关。最后，我们确定并强调了一些开放的问题和未来工作的挑战，旨在激励研究人员和利益相关者在国际关系领域和以外更好地理解和减轻在这个 LLM 时代的国际关系的偏见和不公平问题。我们也一直保持着一个 GitHub 资源库，用来存放这个不断增长的 https://GitHub.com/kid-22/llm-ir-bias-fairness-survey 的相关文章和资源。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bias+and+Unfairness+in+Information+Retrieval+Systems:+New+Challenges+in+the+LLM+Era)|0|
|[Approximating Memorization Using Loss Surface Geometry for Dataset Pruning and Summarization](https://doi.org/10.1145/3637528.3671985)|Andrea Agiollo, Young In Kim, Rajiv Khanna|University of Bologna, Bologna, Italy; Purdue University, West Lafayette, IN, USA|The sustainable training of modern neural network models represents an open challenge. Several existing methods approach this issue by identifying a subset of relevant data samples from the full training data to be used in model optimization with the goal of matching the performance of the full data training with that of the subset data training. Our work explores using memorization scores to find representative and atypical samples. We demonstrate that memorization-aware dataset summarization improves the subset construction performance. However, computing memorization scores is notably resource-intensive. To this end, we propose a novel method that leverages the discrepancy between sharpness-aware minimization and stochastic gradient descent to capture data points atypicality. We evaluate our metric over several efficient approximation functions for memorization scores - namely proxies -, empirically showing superior correlation and effectiveness. We explore the causes behind our approximation quality, highlighting how typical data points trigger a flatter loss landscape compared to atypical ones. Extensive experiments confirm the effectiveness of our proxy for dataset pruning and summarization tasks, surpassing state-of-the-art approaches both on canonical setups - where atypical data points benefit performance - and few-shot learning scenarios-where atypical data points can be detrimental.|现代神经网络模型的可持续训练是一个开放的挑战。一些现有的方法通过从完整的训练数据中确定一个相关数据样本子集来解决这个问题，这些数据样本将用于模型优化，目标是使完整数据训练的性能与子集数据训练的性能相匹配。我们的工作探索使用记忆分数来寻找代表性和非典型样本。我们证明了记忆感知的数据集摘要能够提高幂集构造的性能。然而，计算记忆分数是显著的资源密集型。为此，我们提出了一种新的方法，利用锐度感知最小化和随机梯度下降之间的差异来捕捉数据点的非典型性。我们评估我们的度量在几个有效的近似函数的记忆分数-即代理-，经验表明优越的相关性和有效性。我们探索了近似质量背后的原因，强调了典型数据点与非典型数据点相比如何触发更平坦的损失景观。大量的实验证实了我们的代理对于数据集裁剪和总结任务的有效性，超越了在规范设置(非典型数据点有利于性能)和少数学习场景(非典型数据点可能是有害的)上的最新方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Approximating+Memorization+Using+Loss+Surface+Geometry+for+Dataset+Pruning+and+Summarization)|0|
|[Evading Community Detection via Counterfactual Neighborhood Search](https://doi.org/10.1145/3637528.3671896)|Andrea Bernini, Fabrizio Silvestri, Gabriele Tolomei|Sapienza University of Rome, Rome, Italy|Community detection techniques are useful for social media platforms todiscover tightly connected groups of users who share common interests. However,this functionality often comes at the expense of potentially exposingindividuals to privacy breaches by inadvertently revealing their tastes orpreferences. Therefore, some users may wish to preserve their anonymity and optout of community detection for various reasons, such as affiliation withpolitical or religious organizations, without leaving the platform. In thisstudy, we address the challenge of community membership hiding, which involvesstrategically altering the structural properties of a network graph to preventone or more nodes from being identified by a given community detectionalgorithm. We tackle this problem by formulating it as a constrainedcounterfactual graph objective, and we solve it via deep reinforcementlearning. Extensive experiments demonstrate that our method outperformsexisting baselines, striking the best balance between accuracy and cost.|社区检测技术对于社交媒体平台发现有共同兴趣的紧密联系的用户群非常有用。然而，这种功能往往是以牺牲个人隐私被侵犯的潜在风险为代价的，因为它无意中暴露了个人的品味或偏好。因此，一些用户可能希望保持匿名，并出于各种原因，例如与政治或宗教组织的联系，而不离开平台，拒绝进行社区检测。在这项研究中，我们解决了社区成员隐藏的挑战，这涉及到策略性地改变网络图的结构特性，以防止一个或多个节点被给定的社区检测算法识别。我们把这个问题表述为一个有约束的反事实图目标，并通过深度强化学习来解决它。大量的实验表明，我们的方法性能优于现有的基线，在准确性和成本之间达到了最佳的平衡。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Evading+Community+Detection+via+Counterfactual+Neighborhood+Search)|0|
|[FoRAG: Factuality-optimized Retrieval Augmented Generation for Web-enhanced Long-form Question Answering](https://doi.org/10.1145/3637528.3672065)|Tianchi Cai, Zhiwen Tan, Xierui Song, Tao Sun, Jiyan Jiang, Yunqi Xu, Yinger Zhang, Jinjie Gu|Tsinghua University, Beijing, China; Ant Group, Hangzhou, China|Retrieval Augmented Generation (RAG) has become prevalent in question-answering (QA) tasks due to its ability of utilizing search engine to enhance the quality of long-form question-answering (LFQA). Despite the emergence of various open source methods and web-enhanced commercial systems such as Bing Chat, two critical problems remain unsolved, i.e., the lack of factuality and clear logic in the generated long-form answers. In this paper, we remedy these issues via a systematic study on answer generation in web-enhanced LFQA. Specifically, we first propose a novel outline-enhanced generator to achieve clear logic in the generation of multifaceted answers and construct two datasets accordingly. Then we propose a factuality optimization method based on a carefully designed doubly fine-grained RLHF framework, which contains automatic evaluation and reward modeling in different levels of granularity. Our generic framework comprises conventional fine-grained RLHF methods as special cases. Extensive experiments verify the superiority of our proposed Factuality-optimized RAG (FoRAG) method on both English and Chinese benchmarks. In particular, when applying our method to Llama2-7B-chat, the derived model FoRAG-L-7B outperforms WebGPT-175B in terms of three commonly used metrics (i.e., coherence, helpfulness, and factuality), while the number of parameters is much smaller (only 1/24 of that of WebGPT-175B). Our datasets and models are made publicly available for better reproducibility.https://huggingface.co/forag łabelfootnote_dataset_url|检索增强生成技术(RAG)由于能够利用搜索引擎提高长形式问答(LFQA)的质量，在问答(QA)任务中得到了广泛的应用。尽管出现了各种开源方法和网络增强的商业系统，如必应聊天，两个关键问题仍然没有得到解决，即缺乏事实性和清晰的逻辑生成的长形式的答案。本文通过对网络增强型 LFQA 中问题生成的系统研究，解决了这些问题。具体来说，我们首先提出了一种新的轮廓增强生成器，以实现清晰的逻辑生成多方面的答案，并相应地构造两个数据集。然后提出了一种基于精心设计的双细粒度 RLHF 框架的事实优化方法，该框架包含了不同粒度级别的自动评价和奖励建模。我们的通用框架包括传统的细粒度 RLHF 方法作为特殊情况。大量的实验验证了我们提出的基于事实优化的 RAG (FoRAG)方法在中英文基准测试中的优越性。特别是，当将我们的方法应用于 Llama2-7B-chat 时，衍生模型 FoRAG-L-7B 在三个常用指标(即一致性，有益性和真实性)方面优于 WebGPT-175B，而参数的数量要小得多(只有 WebGPT-175B 的1/24)。我们的数据集和模型公开发布，以便更好地重现。 https://huggingface.co/forag abelfoonote _ datset _ url|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FoRAG:+Factuality-optimized+Retrieval+Augmented+Generation+for+Web-enhanced+Long-form+Question+Answering)|0|
|[A Hierarchical Context Augmentation Method to Improve Retrieval-Augmented LLMs on Scientific Papers](https://doi.org/10.1145/3637528.3671847)|TianYi Che, XianLing Mao, Tian Lan, Heyan Huang|Beijing Institute of Technology, Beijing, China|Scientific papers of a large scale on the Internet encompass a wealth of data and knowledge, attracting the attention of numerous researchers. To fully utilize these knowledge, Retrieval-Augmented Large Language Models (LLMs) usually leverage large-scale scientific corpus to train and then retrieve relevant passages from external memory to improve generation, which have demonstrated outstanding performance. However, existing methods can only capture one-dimension fragmented textual information without incorporating hierarchical structural knowledge, eg. the deduction relationship of abstract and main body, which makes it difficult to grasp the central thought of papers. To tackle this problem, we propose a hierarchical context augmentation method, which helps Retrieval-Augmented LLMs to autoregressively learn the structure knowledge of scientific papers. Specifically, we utilize the document tree to represent the hierarchical relationship of a paper and enhance the structure information of scientific context from three aspects: scale, format and global information. First, we think each top-bottom path of document tree is a logical independent context, which can be used to largely increase the scale of extracted structural corpus. Second, we propose a novel label-based format to represent the structure of context in textual sequences, unified between training and inference. Third, we introduce the global information of retrieved passages to further enhance the structure of context. Extensive experiments on three scientific tasks show that the proposed method significantly improves the performance of Retrieval-Augmented LLMs on all tasks. Besides, our method achieves start-of-art performance in Question Answer task and outperforms ChatGPT. Moreover, it also brings considerate gains with irrelevant retrieval passages, illustrating its effectiveness on practical application scenarios.|互联网上的大量科学论文包含了丰富的数据和知识，吸引了众多研究者的关注。为了充分利用这些知识，检索增强型大语言模型通常利用大规模的科学语料库来训练和检索外部记忆中的相关段落，以提高生成能力。然而，现有的方法只能捕捉一维零碎的文本信息，没有结合层次结构知识，如抽象与主体的演绎关系，难以把握论文的中心思想。针对这一问题，提出了一种层次上下文增强方法，该方法可以帮助检索增强 LLM 自回归地学习科技论文的结构知识。具体来说，我们利用文档树来表示论文的层次关系，并从规模、格式和全局信息三个方面增强科学语境的结构信息。首先，我们认为文档树的每一条顶底路径都是一个逻辑独立的上下文，可以用来大幅度增加提取结构化语料的规模。其次，我们提出了一种新的基于标签的格式来表示文本序列中的上下文结构，统一于训练和推理。第三，引入检索段落的全局信息，进一步增强语境结构。在三个科学任务上的大量实验表明，该方法可以显著提高检索增强 LLM 在所有任务上的性能。此外，该方法在问答任务中取得了较好的启动性能，优于 ChatGPT。此外，它也带来了相当的收益与不相关的检索段落，说明其在实际应用场景的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Hierarchical+Context+Augmentation+Method+to+Improve+Retrieval-Augmented+LLMs+on+Scientific+Papers)|0|
|[Maximum-Entropy Regularized Decision Transformer with Reward Relabelling for Dynamic Recommendation](https://doi.org/10.1145/3637528.3671750)|Xiaocong Chen, Siyu Wang, Lina Yao|The University of New South Wales, Sydney, Australia; Data 61, CSIRO & The University of New South Wales, Eveleigh, Australia; Data 61, CSIRO, Eveleigh, Australia|Reinforcement learning-based recommender systems have recently gainedpopularity. However, due to the typical limitations of simulation environments(e.g., data inefficiency), most of the work cannot be broadly applied in alldomains. To counter these challenges, recent advancements have leveragedoffline reinforcement learning methods, notable for their data-driven approachutilizing offline datasets. A prominent example of this is the DecisionTransformer. Despite its popularity, the Decision Transformer approach hasinherent drawbacks, particularly evident in recommendation methods based on it.This paper identifies two key shortcomings in existing DecisionTransformer-based methods: a lack of stitching capability and limitedeffectiveness in online adoption. In response, we introduce a novel methodologynamed Max-Entropy enhanced Decision Transformer with Reward Relabeling forOffline RLRS (EDT4Rec). Our approach begins with a max entropy perspective,leading to the development of a max entropy enhanced exploration strategy. Thisstrategy is designed to facilitate more effective exploration in onlineenvironments. Additionally, to augment the model's capability to stitchsub-optimal trajectories, we incorporate a unique reward relabeling technique.To validate the effectiveness and superiority of EDT4Rec, we have conductedcomprehensive experiments across six real-world offline datasets and in anonline simulator.|基于强化学习的推荐系统最近变得流行起来。然而，由于模拟环境的典型局限性(例如，数据效率低下) ，大多数工作不能广泛应用于所有领域。为了应对这些挑战，最近的进步已经利用了离线强化学习方法，值得注意的是它们利用离线数据集的数据驱动方法。这方面的一个突出例子是决策转换器。尽管决策转换器方法很流行，但是它有固有的缺点，在基于它的推荐方法中尤其明显。本文指出了现有基于决策变压器的方法存在的两个关键缺陷: 缺乏拼接能力和在线采用的有效性有限。作为回应，我们介绍了一种新的方法，最大熵增强决策变压器与奖励重新标记离线 RLRS (EDT4Rec)。我们的方法从最大熵的角度出发，导致了最大熵增强勘探策略的发展。该策略旨在促进在线环境中更有效的探索。此外，为了增强模型缝合次优轨迹的能力，我们纳入了独特的奖励重新标记技术。为了验证 EDT4Rec 的有效性和优越性，我们在六个真实世界的离线数据集和在线模拟器中进行了全面的实验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Maximum-Entropy+Regularized+Decision+Transformer+with+Reward+Relabelling+for+Dynamic+Recommendation)|0|
|[Retrieval-Augmented Hypergraph for Multimodal Social Media Popularity Prediction](https://doi.org/10.1145/3637528.3672041)|Zhangtao Cheng, Jienan Zhang, Xovee Xu, Goce Trajcevski, Ting Zhong, Fan Zhou|; University of Electronic Science and Technology of China, Chengdu, Sichuan, China; Iowa State University, Ames, Iowa, USA|Accurately predicting the popularity of multimodal user-generated content (UGC) is fundamental for many real-world applications such as online advertising and recommendation. Existing approaches generally focus on limited contextual information within individual UGCs, yet overlook the potential benefit of exploiting meaningful knowledge in relevant UGCs. In this work, we propose RAGTrans, an aspect-aware retrieval-augmented multi-modal hypergraph transformer that retrieves pertinent knowledge from a multi-modal memory bank and enhances UGC representations via neighborhood knowledge aggregation on multi-model hypergraphs. In particular, we initially retrieve relevant multimedia instances from a large corpus of UGCs via the aspect information and construct a knowledge-enhanced hypergraph based on retrieved relevant instances. This allows capturing meaningful contextual information across the data. We then design a novel bootstrapping hypergraph transformer on multimodal hypergraphs to strengthen UGC representations across modalities via customizing a propagation algorithm to effectively diffuse information across nodes and edges. Additionally, we propose a user-aware attention-based fusion module to comprise the enriched UGC representations for popularity prediction. Extensive experiments on real-world social media datasets demonstrate that RAGTrans outperforms state-of-the-art popularity prediction models across settings.|准确预测多模式用户生成内容的流行程度对于许多现实世界的应用程序(如在线广告和推荐)至关重要。现有的方法一般集中于个别教资会内有限的上下文资料，但却忽略了利用相关教资会内有意义的知识的潜在好处。在这项工作中，我们提出了 RAGTrans，一个方面感知检索增强型多模态超图转换器，它可以从多模态记忆库中检索相关知识，并通过多模态超图上的邻域知识聚合来增强 UGC 表示。特别地，我们首先通过方面信息从一个大型的用户教学资源库中检索相关的多媒体实例，然后在检索到的相关实例的基础上构造一个知识增强的超图。这允许跨数据捕获有意义的上下文信息。然后在多模态超图上设计了一种新的自举超图转换器，通过定制传播算法有效地在节点和边上传播信息，从而增强跨模态的 UGC 表示。此外，我们还提出了一个基于用户感知的注意力融合模块来构成用户用户生成表示，用于流行度预测。在真实世界的社交媒体数据集上进行的大量实验表明，RAGTrans 在不同设置下的流行程度预测模型优于最先进的模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Retrieval-Augmented+Hypergraph+for+Multimodal+Social+Media+Popularity+Prediction)|0|
|[ROTAN: A Rotation-based Temporal Attention Network for Time-Specific Next POI Recommendation](https://doi.org/10.1145/3637528.3671809)|Shanshan Feng, Feiyu Meng, Lisi Chen, Shuo Shang, Yew Soon Ong|; Centre for Frontier AI Research, ASTAR & Nanyang Technological University, Singapore, Singapore; University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, Sichuan, China|The next Point-of-interest recommendation has attracted extensive research interest recently, which predicts users' subsequent movements. The main challenge is how to effectively capture users' personalized sequential transitions in check-in trajectory, and various methods have been developed. However, most existing studies ignore the temporal information when conducting the next POI recommendation. To fill this gap, we investigate a time-specific next POI recommendation task, which additionally incorporates the target time information. We propose a brand new Time2Rotation technique to capture the temporal information. Different from conventional methods, we represent timeslots as rotation vectors and then perform the rotation operations. Based on the Time2Rotation technique, we propose a novel rotation-based temporal attention network, namely ROTAN, for the time-specific next POI recommendation task. The ROTAN begins by building a collaborative POI transition graph, capturing the asymmetric temporal influence in sequential transitions. After that, it incorporates temporal information into the modeling of individual check-in trajectories, extracting separate representations for user preference and POI influence to reflect their distinct temporal patterns. Lastly, the target time is integrated to generate recommendations. Extensive experiments are conducted on three real-world datasets, which demonstrates the advantages of the proposed Time2Rotation technique and ROTAN recommendation model.|最近，下一个兴趣点推荐引起了广泛的研究兴趣，它可以预测用户随后的活动。如何有效地捕获用户在签入轨迹中的个性化顺序转换是其面临的主要挑战，各种方法已经被开发出来。然而，大多数现有的研究忽略了时间信息进行下一个 POI 建议。为了填补这个空白，我们研究了一个特定于时间的下一个 POI 推荐任务，它还包含了目标时间信息。我们提出了一种全新的 Time2旋转技术来捕获时间信息。与传统的方法不同，我们将时隙表示为旋转向量，然后进行旋转运算。基于 Time2旋转技术，我们提出了一个新的基于旋转的时间注意网络，即 ROTAN，用于特定时间的下一个 POI 推荐任务。ROTAN 从建立一个合作的 POI 转换图开始，捕捉连续转换中不对称的时间影响。然后，将时间信息融入到单个签入轨迹的建模中，提取用户偏好和 POI 影响的独立表示，以反映它们不同的时间模式。最后，整合目标时间来生成建议。在三个真实世界的数据集上进行了大量的实验，这些实验证明了提出的 Time2旋转技术和 ROTAN 推荐模型的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ROTAN:+A+Rotation-based+Temporal+Attention+Network+for+Time-Specific+Next+POI+Recommendation)|0|
|[AutoXPCR: Automated Multi-Objective Model Selection for Time Series Forecasting](https://doi.org/10.1145/3637528.3672057)|Raphael Fischer, Amal Saadallah||Automated machine learning (AutoML) streamlines the creation of ML models, but few specialized methods have approached the challenging domain of time series forecasting. Deep neural networks (DNNs) often deliver state-of-the-art predictive performance for forecasting data, however these models are also criticized for being computationally intensive black boxes. As a result, when searching for the "best" model, it is crucial to also acknowledge other aspects, such as interpretability and resource consumption. In this paper, we propose AutoXPCR - a novel method that produces DNNs for forecasting under consideration of multiple objectives in an automated and explainable fashion. Our approach leverages meta-learning to estimate any model's performance along PCR criteria, which encompass (P)redictive error, (C)omplexity, and (R)esource demand. Explainability is addressed on multiple levels, as AutoXPCR pro-vides by-product explanations of recommendations and allows to interactively control the desired PCR criteria importance and trade-offs. We demonstrate the practical feasibility AutoXPCR across 108 forecasting data sets from various domains. Notably, our method outperforms competing AutoML approaches - on average, it only requires 20% of computation costs for recommending highly efficient models with 85% of the empirical best quality.|自动机器学习(AutoML)简化了机器学习模型的创建，但很少有专门的方法能够接近时间序列预测这一具有挑战性的领域。深度神经网络(DNN)往往提供最先进的预测性能的预测数据，但这些模型也被批评为计算密集型黑盒。因此，在寻找“最佳”模型时，关键是还要承认其他方面，如可解释性和资源消耗。在本文中，我们提出 AutoXPCR-一种新的方法，产生 DNN 的预测考虑多个目标，在一个自动化和可解释的方式。我们的方法利用元学习来估计任何模型沿 PCR 标准的性能，其中包括(P)预测误差，(C)复杂性和(R)资源需求。可解释性是在多个层面上解决的，因为 AutoXPCR 提供建议的副产品解释，并允许交互式地控制所需的 PCR 标准的重要性和权衡。我们论证了 AutoXPCR 在108个不同领域的预测数据集中的实际可行性。值得注意的是，我们的方法优于竞争对手 AutoML 方法-平均而言，它只需要20% 的计算成本推荐高效率的模型与85% 的经验最佳质量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AutoXPCR:+Automated+Multi-Objective+Model+Selection+for+Time+Series+Forecasting)|0|
|[Topology-Driven Multi-View Clustering via Tensorial Refined Sigmoid Rank Minimization](https://doi.org/10.1145/3637528.3672070)|Zhibin Gu, Zhendong Li, Songhe Feng||Benefiting from the effective exploitation of the high-order correlations across multiple views, tensor-based multi-view clustering (TMVC) has garnered considerable attention in recent years. Nevertheless, prior TMVC techniques commonly involve assembling multiple view-specific spatial similarity graphs into a three-dimensional tensor, overlooking the intrinsic topological structure essential for precise clustering of data within a manifold. Additionally, mainstream techniques are constrained by equally shrinking all singular values to recover a low-rank tensor, limiting their capacity to distinguish significant variations among different singular values. In this investigation, we present an innovative TMVC framework termed toPology-driven multi-view clustering viA refined teNsorial sigmoiD rAnk minimization (PANDA ). Specifically, PANDA extracts view-specific topological structures from Euclidean graphs and intricately integrates them into a low-rank three-dimensional tensor, facilitating the concurrent utilization of intra-view topological connectivity and inter-view high-order correlations. Moreover, we develop a refined sigmoid function as the tighter surrogate to tensor rank, enabling the exploration of significant information of heterogeneous singular values. Meanwhile, the topological structures are merged into a unified structure with varying weights, associated with a connectivity constraint, empowering the significant divergence among views and the explicit cluster structure of the target graph are simultaneously leveraged. Extensive experiments demonstrate the superiority of PANDA, outperforming SOTA methods.|基于张量的多视图聚类算法(TMVC)得益于多视图高阶相关性的有效利用，近年来引起了人们的广泛关注。然而，先前的 TMVC 技术通常涉及将多个视图特定的空间相似性图组装成一个三维张量，忽略了流形中数据精确聚类所必需的内在拓扑结构。此外，主流技术受到同样缩小所有奇异值以恢复低秩张量的限制，从而限制了它们区分不同奇异值之间显著变化的能力。在这项研究中，我们提出了一个创新的 TMVC 框架，称为拓扑驱动的多视图聚类通过一个精化的 teNsorial sigmoiD rRank 最小化(PANDA)。具体而言，PANDA 从欧几里德图中提取视图特定的拓扑结构，并将其复杂地集成到一个低秩三维张量中，促进视图内拓扑连通性和视图间高阶相关性的并发利用。此外，我们发展了一个精确的 S形函数，作为张量级更紧密的替代品，使探索异质奇异值的重要信息成为可能。同时，将拓扑结构合并为一个具有不同权重的统一结构，并结合连通性约束，赋予视图之间的显著差异，同时利用目标图的显式聚类结构。大量的实验证明了 PANDA 方法的优越性，优于 SOTA 方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Topology-Driven+Multi-View+Clustering+via+Tensorial+Refined+Sigmoid+Rank+Minimization)|0|
|[Ranking with Slot Constraints](https://doi.org/10.1145/3637528.3672000)|Wentao Guo, Andrew Wang, Bradon Thymes, Thorsten Joachims||We introduce the problem of ranking with slot constraints, which can be used to model a wide range of application problems -- from college admission with limited slots for different majors, to composing a stratified cohort of eligible participants in a medical trial. We show that the conventional Probability Ranking Principle (PRP) can be highly sub-optimal for slot-constrained ranking problems, and we devise a new ranking algorithm, called MatchRank. The goal of MatchRank is to produce rankings that maximize the number of filled slots if candidates are evaluated by a human decision maker in the order of the ranking. In this way, MatchRank generalizes the PRP, and it subsumes the PRP as a special case when there are no slot constraints. Our theoretical analysis shows that MatchRank has a strong approximation guarantee without any independence assumptions between slots or candidates. Furthermore, we show how MatchRank can be implemented efficiently. Beyond the theoretical guarantees, empirical evaluations show that MatchRank can provide substantial improvements over a range of synthetic and real-world tasks.|我们介绍了有时间限制的排名问题，这个问题可以用来模拟范围广泛的申请问题——从不同专业有时间限制的大学录取，到在医学试验中合格参与者的分层队列组成。我们证明了传统的概率排序原则(PRP)对于时隙约束排序问题可能是高度次优的，并且我们设计了一种新的排序算法，称为 MatchRank。MatchRank 的目标是，如果候选人是由人类决策者按照排名顺序进行评估的，那么它将产生最大化已填补空缺数量的排名。通过这种方式，MatchRank 对 PRP 进行泛化，当没有时隙约束时，它将 PRP 包含为特殊情况。我们的理论分析表明，MatchRank 有一个强大的近似保证，没有任何独立的假设之间的插槽或候选人。此外，我们还展示了如何有效地实现 MatchRank。除了理论上的保证，经验性的评估表明，MatchRank 可以在一系列合成和现实世界的任务上提供实质性的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ranking+with+Slot+Constraints)|0|
|[Consistency and Discrepancy-Based Contrastive Tripartite Graph Learning for Recommendations](https://doi.org/10.1145/3637528.3672056)|Linxin Guo, Yaochen Zhu, Min Gao, Yinghui Tao, Junliang Yu, Chen Chen|Chongqing University, Chongqing, China; University of Virginia, Charlottesville, VA, USA; Institute of Guizhou Aerospace Measuring and Testing Technology, Guiyang, China; the University of Queensland, Queensland, Australia|Tripartite graph-based recommender systems markedly diverge from traditional models by recommending unique combinations such as user groups and item bundles. Despite their effectiveness, these systems exacerbate the long-standing cold-start problem in traditional recommender systems, because any number of user groups or item bundles can be formed among users or items. To address this issue, we introduce a Consistency and Discrepancy-based graph contrastive learning method for tripartite graph-based Recommendation (CDR). This approach leverages two novel meta-path-based metrics-consistency and discrepancy-to capture nuanced, implicit associations between the recommended objects and the recommendees. These metrics, indicative of high-order similarities, can be efficiently calculated with infinite graph convolutional networks (GCN) layers under a multi-objective optimization framework, using the limit theory of GCN. Additionally, we introduce a novel Contrastive Divergence (CD) loss, which can seamlessly integrate the consistency and discrepancy metrics into the contrastive objective as the positive and contrastive supervision signals to learn node representations, enhancing the pairwise ranking of recommended objects and proving particularly valuable in severe cold-start scenarios. Extensive experiments demonstrate the effectiveness of the proposed CDR. The code is released at https://github.com/foodfaust/CDR.|基于三方图表的推荐系统通过推荐独特的组合，如用户组和项目包，明显地区别于传统模型。尽管这些系统很有效，但它们加剧了传统推荐系统中长期存在的“冷启动”问题，因为在用户或项目之间可以形成任意数量的用户组或项目包。为了解决这个问题，我们提出了一种基于一致性和差异性的图形对比学习方法。这种方法利用两种新颖的基于元路径的度量——一致性和差异性——来捕获被推荐对象和被推荐对象之间微妙的、隐含的关联。在多目标优化框架下，利用无穷图卷积网络(GCN)的极限理论，可以有效地计算这些指标，它们表示高阶相似性。此外，我们引入了一种新的对比发散(CD)损失，它可以无缝地将一致性和差异度量集成到对比目标中，作为正向和对比监督信号来学习节点表示，提高推荐对象的成对排序，并证明在严重的冷启动情况下特别有价值。大量的实验证明了所提出的 CDR 算法的有效性。密码在 https://github.com/foodfaust/cdr 发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Consistency+and+Discrepancy-Based+Contrastive+Tripartite+Graph+Learning+for+Recommendations)|0|
|[Binder: Hierarchical Concept Representation through Order Embedding of Binary Vectors](https://doi.org/10.1145/3637528.3671793)|Croix Gyurek, Niloy Talukder, Mohammad Al Hasan|Indiana University at Indianapolis, Indianapolis, IN, USA; University of Waterloo, Waterloo, Ontario, Canada|For natural language understanding and generation, embedding concepts using an order-based representation is an essential task. Unlike traditional point vector based representation, an order-based representation imposes geometric constraints on the representation vectors for explicitly capturing various semantic relationships that may exist between a pair of concepts. In existing literature, several approaches on order-based embedding have been proposed, mostly focusing on capturing hierarchical relationships; examples include vectors in Euclidean space, complex, Hyperbolic, order, and Box Embedding. Box embedding creates region-based rich representation of concepts, but along the process it sacrifices simplicity, requiring a custom-made optimization scheme for learning the representation. Hyperbolic embedding improves embedding quality by exploiting the ever-expanding property of Hyperbolic space, but it also suffers from the same fate as box embedding as gradient descent like optimization is not simple in the Hyperbolic space. In this work, we propose Binder, a novel approach for order-based representation. Binder uses binary vectors for embedding, so the embedding vectors are compact with an order of magnitude smaller footprint than other methods. Binder uses a simple and efficient optimization scheme for learning representation vectors with a linear time complexity. Our comprehensive experimental results show that Binder is very accurate, yielding competitive results on the representation task. But Binder stands out from its competitors on the transitive closure link prediction task as it can learn concept embeddings just from the direct edges, whereas all existing order-based approaches rely on the indirect edges. In particular, Binder achieves a whopping 70% higher F1-score than the second best method (98.6% vs 29%) in our largest dataset, WordNet Nouns (743,241 edges), when using only direct edges during training.|对于自然语言的理解和生成，使用基于顺序的表示来嵌入概念是一个基本的任务。与传统的基于点向量的表示不同，基于顺序的表示对表示向量施加几何约束，以显式地捕获可能存在于一对概念之间的各种语义关系。在现有的文献中，已经提出了几种基于顺序的嵌入方法，主要集中在捕获层次关系，例如欧氏空间中的向量，复数，双曲，顺序和盒嵌入。框嵌入创建了基于区域的概念丰富表示，但是在这个过程中它牺牲了简单性，需要一个定制的优化方案来学习表示。双曲嵌入通过利用双曲空间不断扩展的特性来提高嵌入质量，但是它也遭受着与盒子嵌入相同的命运，因为像优化这样的梯度下降法在双曲空间中并不简单。在这项工作中，我们提出了一种新的基于顺序的表示方法 Binder。Binder 使用二进制向量进行嵌入，因此嵌入向量比其他方法更紧凑，数量级更小。Binder 使用一种简单有效的优化方法来学习具有线性时间复杂度的表示向量。我们的综合实验结果表明，粘合剂是非常准确的，产生了竞争结果的表示任务。但是 Binder 在传递闭包链接预测任务中脱颖而出，因为它只能从直接边缘学习概念嵌入，而现有的基于顺序的方法都依赖于间接边缘。特别是，在我们最大的数据集 WordNet 名词(743,241条边)中，当仅在训练期间使用直接边时，Binder 比第二最佳方法(98.6% 比29%)高出70% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Binder:+Hierarchical+Concept+Representation+through+Order+Embedding+of+Binary+Vectors)|0|
|[An Efficient Local Search Algorithm for Large GD Advertising Inventory Allocation with Multilinear Constraints](https://doi.org/10.1145/3637528.3671811)|Xiang He, Wuyang Mao, Zhenghang Xu, Yuanzhe Gu, Yundu Huang, Zhonglin Zu, Liang Wang, Mengyu Zhao, Mengchuan Zou|; Alibaba Group, Beijing, China; Alibaba Group, Hangzhou, China|The Guaranteed Delivery (GD) advertising is a crucial component of the online advertising industry, and the allocation of inventory in GD advertising is an important procedure that influences directly the ability of the publisher to fulfill the requirements and increase its revenues. Nowadays, as the requirements of advertisers become more and more diverse and fine-grained, the focus ratio requirement, which states that the portion of allocated impressions of a designated contract on focus media among all possible media should be greater than another contract, often appears in business scenarios. However, taking these requirements into account brings hardness for the GD advertising inventory allocation as the focus ratio requirements involve non-convex multilinear constraints. Existing methods which rely on the convex properties are not suitable for processing this problem, while mathematical programming or constraint-based heuristic solvers are unable to produce high-quality solutions within the time limit. Therefore, we propose a local search framework to address this challenge. It incorporates four new operators designed for handling multilinear constraints and a two-mode algorithmic architecture. Experimental results demonstrate that our algorithm is able to compute high-quality allocations with better business metrics compared to the state-of-the-art mathematical programming or constraint based heuristic solvers. Moreover, our algorithm is able to handle the general multilinear constraints and we hope it could be used to solve other problems in GD advertising with similar requirements.|保送广告是网络广告业的重要组成部分，而保送广告的库存分配是直接影响发行商满足需求和增加收入的重要环节。如今，随着广告商的要求变得越来越多样化和细化，焦点比率要求往往出现在商业场景中。然而，考虑到这些要求，GD 广告库存分配的困难，因为焦点比率的要求涉及非凸多线性约束。现有的依赖于凸性质的方法不适合处理这个问题，而数学规划或基于约束的启发式求解器不能在时间限制内产生高质量的解。因此，我们提出了一个本地搜索框架来应对这一挑战。它包括四个新的运算符设计处理多线性约束和一个双模算法体系结构。实验结果表明，与最先进的数学规划或基于约束的启发式求解器相比，该算法能够以更好的业务指标计算高质量的分配。此外，我们的算法能够处理一般的多线性约束，我们希望它可以用来解决其他问题广东广告类似的要求。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Efficient+Local+Search+Algorithm+for+Large+GD+Advertising+Inventory+Allocation+with+Multilinear+Constraints)|0|
|[Double Correction Framework for Denoising Recommendation](https://doi.org/10.1145/3637528.3671692)|Zhuangzhuang He, Yifan Wang, Yonghui Yang, Peijie Sun, Le Wu, Haoyue Bai, Jinqi Gong, Richang Hong, Min Zhang|; University of Macau, Macau, China; Tsinghua University, Beijing, China; Hefei University of Technology, Hefei, China|As its availability and generality in online services, implicit feedback ismore commonly used in recommender systems. However, implicit feedback usuallypresents noisy samples in real-world recommendation scenarios (such asmisclicks or non-preferential behaviors), which will affect precise userpreference learning. To overcome the noisy samples problem, a popular solutionis based on dropping noisy samples in the model training phase, which followsthe observation that noisy samples have higher training losses than cleansamples. Despite the effectiveness, we argue that this solution still haslimits. (1) High training losses can result from model optimization instabilityor hard samples, not just noisy samples. (2) Completely dropping of noisysamples will aggravate the data sparsity, which lacks full data exploitation.To tackle the above limitations, we propose a Double Correction Framework forDenoising Recommendation (DCF), which contains two correction components fromviews of more precise sample dropping and avoiding more sparse data. In thesample dropping correction component, we use the loss value of the samples overtime to determine whether it is noise or not, increasing dropping stability.Instead of averaging directly, we use the damping function to reduce the biaseffect of outliers. Furthermore, due to the higher variance exhibited by hardsamples, we derive a lower bound for the loss through concentration inequalityto identify and reuse hard samples. In progressive label correction, weiteratively re-label highly deterministic noisy samples and retrain them tofurther improve performance. Finally, extensive experimental results on threedatasets and four backbones demonstrate the effectiveness and generalization ofour proposed framework.|由于在线服务的可用性和普遍性，隐式反馈在推荐系统中得到了广泛的应用。然而，在现实推荐场景中，隐式反馈通常会产生噪声样本(如错误点击或非优先行为) ，从而影响精确的用户偏好学习。为了克服噪声样本问题，提出了一种在模型训练阶段丢弃噪声样本的方法，该方法通过观察噪声样本比清洗样本具有更高的训练损失。尽管有效，我们认为这个解决方案仍然有局限性。(1)模型优化的不稳定性或硬样本会导致高训练损失，而不仅仅是噪声样本。(2)完全丢弃噪声样本会加剧数据稀疏性，而数据稀疏性缺乏充分的数据利用。针对上述限制，我们建议采用双修正框架去噪建议(DCF) ，其中包含两个修正组成部分，从更精确的丢弃样本和避免更稀疏的数据的角度出发。在样本掉落校正分量中，我们利用样本的损失值来判断样本是否为噪声，增加了掉落的稳定性。我们使用阻尼函数来减少异常值的偏差效应，而不是直接求平均值。此外，由于硬样本具有较高的方差，我们通过浓度不等式得到了一个损失的下界来识别和重用硬样本。在渐进式标记校正中，反复重新标记高确定性噪声样本，并对它们进行再训练以进一步提高性能。最后，在三个数据集和四个骨干上的大量实验结果证明了该框架的有效性和通用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Double+Correction+Framework+for+Denoising+Recommendation)|0|
|[Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models](https://doi.org/10.1145/3637528.3671932)|Zhibo Hu, Chen Wang, Yanfeng Shu, HyeYoung Paik, Liming Zhu|CSIRO Data61, Hobart, Tasmania, Australia; University of New South Wales, Sydney, NSW, Australia; The University of New South Wales & CSIRO Data61, Sydney, NSW, Australia; CSIRO Data61 & The University of New South Wales, Sydney, NSW, Australia; CSIRO Data61 & University of New South Wales, Sydney, NSW, Australia|The robustness of large language models (LLMs) becomes increasingly importantas their use rapidly grows in a wide range of domains. Retrieval-AugmentedGeneration (RAG) is considered as a means to improve the trustworthiness oftext generation from LLMs. However, how the outputs from RAG-based LLMs areaffected by slightly different inputs is not well studied. In this work, wefind that the insertion of even a short prefix to the prompt leads to thegeneration of outputs far away from factually correct answers. Wesystematically evaluate the effect of such prefixes on RAG by introducing anovel optimization technique called Gradient Guided Prompt Perturbation (GGPP).GGPP achieves a high success rate in steering outputs of RAG-based LLMs totargeted wrong answers. It can also cope with instructions in the promptsrequesting to ignore irrelevant context. We also exploit LLMs' neuronactivation difference between prompts with and without GGPP perturbations togive a method that improves the robustness of RAG-based LLMs through a highlyeffective detector trained on neuron activation triggered by GGPP generatedprompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness ofour methods.|随着大型语言模型在广泛领域的应用迅速增长，其健壮性变得越来越重要。检索-增强生成(RAG)被认为是提高 LLM 文本生成可信度的一种手段。然而，基于 RAG 的 LLM 的输出如何受到稍微不同的输入的影响还没有得到很好的研究。在这项工作中，我们发现，即使在提示符中插入一个短的前缀，也会导致输出远离事实上正确的答案。通过引入梯度引导提示扰动(GGPP)这一新的优化技术，系统地评价了这些前缀对 RAG 的影响。GGPP 在引导基于 RAG 的 LLM 的输出定位错误答案方面取得了很高的成功率。它还可以处理提示中要求忽略不相关上下文的指令。我们还利用具有和不具有 GGPP 扰动的提示之间的 LLM 的神经元激活差异，通过对由 GGPP 产生的提示触发的神经元激活进行训练的高效检测器来提高基于 RAG 的 LLM 的鲁棒性。我们对开源 LLM 的评估证明了我们方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Prompt+Perturbation+in+Retrieval-Augmented+Generation+based+Large+Language+Models)|0|
|[Dynamic Neural Dowker Network: Approximating Persistent Homology in Dynamic Directed Graphs](https://doi.org/10.1145/3637528.3671980)|Hao Li, Hao Jiang, Jiajun Fan, Dongsheng Ye, Liang Du|Electronic Information School, Wuhan University, Wuhan, Hubei, China|Persistent homology, a fundamental technique within Topological Data Analysis (TDA), captures structural and shape characteristics of graphs, yet encounters computational difficulties when applied to dynamic directed graphs. This paper introduces the Dynamic Neural Dowker Network (DNDN), a novel framework specifically designed to approximate the results of dynamic Dowker filtration, aiming to capture the high-order topological features of dynamic directed graphs. Our approach creatively uses line graph transformations to produce both source and sink line graphs, highlighting the shared neighbor structures that Dowker complexes focus on. The DNDN incorporates a Source-Sink Line Graph Neural Network (SSLGNN) layer to effectively capture the neighborhood relationships among dynamic edges. Additionally, we introduce an innovative duality edge fusion mechanism, ensuring that the results for both the sink and source line graphs adhere to the duality principle intrinsic to Dowker complexes. Our approach is validated through comprehensive experiments on real-world datasets, demonstrating DNDN's capability not only to effectively approximate dynamic Dowker filtration results but also to perform exceptionally in dynamic graph classification tasks.|持久同调是拓扑数据分析(TDA)中的一项基本技术，它捕获图的结构和形状特征，但在应用于动态有向图时遇到了计算困难。本文介绍了动态神经道克网络(DNDN) ，这是一种专门设计来逼近动态道克滤波结果的新框架，旨在捕获动态有向图的高阶拓扑特征。我们的方法创造性地使用线图转换来生成源和汇线图，突出道克复合体关注的共享邻居结构。DNDN 结合源-汇线图神经网络(SSLGNN)层，有效地捕获动态边之间的邻域关系。此外，我们还引入了一种创新的对偶边融合机制，确保汇和源线图的结果都坚持道克复合体固有的对偶原则。通过对实际数据集的全面实验验证了该方法的有效性，证明了 DNDN 不仅能有效逼近动态 Dowker 滤波结果，而且能在动态图形分类任务中表现优异。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamic+Neural+Dowker+Network:+Approximating+Persistent+Homology+in+Dynamic+Directed+Graphs)|0|
|[RecExplainer: Aligning Large Language Models for Explaining Recommendation Models](https://doi.org/10.1145/3637528.3671802)|Yuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, Xing Xie|University of Science and Technology of China, Hefei, China; Microsoft Research Asia, Beijing, China|Recommender systems are widely used in online services, with embedding-based models being particularly popular due to their expressiveness in representing complex signals. However, these models often function as a black box, making them less transparent and reliable for both users and developers. Recently, large language models (LLMs) have demonstrated remarkable intelligence in understanding, reasoning, and instruction following. This paper presents the initial exploration of using LLMs as surrogate models to explaining black-box recommender models. The primary concept involves training LLMs to comprehend and emulate the behavior of target recommender models. By leveraging LLMs' own extensive world knowledge and multi-step reasoning abilities, these aligned LLMs can serve as advanced surrogates, capable of reasoning about observations. Moreover, employing natural language as an interface allows for the creation of customizable explanations that can be adapted to individual user preferences. To facilitate an effective alignment, we introduce three methods: behavior alignment, intention alignment, and hybrid alignment. Behavior alignment operates in the language space, representing user preferences and item information as text to mimic the target model's behavior; intention alignment works in the latent space of the recommendation model, using user and item representations to understand the model's behavior; hybrid alignment combines both language and latent spaces. Comprehensive experiments conducted on three public datasets show that our approach yields promising results in understanding and mimicking target models, producing high-quality, high-fidelity, and distinct explanations. Our code is available at https://github.com/microsoft/RecAI.|推荐系统在在线服务中得到了广泛的应用，基于嵌入的推荐系统模型因其表达复杂信号的能力而受到人们的青睐。然而，这些模型通常起到黑匣子的作用，使它们对用户和开发人员来说都不那么透明和可靠。近年来，大型语言模型(LLM)在理解、推理和指令跟随方面显示出非凡的智力。本文介绍了利用 LLM 作为代理模型来解释黑盒推荐模型的初步探索。主要概念包括训练 LLM 理解和仿真目标推荐模型的行为。通过利用 LLM 自身丰富的世界知识和多步推理能力，这些对齐的 LLM 可以作为高级代理，能够对观测进行推理。此外，使用自然语言作为界面允许创建可定制的解释，可以适应个人用户的喜好。为了实现有效的校准，我们介绍了三种方法: 行为校准、意图校准和混合校准。行为对齐操作在语言空间中，将用户偏好和项目信息表示为文本，以模仿目标模型的行为; 意图对齐工作在推荐模型的潜在空间中，使用用户和项目表示来理解模型的行为; 混合对齐结合了语言和潜在空间。在三个公共数据集上进行的综合实验表明，我们的方法在理解和模仿目标模型方面产生了有希望的结果，产生了高质量、高保真度和独特的解释。我们的代码可以在 https://github.com/microsoft/recai 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RecExplainer:+Aligning+Large+Language+Models+for+Explaining+Recommendation+Models)|0|
|[Customizing Graph Neural Network for CAD Assembly Recommendation](https://doi.org/10.1145/3637528.3671788)|Fengqi Liang, Huan Zhao, Yuhan Quan, Wei Fang, Chuan Shi|4Paradigm Inc., Beijing, China; Beijing University of Post and Telecommunication, Beijing, China|CAD assembly modeling, which refers to using CAD software to design new products from a catalog of existing machine components, is important in the industrial field. The graph neural network (GNN) based recommender system for CAD assembly modeling can help designers make decisions and speed up the design process by recommending the next required component based on the existing components in CAD software. These components can be represented as a graph naturally. However, present recommender systems for CAD assembly modeling adopt fixed GNN architectures, which may be sub-optimal for different manufacturers with different data distribution. Therefore, to customize a well-suited recommender system for different manufacturers, we propose a novel neural architecture search (NAS) framework, dubbed CusGNN, which can design data-specific GNN automatically. Specifically, we design a search space from three dimensions (i.e., aggregation, fusion, and readout functions), which contains a wide variety of GNN architectures. Then, we develop an effective differentiable search algorithm to search high-performing GNN from the search space. Experimental results show that the customized GNNs achieve 1.5-5.1% higher top-10 accuracy compared to previous manual designed methods, demonstrating the superiority of the proposed approach. Code and data are available at https://github.com/BUPT-GAMMA/CusGNN.|CAD 装配建模是指利用 CAD 软件从现有机械零部件目录中设计新产品，在工业领域具有重要意义。基于图形神经网络(GNN)的 CAD 装配建模推荐系统可以帮助设计人员根据 CAD 软件中现有的组件推荐下一个需要的组件，从而帮助设计人员做出决策并加快设计过程。这些组件可以自然地表示为一个图形。然而，现有的 CAD 装配建模推荐系统采用固定的 GNN 体系结构，对于具有不同数据分布的不同制造商可能是次优的。因此，为了为不同的制造商定制一个非常适合的推荐系统，我们提出了一个新的神经结构搜索(NAS)框架，称为 CusGNN，它可以自动设计数据特定的 GNN。具体来说，我们从三个维度(即聚合、融合和读出函数)设计一个搜索空间，其中包含多种 GNN 体系结构。然后，我们开发了一个有效的可微搜索算法来搜索高性能的 GNN 从搜索空间。实验结果表明，与以往的手工设计方法相比，自定义 GNN 的前10位精度提高了1.5 -5.1% ，证明了该方法的优越性。代码和数据可在 https://github.com/bupt-gamma/cusgnn 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Customizing+Graph+Neural+Network+for+CAD+Assembly+Recommendation)|0|
|[When Box Meets Graph Neural Network in Tag-aware Recommendation](https://doi.org/10.1145/3637528.3671973)|Fake Lin, Ziwei Zhao, Xi Zhu, Da Zhang, Shitian Shen, Xueying Li, Tong Xu, Suojuan Zhang, Enhong Chen|University of Science and Technology of China, Hefei, China; Alibaba Group, Hangzhou, NC, USA; Alibaba Group, Hangzhou, China; Army Engineering University of PLA, Nanjing, China|Last year has witnessed the re-flourishment of tag-aware recommender systems supported by the LLM-enriched tags. Unfortunately, though large efforts have been made, current solutions may fail to describe the diversity and uncertainty inherent in user preferences with only tag-driven profiles. Recently, with the development of geometry-based techniques, e.g., box embeddings, the diversity of user preferences now could be fully modeled as the range within a box in high dimension space. However, defect still exists as these approaches are incapable of capturing high-order neighbor signals, i.e., semantic-rich multi-hop relations within the user-tag-item tripartite graph, which severely limits the effectiveness of user modeling. To deal with this challenge, in this paper, we propose a novel framework, called BoxGNN, to perform message aggregation via combinations of logical operations, thereby incorporating high-order signals. Specifically, we first embed users, items, and tags as hyper-boxes rather than simple points in the representation space, and define two logical operations, i.e., union and intersection, to facilitate the subsequent process. Next, we perform the message aggregation mechanism via the combination of logical operations, to obtain the corresponding high-order box representations. Finally, we adopt a volume-based learning objective with Gumbel smoothing techniques to refine the representation of boxes. Extensive experiments on two publicly available datasets and one LLM-enhanced e-commerce dataset have validated the superiority of BoxGNN compared with various state-of-the-art baselines. The code is released online: https://github.com/critical88/BoxGNN.|去年见证了标签感知推荐系统的重新繁荣，这些系统由 LLM 丰富的标签支持。不幸的是，尽管已经做出了巨大的努力，目前的解决方案可能无法描述用户偏好中固有的多样性和不确定性，只能使用标签驱动的配置文件。近年来，随着几何技术的发展，如盒子嵌入，用户偏好的多样性现在可以完全模拟为高维空间中盒子内的范围。然而，由于这些方法不能捕获高阶相邻信号，即用户标签项三部分图中语义丰富的多跳关系，这严重限制了用户建模的有效性。为了应对这一挑战，在本文中，我们提出了一种新的框架，称为 BoxGNN，通过逻辑操作的组合来执行消息聚合，从而合并高阶信号。具体来说，我们首先将用户、项目和标记作为超盒而不是表示空间中的简单点嵌入，并定义两个逻辑操作，即联合和交集，以方便后续处理。接下来，我们通过逻辑操作的组合执行消息聚合机制，以获得相应的高阶框表示。最后，我们采用基于体积的学习目标和 Gumbel 平滑技术来改善盒子的表示。在两个公开可用的数据集和一个 LLM 增强的电子商务数据集上的大量实验验证了 BoxGNN 相对于各种最新基线的优越性。代码在网上发布:  https://github.com/critical88/boxgnn。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=When+Box+Meets+Graph+Neural+Network+in+Tag-aware+Recommendation)|0|
|[Fast Query of Biharmonic Distance in Networks](https://doi.org/10.1145/3637528.3671856)|Changan Liu, Ahad N. Zehmakan, Zhongzhi Zhang|Fudan University, Shanghai, China; Australian National University, Canberra, Australia|Thebiharmonic distance (BD) is a fundamental metric that measures the distance of two nodes in a graph. It has found applications in network coherence, machine learning, and computational graphics, among others. In spite of BD's importance, efficient algorithms for the exact computation or approximation of this metric on large graphs remain notably absent. In this work, we provide several algorithms to estimate BD, building on a novel formulation of this metric. These algorithms enjoy locality property (that is, they only read a small portion of the input graph) and at the same time possess provable performance guarantees. In particular, our main algorithms approximate the BD between any node pair with an arbitrarily small additive error ε in time O(1/ε2 poly(log n/ε)). Furthermore, we perform an extensive empirical study on several benchmark networks, validating the performance and accuracy of our algorithms.|双调和距离(BD)是度量图中两个节点之间距离的基本度量。它在网络一致性、机器学习和计算图形学等领域都有应用。尽管 BD 的重要性，有效的算法精确计算或逼近这一度量在大图仍然明显缺乏。在这项工作中，我们提供了几种算法来估计 BD，建立在一个新的公式的这个度量。这些算法具有局部性(也就是说，它们只读取输入图的一小部分) ，同时具有可证明的性能保证。特别地，我们的主要算法在时间 O (1/ε2多边形(log n/ε))上具有任意小的加性误差 ε 的任意节点对之间逼近 BD。此外，我们对几个基准网络进行了广泛的实证研究，验证了我们的算法的性能和准确性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+Query+of+Biharmonic+Distance+in+Networks)|0|
|[Multi-Task Learning for Routing Problem with Cross-Problem Zero-Shot Generalization](https://doi.org/10.1145/3637528.3672040)|Fei Liu, Xi Lin, Zhenkun Wang, Qingfu Zhang, Tong Xialiang, Mingxuan Yuan|Huawei Technologies Ltd., Hong Kong, China; Southern University of Science and Technology, Shenzhen, China; City University of Hong Kong, Hong Kong, China; Huawei Technologies Ltd., Shenzhen, China|Vehicle routing problems (VRP) are very important in many real-world applications and has been studied for several decades. Recently, neural combinatorial optimization (NCO) has attracted growing research effort. NCO is to train a neural network model to solve an optimization problem in question. However, existing NCO methods often build a different model for each routing problem, which significantly hinders their application in some areas where there are many different VRP variants to solve. In this work, we make a first attempt to tackle the crucial challenge of cross-problem generalization in NCO. We formulate VRPs as different combinations of a set of shared underlying attributes and solve them simultaneously via a single model through attribute composition. In this way, our proposed model can successfully solve VRPs with unseen attribute combinations in a zero-shot generalization manner. In our experiments, the neural model is trained on five VRP variants and its performance is tested on eleven VRP variants. The experimental results show that the model demonstrates superior performance on these eleven VRP variants, reducing the average gap to around 5% from over 20% and achieving a notable performance boost on both benchmark datasets and real-world logistics scenarios.|车辆路径问题(VRP)是车辆路径问题的一个重要研究方向。最近，神经组合优化(NCO)已经吸引了越来越多的研究人员。神经网络模型来解决最佳化问题问题。然而，现有的 NCO 方法往往为每个路由问题建立不同的模型，这严重阻碍了它们在有许多不同的 VRP 变量需要解决的一些领域的应用。在这项工作中，我们首次尝试解决 NCO 中跨问题泛化的关键挑战。我们将 VRP 描述为一组共享的底层属性的不同组合，并通过属性组合的方法通过单一模型同时求解。通过这种方法，我们提出的模型可以成功地解决具有不可见属性组合的 VRP 问题。在我们的实验中，神经模型被训练在五个 VRP 变体上，它的性能被测试在十一个 VRP 变体上。实验结果显示，该模型在这十一种 VRP 变种上表现卓越，将平均差距从超过20% 缩小至约5% ，并在基准数据集和现实世界物流场景上均取得显著的性能提升。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Task+Learning+for+Routing+Problem+with+Cross-Problem+Zero-Shot+Generalization)|0|
|[Low Rank Multi-Dictionary Selection at Scale](https://doi.org/10.1145/3637528.3671723)|Boya Ma, Maxwell McNeil, Abram Magner, Petko Bogdanov|Department of Computer Science, University at Albany, State University of New York, Albany, NY, USA|The sparse dictionary coding framework represents signals as a linear combination of a few predefined dictionary atoms. It has been employed for images, time series, graph signals and recently for 2-way (or 2D) spatio-temporal data employing jointly temporal and spatial dictionaries. Large and over-complete dictionaries enable high-quality models, but also pose scalability challenges which are exacerbated in multi-dictionary settings. Hence, an important problem that we address in this paper is: How to scale multi-dictionary coding for large dictionaries and datasets? We propose a multi-dictionary atom selection technique for low-rank sparse coding named LRMDS. To enable scalability to large dictionaries and datasets, it progressively selects groups of row-column atom pairs based on their alignment with the data and performs convex relaxation coding via the corresponding sub-dictionaries. We demonstrate both theoretically and experimentally that when the data has a low-rank encoding with a sparse subset of the atoms, LRMDS is able to select them with strong guarantees under mild assumptions. Furthermore, we demonstrate the scalability and quality of LRMDS in both synthetic and real-world datasets and for a range of coding dictionaries. It achieves 3 times to 10 times speed-up compared to baselines, while obtaining up to two orders of magnitude improvement in representation quality on some of the real world datasets given a fixed target number of atoms.|稀疏字典编码框架将信号表示为几个预定义字典原子的线性组合。它已经应用于图像，时间序列，图形信号和最近的2路(或2D)时空数据使用联合时间和空间字典。大型和过于完整的字典使高质量的模型成为可能，但也带来了可伸缩性方面的挑战，这些挑战在多字典设置中会加剧。因此，本文要解决的一个重要问题是: 如何对大型字典和数据集进行多字典编码？提出了一种低秩稀疏编码的多字典原子选择技术 LRMDS。为了使大型字典和数据集具有可伸缩性，它逐步根据行-列原子对与数据的对齐情况选择它们的组，并通过相应的子字典执行凸松弛编码。我们在理论和实验上都证明了，当数据具有一个低秩编码和一个稀疏的原子子集时，LRMDS 能够在温和的假设条件下选择它们，并且具有很强的保证性。此外，我们证明了 LRMDS 在合成和真实世界数据集和一系列编码字典中的可伸缩性和质量。与基线相比，它的速度提高了3到10倍，同时在给定固定目标原子数量的情况下，一些真实世界数据集的表示质量提高了两个数量级。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Low+Rank+Multi-Dictionary+Selection+at+Scale)|0|
|[ImputeFormer: Low Rankness-Induced Transformers for Generalizable Spatiotemporal Imputation](https://doi.org/10.1145/3637528.3671751)|Tong Nie, Guoyang Qin, Wei Ma, Yuewen Mei, Jian Sun|The Hong Kong Polytechnic University, Hong Kong SAR, China; Tongji University, Shanghai, China|Missing data is a pervasive issue in both scientific and engineering tasks, especially for the modeling of spatiotemporal data. Existing imputation solutions mainly include low-rank models and deep learning models. The former assumes general structural priors but has limited model capacity. The latter possesses salient expressivity, but lacks prior knowledge of the underlying spatiotemporal structures. Leveraging the strengths of both two paradigms, we demonstrate a low rankness-induced Transformer to achieve a balance between strong inductive bias and high expressivity. The exploitation of the inherent structures of spatiotemporal data enables our model to learn balanced signal-noise representations, making it generalizable for a variety of imputation tasks. We demonstrate its superiority in terms of accuracy, efficiency, and versatility in heterogeneous datasets, including traffic flow, solar energy, smart meters, and air quality. Promising empirical results provide strong conviction that incorporating time series primitives, such as low-rankness, can substantially facilitate the development of a generalizable model to approach a wide range of spatiotemporal imputation problems.|缺失数据是科学和工程任务中普遍存在的问题，特别是对于时空数据的建模。现有的归责解决方案主要包括低阶模型和深度学习模型。前者假设一般结构先验，但模型容量有限。后者具有显著的表现力，但缺乏对潜在时空结构的先验知识。利用这两种模式的优势，我们展示了一个低等级感应变压器，以实现强感应偏置和高表达性之间的平衡。利用时空数据的固有结构，使我们的模型能够学习平衡的信号-噪声表示，使它可以推广到各种插补任务。我们证明了它在准确性、效率和异构数据集的通用性方面的优势，包括交通流量、太阳能、智能仪表和空气质量。有希望的经验结果提供了强有力的信念，结合时间序列原语，如低秩，可以大大促进发展一个可推广的模型来处理广泛的时空插补问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ImputeFormer:+Low+Rankness-Induced+Transformers+for+Generalizable+Spatiotemporal+Imputation)|0|
|[Improving the Consistency in Cross-Lingual Cross-Modal Retrieval with 1-to-K Contrastive Learning](https://doi.org/10.1145/3637528.3671787)|Zhijie Nie, Richong Zhang, Zhangchi Feng, Hailang Huang, Xudong Liu|CCSE, Beihang University, Beijing, China|Cross-lingual Cross-modal Retrieval (CCR) is an essential task in web search, which aims to break the barriers between modality and language simultaneously and achieves image-text retrieval in the multi-lingual scenario with a single model. In recent years, excellent progress has been made based on cross-lingual cross-modal pre-training; particularly, the methods based on contrastive learning on large-scale data have significantly improved retrieval tasks. However, these methods directly follow the existing pre-training methods in the cross-lingual or cross-modal domain, leading to two problems of inconsistency in CCR: The methods with cross-lingual style suffer from the intra-modal error propagation, resulting in inconsistent recall performance across languages in the whole dataset. The methods with cross-modal style suffer from the inter-modal optimization direction bias, resulting in inconsistent rank across languages within each instance, which cannot be reflected by Recall@K. To solve these problems, we propose a simple but effective 1-to-K contrastive learning method, which treats each language equally and eliminates error propagation and optimization bias. In addition, we propose a new evaluation metric, Mean Rank Variance (MRV), to reflect the rank inconsistency across languages within each instance. Extensive experiments on four CCR datasets show that our method improves both recall rates and MRV with smaller-scale pre-trained data, achieving the new state-of-art.|跨语言交叉模式检索(CCR)是网络搜索中的一项重要任务，其目的是同时打破语言和情态之间的界限，以单一的模型实现多语种情景下的图像-文本检索。近年来，基于跨语言跨模式预训练的检索方法取得了显著的进展，尤其是基于大规模数据的对比学习方法显著改善了检索任务。然而，这些方法直接遵循现有的跨语言或跨模式领域的预训练方法，导致 CCR 中的两个不一致问题: 跨语言风格的方法受到模式内错误传播的影响，导致整个数据集中跨语言的召回性能不一致。具有跨模态风格的方法存在多模态优化方向偏差，导致每个实例中语言间的排名不一致，Recall@K 不能反映这一点。为了解决这些问题，我们提出了一种简单而有效的1-To-K 对比学习方法，该方法对每种语言一视同仁，消除了错误传播和优化偏差。此外，我们提出了一个新的评估指标，平均秩方差(MRV) ，以反映秩不一致的语言在每个实例。在四个 CCR 数据集上的大量实验表明，我们的方法利用较小规模的预训练数据提高了召回率和 MRV，实现了新的技术水平。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+the+Consistency+in+Cross-Lingual+Cross-Modal+Retrieval+with+1-to-K+Contrastive+Learning)|0|
|[CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent](https://doi.org/10.1145/3637528.3671837)|LiangBo Ning, Shijie Wang, Wenqi Fan, Qing Li, Xin Xu, Hao Chen, Feiran Huang|; The Hong Kong Polytechnic University, Hong Kong, China; Jinan University, Guangzhou, China|Recently, Large Language Model (LLM)-empowered recommender systems (RecSys) have brought significant advances in personalized user experience and have attracted considerable attention. Despite the impressive progress, the research question regarding the safety vulnerability of LLM-empowered RecSys still remains largely under-investigated. Given the security and privacy concerns, it is more practical to focus on attacking the black-box RecSys, where attackers can only observe the system's inputs and outputs. However, traditional attack approaches employing reinforcement learning (RL) agents are not effective for attacking LLM-empowered RecSys due to the limited capabilities in processing complex textual inputs, planning, and reasoning. On the other hand, LLMs provide unprecedented opportunities to serve as attack agents to attack RecSys because of their impressive capability in simulating human-like decision-making processes. Therefore, in this paper, we propose a novel attack framework called CheatAgent by harnessing the human-like capabilities of LLMs, where an LLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our method first identifies the insertion position for maximum impact with minimal input modification. After that, the LLM agent is designed to generate adversarial perturbations to insert at target positions. To further improve the quality of generated perturbations, we utilize the prompt tuning technique to improve attacking strategies via feedback from the victim RecSys iteratively. Extensive experiments across three real-world datasets demonstrate the effectiveness of our proposed attacking method.|近年来，大语言模型(LLM)授权的推荐系统(RecSys)在个性化用户体验方面取得了重大进展，引起了人们的广泛关注。尽管取得了令人印象深刻的进展，关于 LLM 授权的 RecSys 的安全漏洞的研究问题仍然在很大程度上没有得到充分的研究。考虑到安全和隐私问题，更实际的做法是集中攻击黑匣子 RecSys，攻击者只能观察系统的输入和输出。然而，传统的使用强化学习代理的攻击方法并不能有效地攻击具有 LLM 授权的 RecSys，因为它在处理复杂的文本输入、规划和推理方面的能力有限。另一方面，LLM 提供了前所未有的机会作为攻击代理攻击 RecSys，因为它们在模拟类人决策过程方面具有令人印象深刻的能力。因此，本文提出了一种新的攻击框架——欺骗代理(CheatAgent) ，利用 LLM 的类人功能，开发了一种基于 LLM 的代理来攻击 LLM 授权的 RecSys。具体来说，我们的方法首先确定插入位置，以便在最小的输入修改下获得最大的影响。在此之后，LLM 代理被设计为产生对抗性扰动插入到目标位置。为了进一步提高生成扰动的质量，我们利用快速调整技术，通过迭代地从受害者 RecSys 反馈来改进攻击策略。通过对三个实际数据集的大量实验证明了我们提出的攻击方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CheatAgent:+Attacking+LLM-Empowered+Recommender+Systems+via+LLM+Agent)|0|
|[Reliable Confidence Intervals for Information Retrieval Evaluation Using Generative A.I](https://doi.org/10.1145/3637528.3671883)|Harrie Oosterhuis, Rolf Jagerman, Zhen Qin, Xuanhui Wang, Michael Bendersky|Google Research & Radboud University, Amsterdam, Netherlands; Google Research, New York City, USA; Google Research, Mountain View, USA; Google Research, Amsterdam, Netherlands|The traditional evaluation of information retrieval (IR) systems is generally very costly as it requires manual relevance annotation from human experts. Recent advancements in generative artificial intelligence -specifically large language models (LLMs)- can generate relevance annotations at an enormous scale with relatively small computational costs. Potentially, this could alleviate the costs traditionally associated with IR evaluation and make it applicable to numerous low-resource applications. However, generated relevance annotations are not immune to (systematic) errors, and as a result, directly using them for evaluation produces unreliable results. In this work, we propose two methods based on prediction-powered inference and conformal risk control that utilize computer-generated relevance annotations to place reliable confidence intervals (CIs) around IR evaluation metrics. Our proposed methods require a small number of reliable annotations from which the methods can statistically analyze the errors in the generated annotations. Using this information, we can place CIs around evaluation metrics with strong theoretical guarantees. Unlike existing approaches, our conformal risk control method is specifically designed for ranking metrics and can vary its CIs per query and document. Our experimental results show that our CIs accurately capture both the variance and bias in evaluation based on LLM annotations, better than the typical empirical bootstrapping estimates. We hope our contributions bring reliable evaluation to the many IR applications where this was traditionally infeasible.|传统的信息检索系统评估通常是非常昂贵的，因为它需要人类专家的手工相关注释。生成性人工智能的最新进展——特别是大语言模型(LLM)——能够以相对较小的计算成本产生大规模的相关注释。这可能会减轻传统上与 IR 评估相关的成本，并使其适用于许多低资源的应用程序。然而，生成的相关性注释不能免疫(系统)错误，因此，直接使用它们进行评估会产生不可靠的结果。在这项工作中，我们提出了两种基于预测推理和保形风险控制的方法，利用计算机生成的相关性注释，将可靠的置信区间(CI)放置在 IR 评估指标周围。我们提出的方法需要少量可靠的注释，这些方法可以从中统计分析生成的注释中的错误。利用这些信息，我们可以将 CI 放在具有强大理论保证的评估指标周围。与现有的方法不同，我们的适形风险控制方法是专门为排序指标而设计的，并且可以根据查询和文档改变其 CI。我们的实验结果表明，我们的 CI 能够准确地捕获基于 LLM 注释的评估中的方差和偏差，比典型的经验自举估计更好。我们希望我们的贡献能够为许多传统上不可行的 IR 应用带来可靠的评估。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reliable+Confidence+Intervals+for+Information+Retrieval+Evaluation+Using+Generative+A.I)|0|
|[How Powerful is Graph Filtering for Recommendation](https://doi.org/10.1145/3637528.3671789)|Shaowen Peng, Xin Liu, Kazunari Sugiyama, Tsunenori Mine|National Institute of Advanced Industrial Science and Technology Tokyo, Japan; NARA Institute of Science and Technology, Nara, Japan; Osaka Seikei University Osaka, Japan; Kyushu University Fukuoka, Japan|It has been shown that the effectiveness of graph convolutional network (GCN) for recommendation is attributed to the spectral graph filtering. Most GCN-based methods consist of a graph filter or followed by a low-rank mapping optimized based on supervised training. However, we show two limitations suppressing the power of graph filtering: (1) Lack of generality. Due to the varied noise distribution, graph filters fail to denoise sparse data where noise is scattered across all frequencies, while supervised training results in worse performance on dense data where noise is concentrated in middle frequencies that can be removed by graph filters without training. (2) Lack of expressive power. We theoretically show that linear GCN (LGCN) that is effective on collaborative filtering (CF) cannot generate arbitrary embeddings, implying the possibility that optimal data representation might be unreachable. To tackle the first limitation, we show close relation between noise distribution and the sharpness of spectrum where a sharper spectral distribution is more desirable causing data noise to be separable from important features without training. Based on this observation, we propose a generalized graph normalization (G2N) with hyperparameters adjusting the sharpness of spectral distribution in order to redistribute data noise to assure that it can be removed by graph filtering without training. As for the second limitation, we propose an individualized graph filter (IGF) adapting to the different confidence levels of the user preference that interactions can reflect, which is proved to be able to generate arbitrary embeddings. By simplifying LGCN, we further propose a simplified graph filtering for CF (SGFCF) which only requires the top-K singular values for recommendation. Finally, experimental results on four datasets with different density settings demonstrate the effectiveness and efficiency of our proposed methods.|研究表明，图卷积网络(GCN)的推荐有效性归功于谱图滤波。大多数基于 GCN 的方法包括一个图滤波器或后跟一个基于监督训练优化的低秩映射。然而，我们发现了抑制图滤波能力的两个限制: (1)缺乏通用性。由于噪声分布的多样性，图形滤波器在噪声散布于所有频率的稀疏数据中无法去噪，而监督训练在噪声集中于中间频率的稠密数据中效果较差，图形滤波器不需要训练就可以去除噪声。(2)缺乏表达能力。我们从理论上证明了对协同过滤有效的线性 GCN (LGCN)不能产生任意的嵌入，这意味着最佳数据表示可能无法实现。为了解决第一个局限性，我们展示了噪声分布和频谱清晰度之间的密切关系，其中更清晰的频谱分布更可取，使得数据噪声可以不经训练地从重要特征中分离出来。在此基础上，我们提出了一种带有超参数的广义图归一化算法(G2N) ，该算法通过调整光谱分布的清晰度来重新分布数据噪声，以保证不需要训练就可以通过图滤波去除噪声。针对第二个限制，我们提出了一种个性化图形滤波器(IGF) ，它能够适应交互所反映的用户偏好的不同置信水平，并且能够产生任意的嵌入。通过简化 LGCN，我们进一步提出了一种简化的 CF (SGFCF)图形滤波器，它只需要最高 K 的奇异值作为推荐值。最后，在四个不同密度设置的数据集上的实验结果表明了本文方法的有效性和高效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+Powerful+is+Graph+Filtering+for+Recommendation)|0|
|[STEMO: Early Spatio-temporal Forecasting with Multi-Objective Reinforcement Learning](https://doi.org/10.1145/3637528.3671922)|Wei Shao, Yufan Kang, Ziyan Peng, Xiao Xiao, Lei Wang, Yuhui Yang, Flora D. Salim|Data61, CSIRO, Clayton, Victoria, Australia; RMIT University, Melbourne, Victoria, Australia; University of New South Wales, Sydney, Australia; Xidian University, Xi'an, China; Zhejiang University, Hangzhou, China|Accuracy and timeliness are indeed often conflicting goals in predictiontasks. Premature predictions may yield a higher rate of false alarms, whereasdelaying predictions to gather more information can render them too late to beuseful. In applications such as wildfires, crimes, and traffic jams, timelyforecasting are vital for safeguarding human life and property. Consequently,finding a balance between accuracy and timeliness is crucial. In this paper, wepropose an early spatio-temporal forecasting model based on Multi-Objectivereinforcement learning that can either implement an optimal policy given apreference or infer the preference based on a small number of samples. Themodel addresses two primary challenges: 1) enhancing the accuracy of earlyforecasting and 2) providing the optimal policy for determining the mostsuitable prediction time for each area. Our method demonstrates superiorperformance on three large-scale real-world datasets, surpassing existingmethods in early spatio-temporal forecasting tasks.|在预测任务中，准确性和及时性往往是矛盾的目标。过早的预测可能会产生更高的错误警报率，而为了收集更多信息而推迟预测可能会使预测太晚而无法发挥作用。在野火、犯罪和交通堵塞等应用中，及时预报对保护人类生命和财产至关重要。因此，在准确性和及时性之间找到一个平衡点是至关重要的。本文提出了一种基于多目标强化学习的早期时空预测模型，该模型既可以实现给定偏好的最优策略，也可以基于少量样本进行偏好推断。该模型解决了两个主要的挑战: 1)提高早期预测的准确性和2)为确定每个地区最合适的预测时间提供最优策略。该方法在三个大规模真实世界数据集上表现出优越的性能，在早期时空预测任务中优于现有方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=STEMO:+Early+Spatio-temporal+Forecasting+with+Multi-Objective+Reinforcement+Learning)|0|
|[Marrying Dialogue Systems with Data Visualization: Interactive Data Visualization Generation from Natural Language Conversations](https://doi.org/10.1145/3637528.3671935)|Yuanfeng Song, Xuefang Zhao, Raymond ChiWing Wong|AI Group, WeBank Co., Ltd., Shenzhen, China; The Hong Kong University of Science and Technology, Hong Kong, China|Data visualization (DV) has become the prevailing tool in the market due to its effectiveness into illustrating insights in vast amounts of data. To lower the barrier of using DVs, automatic DV tasks, such as natural language question (NLQ) to visualization translation (formally called text-to-vis), have been investigated in the research community. However, text-to-vis assumes the NLQ to be well-organized and expressed in a single sentence. However, in real-world settings, complex DV is needed through consecutive exchanges between the DV system and the users. In this paper, we propose a new task named CoVis, short for Conversational text-to-Visualization, aiming at constructing DVs through a series of interactions between users and the system. Since it is the task which has not been studied in the literature, we first build a benchmark dataset named Dial-NVBench, including dialogue sessions with a sequence of queries from a user and responses from the system. The ultimate goal of each dialogue session is to create a suitable DV. However, this process can contain diverse dialogue queries, such as seeking information about the dataset, manipulating parts of the data, and visualizing the data. Then, we propose a multi-modal neural network named MMCoVisNet to answer these DV-related queries. In particular, MMCoVisNet first fully understands the dialogue context and determines the corresponding responses. Then, it uses adaptive decoders to provide the appropriate replies: (i) a straightforward text decoder is used to produce general responses, (ii) an SQL-form decoder is applied to synthesize data querying responses, and (iii) a DV-form decoder tries to construct the appropriate DVs. We comparatively evaluate MMCoVisNet with other baselines over our proposed benchmark dataset. Experimental results validate that MMCoVisNet performs better than existing baselines and achieves a state-of-the-art performance.|数据可视化(DV)已成为市场上流行的工具，因为它能有效地用大量数据来说明见解。为了降低数字视频的使用障碍，自动数字视频任务，如自然语言问题(NLQ)到可视化翻译(正式称为文本到视觉) ，已经在研究界进行了研究。然而，文本-视觉假设 NLQ 是组织良好的，并表达在一个单一的句子。然而，在现实世界中，复杂的 DV 需要通过 DV 系统与用户之间的连续交换。本文提出了一个新的任务 CoVis，即会话文本到可视化(Conversational text-to-Visualization) ，旨在通过用户与系统之间的一系列交互来构建 DVs。本文首先构建了一个基准数据集 Dial-NVBench，包括用户的对话会话和系统的响应。每个对话会议的最终目标是创建一个合适的 DV。但是，这个过程可以包含不同的对话查询，例如查找有关数据集的信息、操作部分数据和可视化数据。然后，提出了一种多模态神经网络 MMCoVisNet 来回答这些与 DV 相关的查询。具体来说，MMCoVisNet 首先完全理解对话语境并确定相应的响应。然后，它使用自适应解码器提供适当的答复: (i)一个简单的文本解码器用于产生一般的响应，(ii)一个 SQL 形式的解码器用于合成数据查询响应，和(iii)一个 DV 形式的解码器试图构造适当的 DVs。我们比较评估 MMCoVisNet 与其他基线在我们提出的基准数据集。实验结果验证了 MMCoVisNet 的性能优于现有的基准线，达到了最先进的性能水平。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Marrying+Dialogue+Systems+with+Data+Visualization:+Interactive+Data+Visualization+Generation+from+Natural+Language+Conversations)|0|
|[Towards Robust Recommendation via Decision Boundary-aware Graph Contrastive Learning](https://doi.org/10.1145/3637528.3671661)|Jiakai Tang, Sunhao Dai, Zexu Sun, Xu Chen, Jun Xu, Wenhui Yu, Lantao Hu, Peng Jiang, Han Li|Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; Kuaishou Technology, Beijing, China|In recent years, graph contrastive learning (GCL) has received increasing attention in recommender systems due to its effectiveness in reducing bias caused by data sparsity. However, most existing GCL models rely on heuristic approaches and usually assume entity independence when constructing contrastive views. We argue that these methods struggle to strike a balance between semantic invariance and view hardness across the dynamic training process, both of which are critical factors in graph contrastive learning. To address the above issues, we propose a novel GCL-based recommendation framework RGCL, which effectively maintains the semantic invariance of contrastive pairs and dynamically adapts as the model capability evolves through the training process. Specifically, RGCL first introduces decision boundary-aware adversarial perturbations to constrain the exploration space of contrastive augmented views, avoiding the decrease of task-specific information. Furthermore, to incorporate global user-user and item-item collaboration relationships for guiding on the generation of hard contrastive views, we propose an adversarial-contrastive learning objective to construct a relation-aware view-generator. Besides, considering that unsupervised GCL could potentially narrower margins between data points and the decision boundary, resulting in decreased model robustness, we introduce the adversarial examples based on maximum perturbations to achieve margin maximization. We also provide theoretical analyses on the effectiveness of our designs. Through extensive experiments on five public datasets, we demonstrate the superiority of RGCL compared against twelve baseline models.|近年来，图形对比学习(GCL)由于能够有效地减少数据稀疏带来的偏差，在推荐系统中得到了越来越多的关注。然而，大多数现有的 GCL 模型依赖于启发式方法，并且在构造对比视图时通常假设实体独立。我们认为这些方法在动态训练过程中很难在语义不变性和视图硬度之间取得平衡，这两者都是图形对比学习的关键因素。为了解决上述问题，我们提出了一种新的基于 GCL 的推荐框架 RGCL，该框架有效地保持了对比对的语义不变性，并随着模型能力在训练过程中的发展而动态适应。具体来说，RGCL 首先引入了决策边界感知的对抗扰动来约束对比增广视图的探索空间，避免了任务特定信息的减少。此外，为了整合全局用户-用户和项目-项目协作关系来指导硬对比视图的生成，我们提出了一个对抗对比学习目标来构造一个关系感知视图生成器。此外，考虑到无监督的 GCL 可能会缩小数据点与决策边界之间的利润率，从而降低模型的稳健性，我们引入了基于最大扰动的对抗性例子，以实现利润率最大化。我们还对我们的设计的有效性提供了理论分析。通过对5个公共数据集的大量实验，我们证明了 RgCL 相对于十二个基线模型的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Robust+Recommendation+via+Decision+Boundary-aware+Graph+Contrastive+Learning)|0|
|[Reinforced Compressive Neural Architecture Search for Versatile Adversarial Robustness](https://doi.org/10.1145/3637528.3672009)|Dingrong Wang, Hitesh Sapkota, Zhiqiang Tao, Qi Yu|Rochester Institute of Technology, Rochester, NY, USA; Amazon Inc., Sunnyvale, CA, USA|Prior research on neural architecture search (NAS) for adversarial robustness has revealed that a lightweight and adversarially robust sub-network could exist in a non-robust large teacher network. Such a sub-network is generally discovered based on heuristic rules to perform neural architecture search. However, heuristic rules are inadequate to handle diverse adversarial attacks and different "teacher" network capacity. To address this key challenge, we propose Reinforced Compressive Neural Architecture Search (RC-NAS), aiming to achieve Versatile Adversarial Robustness. Specifically, we define novel task settings that compose datasets, adversarial attacks, and teacher network configuration. Given diverse tasks, we develop an innovative dual-level training paradigm that consists of a meta-training and a fine-tuning phase to effectively expose the RL agent to diverse attack scenarios (in meta-training), and make it adapt quickly to locate an optimal sub-network (in fine-tuning) for previously unseen scenarios. Experiments show that our framework could achieve adaptive compression towards different initial teacher networks, datasets, and adversarial attacks, resulting in more lightweight and adversarially robust architectures. We also provide a theoretical analysis to explain why the reinforcement learning (RL)-guided adversarial architectural search helps adversarial robustness over standard adversarial training methods.|针对对抗性鲁棒性的神经网络结构搜索(NAS)研究表明，在非鲁棒的大型教师网络中存在一个轻量级的对抗性鲁棒子网络。这样的子网络通常是基于启发式规则发现的，用于执行神经结构搜索。然而，启发式规则不足以处理不同的对手攻击和不同的“教师”网络容量。为了解决这一关键问题，我们提出了增强压缩神经结构搜索(RC-NAS) ，旨在实现通用的对抗鲁棒性。具体来说，我们定义了组成数据集、对抗性攻击和教师网络配置的新任务设置。考虑到不同的任务，我们开发了一个创新的双级训练范例，其中包括元训练和微调阶段，以有效地将 RL 代理暴露于不同的攻击场景(在元训练中) ，并使其快速适应定位最佳子网络(在微调中)以前看不见的场景。实验表明，我们的框架能够针对不同的初始教师网络、数据集和对抗性攻击实现自适应压缩，从而产生更轻量级和对抗性更强的体系结构。我们还提供了一个理论分析来解释为什么强化学习指导的对抗架构搜索比标准的对抗训练方法有助于对抗的稳健性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reinforced+Compressive+Neural+Architecture+Search+for+Versatile+Adversarial+Robustness)|0|
|[Routing Evidence for Unseen Actions in Video Moment Retrieval](https://doi.org/10.1145/3637528.3671693)|Guolong Wang, Xun Wu, Zheng Qin, Liangliang Shi|; Tsinghua University, Beijing, China|Video moment retrieval (VMR) is a cutting-edge vision-language task locating a segment in a video according to the query. Though the methods have achieved significant performance, they assume that training and testing samples share the same action types, hindering real-world application. In this paper, we specifically consider a new problem: video moment retrieval by queries with unseen actions. We propose a plug-and-play structure, Routing Evidence (RE), with multiple evidence-learning heads and dynamically route one to locate a sentence with an unseen action. Each evidence-learning head estimates the uncertainty while regressing timestamps. We formulate the evidence distribution by a Normal-Inverse Gamma function and design a router to select the most appropriate distribution for a sample. Empirically, we study the efficacy of RE on three updated databases where training and testing samples contain different action types. We find that RE outperforms other state-of-the-art methods with a more robust predictor. Code and data will be available at https://github.com/dieuroi/Routing-Evidence.|视频矩检索(VMR)是一种根据查询对视频片段进行定位的前沿视觉语言任务。尽管这些方法已经取得了显著的性能，但是它们假设训练和测试样本共享相同的动作类型，从而阻碍了真实世界的应用。在本文中，我们特别考虑了一个新的问题: 视频时刻检索的查询与看不见的行动。我们提出了一种即插即用的结构，路由证据(RE) ，具有多个证据学习头，并动态路由其中一个来定位一个看不见动作的句子。每个证据学习负责人在回归时间戳时估计不确定性。我们利用一个正反伽玛函数来表达证据分布，并设计一个路由器来选择最适合样本的分布。通过实证研究，我们在三个更新的训练和测试样本包含不同行为类型的数据库上研究了 RE 的效能。我们发现 RE 比其他最先进的方法具有更强大的预测器。代码和数据将在 https://github.com/dieuroi/routing-evidence 公布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Routing+Evidence+for+Unseen+Actions+in+Video+Moment+Retrieval)|0|
|[Unveiling Vulnerabilities of Contrastive Recommender Systems to Poisoning Attacks](https://doi.org/10.1145/3637528.3671795)|Zongwei Wang, Junliang Yu, Min Gao, Hongzhi Yin, Bin Cui, Shazia W. Sadiq|Chongqing University, Chongqing, China; The University of Queensland, Brisbane, Australia; Peking University, Beijing, China|Contrastive learning (CL) has recently gained prominence in the domain of recommender systems due to its great ability to enhance recommendation accuracy and improve model robustness. Despite its advantages, this paper identifies a vulnerability of CL-based recommender systems that they are more susceptible to poisoning attacks aiming to promote individual items. Our analysis indicates that this vulnerability is attributed to the uniform spread of representations caused by the InfoNCE loss. Furthermore, theoretical and empirical evidence shows that optimizing this loss favors smooth spectral values of representations. This finding suggests that attackers could facilitate this optimization process of CL by encouraging a more uniform distribution of spectral values, thereby enhancing the degree of representation dispersion. With these insights, we attempt to reveal a potential poisoning attack against CL-based recommender systems, which encompasses a dual-objective framework: one that induces a smoother spectral value distribution to amplify the InfoNCE loss's inherent dispersion effect, named dispersion promotion; and the other that directly elevates the visibility of target items, named rank promotion. We validate the threats of our attack model through extensive experimentation on four datasets. By shedding light on these vulnerabilities, our goal is to advance the development of more robust CL-based recommender systems. The code is available at https://github.com/CoderWZW/ARLib.|对比学习由于具有提高推荐精度和增强模型鲁棒性的能力，近年来在推荐系统领域得到了广泛的应用。本文认为基于 CL 的推荐系统虽然具有一定的优势，但是它们更容易受到旨在推广个别项目的中毒攻击。我们的分析表明，这一漏洞是由于表示的统一传播造成的信息 NCE 的损失。此外，理论和经验证明表明，优化这种损失有利于表示的平滑光谱值。这一发现表明，攻击者可以通过鼓励谱值的更均匀分布来促进 CL 的优化过程，从而提高表示离散度。有了这些见解，我们试图揭示一种针对基于 CL 的推荐系统的潜在中毒攻击，其中包含一个双重目标框架: 一个是诱导更平滑的光谱值分布，以放大 InfoNCE 损失的固有分散效应，称为分散促进; 另一个是直接提高目标项目的可见性，称为等级促进。我们通过在四个数据集上的大量实验验证了我们的攻击模型的威胁性。通过阐明这些漏洞，我们的目标是推动开发更健壮的基于 CL 的推荐系统。密码可在 https://github.com/coderwzw/arlib 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unveiling+Vulnerabilities+of+Contrastive+Recommender+Systems+to+Poisoning+Attacks)|0|
|[FedSAC: Dynamic Submodel Allocation for Collaborative Fairness in Federated Learning](https://doi.org/10.1145/3637528.3671748)|Zihui Wang, Zheng Wang, Lingjuan Lyu, Zhaopeng Peng, Zhicheng Yang, Chenglu Wen, Rongshan Yu, Cheng Wang, Xiaoliang Fan|; Sony AI, Zurich, Swaziland|Collaborative fairness stands as an essential element in federated learningto encourage client participation by equitably distributing rewards based onindividual contributions. Existing methods primarily focus on adjustinggradient allocations among clients to achieve collaborative fairness. However,they frequently overlook crucial factors such as maintaining consistency acrosslocal models and catering to the diverse requirements of high-contributingclients. This oversight inevitably decreases both fairness and model accuracyin practice. To address these issues, we propose FedSAC, a novel Federatedlearning framework with dynamic Submodel Allocation for Collaborative fairness,backed by a theoretical convergence guarantee. First, we present the concept of"bounded collaborative fairness (BCF)", which ensures fairness by tailoringrewards to individual clients based on their contributions. Second, toimplement the BCF, we design a submodel allocation module with a theoreticalguarantee of fairness. This module incentivizes high-contributing clients withhigh-performance submodels containing a diverse range of crucial neurons,thereby preserving consistency across local models. Third, we further develop adynamic aggregation module to adaptively aggregate submodels, ensuring theequitable treatment of low-frequency neurons and consequently enhancing overallmodel accuracy. Extensive experiments conducted on three public benchmarksdemonstrate that FedSAC outperforms all baseline methods in both fairness andmodel accuracy. We see this work as a significant step towards incentivizingbroader client participation in federated learning. The source code isavailable at https://github.com/wangzihuixmu/FedSAC.|协作公平是联合学习的一个重要组成部分，通过公平分配基于个人贡献的奖励来鼓励客户参与。现有的方法主要集中在调整客户间的梯度分配以实现协作公平。然而，他们经常忽略一些关键因素，例如保持当地模式的一致性以及满足高贡献客户的不同需求。这种疏忽不可避免地降低了实践中的公平性和模型的准确性。为了解决这些问题，我们提出了 FedSAC，这是一个新的联邦学习框架，具有动态子模型分配的协作公平性，在理论趋同保证的支持下。首先，我们提出了“有限协作公平(BCF)”的概念，它通过根据客户的贡献量身定制奖励来确保公平性。其次，为了实现 BCF，我们设计了一个具有公平性理论保证的子模型分配模块。该模块通过包含多种关键神经元的高性能子模型激励高贡献客户，从而保持局部模型之间的一致性。第三，我们进一步发展动态聚集模块，以自适应聚集子模型，确保公平的治疗低频神经元，从而提高整体模型的准确性。在三个公共基准上进行的大量实验表明，FedSAC 在公平性和模型准确性方面优于所有基准方法。我们认为这项工作是激励更广泛的客户参与联合学习的重要一步。源代码可以在 https://github.com/wangzihuixmu/fedsac 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedSAC:+Dynamic+Submodel+Allocation+for+Collaborative+Fairness+in+Federated+Learning)|0|
|[Unifying Graph Convolution and Contrastive Learning in Collaborative Filtering](https://doi.org/10.1145/3637528.3671840)|Yihong Wu, Le Zhang, Fengran Mo, Tianyu Zhu, Weizhi Ma, JianYun Nie|Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China; Université de Montréal, Montréal, Canada; MIIT Key Laboratory of Data Intelligence and Management, Beihang University, Beijing, China; Mila - Quebec AI Institute, Montréal, Canada|Graph-based models and contrastive learning have emerged as prominent methods in Collaborative Filtering (CF). While many existing models in CF incorporate these methods in their design, there seems to be a limited depth of analysis regarding the foundational principles behind them. This paper bridges graph convolution, a pivotal element of graph-based models, with contrastive learning through a theoretical framework. By examining the learning dynamics and equilibrium of the contrastive loss, we offer a fresh lens to understand contrastive learning via graph theory, emphasizing its capability to capture high-order connectivity. Building on this analysis, we further show that the graph convolutional layers often used in graph-based models are not essential for high-order connectivity modeling and might contribute to the risk of oversmoothing. Stemming from our findings, we introduce Simple Contrastive Collaborative Filtering (SCCF), a simple and effective algorithm based on a naive embedding model and a modified contrastive loss. The efficacy of the algorithm is demonstrated through extensive experiments across four public datasets. The experiment code is available at https://github.com/wu1hong/SCCF.|基于图表的模型和对比学习已经成为协同过滤研究的主要方法。虽然 CF 中的许多现有模型在其设计中包含了这些方法，但似乎对其背后的基本原则的分析深度有限。本文通过一个理论框架，将图卷积这一基于图的模型的关键要素与对比学习结合起来。通过研究对比损失的学习动力学和均衡性，我们提供了一个新的视角，通过图论来理解对比学习，强调其捕捉高阶连通性的能力。在此分析的基础上，我们进一步表明，在基于图的模型中经常使用的图卷积层并不是高阶连通性建模所必需的，而且可能会导致过度平滑的风险。根据我们的发现，我们介绍了简单对比协同过滤(SCCF) ，这是一个简单而有效的算法，基于一个幼稚的嵌入模型和一个修正的对比度损失。通过对四个公共数据集的大量实验，验证了算法的有效性。实验代码可在 https://github.com/wu1hong/sccf 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unifying+Graph+Convolution+and+Contrastive+Learning+in+Collaborative+Filtering)|0|
|[Towards Lightweight Graph Neural Network Search with Curriculum Graph Sparsification](https://doi.org/10.1145/3637528.3671706)|Beini Xie, Heng Chang, Ziwei Zhang, Zeyang Zhang, Simin Wu, Xin Wang, Yuan Meng, Wenwu Zhu|DCST, BNRist, Tsinghua University, Beijing, China; Lanzhou University, Lanzhou, China; DCST, Tsinghua University, Beijing, China|Graph Neural Architecture Search (GNAS) has achieved superior performance on various graph-structured tasks. However, existing GNAS studies overlook the applications of GNAS in resource-constrained scenarios. This paper proposes to design a joint graph data and architecture mechanism, which identifies important sub-architectures via the valuable graph data. To search for optimal lightweight Graph Neural Networks (GNNs), we propose a Lightweight Graph Neural Architecture Search with Graph SparsIfication and Network Pruning (GASSIP) method. In particular, GASSIP comprises an operation-pruned architecture search module to enable efficient lightweight GNN search. Meanwhile, we design a novel curriculum graph data sparsification module with an architecture-aware edge-removing difficulty measurement to help select optimal sub-architectures. With the aid of two differentiable masks, we iteratively optimize these two modules to efficiently search for the optimal lightweight architecture. Extensive experiments on five benchmarks demonstrate the effectiveness of GASSIP. Particularly, our method achieves on-par or even higher node classification performance with half or fewer model parameters of searched GNNs and a sparser graph.|图形神经结构搜索(GNAS)已经在各种图形结构的任务中取得了优异的性能。然而，现有的 GNAS 研究忽视了 GNAS 在资源受限情况下的应用。本文提出了一种联合图数据和体系结构机制，通过有价值的图数据来识别重要的子体系结构。为了寻找最优的轻量级图神经网络(GNN) ，提出了一种基于图稀疏化和网络剪枝的轻量级图神经网络体系结构搜索(GASSIP)方法。特别是，GASSIP 包含一个操作修剪的体系结构搜索模块，以支持高效的轻量级 GNN 搜索。同时，我们设计了一个新颖的课程图数据稀疏化模块，该模块具有体系结构感知的边缘去除困难度度量，以帮助选择最佳的子体系结构。借助于两个可微掩模，我们对这两个模块进行迭代优化，以有效地寻找最优的轻量级体系结构。在五个基准上的大量实验证明了 GASSIP 的有效性。特别地，我们的方法只需要搜索到的 GNN 的一半或更少的模型参数和一个更稀疏的图，就可以获得同等甚至更高的节点分类性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Lightweight+Graph+Neural+Network+Search+with+Curriculum+Graph+Sparsification)|0|
|[Revisiting Reciprocal Recommender Systems: Metrics, Formulation, and Method](https://doi.org/10.1145/3637528.3671734)|Chen Yang, Sunhao Dai, Yupeng Hou, Wayne Xin Zhao, Jun Xu, Yang Song, Hengshu Zhu|; Nanbeige Lab, BOSS Zhipin, Beijing, China; Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; University of California, San Diego, La Jolla, USA; Career Science Lab, BOSS Zhipin, Beijing, China|Reciprocal recommender systems (RRS), conducting bilateral recommendations between two involved parties, have gained increasing attention for enhancing matching efficiency. However, the majority of existing methods in the literature still reuse conventional ranking metrics to separately assess the performance on each side of the recommendation process. These methods overlook the fact that the ranking outcomes of both sides collectively influence the effectiveness of the RRS, neglecting the necessity of a more holistic evaluation and a capable systemic solution. In this paper, we systemically revisit the task of reciprocal recommendation, by introducing the new metrics, formulation, and method. Firstly, we propose five new evaluation metrics that comprehensively and accurately assess the performance of RRS from three distinct perspectives: overall coverage, bilateral stability, and balanced ranking. These metrics provide a more holistic understanding of the system's effectiveness and enable a comprehensive evaluation. Furthermore, we formulate the RRS from a causal perspective, formulating recommendations as bilateral interventions, which can better model the decoupled effects of potential influencing factors. By utilizing the potential outcome framework, we further develop a model-agnostic causal reciprocal recommendation method that considers the causal effects of recommendations. Additionally, we introduce a reranking strategy to maximize matching outcomes, as measured by the proposed metrics. Extensive experiments on two real-world datasets from recruitment and dating scenarios demonstrate the effectiveness of our proposed metrics and approach. The code and dataset are available at: https://github.com/RUCAIBox/CRRS.|互惠推荐系统(RRS)在两个相关方之间进行双边推荐，在提高匹配效率方面受到越来越多的关注。然而，文献中的大多数现有方法仍然重用传统的排名指标来分别评估推荐过程的各个方面的性能。这些方法忽略了这样一个事实，即双方的排名结果共同影响区域资源规划的有效性，忽视了更加全面的评估和有能力的系统解决方案的必要性。在本文中，我们系统地重新审视互惠推荐的任务，通过介绍新的指标，公式和方法。首先，我们提出了五个新的评价指标，全面和准确地评估 RRS 的绩效从三个不同的角度: 整体覆盖，双边稳定性和平衡排名。这些指标提供了对系统有效性的更全面的理解，并能够进行全面的评估。此外，我们从因果关系的角度制定 RRS，提出建议作为双边干预措施，可以更好地模拟潜在影响因素的解耦效应。通过利用潜在的结果框架，我们进一步开发了一个模型无关的因果互惠推荐方法，考虑了推荐的因果效应。此外，我们引入了一个重新排序策略，以最大限度地匹配结果，由提议的度量衡量。来自招聘和约会场景的两个真实世界数据集的大量实验证明了我们提出的指标和方法的有效性。代码和数据集可在以下 https://github.com/rucaibox/crrs 获得:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+Reciprocal+Recommender+Systems:+Metrics,+Formulation,+and+Method)|0|
|[Graph Bottlenecked Social Recommendation](https://doi.org/10.1145/3637528.3671807)|Yonghui Yang, Le Wu, Zihan Wang, Zhuangzhuang He, Richang Hong, Meng Wang|Hefei University of Technology, Hefei, China|With the emergence of social networks, social recommendation has become an essential technique for personalized services. Recently, graph-based social recommendations have shown promising results by capturing the high-order social influence. Most empirical studies of graph-based social recommendations directly take the observed social networks into formulation, and produce user preferences based on social homogeneity. Despite the effectiveness, we argue that social networks in the real-world are inevitably noisy~(existing redundant social relations), which may obstruct precise user preference characterization. Nevertheless, identifying and removing redundant social relations is challenging due to a lack of labels. In this paper, we focus on learning the denoised social structure to facilitate recommendation tasks from an information bottleneck perspective. Specifically, we propose a novel Graph Bottlenecked Social Recommendation (GBSR) framework to tackle the social noise issue. GBSR is a model-agnostic social denoising framework, that aims to maximize the mutual information between the denoised social graph and recommendation labels, meanwhile minimizing it between the denoised social graph and the original one. This enables GBSR to learn the minimal yet sufficient social structure, effectively reducing redundant social relations and enhancing social recommendations. Technically, GBSR consists of two elaborate components, preference-guided social graph refinement, and HSIC-based bottleneck learning. Extensive experimental results demonstrate the superiority of the proposed GBSR, including high performances and good generality combined with various backbones. Our code is available at: https://github.com/yimutianyang/KDD24-GBSR.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Bottlenecked+Social+Recommendation)|0|
|[Efficient and Effective Anchored Densest Subgraph Search: A Convex-programming based Approach](https://doi.org/10.1145/3637528.3671727)|Xiaowei Ye, RongHua Li, Lei Liang, Zhizhen Liu, Longlong Lin, Guoren Wang|; Southwest University, Chongqing, China; Ant Group, Hangzhou, China; Beijing Institute of Technology, Beijing, China|The quest to identify local dense communities closely connected to predetermined seed nodes is vital across numerous applications. Given the seed nodes R, the R-subgraph density of a subgraph S is defined as traditional graph density of S with penalties on the nodes in S / R. The state-of-the-art (SOTA) anchored densest subgraph model, which is based on R-subgraph density, is designed to address the community search problem. However, it often struggles to efficiently uncover truly dense communities. To eliminate this issue, we propose a novel NR-subgraph density metric, a nuanced measure that identifies communities intimately linked to seed nodes and also exhibiting overall high graph density. We redefine the anchored densest subgraph search problem through the lens of NR-subgraph density and cast it as a Linear Programming (LP) problem. This allows us to transition into a dual problem, tapping into the efficiency and effectiveness of convex programming-based iterative algorithm. To solve this redefined problem, we propose two algorithms: FDP, an iterative method that swiftly attains near-optimal solutions, and FDPE, an exact approach that ensures full convergence. We perform extensive experiments on 12 real-world networks. The results show that our proposed algorithms not only outperform the SOTA methods by 3.6~14.1 times in terms of running time, but also produce subgraphs with superior internal quality.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+and+Effective+Anchored+Densest+Subgraph+Search:+A+Convex-programming+based+Approach)|0|
|[Approximate Matrix Multiplication over Sliding Windows](https://doi.org/10.1145/3637528.3671819)|Ziqi Yao, Lianzhi Li, Mingsong Chen, Xian Wei, Cheng Chen|East China Normal University, Shanghai, China|Large-scale streaming matrix multiplication is very common in various applications, sparking significant interest in develop efficient algorithms for approximate matrix multiplication (AMM) over streams. In addition, many practical scenarios require to process time-sensitive data and aim to compute matrix multiplication for most recent columns of the data matrices rather than the entire matrices, which motivated us to study efficient AMM algorithms over sliding windows. In this paper, we present two novel deterministic algorithms for this problem and provide corresponding error guarantees. We further reduce the space and time costs of our methods for sparse matrices by performing an approximate singular value decomposition which can utilize the sparsity of matrices. Extensive experimental results on both synthetic and real-world datasets validate our theoretical analysis and highlight the efficiency of our methods.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Approximate+Matrix+Multiplication+over+Sliding+Windows)|0|
|[Unsupervised Generative Feature Transformation via Graph Contrastive Pre-training and Multi-objective Fine-tuning](https://doi.org/10.1145/3637528.3672015)|Wangyang Ying, Dongjie Wang, Xuanming Hu, Yuanchun Zhou, Charu C. Aggarwal, Yanjie Fu|International Business Machines T. J. Watson Research Center, Yorktown Heights, USA; Computer Network Information Center, Chinese Academy of Sciences, Beijing, China; Arizona State University, Tempe, AZ, USA; The University of Kansas, Lawrence, KS, USA|Feature transformation is to derive a new feature set from original features to augment the AI power of data. In many science domains such as material performance screening, while feature transformation can model material formula interactions and compositions and discover performance drivers, supervised labels are collected from expensive and lengthy experiments. This issue motivates an Unsupervised Feature Transformation Learning (UFTL) problem. Prior literature, such as manual transformation, supervised feedback guided search, and PCA, either relies on domain knowledge or expensive supervised feedback, or suffers from large search space, or overlooks non-linear feature-feature interactions. UFTL imposes a major challenge on existing methods: how to design a new unsupervised paradigm that captures complex feature interactions and avoids large search space? To fill this gap, we connect graph, contrastive, and generative learning to develop a measurement-pretrain-finetune paradigm for UFTL. For unsupervised feature set utility measurement, we propose a feature value consistency preservation perspective and develop a mean discounted cumulative gain like unsupervised metric to evaluate feature set utility. For unsupervised feature set representation pretraining, we regard a feature set as a feature-feature interaction graph, and develop an unsupervised graph contrastive learning encoder to embed feature sets into vectors. For generative transformation finetuning, we regard a feature set as a feature cross sequence and feature transformation as sequential generation. We develop a deep generative feature transformation model that coordinates the pretrained feature set encoder and the gradient information extracted from a feature set utility evaluator to optimize a transformed feature generator. Finally, we conduct extensive experiments to demonstrate the effectiveness, efficiency, traceability, and explicitness of our framework.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unsupervised+Generative+Feature+Transformation+via+Graph+Contrastive+Pre-training+and+Multi-objective+Fine-tuning)|0|
|[Personalized Federated Continual Learning via Multi-Granularity Prompt](https://doi.org/10.1145/3637528.3671948)|Hao Yu, Xin Yang, Xin Gao, Yan Kang, Hao Wang, Junbo Zhang, Tianrui Li|; Webank, Shenzhen, China; JD Intelligent Cities Research & JD iCity, JD Technology, Beijing, China; School of Computing and Artificial Intelligence, Southwest Jiaotong University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China|Personalized Federated Continual Learning (PFCL) is a new practical scenario that poses greater challenges in sharing and personalizing knowledge. PFCL not only relies on knowledge fusion for server aggregation at the global spatial-temporal perspective but also needs model improvement for each client according to the local requirements. Existing methods, whether in Personalized Federated Learning (PFL) or Federated Continual Learning (FCL), have overlooked the multi-granularity representation of knowledge, which can be utilized to overcome Spatial-Temporal Catastrophic Forgetting (STCF) and adopt generalized knowledge to itself by coarse-to-fine human cognitive mechanisms. Moreover, it allows more effectively to personalized shared knowledge, thus serving its own purpose. To this end, we propose a novel concept called multi-granularity prompt, i.e., coarse-grained global prompt acquired through the common model learning process, and fine-grained local prompt used to personalize the generalized representation. The former focuses on efficiently transferring shared global knowledge without spatial forgetting, and the latter emphasizes specific learning of personalized local knowledge to overcome temporal forgetting. In addition, we design a selective prompt fusion mechanism for aggregating knowledge of global prompts distilled from different clients. By the exclusive fusion of coarse-grained knowledge, we achieve the transmission and refinement of common knowledge among clients, further enhancing the performance of personalization. Extensive experiments demonstrate the effectiveness of the proposed method in addressing STCF as well as improving personalized performance.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalized+Federated+Continual+Learning+via+Multi-Granularity+Prompt)|0|
|[DipDNN: Preserving Inverse Consistency and Approximation Efficiency for Invertible Learning](https://doi.org/10.1145/3637528.3672036)|Jingyi Yuan, Yang Weng, Erik Blasch|Arizona State University, Tempe, AZ, USA; Air Force Research Lab, Arlington, VA, USA|Consistent bi-directional inferences are the key for many machine learning applications. Without consistency, inverse learning-based inferences can cause fuzzy images, erroneous control signals, and cascading failure in SCADA systems. Since standard deep neural networks (DNNs) are not inherently invertible to offer consistency, some past methods reconstruct DNN architecture analytically for one-to-one correspondence but compromise key features such as universal approximation. Other work maintains the capability of universal approximation in DNNs via iterative numerical approximation. However, these methods limit their applications significantly due to Lipschitz conditions and issues of numerical convergence. The dilemma of the analytical and numerical methods is the incompatibility between nonlinear layer compositions and bijective function construction for inverse modeling. Based on the observation, we propose decomposed-invertible-pathway DNNs (DipDNN). It relaxes the redundant reconstruction of nested DNN in the former methods and eases the Lipschitz constraint. As a result, we strictly guarantee the consistency of global inverse modeling without harming DNN's capability for universal approximation. As numerical stability and generalizability are keys for controlling critical infrastructures, we integrate contractive property with a parallel structure for inductive biases, leading to stable performance. Numerical results show that DipDNN performs significantly better than past methods, thanks to its enforcement of inverse consistency, numerical stability, and physical regularization.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DipDNN:+Preserving+Inverse+Consistency+and+Approximation+Efficiency+for+Invertible+Learning)|0|
|[Conditional Logical Message Passing Transformer for Complex Query Answering](https://doi.org/10.1145/3637528.3671869)|Chongzhi Zhang, Zhiping Peng, Junhao Zheng, Qianli Ma|Guangdong University of Petrochemical Technology & Jiangmen Polytechnic, Maoming, China; South China University of Technology, Guangzhou, China|Complex Query Answering (CQA) over Knowledge Graphs (KGs) is a challenging task. Given that KGs are usually incomplete, neural models are proposed to solve CQA by performing multi-hop logical reasoning. However, most of them cannot perform well on both one-hop and multi-hop queries simultaneously. Recent work proposes a logical message passing mechanism based on the pre-trained neural link predictors. While effective on both one-hop and multi-hop queries, it ignores the difference between the constant and variable nodes in a query graph. In addition, during the node embedding update stage, this mechanism cannot dynamically measure the importance of different messages, and whether it can capture the implicit logical dependencies related to a node and received messages remains unclear. In this paper, we propose Conditional Logical Message Passing Transformer (CLMPT), which considers the difference between constants and variables in the case of using pre-trained neural link predictors and performs message passing conditionally on the node type. We empirically verified that this approach can reduce computational costs without affecting performance. Furthermore, CLMPT uses the transformer to aggregate received messages and update the corresponding node embedding. Through the self-attention mechanism, CLMPT can assign adaptive weights to elements in an input set consisting of received messages and the corresponding node and explicitly model logical dependencies between various elements. Experimental results show that CLMPT is a new state-of-the-art neural CQA model. https://github.com/qianlima-lab/CLMPT.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Conditional+Logical+Message+Passing+Transformer+for+Complex+Query+Answering)|0|
|[Natural Language Explainable Recommendation with Robustness Enhancement](https://doi.org/10.1145/3637528.3671781)|Jingsen Zhang, Jiakai Tang, Xu Chen, Wenhui Yu, Lantao Hu, Peng Jiang, Han Li|Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; Kuaishou Technology, Beijing, China|Natural language explainable recommendation has become a promising direction to facilitate more efficient and informed user decisions. Previous models mostly focus on how to enhance the explanation accuracy. However, the robustness problem has been largely ignored, which requires the explanations generated for similar user-item pairs should not be too much different. Different from traditional classification problems, improving the robustness of natural languages has two unique characteristics: (1) Different token importances, that is, different tokens play various roles in representing the complete sentence, and the robustness requirements for predicting them should also be different. (2) Continuous token semantics, that is, the similarity of the output should be judged based on semantics, and the sequences without any token-level overlap may also be highly similar. Based on these characteristics, we formulate and solve a novel problem in the recommendation domain, that is, robust natural language explainable recommendation. To the best of our knowledge, it is the first time in this field. Specifically, we base our modeling on adversarial robust optimization and design four types of heuristic methods to modify the adversarial outputs with weighted token probabilities and synonym replacements. Furthermore, to consider the mutual influence between the above characteristics, we regard language generation as a decision-making problem and design a dual-policy reinforcement learning framework to improve the robustness of the generated languages. We conduct extensive experiments to demonstrate the effectiveness of our framework.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Natural+Language+Explainable+Recommendation+with+Robustness+Enhancement)|0|
|[Enabling Collaborative Test-Time Adaptation in Dynamic Environment via Federated Learning](https://doi.org/10.1145/3637528.3671908)|Jiayuan Zhang, Xuefeng Liu, Yukang Zhang, Guogang Zhu, Jianwei Niu, Shaojie Tang|; Shenzhen International Graduate School, Tsinghua University, Beijing, China; Jindal School of Management, The University of Texas at Dallas, Richardson, TX, USA|Deep learning models often suffer performance degradation when test data diverges from training data. Test-Time Adaptation (TTA) aims to adapt a trained model to the test data distribution using unlabeled test data streams. In many real-world applications, it is quite common for the trained model to be deployed across multiple devices simultaneously. Although each device can execute TTA independently, it fails to leverage information from the test data of other devices. To address this problem, we introduce Federated Learning (FL) to TTA to facilitate on-the-fly collaboration among devices during test time. The workflow involves clients (i.e., the devices) executing TTA locally, uploading their updated models to a central server for aggregation, and downloading the aggregated model for inference. However, implementing FL in TTA presents many challenges, especially in establishing inter-client collaboration in dynamic environment, where the test data distribution on different clients changes over time in different manners. To tackle these challenges, we propose a server-side Temporal-Spatial Aggregation (TSA) method. TSA utilizes a temporal-spatial attention module to capture intra-client temporal correlations and inter-client spatial correlations. To further improve robustness against temporal-spatial heterogeneity, we propose a heterogeneity-aware augmentation method and optimize the module using a self-supervised approach. More importantly, TSA can be implemented as a plug-in to TTA methods in distributed environments. Experiments on multiple datasets demonstrate that TSA outperforms existing methods and exhibits robustness across various levels of heterogeneity. The code is available at https://github.com/ZhangJiayuan-BUAA/FedTSA.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enabling+Collaborative+Test-Time+Adaptation+in+Dynamic+Environment+via+Federated+Learning)|0|
|[Topology-aware Embedding Memory for Continual Learning on Expanding Networks](https://doi.org/10.1145/3637528.3671732)|Xikun Zhang, Dongjin Song, Yixin Chen, Dacheng Tao|Washington University, Saint Louis, St. Louis, MO, USA; The University of Sydney, Sydney, NSW, Australia; University of Connecticut, Storrs, CT, USA|Memory replay based techniques have shown great success for continual learning with incrementally accumulated Euclidean data. Directly applying them to continually expanding networks, however, leads to the potential memory explosion problem due to the need to buffer representative nodes and their associated topological neighborhood structures. To this end, we systematically analyze the key challenges in the memory explosion problem, and present a general framework,i.e., Parameter Decoupled Graph Neural Networks (PDGNNs) with Topology-aware Embedding Memory (TEM), to tackle this issue. The proposed framework not only reduces the memory space complexity from O (ndL) to O (n)1: memory budget, d: average node degree, L: the radius of the GNN receptive field, but also fully utilizes the topological information for memory replay. Specifically, PDGNNs decouple trainable parameters from the computation ego-subnetwork viaTopology-aware Embeddings (TEs), which compress ego-subnetworks into compact vectors (i.e., TEs) to reduce the memory consumption. Based on this framework, we discover a unique pseudo-training effect in continual learning on expanding networks and this effect motivates us to develop a novel coverage maximization sampling strategy that can enhance the performance with a tight memory budget. Thorough empirical studies demonstrate that, by tackling the memory explosion problem and incorporating topological information into memory replay, PDGNNs with TEM significantly outperform state-of-the-art techniques, especially in the challenging class-incremental setting.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Topology-aware+Embedding+Memory+for+Continual+Learning+on+Expanding+Networks)|0|
|[Urban-Focused Multi-Task Offline Reinforcement Learning with Contrastive Data Sharing](https://doi.org/10.1145/3637528.3671823)|Xinbo Zhao, Yingxue Zhang, Xin Zhang, Yu Yang, Yiqun Xie, Yanhua Li, Jun Luo|Logistics and Supply Chain MultiTech R&D Centre, Hong Kong, Hong Kong; Worcester Polytechnic Institute, Worcester, MA, USA; University of Maryland, College Park, College Park, MD, USA; San Diego State University, San Diego, CA, USA; Lehigh University, Bethlehem, PA, USA; Binghamton University, Binghamton, NY, USA|Enhancing diverse human decision-making processes in an urban environment is a critical issue across various applications, including ride-sharing vehicle dispatching, public transportation management, and autonomous driving. Offline reinforcement learning (RL) is a promising approach to learn and optimize human urban strategies (or policies) from pre-collected human-generated spatial-temporal urban data. However, standard offline RL faces two significant challenges: (1) data scarcity and data heterogeneity, and (2) distributional shift. In this paper, we introduce MODA - a Multi-Task Offline Reinforcement Learning with Contrastive Data Sharing approach. MODA addresses the challenges of data scarcity and heterogeneity in a multi-task urban setting through Contrastive Data Sharing among tasks. This technique involves extracting latent representations of human behaviors by contrasting positive and negative data pairs. It then shares data presenting similar representations with the target task, facilitating data augmentation for each task. Moreover, MODA develops a novel model-based multi-task offline RL algorithm. This algorithm constructs a robust Markov Decision Process (MDP) by integrating a dynamics model with a Generative Adversarial Network (GAN). Once the robust MDP is established, any online RL or planning algorithm can be applied. Extensive experiments conducted in a real-world multi-task urban setting validate the effectiveness of MODA. The results demonstrate that MODA exhibits significant improvements compared to state-of-the-art baselines, showcasing its capability in advancing urban decision-making processes. We also made our code available to the research community.|在城市环境中增强多样化的人类决策过程是一个跨越多种应用的关键问题，包括拼车调度、公共交通管理和自主驾驶。离线强化学习是从预先收集的人类生成的时空城市数据中学习和优化人类城市策略(或政策)的一种很有前途的方法。然而，标准的离线 RL 面临两个重大挑战: (1)数据稀缺性和数据异构性，以及(2)分布式转移。这篇文章介绍了 MODA-一个基于对比数据共享的多任务脱机强化学习。MODA 通过任务之间的对比数据共享，解决了多任务城市环境中数据稀缺性和异质性的挑战。这种技术包括通过对比正负数据对来提取人类行为的潜在表征。然后，它与目标任务共享呈现类似表示的数据，这有助于为每个任务增强数据。此外，MODA 开发了一种新的基于模型的多任务离线 RL 算法。该算法通过将动力学模型与生成对抗网络(gAN)相结合来构建一个鲁棒的马可夫决策过程(mDP)。一旦稳健的 MDP 建立，任何在线 RL 或规划算法都可以应用。在现实世界多任务城市环境中进行的大量实验验证了 MODA 的有效性。结果表明，与最先进的基线相比，MODA 显示出显著的改进，显示了其推进城市决策进程的能力。我们还向研究团体提供了我们的代码。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Urban-Focused+Multi-Task+Offline+Reinforcement+Learning+with+Contrastive+Data+Sharing)|0|
|[Synthesizing Multimodal Electronic Health Records via Predictive Diffusion Models](https://doi.org/10.1145/3637528.3671836)|Yuan Zhong, Xiaochen Wang, Jiaqi Wang, Xiaokun Zhang, Yaqing Wang, Mengdi Huai, Cao Xiao, Fenglong Ma|GE Healthcare, Seattle, WA, USA; Purdue University, West Lafayette, IN, USA; The Pennsylvania State University, University Park, PA, USA; The Penn State University, University Park, PA, USA; Dalian University of Technology, Dalian, Liaoning, China; Iowa State University, Ames, IA, USA|Synthesizing electronic health records (EHR) data has become a preferredstrategy to address data scarcity, improve data quality, and model fairness inhealthcare. However, existing approaches for EHR data generation predominantlyrely on state-of-the-art generative techniques like generative adversarialnetworks, variational autoencoders, and language models. These methodstypically replicate input visits, resulting in inadequate modeling of temporaldependencies between visits and overlooking the generation of time information,a crucial element in EHR data. Moreover, their ability to learn visitrepresentations is limited due to simple linear mapping functions, thuscompromising generation quality. To address these limitations, we propose anovel EHR data generation model called EHRPD. It is a diffusion-based modeldesigned to predict the next visit based on the current one while alsoincorporating time interval estimation. To enhance generation quality anddiversity, we introduce a novel time-aware visit embedding module and apioneering predictive denoising diffusion probabilistic model (PDDPM).Additionally, we devise a predictive U-Net (PU-Net) to optimize P-DDPM.Weconduct experiments on two public datasets and evaluate EHRPD from fidelity,privacy, and utility perspectives. The experimental results demonstrate theefficacy and utility of the proposed EHRPD in addressing the aforementionedlimitations and advancing EHR data generation.|合成电子健康记录(EHR)数据已经成为解决数据稀缺性、提高数据质量和医疗保健公平性的首选策略。然而，现有的 EHR 数据生成方法主要依赖于最先进的生成技术，如生成对抗网络、变分自动编码器和语言模型。这些方法通常重复输入访问，导致访问之间的时间依赖性建模不足，并忽略了时间信息的生成，这是电子健康记录数据中的一个关键要素。此外，由于简单的线性映射函数，他们学习访问表示的能力受到限制，从而影响了生成质量。为了解决这些局限性，我们提出了一种新的 EHR 数据生成模型 EHRPD。这是一个基于扩散的模型，旨在预测下一次访问的基础上，当前的一个，同时也结合了时间间隔估计。为了提高产生的质量和多样性，我们引入了一种新的时间感知访问嵌入模块和先锋预测去噪扩散概率模型(PDDPM)。此外，我们设计了一个预测 U 网(PU-Net)来优化 P-DDPM。我们在两个公共数据集上进行实验，并从保真度、隐私和效用的角度评估 EHRPD。实验结果证明了提出的 EHRPD 在解决上述局限性和推进 EHR 数据生成方面的有效性和实用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Synthesizing+Multimodal+Electronic+Health+Records+via+Predictive+Diffusion+Models)|0|
|[Generative AI in E-Commerce: What Can We Expect?](https://doi.org/10.1145/3637528.3672503)|Haixun Wang|Instacart, San Francisco, CA, USA|The impact of generative AI on e-commerce is profound. It has significantly improved the understanding of user intent and serves as a comprehensive product knowledge graph. However, the most substantial disruptions are yet to come, partic- ularly through the rise of autonomous agents. In this talk, I will outline a tentative path toward a future where e-commerce not only offers an unparalleled customer experience but also thrives in a world dominated by generative AI and autonomous agents.|生成性人工智能对电子商务的影响是深远的。它显著提高了对用户意图的理解，并作为一个全面的产品知识图。然而，最重大的破坏还没有到来，特别是通过自主代理的兴起。在这次演讲中，我将概述一条通向未来的试探性道路，在这个未来，电子商务不仅能提供无与伦比的客户体验，而且还能在一个由生成性人工智能和自主代理主导的世界中蓬勃发展。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+AI+in+E-Commerce:+What+Can+We+Expect?)|0|
|[LiRank: Industrial Large Scale Ranking Models at LinkedIn](https://doi.org/10.1145/3637528.3671561)|Fedor Borisyuk, Mingzhou Zhou, Qingquan Song, Siyu Zhu, Birjodh Tiwana, Ganesh Parameswaran, Siddharth Dangi, Lars Hertel, Qiang Charles Xiao, Xiaochen Hou, Yunbo Ouyang, Aman Gupta, Sheallika Singh, Dan Liu, Hailing Cheng, Lei Le, Jonathan Hung, Sathiya Keerthi, Ruoyan Wang, Fengyu Zhang, Mohit Kothari, Chen Zhu, Daqi Sun, Yun Dai, Xun Luan, Sirou Zhu, Zhiwei Wang, Neil Daftary, Qianqi Shen, Chengming Jiang, Haichao Wei, Maneesh Varshney, Amol Ghoting, Souvik Ghosh|LinkedIn, Mountain View, CA, USA|We present LiRank, a large-scale ranking framework at LinkedIn that brings to production state-of-the-art modeling architectures and optimization methods. We unveil several modeling improvements, including Residual DCN, which adds attention and residual connections to the famous DCNv2 architecture. We share insights into combining and tuning SOTA architectures to create a unified model, including Dense Gating, Transformers and Residual DCN. We also propose novel techniques for calibration and describe how we productionalized deep learning based explore/exploit methods. To enable effective, production-grade serving of large ranking models, we detail how to train and compress models using quantization and vocabulary compression. We provide details about the deployment setup for large-scale use cases of Feed ranking, Jobs Recommendations, and Ads click-through rate (CTR) prediction. We summarize our learnings from various A/B tests by elucidating the most effective technical approaches. These ideas have contributed to relative metrics improvements across the board at LinkedIn: +0.5% member sessions in the Feed, +1.76% qualified job applications for Jobs search and recommendations, and +4.3% for Ads CTR. We hope this work can provide practical insights and solutions for practitioners interested in leveraging large-scale deep ranking systems.|我们介绍 LiRank，一个 LinkedIn 的大规模排名框架，它带来了最先进的建模架构和优化方法。我们揭示了几个建模改进，包括残余 DCN，它为著名的 DCNv2架构增加了注意力和残余连接。我们分享了结合和调整 SOTA 架构以创建统一模型的见解，包括致密门控、变压器和剩余 DCN。我们还提出了新的校准技术，并描述了如何生产基于深度学习的探索/开发方法。为了实现大型排名模型的高效、生产级服务，我们详细介绍了如何使用量化和词汇压缩对模型进行训练和压缩。我们提供了关于 Feed 排名、工作推荐和广告点进率(ctrl)预测等大规模用例部署设置的详细信息。我们通过阐明最有效的技术方法来总结我们从各种 A/B 测试中学到的东西。这些想法促进了 LinkedIn 的相关指标的全面改善: Feed 会员增加0.5% ，工作搜索和推荐的合格工作申请增加1.76% ，广告点击率增加4.3% 。我们希望这项工作可以提供实用的见解和解决方案的从业人员有兴趣利用大规模的深入排名系统。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LiRank:+Industrial+Large+Scale+Ranking+Models+at+LinkedIn)|0|
|[Enhancing Multi-field B2B Cloud Solution Matching via Contrastive Pre-training](https://doi.org/10.1145/3637528.3671513)|Haonan Chen, Zhicheng Dou, Xuetong Hao, Yunhao Tao, Shiren Song, Zhenli Sheng|Renmin University of China, Beijing, China; Huawei Cloud Computing, Hangzhou, China|Cloud solutions have gained significant popularity in the technology industry as they offer a combination of services and tools to tackle specific problems. However, despite their widespread use, the task of identifying appropriate company customers for a specific target solution to the sales team of a solution provider remains a complex business problem that existing matching systems have yet to adequately address. In this work, we study the B2B solution matching problem and identify two main challenges of this scenario: (1) the modeling of complex multi-field features and (2) the limited, incomplete, and sparse transaction data. To tackle these challenges, we propose a framework CAMA, which is built with a hierarchical multi-field matching structure as its backbone and supplemented by three data augmentation strategies and a contrastive pre-training objective to compensate for the imperfections in the available data. Through extensive experiments on a real-world dataset, we demonstrate that CAMA outperforms several strong baseline matching models significantly. Furthermore, we have deployed our matching framework on a system of Huawei Cloud. Our observations indicate an improvement of about 30% compared to the previous online model in terms of Conversion Rate (CVR), which demonstrates its great business value.|云解决方案已经在技术行业中大受欢迎，因为它们提供了解决特定问题的服务和工具的组合。然而，尽管它们被广泛使用，但是为解决方案供应商的销售团队确定合适的公司客户以提供特定目标解决方案的任务仍然是一个复杂的业务问题，现有的匹配系统尚未充分解决这个问题。在这项工作中，我们研究了 B2B 解决方案匹配问题，并确定了这个场景的两个主要挑战: (1)复杂的多领域特征的建模和(2)有限的，不完整的，稀疏的事务数据。为了应对这些挑战，我们提出了一个框架 CAMA，它以层次化的多领域匹配结构为骨干，并辅以三个数据增强策略和一个对比的预训练目标来弥补现有数据中的不完善。通过对一个真实世界数据集的大量实验，我们证明了 CAMA 显著优于几个强基线匹配模型。此外，我们已经在华为云系统上部署了我们的匹配框架。我们的观察表明，在转化率(CVR)方面，与以前的在线模型相比，大约有30% 的改进，这表明了其巨大的商业价值。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Multi-field+B2B+Cloud+Solution+Matching+via+Contrastive+Pre-training)|0|
|[GRILLBot In Practice: Lessons and Tradeoffs Deploying Large Language Models for Adaptable Conversational Task Assistants](https://doi.org/10.1145/3637528.3671622)|Sophie Fischer, Carlos Gemmell, Niklas Tecklenburg, Iain Mackie, Federico Rossetto, Jeffrey Dalton|University of Edinburgh, Edinburgh, United Kingdom; University of Glasgow, Glasgow, United Kingdom|We tackle the challenge of building real-world multimodal assistants for complex real-world tasks. We describe the practicalities and challenges of developing and deploying GRILLBot, a leading (first and second prize winning in 2022 and 2023) system deployed in the Alexa Prize TaskBot Challenge. Building on our Open Assistant Toolkit (OAT) framework, we propose a hybrid architecture that leverages Large Language Models (LLMs) and specialised models tuned for specific subtasks requiring very low latency. OAT allows us to define when, how and which LLMs should be used in a structured and deployable manner. For knowledge-grounded question answering and live task adaptations, we show that LLM reasoning abilities over task context and world knowledge outweigh latency concerns. For dialogue state management, we implement a code generation approach and show that specialised smaller models have 84% effectiveness with 100x lower latency. Overall, we provide insights and discuss tradeoffs for deploying both traditional models and LLMs to users in complex real-world multimodal environments in the Alexa TaskBot challenge. These experiences will continue to evolve as LLMs become more capable and efficient -- fundamentally reshaping OAT and future assistant architectures.|我们应对为复杂的现实世界任务构建现实世界多式联运助手的挑战。我们描述了开发和部署 GRILLBot 的实用性和挑战性，这是一个在 Alexa Prize TaskBot 挑战赛中部署的领先(2022年和2023年获得一等奖和二等奖)系统。在我们的 Open Assistant Toolkit (OAT)框架的基础上，我们提出了一种混合体系结构，它利用大语言模型(LLM)和针对需要非常低延迟的特定子任务调优的专门模型。OAT 允许我们定义何时、如何以及以结构化和可部署的方式使用哪些 LLM。对于基于知识的问题回答和实时任务适应，我们表明 LLM 在任务上下文和世界知识上的推理能力大于对延迟的关注。对于对话状态管理，我们实现了一个代码生成方法，并显示专门的小型模型具有84% 的有效性，延迟减少了100倍。总的来说，我们在 Alexa TaskBot 挑战中为复杂的现实世界多通道环境中的用户部署传统模型和 LLM 提供了见解并讨论了折衷方案。随着 LLM 变得更加有能力和高效，这些经验将继续发展——从根本上重塑 OAT 和未来的助理架构。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GRILLBot+In+Practice:+Lessons+and+Tradeoffs+Deploying+Large+Language+Models+for+Adaptable+Conversational+Task+Assistants)|0|
|[Enhancing E-commerce Spelling Correction with Fine-Tuned Transformer Models](https://doi.org/10.1145/3637528.3671625)|Arnab Dutta, Gleb Polushin, Xiaoshuang Zhang, Daniel Stein|eBay GmbH, Dreilinden, Germany; eBay Inc., Shanghai, China; eBay GmbH, Aachen, Germany|In the realm of e-commerce, the process of search stands as the primary point of interaction for users, wielding a profound influence on the platform's revenue generation. Notably, spelling correction assumes a pivotal role in shaping the user's search experience by rectifying erroneous query inputs, thus facilitating more accurate retrieval outcomes. Within the scope of this research paper, our aim is to enhance the existing state-of-the-art discriminative model performance with generative modelling strategies while concurrently addressing the engineering concerns associated with real-time online latency, inherent to models of this category. We endeavor to refine LSTM-based classification models for spelling correction through a generative fine-tuning approach hinged upon pre-trained language models. Our comprehensive offline assessments have yielded compelling results, showcasing that transformer-based architectures, such as BART (developed by Facebook) and T5 (a product of Google), have achieved a 4% enhancement in F1 score compared to baseline models for the English language sites. Furthermore, to mitigate the challenges posed by latency, we have incorporated model pruning techniques like no-teacher distillation. We have undertaken the deployment of our model (English only) as an A/B test candidate for real-time e-commerce traffic, encompassing customers from the US and the UK. The model attest to a 100% successful request service rate within real-time scenarios, with median, 90th percentile, and 99th percentile (p90/p99) latencies comfortably falling below production service level agreements. Notably, these achievements are further reinforced by positive customer engagement, transactional and search page metrics, including a significant reduction in instances of search results page with low or almost zero recall. Moreover, we have also extended our efforts into fine-tuning a multilingual model, which, notably, exhibits substantial accuracy enhancements, amounting to a minimum of 16%, across four distinct European languages and English.|在电子商务领域，搜索过程是用户交互的主要点，对平台的收入产生深远的影响。值得注意的是，拼写纠正通过纠正错误的查询输入，在塑造用户的搜索体验方面扮演着关键的角色，从而促进更准确的检索结果。在这篇研究论文的范围内，我们的目标是通过生成建模策略来提高现有的最先进的判别模型性能，同时解决与这类模型固有的实时在线延迟相关的工程问题。我们致力于完善基于 LSTM 的拼写校正分类模型，通过一种基于预训练语言模型的生成式微调方法。我们全面的离线评估已经产生了引人注目的结果，表明基于转换器的架构，如 BART (由 Facebook 开发)和 T5(谷歌的产品) ，与英语网站的基线模型相比，F1得分提高了4% 。此外，为了缓解延迟带来的挑战，我们已经采用了模型修剪技术，如非教师精馏。我们已经开始部署我们的模型(只有英语)作为一个 A/B 测试的实时电子商务流量的候选人，包括来自美国和英国的客户。该模型证明了在实时场景中100% 的成功请求服务率，中位数，第90百分位数和第99百分位数(p90/p99)延迟轻松地低于生产服务水平协议。值得注意的是，这些成就进一步得到了积极的客户参与度、交易和搜索页面指标的加强，包括搜索结果页面实例的显著减少，这些实例的召回率很低或几乎为零。此外，我们还将努力扩展到对多语言模型进行微调，特别是在四种不同的欧洲语言和英语之间，该模型显示出大幅度的准确性增强，最低达到16% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+E-commerce+Spelling+Correction+with+Fine-Tuned+Transformer+Models)|0|
|[Personalised Drug Identifier for Cancer Treatment with Transformers using Auxiliary Information](https://doi.org/10.1145/3637528.3671652)|Aishwarya Jayagopal, Hansheng Xue, Ziyang He, Robert J. Walsh, Krishna Kumar Hariprasannan, David Shao Peng Tan, Tuan Zea Tan, Jason J. Pitt, Anand D. Jeyasekharan, Vaibhav Rajan|National University Cancer Institute, Singapore, Singapore; National University of Singapore, Singapore, Singapore; Cancer Science Institute of Singapore, Singapore, Singapore|Cancer remains a global challenge due to its growing clinical and economic burden. Its uniquely personal manifestation, which makes treatment difficult, has fuelled the quest for personalized treatment strategies. Thus, genomic profiling is increasingly becoming part of clinical diagnostic panels. Effective use of such panels requires accurate drug response prediction (DRP) models, which are challenging to build due to limited labelled patient data. Previous methods to address this problem have used various forms of transfer learning. However, they do not explicitly model the variable length sequential structure of the list of mutations in such diagnostic panels. Further, they do not utilize auxiliary information (like patient survival) for model training. We address these limitations through a novel transformer-based method, which surpasses the performance of state-of-the-art DRP models on benchmark data. Code for our method is available at https://github.com/CDAL-SOC/PREDICT-AI. We also present the design of a treatment recommendation system (TRS), which is currently deployed at the National University Hospital, Singapore and is being evaluated in a clinical trial. We discuss why the recommended drugs and their predicted scores alone, obtained from DRP models, are insufficient for treatment planning. Treatment planning for complex cancer cases, in the face of limited clinical validation, requires assessment of many other factors, including several indirect sources of evidence on drug efficacy. We discuss key lessons learnt on model validation and use of indirect supporting evidence to build clinicians' trust and aid their decision making.|癌症由于其日益增长的临床和经济负担，仍然是一个全球性的挑战。它独特的个人表现使得治疗变得困难，也促进了对个性化治疗策略的探索。因此，基因组图谱正日益成为临床诊断小组的一部分。有效使用这些小组需要准确的药物反应预测(DRP)模型，这是具有挑战性的建立由于有限的标记患者数据。以往解决这一问题的方法都采用了各种形式的迁移学习。然而，他们没有明确地模拟这些诊断小组中突变列表的可变长度顺序结构。此外，他们不利用辅助信息(如患者生存)的模型训练。我们通过一种新的基于变压器的方法来解决这些局限性，该方法在基准数据上的性能超过了最先进的 DRP 模型。我们方法的代码可在 https://github.com/cdal-soc/predict-ai 下载。我们还介绍了一个治疗推荐系统(TRS)的设计，该系统目前部署在新加坡国立大学医院，正在进行临床试验评估。我们讨论为什么推荐的药物和他们的预测评分单独从 DRP 模型获得，不足以进行治疗计划。复杂癌症病例的治疗计划，面对有限的临床验证，需要评估许多其他因素，包括药物疗效的几个间接证据来源。我们讨论在模型验证和使用间接支持证据建立临床医生的信任和帮助他们的决策的关键经验教训。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalised+Drug+Identifier+for+Cancer+Treatment+with+Transformers+using+Auxiliary+Information)|0|
|[ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems](https://doi.org/10.1145/3637528.3671571)|Pengyue Jia, Yejing Wang, Zhaocheng Du, Xiangyu Zhao, Yichao Wang, Bo Chen, Wanyu Wang, Huifeng Guo, Ruiming Tang|Huawei Noah's Ark Lab, Shenzhen, China; City University of Hong Kong, Hong Kong, China|Deep Recommender Systems (DRS) are increasingly dependent on a large number of feature fields for more precise recommendations. Effective feature selection methods are consequently becoming critical for further enhancing the accuracy and optimizing storage efficiencies to align with the deployment demands. This research area, particularly in the context of DRS, is nascent and faces three core challenges. Firstly, variant experimental setups across research papers often yield unfair comparisons, obscuring practical insights. Secondly, the existing literature's lack of detailed analysis on selection attributes, based on large-scale datasets and a thorough comparison among selection techniques and DRS backbones, restricts the generalizability of findings and impedes deployment on DRS. Lastly, research often focuses on comparing the peak performance achievable by feature selection methods. This approach is typically computationally infeasible for identifying the optimal hyperparameters and overlooks evaluating the robustness and stability of these methods. To bridge these gaps, this paper presents ERASE, a comprehensive bEnchmaRk for feAture SElection for DRS. ERASE comprises a thorough evaluation of eleven feature selection methods, covering both traditional and deep learning approaches, across four public datasets, private industrial datasets, and a real-world commercial platform, achieving significant enhancement. Our code is available online for ease of reproduction.|深度推荐系统(DRS)越来越依赖于大量的特征字段以获得更精确的推荐。因此，有效的特征选择方法对于进一步提高准确性和优化存储效率以满足部署需求变得至关重要。这一研究领域，特别是在 DRS 的背景下，还处于起步阶段，面临着三个核心挑战。首先，不同研究论文的不同实验设置往往产生不公平的比较，模糊了实践的洞察力。其次，现有文献缺乏对选择属性的详细分析，基于大规模的数据集，对选择技术和 DRS 骨干进行了深入的比较，限制了研究结果的普遍性，阻碍了 DRS 的部署。这种方法通常是计算不可行的，以确定最佳的超参数和忽略评估这些方法的稳健性和稳定性。为了弥补这些差距，本文提出了 ERASE，一个全面的 bEnchmaRk 特征选择为 DRS ERASE 包括十一个特征选择方法的彻底评估，涵盖传统和深度学习方法，四个公共数据集，私人工业数据集和一个真实世界的商业平台，实现了显着的增强。我们的代码可以在线复制。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ERASE:+Benchmarking+Feature+Selection+Methods+for+Deep+Recommender+Systems)|0|
|[Beyond Binary Preference: Leveraging Bayesian Approaches for Joint Optimization of Ranking and Calibration](https://doi.org/10.1145/3637528.3671577)|Chang Liu, Qiwei Wang, Wenqing Lin, Yue Ding, Hongtao Lu|Tencent, Shenzhen, China; Shanghai Jiao Tong University, Shanghai, China|Predicting click-through rate (CTR) is a critical task in recommendation systems, where the models are optimized with pointwise loss to infer the probability of items being clicked. In industrial practice, applications also require ranking items based on these probabilities. Existing solutions primarily combine the ranking-based loss, i.e., pairwise and listwise loss, with CTR prediction. However, they can hardly calibrate or generalize well in CTR scenarios where the clicks reflect the binary preference. This is because the binary click feedback leads to a large number of ties, which renders high data sparsity. In this paper, we propose an effective data augmentation strategy, named Beyond Binary Preference (BBP) training framework, to address this problem. Our key idea is to break the ties by leveraging Bayesian approaches, where the beta distribution models click behavior as probability distributions in the training data that naturally break ties. Therefore, we can obtain an auxiliary training label that generates more comparable pairs and improves the ranking performance. Besides, BBP formulates ranking and calibration as a multi-task framework to optimize both objectives simultaneously. Through extensive offline experiments and online tests on various datasets, we demonstrate that BBP significantly outperforms state-of-the-art methods in both ranking and calibration capabilities, showcasing its effectiveness in addressing the limitations of existing methods. Our code is available at https://github.com/AlvinIsonomia/BBP.|在推荐系统中，预测点进率(ctrl)是一项关键的任务，模型通过逐点丢失的方式进行优化，以推断项目被点击的概率。在工业实践中，应用程序还需要根据这些概率对项目进行排序。现有的解决方案主要结合了基于排名的损失，即成对损失和列表损失，以及 CTR 预测。然而，在点击反映二进制偏好的 CTR 场景中，它们很难校准或推广。这是因为二进制单击反馈会导致大量关联，从而导致高数据稀疏性。针对这一问题，本文提出了一种有效的数据增强策略——超越二进制偏好(Beyond Binary Preferences，BBP)训练框架。我们的主要想法是通过利用贝叶斯方法来打破这种联系，在贝叶斯方法中，Β分布模型将行为作为训练数据中的概率分布来点击，从而自然地打破联系。因此，我们可以得到一个辅助训练标签，生成更多的可比对，提高排序性能。此外，BBP 作为一个多任务框架制定排序和校准，以优化两个目标同时进行。通过大量的离线实验和对各种数据集的在线测试，我们证明了 BBP 在排序和校准能力方面显著优于最先进的方法，展示了它在解决现有方法的局限性方面的有效性。我们的代码可以在 https://github.com/alvinisonomia/bbp 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Beyond+Binary+Preference:+Leveraging+Bayesian+Approaches+for+Joint+Optimization+of+Ranking+and+Calibration)|0|
|[Optimizing Novelty of Top-k Recommendations using Large Language Models and Reinforcement Learning](https://doi.org/10.1145/3637528.3671618)|Amit Sharma, Hua Li, Xue Li, Jian Jiao|Microsoft Bing Ads, Mountain View, USA; Microsoft Research, Bengaluru, India; Microsoft Bing Ads, Redmond, USA|Given an input query, a recommendation model is trained using user feedback data (e.g., click data) to output a ranked list of items. In real-world systems, besides accuracy, an important consideration for a new model is novelty of its top-k recommendations w.r.t. an existing deployed model. However, novelty of top-k items is a difficult goal to optimize a model for, since it involves a non-differentiable sorting operation on the model's predictions. Moreover, novel items, by definition, do not have any user feedback data. Given the semantic capabilities of large language models, we address these problems using a reinforcement learning (RL) formulation where large language models provide feedback for the novel items. However, given millions of candidate items, the sample complexity of a standard RL algorithm can be prohibitively high. To reduce sample complexity, we reduce the top-k list reward to a set of item-wise rewards and reformulate the state space to consist of tuples such that the action space is reduced to a binary decision; and show that this reformulation results in a significantly lower complexity when the number of items is large. We evaluate the proposed algorithm on improving novelty for a query-ad recommendation task on a large-scale search engine. Compared to supervised finetuning on recent pairs, the proposed RL-based algorithm leads to significant novelty gains with minimal loss in recall. We obtain similar results on the ORCAS query-webpage matching dataset and a product recommendation dataset based on Amazon reviews.|给定一个输入查询，使用用户反馈数据(例如，单击数据)来训练推荐模型，以输出一个排序的项目列表。在现实世界的系统中，除了准确性之外，新模型的一个重要考虑因素是它的 top-k 推荐 W.r.t. (现有的部署模型)的新颖性。然而，前 k 项的新颖性是一个很难优化模型的目标，因为它涉及到对模型预测的不可微排序操作。此外，根据定义，新项目没有任何用户反馈数据。考虑到大型语言模型的语义能力，我们使用强化学习(RL)公式来解决这些问题，其中大型语言模型为新项目提供反馈。然而，给定数百万个候选项，标准 RL 算法的样本复杂度可能高得令人望而却步。为了降低样本复杂度，我们将 top-k 列表奖励减少为一组项目奖励，并将状态空间重新表述为由元组组成的状态空间，从而将操作空间减少为二进制决策; 结果表明，当项目数量较大时，这种重新表述显著降低了复杂度。针对大规模搜索引擎中的查询广告推荐任务，评估了该算法的新颖性。与最近对的监督微调相比，本文提出的基于 RL 的算法在召回损失最小的情况下获得了显著的新颖性增益。在 ORCAS 查询-网页匹配数据集和基于 Amazon 评论的产品推荐数据集上，我们得到了类似的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+Novelty+of+Top-k+Recommendations+using+Large+Language+Models+and+Reinforcement+Learning)|0|
|[Measuring an LLM's Proficiency at using APIs: A Query Generation Strategy](https://doi.org/10.1145/3637528.3671592)|Ying Sheng, Sudeep Gandhe, Bhargav Kanagal, Nick Edmonds, Zachary Fisher, Sandeep Tata, Aarush Selvan|Google Research, Mountain View, CA, USA; Google, Mountain View, CA, USA|Connecting Large Language Models (LLMs) with the ability to leverage APIs (Web Search, Charting, Calculators, Calendar, Flight Search, Hotel Search, Data Lookup, etc. ) is likely to allow us to solve a variety of new hard problems. Several research efforts have made this observation and suggested recipes for LLMs to emit API calls, and proposed mechanisms by which they can generate additional text conditioned on the output for the API call. However, in practice, the focus has been on relatively simple slot-filling tasks that make an API call rather unlocking novel capabilities by combining different tools, reasoning over the response from a tool, making multiple invocations, or complex planning. In this paper, we pose the following question: what does it mean to say that an LLM is proficient at using a set of APIs? We answer this question in the context of structured APIs by defining seven capabilities for API-use. We provide an approach for generating synthetic tasks that exercise each of these capabilities given only the description of an API. We argue that this provides practitioners with a principled way to construct a dataset to evaluate an LLM's ability to use a given set of APIs. Through human evaluations, we show that our approach produces high-quality tasks for each of the seven capabilities. We also describe how we used this approach to on-board new API and create principled evaluation sets for multiple LLM-based products.|将大型语言模型(LLM)与能够利用 API (网络搜索、制图、计算器、日历、航班搜索、酒店搜索、数据查询等)联系起来，可能会让我们解决各种新的难题。一些研究工作已经做出了这样的观察，并提出了 LLM 发出 API 调用的方法，以及提出了一些机制，通过这些机制，LLM 可以根据 API 调用的输出生成额外的文本。然而，在实践中，重点一直放在相对简单的插槽填充任务上，这些任务通过组合不同的工具、对工具的响应进行推理、进行多次调用或复杂的计划，使 API 调用相当于解锁新功能。在本文中，我们提出以下问题: 说 LLM 精通使用一组 API 意味着什么？我们在结构化 API 的上下文中通过定义七种 API 使用功能来回答这个问题。我们提供了一种生成综合任务的方法，这些任务只需要对 API 进行描述，就可以实现这些功能中的每一个。我们认为，这为从业者提供了一种原则性的方法来构建数据集，以评估 LLM 使用给定 API 集的能力。通过人工评估，我们表明我们的方法为七种能力中的每一种都产生了高质量的任务。我们还描述了如何使用这种方法来开发新的 API，以及如何为多个基于 LLM 的产品创建原则性评估集。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Measuring+an+LLM's+Proficiency+at+using+APIs:+A+Query+Generation+Strategy)|0|
|[PEMBOT: Pareto-Ensembled Multi-task Boosted Trees](https://doi.org/10.1145/3637528.3671619)|Gokul Swamy, Anoop Saladi, Arunita Das, Shobhit Niranjan|Amazon, Bengaluru, KA, India; Amazon, Seattle, WA, USA|Multi-task problems frequently arise in machine learning when there are multiple target variables, which share a common synergy while being sufficiently different that optimizing on any of the task does not necessarily imply an optimum for the others. In this work, we develop PEMBOT, a novel Pareto-based multi-task classification framework using a gradient boosted tree architecture. The proposed methodology involves a) generating multiple instances of Pareto optimal trees, b) diverse subset selection using a determinantal point process (DPP) model, and c) ensembling of diverse Pareto optimal trees to yield the final output. We tested our framework on a problem from an e-commerce domain wherein the task is to predict at order placement time the different adverse scenarios in the order shipment journey such as the package getting lost or damaged during shipment. This model enables us to take preemptive measures to prevent these scenarios from happening resulting in significant operational cost savings. Further, to show the generality of our approach, we demonstrate the performance of our algorithm on a publicly available wine quality prediction dataset and compare against state-of-the-art baselines.|多任务问题经常出现在机器学习时，有多个目标变量，共享一个共同的协同作用，同时是充分不同的，优化任何一个任务并不一定意味着最优的其他。在这项工作中，我们开发 PEMBOT，一个新颖的基于 Pareto 的多任务分类框架使用梯度增强树结构。提出的方法包括 a)生成多个帕累托最优树的实例，b)使用行列式点过程(DPP)模型进行多样化的子集选择，以及 c)将不同的帕累托最优树集合起来以产生最终的输出。我们测试了我们的框架从一个电子商务领域的问题，其中的任务是预测在订单放置时间不同的不利情况下的订单装运旅程，如包裹丢失或损坏在装运期间。该模型使我们能够采取先发制人的措施，防止这些情况发生，从而大大节省运营成本。此外，为了展示我们方法的通用性，我们在一个公开的葡萄酒质量预测数据集上演示了我们算法的性能，并与最先进的基线进行了比较。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PEMBOT:+Pareto-Ensembled+Multi-task+Boosted+Trees)|0|
|[Enhancing Personalized Headline Generation via Offline Goal-conditioned Reinforcement Learning with Large Language Models](https://doi.org/10.1145/3637528.3671638)|Xiaoyu Tan, Leijun Cheng, Xihe Qiu, Shaojie Shi, Yuan Cheng, Wei Chu, Yinghui Xu, Yuan Qi|; AI3 Institute, Fudan University, Shanghai, China; INF Technology (Shanghai) Co., Ltd., Shanghai, China|Recently, significant advancements have been made in Large Language Models (LLMs) through the implementation of various alignment techniques. These techniques enable LLMs to generate highly tailored content in response to diverse user instructions. Consequently, LLMs have the potential to serve as robust, customizable recommendation systems in the field of content recommendation. However, using LLMs with user individual information and online exploration remains a challenge, which are important perspectives in developing personalized news headline generation algorithms. In this paper, we propose a novel framework to generate personalized news headlines using LLMs with extensive online exploration. The proposed approach involves initially training an offline goal-conditioned policy using supervised learning. Subsequently, online exploration is employed to collect new data for the next training iteration. Results from simulations, experiments, and real-word scenario demonstrate that our framework achieves outstanding performance on established benchmarks and can effectively generate personalized headlines under different reward settings. By treating the LLM as a goal-conditioned agent, the model can perform online exploration by modifying the goals without frequently retraining the model. To the best of our knowledge, this work represents the first investigation into the capability of LLMs to generate customized news headlines with goal-conditioned reinforcement learning via supervised learning within LLMs.|最近，通过实现各种对齐技术，大型语言模型(LLM)取得了重大进展。这些技术使 LLM 能够根据不同的用户指令生成高度定制的内容。因此，LLM 有可能成为内容推荐领域中健壮的、可定制的推荐系统。然而，利用具有用户个人信息和在线探索的 LLM 仍然是一个挑战，这是开发个性化新闻标题生成算法的重要视角。在本文中，我们提出了一个新的框架来生成个性化的新闻标题使用 LLM 广泛的在线探索。拟议中的方法包括最初使用监督式学习训练一种离线的以目标为条件的政策。随后，采用在线探索的方法为下一次训练迭代收集新的数据。模拟、实验和真实场景的结果表明，我们的框架在已建立的基准上取得了出色的性能，并且能够在不同的奖励设置下有效地生成个性化的标题。通过将 LLM 视为目标条件智能体，该模型可以通过修改目标进行在线探索，而无需频繁地对模型进行再训练。据我们所知，这项工作代表了首次调查的能力，生成定制的新闻标题与目标条件的强化学习通过监督式学习内部 LLM。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Personalized+Headline+Generation+via+Offline+Goal-conditioned+Reinforcement+Learning+with+Large+Language+Models)|0|
|[Future Impact Decomposition in Request-level Recommendations](https://doi.org/10.1145/3637528.3671506)|Xiaobei Wang, Shuchang Liu, Xueliang Wang, Qingpeng Cai, Lantao Hu, Han Li, Peng Jiang, Kun Gai, Guangming Xie|Kuaishou Technology, Beijing, China; Unaffiliated, Beijing, China; Peking University, Beijing, China|In recommender systems, reinforcement learning solutions have shown promisingresults in optimizing the interaction sequence between users and the systemover the long-term performance. For practical reasons, the policy's actions aretypically designed as recommending a list of items to handle users' frequentand continuous browsing requests more efficiently. In this list-wiserecommendation scenario, the user state is updated upon every request in thecorresponding MDP formulation. However, this request-level formulation isessentially inconsistent with the user's item-level behavior. In this study, wedemonstrate that an item-level optimization approach can better utilize itemcharacteristics and optimize the policy's performance even under therequest-level MDP. We support this claim by comparing the performance ofstandard request-level methods with the proposed item-level actor-criticframework in both simulation and online experiments. Furthermore, we show thata reward-based future decomposition strategy can better express the item-wisefuture impact and improve the recommendation accuracy in the long term. Toachieve a more thorough understanding of the decomposition strategy, we proposea model-based re-weighting framework with adversarial learning that furtherboost the performance and investigate its correlation with the reward-basedstrategy.|在推荐系统中，强化学习解决方案在优化用户与系统之间的交互顺序方面取得了令人满意的效果。出于实际原因，该策略的行动通常被设计为推荐一个项目列表，以更有效地处理用户频繁和连续的浏览请求。在这个列表-智能推荐场景中，用户状态会根据相应 MDP 公式中的每个请求进行更新。但是，这个请求级别的公式与用户的项目级别行为本质上是不一致的。研究表明，即使在请求级 MDP 下，项目级优化方法也能更好地利用项目特征，优化策略性能。我们通过比较标准请求级方法和提出的项目级参与者批评框架在模拟和在线实验中的性能来支持这一说法。进一步，我们发现基于奖励的未来分解策略能够更好地表达项目对未来的影响，从长远来看能够提高推荐的准确性。为了更深入地理解分解策略，我们提出了基于模型的对抗性学习重新加权框架，进一步提高绩效，并研究其与奖励策略的相关性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Future+Impact+Decomposition+in+Request-level+Recommendations)|0|
|[A Self-boosted Framework for Calibrated Ranking](https://doi.org/10.1145/3637528.3671570)|Shunyu Zhang, Hu Liu, Wentian Bao, Enyun Yu, Yang Song|Kuaishou Technology, Beijing, China; Columbia University, Beijing, China; Northeasten University, Beijing, China|Scale-calibrated ranking systems are ubiquitous in real-world applications nowadays, which pursue accurate ranking quality and calibrated probabilistic predictions simultaneously. For instance, in the advertising ranking system, the predicted click-through rate (CTR) is utilized for ranking and required to be calibrated for the downstream cost-per-click ads bidding. Recently, multi-objective based methods have been wildly adopted as a standard approach for Calibrated Ranking, which incorporates the combination of two loss functions: a pointwise loss that focuses on calibrated absolute values and a ranking loss that emphasizes relative orderings. However, when applied to industrial online applications, existing multi-objective CR approaches still suffer from two crucial limitations First, previous methods need to aggregate the full candidate list within a single mini-batch to compute the ranking loss. Such aggregation strategy violates extensive data shuffling which has long been proven beneficial for preventing overfitting, and thus degrades the training effectiveness. Second, existing multi-objective methods apply the two inherently conflicting loss functions on a single probabilistic prediction, which results in a sub-optimal trade-off between calibration and ranking. To tackle the two limitations, we propose a Self-Boosted framework for Calibrated Ranking (SBCR). In SBCR, the predicted ranking scores by the online deployed model are dumped into context features. With these additional context features, each single item can perceive the overall distribution of scores in the whole ranking list, so that the ranking loss can be constructed without the need for sample aggregation. As the deployed model is a few versions older than the training model, the dumped predictions reveal what was failed to learn and keep boosting the model to correct previously mis-predicted items. Moreover, a calibration module is introduced to decouple the point loss and ranking loss. The two losses are applied before and after the calibration module separately, which elegantly addresses the sub-optimal trade-off problem. We conduct comprehensive experiments on industrial scale datasets and online A/B tests, demonstrating that SBCR can achieve advanced performance on both calibration and ranking. Our method has been deployed on the video search system of Kuaishou, and results in significant performance improvements on CTR and the total amount of time users spend on Kuaishou.|标度校准的排序系统在现实世界的应用中是普遍存在的，它追求准确的排序质量和校准的概率预测同时进行。例如，在广告排名系统中，预测点进率(ctrl)被用于排名，并且需要根据下游按点击次数计费的广告投标进行校准。最近，基于多目标的方法已被广泛采用作为标准方法校准排名，其中包括两个损失函数的组合: 点态损失的重点是校准绝对值和排名损失的重点是相对排序。然而，现有的多目标 CR 方法在应用于工业在线应用时，仍然存在两个关键的局限性。首先，以前的方法需要在一个小批量内聚合完整的候选名单来计算排名损失。这种聚合策略违反了长期以来被证明有利于防止过拟合的广泛的数据重组策略，从而降低了训练效率。其次，现有的多目标方法将两个内在冲突的损失函数应用于单一的概率预测，导致校准和排序之间的次优平衡。为了解决这两个限制，我们提出了一个自我增强的校准排名(SBCR)框架。在 SBCR，通过在线部署模型预测的排名分数被转化为上下文特征。通过这些附加的上下文特征，每个单独的项目可以感知整个排名列表中分数的总体分布，从而不需要样本聚合就可以构造出排名损失。由于部署的模型比训练模型旧了几个版本，倾销预测揭示了未能学习的内容，并不断提升模型以纠正先前错误预测的项目。同时，引入了一个标定模块，实现了点损和等级损的解耦。这两种损耗分别应用于校准模块之前和之后，从而巧妙地解决了次优折衷问题。我们对工业规模的数据集和在线 A/B 测试进行了全面的实验，结果表明 SBCR 在校准和排序方面都具有较高的性能。我们的方法已经应用于 Kuaishou 的视频搜索系统，在点击率和用户在 Kuaishou 花费的总时间方面取得了显著的性能改善。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Self-boosted+Framework+for+Calibrated+Ranking)|0|
|[Bringing Multimodality to Amazon Visual Search System](https://doi.org/10.1145/3637528.3671640)|Xinliang Zhu, ShengWei Huang, Han Ding, Jinyu Yang, Kelvin Chen, Tao Zhou, Tal Neiman, Ouye Xie, Son Tran, Benjamin Z. Yao, Douglas Gray, Anuj Bindal, Arnab Dhua|Amazon.com, New York, New York, USA; Amazon.com, Seattle, WA, USA; Amazon.com, Palo Alto, CA, USA; Amazon.com, Santa Clara, CA, USA|Image to image matching has been well studied in the computer vision community. Previous studies mainly focus on training a deep metric learning model matching visual patterns between the query image and gallery images. In this study, we show that pure image-to- image matching suffers from false positives caused by matching to local visual patterns. To alleviate this issue, we propose to leverage recent advances in vision-language pretraining research. Specifically, we introduce additional image-text alignment losses into deep metric learning, which serve as constraints to the image-to-image matching loss. With additional alignments between the text (e.g., product title) and image pairs, the model can learn concepts from both modalities explicitly, which avoids matching low-level visual features. We progressively develop two variants, a 3-tower and a 4-tower model, where the latter takes one more short text query input. Through extensive experiments, we show that this change leads to a substantial improvement to the image to image matching problem. We further leveraged this model for multimodal search, which takes both image and reformulation text queries to improve search quality. Both offline and online experiments show strong improvements on the main metrics. Specifically, we see 4.95% relative improvement on image matching click through rate with the 3-tower model and 1.13% further improvement from the 4-tower model.|图像到图像的匹配已经在计算机视觉领域得到了很好的研究。以往的研究主要集中在训练一个匹配查询图像和画廊图像视觉模式的深度度量学习模型。在本研究中，我们发现纯粹的图像对图像的匹配会受到局部视觉模式匹配所引起的假阳性的影响。为了缓解这一问题，我们建议利用视觉语言预训研究的最新进展。具体地说，我们在深度度量学习中引入了额外的图像-文本对齐损失，作为图像-图像匹配损失的约束条件。通过在文本(例如，产品标题)和图像对之间进行额外的对齐，模型可以显式地从两种模式中学习概念，从而避免匹配低层次的视觉特征。我们逐步开发了两个变体，一个3塔模型和一个4塔模型，后者需要更多的短文本查询输入。通过大量的实验，我们发现这种改变使得图像到图像的匹配问题得到了实质性的改善。我们进一步利用该模型进行多模态搜索，该模型同时采用图像查询和重构文本查询来提高搜索质量。离线和在线实验都显示了主要指标的强大改进。具体来说，我们发现三塔模型的图像匹配点击率相对提高了4.95% ，而四塔模型的图像匹配点击率进一步提高了1.13% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bringing+Multimodality+to+Amazon+Visual+Search+System)|0|
|[A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models](https://doi.org/10.1145/3637528.3671470)|Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, TatSeng Chua, Qing Li|The Hong Kong Polytechnic University, Hong Kong, China; Baidu Inc., Beijing, CN; National university of Singapore, Singapore, SG|As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at: https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/|作为人工智能中最先进的技术之一，检索增强生成(RAG)技术可以提供可靠的、最新的外部知识，为大量的任务提供巨大的方便。特别是在人工智能生成内容(AIGC)的时代，强大的检索能力提供额外的知识使 RAG 能够协助现有的生成人工智能产生高质量的输出。近年来，大语言模型(LLM)在语言理解和语言生成方面显示出了革命性的能力，但仍然面临着诸如幻觉和过时的内部知识等固有的局限性。考虑到 RAG 在提供最新和有用的辅助信息方面的强大能力，检索增强大型语言模型(RA-LLM)已经出现，以利用外部和权威的知识库，而不是仅仅依赖于模型的内部知识，以增强 LLM 生成内容的质量。在这项调查中，我们全面回顾了 RA-LLM 的现有研究，包括三个主要的技术视角: 此外，为了提供更深入的见解，我们讨论了当前的局限性和未来研究的几个有希望的方向。有关这项调查的最新资料，可浏览以下 https://advanced-recommender-systems.github.io/rag-meets-llms/ :|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Survey+on+RAG+Meeting+LLMs:+Towards+Retrieval-Augmented+Large+Language+Models)|0|
|[Multimodal Pretraining, Adaptation, and Generation for Recommendation: A Survey](https://doi.org/10.1145/3637528.3671473)|Qijiong Liu, Jieming Zhu, Yanting Yang, Quanyu Dai, Zhaocheng Du, XiaoMing Wu, Zhou Zhao, Rui Zhang, Zhenhua Dong|Huawei Noah Ark Lab, Shenzhen, China; The HK PolyU, Hong Kong, China; Huawei Noah's Ark Lab, Shenzhen, China; Huazhong University of Science and Technology & ruizhang.info, Shenzhen, China; Zhejiang University, Hangzhou, China|Personalized recommendation serves as a ubiquitous channel for users to discover information tailored to their interests. However, traditional recommendation models primarily rely on unique IDs and categorical features for user-item matching, potentially overlooking the nuanced essence of raw item contents across multiple modalities such as text, image, audio, and video. This underutilization of multimodal data poses a limitation to recommender systems, especially in multimedia services like news, music, and short-video platforms. The recent advancements in large multimodal models offer new opportunities and challenges in developing content-aware recommender systems. This survey seeks to provide a comprehensive exploration of the latest advancements and future trajectories in multimodal pretraining, adaptation, and generation techniques, as well as their applications in enhancing recommender systems. Furthermore, we discuss current open challenges and opportunities for future research in this dynamic domain. We believe that this survey, alongside the curated resources, will provide valuable insights to inspire further advancements in this evolving landscape.|个性化推荐是用户发现适合自己兴趣的信息的一个无处不在的渠道。然而，传统的推荐模型主要依赖于用户项匹配的唯一 ID 和分类特性，这可能忽略了原始项目内容在多种模式(如文本、图像、音频和视频)中的细微差别。这种多模式数据的利用率不足对推荐系统造成了限制，特别是在新闻、音乐和短视频平台等多媒体服务中。大型多模式模型的最新进展为开发内容感知的推荐系统提供了新的机遇和挑战。本调查旨在全面探讨多模式预培训、适应和生成技术的最新进展和未来发展轨迹，以及它们在加强推荐系统方面的应用。此外，我们讨论了目前在这一动态领域的开放性挑战和未来研究的机遇。我们相信，这项调查，以及策划的资源，将提供宝贵的见解，以激励在这个不断变化的景观进一步发展。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multimodal+Pretraining,+Adaptation,+and+Generation+for+Recommendation:+A+Survey)|0|
|[AI for Education (AI4EDU): Advancing Personalized Education with LLM and Adaptive Learning](https://doi.org/10.1145/3637528.3671498)|Qingsong Wen, Jing Liang, Carles Sierra, Rose Luckin, Richard Jiarui Tong, Zitao Liu, Peng Cui, Jiliang Tang|Squirrel Ai Learning, Bellevue, USA; University College London, London, United Kingdom; Michigan State University, East Lansing, USA; Tsinghua University, Beijing, China; Squirrel Ai Learning, Shanghai, China; Jinan University, Guangzhou, China; IIIA of the Spanish National Research Council, Barcelona, Spain|Recent advanced AI technologies, especially large language models (LLMs) like GPTs, have significantly advanced the field of data mining and led to the development of various LLM-based applications. AI for education (AI4EDU) is a vibrant multi-disciplinary field of data mining, machine learning, and education, with increasing importance and extraordinary potential. In this field, LLM and adaptive learning-based models can be utilized as interfaces in human-in-the-loop education systems, where the model serves as a mediator among the teacher, students, and machine capabilities, including its own. This perspective has several benefits, including the ability to personalize interactions, allow unprecedented flexibility and adaptivity for human-AI collaboration and improve the user experience. However, several challenges still exist, including the need for more robust and efficient algorithms, designing effective user interfaces, and ensuring ethical considerations are addressed. This workshop aims to bring together researchers and practitioners from academia and industry to explore cutting-edge AI technologies for personalized education, especially the potential of LLMs and adaptive learning technologies.|近年来，先进的人工智能技术，特别是大型语言模型(LLM) ，如 GPTs，极大地推动了数据挖掘领域的发展，并导致了各种基于 LLM 的应用程序的开发。人工智能教育(AI4EDU)是一个充满活力的多学科领域的数据挖掘，机器学习和教育，越来越重要和非凡的潜力。在这个领域，LLM 和基于自适应学习的模型可以用作人在环路教育系统的接口，在这个系统中，模型充当教师、学生和机器能力(包括它自己的能力)之间的中介。这种视角有几个好处，包括个性化交互的能力，允许前所未有的灵活性和人工智能协作的适应性，并改善用户体验。然而，仍然存在一些挑战，包括需要更加健壮和高效的算法，设计有效的用户界面，并确保道德考虑得到解决。这个工作坊旨在汇聚来自学术界和业界的研究人员和从业员，探讨尖端人工智能技术在个人化教育方面的应用，特别是 LLM 和在线机机器学习技术的潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AI+for+Education+(AI4EDU):+Advancing+Personalized+Education+with+LLM+and+Adaptive+Learning)|0|
|[Understanding Inter-Session Intentions via Complex Logical Reasoning](https://doi.org/10.1145/3637528.3671808)|Jiaxin Bai, Chen Luo, Zheng Li, Qingyu Yin, Yangqiu Song|Department of CSE, HKUST, Hong Kong, China; Amazon.com Inc, Palo Alto, USA|Understanding user intentions is essential for improving product recommendations, navigation suggestions, and query reformulations. However, user intentions can be intricate, involving multiple sessions and attribute requirements connected by logical operators such as And, Or, and Not. For instance, a user may search for Nike or Adidas running shoes across various sessions, with a preference for purple. In another example, a user may have purchased a mattress in a previous session and is now looking for a matching bed frame without intending to buy another mattress. Existing research on session understanding has not adequately addressed making product or attribute recommendations for such complex intentions. In this paper, we present the task of logical session complex query answering (LS-CQA), where sessions are treated as hyperedges of items, and we frame the problem of complex intention understanding as an LS-CQA task on an aggregated hypergraph of sessions, items, and attributes. This is a unique complex query answering task with sessions as ordered hyperedges. We also introduce a new model, the Logical Session Graph Transformer (LSGT), which captures interactions among items across different sessions and their logical connections using a transformer structure. We analyze the expressiveness of LSGT and prove the permutation invariance of the inputs for the logical operators. By evaluating LSGT on three datasets, we demonstrate that it achieves state-of-the-art results.|理解用户的意图对于改进产品推荐、导航建议和查询重新编排至关重要。但是，用户的意图可能很复杂，涉及多个会话和属性需求，这些需求由逻辑运算符连接，例如 And、 Or 和 Not。例如，用户可能在不同的会话中搜索耐克或阿迪达斯的跑鞋，偏爱紫色。在另一个例子中，用户可能已经在前一个阶段购买了一个床垫，现在正在寻找一个匹配的床架，而无意购买另一个床垫。关于会话理解的现有研究尚未充分解决为这种复杂意图提供产品或属性建议的问题。本文提出了逻辑会话复杂查询回答(LS-CQA)任务，其中会话被视为项目的超边缘，并将复杂意图理解问题框架为会话、项目和属性聚合超图上的 LS-CQA 任务。这是一个独特的复杂查询应答任务，会话是有序的超边缘。我们还介绍了一个新模型，逻辑会话图形转换器(Logical Session Graph former，LSGT) ，它使用转换器结构捕获跨不同会话的项之间的交互及其逻辑连接。分析了 LSGT 的表达性，证明了逻辑算子输入的置换不变性。通过对三个数据集上的 LSGT 进行评估，我们证明它达到了最先进的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+Inter-Session+Intentions+via+Complex+Logical+Reasoning)|0|
|[Online Preference Weight Estimation Algorithm with Vanishing Regret for Car-Hailing in Road Network](https://doi.org/10.1145/3637528.3671664)|Yucen Gao, Zhehao Zhu, Mingqian Ma, Fei Gao, Hui Gao, Yangguang Shi, Xiaofeng Gao|Northwestern University, Evanston, IL, USA; Didi Global Inc., Beijing, China; Shandong University, Qingdao, China; Shanghai Jiao Tong University, Shanghai, China|Car-hailing services play an important role in the modern transportation system, and the utilities of the service providers highly depend on the efficiency of route planning algorithms. A widely adopted route planning framework is to assign weights to roads and compute the routes with the shortest path algorithms. Existing techniques of weight-assigning often focus on the traveling time and length of the roads, but cannot incorporate with the preferences of the passengers (users). In this paper, a set of preference weight estimation models is employed to capture the users' preferences over paths in car-hailing with their historical choices. Since the user preferences may vary dynamically over time, it is a challenging task to make real-time decisions over the models. The main technical contribution of this paper is to propose an online learning-based preference weight chasing (PWC) algorithm to solve this problem. The worst-case performance of PWC is analyzed with the metric regret, and it is proved that PWC has a vanishing regret, which means that the time-averaged loss concerning the fixed in-hindsight best model tends to zero. Experiments based on real-world datasets are conducted to verify the effectiveness and efficiency of our algorithm. The code is available at https://github.com/GaoYucen/PWC.|叫车服务在现代交通系统中占有举足轻重的地位，路径规划算法的有效性直接决定了叫车服务提供商的效用。一种广泛采用的路径规划框架是给道路赋权，并用最短路径算法计算路径。现有的权重分配技术往往侧重于道路的行驶时间和长度，但不能与乘客(用户)的偏好相结合。本文利用一组偏好权重估计模型，通过用户的历史选择来捕捉用户对叫车路径的偏好。由于用户偏好可能随时间动态变化，因此在模型上实时做出决策是一项具有挑战性的任务。本文的主要技术贡献是提出了一种基于在线学习的偏好权重追踪(PWC)算法来解决这一问题。利用度量遗憾对 PWC 的最坏情况性能进行了分析，证明了 PWC 具有消失遗憾，这意味着与固定的事后最佳模型相关的时间平均损失趋于零。基于实际数据集进行了实验，验证了算法的有效性和高效性。密码可在 https://github.com/gaoyucen/pwc 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+Preference+Weight+Estimation+Algorithm+with+Vanishing+Regret+for+Car-Hailing+in+Road+Network)|0|
|[Robust Auto-Bidding Strategies for Online Advertising](https://doi.org/10.1145/3637528.3671729)|Qilong Lin, Zhenzhe Zheng, Fan Wu|Shanghai Jiao Tong University, Shanghai, China|In online advertising, existing auto-bidding strategies for bid shading mainly adopt the approach of first predicting the winning price distribution and then calculating the optimal bid. However, the winning price information available to the Demand Side Platforms (DSPs) is extremely limited, and the associated uncertainties make it challenging for DSPs to accurately estimate winning price distribution. To address this challenge, we conducted a comprehensive analysis of the process by which DSPs obtain winning price information, and abstracted two types of uncertainties from it: known uncertainty and unknown uncertainty. Based on these uncertainties, we proposed two levels of robust bidding strategies: Robust Bidding for Censorship (RBC) and Robust Bidding for Distribution Shift (RBDS), which offer guarantees for the surplus in the worst-case scenarios under uncertain conditions. Experimental results on public datasets demonstrate that our robust bidding strategies consistently enable DSPs to achieve superior surpluses, both on test sets and under worst-case conditions.|在网络广告中，现有的自动报价策略主要采用先预测中标价格分布，然后计算最优报价的方法。然而，需求侧平台(DSP)可获得的中标价格信息是极其有限的，并且相关的不确定性使得 DSP 准确估计中标价格分布具有挑战性。为了应对这一挑战，我们对 DSP 获取中标价格信息的过程进行了全面的分析，并从中提取出两类不确定性: 已知不确定性和未知不确定性。基于这些不确定性，我们提出了两个层次的鲁棒竞价策略: 鲁棒审查竞价(RBC)和鲁棒分配偏移竞价(RBDS) ，为不确定条件下最坏情况下的盈余提供保证。对公共数据集的实验结果表明，我们的稳健的投标策略始终使得 DSP 能够在测试集和最坏情况下实现优越的盈余。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Auto-Bidding+Strategies+for+Online+Advertising)|0|
|[QSketch: An Efficient Sketch for Weighted Cardinality Estimation in Streams](https://doi.org/10.1145/3637528.3671695)|Yiyan Qi, Rundong Li, Pinghui Wang, Yufang Sun, Rui Xing|International Digital Economy Academy (IDEA), Shenzhen, China; MOE KLINNS Lab, Shaanxi Normal University, Xi'an, China; MOE KLINNS Lab, Xi'an Jiaotong University, Xi'an, China|Estimating cardinality, i.e., the number of distinct elements, of a data stream is a fundamental problem in areas like databases, computer networks, and information retrieval. This study delves into a broader scenario where each element carries a positive weight. Unlike traditional cardinality estimation, limited research exists on weighted cardinality, with current methods requiring substantial memory and computational resources, challenging for devices with limited capabilities and real-time applications like anomaly detection. To address these issues, we propose QSketch, a memory-efficient sketch method for estimating weighted cardinality in streams. QSketch uses a quantization technique to condense continuous variables into a compact set of integer variables, with each variable requiring only 8 bits, making it 8 times smaller than previous methods. Furthermore, we leverage dynamic properties during QSketch generation to significantly enhance estimation accuracy and achieve a lower time complexity of O(1) for updating estimations upon encountering a new element. Experimental results on synthetic and real-world datasets show that QSketch is approximately 30% more accurate and two orders of magnitude faster than the state-of-the-art, using only 1/8 of the memory.|在数据库、计算机网络和信息检索等领域，估计数据流的基数(即不同元素的数量)是一个基本问题。这项研究深入到一个更广泛的情况下，其中每一个因素具有积极的权重。与传统的基数估计不同，对加权基数的研究有限，目前的方法需要大量的内存和计算资源，对于功能有限的设备和实时应用(如异常检测)具有挑战性。为了解决这些问题，我们提出了 QSketch，一种内存高效的草图方法，用于估计流中的加权基数。QSketch 使用量化技术将连续变量压缩为一组紧凑的整数变量，每个变量只需要8位，使其比以前的方法小8倍。此外，我们利用 QSketch 生成过程中的动态特性来显著提高估计精度，并且在遇到新元素时更新估计时降低了 O (1)的时间复杂度。在合成和真实数据集上的实验结果显示，QSketch 只使用了1/8的内存，比最先进的数量级提高了大约30% 的准确率和2% 的速度。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=QSketch:+An+Efficient+Sketch+for+Weighted+Cardinality+Estimation+in+Streams)|0|
|[Make Your Home Safe: Time-aware Unsupervised User Behavior Anomaly Detection in Smart Homes via Loss-guided Mask](https://doi.org/10.1145/3637528.3671708)|Jingyu Xiao, Zhiyao Xu, Qingsong Zou, Qing Li, Dan Zhao, Dong Fang, Ruoyu Li, Wenxin Tang, Kang Li, Xudong Zuo, Penghui Hu, Yong Jiang, Zixuan Weng, Michael R. Lyu|Beijing Jiaotong University, Beijing, China; Tsinghua Shenzhen International Graduate School, Shenzhen, China; Tsinghua University, Beijing, China; Tencent, Shenzhen, China; The Chinese University of Hong Kong, Hong Kong, China; Tsinghua Shenzhen International Graduate School & Peng Cheng Laboratory, Shenzhen, China; Xi'an University of Electronic Science and Technology, Xi'an, China; Peng Cheng Laborotary, Shenzhen, China; Peng Cheng Laboratory, Shenzhen, China; Peng Cheng Laboratory & Tsinghua Shenzhen International Graduate School, Shenzhen, China|Smart homes, powered by the Internet of Things, offer great convenience but also pose security concerns due to abnormal behaviors, such as improper operations of users and potential attacks from malicious attackers. Several behavior modeling methods have been proposed to identify abnormal behaviors and mitigate potential risks. However, their performance often falls short because they do not effectively learn less frequent behaviors, consider temporal context, or account for the impact of noise in human behaviors. In this paper, we propose SmartGuard, an autoencoder-based unsupervised user behavior anomaly detection framework. First, we design a Loss-guided Dynamic Mask Strategy (LDMS) to encourage the model to learn less frequent behaviors, which are often overlooked during learning. Second, we propose a Three-level Time-aware Position Embedding (TTPE) to incorporate temporal information into positional embedding to detect temporal context anomaly. Third, we propose a Noise-aware Weighted Reconstruction Loss (NWRL) that assigns different weights for routine behaviors and noise behaviors to mitigate the interference of noise behaviors during inference. Comprehensive experiments demonstrate that SmartGuard consistently outperforms state-of-the-art baselines and also offers highly interpretable results.|物联网驱动的智能家居不仅提供了极大的便利，而且由于用户的不正常操作和恶意攻击者的潜在攻击等不正常行为，也引起了安全方面的担忧。为了识别异常行为和降低潜在风险，人们提出了几种行为建模方法。然而，他们的表现往往不足，因为他们没有有效地学习较少的频繁行为，考虑时间背景，或说明噪音的影响，人类行为。在本文中，我们提出了 SmartGuard，一个基于自动编码器的无监督用户行为异常检测框架。首先，我们设计了一个损失引导的动态掩模策略(LDMS)来鼓励模型学习较少频繁的行为，这些行为在学习过程中经常被忽略。其次，提出了一种三级时间感知位置嵌入算法(TTPE) ，将时间信息融合到位置嵌入算法中，检测时间上下文异常。第三，提出了一种噪声感知加权重构损失(NWRL)算法，该算法对日常行为和噪声行为赋予不同的权重，以减轻推理过程中噪声行为的干扰。综合实验表明，SmartGuard 始终优于最先进的基线，并提供高度可解释的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Make+Your+Home+Safe:+Time-aware+Unsupervised+User+Behavior+Anomaly+Detection+in+Smart+Homes+via+Loss-guided+Mask)|0|
|[Top-Down Bayesian Posterior Sampling for Sum-Product Networks](https://doi.org/10.1145/3637528.3671876)|Soma Yokoi, Issei Sato|The University of Tokyo, Tokyo, Japan|Sum-product networks (SPNs) are probabilistic models characterized by exactand fast evaluation of fundamental probabilistic operations. Its superiorcomputational tractability has led to applications in many fields, such asmachine learning with time constraints or accuracy requirements and real-timesystems. The structural constraints of SPNs supporting fast inference, however,lead to increased learning-time complexity and can be an obstacle to buildinghighly expressive SPNs. This study aimed to develop a Bayesian learningapproach that can be efficiently implemented on large-scale SPNs. We derived anew full conditional probability of Gibbs sampling by marginalizing multiplerandom variables to expeditiously obtain the posterior distribution. Thecomplexity analysis revealed that our sampling algorithm works efficiently evenfor the largest possible SPN. Furthermore, we proposed a hyperparameter tuningmethod that balances the diversity of the prior distribution and optimizationefficiency in large-scale SPNs. Our method has improved learning-timecomplexity and demonstrated computational speed tens to more than one hundredtimes faster and superior predictive performance in numerical experiments onmore than 20 datasets.|和积网络(Sum-product network，SPN)是基本概率运算的精确和快速评估的概率模型拥有属性。其优越的计算易处理性在许多领域得到了应用，例如具有时间约束或精度要求的机器学习和实时系统。然而，支持快速推理的 SPN 的结构约束导致了学习时间复杂性的增加，并可能成为构建高表达性 SPN 的障碍。本研究旨在发展一个可以在大型 SPN 上有效执行的贝叶斯学习方法。我们通过边缘化多重随机变量得到了一个全新的吉布斯抽样条件概率，以便快速获得后验概率。复杂性分析表明，我们的抽样算法工作效率甚至最大的可能 SPN。在此基础上，提出了一种平衡先验分布多样性和优化效率的超参数调谐方法。在20多个数据集的数值实验中，该方法提高了学习时间的复杂度，并显示出数十倍至一百倍以上的计算速度和优越的预测性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Top-Down+Bayesian+Posterior+Sampling+for+Sum-Product+Networks)|0|
|[CompanyKG: A Large-Scale Heterogeneous Graph for Company Similarity Quantification](https://doi.org/10.1145/3637528.3671515)|Lele Cao, Vilhelm von Ehrenheim, Mark GranrothWilding, Richard Anselmo Stahl, Andrew McCornack, Armin Catovic, Dhiana Deva Cavalcanti Rocha|Motherbrain, EQT Group & QA.tech, Stockholm, Sweden; Motherbrain, EQT Group, Stockholm, Sweden; Motherbrain, EQT Group & Silo AI, Stockholm, Sweden|In the investment industry, it is often essential to carry out fine-grainedcompany similarity quantification for a range of purposes, including marketmapping, competitor analysis, and mergers and acquisitions. We propose andpublish a knowledge graph, named CompanyKG, to represent and learn diversecompany features and relations. Specifically, 1.17 million companies arerepresented as nodes enriched with company description embeddings; and 15different inter-company relations result in 51.06 million weighted edges. Toenable a comprehensive assessment of methods for company similarityquantification, we have devised and compiled three evaluation tasks withannotated test sets: similarity prediction, competitor retrieval and similarityranking. We present extensive benchmarking results for 11 reproduciblepredictive methods categorized into three groups: node-only, edge-only, andnode+edge. To the best of our knowledge, CompanyKG is the first large-scaleheterogeneous graph dataset originating from a real-world investment platform,tailored for quantifying inter-company similarity.|在投资行业，为了一系列目的(包括市场地图、竞争对手分析和併购)进行细粒度的公司相似性量化往往是必不可少的。我们提出并发布了一个知识图，命名为 CompanyKG，用于表示和学习公司的各种特征和关系。具体来说，117万家公司被表示为丰富了公司描述嵌入的节点; 15种不同的公司间关系产生了5106万个加权边。为了对企业相似性量化方法进行综合评价，我们设计并编制了三个评价任务: 相似性预测、竞争对手检索和相似性排名。我们提出了广泛的基准测试结果的11个可重复的预测方法分为三组: 节点只有，边缘，和节点 + 边缘。据我们所知，CompanyKG 是第一个来自现实世界投资平台的大规模异构图形数据集，专门用于量化公司间的相似性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CompanyKG:+A+Large-Scale+Heterogeneous+Graph+for+Company+Similarity+Quantification)|0|
|[CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset for Advancing Graph Machine Learning](https://doi.org/10.1145/3637528.3671538)|Ulrik FriisJensen, Frederik L. Johansen, Andy S. Anker, Erik B. Dam, Kirsten M. Ø. Jensen, Raghavendra Selvan|; Department of Computer Science, University of Copenhagen, Copenhagen, Denmark; Department of Chemistry, University of Copenhagen, Copenhagen, Denmark|Advances in graph machine learning (ML) have been driven by applications in chemistry, as graphs have remained the most expressive representations of molecules. This has led to progress within both fields, as challenging chemical data has helped improve existing methods and to develop new ones. While early graph ML methods focused primarily on small organic molecules, more recently, the scope of graph ML has expanded to include inorganic materials. Modelling the periodicity and symmetry of inorganic crystalline materials poses unique challenges, which existing graph ML methods are unable to immediately address. Moving to inorganic nanomaterials further increases complexity as the scale of number of nodes within each graph can be broad (10 to 100k). In addition, the bulk of existing graph ML focuses on characterising molecules and materials by predicting target properties with graphs as input. The most exciting applications of graph ML will be in their generative capabilities, in order to explore the vast chemical space from a data-driven perspective. Currently, generative modelling of graphs is not at par with other domains such as images or text, as generating chemically valid molecules and materials of varying properties is not straightforward. In this work, we invite the graph ML community to address these open challenges by presenting two new chemically-informed large-scale inorganic (CHILI) nanomaterials datasets. These datasets contain nanomaterials of different scales and properties represented as graphs of varying sizes. The first dataset is a medium-scale dataset (with overall >6M nodes, >49M edges) of mono-metallic oxide nanomaterials generated from 12 selected crystal types (CHILI-3K). This dataset has a narrower chemical scope focused on an interesting part of chemical space with a lot of active research. The second is a large-scale dataset (with overall >183M nodes, >1.2B edges) of nanomaterials generated from experimentally determined crystal structures (CHILI-100K). The crystal structures used in CHILI-100K are obtained from a curated subset of the Crystallography Open Database (COD) and has a broader chemical scope covering database entries for 68 metals and 11 non-metals. We define 11 property prediction tasks covering node-, edge-, and graph- level tasks that span classification and regression. In addition we also define structure prediction tasks, which are of special interest for nanomaterial research. We benchmark the performance of a wide array of baseline methods starting with simple baselines to multiple off-the-shelf graph neural networks. Based on these benchmarking results, we highlight areas which need future work to achieve useful performance for applications in (nano) materials chemistry. To the best of our knowledge, CHILI-3K and CHILI-100K are the first open-source nanomaterial datasets of this scale - both on the individual graph level and of the dataset as a whole - and the only nanomaterials datasets with high structural and elemental diversity.|图形机器学习(ML)的发展受到化学应用的推动，因为图形仍然是分子最有表现力的表现形式。这导致了这两个领域的进展，因为具有挑战性的化学数据有助于改进现有方法和开发新的方法。虽然早期的图形 ML 方法主要集中在小有机分子，最近，图形 ML 的范围已经扩大到包括无机材料。模拟无机晶体材料的周期性和对称性提出了独特的挑战，现有的图 ML 方法无法立即解决。转向无机纳米材料进一步增加了复杂性，因为每个图中节点的数量范围可以很宽(10到100k)。此外，现有的大部分图 ML 侧重于通过以图形作为输入来预测目标特性来表征分子和材料。图形机器学习最令人兴奋的应用将在于它们的生成能力，以便从数据驱动的角度探索广阔的化学空间。目前，图形的生成建模与图像或文本等其他领域不同，因为生成具有不同性质的化学有效分子和材料并不简单。在这项工作中，我们邀请图形机器学习社区解决这些开放的挑战，提出两个新的化学信息大规模无机(CHILI)纳米材料数据集。这些数据集包含不同尺度和性质的纳米材料，表示为不同尺寸的图形。第一个数据集是由12种选定的晶体类型(CHILI-3K)产生的单金属氧化物纳米材料的中等规模数据集(总体 > 6M 节点，> 49M 边)。这个数据集有一个较窄的化学范围集中在一个有趣的部分的化学空间与许多活跃的研究。第二个是由实验确定的晶体结构(CHILI-100K)产生的纳米材料的大规模数据集(总体 > 183M 节点，> 1.2 B 边)。CHILI-100K 中使用的晶体结构是从晶体学开放数据库(COD)的一个精选子集中获得的，并且具有更广泛的化学范围，包括68种金属和11种非金属的数据库条目。我们定义了11个属性预测任务，涵盖了分类和回归的节点、边和图级任务。此外，我们还定义了纳米材料研究特别感兴趣的结构预测任务。从简单的基线到多个现成的图形神经网络，我们测试了大量基线方法的性能。在这些基准测试结果的基础上，我们强调了在(纳米)材料化学中实现有用性能需要进一步工作的领域。据我们所知，CHILI-3K 和 CHILI-100K 是第一个这种规模的开源纳米材料数据集-无论是在单个图形水平还是整个数据集-也是唯一具有高结构和元素多样性的纳米材料数据集。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CHILI:+Chemically-Informed+Large-scale+Inorganic+Nanomaterials+Dataset+for+Advancing+Graph+Machine+Learning)|0|
|[Offline Reinforcement Learning for Optimizing Production Bidding Policies](https://doi.org/10.1145/3637528.3671555)|Dmytro Korenkevych, Frank Cheng, Artsiom Balakir, Alex Nikulkov, Lingnan Gao, Zhihao Cen, Zuobing Xu, Zheqing Zhu|AI at Meta, Sunnyvale, USA; Meta Platform Inc., Menlo Park, USA; AI at Meta, Menlo Park, USA; AI at Meta, Bellevue, USA|The online advertising market, with its thousands of auctions run per second, presents a daunting challenge for advertisers who wish to optimize their spend under a budget constraint. Thus, advertising platforms typically provide automated agents to their customers, which act on their behalf to bid for impression opportunities in real time at scale. Because these proxy agents are owned by the platform but use advertiser funds to operate, there is a strong practical need to balance reliability and explainability of the agent with optimizing power. We propose a generalizable approach to optimizing bidding policies in production environments by learning from real data using offline reinforcement learning. This approach can be used to optimize any differentiable base policy (practically, a heuristic policy based on principles which the advertiser can easily understand), and only requires data generated by the base policy itself. We use a hybrid agent architecture that combines arbitrary base policies with deep neural networks, where only the optimized base policy parameters are eventually deployed, and the neural network part is discarded after training. We demonstrate that such an architecture achieves statistically significant performance gains in both simulated and at-scale production bidding environments. Our approach does not incur additional infrastructure, safety, or explainability costs, as it directly optimizes parameters of existing production routines without replacing them with black box-style models like neural networks.|在线广告市场每秒钟有数千场拍卖，对于那些希望在预算线下优化支出的广告客户来说，这是一个艰巨的挑战。因此，广告平台通常向客户提供自动化代理，代表客户实时大规模地争取印象机会。由于这些代理商属于平台所有，但利用广告客户的资金进行运作，因此在代理商的可靠性和可解释性与优化权力之间存在着很强的现实需求。我们提出了一个通用的方法来优化投标政策在生产环境中通过学习使用离线强化学习的真实数据。这种方法可以用来优化任何可微的基本策略(实际上是一种基于广告商容易理解的原则的启发式策略) ，并且只需要基本策略本身生成的数据。我们采用混合智能体结构，将任意基本策略与深度神经网络相结合，最终只部署优化后的基本策略参数，训练后丢弃神经网络部分。我们证明了这样的体系结构在模拟和大规模生产投标环境中都能获得统计上显著的性能提高。我们的方法不会产生额外的基础设施、安全或可解释性成本，因为它直接优化了现有生产例程的参数，而没有用神经网络等黑箱模型来替代它们。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Offline+Reinforcement+Learning+for+Optimizing+Production+Bidding+Policies)|0|
|[Spending Programmed Bidding: Privacy-friendly Bid Optimization with ROI Constraint in Online Advertising](https://doi.org/10.1145/3637528.3671540)|Yumin Su, Min Xiang, Yifei Chen, Yanbiao Li, Tian Qin, Hongyi Zhang, Yasong Li, Xiaobing Liu|ByteDance Inc., Singapore; ByteDance Inc., San Jose, CA, USA; ByteDance Inc., Beijing, China|Privacy policies have disrupted the multi-billion dollar online advertising market by making real-time and precise user data untraceable, which poses significant challenges to the optimization of Return-On-Investment (ROI) constrained products in the online advertising industry. Privacy protection strategies, including event aggregation and reporting delays, hinder access to detailed and instantaneous feedback data, thus incapacitating traditional identity-revealing attribution techniques. In this paper, we introduces a novel Spending Programmed Bidding (SPB) framework to navigate these challenges. SPB is a two-stage framework that separates long horizon delivery spend planning (the macro stage) and short horizon bidding execution (the micro stage). The macro stage models the target ROI to achieve maximum utility and derives the expected spend, whereas the micro stage optimizes the bid price given the expected spend. We further extend our framework to the cross-channel scenario where the agent bids in both privacy-constrained and identity-revealing attribution channels. We find that when privacy-constrained channels are present, SPB is superior to state-of-the-art bidding methods in both offline datasets and online experiments on a large ad platform.|隐私政策扰乱了数十亿美元的在线广告市场，使实时和精确的用户数据无法追踪，这对在线广告业中投资回报率(ROI)受限产品的优化提出了重大挑战。隐私保护策略，包括事件聚合和报告延迟，阻碍访问详细的和即时的反馈数据，从而使传统的身份披露归因技术丧失能力。在本文中，我们介绍了一个新的支出计划投标(SPB)框架来应对这些挑战。SPB 是一个分为两个阶段的框架，分别为长期交付支出计划(宏观阶段)和短期投标执行(微观阶段)。宏观阶段建立目标投资回报率模型以实现效用最大化并导出预期支出，而微观阶段在给定预期支出的情况下优化投标价格。我们进一步将我们的框架扩展到跨通道场景，其中代理在隐私约束和身份披露的属性通道中竞标。我们发现，当隐私受限的渠道存在时，SPB 在离线数据集和大型广告平台上的在线实验中都优于最先进的投标方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Spending+Programmed+Bidding:+Privacy-friendly+Bid+Optimization+with+ROI+Constraint+in+Online+Advertising)|0|
|[Know in AdVance:  Linear-Complexity Forecasting of Ad Campaign Performance with Evolving User Interest](https://doi.org/10.1145/3637528.3671528)|Xiaoyu Wang, Yonghui Guo, Hui Sheng, Peili Lv, Chi Zhou, Wei Huang, Shiqin Ta, Dongbo Huang, Xiujin Yang, Lan Xu, Hao Zhou, Yusheng Ji|; Tencent Advertising, Shanghai, China; University of Science and Technology of China & National Institute of Informatics, Hefei, China|Real-time Bidding (RTB) advertisers wish to know in advance theexpected cost and yield of ad campaigns to avoid trial-and-error expenses.However, Campaign Performance Forecasting (CPF), a sequence modeling taskinvolving tens of thousands of ad auctions, poses challenges of evolving userinterest, auction representation, and long context, making coarse-grained andstatic-modeling methods sub-optimal. We propose AdVance, a time-awareframework that integrates local auction-level and global campaign-levelmodeling. User preference and fatigue are disentangled using a time-positionedsequence of clicked items and a concise vector of all displayed items.Cross-attention, conditioned on the fatigue vector, captures the dynamics ofuser interest toward each candidate ad. Bidders compete with each other,presenting a complete graph similar to the self-attention mechanism. Hence, weemploy a Transformer Encoder to compress each auction into embedding by solvingauxiliary tasks. These sequential embeddings are then summarized by aconditional state space model (SSM) to comprehend long-range dependencies whilemaintaining global linear complexity. Considering the irregular time intervalsbetween auctions, we make SSM's parameters dependent on the current auctionembedding and the time interval. We further condition SSM's global predictionson the accumulation of local results. Extensive evaluations and ablationstudies demonstrate its superiority over state-of-the-art methods. AdVance hasbeen deployed on the Tencent Advertising platform, and A/B tests show aremarkable 4.5% uplift in Average Revenue per User (ARPU).|实时竞价(RTB)广告商希望提前知道广告活动的预期成本和收益，以避免试错成本。然而，运动性能预测(CPF)是一个涉及数以万计的广告拍卖的序列建模任务，面临着用户兴趣、拍卖表示和长上下文的不断变化的挑战，使得粗粒度和静态建模方法不再是最优的。我们提出了一个时间感知框架，集成了本地拍卖级别和全球活动级别的建模。使用点击项目的时间定位序列和所有显示项目的简洁向量，用户偏好和疲劳被分离开来。交叉注意力，以疲劳向量为条件，捕捉用户对每个候选广告的兴趣动态。竞标者相互竞争，呈现出一个类似于自我注意机制的完整图表。因此，我们使用变压器编码器通过解决辅助任务将每个拍卖压缩成嵌入。然后用条件状态空间模型(SSM)对这些顺序嵌入进行总结，以便在保持全局线性复杂度的同时理解长程依赖关系。考虑到拍卖之间的时间间隔不规则，我们使得 SSM 的参数依赖于当前的拍卖嵌入和时间间隔。我们进一步将 SSM 的全局预测建立在局部结果累积的基础上。广泛的评估和消融研究表明其优于最先进的方法。在腾讯广告平台上已经部署了 Advanced，a/b 测试显示平均每用户收入(ARPU)显著提高了4.5% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Know+in+AdVance:++Linear-Complexity+Forecasting+of+Ad+Campaign+Performance+with+Evolving+User+Interest)|0|
|[Trinity: Syncretizing Multi-/Long-Tail/Long-Term Interests All in One](https://doi.org/10.1145/3637528.3671651)|Jing Yan, Liu Jiang, Jianfei Cui, Zhichen Zhao, Xingyan Bin, Feng Zhang, Zuotao Liu|ByteDance Inc., Beijing, China; ByteDance Inc., Shanghai, China|Interest modeling in recommender system has been a constant topic for improving user experience, and typical interest modeling tasks (e.g. multi-interest, long-tail interest and long-term interest) have been investigated in many existing works. However, most of them only consider one interest in isolation, while neglecting their interrelationships. In this paper, we argue that these tasks suffer from a common "interest amnesia" problem, and a solution exists to mitigate it simultaneously. We propose a novel and unified framework in the retrieval stage, "Trinity", to solve interest amnesia problem and improve multiple interest modeling tasks. We construct a real-time clustering system that enables us to project items into enumerable clusters, and calculate statistical interest histograms over these clusters. Based on these histograms, Trinity recognizes underdelivered themes and remains stable when facing emerging hot topics. Its derived retrievers have been deployed on the recommender system of Douyin, significantly improving user experience and retention. We believe that such practical experience can be well generalized to other scenarios.|兴趣建模一直是提高用户体验的推荐系统，现有的许多研究工作都对典型的兴趣建模任务(如多重兴趣、长尾兴趣和长期兴趣)进行了研究。然而，他们中的大多数只考虑了孤立的一个利益，而忽视了他们之间的相互关系。在本文中，我们认为，这些任务遭受一个共同的“利益健忘症”问题，并存在一个解决方案，以减轻它同时。为了解决兴趣健忘问题和改进多兴趣建模任务，我们提出了一个新的统一的检索框架“三位一体”。我们构造了一个实时聚类系统，使我们能够将项目投射到可枚举的聚类中，并计算这些聚类上的统计兴趣直方图。基于这些直方图，三一认识到交付不足的主题，并保持稳定时，面对新兴的热门话题。它派生出来的检索器已经部署在 Douyin 的推荐系统上，大大提高了用户体验和保留率。我们相信，这种实践经验可以很好地推广到其他情况。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Trinity:+Syncretizing+Multi-/Long-Tail/Long-Term+Interests+All+in+One)|0|
|[Temporal Uplift Modeling for Online Marketing](https://doi.org/10.1145/3637528.3671560)|Xin Zhang, Kai Wang, Zengmao Wang, Bo Du, Shiwei Zhao, Runze Wu, Xudong Shen, Tangjie Lv, Changjie Fan|Netease Fuxi AI Lab, Hangzhou, China; NetEase Fuxi AI Lab, Hangzhou, China; Wuhan University, Wuhan, China|In recent years, uplift modeling, also known as individual treatment effect (ITE) estimation, has seen wide applications in online marketing, such as delivering one-time issuance of coupons or discounts to motivate users' purchases. However, complex yet more realistic scenarios involving multiple interventions over time on users are still rarely explored. The challenges include handling the bias from time-varying confounders, determining optimal treatment timing, and selecting among numerous treatments. In this paper, to tackle the aforementioned challenges, we present a temporal point process-based uplift model (TPPUM) that utilizes users' temporal event sequences to estimate treatment effects via counterfactual analysis and temporal point processes. In this model, marketing actions are considered as treatments, user purchases as outcome events, and how treatments alter the future conditional intensity function of generating outcome events as the uplift. Empirical evaluations demonstrate that our method outperforms existing baselines on both real-world and synthetic datasets. In the online experiment conducted in a discounted bundle recommendation scenario involving an average of 3 to 4 interventions per day and hundreds of treatment candidates, we demonstrate how our model outperforms current state-of-the-art methods in selecting the appropriate treatment and timing of treatment, resulting in a 3.6% increase in application-level revenue.|近年来，提升模型，也被称为个体治疗效果(ITE)评估，已经在网络营销中得到了广泛的应用，例如提供一次性发行优惠券或折扣，以激励用户的购买。然而，涉及随着时间的推移对用户进行多种干预的复杂但更现实的场景仍然很少被探索。这些挑战包括处理来自时变混杂因素的偏倚，确定最佳治疗时机，以及在众多治疗方案中进行选择。为了解决上述问题，本文提出了一种基于时间点过程的提升模型(TPPUM) ，该模型利用用户的时间事件序列，通过反事实分析和时间点过程来估计治疗效果。在这个模型中，营销行为被认为是治疗，用户购买作为结果事件，以及治疗如何改变未来的条件强度函数，产生结果事件作为提升。经验评估表明，我们的方法优于现有的基线上的真实世界和合成数据集。在折扣捆绑推荐情景下进行的在线实验中，涉及平均每天3至4次干预和数百个治疗候选人，我们证明了我们的模型在选择适当的治疗和治疗时机方面如何优于当前最先进的方法，导致应用级收入增加3.6% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Temporal+Uplift+Modeling+for+Online+Marketing)|0|
|[STATE: A Robust ATE Estimator of Heavy-Tailed Metrics for Variance Reduction in Online Controlled Experiments](https://doi.org/10.1145/3637528.3672352)|Hao Zhou, Kun Sun, Shaoming Li, Yangfeng Fan, Guibin Jiang, Jiaqi Zheng, Tao Li|State Key Laboratory for Novel Software Technology, Nanjing University & Meituan, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Meituan, Beijing, China|Online controlled experiments play a crucial role in enabling data-driven decisions across a wide range of companies. Variance reduction is an effective technique to improve the sensitivity of experiments, achieving higher statistical power while using fewer samples and shorter experimental periods. However, typical variance reduction methods (e.g., regression-adjusted estimators) are built upon the intuitional assumption of Gaussian distributions and cannot properly characterize the real business metrics with heavy-tailed distributions. Furthermore, outliers diminish the correlation between pre-experiment covariates and outcome metrics, greatly limiting the effectiveness of variance reduction. In this paper, we develop a novel framework that integrates the Student's t-distribution with machine learning tools to fit heavy-tailed metrics and construct a robust average treatment effect estimator in online controlled experiments, which we call STATE. By adopting a variational EM method to optimize the loglikehood function, we can infer a robust solution that greatly eliminates the negative impact of outliers and achieves significant variance reduction. Moreover, we extend the STATE method from count metrics to ratio metrics by utilizing linear transformation that preserves unbiased estimation, whose variance reduction is more complex but less investigated in existing works. Finally, both simulations on synthetic data and long-term empirical results on Meituan experiment platform demonstrate the effectiveness of our method. Compared with the state-of-the-art estimators (CUPAC/MLRATE), STATE achieves over 50% variance reduction, indicating it can reach the same statistical power with only half of the observations, or half the experimental duration.|在线控制实验在许多公司实现数据驱动的决策方面发挥着至关重要的作用。方差约简是一种提高实验灵敏度的有效方法，可以在使用较少样本和较短实验周期的情况下获得较高的统计效率。然而，典型的方差减少方法(例如，回归调整估计)是建立在高斯分布的直观假设之上的，不能正确地描述实际业务指标的重尾分布。此外，异常值降低了实验前协变量与结果指标之间的相关性，极大地限制了方差减少的有效性。本文提出了一种新的学生 t 分布与机器学习工具相结合的框架，用于在线对照实验，构造了一个稳健的平均治疗效果估计器，我们称之为状态估计器。通过采用变分 EM 方法对对数似然函数进行优化，可以推导出一个鲁棒解，大大消除了异常值的负面影响，实现了显著的方差减少。此外，我们将 STATE 方法从计数指标扩展到比率指标，使用了保留无偏估计的线性映射，其方差减少更为复杂，但在现有工作中研究较少。最后，对合成数据的模拟和在美团实验平台上的长期实验结果都证明了该方法的有效性。与最先进的估计器(CUPAC/MLRATE)相比，STATE 实现了超过50% 的方差减少，表明它可以达到相同的统计功率只有一半的观察，或一半的实验持续时间。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=STATE:+A+Robust+ATE+Estimator+of+Heavy-Tailed+Metrics+for+Variance+Reduction+in+Online+Controlled+Experiments)|0|
|[Practical Machine Learning for Streaming Data](https://doi.org/10.1145/3637528.3671442)|Heitor Murilo Gomes, Albert Bifet|; AI Institute, University of Waikato & LTCI, Télécom Paris, IP Paris, Waikato, New Zealand|Machine Learning for Data Streams has been an important area of research since the late 1990s, and its use in industry has grown significantly over the last few years. However, there is still a gap between the cutting-edge research and the tools that are readily available, which makes it challenging for practitioners, including experienced data scientists, to implement and evaluate these methods in this complex domain. Our tutorial aims to bridge this gap with a dual focus. We will discuss important research topics, such as partially delayed labeled streams, while providing practical demonstrations of their implementation and assessment using CapyMOA, an open-source library that provides efficient algorithm implementations through a high-level Python API. Source code is available in https://github.com/adaptive-machine-learning/CapyMOA while the accompanying tutorials and installation guide are available in https://capymoa.org/.|自20世纪90年代末以来，数据流机器学习一直是一个重要的研究领域，在过去几年中，它在工业中的应用显著增长。然而，在尖端研究和现有工具之间仍然存在差距，这使得从业人员，包括有经验的数据科学家，在这个复杂的领域实施和评估这些方法具有挑战性。我们的教程旨在通过双重关注来弥补这一差距。我们将讨论一些重要的研究主题，比如部分延迟的标记流，同时使用 CapyMOA 提供实际的实现和评估演示，CapyMOA 是一个开源库，通过高级 Python API 提供高效的算法实现。源代码可以在 https://github.com/adaptive-machine-learning/capymoa 中找到，相应的教程和安装指南也可以在 https://capymoa.org/中找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Practical+Machine+Learning+for+Streaming+Data)|0|
|[Empower an End-to-end Scalable and Interpretable Data Science Ecosystem using Statistics, AI and Domain Science](https://doi.org/10.1145/3637528.3672194)|Xihong Lin|Harvard University, Boston, MA, USA|The data science ecosystem encompasses data fairness, statistical, ML and AI methods and tools, interpretable data analysis and results, and trustworthy decision-making. Rapid advancements in AI have revolutionized data utilization and enabled machines to learn from data more effectively. Statistics, as the science of learning from data while accounting for uncertainty, plays a pivotal role in addressing complex real-world problems and facilitating trustworthy decision-making. In this talk, I will discuss the challenges and opportunities involved in building an end-to-end scalable and interpretable data science ecosystem using the analysis of whole genome sequencing studies and biobanks that integrates statistics, ML/AI, and genomic and health science as an example. Biobanks collect whole genome data, electronic health records and epidemiological data. I will illustrate key points using the analysis of multi-ancestry whole genome sequencing studies and biobanks by discussing a few scalable and interpretable statistical and ML/AI methods, tools and data science resources. Specifically, first, data fairness and diversity is a critical pillar of a trustworthy data science ecosystem. About 85+% of genome wide association study samples in the last 15 years are European, resulting in disparity in genetic research. I will discuss the community effort on improving diversity in genetic studies in the last 10 years. I will present trans-ancestry polygenic risk scores (PRS) using millions of common genetic variants across the genome by leveraging large GWAS sample sizes of European and smaller sample sizes of under-represented populations for predicting disease risk using transfer learning and genetic association summary statistics. The performance of deep learning methods for PRS will also be discussed. Second, scalability in cloud platforms is critical for large scale affordable analysis for multi-ancestry biobanks and whole genome studies. I will discuss improving scalability in cloud-computing using interpretable sparsity via FastSparseGRM. To build an interpretable and powerful end-to-end ecosystem of rare variant analysis of large scale whole genome sequencing studies and biobanks, I will first introduce FAVOR, a multi-faceted variant functional annotation database and portal of all possible 9 billions of variants across the whole genome. I will discuss FAVOR-GPT, a LLM interface of the FAVOR functional annotation database to improve user experience for navigating FAVOR and performing variant functional annotation query and variant functional summary statistics calculations. I will also discuss FAVORannotator which can be used to functionally annotate any whole genome sequencing studies. I will also discuss STAAR and STAAR and STAARpipeline, the WGS rare variant analysis pipeline that boosts the power of WGS rare variant association analysis by dynamically incorporating multi-faceted variant functional annotations. Extension of incorporating single-cell data in WGS analysis will also be discussed. I will also discuss ensemble methods that improve the power of rare variant association tests. Cloud-deployment of these resources and tools in several ecosystems will be presented, such as RAP for the UK biobank, AnVIL for the NHGRI Genome Sequencing Program and All of Us, and BioData Catalyst for the NHLBI Trans-omics Precision Medine Program (TOPMed). This talk aims to ignite proactive and thought-provoking discussions, foster collaboration, and cultivate open-minded approaches to advance scientific discovery.|数据科学生态系统包括数据公平、统计学、机器学习和人工智能方法和工具、可解释的数据分析和结果以及可信赖的决策。人工智能的快速发展使数据利用发生了革命性的变化，使机器能够更有效地从数据中学习。统计学是一门从数据中学习同时考虑不确定性的科学，在解决复杂的现实世界问题和促进可信赖的决策方面发挥着关键作用。在这次演讲中，我将以整合统计学、机器学习/人工智能以及基因组和健康科学的全基因组测序研究和生物库分析为例，讨论建立一个端到端可扩展和可解释的数据科学生态系统所面临的挑战和机遇。生物库收集全基因组数据、电子健康记录和流行病学数据。我将通过讨论一些可扩展和可解释的统计学和机器学习/人工智能方法、工具和数据科学资源，用多血统全基因组测序研究和生物库的分析来说明要点。具体来说，首先，数据公平性和多样性是可信赖的数据科学生态系统的关键支柱。在过去的15年中，大约85% 以上的全基因组关联研究样本是欧洲的，这导致了遗传学研究的差异。我将讨论在过去10年中社区在提高基因研究多样性方面所做的努力。我将通过利用欧洲的大型 GWAS 样本量和代表性不足人群的较小样本量，使用转移学习和遗传关联总结统计来预测疾病风险，从而使用数百万个常见基因变体来呈现跨血统多基因风险评分(PRS)。本文还将讨论 PRS 深度学习方法的性能。其次，云平台的可扩展性对于多血统生物库和全基因组研究的大规模经济分析至关重要。我将讨论如何通过 FastSparseGRM 使用可解释的稀疏性来提高云计算的可伸缩性。为了建立一个可解释和强大的端到端生态系统的罕见变异分析的大规模全基因组测序研究和生物库，我将首先介绍 FAVOR，一个多方面的变异功能注释数据库和门户网站的所有可能的90亿个变异整个基因组。我将讨论 FAVOR-GPT，它是 FAVOR 功能注释数据库的 LLM 接口，用于改善用户在导航 FAVOR 和执行变体功能注释查询和变体功能摘要统计计算时的体验。我还将讨论 FAVORannotator，它可以用来对任何全基因组测序研究进行功能性注释。我还将讨论 STAAR 和 STAAR 以及 STAAR 流水线，这是 WGS 稀有变量分析流水线，它通过动态结合多方面的变量功能注释来提高 WGS 稀有变量关联分析的能力。还将讨论将单细胞数据纳入 WGS 分析的扩展。我还将讨论提高稀有变量关联测试能力的集成方法。将介绍这些资源和工具在几个生态系统中的云部署，例如英国生物库的 RAP，NHGRI 基因组测序计划和我们所有人的 AnVIL，以及 NHLBI 反式组学精确医学计划(TOPMed)的 BioData Catalyst。这次讲座的目的是激发积极主动和发人深省的讨论，促进合作，并培养开放的方法，以推动科学发现。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Empower+an+End-to-end+Scalable+and+Interpretable+Data+Science+Ecosystem+using+Statistics,+AI+and+Domain+Science)|0|
|[Semi-Supervised Learning for Time Series Collected at a Low Sampling Rate](https://doi.org/10.1145/3637528.3672033)|Minyoung Bae, Yooju Shin, Youngeun Nam, Youngseop Lee, JaeGil Lee|KAIST, Daejeon, Republic of Korea; Samsung Electronics Co., Ltd., Suwon-si, Republic of Korea|Although time-series classification has many applications in healthcare and manufacturing, the high cost of data collection and labeling hinders its widespread use. To reduce data collection and labeling costs while maintaining high classification accuracy, we propose a novel problem setting, called semi-supervised learning with low-sampling-rate time series, in which the majority of time series are collected at a low sampling rate and are unlabeled whereas the minority of time series are collected at a high sampling rate and are labeled. For this novel problem scenario, we develop the SemiTSR framework equipped with the super-resolution module and the semi-supervised learning module. Here, low-sampling-rate time series are upsampled precisely, taking periodicity and trend at each timestamp into account, and both labeled and unlabeled high-sampling-rate time series are utilized for training. In particular, consistency regularization between artificially downsampled time series derived from an original high-sampling-rate time series is effective at overcoming limited sampling rates. We demonstrate that SemiTSR significantly outperforms conventional semi-supervised learning techniques by assuring high classification accuracy with low-sampling-rate time series.|尽管时间序列分类在医疗保健和制造业中有着广泛的应用，但是高昂的数据采集和标记成本阻碍了它的广泛应用。为了减少数据收集和标记成本，同时保持高分类准确性，我们提出了一个新的问题设置，称为低采样率时间序列的半监督学习，其中大多数时间序列是以低采样率收集和未标记，而少数时间序列是以高采样率收集和标记。对于这个新的问题场景，我们开发了配备了超分辨率模块和半监督学习模块的 SemitSR 框架。该方法对低采样率时间序列进行精确的上采样，同时考虑每个时间戳的周期性和趋势性，采用标记和未标记的高采样率时间序列进行训练。特别是，人工下采样时间序列之间的一致性正则化源于一个原始的高采样率时间序列是有效的克服有限的采样率。我们证明了 SemitSR 通过保证低采样率时间序列的高分类精度，明显优于传统的半监督学习分类技术。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semi-Supervised+Learning+for+Time+Series+Collected+at+a+Low+Sampling+Rate)|0|
|[Meta Clustering of Neural Bandits](https://doi.org/10.1145/3637528.3671691)|Yikun Ban, Yunzhe Qi, Tianxin Wei, Lihui Liu, Jingrui He|University of Illinois, Urbana Champaign, Champaign, IL, USA; University of Illinois, Urbana-Champaign, Champaign, IL, USA; University of Illinois at Urbana-Champaign, Champaign, IL, USA|The contextual bandit has been identified as a powerful framework to formulate the recommendation process as a sequential decision-making process, where each item is regarded as an arm and the objective is to minimize the regret of T rounds. In this paper, we study a new problem, Clustering of Neural Bandits, by extending previous work to the arbitrary reward function, to strike a balance between user heterogeneity and user correlations in the recommender system. To solve this problem, we propose a novel algorithm called M-CNB, which utilizes a meta-learner to represent and rapidly adapt to dynamic clusters, along with an informative Upper Confidence Bound (UCB)-based exploration strategy. We provide an instance-dependent performance guarantee for the proposed algorithm that withstands the adversarial context, and we further prove the guarantee is at least as good as state-of-the-art (SOTA) approaches under the same assumptions. In extensive experiments conducted in both recommendation and online classification scenarios, M-CNB outperforms SOTA baselines. This shows the effectiveness of the proposed approach in improving online recommendation and online classification performance.|环境强盗已被确定为一个强大的框架，以制定推荐过程作为一个顺序决策过程，其中每个项目被视为一个手臂，目标是尽量减少 T 轮的遗憾。在这篇文章中，我们研究了一个新的问题，神经盗贼聚类，通过扩展以前的工作到任意奖励函数，来平衡用户异质性和用户相关性的推荐系统。为了解决这一问题，我们提出了一种新的算法 M-CNB，该算法利用元学习器来表示和快速适应动态聚类，同时提出了一种基于信息上置信界(UCB)的探索策略。我们提供了一个实例相关的性能保证提出的算法，抵御对手的背景下，我们进一步证明的保证是至少一样好的国家-最先进的(SOTA)方法在相同的假设条件下。在推荐和在线分类场景中进行的大量实验中，M-CNB 的性能优于 SOTA 基线。这表明了该方法在改善在线推荐和在线分类性能方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Meta+Clustering+of+Neural+Bandits)|0|
|[Popularity-Aware Alignment and Contrast for Mitigating Popularity Bias](https://doi.org/10.1145/3637528.3671824)|Miaomiao Cai, Lei Chen, Yifan Wang, Haoyue Bai, Peijie Sun, Le Wu, Min Zhang, Meng Wang|DCST, Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; DCST, Tsinghua University & Quan Cheng Laboratory, Beijing, China; Hefei University of Technology, Hefei, China|Collaborative Filtering (CF) typically suffers from the significant challengeof popularity bias due to the uneven distribution of items in real-worlddatasets. This bias leads to a significant accuracy gap between popular andunpopular items. It not only hinders accurate user preference understanding butalso exacerbates the Matthew effect in recommendation systems. To alleviatepopularity bias, existing efforts focus on emphasizing unpopular items orseparating the correlation between item representations and their popularity.Despite the effectiveness, existing works still face two persistent challenges:(1) how to extract common supervision signals from popular items to improve theunpopular item representations, and (2) how to alleviate the representationseparation caused by popularity bias. In this work, we conduct an empiricalanalysis of popularity bias and propose Popularity-Aware Alignment and Contrast(PAAC) to address two challenges. Specifically, we use the common supervisorysignals modeled in popular item representations and propose a novelpopularity-aware supervised alignment module to learn unpopular itemrepresentations. Additionally, we suggest re-weighting the contrastive learningloss to mitigate the representation separation from a popularity-centricperspective. Finally, we validate the effectiveness and rationale of PAAC inmitigating popularity bias through extensive experiments on three real-worlddatasets. Our code is available athttps://github.com/miaomiao-cai2/KDD2024-PAAC.|由于现实世界数据集中项目的分布不均匀，协同过滤(CF)通常会受到流行偏见的严重挑战。这种偏差导致流行和不流行的项目之间的准确性差距很大。它不仅阻碍了准确的用户偏好理解，而且加剧了推荐系统中的马太效应。为了减轻流行偏见，现有的研究集中在强调不受欢迎的项目或者区分项目表示和它们的流行度之间的关系。尽管有效，现有的研究仍然面临两个持续的挑战: (1)如何从流行项目中提取共同的监督信号来改善不流行项目的表征; (2)如何缓解流行偏差造成的表征分离。在这项工作中，我们进行了流行偏差的实证分析，并提出流行意识的对齐和对比(PAAC) ，以解决两个挑战。具体来说，我们使用流行项表示中的公共监督信号，并提出了一种新颖的流行感知监督对齐模块来学习不流行的项表示。此外，我们建议重新权衡对比学习损失，以减轻从流行为中心的观点表征分离。最后，我们通过在三个实际数据集上的大量实验，验证了 PAAC 消除流行偏差的有效性和合理性。我们的代码是可用的 https:// github.com/miaomiao-cai2/kdd2024-paac。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Popularity-Aware+Alignment+and+Contrast+for+Mitigating+Popularity+Bias)|0|
|[Enhancing Contrastive Learning on Graphs with Node Similarity](https://doi.org/10.1145/3637528.3671898)|Hongliang Chi, Yao Ma|Rensselaer Polytechnic Institute, Troy, NY, USA|Graph Neural Networks (GNNs) have achieved great success in learning graph representations and thus facilitating various graph-related tasks. However, most GNN methods adopt a supervised learning setting, which is not always feasible in real-world applications due to the difficulty to obtain labeled data. Hence, graph self-supervised learning has been attracting increasing attention. Graph contrastive learning (GCL) is a representative framework for self-supervised learning. In general, GCL learns node representations by contrasting semantically similar nodes (positive samples) and dissimilar nodes (negative samples) with anchor nodes. Without access to labels, positive samples are typically generated by data augmentation, and negative samples are uniformly sampled from the entire graph, which leads to a sub-optimal objective. Specifically, data augmentation naturally limits the number of positive samples that involve in the process (typically only one positive sample is adopted). On the other hand, the random sampling process would inevitably select false-negative samples (samples sharing the same semantics with the anchor). These issues limit the learning capability of GCL. In this work, we propose an enhanced objective that addresses the aforementioned issues. We first introduce an unachievable ideal objective that contains all positive samples and no false-negative samples. This ideal objective is then transformed into a probabilistic form based on the distributions for sampling positive and negative samples. We then model these distributions with node similarity and derive the enhanced objective. Comprehensive experiments on various datasets demonstrate the effectiveness of the proposed enhanced objective under different settings.|图形神经网络(GNN)在学习图形表示方面取得了巨大的成功，从而为各种图形相关的任务提供了便利。然而，大多数 GNN 方法采用监督式学习设置，这在现实应用中并不总是可行的，因为很难获得标记数据。因此，图的自监督学习越来越受到人们的关注。图形对比学习(GCL)是一种典型的自监督学习框架。通常，GCL 通过对比语义相似的节点(正样本)和不同的节点(负样本)与锚节点来学习节点表示。由于无法获得标签，正面样本通常通过数据增量生成，而负面样本则从整个图中均匀抽取，从而导致次优目标。具体来说，数据增强自然地限制了过程中涉及的阳性样本的数量(通常只采用一个阳性样本)。另一方面，随机抽样过程不可避免地会选择假阴性样本(样本与锚具有相同的语义)。这些问题限制了 GCL 的学习能力。在这项工作中，我们提出了一个解决上述问题的强化目标。我们首先介绍一个不可能实现的理想目标，它包含所有的正样本和没有假阴性样本。然后将这个理想目标转化为基于正负样本抽样分布的概率形式。然后利用节点相似性对这些分布进行建模，得到增强目标。在不同数据集上的综合实验表明了该增强目标在不同设置下的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Contrastive+Learning+on+Graphs+with+Node+Similarity)|0|
|[Fairness in Streaming Submodular Maximization Subject to a Knapsack Constraint](https://doi.org/10.1145/3637528.3671778)|Shuang Cui, Kai Han, Shaojie Tang, Feng Li, Jun Luo|; Nanyang Technological University, Singapore, Singapore; School of Computer Science and Technology, Soochow University, Suzhou, Jiangsu, China; The University of Texas at Dallas, Richardson, TX, USA|Submodular optimization has been identified as a powerful tool for many data mining applications, where a representative subset of moderate size needs to be extracted from a large-scale dataset. In scenarios where data points possess sensitive attributes such as age, gender, or race, it becomes imperative to integrate fairness measures into submodular optimization to mitigate bias and discrimination. In this paper, we study the fundamental problem of fair submodular maximization subject to a knapsack constraint and propose the first streaming algorithm for it with provable performance guarantees for both monotone and non-monotone submodular functions. As a byproduct, we also propose a streaming algorithm for submodular maximization subject to a partition matroid and a knapsack constraint, significantly improving the performance bounds achieved by previous work. We conduct extensive experiments on real-world applications such as movie recommendation, image summarization, and maximum coverage in social networks. The experimental results strongly demonstrate the superiority of our proposed algorithms in terms of both fairness and utility.|子模块优化已被确定为许多数据挖掘应用程序的强大工具，其中需要从大规模数据集中提取中等规模的代表性子集。在数据点具有敏感属性(如年龄、性别或种族)的情况下，必须将公平措施纳入子模块优化，以减少偏见和歧视。本文研究了背包约束下的公平子模极大化问题，提出了第一种具有单调和非单调子模函数性能保证的流算法。作为一个副产品，我们还提出了一种子模块最大化的流算法，该算法受到分区拟阵和背包约束的影响，显著提高了以前的工作所取得的性能界限。我们在真实世界的应用上进行了广泛的实验，比如电影推荐、图片摘要和社交网络的最大覆盖率。实验结果有力地证明了我们提出的算法在公平性和实用性方面的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fairness+in+Streaming+Submodular+Maximization+Subject+to+a+Knapsack+Constraint)|0|
|[AGS-GNN: Attribute-guided Sampling for Graph Neural Networks](https://doi.org/10.1145/3637528.3671940)|Siddhartha Shankar Das, S. M. Ferdous, Mahantesh M. Halappanavar, Edoardo Serra, Alex Pothen|Pacific Northwest National Lab., Richland, WA, USA; Boise State University, Boise, ID, USA; Purdue University, West Lafayette, IN, USA|We propose AGS-GNN, a novel attribute-guided sampling algorithm for Graph Neural Networks (GNNs). AGS-GNN exploits the node features and the connectivity structure of a graph while simultaneously adapting for both homophily and heterophily in graphs. In homophilic graphs, vertices of the same class are more likely to be adjacent, but vertices of different classes tend to be adjacent in heterophilic graphs. GNNs have been successfully applied to homophilic graphs, but their utility to heterophilic graphs remains challenging. The state-of-the-art GNNs for heterophilic graphs use the full neighborhood of a node instead of sampling it, and hence do not scale to large graphs and are not inductive. We develop dual-channel sampling techniques based on feature-similarity and feature-diversity to select subsets of neighbors for a node that capture adaptive information from homophilic and heterophilic neighborhoods. Currently, AGS-GNN is the only algorithm that explicitly controls homophily in the sampled subgraph through similar and diverse neighborhood samples. For diverse neighborhood sampling, we employ submodularity, a novel contribution in this context. We pre-compute the sampling distribution in parallel, achieving the desired scalability. Using an extensive dataset consisting of 35 small (< 100K nodes) and large (- 100K nodes) homophilic and heterophilic graphs, we demonstrate the superiority of AGS-GNN compared to the state-of-the-art approaches. AGS-GNN achieves test accuracy comparable to the best-performing heterophilic GNNs, even outperforming methods that use the entire graph for node classification. AGS-GNN converges faster than methods that sample neighborhoods randomly, and can be incorporated into existing GNN models that employ node or graph sampling.|提出了一种新的图形神经网络属性导向采样算法 AGS-GNN。AGS-GNN 利用图的节点特征和连通结构，同时适应图的同质性和异质性。在同类图中，同类的顶点更容易相邻，但在异类图中，不同类的顶点更容易相邻。GNN 已经成功地应用于同亲图，但是它们在异亲图中的应用仍然具有挑战性。异质图的最新 GNN 使用节点的完全邻域代替抽样，因此不能伸缩到大图，也不能归纳。我们发展了基于特征相似性和特征多样性的双通道采样技术来选择一个节点的邻居子集，从同类和异类邻居中捕获自适应信息。目前，AGS-GNN 算法是唯一一种通过相似和不同的邻域样本显式控制采样子图同调性的算法。对于不同的邻域抽样，我们采用次模块化，这是在这种情况下的一个新的贡献。我们并行地预先计算了采样分布，达到了预期的可扩展性。使用由35个小(< 100K 节点)和大(- 100K 节点)同质和异质图组成的广泛数据集，我们证明了 AGS-GNN 与最先进的方法相比的优越性。AGS-GNN 获得了与性能最好的异质 GNN 相当的测试精度，甚至优于使用整个图进行节点分类的方法。AGS-GNN 比随机抽样邻域的方法收敛得更快，并且可以合并到采用节点或图抽样的现有 GNN 模型中。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AGS-GNN:+Attribute-guided+Sampling+for+Graph+Neural+Networks)|0|
|[Estimated Judge Reliabilities for Weighted Bradley-Terry-Luce Are Not Reliable](https://doi.org/10.1145/3637528.3671907)|Andrew F. Dreher, Etienne Vouga, Donald S. Fussell|The University of Texas at Austin, Austin, TX, USA|There are many applications for which we want to learn a latent scale for subjective properties, such as the excitement of a photo or the legibility of a font; however, obtaining human-labeled data is costly and time-consuming. One oft-used method for acquiring these labels, despite the cost being quadratic in the number of items, is the method of pairwise comparisons since this method minimizes the effect of biases and generally can be used effectively outside of a controlled environment. Crowdsourcing appears to be a panacea since online platforms provide affordable access to numerous people, but these participants, judges, vary in diligence and expertise. Several methods have been proposed to assign weights to judges based on their responses relative to everyone else, the goal being to reduce exposure to poor performers, hopefully upgrading the quality of the data. Our research focuses on two natural extensions to the Bradley-Terry-Luce formulation of scaling that jointly optimize for both scale value and judge weights. While both methods appear to perform at least as well as the unweighted formulation on average with well-behaved judges, we report a previously unknown flaw, revealing that the resultant judge weights should not be interpreted as reliabilities. Consequently, these values should not be leveraged for decisions about the judges, such as for active sampling or to validate the participant pool.|有许多应用程序，我们想要了解一个潜在的规模主观属性，如兴奋的照片或字体的可读性; 然而，获取人类标记的数据是昂贵的和耗时的。一个常用的获取这些标签的方法，尽管成本在项目数量上是二次的，是成对比较的方法，因为这种方法最大限度地减少了偏差的影响，通常可以在受控环境之外有效地使用。众包似乎是一种灵丹妙药，因为在线平台为许多人提供了负担得起的访问途径，但这些参与者、法官在勤奋程度和专业知识方面各不相同。已经提出了几种方法，根据法官相对于其他所有人的答复来确定其权重，目的是减少表现不佳的风险，希望能够提高数据的质量。我们的研究集中在两个自然扩展的布拉德利-特里-卢斯公式的尺度，共同优化的标度值和判断权重。虽然这两种方法在表现良好的法官中表现至少和未加权公式一样好，但我们报告了一个以前未知的缺陷，揭示了由此产生的法官权重不应该被解释为可靠性。因此，不应该将这些值用于有关法官的决策，例如用于主动抽样或验证参与者池。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Estimated+Judge+Reliabilities+for+Weighted+Bradley-Terry-Luce+Are+Not+Reliable)|0|
|[Influence Maximization via Graph Neural Bandits](https://doi.org/10.1145/3637528.3671983)|Yuting Feng, Vincent Y. F. Tan, Bogdan Cautis|; Department of Mathematics, Department of ECE, National University of Singapore, Singapore, Singapore; CNRS LISN, University of Paris-Saclay, Orsay, France|We consider a ubiquitous scenario in the study of Influence Maximization(IM), in which there is limited knowledge about the topology of the diffusionnetwork. We set the IM problem in a multi-round diffusion campaign, aiming tomaximize the number of distinct users that are influenced. Leveraging thecapability of bandit algorithms to effectively balance the objectives ofexploration and exploitation, as well as the expressivity of neural networks,our study explores the application of neural bandit algorithms to the IMproblem. We propose the framework IM-GNB (Influence Maximization with GraphNeural Bandits), where we provide an estimate of the users' probabilities ofbeing influenced by influencers (also known as diffusion seeds). This initialestimate forms the basis for constructing both an exploitation graph and anexploration one. Subsequently, IM-GNB handles the exploration-exploitationtradeoff, by selecting seed nodes in real-time using Graph ConvolutionalNetworks (GCN), in which the pre-estimated graphs are employed to refine theinfluencers' estimated rewards in each contextual setting. Through extensiveexperiments on two large real-world datasets, we demonstrate the effectivenessof IM-GNB compared with other baseline methods, significantly improving thespread outcome of such diffusion campaigns, when the underlying network isunknown.|在影响最大化(IM)的研究中，我们考虑了一个无处不在的场景，其中关于扩散网络的拓扑结构的知识是有限的。我们将 IM 问题设置为一个多轮扩散运动，目的是最大限度地增加受影响的不同用户的数量。利用强盗算法的能力来有效地平衡勘探和开发的目标，以及神经网络的表达能力，我们的研究探索了神经强盗算法在 IMN 问题中的应用。我们提出了 IM-GNB (影响最大化与图形神经绑定)的框架，其中我们提供了一个估计的用户的概率受影响者(也称为扩散种子)的影响。这个初始估计是构造开发图和开发图的基础。随后，IM-GNB 通过使用图卷积网络(Graph ConvolutionalNetworks，GCN)实时选择种子节点来处理探索-开发权衡，其中预估图被用于在每个上下文环境中细化影响者的估计奖励。通过在两个大型真实世界数据集上的广泛实验，我们证明了 IM-GNB 与其他基线方法相比的有效性，显着改善了这种扩散运动的传播结果，当底层网络是未知的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Influence+Maximization+via+Graph+Neural+Bandits)|0|
|[A Unified Core Structure in Multiplex Networks: From Finding the Densest Subgraph to Modeling User Engagement](https://doi.org/10.1145/3637528.3672011)|Farnoosh Hashemi, Ali Behrouz|Cornell University, Ithaca, NY, USA|In many complex systems, the interactions between objects span multipleaspects. Multiplex networks are accurate paradigms to model such systems, whereeach edge is associated with a type. A key graph mining primitive is extractingdense subgraphs, and this has led to interesting notions such as K-cores, knownas building blocks of complex networks. Despite recent attempts to extend thenotion of core to multiplex networks, existing studies suffer from a subset ofthe following limitations: They 1) force all nodes to exhibit their high degreein the same set of relation types while in multiplex networks some connectiontypes can be noisy for some nodes, 2) either require high computational cost ormiss the complex information of multiplex networks, and 3) assume the sameimportance for all relation types. We introduce S-core, a novel and unifyingfamily of dense structures in multiplex networks that uses a function S(.) tosummarize the degree vector of each node. We then discuss how one can choose aproper S(.) from the data. To demonstrate the usefulness of S-cores, we focuson finding the densest subgraph as well as modeling user engagement inmultiplex networks. We present a new density measure in multiplex networks anddiscuss its advantages over existing density measures. We show that the problemof finding the densest subgraph in multiplex networks is NP-hard and design anefficient approximation algorithm based on S-cores. Finally, we present a newmathematical model of user engagement in the presence of different relationtypes. Our experiments shows the efficiency and effectiveness of our algorithmsand supports the proposed mathematical model of user engagement.|在许多复杂系统中，对象之间的交互跨越多个方面。多路网络是模拟这种系统的精确范例，其中每个边都与一个类型相关联。一个关键的图挖掘原语是提取稠密子图，这导致了一些有趣的概念，如 K 核，即复杂网络的构建块。尽管最近试图将核心概念扩展到多路网络，但现有的研究存在以下局限性的子集: 1)迫使所有节点在同一组关系类型中表现出高度的高度，而在多路网络中，一些连接类型对于某些节点可能是噪声的，2)要么需要高计算成本，要么忽略多路网络的复杂信息，3)对所有关系类型承担同样的重要性。我们介绍了 S- 核，一个新的和统一的密集结构家族在多路网络中使用一个函数 S (。)汇总每个节点的度向量。然后我们讨论如何选择一个合适的 S (。)从数据中。为了证明 S 核的有效性，我们着重于寻找密度最大的子图，以及在多路网络中建立用户参与模型。提出了一种新的多路网络密度度量方法，并讨论了它相对于现有密度度量方法的优越性。我们证明了在多路网络中寻找最密集子图的问题是 NP 难的，并且设计了一个基于 S 核的高效近似演算法。最后，我们提出了一个新的数学模型的用户参与在不同的关系类型的存在。实验结果表明了该算法的有效性，并支持所提出的用户参与的数学模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Unified+Core+Structure+in+Multiplex+Networks:+From+Finding+the+Densest+Subgraph+to+Modeling+User+Engagement)|0|
|[Budgeted Multi-Armed Bandits with Asymmetric Confidence Intervals](https://doi.org/10.1145/3637528.3671833)|Marco Heyden, Vadim Arzamasov, Edouard Fouché, Klemens Böhm|Karlsruhe Institute of Technology, Karlsruhe, Germany|We study the stochastic Budgeted Multi-Armed Bandit (MAB) problem, where a player chooses from K arms with unknown expected rewards and costs. The goal is to maximize the total reward under a budget constraint. A player thus seeks to choose the arm with the highest reward-cost ratio as often as possible. Current approaches for this problem have several issues, which we illustrate. To overcome them, we propose a new upper confidence bound (UCB) sampling policy, ømega-UCB, that uses asymmetric confidence intervals. These intervals scale with the distance between the sample mean and the bounds of a random variable, yielding a more accurate and tight estimation of the reward-cost ratio compared to our competitors. We show that our approach has sublinear instance-dependent regret in general and logarithmic regret for parameter ρ ≥ 1, and that it outperforms existing policies consistently in synthetic and real settings.|我们研究随机预算多臂老虎机(MAB)问题，其中玩家从 k 武器选择未知的预期回报和成本。我们的目标是最大化预算线下的总回报。因此，玩家尽可能多地选择奖励成本比率最高的手臂。目前解决这个问题的方法有几个问题，我们将举例说明。为了克服这些问题，我们提出了一种新的上置信区间(UCB)抽样策略，使用非对称置信区间。这些区间随着样本平均值和随机变量界限之间的距离而变化，与我们的竞争对手相比，可以得到更准确和严格的报酬-成本比率估计。我们证明了我们的方法在一般情况下具有次线性实例依赖遗憾和参数 ρ ≥1的对数遗憾，并且它在合成和实际情况下一致地优于现有的策略。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Budgeted+Multi-Armed+Bandits+with+Asymmetric+Confidence+Intervals)|0|
|[Can Modifying Data Address Graph Domain Adaptation?](https://doi.org/10.1145/3637528.3672058)|Renhong Huang, Jiarong Xu, Xin Jiang, Ruichuan An, Yang Yang|Zhejiang University & Fudan University, Hangzhou, China; Xi'an Jiaotong University, Xi'an, China; Zhejiang University, Hangzhou, China; Fudan University, Shanghai, China; Lehigh University, Bethlehem, PA, USA|Graph neural networks (GNNs) have demonstrated remarkable success in numerous graph analytical tasks. Yet, their effectiveness is often compromised in real-world scenarios due to distribution shifts, limiting their capacity for knowledge transfer across changing environments or domains. Recently, Unsupervised Graph Domain Adaptation (UGDA) has been introduced to resolve this issue. UGDA aims to facilitate knowledge transfer from a labeled source graph to an unlabeled target graph. Current UGDA efforts primarily focus on model-centric methods, such as employing domain invariant learning strategies and designing model architectures. However, our critical examination reveals the limitations inherent to these model-centric methods, while a data-centric method allowed to modify the source graph provably demonstrates considerable potential. This insight motivates us to explore UGDA from a data-centric perspective. By revisiting the theoretical generalization bound for UGDA, we identify two data-centric principles for UGDA: alignment principle and rescaling principle. Guided by these principles, we propose GraphAlign, a novel UGDA method that generates a small yet transferable graph. By exclusively training a GNN on this new graph with classic Empirical Risk Minimization (ERM), GraphAlign attains exceptional performance on the target graph. Extensive experiments under various transfer scenarios demonstrate the GraphAlign outperforms the best baselines by an average of 2.16%, training on the generated graph as small as 0.25~1% of the original training graph.|图形神经网络(GNN)在许多图形分析任务中取得了显著的成功。然而，在现实世界中，由于分布变化，它们的有效性往往受到影响，限制了它们在不断变化的环境或领域中进行知识转移的能力。近年来，无监督图域自适应技术(UGDA)被引入解决这一问题。UGDA 的目的是促进知识从一个有标记的源图向一个无标记的目标图的转移。目前 UGDA 的工作主要集中在以模型为中心的方法上，如使用领域不变学习策略和设计模型体系结构。然而，我们的批判性研究揭示了这些以模型为中心的方法固有的局限性，而允许修改源图的以数据为中心的方法证明了相当大的潜力。这种洞察力促使我们从以数据为中心的角度探索 UGDA。通过回顾 UGDA 的理论推广界限，我们确定了 UGDA 的两个以数据为中心的原则: 对齐原则和重标度原则。在这些原则的指导下，我们提出了 GraphAlign，一种新颖的 UGDA 方法，它可以生成一个小而可转移的图。通过使用经典的经验风险最小化(ERM)在这个新图上专门训练 GNN，GraphAlign 在目标图上获得了出色的性能。在各种传输场景下进行的大量实验表明，GraphAlign 的性能平均比最佳基线高出2.16% ，在生成的图上进行训练，训练量仅为原始训练图的0.25 ~ 1% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+Modifying+Data+Address+Graph+Domain+Adaptation?)|0|
|[Uplift Modelling via Gradient Boosting](https://doi.org/10.1145/3637528.3672019)|Bulat Ibragimov, Anton Vakhrushev|; Sber AI Lab, Moscow, Russian Federation|The Gradient Boosting machine learning ensemble algorithm, well-known for its proficiency and superior performance in intricate machine learning tasks, has encountered limited success in the realm of uplift modeling. Uplift modeling is a challenging task that necessitates a known target for the precise computation of the training gradient. The prevailing two-model strategies, which separately model treatment and control outcomes, are encumbered with limitations as they fail to directly tackle the uplift problem. This paper presents an innovative approach to uplift modeling that employs Gradient Boosting. Unlike previous works, our algorithm utilizes multioutput boosting model and calculates the uplift gradient based on intermediate surrogate predictions and directly models the concealed target. This method circumvents the requirement for a known target and addresses the uplift problem more effectively than existing solutions. Moreover, we broaden the scope of this solution to encompass multitreatment settings, thereby enhancing its applicability. This novel approach not only overcomes the limitations of the traditional two-model strategies but also paves the way for more effective and efficient uplift modeling using Gradient Boosting.|梯度提升机器学习集成算法以其在复杂的机器学习任务中的熟练程度和卓越的性能而闻名，但在提升建模领域却只取得了有限的成功。提升建模是一项具有挑战性的任务，为了精确计算训练梯度，需要一个已知的目标。流行的两个模型的策略，其中单独的模型治疗和控制结果，受到限制，因为他们不能直接解决提升问题。本文提出了一个创新的方法来提升模型的使用梯度提升。与以往的算法不同，该算法采用多输出增强模型，基于中间代理预测计算上升梯度，直接对隐藏目标进行建模。该方法规避了对已知目标的要求，比现有方法更有效地解决了抬升问题。此外，我们扩大了这种解决方案的范围，以包括多种治疗设置，从而增强其适用性。这种新颖的方法不仅克服了传统的双模型策略的局限性，而且为利用梯度提升建立更有效和高效的抬升模型铺平了道路。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Uplift+Modelling+via+Gradient+Boosting)|0|
|[Mutual Distillation Extracting Spatial-temporal Knowledge for Lightweight Multi-channel Sleep Stage Classification](https://doi.org/10.1145/3637528.3671981)|Ziyu Jia, Haichao Wang, Yucheng Liu, Tianzi Jiang|Tsinghua-Berkeley Shenzhen Institute, Tsinghua University, Shenzhen, China; Institute of Automation, Chinese Academy of Science, Beijing, China; University of Southern California, Los Angeles, USA; Institute of Automation, Chinese Academy of Sciences, Beijing, China|Sleep stage classification has important clinical significance for the diagnosis of sleep-related diseases. To pursue more accurate sleep stage classification, multi-channel sleep signals are widely used due to the rich spatial-temporal information contained. However, it leads to a great increment in the size and computational costs, which constrain the application of multi-channel sleep models on hardware devices. Knowledge distillation is an effective way to compress models, yet existing knowledge distillation methods cannot fully extract and transfer the spatial-temporal knowledge in the multi-channel sleep signals. To solve the problem, we propose a general knowledge distillation framework for multi-channel sleep stage classification called spatial-temporal mutual distillation. Based on the spatial relationship of human body and the temporal transition rules of sleep signals, the spatial and temporal modules are designed to extract the spatial-temporal knowledge, thus help the lightweight student model learn the rich spatial-temporal knowledge from large-scale teacher model. The mutual distillation framework transfers the spatial-temporal knowledge mutually. Teacher model and student model can learn from each other, further improving the student model. The results on the ISRUC-III and MASS-SS3 datasets show that our proposed framework compresses the sleep models effectively with minimal performance loss and achieves the state-of-the-art performance compared to the baseline methods.|睡眠分期对睡眠相关疾病的诊断具有重要的临床意义。为了追求更准确的睡眠阶段分类，多通道睡眠信号由于包含了丰富的时空信息而被广泛应用。然而，这会导致系统规模和计算成本的大幅度增加，从而限制了多通道睡眠模型在硬件设备上的应用。知识提取是一种有效的模型压缩方法，但现有的知识提取方法不能完全提取和转移多通道睡眠信号中的时空知识。为了解决这个问题，我们提出了一个通用的多通道睡眠阶段分类知识提取框架，称为时空互提取。基于人体的空间关系和睡眠信号的时间转换规则，设计了空间和时间模块来提取时空知识，从而帮助轻量级学生模型从大规模教师模型中学习到丰富的时空知识。相互精馏框架相互传递时空知识。教师模式与学生模式可以相互借鉴，进一步完善学生模式。ISRUC-III 和 MASS-SS3数据集的结果表明，我们提出的框架有效地压缩了睡眠模型，性能损失最小，并达到了最先进的性能相比，基线方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mutual+Distillation+Extracting+Spatial-temporal+Knowledge+for+Lightweight+Multi-channel+Sleep+Stage+Classification)|0|
|[Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain](https://doi.org/10.1145/3637528.3672069)|Amin Karimi Monsefi, Payam Karisani, Mengxi Zhou, Stacey Choi, Nathan Doble, Heng Ji, Srinivasan Parthasarathy, Rajiv Ramnath|Department of Computer Science and Engineering, The Ohio State University, Columbus, OH, USA; Department of Computer Science, University of Illinois Urbana-Champaign, Urbana, IL, USA; Department of Ophthalmology and Visual Science, The Ohio State University, Columbus, OH, USA|Standard modern machine-learning-based imaging methods have faced challenges in medical applications due to the high cost of dataset construction and, thereby, the limited labeled training data available. Additionally, upon deployment, these methods are usually used to process a large volume of data on a daily basis, imposing a high maintenance cost on medical facilities. In this paper, we introduce a new neural network architecture, termed LoGoNet, with a tailored self-supervised learning (SSL) method to mitigate such challenges. LoGoNet integrates a novel feature extractor within a U-shaped architecture, leveraging Large Kernel Attention (LKA) and a dual encoding strategy to capture both long-range and short-range feature dependencies adeptly. This is in contrast to existing methods that rely on increasing network capacity to enhance feature extraction. This combination of novel techniques in our model is especially beneficial in medical image segmentation, given the difficulty of learning intricate and often irregular body organ shapes, such as the spleen. Complementary, we propose a novel SSL method tailored for 3D images to compensate for the lack of large labeled datasets. Our method combines masking and contrastive learning techniques within a multi-task learning framework and is compatible with both Vision Transformer (ViT) and CNN-based models. We demonstrate the efficacy of our methods in numerous tasks across two standard datasets (i.e., BTCV and MSD). Benchmark comparisons with eight state-of-the-art models highlight LoGoNet's superior performance in both inference time and accuracy. Code available at: https://github.com/aminK8/Masked-LoGoNet.|标准的现代机器学习成像方法在医学应用中面临的挑战，由于高成本的数据集建设，从而，有限的标记训练数据可用。此外，在部署时，这些方法通常用于每天处理大量数据，给医疗设施带来高昂的维护成本。在本文中，我们介绍了一种新的神经网络结构，称为 LoGoNet，它采用一种量身定制的自监督学习(SSL)方法来缓解这种挑战。LoGoNet 在 U 形架构中集成了一种新颖的特征提取器，利用大内核注意力(LKA)和双重编码策略来巧妙地捕获长期和短期特征依赖。这与现有的依靠增加网络容量来增强特征提取的方法形成了对比。在我们的模型中，这种新技术的结合在医学图像分割中尤其有益，因为学习复杂且通常不规则的身体器官形状(如脾脏)很困难。作为补充，我们提出了一种新的针对三维图像的 SSL 方法，以弥补缺少大型标记数据集的不足。该方法将掩蔽学习和对比学习技术结合在一个多任务学习框架中，兼容视觉变换器(ViT)和基于 CNN 的模型。我们证明了我们的方法在两个标准数据集(即 BTCV 和 MSD)的许多任务中的有效性。与八个最先进模型的基准比较突出了 LoGoNet 在推断时间和准确性方面的卓越性能。密码:  https://github.com/amink8/masked-logonet。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Masked+LoGoNet:+Fast+and+Accurate+3D+Image+Analysis+for+Medical+Domain)|0|
|[Fast and Accurate Domain Adaptation for Irregular Tensor Decomposition](https://doi.org/10.1145/3637528.3671670)|Junghun Kim, Ka Hyun Park, JunGi Jang, U Kang|Seoul National University, Seoul, Republic of Korea; University of Illinois at Urbana-Champaign, Illinois, IL, USA|Given an irregular tensor from a newly emerging domain, how can we quickly and accurately capture its patterns utilizing existing irregular tensors in multiple domains? The problem is of great importance for various tasks such as finding patterns of a new disease using pre-existing diseases data. This is challenging as new target tensors have limited information due to their recent emergence. Thus, carefully utilizing the existing source tensors for analyzing the target tensor is helpful. PARAFAC2 decomposition is a strong tool for finding the patterns of irregular tensors, and the patterns are used in many applications such as missing value prediction and anomaly detection. However, previous PARAFAC2-based works cannot adaptably handle newly emerging target tensors utilizing the source tensors. In this work, we propose Meta-P2, a fast and accurate domain adaptation method for irregular tensor decomposition. Meta-P2 generates a meta factor matrix from the multiple source domains, by domain adaptation and meta-update steps. Meta-P2 quickly and accurately finds the patterns of the new irregular tensor utilizing the meta factor matrix. Extensive experiments on real-world datasets show that Meta-P2 achieves the best performance in various downstream tasks including missing value prediction and anomaly detection tasks.|假设一个不规则张量来自一个新兴的领域，我们如何能够快速准确地捕捉其模式利用现有的不规则张量在多个领域？这个问题对于各种任务都非常重要，例如利用已有的疾病数据找到新疾病的模式。这是具有挑战性的，因为新的目标张量由于最近的出现而信息有限。因此，仔细利用现有的源张量来分析目标张量是有帮助的。PARAFAC2分解是寻找不规则张量模式的一个强有力的工具，这些模式在许多应用中被使用，例如缺失值预测和异常检测。然而，以往基于 PARAFAC2的工作不能自适应地处理新出现的目标张量利用源张量。在这项工作中，我们提出了 Meta-P2，一个快速和准确的区域自适应方法的不规则张量分解。元 P2通过领域适应和元更新步骤从多个源域生成元因子矩阵。Meta-P2利用元因子矩阵快速准确地找到新的不规则张量的模式。对真实世界数据集的大量实验表明，Meta-p2在各种下游任务(包括缺失值预测和异常检测任务)中取得了最佳性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+and+Accurate+Domain+Adaptation+for+Irregular+Tensor+Decomposition)|0|
|[SLADE: Detecting Dynamic Anomalies in Edge Streams without Labels via Self-Supervised Learning](https://doi.org/10.1145/3637528.3671845)|Jongha Lee, Sunwoo Kim, Kijung Shin|KAIST, Seoul, Republic of Korea|To detect anomalies in real-world graphs, such as social, email, and financial networks, various approaches have been developed. While they typically assume static input graphs, most real-world graphs grow over time, naturally represented as edge streams. In this context, we aim to achieve three goals: (a) instantly detecting anomalies as they occur, (b) adapting to dynamically changing states, and (c) handling the scarcity of dynamic anomaly labels. In this paper, we propose SLADE (Self-supervised Learning for Anomaly Detection in Edge Streams) for rapid detection of dynamic anomalies in edge streams, without relying on labels. SLADE detects the shifts of nodes into abnormal states by observing deviations in their interaction patterns over time. To this end, it trains a deep neural network to perform two self-supervised tasks: (a) minimizing drift in node representations and (b) generating long-term interaction patterns from short-term ones. Failure in these tasks for a node signals its deviation from the norm. Notably, the neural network and tasks are carefully designed so that all required operations can be performed in constant time (w.r.t. the graph size) in response to each new edge in the input stream. In dynamic anomaly detection across four real-world datasets, SLADE outperforms nine competing methods, even those leveraging label supervision. Our code and datasets are available at https://github.com/jhsk777/SLADE.|为了检测现实世界图表中的异常，如社交、电子邮件和金融网络，已经开发了各种方法。虽然它们通常假设静态输入图，但大多数真实世界的图随着时间的推移而增长，自然地表示为边流。在这种背景下，我们的目标是实现三个目标: (a)在异常发生时即时检测异常，(b)适应动态变化的状态，和(c)处理动态异常标签的稀缺性。在这篇文章中，我们提出了边缘流异常检测的自我监督学习(SLADE)来快速检测边缘流中的动态异常，而不依赖于标签。SLADE 通过观察节点交互模式随时间的变化来检测节点进入异常状态的变化。为此，它训练一个深层神经网络来执行两个自我监督的任务: (a)最小化节点表示的漂移和(b)从短期的产生长期的交互模式。节点在这些任务中的失败表明它偏离了规范。值得注意的是，神经网络和任务都经过了精心设计，以便所有需要的操作都可以在不变的时间内执行(图形大小) ，以响应输入流中的每个新边缘。在跨越四个现实世界数据集的动态异常检测中，SLADE 的表现优于9种竞争方法，甚至优于那些利用标签监督的方法。我们的代码和数据集 https://github.com/jhsk777/slade 可用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SLADE:+Detecting+Dynamic+Anomalies+in+Edge+Streams+without+Labels+via+Self-Supervised+Learning)|0|
|[Scalable Multitask Learning Using Gradient-based Estimation of Task Affinity](https://doi.org/10.1145/3637528.3671835)|Dongyue Li, Aneesh Sharma, Hongyang R. Zhang|Google, Mountain View, USA; Northeastern University, Boston, USA|Multitask learning is a widely used paradigm for training models on diverse tasks, with applications ranging from graph neural networks to language model fine-tuning. Since tasks may interfere with each other, a key notion for modeling their relationships is task affinity. This includes pairwise task affinity, computed among pairs of tasks, and higher-order affinity, computed among subsets of tasks. Naively computing either of them requires repeatedly training on data pooled from various task combinations, which is computationally intensive. We present a new algorithm Grad-TAG that can estimate task affinities without this repeated training. The key idea of Grad-TAG is to train a "base" model for all tasks and then use a linearization technique to estimate the loss of any other model with a specific task combination. The linearization works by computing a gradient-based first-order approximation of the loss, using low-dimensional projections of gradients as features in a logistic regression trained to predict labels for the specific task combination. We show theoretically that the linearized model can provably approximate the loss when the gradient-based approximation is accurate, and also empirically verify that on several large models. Then, given the estimated task affinity matrix, we design a semi-definite program for clustering to group similar tasks that maximize the average density of clusters. We evaluate Grad-TAG's performance across seven datasets, including multi-label classification on graphs, and instruction fine-tuning of language models. Our results show that our task affinity estimates are within 2.7% distance of the true affinities while needing only 3% of FLOPs compared to full training. On our largest graph with 21M edges and 500 labeling tasks, our algorithm delivers an estimate accurate to within 5% of the true affinities, while using only 112.3 GPU hours. Our results show that Grad-TAG achieves excellent performance and runtime tradeoffs compared to existing approaches.|多任务学习是一种广泛应用于不同任务的训练模型，应用范围从图形神经网络到语言模型微调。由于任务之间可能会相互干扰，因此对它们的关系进行建模的一个关键概念是任务相关性。这包括成对任务关联(在任务对之间计算)和高阶关联(在任务子集之间计算)。天真地计算其中任何一个都需要对从各种任务组合中汇集的数据进行反复训练，这是计算密集型的。我们提出了一种新的算法梯度 TAG，可以估计任务的亲和力，而无需这种重复的训练。梯度 TAG 的核心思想是为所有任务训练一个“基本”模型，然后使用线性化技术估计任何其他模型与特定任务组合的损失。线性化的工作原理是通过计算基于梯度的一阶近似损失，使用梯度的低维投影作为特征的 Logit模型训练来预测特定任务组合的标签。从理论上证明了当基于梯度的近似准确时，线性化模型可以有效地逼近损失，并在几个大型模型上进行了实验验证。然后，给出估计的任务亲和矩阵，设计一个半确定的聚类程序，将相似的任务分组，最大化聚类的平均密度。我们评估了 Grad-TAG 在七个数据集上的性能，包括图的多标签分类和语言模型的指令微调。我们的研究结果表明，我们的任务亲和力估计在2.7% 的距离真正的亲和力，而只需要3% 的 FLOP 相比，充分训练。在我们最大的21M 边和500个标记任务的图表中，我们的算法只使用了112.3 GPU 时间，但是估计的精确度在真实亲和力的5% 以内。我们的研究结果表明，与现有的方法相比，Grad-TAG 实现了出色的性能和运行时折衷。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scalable+Multitask+Learning+Using+Gradient-based+Estimation+of+Task+Affinity)|0|
|[Truthful Bandit Mechanisms for Repeated Two-stage Ad Auctions](https://doi.org/10.1145/3637528.3671813)|Haoming Li, Yumou Liu, Zhenzhe Zheng, Zhilin Zhang, Jian Xu, Fan Wu|Alibaba Group, Beijing, China; The Chinese University of Hong Kong, Shenzhen, Shenzhen, China; Shanghai Jiao Tong University, Shanghai, China|Online advertising platforms leverage a two-stage auction architecture to deliver personalized ads to users with low latency. The first stage efficiently selects a small subset of promising candidates out of the complete pool of ads. In the second stage, an auction is conducted within the subset to determine the winning ad for display, using click-through-rate predictions from the second-stage machine learning model. In this work, we investigate the online learning process of the first-stage subset selection policy, while ensuring game-theoretic properties in repeated two-stage ad auctions. Specifically, we model the problem as designing a combinatorial bandit mechanism with a general reward function, as well as additional requirements of truthfulness and individual rationality (IR). We establish an O(T) regret lower bound for truthful bandit mechanisms, which demonstrates the challenge of simultaneously achieving allocation efficiency and truthfulness. To circumvent this impossibility result, we introduce truthful α-approximation oracles and evaluate the bandit mechanism through α-approximation regret. Two mechanisms are proposed, both of which are ex-post truthful and ex-post IR. The first mechanism is an explore-then-commit mechanism with regret O(T2/3 ), and the second mechanism achieves an improved O(log T /ΔΦ2) regret where ΔΦ is a distribution-dependent gap, but requires additional assumptions on the oracles and information about the strategic bidders.|在线广告平台利用两阶段拍卖架构向低延迟的用户提供个性化广告。第一阶段有效地从完整的广告库中选出一小部分有前途的候选人。在第二阶段，使用第二阶段机器学习模型的点击率预测，在子集内进行拍卖，以确定用于显示的获胜广告。本文在保证重复两阶段广告拍卖的博弈性质的前提下，研究了第一阶段子集选择策略的在线学习过程。具体地说，我们将这个问题建模为具有一般奖励函数的组合强盗机制的设计，以及对真实性和个体理性的附加要求。我们建立了真实性盗贼机制的 O (T)后悔下限，这表明了同时实现分配效率和真实性的挑战。为了避免这种不可能的结果，我们引入了真实的 α-近似神谕，并通过 α-近似悔恨来评估盗贼的机制。提出了两种机制，即事后真实机制和事后信息检索机制。第一种机制是带有遗憾 O (T2/3)的探索-然后提交机制，第二种机制实现了一个改进的 O (log T/ΔΦ2)遗憾，其中 ΔΦ 是一个分布依赖的缺口，但需要额外的先知假设和关于战略投标人的信息。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Truthful+Bandit+Mechanisms+for+Repeated+Two-stage+Ad+Auctions)|0|
|[Self-Distilled Disentangled Learning for Counterfactual Prediction](https://doi.org/10.1145/3637528.3671782)|Xinshu Li, Mingming Gong, Lina Yao|The University of New South Wales, Sydney, Australia; CSIRO's Data61 & The University of New South Wales, Sydney, Australia; The University of Melbourne & MBZUAI, Melbourne, Australia|The advancements in disentangled representation learning significantlyenhance the accuracy of counterfactual predictions by granting precise controlover instrumental variables, confounders, and adjustable variables. Anappealing method for achieving the independent separation of these factors ismutual information minimization, a task that presents challenges in numerousmachine learning scenarios, especially within high-dimensional spaces. Tocircumvent this challenge, we propose the Self-Distilled Disentanglementframework, referred to as SD^2. Grounded in information theory, it ensurestheoretically sound independent disentangled representations without intricatemutual information estimator designs for high-dimensional representations. Ourcomprehensive experiments, conducted on both synthetic and real-world datasets,confirms the effectiveness of our approach in facilitating counterfactualinference in the presence of both observed and unobserved confounders.|在分离表征学习的进步显着提高准确性的反事实预测授予精确控制工具变量，混杂因素和可调变量。实现这些因素独立分离的一个有吸引力的方法是相互信息最小化，这个任务在许多机器学习场景中提出了挑战，特别是在高维空间中。为了规避这个挑战，我们提出了自我提取的分离框架，称为 SD ^ 2。它以信息论为理论基础，保证了理论上独立的解纠缠表示，而不需要对高维表示进行复杂的互信息估计器设计。我们在合成和真实世界数据集上进行的综合实验证实了我们的方法在促进反事实推理方面的有效性，在观察到和未观察到的混杂因素存在的情况下。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Distilled+Disentangled+Learning+for+Counterfactual+Prediction)|0|
|[Predicting Long-term Dynamics of Complex Networks via Identifying Skeleton in Hyperbolic Space](https://doi.org/10.1145/3637528.3671968)|Ruikun Li, Huandong Wang, Jinghua Piao, Qingmin Liao, Yong Li|Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Department of Electronic Engineering BNRist, Tsinghua University, Beijing, China|Learning complex network dynamics is fundamental for understanding, modeling, and controlling real-world complex systems. Though great efforts have been made to predict the future states of nodes on networks, the capability of capturing long-term dynamics remains largely limited. This is because they overlook the fact that long-term dynamics in complex network are predominantly governed by their inherent low-dimensional manifolds, i.e., skeletons. Therefore, we propose the Dynamics-Invariant Skeleton Neural Network (DiskNet), which identifies skeletons of complex networks based on the renormalization group structure in hyperbolic space to preserve both topological and dynamics properties. Specifically, we first condense complex networks with various dynamics into simple skeletons through physics-informed hyperbolic embeddings. Further, we design graph neural ordinary differential equations to capture the condensed dynamics on the skeletons. Finally, we recover the skeleton networks and dynamics to the original ones using a degree-based super-resolution module. Extensive experiments across three representative dynamics as well as five real-world and two synthetic networks demonstrate the superior performances of the proposed DiskNet, which outperforms the state-of-the-art baselines by an average of 10.18% in terms of long-term prediction accuracy. Code for reproduction is available at: https://github.com/tsinghua-fib-lab/DiskNet.|学习复杂网络动力学是理解、建模和控制现实世界复杂系统的基础。尽管人们在预测网络节点的未来状态方面做出了巨大的努力，但是捕捉长期动态的能力仍然受到很大的限制。这是因为他们忽略了这样一个事实，即复杂网络中的长期动力学主要受其固有的低维流形支配，即骨架。因此，我们提出了动力学不变骨架神经网络(diskNet) ，它基于重整化群的双曲空间结构来识别复杂网络的骨架，以保持拓扑和动力学特性。具体来说，我们首先通过基于物理信息的双曲嵌入将具有各种动力学的复杂网络压缩成简单的骨架。进一步，我们设计图形神经元常微分方程来捕捉骨架上的凝聚动力学。最后，我们使用基于度的超分辨率模型恢复骨架网络和动力学模型。在三个有代表性的动态以及五个真实世界和两个合成网络上进行的广泛实验证明了所提议的 DiskNet 的优越性能，它在长期预测准确性方面比最先进的基线平均高出10.18% 。复制代码可在以下 https://github.com/tsinghua-fib-lab/disknet 找到:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Predicting+Long-term+Dynamics+of+Complex+Networks+via+Identifying+Skeleton+in+Hyperbolic+Space)|0|
|[Image Similarity Using an Ensemble of Context-Sensitive Models](https://doi.org/10.1145/3637528.3672004)|Zukang Liao, Min Chen|University of Oxford, Oxford, United Kingdom|Image similarity has been extensively studied in computer vision. In recent years, machine-learned models have shown their ability to encode more semantics than traditional multivariate metrics. However, in labelling semantic similarity, assigning a numerical score to a pair of images is impractical, making the improvement and comparisons on the task difficult. In this work, we present a more intuitive approach to build and compare image similarity models based on labelled data in the form of A:R vs B:R, i.e., determining if an image A is closer to a reference image R than another image B. We address the challenges of sparse sampling in the image space (R, A, B) and biases in the models trained with context-based data by using an ensemble model. Our testing results show that the ensemble model constructed performs ~5% better than the best individual context-sensitive models. They also performed better than the models that were directly fine-tuned using mixed imagery data as well as existing deep embeddings, e.g., CLIP [30] and DINO [3]. This work demonstrates that context-based labelling and model training can be effective when an appropriate ensemble approach is used to alleviate the limitation due to sparse sampling.|图像相似性在计算机视觉中得到了广泛的研究。近年来，机器学习模型已经显示出它们比传统的多元度量标准具有更强的语义编码能力。然而，在标注语义相似度时，为一对图像分配一个数值得分是不切实际的，这给任务的改进和比较带来了困难。在这项工作中，我们提出了一个更直观的方法来建立和比较图像相似性模型的基础上的标记数据的形式 A: R 对 B: R，即，确定是否一个图像 A 更接近一个参考图像 R 比另一个图像 B。针对图像空间(R，A，B)的稀疏采样和基于上下文数据训练的模型中的偏差问题，提出了一种集成模型。我们的测试结果表明，所构建的集成模型比最好的个体上下文敏感模型的性能要好约5% 。他们也比使用混合图像数据以及现有的深度嵌入(例如 CLIP [30]和 DINO [3])直接微调的模型表现得更好。这项工作表明，基于上下文的标记和模型训练可以有效地使用适当的集成方法，以减轻由于稀疏抽样的限制。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Image+Similarity+Using+an+Ensemble+of+Context-Sensitive+Models)|0|
|[Neural Collapse Inspired Debiased Representation Learning for Min-max Fairness](https://doi.org/10.1145/3637528.3671902)|Shenyu Lu, Junyi Chai, Xiaoqian Wang|Purdue University, West Lafayette, IN, USA|Although machine learning algorithms demonstrate impressive performance, their trustworthiness remains a critical issue, particularly concerning fairness when implemented in real-world applications. Many notions of group fairness aim to minimize disparities in performance across protected groups. However, it can inadvertently reduce performance in certain groups, leading to sub-optimal outcomes. In contrast, Min-max group fairness notion prioritizes the improvement for the worst-performing group, thereby advocating a utility-promoting approach to fairness. However, it has been proven that existing efforts to achieve Min-max fairness exhibit limited effectiveness. In response to this challenge, we leverage the recently proposed "Neural Collapse'' framework to re-examine Empirical Risk Minimization (ERM) training, specifically investigating the root causes of poor performance in minority groups. The layer-peeled model is employed to decompose a network into two parts: an encoder to learn latent representation, and a subsequent classifier, with a systematic characterization of their training behaviors being conducted. Our analysis reveals that while classifiers achieve maximum separation, the separability of representations is insufficient, particularly for minority groups. This indicates the sub-optimal performance in minority groups stems from less separable representations, rather than classifiers. To tackle this issue, we introduce a novel strategy that incorporates a frozen classifier to directly enhance representation. Furthermore, we introduce two easily implemented loss functions to guide the learning process. The experimental assessments carried out on real-world benchmark datasets spanning the domains of Computer Vision, Natural Language Processing, and Tabular data demonstrate that our approach outperforms existing state-of-the-art methods in promoting the Min-max fairness notion.|尽管机器学习算法表现出了令人印象深刻的性能，但它们的可信性仍然是一个关键问题，特别是在实际应用中实现时的公平性。群体公平的许多概念旨在最大限度地缩小不同受保护群体之间的绩效差距。然而，它可能无意中降低某些群体的绩效，导致次优结果。相比之下，最小-最大群体公平理念优先考虑绩效最差群体的改善，从而提倡效用促进的公平方法。然而，已经证明现有的实现最小最大公平性的努力效果有限。为了应对这一挑战，我们利用最近提出的“神经崩溃”框架来重新审视经验风险最小化(ERM)训练，特别是调查少数群体表现不佳的根本原因。分层剥离模型被用来将网络分解为两部分: 一个编码器来学习潜在表征，一个后续的分类器，系统的角色塑造他们的训练行为被执行。我们的分析表明，虽然量词实现了最大的分离，但表征的可分性是不够的，特别是对于少数群体。这表明少数群体的次优性能源于较少的可分离表示，而不是分类器。为了解决这个问题，我们引入了一个新的策略，包括一个冻结的分类器，直接增强表示。此外，我们还引入了两个易于实现的损失函数来指导学习过程。对计算机视觉、自然语言处理和表格数据领域的现实世界基准数据集进行的实验评估表明，我们的方法在促进最小最大公平概念方面优于现有的最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Collapse+Inspired+Debiased+Representation+Learning+for+Min-max+Fairness)|0|
|[AdaGMLP: AdaBoosting GNN-to-MLP Knowledge Distillation](https://doi.org/10.1145/3637528.3671699)|Weigang Lu, Ziyu Guan, Wei Zhao, Yaming Yang|Xidian University, Xi'an, Shannxi, China; Xidian University, Xi'an, Shaanxi, China|Graph Neural Networks (GNNs) have revolutionized graph-based machinelearning, but their heavy computational demands pose challenges forlatency-sensitive edge devices in practical industrial applications. Inresponse, a new wave of methods, collectively known as GNN-to-MLP KnowledgeDistillation, has emerged. They aim to transfer GNN-learned knowledge to a moreefficient MLP student, which offers faster, resource-efficient inference whilemaintaining competitive performance compared to GNNs. However, these methodsface significant challenges in situations with insufficient training data andincomplete test data, limiting their applicability in real-world applications.To address these challenges, we propose AdaGMLP, an AdaBoosting GNN-to-MLPKnowledge Distillation framework. It leverages an ensemble of diverse MLPstudents trained on different subsets of labeled nodes, addressing the issue ofinsufficient training data. Additionally, it incorporates a Node Alignmenttechnique for robust predictions on test data with missing or incompletefeatures. Our experiments on seven benchmark datasets with different settingsdemonstrate that AdaGMLP outperforms existing G2M methods, making it suitablefor a wide range of latency-sensitive real-world applications. We havesubmitted our code to the GitHub repository(https://github.com/WeigangLu/AdaGMLP-KDD24).|图形神经网络(GNN)已经给基于图的机器学习带来了革命性的变化，但是它们繁重的计算需求给实际工业应用中的延迟敏感边缘设备带来了挑战。响应，一个新的方法的浪潮，统称为 GNN 到 MLP 知识蒸馏，已经出现。他们的目标是将 GNN 学到的知识传授给一个更有效率的 MLP 学生，MLP 学生能够提供更快速、资源效率更高的推理，同时保持与 GNN 学生相比的竞争力。然而，这些方法在训练数据不足和测试数据不完整的情况下面临着重大挑战，限制了它们在实际应用中的适用性。为了解决这些挑战，我们提出 AdaGMLP，一个 AdaBoosted GNN-to-MLP 知识提取框架。它利用不同的 MLP 学生在不同的标记节点子集上接受训练的集合，解决了训练数据不足的问题。此外，它还结合了节点对齐技术，用于对缺少或不完整特性的测试数据进行健壮的预测。我们在七个不同设置的基准数据集上的实验表明，AdaGMLP 优于现有的 G2M 方法，使其适用于各种对延迟敏感的现实世界应用。我们已经将代码提交给了 GitHub 存储库( https://GitHub.com/weiganglu/adagmlp-kdd24)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AdaGMLP:+AdaBoosting+GNN-to-MLP+Knowledge+Distillation)|0|
|[Handling Varied Objectives by Online Decision Making](https://doi.org/10.1145/3637528.3671812)|Lanjihong Ma, ZhenYu Zhang, YaoXiang Ding, ZhiHua Zhou|; State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China; Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan|Conventional machine learning typically assume a fixed learning objective throughout the learning process. However, for real-world tasks in open and dynamic environments, objectives can change frequently. For example, in autonomous driving, a car has several default modes, but a user's concern for speed and fuel consumption varies depending on road conditions and personal needs. We formulate this problem as learning with varied objectives (LVO), where the goal is to optimize a dynamic weighted combination of multiple sub-objectives by sequentially selecting actions that incur different losses on these sub-objectives. We propose the VaRons algorithm, which estimates the action-wise performance on each sub-objective and adaptively selects decisions according to the dynamic requirements on different sub-objectives. Further, we extend our approach to cases involving contextual representations and propose the ConVaRons algorithm, assuming parameterized linear structure that links contextual features to the main objective. Both the VaRons and ConVaRons are provably minimax optimal with respect to the time horizon T, with ConVaRons showing better dependency with the number of sub-objectives K. Experiments on dynamic classifier and real-world cluster service allocation tasks validate the effectiveness of our methods and support our theoretical findings.|传统的机器学习通常在整个学习过程中假定一个固定的学习目标。然而，对于开放和动态环境中的实际任务，目标可能会频繁变化。例如，在自动驾驶中，汽车有几种默认模式，但是用户对于速度和油耗的关注取决于路况和个人需求。我们将这个问题表述为具有不同目标(LVO)的学习，其目标是通过依次选择在这些子目标上导致不同损失的行动来优化多个子目标的动态加权组合。我们提出了 VaRons 算法，该算法估计每个子目标的行动性能，并根据不同子目标的动态需求自适应地选择决策。进一步，我们将我们的方法扩展到涉及上下文表示的情况，并提出了 ConVaRons 算法，假设参数化的线性结构，连接上下文特征的主要目标。VaRons 和 ConVaRons 都可证明是最小最优的时间范围 T，与 ConVaRons 显示更好的依赖与子目标的数量 K 的实验动态分类器和现实世界的集群服务分配任务验证了我们的方法的有效性，并支持我们的理论研究结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Handling+Varied+Objectives+by+Online+Decision+Making)|0|
|[Quantifying and Estimating the Predictability Upper Bound of Univariate Numeric Time Series](https://doi.org/10.1145/3637528.3671995)|Jamal Mohammed, Michael H. Böhlen, Sven Helmer|University of Zurich, Zurich, Switzerland|The intrinsic predictability of a given time series indicates how well an (ideal) algorithm could potentially predict it when trained on the time series data. Being able to compute the intrinsic predictability helps the developers of prediction algorithms immensely in deciding whether there is further optimization potential, as it tells them how close they are to what is (theoretically) achievable. We call the intrinsic predictability the predictability upper bound ¶imax and propose a novel method for quantifying and estimating it for univariate numeric time series. So far, this has only been done for symbolic time series, even though most real-world time series are numeric by nature. We base our technique on the close relationship between entropy and predictability, utilizing the entropy rate of a time series to compute ¶imax . Since existing entropy rate estimators, such as those based on the Lempel-Ziv compression algorithm, only work for symbolic data, we develop new estimators using tolerance thresholds for matching numeric values. We demonstrate that ¶imax is an effective upper bound that characterizes the intrinsic predictability of a time series. We give formal proofs and we validate our arguments experimentally by comparing ¶imax with the prediction accuracy of different state-of-the-art models on various real-world datasets from different domains.|给定时间序列的内在可预测性表明(理想的)算法在对时间序列数据进行训练时能够很好地预测它。能够计算内在的可预测性对预测算法的开发人员决定是否存在进一步的优化潜力有很大的帮助，因为它告诉他们离(理论上)可实现的目标有多近。本文将内在可预测性称为可预测性上界 imax，提出了一种新的单变量数值时间序列的可预测性上界的量化和估计方法。到目前为止，这只是针对符号时间序列，尽管大多数现实世界的时间序列本质上是数字的。我们的技术基于熵和可预测性之间的密切关系，利用时间序列的熵率来计算 imax。由于现有的熵速率估计器，例如基于 Lempel-Ziv 压缩算法的熵速率估计器，只对符号数据起作用，因此我们使用容差阈值来对数值进行匹配。我们证明了 imax 是表征时间序列内在可预测性的一个有效上界。我们给出了形式上的证明，并且通过比较 imax 与不同领域的不同现实世界数据集上不同最新模型的预测精度，实验验证了我们的论点。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Quantifying+and+Estimating+the+Predictability+Upper+Bound+of+Univariate+Numeric+Time+Series)|0|
|[Scalable Rule Lists Learning with Sampling](https://doi.org/10.1145/3637528.3671989)|Leonardo Pellegrina, Fabio Vandin|Dept. of Information Engineering, University of Padova, Padova, Italy|Learning interpretable models has become a major focus of machine learningresearch, given the increasing prominence of machine learning in sociallyimportant decision-making. Among interpretable models, rule lists are among thebest-known and easily interpretable ones. However, finding optimal rule listsis computationally challenging, and current approaches are impractical forlarge datasets. We present a novel and scalable approach to learn nearly optimal rule listsfrom large datasets. Our algorithm uses sampling to efficiently obtain anapproximation of the optimal rule list with rigorous guarantees on the qualityof the approximation. In particular, our algorithm guarantees to find a rulelist with accuracy very close to the optimal rule list when a rule list withhigh accuracy exists. Our algorithm builds on the VC-dimension of rule lists,for which we prove novel upper and lower bounds. Our experimental evaluation onlarge datasets shows that our algorithm identifies nearly optimal rule listswith a speed-up up to two orders of magnitude over state-of-the-art exactapproaches. Moreover, our algorithm is as fast as, and sometimes faster than,recent heuristic approaches, while reporting higher quality rule lists. Inaddition, the rules reported by our algorithm are more similar to the rules inthe optimal rule list than the rules from heuristic approaches.|随着机器学习在社会决策中的重要性日益突出，学习可解释模型已经成为机器学习研究的一个主要焦点。在可解释的模型中，规则列表是最著名和易于解释的模型之一。然而，找到最佳的规则列表是计算上的挑战，并且当前的方法对于大型数据集是不切实际的。我们提出了一个新的和可扩展的方法来学习几乎最优的规则列表从大数据集。该算法采用抽样的方法，在严格保证近似质量的前提下，有效地获得了最优规则列表的近似。特别是当存在高精度规则列表时，我们的算法保证能够找到精度非常接近最优规则列表的规则列表。我们的算法建立在规则列表的 VC 维数的基础上，证明了新的上下界。我们在大型数据集上的实验评估表明，我们的算法识别几乎最优的规则列表的速度比最先进的精确方法快两数量级。此外，我们的算法与最近的启发式方法一样快，有时甚至更快，同时报告更高质量的规则列表。此外，我们的算法报告的规则更接近于最优规则列表中的规则，而不是启发式方法中的规则。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scalable+Rule+Lists+Learning+with+Sampling)|0|
|[Fredformer: Frequency Debiased Transformer for Time Series Forecasting](https://doi.org/10.1145/3637528.3671928)|Xihao Piao, Zheng Chen, Taichi Murayama, Yasuko Matsubara, Yasushi Sakurai|SANKEN, Osaka University, Osaka, Japan|The Transformer model has shown leading performance in time series forecasting. Nevertheless, in some complex scenarios, it tends to learn low-frequency features in the data and overlook high-frequency features, showing a frequency bias. This bias prevents the model from accurately capturing important high-frequency data features. In this paper, we undertake empirical analyses to understand this bias and discover that frequency bias results from the model disproportionately focusing on frequency features with higher energy. Based on our analysis, we formulate this bias and propose Fredformer, a Transformer-based framework designed to mitigate frequency bias by learning features equally across different frequency bands. This approach prevents the model from overlooking lower amplitude features important for accurate forecasting. Extensive experiments show the effectiveness of our proposed approach, which can outperform other baselines in different real-world time-series datasets. Furthermore, we introduce a lightweight variant of the Fredformer with an attention matrix approximation, which achieves comparable performance but with much fewer parameters and lower computation costs. The code is available at: https://github.com/chenzRG/Fredformer|变压器模型在时间序列预测中表现出领先的性能。然而，在一些复杂的场景中，它倾向于学习数据中的低频特征，而忽略高频特征，表现出频率偏差。这种偏差使模型无法准确地捕获重要的高频数据特征。本文通过实证分析了解这种偏差，发现模型的频率偏差过多地集中在高能量的频率特征上。基于我们的分析，我们制定了这种偏差，并提出了 Fredformer，一个基于变压器的框架，旨在减少频率偏差的学习功能在不同的频段相等。这种方法可以防止模型忽略对于精确预测非常重要的低振幅特征。大量的实验证明了该方法的有效性，在不同的实际时间序列数据集中，该方法的性能优于其他基线。此外，我们还引入了一个轻量级的 Fredform- 注意矩阵近似方法，该方法具有相当的性能，但是参数更少，计算量更小。密码可于以下 https://github.com/chenzrg/fredformer 索取:|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fredformer:+Frequency+Debiased+Transformer+for+Time+Series+Forecasting)|0|
|[ORCDF: An Oversmoothing-Resistant Cognitive Diagnosis Framework for Student Learning in Online Education Systems](https://doi.org/10.1145/3637528.3671988)|Hong Qian, Shuo Liu, Mingjia Li, Bingdong Li, Zhi Liu, Aimin Zhou|; School of Computer Science and Technology, East China Normal University, Shanghai, China|Cognitive diagnosis models (CDMs) are designed to learn students' mastery levels using their response logs. CDMs play a fundamental role in online education systems since they significantly influence downstream applications such as teachers' guidance and computerized adaptive testing. Despite the success achieved by existing CDMs, we find that they suffer from a thorny issue that the learned students' mastery levels are too similar. This issue, which we refer to as oversmoothing, could diminish the CDMs' effectiveness in downstream tasks. CDMs comprise two core parts: learning students' mastery levels and assessing mastery levels by fitting the response logs. This paper contends that the oversmoothing issue arises from that existing CDMs seldom utilize response signals on exercises in the learning part but only use them as labels in the assessing part. To this end, this paper proposes an oversmoothing-resistant cognitive diagnosis framework (ORCDF) to enhance existing CDMs by utilizing response signals in the learning part. Specifically, ORCDF introduces a novel response graph to inherently incorporate response signals as types of edges. Then, ORCDF designs a tailored response-aware graph convolution network (RGC) that effectively captures the crucial response signals within the response graph. Via ORCDF, existing CDMs are enhanced by replacing the input embeddings with the outcome of RGC, allowing for the consideration of response signals on exercises in the learning part. Extensive experiments on real-world datasets show that ORCDF not only helps existing CDMs alleviate the oversmoothing issue but also significantly enhances the models' prediction and interpretability performance. Moreover, the effectiveness of ORCDF is validated in the downstream task of computerized adaptive testing.|认知诊断模型(CDM)的目的是了解学生的掌握水平使用他们的反应日志。清洁发展机制在在线教育系统中发挥着重要作用，因为它们对教师指导和计算机化适应性测试等下游应用具有重要影响。尽管现有的清洁发展机制取得了成功，但我们发现它们存在一个棘手的问题，即学生的掌握水平过于相似。这个我们称之为过度平滑的问题，可能会削弱清洁发展机制在下游任务中的有效性。清洁发展机制包括两个核心部分: 学习学生的掌握水平和通过拟合响应日志评估掌握水平。本文认为，现有的清洁发展机制很少利用学习部分练习的反应信号，而只是在评估部分使用作为标记，从而产生了过度平滑的问题。为此，本文提出了一种抗过平滑认知诊断框架(ORCDF) ，利用学习部分的响应信号来增强现有的 CDM。具体来说，ORCDF 引入了一种新的响应图，将响应信号内在地合并为边的类型。然后，ORCDF 设计一个量身定制的响应感知图卷积网络(RGC) ，有效地捕获响应图中的关键响应信号。通过 ORCDF，现有的清洁发展机制得到了加强，将输入嵌入改为研资局的成果，从而允许在学习部分考虑练习的回应信号。在实际数据集上的大量实验表明，ORCDF 不仅有助于缓解现有 CDM 的过平滑问题，而且显著提高了模型的预测和可解释性能。在计算机自适应测试的下游任务中，验证了 ORCDF 算法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ORCDF:+An+Oversmoothing-Resistant+Cognitive+Diagnosis+Framework+for+Student+Learning+in+Online+Education+Systems)|0|
|[LARP: Language Audio Relational Pre-training for Cold-Start Playlist Continuation](https://doi.org/10.1145/3637528.3671772)|Rebecca Salganik, Xiaohao Liu, Yunshan Ma, Jian Kang, TatSeng Chua|University of Rochester, Rochester, NY, USA; National University of Singapore, Singapore, Singapore|As online music consumption increasingly shifts towards playlist-based listening, the task of playlist continuation, in which an algorithm suggests songs to extend a playlist in a personalized and musically cohesive manner, has become vital to the success of music streaming services. Currently, many existing playlist continuation approaches rely on collaborative filtering methods to perform their recommendations. However, such methods will struggle to recommend songs that lack interaction data, an issue known as the cold-start problem. Current approaches to this challenge design complex mechanisms for extracting relational signals from sparse collaborative signals and integrating them into content representations. However, these approaches leave content representation learning out of scope and utilize frozen, pre-trained content models that may not be aligned with the distribution or format of a specific musical setting. Furthermore, even the musical state-of-the-art content modules are either (1) incompatible with the cold-start setting or (2) unable to effectively integrate cross-modal and relational signals. In this paper, we introduce LARP, a multi-modal cold-start playlist continuation model, to effectively overcome these limitations. LARP is a three-stage contrastive learning framework that integrates both multi-modal and relational signals into its learned representations. Our framework uses increasing stages of task-specific abstraction: within-track (language-audio) contrastive loss, track-track contrastive loss, and track-playlist contrastive loss. Experimental results on two publicly available datasets demonstrate the efficacy of LARP over uni-modal and multi-modal models for playlist continuation in a cold-start setting. Finally, this work pioneers the perspective of addressing cold-start recommendation via relational representation learning. Code and dataset are released at: https://github.com/Rsalganik1123/LARP/|随着在线音乐消费越来越多地转向基于播放列表的聆听，播放列表延续的任务已经成为音乐流媒体服务成功的关键。在播放列表延续中，算法建议歌曲以个性化和音乐内聚的方式扩展播放列表。目前，许多现有的播放列表延续方法依赖于协同过滤方法来执行他们的建议。然而，这种方法将很难推荐缺乏交互数据的歌曲，这个问题被称为冷启动问题。目前针对这一挑战的方法设计了复杂的机制，用于从稀疏的协作信号中提取关系信号，并将其集成到内容表示中。然而，这些方法使得内容表示学习脱离了范围，并且使用了冻结的、预先训练的内容模型，这些模型可能与特定音乐环境的分布或格式不一致。此外，即使音乐国家的最先进的内容模块要么(1)与冷启动设置不兼容，要么(2)不能有效地整合跨模态和关系信号。在本文中，我们引入了 LARP，一个多模态的冷启动播放列表延续模型，以有效地克服这些局限性。LARP 是一个三阶段对比学习框架，它将多模态信号和关系信号整合到学习表示中。我们的框架使用了不断增加的特定于任务的抽象阶段: 轨道内(语言-音频)对比度丢失、轨道-轨道对比度丢失和轨道-播放列表对比度丢失。在两个公开数据集上的实验结果证明了 LARP 在冷启动环境下对播放列表延续的单模态和多模态模型的有效性。最后，本文开创了通过关系表示学习解决冷启动推荐问题的先河。代码和数据集在以下 https://github.com/rsalganik1123/larp/发布|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LARP:+Language+Audio+Relational+Pre-training+for+Cold-Start+Playlist+Continuation)|0|
|[CrossLight: Offline-to-Online Reinforcement Learning for Cross-City Traffic Signal Control](https://doi.org/10.1145/3637528.3671927)|Qian Sun, Rui Zha, Le Zhang, Jingbo Zhou, Yu Mei, Zhiling Li, Hui Xiong|; Baidu Research, Baidu Inc., Beijing, China; School of Computer Science, University of Science and Technology of China, Hefei, China; Department of Intelligent Driving Group Business Management, Baidu Inc., Beijing, China; Department of Intelligent Transportation System, Baidu Inc., Beijing, China|The recent advancements in Traffic Signal Control (TSC) have highlighted the potential of Reinforcement Learning (RL) as a promising solution to alleviate traffic congestion. Current research in this area primarily concentrates on either online or offline learning strategies, aiming to create optimized policies for specific cities. Nevertheless, the transferability of these policies to new cities is impeded by constraints such as the limited availability of high-quality data and the expensive and risky exploration process. To this end, in this paper, we present an innovative cross-city Traffic Signal Control (TSC) paradigm called CrossLight. Our approach involves meta training using offline data from source cities and adaptively fine-tuning in the target city. This novel methodology aims to address the challenges of transferring TSC policies across different cities effectively. In our proposed approach, we start by acquiring meta-decision pattern knowledge through trajectory dynamics reconstruction via pre-training in source cities. To address disparities in road network topologies between cities, we dynamically construct city topological structures based on the extracted meta-knowledge during the offline meta-training phase. These structures are then used to distill pattern-structure aware representations of decision trajectories from the source cities. To identify effective initial parameters for the learnable components, we employ the Model-Agnostic Meta-Learning (MAML) framework, a popular meta-learning approach. During adaptive fine-tuning in the target city, we introduce a replay buffer that is iteratively updated using online interactions with a rank and filter mechanism. This mechanism, along with a carefully designed exploration strategy, ensures a balance between exploitation and exploration, thereby fostering both the diversity and quality of the trajectories for fine-tuning. Finally, extensive experiments across four cities validate that CrossLight achieves comparable performance in new cities with minimal fine-tuning iterations, surpassing both existing online and offline methods. This success underscores that our CrossLight framework emerges as a groundbreaking and potent paradigm, offering a feasible and effective solution to the intelligent transportation community.|交通信号控制(TSC)的最新进展凸显了强化学习作为缓解交通交通堵塞的一个有前途的解决方案的潜力。目前该领域的研究主要集中在线上或线下学习策略，旨在为特定城市制定优化政策。然而，这些政策向新城市的转移受到诸如高质量数据有限以及昂贵和危险的勘探过程等制约因素的阻碍。为此，在本文中，我们提出了一个创新的跨城市交通信号控制(TSC)范例称为交叉灯。我们的方法包括使用来自源城市的离线数据进行元培训，并在目标城市进行自适应微调。这种新颖的方法旨在解决在不同城市之间有效转移 TSC 政策的挑战。在我们提出的方法中，我们从获取元决策模式的知识开始，通过轨迹动力学重建通过预训练在源城市。针对城市间道路网络拓扑结构的差异，在离线元训练阶段，基于提取的元知识动态构建城市拓扑结构。然后使用这些结构从源城市中提取决策轨迹的模式结构感知表示。为了确定可学习组件的有效初始参数，我们采用了模型不可知元学习(MAML)框架，这是一种流行的元学习方法。在目标城市的自适应微调过程中，我们引入了一个重放缓冲区，该缓冲区使用带有等级和过滤机制的在线交互进行迭代更新。这一机制连同精心设计的勘探战略，确保了开发和勘探之间的平衡，从而促进了微调轨迹的多样性和质量。最后，在四个城市进行的大量实验证实，CrossLight 在新城市中以最少的微调迭代实现了可比的性能，超过了现有的两种在线和离线方法。这一成功突出表明，我们的 CrossLight 框架是一个开创性的、有力的范例，为智能交通社区提供了一个可行的、有效的解决方案。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CrossLight:+Offline-to-Online+Reinforcement+Learning+for+Cross-City+Traffic+Signal+Control)|0|
|[Going Where, by Whom, and at What Time: Next Location Prediction Considering User Preference and Temporal Regularity](https://doi.org/10.1145/3637528.3671916)|Tianao Sun, Ke Fu, Weiming Huang, Kai Zhao, Yongshun Gong, Meng Chen|Robinson College of Business, Georgia State University, Atlanta, GA, USA; School of Software, Shandong University, Jinan, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore|Next location prediction is a crucial task in human mobility modeling, and is pivotal for many downstream applications like location-based recommendation and transportation planning. Although there has been a large body of research tackling this problem, the usefulness of user preference and temporal regularity remains underrepresented. Specifically, previous studies usually neglect the explicit user preference information entailed from human trajectories and fall short in utilizing the arrival time of next location, as a key determinant on next location. To address these limitations, we propose a Multi-Context aware Location Prediction model (MCLP) to predict next locations for individuals, where it explicitly models user preference and the next arrival time as context. First, we utilize a topic model to extract user preferences for different types of locations from historical human trajectories. Second, we develop an arrival time estimator to construct a robust arrival time embedding based on the multi-head attention mechanism. The two components provide pivotal contextual information for the subsequent prediction. Finally, we utilize the Transformer architecture to mine sequential patterns and integrate multiple contextual information to predict the next locations. Experimental results on two real-world mobility datasets show that our proposed MCLP outperforms baseline methods.|下一步的位置预测是人类流动性建模中的一个关键任务，对于基于位置的推荐和交通规划等许多下游应用来说都是至关重要的。虽然已有大量研究处理这一问题，但用户偏好和时间规律的有用性仍然没有得到充分体现。具体而言，以往的研究往往忽略了人类轨迹所带来的明确的用户偏好信息，而没有充分利用下一个位置的到达时间作为下一个位置的关键决定因素。为了解决这些局限性，我们提出了一个多上下文感知位置预测模型(MCLP)来预测个人的下一个位置，它显式地模拟用户偏好和下一个到达时间作为上下文。首先，我们利用主题模型从历史人类轨迹中提取不同类型位置的用户偏好。其次，提出了一种基于多目标注意机制的鲁棒到达时间估计器，构造了一种鲁棒到达时间嵌入算法。这两个组件为后续预测提供关键的上下文信息。最后，我们利用 former 体系结构来挖掘序列模式，并集成多个上下文信息来预测下一个位置。在两个实际移动数据集上的实验结果表明，我们提出的 MCLP 方法的性能优于基线方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Going+Where,+by+Whom,+and+at+What+Time:+Next+Location+Prediction+Considering+User+Preference+and+Temporal+Regularity)|0|
|[EcoVal: An Efficient Data Valuation Framework for Machine Learning](https://doi.org/10.1145/3637528.3672068)|Ayush K. Tarun, Vikram S. Chundawat, Murari Mandal, Hong Ming Tan, Bowei Chen, Mohan S. Kankanhalli|Adam Smith Business School, University of Glasgow, Glasgow, United Kingdom; Ola Krutrim, Bangalore, India; RespAI Lab, Bhubaneswar, India; National University of Singapore, Singapore, Singapore; RespAI Lab, Kalinga Institute of Industrial Technology, Bhubaneswar, India; NUS Business School, National University of Singapore, Singapore, Singapore|Quantifying the value of data within a machine learning workflow can play a pivotal role in making more strategic decisions in machine learning initiatives. The existing Shapley value based frameworks for data valuation in machine learning are computationally expensive as they require considerable amount of repeated training of the model to obtain the Shapley value. In this paper, we introduce an efficient data valuation framework EcoVal, to estimate the value of data for machine learning models in a fast and practical manner. Instead of directly working with individual data sample, we determine the value of a cluster of similar data points. This value is further propagated amongst all the member cluster points. We show that the overall value of the data can be determined by estimating the intrinsic and extrinsic value of each data. This is enabled by formulating the performance of a model as aproduction function, a concept which is popularly used to estimate the amount of output based on factors like labor and capital in a traditional free economic market. We provide a formal proof of our valuation technique and elucidate the principles and mechanisms that enable its accelerated performance. We demonstrate the real-world applicability of our method by showcasing its effectiveness for both in-distribution and out-of-sample data. This work addresses one of the core challenges of efficient data valuation at scale in machine learning models. The code is available at https://github.com/respai-lab/ecoval.|对机器学习工作流中的数据价值进行量化，可以在机器学习活动中作出更具战略性的决策方面发挥关键作用。现有的基于 Shapley 值的机器学习数据估值框架计算量很大，因为它们需要对模型进行大量的重复训练才能获得 Shapley 值。本文介绍了一种高效的数据价值评估框架 EcoVal，用于快速、实用地评估机器学习模型的数据价值。我们不直接处理单个数据样本，而是确定类似数据点集群的值。此值将在所有成员群集点之间进一步传播。我们表明，数据的总体价值可以通过估计每个数据的内在和外在价值来确定。这是通过将模型的性能表述为生产函数来实现的，在传统的自由经济市场中，生产函数这一概念被广泛用于根据劳动力和资本等因素来估计产出量。我们为我们的评估技术提供了一个正式的证明，并阐明了使其加速性能的原则和机制。我们通过展示其对分布内和样本外数据的有效性来证明我们的方法在现实世界中的适用性。这项工作解决了在机器学习模型的规模有效的数据估值的核心挑战之一。密码可在 https://github.com/respai-lab/ecoval 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EcoVal:+An+Efficient+Data+Valuation+Framework+for+Machine+Learning)|0|
|[Causal Estimation of Exposure Shifts with Neural Networks and an Application to Inform Air Quality Standards in the US](https://doi.org/10.1145/3637528.3671761)|Mauricio Tec, Kevin Josey, Oladimeji Mudele, Francesca Dominici|Harvard University, Cambridge, MA, USA; Colorado School of Public Health, Aurora, CO, USA|A fundamental task in causal inference is estimating the effect of a distribution shift in the treatment variable. We refer to this problem as shift-response function (SRF) estimation. Existing neural network methods for causal inference lack theoretical guarantees and practical implementations for SRF estimation. In this paper, we introduce Targeted Regularization for Exposure Shifts with Neural Networks (TRESNET), a method to estimate SRFs with robustness and efficiency guarantees. Our contributions are twofold. First, we propose a targeted regularization loss for neural networks with theoretical properties that ensure double robustness and asymptotic efficiency specific to SRF estimation. Second, we extend targeted regularization to support loss functions from the exponential family to accommodate non-continuous outcome distributions (e.g., discrete counts). We conduct benchmark experiments demonstrating TRESNET's broad applicability and competitiveness. We then apply our method to a key policy question in public health to estimate the causal effect of revising the US National Ambient Air Quality Standards (NAAQS) for PM 2.5 from 12 μg/m3 to 9 μg/m3. This change has been recently proposed by the US Environmental Protection Agency (EPA). Our goal is to estimate the reduction in deaths that would result from this anticipated revision using data consisting of 68 million individuals across the U.S.|因果推理的一个基本任务是估计治疗变量中分布偏移的影响。我们把这个问题称为移位响应函数(SRF)估计。现有的神经网络因果推理方法缺乏 SRF 估计的理论保证和实际应用。本文介绍了基于神经网络的曝光位移目标正则化方法(TRESNET) ，这是一种具有鲁棒性和有效性保证的 SRF 估计方法。我们的贡献是双重的。首先，我们提出了一个目标正则化损失的神经网络的理论性质，确保双鲁棒性和渐近效率特定的 SRF 估计。其次，我们扩展有针对性的正则化来支持来自指数族的损失函数，以适应非连续的结果分布(例如，离散计数)。我们进行的基准实验证明了 TRESNET 的广泛适用性和竞争力。然后，我们将我们的方法应用于公共卫生的一个关键政策问题，以估计将美国国家环境空气质量标准(NAAQS)的 PM2.5从12μg/m3修订为9μg/m3的因果效应。美国环境保护署(EPA)最近提出了这一改变。我们的目标是利用全美6800万个人的数据估计这一预期修正可能导致的死亡人数减少。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causal+Estimation+of+Exposure+Shifts+with+Neural+Networks+and+an+Application+to+Inform+Air+Quality+Standards+in+the+US)|0|
|[Online Drift Detection with Maximum Concept Discrepancy](https://doi.org/10.1145/3637528.3672016)|Ke Wan, Yi Liang, Susik Yoon|University of Illinois at Urbana-Champaign, Urbana, IL, USA; Korea University, Seoul, Republic of Korea; Fudan University, Shanghai, China|Continuous learning from an immense volume of data streams becomes exceptionally critical in the internet era. However, data streams often do not conform to the same distribution over time, leading to a phenomenon called concept drift. Since a fixed static model is unreliable for inferring concept-drifted data streams, establishing an adaptive mechanism for detecting concept drift is crucial. Current methods for concept drift detection primarily assume that the labels or error rates of downstream models are given and/or underlying statistical properties exist in data streams. These approaches, however, struggle to address high-dimensional data streams with intricate irregular distribution shifts, which are more prevalent in real-world scenarios. In this paper, we propose MCD-DD, a novel concept drift detection method based on maximum concept discrepancy, inspired by the maximum mean discrepancy. Our method can adaptively identify varying forms of concept drift by contrastive learning of concept embeddings without relying on labels or statistical properties. With thorough experiments under synthetic and real-world scenarios, we demonstrate that the proposed method outperforms existing baselines in identifying concept drifts and enables qualitative analysis with high explainability.|在互联网时代，从海量数据流中不断学习变得尤为重要。然而，随着时间的推移，数据流往往不符合相同的分布，从而导致一种称为概念漂移的现象。由于固定的静态模型对于推断概念漂移数据流是不可靠的，因此建立一个自适应的概念漂移检测机制是至关重要的。目前的概念漂移检测方法主要假设下游模型的标签或错误率已经给出，并且/或者数据流中存在潜在的统计特性。然而，这些方法很难处理具有复杂的不规则分布变化的高维数据流，这种情况在现实世界中更为普遍。本文受最大均值偏差的启发，提出了一种新的基于最大均值偏差的概念漂移检测方法 MCD-DD。该方法通过概念嵌入的对比学习自适应地识别不同形式的概念漂移，而不依赖于标签或统计特性。通过在合成和真实场景下的全面实验，我们证明了该方法在识别概念漂移方面优于现有的基线，并且能够进行高解释性的定性分析。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+Drift+Detection+with+Maximum+Concept+Discrepancy)|0|
|[CE-RCFR: Robust Counterfactual Regression for Consensus-Enabled Treatment Effect Estimation](https://doi.org/10.1145/3637528.3672054)|Fan Wang, Chaochao Chen, Weiming Liu, Tianhao Fan, Xinting Liao, Yanchao Tan, Lianyong Qi, Xiaolin Zheng|; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer and Data ScienceCollege of Software, Fuzhou University, Fuzhou, China|Estimating individual treatment effects (ITE) from observational data is challenging due to the absence of counterfactuals and the treatment selection bias. Prevalent ITE estimation methods tackle these challenges by aligning the treated and controlled distributions in the representational space. However, two critical issues have long been overlooked: (1)Mini-batch sampling sensitivity (MSS) issue, where representation distribution alignment at a mini-batch level is vulnerable to poor sampling cases, such as data imbalance and outliers; (2)Inconsistent representation learning (IRL) issue, where representation learning within a unified backbone network suffers from inconsistent gradient update directions due to the distribution skew between different treatment groups. To resolve these issues, we propose CE-RCFR, a Robust CounterFactual Regression framework for Consensus-Enabled causal effect estimation, including a relaxed distribution discrepancy regularizer (RDDR) module and a consensus-enabled aggregator (CEA) module. Specifically, for the robust representation alignment perspective, RDDR addresses the MSS issue by minimizing unbalanced optimal transport divergence between different treatment groups with a relaxed marginal constraint. For the accurate representation optimization perspective, CEA addresses the IRL issue by resolving the consistent gradient update directions on shared parameters within the backbone network. Extensive experiments demonstrate that CE-RCFR significantly outperforms the state-of-the-art methods in treatment effect estimations.|由于缺乏反事实因素和治疗选择偏倚，从观察数据估计个体治疗效果(ITE)是具有挑战性的。目前流行的 ITE 估计方法通过在表征空间中对齐处理过的和控制过的分布来应对这些挑战。然而，有两个关键问题长期以来一直被忽视: (1)小批量抽样敏感性(MSS)问题，其中表征分布在小批量水平上的比对容易受到不良抽样情况的影响，如数据不平衡和异常值; (2)不一致表征学习(IRL)问题，其中表征学习在一个统一的骨干网络中由于不同处理组之间的分布倾斜而遭受不一致的梯度更新方向。为了解决这些问题，我们提出了 CE-RCFR，一个用于共识支持的因果效应估计的鲁棒反事实回归框架，包括一个松弛的分布差异正则化(RDDR)模块和一个共识支持的聚合器(CEA)模块。具体而言，对于鲁棒表示比对的观点，RDDR 通过最小化不同治疗组之间不平衡的最佳运输分歧，并放松边际约束来解决 MSS 问题。从精确表示优化的角度出发，CEA 通过解决骨干网内共享参数的一致梯度更新方向来解决 IRL 问题。广泛的实验表明，CE-RCFR 在治疗效果评估方面显著优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CE-RCFR:+Robust+Counterfactual+Regression+for+Consensus-Enabled+Treatment+Effect+Estimation)|0|
|[Learning from Emergence: A Study on Proactively Inhibiting the Monosemantic Neurons of Artificial Neural Networks](https://doi.org/10.1145/3637528.3671776)|Jiachuan Wang, Shimin Di, Lei Chen, Charles Wang Wai Ng|The Hong Kong University of Science and Technology, Hong Kong SAR, China; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; The Hong Kong University of Science and Technology, (Guangzhou), Guangzhou, China|Recently, emergence has received widespread attention from the research community along with the success of large-scale models. Different from the literature, we hypothesize a key factor that promotes the performance during the increase of scale: the reduction of monosemantic neurons that can only form one-to-one correlations with specific features. Monosemantic neurons tend to be sparser and have negative impacts on the performance in large models. Inspired by this insight, we propose an intuitive idea to identify monosemantic neurons and inhibit them. However, achieving this goal is a non-trivial task as there is no unified quantitative evaluation metric and simply banning monosemantic neurons does not promote polysemanticity in neural networks. Therefore, we first propose a new metric to measure the monosemanticity of neurons with the guarantee of efficiency for online computation, then introduce a theoretically supported method to suppress monosemantic neurons and proactively promote the ratios of polysemantic neurons in training neural networks. We validate our conjecture that monosemanticity brings about performance change at different model scales on a variety of neural networks and benchmark datasets in different areas, including language, image, and physics simulation tasks. Further experiments validate our analysis and theory regarding the inhibition of monosemanticity.|近年来，随着大规模模型的成功应用，涌现现象受到了研究界的广泛关注。与文献不同，我们假设一个关键因素，促进性能在规模的增加: 减少单语义神经元，只能形成一对一的相关性与特定的功能。在大型模型中，单义神经元往往比较稀疏，对性能有负面影响。受此启发，我们提出了一个直观的想法来识别和抑制单义神经元。然而，由于没有统一的定量评价指标，单义神经元的禁止并不能提高神经网络的多义性，因此实现这一目标是一项艰巨的任务。因此，我们首先提出了一种新的度量方法来衡量神经元的单语义性，保证了在线计算的有效性，然后介绍了一种理论支持的方法来抑制单语义神经元，并在训练神经网络中主动提高多语义神经元的比率。我们验证了我们的猜想，即单语义在不同的神经网络和不同领域的基准数据集上，包括语言、图像和物理模拟任务，在不同的模型尺度上带来性能变化。进一步的实验验证了我们关于单语义抑制的分析和理论。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+from+Emergence:+A+Study+on+Proactively+Inhibiting+the+Monosemantic+Neurons+of+Artificial+Neural+Networks)|0|
|[POND: Multi-Source Time Series Domain Adaptation with Information-Aware Prompt Tuning](https://doi.org/10.1145/3637528.3671721)|Junxiang Wang, Guangji Bai, Wei Cheng, Zhengzhang Chen, Liang Zhao, Haifeng Chen|Emory University, Atlanta, GA, USA; NEC Labs America, Princeton, NJ, USA|Time series domain adaptation stands as a pivotal and intricate challengewith diverse applications, including but not limited to human activityrecognition, sleep stage classification, and machine fault diagnosis. Despitethe numerous domain adaptation techniques proposed to tackle this complexproblem, they primarily focus on domain adaptation from a single source domain.Yet, it is more crucial to investigate domain adaptation from multiple domainsdue to the potential for greater improvements. To address this, three importantchallenges need to be overcome: 1). The lack of exploration to utilizedomain-specific information for domain adaptation, 2). The difficulty to learndomain-specific information that changes over time, and 3). The difficulty toevaluate learned domain-specific information. In order to tackle thesechallenges simultaneously, in this paper, we introduce PrOmpt-based domaiNDiscrimination (POND), the first framework to utilize prompts for time seriesdomain adaptation. Specifically, to address Challenge 1, we extend the idea ofprompt tuning to time series analysis and learn prompts to capture common anddomain-specific information from all source domains. To handle Challenge 2, weintroduce a conditional module for each source domain to generate prompts fromtime series input data. For Challenge 3, we propose two criteria to select goodprompts, which are used to choose the most suitable source domain for domainadaptation. The efficacy and robustness of our proposed POND model areextensively validated through experiments across 50 scenarios encompassing fourdatasets. Experimental results demonstrate that our proposed POND modeloutperforms all state-of-the-art comparison methods by up to 66% on theF1-score.|时间序列领域适应是一个关键和复杂的挑战与多种应用，包括但不限于人类活动识别，睡眠阶段分类和机器故障诊断。尽管为了解决这个复杂的问题，提出了许多领域自适应技术，但它们主要集中在从单个源领域进行领域自适应。然而，由于存在更大的改进潜力，因此研究来自多个领域的领域适应性更为重要。为了解决这个问题，我们需要克服三个重要的挑战: 1)。缺乏利用特定领域信息进行领域适应的探索，2)。学习随时间变化的特定领域信息的困难，以及3)。评估学习领域特定信息的困难。为了同时解决这些问题，本文介绍了第一个利用提示进行时间序列域适应的框架——基于提示的域识别(POND)。具体来说，为了解决挑战1，我们将提示调优的思想扩展到时间序列分析，并学习提示从所有源域中捕获公共和特定领域的信息。为了处理挑战2，我们为每个源域引入一个条件模块，从时间序列输入数据生成提示。对于挑战3，我们提出了两个选择好提示的标准，用于选择最适合域适应的源域。我们提出的 POND 模型的有效性和鲁棒性通过包含四个数据集的50个场景的实验得到了广泛的验证。实验结果表明，我们提出的 POND 模型优于所有国家的最先进的比较方法，高达66% 的 F1得分。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=POND:+Multi-Source+Time+Series+Domain+Adaptation+with+Information-Aware+Prompt+Tuning)|0|
|[DPSW-Sketch: A Differentially Private Sketch Framework for Frequency Estimation over Sliding Windows](https://doi.org/10.1145/3637528.3671694)|Yiping Wang, Yanhao Wang, Cen Chen|; East China Normal University, Shanghai, China|The sliding window model of computation captures scenarios in which data are continually arriving in the form of a stream, and only the most recent w items are used for analysis. In this setting, an algorithm needs to accurately track some desired statistics over the sliding window using a small space. When data streams contain sensitive information about individuals, the algorithm is also urgently needed to provide a provable guarantee of privacy. In this paper, we focus on the two fundamental problems of privately (1) estimating the frequency of an arbitrary item and (2) identifying the most frequent items (i.e., heavy hitters), in the sliding window model. We propose DPSW-Sketch, a sliding window framework based on the count-min sketch that not only satisfies differential privacy over the stream but also approximates the results for frequency and heavy-hitter queries within bounded errors in sublinear time and space w.r.t. w. Extensive experiments on five real-world and synthetic datasets show that DPSW-Sketch provides significantly better utility-privacy trade-offs than state-of-the-art methods.|计算的滑动窗口模型捕获数据以流的形式不断到达的场景，并且只使用最新的 w 项进行分析。在这种情况下，算法需要使用一个小空间在滑动窗口上精确地跟踪一些所需的统计信息。当数据流包含有关个人的敏感信息时，也迫切需要该算法提供可证明的隐私保证。本文主要研究滑动窗口模型中的两个基本问题: (1)估计任意项目的频率; (2)识别最频繁项目(即重点项目)。我们提出了 DPSW-Sketch，一个基于 count-min 草图的滑动窗口框架，它不仅满足流上的差分隐私，而且在次线性时间和空间的有界错误内，近似于频率和重点查询的结果。在五个真实世界和合成数据集上的大量实验表明，DPSW-Sketch 比最先进的方法提供了明显更好的效用-隐私权衡。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DPSW-Sketch:+A+Differentially+Private+Sketch+Framework+for+Frequency+Estimation+over+Sliding+Windows)|0|
|[DFGNN: Dual-frequency Graph Neural Network for Sign-aware Feedback](https://doi.org/10.1145/3637528.3671701)|Yiqing Wu, Ruobing Xie, Zhao Zhang, Xu Zhang, Fuzhen Zhuang, Leyu Lin, Zhanhui Kang, Yongjun Xu|; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Tencent, Beijing, China; Institute of Artificial Intelligence, Beihang University & Zhongguancun Laboratory, Beijing, China|The graph-based recommendation has achieved great success in recent years.However, most existing graph-based recommendations focus on capturing userpreference based on positive edges/feedback, while ignoring negativeedges/feedback (e.g., dislike, low rating) that widely exist in real-worldrecommender systems. How to utilize negative feedback in graph-basedrecommendations still remains underexplored. In this study, we first conducteda comprehensive experimental analysis and found that (1) existing graph neuralnetworks are not well-suited for modeling negative feedback, which acts as ahigh-frequency signal in a user-item graph. (2) The graph-based recommendationsuffers from the representation degeneration problem. Based on the twoobservations, we propose a novel model that models positive and negativefeedback from a frequency filter perspective called Dual-frequency Graph NeuralNetwork for Sign-aware Recommendation (DFGNN). Specifically, in DFGNN, thedesigned dual-frequency graph filter (DGF) captures both low-frequency andhigh-frequency signals that contain positive and negative feedback.Furthermore, the proposed signed graph regularization is applied to maintainthe user/item embedding uniform in the embedding space to alleviate therepresentation degeneration problem. Additionally, we conduct extensiveexperiments on real-world datasets and demonstrate the effectiveness of theproposed model. Codes of our model will be released upon acceptance.|近年来，基于图形的推荐方法取得了巨大的成功。然而，大多数现有的基于图表的推荐关注于基于正面边缘/反馈来获取用户偏好，而忽略了现实中广泛存在的负面边缘/反馈(例如，不喜欢，低评价)。如何在基于图表的推荐中利用负面反馈仍然没有得到充分的探索。在本研究中，我们首先进行了全面的实验分析，发现(1)现有的图形神经网络并不适合建立负反馈模型，因为负反馈在用户项目图中扮演高频信号的角色。(2)基于图的推荐存在表示退化问题。基于这两个观察结果，我们提出了一种新的模型，从频率滤波器的角度模拟正反馈和负反馈，称为双频图神经网络(DFGNN)的符号感知推荐。特别是在 DFGNN，设计的双频图形滤波器(dGF)同时捕获包含正反馈和负反馈的低频和高频信号。在嵌入空间中采用符号图正则化方法保持用户/项目嵌入的一致性，以减少表示退化问题。此外，我们在真实世界的数据集上进行了广泛的实验，并证明了该模型的有效性。我们的型号代码将在验收后发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DFGNN:+Dual-frequency+Graph+Neural+Network+for+Sign-aware+Feedback)|0|
|[Predicting Cascading Failures with a Hyperparametric Diffusion Model](https://doi.org/10.1145/3637528.3672048)|Bin Xiang, Bogdan Cautis, Xiaokui Xiao, Olga Mula, Dusit Niyato, Laks V. S. Lakshmanan|Nanyang Technological University, Singapore, Singapore; Eindhoven University of Technology, Eindhoven, Netherlands; CNRSCREATE, Singapore, Singapore; University of Paris-Saclay, CNRS LISN, Saclay, France; National University of Singapore, Singapore, Singapore; University of British Columbia, Vancouver, Canada|In this paper, we study cascading failures in power grids through the lens ofinformation diffusion models. Similar to the spread of rumors or influence inan online social network, it has been observed that failures (outages) in apower grid can spread contagiously, driven by viral spread mechanisms. Weemploy a stochastic diffusion model that is Markovian (memoryless) and local(the activation of one node, i.e., transmission line, can only be caused by itsneighbors). Our model integrates viral diffusion principles with physics-basedconcepts, by correlating the diffusion weights (contagion probabilities betweentransmission lines) with the hyperparametric Information Cascades (IC) model.We show that this diffusion model can be learned from traces of cascadingfailures, enabling accurate modeling and prediction of failure propagation.This approach facilitates actionable information through well-understood andefficient graph analysis methods and graph diffusion simulations. Furthermore,by leveraging the hyperparametric model, we can predict diffusion and mitigatethe risks of cascading failures even in unseen grid configurations, whereasexisting methods falter due to a lack of training data. Extensive experimentsbased on a benchmark power grid and simulations therein show that our approacheffectively captures the failure diffusion phenomena and guides decisions tostrengthen the grid, reducing the risk of large-scale cascading failures.Additionally, we characterize our model's sample complexity, improving upon theexisting bound.|本文通过信息扩散模型的透镜，研究了电网中的连锁故障。与在线社交网络中谣言或影响的传播类似，已经观察到电网故障(停电)可以通过病毒传播机制传染。我们采用马尔可夫(无记忆)和局部(一个节点的激活，即传输线，只能由它的邻居引起)的随机扩散模型。我们的模型通过将扩散权重(传输线之间的传染概率)与超参数信息级联(IC)模型相关联，将病毒扩散原理与基于物理的概念结合起来。我们表明，这种扩散模型可以从级联故障的痕迹中学习，从而能够准确建模和预测故障传播。这种方法通过充分理解和有效的图分析方法和图扩散模拟促进了可操作的信息。此外，通过利用超参数模型，我们可以预测扩散和减轻级联故障的风险，即使在看不见的网格配置，而现有的方法由于缺乏训练数据而步履蹒跚。基于基准电网的大量实验和仿真表明，我们的方法有效地捕获了故障扩散现象，并指导决策加强电网，减少大规模连锁故障的风险。此外，我们描述了我们的模型的样本复杂性，改进了现有的界限。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Predicting+Cascading+Failures+with+a+Hyperparametric+Diffusion+Model)|0|
|[FRNet: Frequency-based Rotation Network for Long-term Time Series Forecasting](https://doi.org/10.1145/3637528.3671713)|Xinyu Zhang, Shanshan Feng, Jianghong Ma, Huiwei Lin, Xutao Li, Yunming Ye, Fan Li, Yew Soon Ong|; Centre for Frontier AI Research, ASTAR, Nanyang Technological University, Singapore, Singapore; Hong Kong Polytechnic University, Hong Kong, China; Harbin Institute of Technology, Shenzhen, China|Long-term time series forecasting (LTSF) aims to predict future values for a long time based on historical data. The period term is an essential component of the time series, which is complex yet important for LTSF. Although existing studies have achieved promising results, they still have limitations in modeling dynamic complicated periods. Most studies only focus on static periods with fixed time steps, while very few studies attempt to capture dynamic periods in the time domain. In this paper, we dissect the original time series in time and frequency domains and empirically find that changes in periods are more easily captured and quantified in the frequency domain. Based on this observation, we propose to explore dynamic period features using rotation in the frequency domain. To this end, we develop the frequency-based rotation network (FRNet), a novel LTSF method to effectively capture the features of the dynamic complicated periods. FRNet decomposes the original time series into period and trend components. Based on the complex-valued linear networks, it leverages a period frequency rotation module to predict the period component and a patch frequency rotation module to predict the trend component, respectively. Extensive experiments on seven real-world datasets consistently demonstrate the superiority of FRNet over various state-of-the-art methods. The source code is available at https://github.com/SiriZhang45/FRNet.|长期时间序列预测(LTSF)的目的是根据历史数据预测未来很长一段时间内的价值。周期项是时间序列的一个重要组成部分，对于长期稳定因子来说，时间序列是复杂而重要的。虽然现有的研究已经取得了很好的成果，但是在动态复杂周期的建模方面仍然存在一定的局限性。大多数研究只关注固定时间步长的静态周期，很少有研究试图捕捉时间域中的动态周期。本文从时间和频率两个方面对原始时间序列进行了剖析，发现在频率域更容易捕捉和量化周期的变化。在此基础上，我们提出了在频域中利用旋转来探索动态周期特征的方法。为此，我们发展了基于频率的旋转网络(FRNet) ，一种新的 LTSF 方法来有效地捕捉动态复杂周期的特征。FRNet 将原始时间序列分解为周期分量和趋势分量。在复值线性网络的基础上，利用周期频率旋转模块预测周期分量，利用补丁频率旋转模块预测趋势分量。在七个真实世界数据集上的大量实验一致地证明了 FRNet 相对于各种最先进的方法的优越性。源代码可在 https://github.com/sirizhang45/frnet 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FRNet:+Frequency-based+Rotation+Network+for+Long-term+Time+Series+Forecasting)|0|
|[Hypformer: Exploring Efficient Transformer Fully in Hyperbolic Space](https://doi.org/10.1145/3637528.3672039)|Menglin Yang, Harshit Verma, Delvin Ce Zhang, Jiahong Liu, Irwin King, Rex Ying|Birla Institute of Technology and Science, Hyderabad, India; Yale University, New Haven, CT, USA; The Chinese University of Hong Kong, Hong Kong, China|Hyperbolic geometry have shown significant potential in modeling complex structured data, particularly those with underlying tree-like and hierarchical structures. Despite the impressive performance of various hyperbolic neural networks across numerous domains, research on adapting the Transformer to hyperbolic space remains limited. Previous attempts have mainly focused on modifying self-attention modules in the Transformer. However, these efforts have fallen short of developing a complete hyperbolic Transformer. This stems primarily from: (i) the absence of well-defined modules in hyperbolic space, including linear transformation layers, LayerNorm layers, activation functions, dropout operations, etc. (ii) the quadratic time complexity of the existing hyperbolic self-attention module w.r.t the number of input tokens, which hinders its scalability. To address these challenges, we propose, Hypformer, a novel hyperbolic Transformer based on the Lorentz model of hyperbolic geometry. In Hypformer, we introduce two foundational blocks that define the essential modules of the Transformer in hyperbolic space. Furthermore, we develop a linear self-attention mechanism in hyperbolic space, enabling hyperbolic Transformer to process billion-scale graph data and long-sequence inputs for the first time. Our experimental results confirm the effectiveness and efficiency of \method across various datasets, demonstrating its potential as an effective and scalable solution for large-scale data representation and large models.|双曲几何在建模复杂的结构化数据方面显示出巨大的潜力，特别是那些具有树状结构和层次结构的数据。尽管各种双曲神经网络在许多领域都有着令人印象深刻的性能，但是关于如何使变压器适应双曲空间的研究仍然很有限。以前的尝试主要集中在修改 Transformer 中的自我关注模块。然而，这些努力都没有开发出一个完整的双曲变压器。这主要是由于: (i)双曲空间中缺乏定义明确的模块，包括线性映射层、层规范层、激活函数、辍学操作等。(ii)现有双曲自我注意模块的二次时间复杂性。为了应对这些挑战，我们提出了一种基于洛伦兹双曲几何模型的新型双曲变压器。在 Hypformer，我们介绍了两个基本模块，它们定义了双曲空间变压器的基本模块。此外，我们还在双曲空间中开发了一种线性自我注意机制，使得双曲变压器第一次能够处理数十亿比例的图形数据和长序列输入。我们的实验结果证实了该方法在不同数据集之间的有效性和效率，证明了该方法作为大规模数据表示和大型模型的有效和可扩展的解决方案的潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hypformer:+Exploring+Efficient+Transformer+Fully+in+Hyperbolic+Space)|0|
|[Practical Single Domain Generalization via Training-time and Test-time Learning](https://doi.org/10.1145/3637528.3671806)|Shuai Yang, Zhen Zhang, Lichuan Gu||Single domain generalization aims to learn a model that generalizes well to unseen target domains by using a related source domain. However, most existing methods only focus on improving the generalization performance of the model during training, making it difficult to achieve satisfactory performance when deployed in the target domain with large domain shifts. In this paper, we propose a Practical Single Domain Generalization (PSDG) method, which first leverages the knowledge in a source domain to establish a model with good generalization ability in the training phase, and subsequently updates the model to adapt to target domain data using knowledge in the unlabeled target domain during the testing phase. Specifically, during training, PSDG leverages a newly proposed style (e.g., background features) generator named StyIN to generate novel domain data. Moreover, PSDG introduces style-diversity regularization to constantly synthesize distinct styles to expand the coverage of training data, and introduces object-consistency regularization to capture consistency between the currently generated data and the original data, making the model filter style knowledge during training. During testing, PSDG uses a sample-aware and sharpness-aware minimization method to seek for a flat entropy minimum surface for further model optimization by using the knowledge in the unlabeled target domain. Using three real-world datasets the experiments have demonstrated the effectiveness of PSDG, in comparison with several state-of-the-art methods.|单域泛化的目的是学习一个模型，通过使用一个相关的源域很好地泛化到不可见的目标域。然而，现有的方法大多局限于提高模型在训练过程中的泛化性能，这使得在目标区域部署时，当目标区域移动较大时，很难获得令人满意的性能。本文提出了一种实用的单领域泛化(PSDG)方法，该方法首先利用源领域的知识在训练阶段建立一个具有良好泛化能力的模型，然后在测试阶段利用未标记目标领域的知识对模型进行更新以适应目标领域数据。具体来说，在培训期间，PSDG 利用一个新提出的样式(例如，背景特性)生成器 StyIN 来生成新的域数据。此外，PSDG 引入样式多样性正则化技术，不断综合不同的样式，扩大训练数据的覆盖范围; 引入对象一致性正则化技术，捕捉当前生成的数据与原始数据之间的一致性，使模型滤波样式知识在训练过程中得到充分利用。在测试过程中，PSDG 利用未标记目标域中的知识，采用样本感知和锐度感知的最小化方法寻找平坦熵最小曲面，以进一步优化模型。利用三个真实世界的数据集，实验证明了 PSDG 的有效性，并与几种最先进的方法进行了比较。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Practical+Single+Domain+Generalization+via+Training-time+and+Test-time+Learning)|0|
|[Rethinking Order Dispatching in Online Ride-Hailing Platforms](https://doi.org/10.1145/3637528.3672028)|Zhaoxing Yang, Haiming Jin, Guiyun Fan, Min Lu, Yiran Liu, Xinlang Yue, Hao Pan, Zhe Xu, Guobin Wu, Qun Li, Xiaotong Wang, Jiecheng Guo|Didi Chuxing, Beijing, China; Shanghai Jiao Tong University, Shanghai, China|Achieving optimal order dispatching has been a long-standing challenge for online ride-hailing platforms. Early methods would make shortsighted matchings as they only consider order prices alone as the edge weights in the driver-order bipartite graph, thus harming the platform's revenue. To address this problem, recent works evaluate the value of the order's destination region to be the long-term income a driver could obtain in average in such region and incorporate it into the order's edge weight to influence the matching results. However, they often result in insufficient driver supplies in many regions, as the values evaluated in different regions vary greatly, mainly because the impact of one region's value on the future number of drivers and revenue in other regions is overlooked. This paper models such impact within a cooperative Markov game, which involves each value's impact over the platform's revenue with the goal to find the optimal region values for revenue maximization. To solve this game, our work proposes a novelgoal-reaching collaboration (GRC) algorithm that realizes credit assignment from a novel goal-reaching perspective, addressing the difficulty for accurate credit assignment with large-scale agents of previous methods and resolving the conflict between credit assignment and offline reinforcement learning. Specifically, during training, GRC predicts the city's future state through an environment model and utilizes a scoring model to rate the predicted states to judge their levels of profitability, where high-scoring states are regarded as the goal states. Then, the policies in the game are updated to promote the city to stay in the goal states for as long as possible. To evaluate GRC, we deploy a baseline policy online in several cities for three weeks to collect real-world dataset. Training and testing results on the collected dataset indicate that our GRC consistently outperforms the baselines in different cities and peak periods.|实现最优订单调度一直是在线叫车平台面临的一个长期挑战。早期的方法只考虑订单价格作为驱动订单二部图中的边缘权重，会造成目光短浅的匹配，从而损害平台的收益。为了解决这个问题，最近的工作评估了订单的目的地区域的价值是驱动程序可以获得的长期收入在这些区域的平均值，并将其纳入订单的边缘权重，以影响匹配结果。然而，它们常常导致许多地区的驱动力供应不足，因为在不同地区评估的价值差异很大，主要是因为一个地区的价值对未来驱动力数量和其他地区的收入的影响被忽视。本文在合作马尔可夫博弈中建立了这种影响的模型，其中涉及到每个价值对平台收入的影响，目的是找到收入最大化的最优区域值。为了解决这个问题，我们的工作提出了一个新的目标达成协作(gc)算法，从一个新的目标达成的角度来实现信用分配，解决了以前方法的大规模代理人准确信用分配的困难，并解决了信用分配和离线强化学习之间的冲突。具体来说，在培训期间，GRC 通过一个环境模型预测城市的未来状态，并利用评分模型对预测状态进行评分，以判断其盈利水平，其中得分较高的状态被视为目标状态。然后，游戏中的政策被更新，以促使城市尽可能长时间地停留在目标状态。为了评估 GRC，我们在几个城市中部署了一个为期三周的在线基线策略来收集真实世界的数据集。对所收集数据集的训练和测试结果表明，我们的 GRC 在不同城市和高峰期的表现始终优于基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rethinking+Order+Dispatching+in+Online+Ride-Hailing+Platforms)|0|
|[BoKA: Bayesian Optimization based Knowledge Amalgamation for Multi-unknown-domain Text Classification](https://doi.org/10.1145/3637528.3671963)|Linzhu Yu, Huan Li, Ke Chen, Lidan Shou|The State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China|With breakthroughs in pretrained language models, a large number of finetuned models specialized in distinct domains have surfaced online. Yet, when faced with a fresh dataset covering multiple (sub)domains, their performance might degrade. Reusing these available finetuned models to train a new model is a more feasible solution than the finetuning method that demands extensive manual labeling. Knowledge Amalgamation (KA) is such a model reusing technique, which derives a new model (termed student model) by amalgamating those trained models (termed teacher models) tailored for distinct domains, bypassing the need for manual labeling. However, when the domains of text samples are unknown, selecting a number of appropriate teacher models (simply called a combination) for reuse becomes complicated. To learn an accurate student model, the classical KA method resorts to manual selections, a process both tedious and inefficient. Our study pioneers the automation of this combination selection process for KA in the fundamental text classification task, an area previously unexplored. In this paper, we introduce BoKA : an automatic knowledge amalgamation framework for identifying a combination that can learn a superior student model without human labor. Through the lens of Bayesian optimization, BoKA iteratively samples a subset of possible combinations for amalgamation instead of manual selections. Furthermore, we introduce a novel KA method tailored for text classification, which guides the student model using both soft and pseudo-hard labels from the teacher models when their predictions are closely aligned; in cases of significant disagreement, it uses randomly generated labels. Experiments on two public multi-domain datasets show that BoKA achieves remarkable efficiency by sampling only up to 5.5% of all potential combinations. Moreover, BoKA is capable of matching or even surpassing leading zero-shot large language models, despite having dozens of times fewer parameters.|随着预先训练语言模型的突破，大量在不同领域专门的微调模型已经在网上浮出水面。然而，当面对覆盖多个(子)域的新数据集时，它们的性能可能会下降。与需要大量人工标记的微调方法相比，重用这些已有的微调模型来训练新模型是一种更可行的解决方案。知识合并(KA)就是这样一种模型重用技术，它通过合并那些为不同领域量身定制的训练模型(称为教师模型) ，绕过人工标记的需要，得到一个新的模型(称为学生模型)。然而，当文本示例的领域是未知的时候，为重用选择一些合适的教师模型(简称为组合)就变得复杂了。为了学习一个准确的学生模型，经典的 KA 方法采用手工选择，这是一个既繁琐又低效的过程。我们的研究开创了自动化的组合选择过程的 KA 在基本文本分类任务，一个领域以前未被探索。在本文中，我们介绍了 BoKA: 一个自动知识融合框架，以确定一个组合，可以学习一个优秀的学生模型没有人工劳动。通过贝叶斯优化的透镜，BoKA 迭代抽样一个子集的可能的组合合并，而不是手工选择。此外，我们还引入了一种适合文本分类的新的 KA 方法，当教师模型的预测紧密一致时，它使用软标签和伪硬标签来引导学生模型; 在出现重大分歧的情况下，它使用随机生成的标签。在两个公开的多领域数据集上进行的实验表明，BoKA 算法只对所有潜在组合的5.5% 进行抽样，就取得了显著的效果。此外，BoKA 能够匹配甚至超越领先的大型语言模型，尽管它的参数少了几十倍。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BoKA:+Bayesian+Optimization+based+Knowledge+Amalgamation+for+Multi-unknown-domain+Text+Classification)|0|
|[Diverse Intra- and Inter-Domain Activity Style Fusion for Cross-Person Generalization in Activity Recognition](https://doi.org/10.1145/3637528.3671828)|Junru Zhang, Lang Feng, Zhidan Liu, Yuhan Wu, Yang He, Yabo Dong, Duanqing Xu|Shenzhen University, Shenzhen, China; Zhejiang University, Hangzhou, Zhejiang, China; Zhejiang University, Hangzhou, China|Existing domain generalization (DG) methods for cross-person generalization tasks often face challenges in capturing intra- and inter-domain style diversity, resulting in domain gaps with the target domain. In this study, we explore a novel perspective to tackle this problem, a process conceptualized as domain padding. This proposal aims to enrich the domain diversity by synthesizing intra- and inter-domain style data while maintaining robustness to class labels. We instantiate this concept using a conditional diffusion model and introduce a style-fused sampling strategy to enhance data generation diversity. In contrast to traditional condition-guided sampling, our style-fused sampling strategy allows for the flexible use of one or more random styles to guide data synthesis. This feature presents a notable advancement: it allows for the maximum utilization of possible permutations and combinations among existing styles to generate a broad spectrum of new style instances. Empirical evaluations on a broad range of datasets demonstrate that our generated data achieves remarkable diversity within the domain space. Both intra- and inter-domain generated data have proven to be significant and valuable, contributing to varying degrees of performance enhancements. Notably, our approach outperforms state-of-the-art DG methods in all human activity recognition tasks.|现有的跨人员泛化任务的域泛化方法在捕获域内和域间风格多样性时往往面临挑战，导致与目标域的域差异。在这项研究中，我们探索了一个新的视角来解决这个问题，一个过程概念化为域填充。该方案旨在通过合成域内和域间样式数据来丰富域多样性，同时保持对类别标签的鲁棒性。我们使用一个条件扩散模型来实例化这个概念，并引入一个样式融合抽样策略来增强数据生成的多样性。与传统的条件引导抽样不同，我们的样式融合抽样策略允许灵活地使用一种或多种随机样式来指导数据合成。这个特性提供了一个显著的进步: 它允许最大限度地利用现有样式之间可能的排列和组合，以生成广泛的新样式实例。对大范围数据集的实证评估表明，我们生成的数据在领域空间内实现了显著的多样性。域内和域间生成的数据都被证明是重要和有价值的，有助于不同程度的性能增强。值得注意的是，我们的方法在所有人类活动识别任务中都优于最先进的 DG 方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Diverse+Intra-+and+Inter-Domain+Activity+Style+Fusion+for+Cross-Person+Generalization+in+Activity+Recognition)|0|
|[Knowledge Distillation with Perturbed Loss: From a Vanilla Teacher to a Proxy Teacher](https://doi.org/10.1145/3637528.3671851)|Rongzhi Zhang, Jiaming Shen, Tianqi Liu, Jialu Liu, Michael Bendersky, Marc Najork, Chao Zhang|Georgia Institute of Technology, Atlanta, GA, USA; Google, New York City, NY, USA|Knowledge distillation is a popular technique to transfer knowledge from a large teacher model to a small student model. Typically, the student learns to imitate the teacher by minimizing the KL divergence of its output distribution with the teacher's output distribution. In this work, we argue that such a learning objective is sub-optimal because there exists a discrepancy between the teacher's output distribution and the ground truth label distribution. Therefore, forcing the student to blindly imitate the unreliable teacher output distribution leads to inferior performance. To this end, we propose a novel knowledge distillation objective PTLoss by first representing the vanilla KL-based distillation loss function via a Maclaurin series and then perturbing the leading-order terms in this series. This perturbed loss implicitly transforms the original teacher into a proxy teacher with a distribution closer to the ground truth distribution. We establish the theoretical connection between this "distribution closeness'' and the student model generalizability, which enables us to select the PTLoss's perturbation coefficients in a principled way. Extensive experiments on six public benchmark datasets demonstrate the effectiveness of PTLoss with teachers of different scales.|知识提取是将知识从大型教师模型转化为小型学生模型的一种流行技术。通常情况下，学生学习模仿教师通过最小化其输出分布的 KL 散度与教师的输出分布。在本研究中，我们认为这样的学习目标是次优的，因为在教师的输出分布和地面真理标签分布之间存在差异。因此，迫使学生盲目模仿不可靠的教师产出分布，导致学习成绩不佳。为此，我们提出了一种新的知识蒸馏目标 PT损失，首先通过 Maclaurin 级数表示基于香草 KL 的蒸馏损失函数，然后扰动该级数的先导项。这种不安的损失隐含地将原来的教师转变为代理教师，代理教师的分布更接近于地面真理分布。我们建立了这种“分布接近度”与学生模型概化性之间的理论联系，从而使我们能够以一种原则性的方式选择 PT损失的扰动系数。在六个公共基准数据集上进行的大量实验证明了 PTloss 在不同尺度教师中的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Distillation+with+Perturbed+Loss:+From+a+Vanilla+Teacher+to+a+Proxy+Teacher)|0|
|[Joint Auction in the Online Advertising Market](https://doi.org/10.1145/3637528.3671746)|Zhen Zhang, Weian Li, Yahui Lei, Bingzhe Wang, Zhicheng Zhang, Qi Qi, Qiang Liu, Xingxing Wang|Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; School of Software, Shandong University, Jinan, China; Meituan Inc., Beijing, China|Online advertising is a primary source of income for e-commerce platforms. In the current advertising pattern, the oriented targets are the online store owners who are willing to pay extra fees to enhance the position of their stores. On the other hand, brand suppliers are also desirable to advertise their products in stores to boost brand sales. However, the currently used advertising mode cannot satisfy the demand of both stores and brand suppliers simultaneously. To address this, we innovatively propose a joint advertising model termed ''Joint Auction'', allowing brand suppliers and stores to collaboratively bid for advertising slots, catering to both their needs. However, conventional advertising auction mechanisms are not suitable for this novel scenario. In this paper, we propose JRegNet, a neural network architecture for the optimal joint auction design, to generate mechanisms that can achieve the optimal revenue and guarantee (near-)dominant strategy incentive compatibility and individual rationality. Finally, multiple experiments are conducted on synthetic and real data to demonstrate that our proposed joint auction significantly improves platform's revenue compared to the known baselines.|在线广告是电子商务平台的主要收入来源。在当前的广告模式下，网络商店的定位目标是那些愿意支付额外费用来提高自己店铺地位的网络商店老板。另一方面，品牌供应商也希望在商店里为他们的产品做广告，以促进品牌销售。然而，目前使用的广告模式并不能同时满足商店和品牌供应商的需求。为了解决这个问题，我们创新性地提出了一种名为“联合拍卖”的联合广告模式，允许品牌供应商和商店合作竞标广告位，以满足他们的需求。然而，传统的广告拍卖机制并不适合这种新颖的场景。在这篇文章中，我们提出了一个用于最优联合拍卖设计的神经网络结构—— JregNet，来生成能够实现最优收益和保证(近)主导策略激励相容和个人理性的机制。最后，通过对合成数据和实际数据的多重实验表明，与已知基线相比，本文提出的联合拍卖方案显著提高了平台的收益。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Joint+Auction+in+the+Online+Advertising+Market)|0|
|[Long-Term Vessel Trajectory Imputation with Physics-Guided Diffusion Probabilistic Model](https://doi.org/10.1145/3637528.3672086)|Zhiwen Zhang, Zipei Fan, Zewu Lv, Xuan Song, Ryosuke Shibasaki|; School of Artificial Intelligence, Jilin University, Changchun, China; Research & Development Department, LocationMind Inc., Tokyo, Japan|Maritime traffic management increasingly relies on vessel position information provided by terrestrial and satellite networks of the Automatic Identification System (AIS). Unfortunately, the problem of missing AIS data can lead to long-term gaps in vessel trajectory, raising corresponding security concerns regarding collision risks and illicit activities. Existing imputation approaches are often constrained by vehicle-based low-sampling trajectories, hindering their ability to address unique characteristics of maritime transportation systems and long-term missing scenarios. To tackle these challenges, we propose a novel generative framework for long-term vessel trajectory imputation. Our framework considers irregular tracks of vessels, which differ from those of cars due to the absence of a structured road network, and ensures the continuity of multi-point imputed trajectories. Specifically, we first utilize a pre-trained trajectory embedding block to capture patterns of vessel movements. Subsequently, we introduce a diffusion-based model for generating missing trajectories, where observed trajectory modeling with transformer encoding architecture and embeddings of both historical vessel trajectory and external factors serve as conditional information. In particular, we design a physics-guided discriminator in the training stage, which imposes kinematic constraints between locations and angles to improve the continuity of the imputed trajectories. Comprehensive experiments and analysis on a real-world AIS dataset confirm the effectiveness of our proposed approach.|海上交通管理越来越依赖自动识别系统地面和卫星网络提供的船只位置信息。遗憾的是，缺少自动识别系统数据的问题可能导致船舶航线的长期空白，从而引起对碰撞风险和非法活动的相应安全关切。现有的估算方法往往受到基于车辆的低采样轨迹的限制，妨碍了它们处理海上运输系统独特特征和长期缺失情景的能力。为了应对这些挑战，我们提出了一个新的长期船舶轨迹插补生成框架。我们的框架考虑了由于缺乏结构化道路网而不同于汽车的不规则船舶轨迹，并确保了多点估算轨迹的连续性。具体来说，我们首先利用一个预先训练的轨迹嵌入块来捕捉血管运动的模式。随后，我们介绍了一个基于扩散的缺失轨迹生成模型，其中观测轨迹建模与变压器编码结构和嵌入的历史船舶轨迹和外部因素作为条件信息。特别地，我们在训练阶段设计了一个物理导引的鉴别器，它在位置和角度之间施加运动学约束，以改善估计轨迹的连续性。通过对一个实际 AIS 数据集的全面实验和分析，证实了该方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Long-Term+Vessel+Trajectory+Imputation+with+Physics-Guided+Diffusion+Probabilistic+Model)|0|
|[All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining](https://doi.org/10.1145/3637528.3671913)|Haihong Zhao, Aochuan Chen, Xiangguo Sun, Hong Cheng, Jia Li||Large Language Models (LLMs) have revolutionized the fields of computer vision (CV) and natural language processing (NLP). One of the most notable advancements of LLMs is that a single model is trained on vast and diverse datasets spanning multiple domains -- a paradigm we term `All in One'. This methodology empowers LLMs with super generalization capabilities, facilitating an encompassing comprehension of varied data distributions. Leveraging these capabilities, a single LLM demonstrates remarkable versatility across a variety of domains -- a paradigm we term `One for All'. However, applying this idea to the graph field remains a formidable challenge, with cross-domain pretraining often resulting in negative transfer. This issue is particularly important in few-shot learning scenarios, where the paucity of training data necessitates the incorporation of external knowledge sources. In response to this challenge, we propose a novel approach called Graph COordinators for PrEtraining (GCOPE), that harnesses the underlying commonalities across diverse graph datasets to enhance few-shot learning. Our novel methodology involves a unification framework that amalgamates disparate graph datasets during the pretraining phase to distill and transfer meaningful knowledge to target tasks. Extensive experiments across multiple graph datasets demonstrate the superior efficacy of our approach. By successfully leveraging the synergistic potential of multiple graph datasets for pretraining, our work stands as a pioneering contribution to the realm of graph foundational model.|大语言模型(LLM)彻底改变了计算机视觉(CV)和自然语言处理(NLP)领域。LLM 最显著的进步之一是，单个模型是在跨越多个领域的大量多样化数据集上进行训练的——我们称之为“一体化”模式。这种方法赋予 LLM 超级泛化能力，促进对各种数据分布的全面理解。利用这些能力，单个 LLM 在多个领域展示了非凡的多功能性——我们称之为“一个为所有人”的范例。然而，将这种思想应用到图论领域仍然是一个巨大的挑战，跨域预训练常常导致负迁移。这个问题在短期学习情况下尤其重要，因为培训数据不足，必须纳入外部知识来源。为了应对这一挑战，我们提出了一种新的方法，称为图形协调器的预训练(GCOPE) ，利用不同的图形数据集的基本共性，以增强少镜头学习。我们的新方法涉及一个统一的框架，在预训练阶段合并不同的图形数据集，以提取和转移有意义的知识到目标任务。跨多个图形数据集的广泛实验证明了我们的方法的优越功效。通过成功地利用多个图形数据集的协同潜力进行预训练，我们的工作成为对图形基础模型领域的开拓性贡献。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=All+in+One+and+One+for+All:+A+Simple+yet+Effective+Method+towards+Cross-domain+Graph+Pretraining)|0|
|[Multi-source Unsupervised Domain Adaptation on Graphs with Transferability Modeling](https://doi.org/10.1145/3637528.3671829)|Tianxiang Zhao, Dongsheng Luo, Xiang Zhang, Suhang Wang|Florida International University, Miami, USA; The Pennsylvania State University, State College, USA|In this paper, we tackle a new problem ofmulti-source unsupervised domain adaptation (MSUDA) for graphs, where models trained on annotated source domains need to be transferred to the unsupervised target graph for node classification. Due to the discrepancy in distribution across domains, the key challenge is how to select good source instances and how to adapt the model. Diverse graph structures further complicate this problem, rendering previous MSUDA approaches less effective. In this work, we present the framework Selective Multi-source Adaptation for Graph (SelMAG ), with a graph-modeling-based domain selector, a sub-graph node selector, and a bi-level alignment objective for the adaptation. Concretely, to facilitate the identification of informative source data, the similarity across graphs is disentangled and measured with the transferability of a graph-modeling task set, and we use it as evidence for source domain selection. A node selector is further incorporated to capture the variation in transferability of nodes within the same source domain. To learn invariant features for adaptation, we align the target domain to selected source data both at the embedding space by minimizing the optimal transport distance and at the classification level by distilling the label function. Modules are explicitly learned to select informative source data and conduct the alignment in virtual training splits with a meta-learning strategy. Experimental results on five graph datasets show the effectiveness of the proposed method.|针对图的多源无监督域自适应(MSUDA)问题，在节点分类时，需要将经过注释源域训练的模型转移到无监督目标图上。由于不同领域之间的分布差异，关键的挑战是如何选择好的源实例以及如何适应模型。不同的图形结构使这个问题更加复杂，使得以前的 MSUDA 方法效率更低。本文提出了一种基于图模型的多源图选择自适应(SelMAG)框架，该框架包括一个基于图模型的域选择器、一个子图节点选择器和一个用于自适应的双层对齐目标。具体来说，为了方便信息源数据的识别，利用图建模任务集的可转移性对图间的相似性进行了解密和度量，并将其作为源域选择的依据。进一步合并节点选择器以捕获同一源域内节点可转移性的变化。为了学习自适应的不变特征，我们通过最小化最优传输距离在嵌入空间和通过提取标签函数在分类级别将目标域与选定的源数据对齐。模块被明确地学习来选择信息源数据，并使用元学习策略在虚拟训练中进行对齐。在五个图形数据集上的实验结果表明了该方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-source+Unsupervised+Domain+Adaptation+on+Graphs+with+Transferability+Modeling)|0|
|[Bridging and Compressing Feature and Semantic Spaces for Robust Graph Neural Networks: An Information Theory Perspective](https://doi.org/10.1145/3637528.3671870)|Luying Zhong, Renjie Lin, Jiayin Li, Shiping Wang, Zheyi Chen|Fujian Normal University, Fuzhou, China; Fuzhou University, Fuzhou, China|The emerging Graph Convolutional Networks (GCNs) have attracted widespread attention in graph learning, due to their good ability of aggregating the information between higher-order neighbors. However, real-world graph data contains high noise and redundancy, making it hard for GCNs to accurately depict the complete relationships between nodes, which seriously degrades the quality of graph representations. Moreover, existing studies commonly ignore the distribution difference between feature and semantic spaces in graphs, causing inferior model generalization. To address these challenges, we propose DIB-RGCN, a novel robust GCN framework, to explore the optimal graph representation with the guidance of the well-designed dual information bottleneck principle. First, we analyze the reasons for distribution differences and theoretically prove that minimal sufficient representations in specific spaces cannot promise optimal performance for downstream tasks. Next, we design new dual channels to regularize feature and semantic spaces, eliminating the sharing of task-irrelevant information between spaces. Different from existing denoising algorithms that adopt a random dropping manner, we innovatively replace potential noisy features and edges with local neighboring representations. This design lowers edge-specific coefficient assignment, alleviating the interference of original representations while retaining graph structures. Further, we maximize the sharing of task-relevant information between feature and semantic spaces to alleviate the difference between them. Using real-world datasets, extensive experiments demonstrate the robustness of the proposed DIB-RGCN, which outperforms state-of-the-art methods on classification tasks.|新兴的图卷积网络(GCNs)以其良好的高阶邻域信息聚合能力，在图学习中引起了广泛的关注。然而，真实世界的图形数据具有高噪声和冗余性，使得 GCNs 难以准确描述节点之间的完整关系，严重影响了图表示的质量。此外，现有的研究普遍忽视了图中特征空间和语义空间的分布差异，导致模型泛化效果不佳。为了应对这些挑战，我们提出了一种新的鲁棒 GCN 框架 DIB-RGCN，以设计良好的双信息瓶颈原则为指导，探索最优图表示。首先，我们分析了分布差异的原因，并从理论上证明了特定空间中的最小充分表示不能保证下游任务的最优性能。接下来，我们设计了新的双通道来规范特征和语义空间，消除了空间之间任务无关信息的共享。与现有的采用随机降噪方式的去噪算法不同，我们创新地用局部邻域表示来代替潜在的噪声特征和边缘。这种设计降低了边特定的系数分配，减少了原始表示的干扰，同时保留了图的结构。进一步，我们最大化特征空间和语义空间之间任务相关信息的共享，以减轻它们之间的差异。利用真实世界的数据集，大量的实验证明了所提出的 DIB-RGCN 算法的鲁棒性，它在分类任务中的性能优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bridging+and+Compressing+Feature+and+Semantic+Spaces+for+Robust+Graph+Neural+Networks:+An+Information+Theory+Perspective)|0|
|[Dynamic Hotel Pricing at Online Travel Platforms: A Popularity and Competitiveness Aware Demand Learning Approach](https://doi.org/10.1145/3637528.3671921)|Fanwei Zhu, Wendong Xiao, Yao Yu, Zemin Liu, Zulong Chen, Weibin Cai|Alibaba Group, Hangzhou, China; Syracuse University, Syracuse, USA; Hangzhou City University, Hangzhou, China; Zhejiang University, Hangzhou, China|Dynamic pricing, which suggests the optimal prices based on the dynamic demands, has received considerable attention in academia and industry. On online hotel booking platforms, room demand fluctuates due to various factors, notably hotel popularity and competition. In this paper, we propose a dynamic pricing approach with popularity and competitiveness-aware demand learning. Specifically, we introduce a novel demand function that incorporates popularity and competitiveness coefficients to comprehensively model the price elasticity of demand. We develop a dynamic demand prediction network that focuses on learning these coefficients in the proposed demand function, enhancing the interpretability and accuracy of price suggestion. The model is trained in a multi-task framework that effectively leverages the correlations of demands among groups of similar hotels to alleviate data sparseness in room-level occupancy prediction. Comprehensive experiments conducted on real-world datasets validate the superiority of our method over state-of-the-art baselines in both demand prediction and dynamic pricing. Our model has been successfully deployed on a popular online travel platform, serving tens of millions of users and hoteliers.|动态定价是指基于动态需求的最优价格，已经受到学术界和工业界的广泛关注。在线酒店预订平台上，客房需求的波动受到多种因素的影响，特别是酒店的受欢迎程度和竞争程度。本文提出了一种基于知名度和竞争力的需求学习的动态定价方法。具体来说，我们引入了一个新的需求函数，将受欢迎程度和竞争力系数结合起来，以全面建立需求的价格弹性模型。我们开发了一个动态的需求预测网络，重点是在建议的需求函数中学习这些系数，提高价格建议的可解释性和准确性。该模型在多任务框架下进行训练，有效地利用了类似酒店群体之间需求的相关性，以缓解客房入住率预测中的数据稀缺性。在实际数据集上进行的综合实验验证了该方法在需求预测和动态定价方面优于最新的基线方法。我们的模式已经成功地部署在一个受欢迎的在线旅游平台上，为数千万用户和酒店管理者服务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamic+Hotel+Pricing+at+Online+Travel+Platforms:+A+Popularity+and+Competitiveness+Aware+Demand+Learning+Approach)|0|
|[Repeat-Aware Neighbor Sampling for Dynamic Graph Learning](https://doi.org/10.1145/3637528.3672001)|Tao Zou, Yuhao Mao, Junchen Ye, Bowen Du|; CCSE Lab, Beihang University, Beijing, China; School of Transportation Science and Engineering, Beihang University, Beijing, China|Dynamic graph learning equips the edges with time attributes and allowsmultiple links between two nodes, which is a crucial technology forunderstanding evolving data scenarios like traffic prediction andrecommendation systems. Existing works obtain the evolving patterns mainlydepending on the most recent neighbor sequences. However, we argue that whethertwo nodes will have interaction with each other in the future is highlycorrelated with the same interaction that happened in the past. Onlyconsidering the recent neighbors overlooks the phenomenon of repeat behaviorand fails to accurately capture the temporal evolution of interactions. To fillthis gap, this paper presents RepeatMixer, which considers evolving patterns offirst and high-order repeat behavior in the neighbor sampling strategy andtemporal information learning. Firstly, we define the first-order repeat-awarenodes of the source node as the destination nodes that have interactedhistorically and extend this concept to high orders as nodes in the destinationnode's high-order neighbors. Then, we extract neighbors of the source node thatinteracted before the appearance of repeat-aware nodes with a slide windowstrategy as its neighbor sequence. Next, we leverage both the first andhigh-order neighbor sequences of source and destination nodes to learn temporalpatterns of interactions via an MLP-based encoder. Furthermore, considering thevarying temporal patterns on different orders, we introduce a time-awareaggregation mechanism that adaptively aggregates the temporal representationsfrom different orders based on the significance of their interaction timesequences. Experimental results demonstrate the superiority of RepeatMixer overstate-of-the-art models in link prediction tasks, underscoring theeffectiveness of the proposed repeat-aware neighbor sampling strategy.|动态图学习使边具有时间属性，允许两个节点之间存在多个链接，这是理解流量预测和推荐系统等不断发展的数据场景的关键技术。现有的工作主要依靠最新的邻居序列获得演化模式。然而，我们认为两个节点在未来是否会有相互作用与过去发生的相同的相互作用高度相关。只考虑最近的邻居忽略了重复行为的现象，无法准确地捕捉相互作用的时间演化。为了填补这一空白，本文提出了一种在邻居采样策略和时间信息学习中首先考虑进化模式和高阶重复行为的迭代混合器。首先，我们将源节点的一阶重复感知节点定义为历史上相互作用过的目的节点，并将这一概念扩展为目的节点高阶邻居中的高阶节点。然后，我们以滑动窗口策略作为邻居序列，提取在重复感知节点出现之前相互作用的源节点的邻居。接下来，我们利用源节点和目标节点的第一个和高阶相邻序列，通过基于 MLP 的编码器学习交互的时间模式。在此基础上，针对不同时间序列的时间模式变化，提出了一种时间感知聚合机制，该机制根据不同时间序列的交互重要性，自适应地聚合不同时间序列的时间表示。实验结果表明，本文提出的重复感知邻居采样策略在链路预测任务中具有优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Repeat-Aware+Neighbor+Sampling+for+Dynamic+Graph+Learning)|0|
|[Machine Learning for Clinical Management: From the Lab to the Hospital](https://doi.org/10.1145/3637528.3672497)|Ricard Gavaldà|Amalfi Analytics & Universitat Politècnica de Catalunya, BarcelonaTech (on leave), Barcelona, Spain|Population aging, increasing social demands, and rising costs of treatments are stressing healthcare systems to the point of risking the sustainability of universal and accessible healthcare. A hope in this dismal panorama is that there are large inefficiencies, and so opportunities for getting more from the same resources. To name a few, avoidable hospitalizations, unnecessary medication and tests, and lack of coordination among healthcare agents are estimated to cost several hundred billion euros per year in the EU. Technology can be useful for locating and reducing these inefficiencies, and within technology, the full exploitation of the data that the system collects to record its activity. In this talk, I will review the case for activity data analytics in healthcare, with two main considerations: 1) The need to include information about resources and costs in the models, in addition to clinical knowledge and patient outcomes, and 2) the need to use mostly data that healthcare organizations already collect and is not locked and distributed in silos. Fortunately, data collected for administrative and billing purposes, even though imperfect, partial, and low resolution, can be used to improve efficiency and safety, as well as fairness and equity. I will focus on the work carried out at Amalfi Analytics, a spin-off of my research group at UPC in Barcelona. On the one hand, we have addressed predictive management in hospitals, from influx to the emergency room to availability of surgical areas, beds, and staff. Anticipating activity, needs, and resource availability lets managers improve critical KPIs, e.g. waiting times, but also reduce staff stress, which leads to fewer medical errors and accidents. On the other hand, we have developed a patient cohort analyzer, based mostly on a recent clustering algorithm, that gives experts a fresh view of their patient population and lets them refine protocols and identify high-risk patient groups. This tool has also been used to support territorial planning and resource allocation. These problems have been extensively addressed in the past, but actual penetration of solutions in hospitals is smaller than one could expect. For example, one can find hundreds of papers on predicting influx to emergency rooms or bed demands, but many of them conclude after producing an AUC figure, and even fewer describe a working system that can be exported from the hospital where they were developed to others at an affordable cost. I will describe the approach taken at Amalfi so that hospitals can have such a solution up and running in a few days of work for their IT departments, in what I think is an interesting combination of software engineering and automatic Machine Learning.|人口老龄化、社会需求增加以及治疗成本上升，正在给医疗保健系统带来压力，以至于可能危及全民医疗保健的可持续性。在这个令人沮丧的全景中，一个希望是存在大量的低效率，因此有机会从同样的资源中获得更多。举几个例子，可避免的住院治疗、不必要的药物治疗和检测，以及医疗机构之间缺乏协调，估计每年在欧盟花费数千亿欧元。技术可以有助于查明和减少这些效率低下的情况，并在技术范围内充分利用系统为记录其活动而收集的数据。在这次演讲中，我将回顾医疗保健中活动数据分析的案例，主要考虑两点: 1)除了临床知识和患者结果之外，还需要在模型中包含有关资源和成本的信息; 2)需要使用医疗保健组织已经收集并且没有被锁定和分发的大部分数据。幸运的是，为管理和计费目的收集的数据，即使不完美、不完整和分辨率低，也可以用来提高效率和安全，以及公平和公正。我将专注于在阿马尔菲分析公司进行的工作，这是我在巴塞罗那的 UPC 研究小组的分支。一方面，我们已经解决了医院的预测性管理问题，从流入急诊室到手术区域、病床和工作人员的可用性。预测活动、需求和资源可用性可以让管理者改善关键的关键绩效指标，例如等待时间，但也可以减少员工压力，从而减少医疗差错和事故的发生。另一方面，我们已经开发了一个患者队列分析器，主要基于最近的聚类算法，使专家对他们的患者人口有一个新的看法，并让他们完善方案和确定高风险患者组。这个工具也被用来支持领土规划和资源分配。这些问题在过去已经得到了广泛的解决，但是解决方案在医院的实际渗透比人们预期的要小。例如，可以找到数百篇关于预测急诊室或床位需求的论文，但是其中许多都是在产生 AUC 数据之后得出的结论，更少的论文描述了可以从医院以负担得起的成本开发给其他人的工作系统。我将描述在阿马尔菲采取的方法，这样医院可以有这样一个解决方案，并在几天的工作为他们的信息技术部门运行，我认为这是一个有趣的组合软件工程和自动机器学习。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Machine+Learning+for+Clinical+Management:+From+the+Lab+to+the+Hospital)|0|
|[Metric Decomposition in A/B Tests](https://doi.org/10.1145/3637528.3671556)|Alex Deng, Luke Hagar, Nathaniel T. Stevens, Tatiana Xifara, Amit Gandhi|University of Pennsylvania, Philadelphia, PA, USA; University of Waterloo, Waterloo, ON, Canada; Airbnb, San Francisco, CA, USA; Airbnb, Seattle, WA, USA|More than a decade ago, CUPED (Controlled Experiments Utilizing Pre-Experiment Data) mainstreamed the idea of variance reduction leveraging pre-experiment covariates. Since its introduction, it has been implemented, extended, and modernized by major online experimentation platforms. Despite the wide adoption, it is known by practitioners that the variance reduction rate from CUPED utilizing pre-experimental data varies case by case and has a theoretical limit. In theory, CUPED can be extended to augment a treatment effect estimator utilizing in-experiment data, but practical guidance on how to construct such an augmentation is lacking. In this article, we fill this gap by proposing a new direction for sensitivity improvement via treatment effect augmentation whereby a target metric of interest is decomposed into components with high signal-to-noise disparity. Inference in the context of this decomposition is developed using both frequentist and Bayesian theory. We provide three real world applications demonstrating different flavors of metric decomposition; these applications illustrate the gain in agility metric decomposition yields relative to an un-decomposed analysis.|十多年前，CUPED (利用实验前数据的对照实验)将利用实验前协变量减少方差的想法主流化。自引入以来，它已经被主要的在线实验平台实现、扩展和现代化。尽管被广泛采用，但是从业人员都知道，利用实验前数据的 CUPED 方差减少率因情况而异，并且有一个理论上的限制。理论上，CUPED 可以扩展到利用实验数据来增强治疗效果估计器，但是对于如何构造这样的增强器缺乏实际指导。在本文中，我们填补了这一空白，提出了一个新的方向，通过治疗效果增强灵敏度改进，其中目标度量的兴趣是分解成具有高信噪比的组件。在这种分解的背景下的推理是发展使用频率论和贝叶斯理论。我们提供了三个现实世界中的应用程序，它们演示了不同风格的度量分解; 这些应用程序说明了相对于未分解的分析，敏捷度量分解产量的增长。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Metric+Decomposition+in+A/B+Tests)|0|
|[LASCA: A Large-Scale Stable Customer Segmentation Approach to Credit Risk Assessment](https://doi.org/10.1145/3637528.3671550)|Yongfeng Gu, Yupeng Wu, Huakang Lu, Xingyu Lu, Hong Qian, Jun Zhou, Aimin Zhou|School of Computer Science and Technology, East China Normal University, Shanghai, China; Ant Group, Hangzhou, Zhejiang, China|Customer segmentation plays a crucial role in credit risk assessment by dividing users into specific risk levels based on their credit scores. Previous methods fail to comprehensively consider the stability in the segmentation process, resulting in frequent changes and inconsistencies in users' risk levels over time. This increases potential risks to a company. To this end, this paper at first introduces and formalizes the concept of stability regret in the segmentation process. However, evaluating stability is challenging due to its black-box nature and the computational burden posed by vast user data sets. To address these challenges, this paper proposes a large-scale stable customer segmentation approach named LASCA. LASCA consists of two phases: high-quality dataset construction (HDC) and reliable data-driven optimization (RDO). Specifically, HDC utilizes an evolutionary algorithm to collect high-quality binning solutions. RDO subsequently builds a reliable surrogate model to search for the most stable binning solution based on the collected dataset. Extensive experiments conducted on real-world large-scale datasets (up to 0.8 billion) show that LASCA surpasses the state-of-the-art binning methods in finding the most stable binning solution. Notably, HDC greatly enhances data quality by 50%. RDO efficiently discovers more stable binning solutions with a 36% improvement in stability, accelerating the optimization process by 25 times via data-driven evaluation. Currently, LASCA has been successfully deployed in the large-scale credit risk assessment system of Alipay.|客户细分在信用风险评估中起着至关重要的作用，它根据用户的信用评分将用户划分为特定的风险级别。以往的方法未能全面考虑分割过程的稳定性，导致用户的风险水平随着时间的推移频繁变化和不一致。这增加了公司的潜在风险。为此，本文首先在分割过程中引入并形式化了稳定性遗憾的概念。然而，评估稳定性是具有挑战性的，因为它的黑盒性质和计算负担所造成的海量用户数据集。针对这些挑战，本文提出了一种大规模稳定的客户细分方法 LASCA。LASCA 包括两个阶段: 高质量数据集构建(HDC)和可靠的数据驱动优化(RDO)。具体来说，HDC 使用进化算法来收集高质量的分组解决方案。RDO 随后构建一个可靠的代理模型，以根据收集的数据集搜索最稳定的装箱解决方案。在现实世界的大规模数据集(多达8亿个)上进行的大量实验表明，LASCA 在寻找最稳定的装箱解决方案方面超过了最先进的装箱方法。值得注意的是，HDC 极大地提高了50% 的数据质量。RDO 有效地发现了更稳定的装箱解决方案，其稳定性提高了36% ，通过数据驱动的评估将优化过程加速了25倍。目前，LASCA 已成功应用于支付宝的大规模信用风险评估系统。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LASCA:+A+Large-Scale+Stable+Customer+Segmentation+Approach+to+Credit+Risk+Assessment)|0|
|[Generative Auto-bidding via Conditional Diffusion Modeling](https://doi.org/10.1145/3637528.3671526)|Jiayan Guo, Yusen Huo, Zhilin Zhang, Tianyu Wang, Chuan Yu, Jian Xu, Bo Zheng, Yan Zhang|Alibaba Group, Beijing, China; Peking University, Beijing, China; Peking University & Alibaba Group, Beijing, China|Auto-bidding plays a crucial role in facilitating online advertising by automatically providing bids for advertisers. Reinforcement learning (RL) has gained popularity for auto-bidding. However, most current RL auto-bidding methods are modeled through the Markovian Decision Process (MDP), which assumes the Markovian state transition. This assumption restricts the ability to perform in long horizon scenarios and makes the model unstable when dealing with highly random online advertising environments. To tackle this issue, this paper introduces AI-Generated Bidding (AIGB), a novel paradigm for auto-bidding through generative modeling. In this paradigm, we propose DiffBid, a conditional diffusion modeling approach for bid generation. DiffBid directly models the correlation between the return and the entire trajectory, effectively avoiding error propagation across time steps in long horizons. Additionally, DiffBid offers a versatile approach for generating trajectories that maximize given targets while adhering to specific constraints. Extensive experiments conducted on the real-world dataset and online A/B test on Alibaba advertising platform demonstrate the effectiveness of DiffBid, achieving 2.81% increase in GMV and 3.36% increase in ROI.|自动竞价通过自动为广告商提供出价，在促进网络广告方面发挥着至关重要的作用。强化学习(RL)在自动竞投中越来越受欢迎。然而，大多数现有的 RL 自动竞价方法是通过马尔科夫决策过程(mDP)建模的，该过程假设 Markovian 政府的过渡。这种假设限制了在长期场景中的执行能力，并使模型在处理高度随机的在线广告环境时变得不稳定。为了解决这一问题，本文提出了一种基于生成模型的自动招标方法——人工智能生成招标(AIGB)。在这个范例中，我们提出了一种条件扩散建模的投标生成方法——迪夫出价。迪夫出价直接建模收益率和整个轨迹之间的相关性，有效地避免了错误传播跨时间步长的长期。此外，迪夫出价提供了一个多功能的方法来生成轨迹，最大限度地给定的目标，同时坚持特定的约束。在阿里巴巴广告平台上对现实世界的数据集和在线 A/B 测试进行了广泛的实验，结果证明了 DiffBid 的有效性，实现了2.81% 的 GMV 增长和3.36% 的投资回报率增长。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+Auto-bidding+via+Conditional+Diffusion+Modeling)|0|
|[Learning Metrics that Maximise Power for Accelerated A/B-Tests](https://doi.org/10.1145/3637528.3671512)|Olivier Jeunen, Aleksei Ustimenko|ShareChat, London, United Kingdom; ShareChat, Edinburgh, United Kingdom|Online controlled experiments are a crucial tool to allow for confident decision-making in technology companies. A North Star metric is defined (such as long-term revenue or user retention), and system variants that statistically significantly improve on this metric in an A/B-test can be considered superior. North Star metrics are typically delayed and insensitive. As a result, the cost of experimentation is high: experiments need to run for a long time, and even then, type-II errors (i.e. false negatives) are prevalent. We propose to tackle this by learning metrics from short-term signals that directly maximise the statistical power they harness with respect to the North Star. We show that existing approaches are prone to overfitting, in that higher average metric sensitivity does not imply improved type-II errors, and propose to instead minimise the p-values a metric would have produced on a log of past experiments. We collect such datasets from two social media applications with over 160 million Monthly Active Users each, totalling over 153 A/B-pairs. Empirical results show that we are able to increase statistical power by up to 78% when using our learnt metrics stand-alone, and by up to 210% when used in tandem with the North Star. Alternatively, we can obtain constant statistical power at a sample size that is down to 12% of what the North Star requires, significantly reducing the cost of experimentation.|在线控制实验是科技公司做出自信决策的重要工具。定义了一个 North Star 指标(如长期收入或用户保留) ，在 A/B 测试中，在这个指标上有统计学显著改善的系统变体可以被认为是优越的。北极星指标通常是延迟和不敏感的。因此，实验的成本很高: 实验需要运行很长时间，即使这样，II 型错误(即假阴性)也很普遍。我们建议通过从短期信号中学习指标来解决这个问题，这些信号可以直接最大限度地利用它们对北极星的统计能力。我们表明，现有的方法容易过度拟合，因为较高的平均度量灵敏度并不意味着改善的 II 型误差，并建议相反，尽量减少在过去的实验日志中产生的度量的 p 值。我们从两个社会媒体应用程序中收集这样的数据集，每个应用程序的每月活跃用户超过1.6亿，总共超过153个 A/B 对。实证结果表明，我们能够增加高达78% 的统计权力时，使用我们的学习指标独立，并高达210% 时，与北极星使用的配合。或者，我们可以在样本大小下降到北极星所需要的12% 时获得恒定的统计功率，大大降低了实验的成本。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Metrics+that+Maximise+Power+for+Accelerated+A/B-Tests)|0|
|[Interpretable Cascading Mixture-of-Experts for Urban Traffic Congestion Prediction](https://doi.org/10.1145/3637528.3671507)|Wenzhao Jiang, Jindong Han, Hao Liu, Tao Tao, Naiqiang Tan, Hui Xiong|; Didichuxing Co. Ltd, Beijing, China; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, Guangdong, China; The Hong Kong University of Science and Technology, Hong Kong, China|Rapid urbanization has significantly escalated traffic congestion,underscoring the need for advanced congestion prediction services to bolsterintelligent transportation systems. As one of the world's largest ride-hailingplatforms, DiDi places great emphasis on the accuracy of congestion predictionto enhance the effectiveness and reliability of their real-time services, suchas travel time estimation and route planning. Despite numerous efforts havebeen made on congestion prediction, most of them fall short in handlingheterogeneous and dynamic spatio-temporal dependencies (e.g., periodic andnon-periodic congestions), particularly in the presence of noisy and incompletetraffic data. In this paper, we introduce a Congestion PredictionMixture-of-Experts, CP-MoE, to address the above challenges. We first propose asparsely-gated Mixture of Adaptive Graph Learners (MAGLs) with congestion-awareinductive biases to improve the model capacity for efficiently capturingcomplex spatio-temporal dependencies in varying traffic scenarios. Then, wedevise two specialized experts to help identify stable trends and periodicpatterns within the traffic data, respectively. By cascading these experts withMAGLs, CP-MoE delivers congestion predictions in a more robust andinterpretable manner. Furthermore, an ordinal regression strategy is adopted tofacilitate effective collaboration among diverse experts. Extensive experimentson real-world datasets demonstrate the superiority of our proposed methodcompared with state-of-the-art spatio-temporal prediction models. Moreimportantly, CP-MoE has been deployed in DiDi to improve the accuracy andreliability of the travel time estimation system.|快速的城市化使交通堵塞显著升级，这突出表明需要先进的拥堵预测服务来支持智能交通系统。作为全球其中一个最大的网约车平台，滴滴非常重视交通挤塞预测的准确性，以提高其实时服务(例如行车时间估计和路线规划)的成效和可靠性。尽管在拥塞预测方面已经做了大量的工作，但是大多数工作在处理异构和动态的时空依赖性(例如，周期性和非周期性的拥塞)方面仍然存在不足，特别是在存在噪声和不完整的交通数据的情况下。在本文中，我们引入了一个拥塞预测混合专家，CP-MoE，以解决上述挑战。我们首先提出具有拥塞感知偏差的自适应图学习器(MAGL)的门限混合模型，以提高模型能力，有效地捕获不同交通场景中复杂的时空依赖关系。然后，我们分别召集两位专家来帮助识别交通数据中的稳定趋势和周期模式。通过将这些专家与 MAGL 级联，CP-MoE 以更强大和可解释的方式提供拥塞预测。此外，采用有序回归策略促进不同专家之间的有效协作。对实际数据集的大量实验表明，与最先进的时空预测模型相比，本文提出的方法具有优越性。更重要的是，滴滴已部署了运输部，以提高旅行时间估计系统的准确性和可靠性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interpretable+Cascading+Mixture-of-Experts+for+Urban+Traffic+Congestion+Prediction)|0|
|[False Positives in A/B Tests](https://doi.org/10.1145/3637528.3671631)|Ron Kohavi, Nanyu Chen|Kohavi, Los Altos, CA, USA; Expedia Group, San Francisco, CA, USA|A/B tests, or online controlled experiments, are used heavily in the software industry to evaluate implementations of ideas, as the paradigm is the gold standard in science for establishing causality: the changes introduced in the treatment caused the changes to the metrics of interest with high probability. What distinguishes software experiments, or A/B tests, from experiments in many other domains is the scale (e.g., over 100 experiment treatments may launch on a given workday in large companies) and the effect sizes that matter to the business are small (e.g., a 3% improvement to conversion rate from a single experiment is a cause for celebration). The humbling reality is that most experiments fail to improve key metrics, and success rates of only about 10-20% are most common. With low success rates, the industry standard alpha threshold of 0.05 implies a high probability of false positives. We begin with motivation about why false positives are expensive in many software domains. We then offer several approaches to estimate the true success rate of experiments, given the observed "win" rate (statistically significant positive improvements), and show examples from Expedia and Optimizely. We offer a modified procedure for experimentation, based in sequential group testing, that selectively extends experiments to reduce false positives, increase power, at a small increase to runtime. We conclude with a discussion of the difference between ideas and experiments in practice, terms that are often incorrectly used interchangeably.|A/B 测试，或在线控制实验，在软件行业被大量使用来评估思想的实施，因为范式是建立因果关系的科学黄金标准: 在治疗中引入的变化以高概率引起了感兴趣度量的变化。软件实验或者 A/B 测试与其他领域的实验的区别在于规模(例如，在大公司的某个工作日可能会有超过100个实验治疗方案)和对企业有影响的效应规模很小(例如，单个实验的转化率提高3% 是值得庆祝的)。令人难堪的现实是，大多数实验都无法改进关键指标，最常见的成功率只有10-20% 左右。由于成功率较低，行业标准 alpha 阈值为0.05意味着误报的可能性很高。我们从为什么假阳性在许多软件领域是昂贵的动机开始。然后，我们提供了几种方法来估计实验的真实成功率，给出观察到的“胜利”率(统计学上显著的积极改进) ，并展示了 Expedia 和 Optimizely 的例子。我们提供了一个修改过的实验过程，基于顺序组测试，有选择地扩展实验，以减少误报，增加功率，在运行时的小幅增加。最后，我们讨论了实践中思想和实验之间的区别，这些术语经常被错误地互换使用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=False+Positives+in+A/B+Tests)|0|
|[Causal Machine Learning for Cost-Effective Allocation of Development Aid](https://doi.org/10.1145/3637528.3671551)|Milan Kuzmanovic, Dennis Frauen, Tobias Hatt, Stefan Feuerriegel|ETH Zurich, Zurich, Switzerland; Munich Center for Machine Learning & LMU Munich, Munich, Germany|The Sustainable Development Goals (SDGs) of the United Nations provide a blueprint of a better future by "leaving no one behind", and, to achieve the SDGs by 2030, poor countries require immense volumes of development aid. In this paper, we develop a causal machine learning framework for predicting heterogeneous treatment effects of aid disbursements to inform effective aid allocation. Specifically, our framework comprises three components: (i) a balancing autoencoder that uses representation learning to embed high-dimensional country characteristics while addressing treatment selection bias; (ii) a counterfactual generator to compute counterfactual outcomes for varying aid volumes to address small sample-size settings; and (iii) an inference model that is used to predict heterogeneous treatment-response curves. We demonstrate the effectiveness of our framework using data with official development aid earmarked to end HIV/AIDS in 105 countries, amounting to more than USD 5.2 billion. For this, we first show that our framework successfully computes heterogeneous treatment-response curves using semi-synthetic data. Then, we demonstrate our framework using real-world HIV data. Our framework points to large opportunities for a more effective aid allocation, suggesting that the total number of new HIV infections could be reduced by up to 3.3% (~50,000 cases) compared to the current allocation practice.|联合国的可持续发展目标(SDGs)提供了一个“不让任何人掉队”的美好未来蓝图。为了在2030年前实现这些目标，贫穷国家需要大量的发展援助。在本文中，我们建立了一个因果机器学习框架，用于预测援助支出的异质处理效果，从而为有效的援助分配提供信息。具体而言，我们的框架包括三个组成部分: (i)使用表示学习来嵌入高维国家特征的平衡自动编码器，同时解决治疗选择偏倚; (ii)反事实生成器计算不同援助量的反事实结果以解决小样本量设置; 和(iii)用于预测异质治疗-反应曲线的推理模型。我们利用官方发展援助的数据，在105个国家展示了我们框架的有效性，这些数据被指定用于终结艾滋病毒/艾滋病，总额超过52亿美元。为此，我们首先展示了我们的框架成功地使用半合成数据计算异质治疗-反应曲线。然后，我们使用真实世界的 HIV 数据来展示我们的框架。我们的框架指出了更有效的援助分配的巨大机会，表明与目前的分配做法相比，新的艾滋病毒感染总数可以减少3.3% (约5万例)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causal+Machine+Learning+for+Cost-Effective+Allocation+of+Development+Aid)|0|
|[Chromosomal Structural Abnormality Diagnosis by Homologous Similarity](https://doi.org/10.1145/3637528.3671642)|Juren Li, Fanzhe Fu, Ran Wei, Yifei Sun, Zeyu Lai, Ning Song, Xin Chen, Yang Yang|Hangzhou Diagens Biotechnology Co., Ltd., Hangzhou, China; Zhejiang University, Hangzhou, China|Pathogenic chromosome abnormalities are very common among the general population. While numerical chromosome abnormalities can be quickly and precisely detected, structural chromosome abnormalities are far more complex and typically require considerable efforts by human experts for identification. This paper focuses on investigating the modeling of chromosome features and the identification of chromosomes with structural abnormalities. Most existing data-driven methods concentrate on a single chromosome and consider each chromosome independently, overlooking the crucial aspect of homologous chromosomes. In normal cases, homologous chromosomes share identical structures, with the exception that one of them is abnormal. Therefore, we propose an adaptive method to align homologous chromosomes and diagnose structural abnormalities through homologous similarity. Inspired by the process of human expert diagnosis, we incorporate information from multiple pairs of homologous chromosomes simultaneously, aiming to reduce noise disturbance and improve prediction performance. Extensive experiments on real-world datasets validate the effectiveness of our model compared to baselines.|致病性染色体异常在一般人群中很常见。虽然数字染色体异常可以快速、准确地检测出来，但结构染色体异常要复杂得多，通常需要人类专家作出相当大的努力来鉴定。本文主要研究染色体特征的建模和结构异常染色体的识别。大多数现有的数据驱动方法集中在一个单一的染色体和考虑每个染色体独立，忽略了同源染色体的关键方面。在正常情况下，同源染色体共享相同的结构，除了其中一个是异常的。因此，我们提出了一种自适应的方法来排列同源染色体和诊断结构异常的同源相似性。受人类专家诊断过程的启发，我们同时整合多对同源染色体的信息，以减少噪声干扰，提高预测性能。在真实世界数据集上的大量实验验证了我们的模型与基线相比的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Chromosomal+Structural+Abnormality+Diagnosis+by+Homologous+Similarity)|0|
|[An Open and Large-Scale Dataset for Multi-Modal Climate Change-aware Crop Yield Predictions](https://doi.org/10.1145/3637528.3671536)|Fudong Lin, Kaleb Guillot, Summer Crawford, Yihe Zhang, Xu Yuan, NianFeng Tzeng|University of Delaware, Newark, DE, USA; University of Louisiana at Lafeyette, Lafayette, LA, USA|Precise crop yield predictions are of national importance for ensuring food security and sustainable agricultural practices. While AI-for-science approaches have exhibited promising achievements in solving many scientific problems such as drug discovery, precipitation nowcasting, etc., the development of deep learning models for predicting crop yields is constantly hindered by the lack of an open and large-scale deep learning-ready dataset with multiple modalities to accommodate sufficient information. To remedy this, we introduce the CropNet dataset, the first terabyte-sized, publicly available, and multi-modal dataset specifically targeting climate change-aware crop yield predictions for the contiguous United States (U.S.) continent at the county level. Our CropNet dataset is composed of three modalities of data, i.e., Sentinel-2 Imagery, WRF-HRRR Computed Dataset, and USDA Crop Dataset, for over 2200 U.S. counties spanning 6 years (2017-2022), expected to facilitate researchers in developing versatile deep learning models for timely and precisely predicting crop yields at the county-level, by accounting for the effects of both short-term growing season weather variations and long-term climate change on crop yields. Besides, we develop the CropNet package, offering three types of APIs, for facilitating researchers in downloading the CropNet data on the fly over the time and region of interest, and flexibly building their deep learning models for accurate crop yield predictions. Extensive experiments have been conducted on our CropNet dataset via employing various types of deep learning solutions, with the results validating the general applicability and the efficacy of the CropNet dataset in climate change-aware crop yield predictions. We have officially released our CropNet dataset on Hugging Face Datasets https://huggingface.co/datasets/CropNet/CropNet and our CropNet package on the Python Package Index (PyPI) https://pypi.org/project/cropnet. Code and tutorials are available at https://github.com/fudong03/CropNet.|准确的作物产量预测对于确保粮食安全和可持续农业做法具有国家重要性。虽然人工智能科学方法在解决诸如药物发现，降水临近预报等许多科学问题方面显示出有希望的成就，但是预测作物产量的深度学习模型的发展不断受到阻碍，因为缺乏具有多种方式以容纳足够信息的开放和大规模的深度学习准备数据集。为了解决这个问题，我们引入了 CropNet 数据集，这是第一个兆字节大小的公开可用的多模式数据集，专门针对县级美国本土(美国)大陆的气候变化意识作物产量预测。我们的 CropNet 数据集由三种模式的数据组成，即哨兵 -2图像，WRF-HRRR 计算数据集和美国农业部作物数据集，跨越6年(2017-2022)的2200多个美国县，预计将促进研究人员开发通用的深度学习模型，通过考虑短期生长季节气候变化和长期气候变化对作物产量的影响，及时和精确地预测县级作物产量。此外，我们还开发了 CropNet 软件包，提供三种类型的 API，以方便研究人员在感兴趣的时间和地区动态下载 CropNet 数据，并灵活地建立深度学习模型，以准确预测作物产量。通过使用各种类型的深度学习解决方案，在我们的 CropNet 数据集上进行了广泛的实验，结果验证了 CropNet 数据集在气候变化意识作物产量预测方面的普遍适用性和有效性。我们已经在 Hugging Face 数据集 https://huggingface.co/Datasets/CropNet/CropNet 上正式发布了我们的 CropNet 数据集，在 Python Package Index (PyPI) https://PyPI.org/project/CropNet 上正式发布了我们的 CropNet 包。代码和教程可在 https://github.com/fudong03/cropnet 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Open+and+Large-Scale+Dataset+for+Multi-Modal+Climate+Change-aware+Crop+Yield+Predictions)|0|
|[Modeling User Retention through Generative Flow Networks](https://doi.org/10.1145/3637528.3671531)|Ziru Liu, Shuchang Liu, Bin Yang, Zhenghai Xue, Qingpeng Cai, Xiangyu Zhao, Zijian Zhang, Lantao Hu, Han Li, Peng Jiang|Kuaishou Technology, Beijing, China; Nanyang Technological University, Singapore, Singapore; City University of Hong Kong, Hong Kong, China|Recommender systems aim to fulfill the user's daily demands. While mostexisting research focuses on maximizing the user's engagement with the system,it has recently been pointed out that how frequently the users come back forthe service also reflects the quality and stability of recommendations.However, optimizing this user retention behavior is non-trivial and posesseveral challenges including the intractable leave-and-return user activities,the sparse and delayed signal, and the uncertain relations between users'retention and their immediate feedback towards each item in the recommendationlist. In this work, we regard the retention signal as an overall estimation ofthe user's end-of-session satisfaction and propose to estimate this signalthrough a probabilistic flow. This flow-based modeling technique canback-propagate the retention reward towards each recommended item in the usersession, and we show that the flow combined with traditional learning-to-rankobjectives eventually optimizes a non-discounted cumulative reward for bothimmediate user feedback and user retention. We verify the effectiveness of ourmethod through both offline empirical studies on two public datasets and onlineA/B tests in an industrial platform.|推荐系统旨在满足用户的日常需求。虽然大多数现有的研究集中在最大限度地提高用户对系统的参与，但是最近有人指出，用户回访服务的频率也反映了推荐的质量和稳定性。然而，优化这种用户保留行为是非常重要的，并且存在一些挑战，包括棘手的离开和返回用户活动，稀疏和延迟信号，以及用户保留与他们对推荐列表中每个项目的即时反馈之间的不确定关系。在这项工作中，我们认为保留信号作为一个用户的会话结束满意度的总体估计，并建议通过一个概率流估计这个信号。这种基于流的建模技术可以反向传播用户会话中每个推荐项目的保留奖励，我们表明流与传统的学习到排名目标相结合，最终优化了用户即时反馈和用户保留的非折扣累积奖励。通过对两个公共数据集的离线实证研究和工业平台上的在线 A/B 检验，验证了本文方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modeling+User+Retention+through+Generative+Flow+Networks)|0|
|[BacktrackSTL: Ultra-Fast Online Seasonal-Trend Decomposition with Backtrack Technique](https://doi.org/10.1145/3637528.3671510)|Haoyu Wang, Hongke Guo, Zhaoliang Zhu, You Zhang, Yu Zhou, Xudong Zheng|Alibaba Group, Zhejiang, Hangzhou, China; Alibaba Group, Beijing, China; Alibaba Group, Hangzhou, Zhejiang, China|Seasonal-trend decomposition (STD) is a crucial task in time series data analysis. Due to the challenges of scalability, there is a pressing need for an ultra-fast online algorithm. However, existing algorithms either fail to handle long-period time series (such as OnlineSTL), or need time-consuming iterative processes (such as OneShotSTL). Therefore, we propose BacktrackSTL, the first non-iterative online STD algorithm with period-independent O(1) update complexity. It is also robust to outlier, seasonality shift and trend jump because of the combination of outlier-resilient smoothing, non-local seasonal filtering and backtrack technique. Experimentally, BacktrackSTL decomposes a value within 1.6 μs, which is 15X faster than the state-of-the-art online algorithm OneShotSTL, while maintaining comparable accuracy to the best offline algorithm RobustSTL. We have also deployed BacktrackSTL on the top of Apache Flink to decompose monitoring metrics in Alibaba Cloud for over a year. Besides, we have open-sourced the artifact of this proposal on GitHub.|季节趋势分解(STD)是时间序列数据分析中的一项重要任务。由于可扩展性的挑战，迫切需要一种超快的在线算法。然而，现有的算法要么不能处理长周期时间序列(如 OnlineSTL) ，要么需要耗时的迭代过程(如 OneShotSTL)。因此，我们提出了 BacktrackSTL，这是第一个具有周期无关的 O (1)更新复杂度的非迭代在线 STD 算法。异常点弹性平滑、非局部季节性滤波和回溯技术相结合，使得该方法对异常点、季节性变化和趋势跳变具有较强的鲁棒性。实验上，BacktrackSTL 在1.6 μs 内分解一个值，这比最先进的在线算法 OneShotSTL 快15倍，同时保持与最好的离线算法 RobustSTL 相当的精度。我们还在 Apache Flink 顶部部署了 BacktrackSTL 来分解阿里巴巴云中的监控指标，时间已经超过一年。此外，我们已经在 GitHub 上开源了这个提议的工件。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BacktrackSTL:+Ultra-Fast+Online+Seasonal-Trend+Decomposition+with+Backtrack+Technique)|0|
|[Deep Ensemble Shape Calibration: Multi-Field Post-hoc Calibration in Online Advertising](https://doi.org/10.1145/3637528.3671529)|Shuai Yang, Hao Yang, Zhuang Zou, Linhe Xu, Shuo Yuan, Yifan Zeng|Shopee Discovery Ads, Beijing, China|In the e-commerce advertising scenario, estimating the true probabilities (known as a calibrated estimate) on Click-Through Rate (CTR) and Conversion Rate (CVR) is critical. Previous research has introduced numerous solutions for addressing the calibration problem. These methods typically involve the training of calibrators using a validation set and subsequently applying these calibrators to correct the original estimated values during online inference. However, what sets e-commerce advertising scenarios is the challenge of multi-field calibration. Multi-field calibration requires achieving calibration in each field. In order to achieve multi-field calibration, it is necessary to have a strong data utilization ability. Because the quantity of pCTR specified range for single field-value (such as user ID and item ID) sample is relatively small, which makes the calibrator more difficult to train. However, existing methods have difficulty effectively addressing these issues. To solve these problems, we propose a new method named Deep Ensemble Shape Calibration (DESC). In terms of business understanding and interpretability, we decompose multi-field calibration into value calibration and shape calibration. We introduce innovative basis calibration functions, which enhance both function expression capabilities and data utilization by combining these basis calibration functions. A significant advancement lies in the development of an allocator capable of allocating the most suitable calibrators to different estimation error distributions within diverse fields and values. We achieve significant improvements in both public and industrial datasets. In online experiments, we observe a +2.5% increase in CVR and +4.0% in GMV (Gross Merchandise Volume). Our code is now available at: https://github.com/HaoYang0123/DESC.|在电子商务广告场景中，估计点进率(CTR)和转化率(CVR)的真实概率(称为校准估计)是至关重要的。以往的研究已经提出了许多解决校准问题的方案。这些方法通常涉及使用验证集对校准器进行训练，然后应用这些校准器在在线推断期间校正原始估计值。然而，设置电子商务广告情景是多领域标定的挑战。多场校准要求实现每个场的校准。为了实现多场校准，必须具有较强的数据利用能力。由于单一场值(如用户 ID 和项目 ID)样品的 pCTR 指定范围的数量相对较小，使得校准器的训练更加困难。然而，现有的方法很难有效地解决这些问题。为了解决这些问题，我们提出了一种新的方法——深度集合形状标定(DESC)。从业务理解和可解释性的角度出发，将多场校准分解为数值校准和形状校准。我们引入了创新的基校正函数，通过将这些基校正函数结合起来，提高了函数表达能力和数据利用率。一个重要的进步在于分配器的发展，能够分配最合适的校准器，以不同的领域和价值不同的估计误差分布。我们在公共数据集和工业数据集方面都取得了显著的改进。在线实验中，我们观察到 CVR 增加了2.5% ，GMV (商品总量)增加了4.0% 。我们的代码现在可以在以下 https://github.com/haoyang0123/desc 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Ensemble+Shape+Calibration:+Multi-Field+Post-hoc+Calibration+in+Online+Advertising)|0|
|[GraphStorm: All-in-one Graph Machine Learning Framework for Industry Applications](https://doi.org/10.1145/3637528.3671603)|Da Zheng, Xiang Song, Qi Zhu, Jian Zhang, Theodore Vasiloudis, Runjie Ma, Houyu Zhang, Zichen Wang, Soji Adeshina, Israt Nisa, Alejandro Mottini, Qingjun Cui, Huzefa Rangwala, Belinda Zeng, Christos Faloutsos, George Karypis|Amazon Search AI, Seattle, WA, USA; Amazon Search AI, Palo Alto, CA, USA; Amazon AWS AI, Santa Clara, CA, USA; Amazon AWS AI, Washington, D.C., USA; Amazon SP, Seattle, WA, USA; Amazon AWS AI, Seattle, WA, USA; Amazon AWS AI, New York, NY, USA|Graph machine learning (GML) is effective in many business applications.However, making GML easy to use and applicable to industry applications withmassive datasets remain challenging. We developed GraphStorm, which provides anend-to-end solution for scalable graph construction, graph model training andinference. GraphStorm has the following desirable properties: (a) Easy to use:it can perform graph construction and model training and inference with just asingle command; (b) Expert-friendly: GraphStorm contains many advanced GMLmodeling techniques to handle complex graph data and improve model performance;(c) Scalable: every component in GraphStorm can operate on graphs with billionsof nodes and can scale model training and inference to different hardwarewithout changing any code. GraphStorm has been used and deployed for over adozen billion-scale industry applications after its release in May 2023. It isopen-sourced in Github: https://github.com/awslabs/graphstorm.|图形机器学习(GML)在许多商业应用中非常有效。然而，使 GML 易于使用并适用于拥有大量数据集的工业应用程序仍然具有挑战性。我们开发了 GraphStorm，它为可伸缩图的构建、图模型的训练和推理提供了端到端的解决方案。GraphStorm 有以下令人满意的特性: (a)易于使用: 它只需一个命令就可以执行图形构造和模型训练和推理; (b)专家友好型: GraphStorm 包含许多先进的 GML 建模技术，可以处理复杂的图形数据和提高模型性能; (c)可扩展性: GraphStorm 中的每个组件都可以操作具有数十亿个节点的图形，并且可以在不改变任何代码的情况下对不同的硬件进行模型训练和推理。GraphStorm 自2023年5月发布以来，已经在超过10亿个规模的行业应用程序中使用和部署。它在 Github 中是开源的:  https://Github.com/awslabs/graphstorm。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphStorm:+All-in-one+Graph+Machine+Learning+Framework+for+Industry+Applications)|0|
|[A Tutorial on Multi-Armed Bandit Applications for Large Language Models](https://doi.org/10.1145/3637528.3671440)|Djallel Bouneffouf, Raphaël Féraud|IBM Research, New York, New York, USA; Orange Orange Innovation, Lannion, France|This tutorial offers a comprehensive guide on using multi-armed bandit (MAB) algorithms to improve Large Language Models (LLMs). As Natural Language Processing (NLP) tasks grow, efficient and adaptive language generation systems are increasingly needed. MAB algorithms, which balance exploration and exploitation under uncertainty, are promising for enhancing LLMs. The tutorial covers foundational MAB concepts, including the exploration-exploitation trade-off and strategies like epsilon-greedy, UCB (Upper Confidence Bound), and Thompson Sampling. It then explores integrating MAB with LLMs, focusing on designing architectures that treat text generation options as arms in a bandit problem. Practical aspects like reward design, exploration policies, and scalability are discussed. Real-world case studies demonstrate the benefits of MAB-augmented LLMs in content recommendation, dialogue generation, and personalized content creation, showing how these techniques improve relevance, diversity, and user engagement.|本教程提供了一个关于使用多臂老虎机(MAB)算法来改进大型语言模型(LLM)的全面指南。随着自然语言处理(NLP)任务的增加，对高效、自适应的语言生成系统的需求越来越大。MAB 算法能够在不确定条件下平衡勘探和开发，在提高 LLM 方面具有广阔的应用前景。本教程涵盖了基本的 MAB 概念，包括勘探-开发权衡和策略，如 epsilon- 贪婪、 UCB (上限置信区间)和 Thompson 抽样。然后，本文探讨了将 MAB 与 LLM 集成在一起的问题，重点是设计将文本生成选项视为土匪问题中的武器的体系结构。讨论了奖励设计、探索策略和可伸缩性等实际问题。现实世界的案例研究证明了 MAB 增强 LLM 在内容推荐、对话生成和个性化内容创建方面的好处，展示了这些技术如何提高相关性、多样性和用户参与度。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Tutorial+on+Multi-Armed+Bandit+Applications+for+Large+Language+Models)|0|
|[Domain-Driven LLM Development: Insights into RAG and Fine-Tuning Practices](https://doi.org/10.1145/3637528.3671445)|José Cassio dos Santos Junior, Rachel Hu, Richard Song, Yunfei Bai|CambioML Corp, San Jose, California, USA; Amazon Web Services, Seattle, Washington, USA; Epsilla, Jersey City, New Jersey, USA|To improve Large Language Model (LLM) performance on domain specific applications, ML developers often leverage Retrieval Augmented Generation (RAG) and LLM Fine-Tuning. RAG extends the capabilities of LLMs to specific domains or an organization's internal knowledge base, without the need to retrain the model. On the other hand, Fine-Tuning approach updates LLM weights with domain-specific data to improve performance on specific tasks. The fine-tuned model is particularly effective to systematically learn new comprehensive knowledge in a specific domain that is not covered by the LLM pre-training. This tutorial walks through the RAG and Fine-Tuning techniques, discusses the insights of their advantages and limitations, and provides best practices of adopting the methodologies for the LLM tasks and use cases. The hands-on labs demonstrate the advanced techniques to optimize the RAG and fine-tuned LLM architecture that handles domain specific LLM tasks. The labs in the tutorial are designed by using a set of open-source python libraries to implement the RAG and fine-tuned LLM architecture.|为了提高特定领域应用程序的大语言模型(LLM)性能，ML 开发人员经常利用检索增强生成(RAG)和 LLM 微调。RAG 将 LLM 的功能扩展到特定领域或组织的内部知识库，而不需要对模型进行再培训。另一方面，微调方法使用特定于领域的数据更新 LLM 权重，以提高特定任务的性能。经过微调的模型对于系统地学习某一特定领域的新的综合知识尤其有效，而这一领域不在 LLM 预训范围之内。本教程介绍了 RAG 和微调技术，讨论了它们的优点和局限性，并提供了为 LLM 任务和用例采用方法的最佳实践。实际操作的实验室演示了优化 RAG 的先进技术和处理领域特定 LLM 任务的微调 LLM 体系结构。本教程中的实验室是通过使用一组开放源码的 python 库来实现 RAG 和经过微调的 LLM 体系结构来设计的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Domain-Driven+LLM+Development:+Insights+into+RAG+and+Fine-Tuning+Practices)|0|
|[Recent and Upcoming Developments in Randomized Numerical Linear Algebra for Machine Learning](https://doi.org/10.1145/3637528.3671461)|Michal Derezinski, Michael W. Mahoney|University of Michigan, Ann Arbor, USA; ICSI, LBNL, and University of California, Berkeley, USA|Large matrices arise in many machine learning and data analysis applications,including as representations of datasets, graphs, model weights, and first andsecond-order derivatives. Randomized Numerical Linear Algebra (RandNLA) is anarea which uses randomness to develop improved algorithms for ubiquitous matrixproblems. The area has reached a certain level of maturity; but recent hardwaretrends, efforts to incorporate RandNLA algorithms into core numericallibraries, and advances in machine learning, statistics, and random matrixtheory, have lead to new theoretical and practical challenges. This articleprovides a self-contained overview of RandNLA, in light of these developments.|大矩阵出现在许多机器学习和数据分析应用中，包括数据集、图形、模型权重以及一阶和二阶导数的表示。随机数值线性代数(RandNLA)是一个利用随机性开发改进算法的领域，用于解决无处不在的矩阵问题。这个领域已经达到了一定程度的成熟; 但是最近的硬件趋势，将 RandNLA 算法融入核心数字库的努力，以及机器学习、统计学和随机矩阵理论的进步，已经导致了新的理论和实践挑战。根据这些发展，本文提供了 RandNLA 的独立概述。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Recent+and+Upcoming+Developments+in+Randomized+Numerical+Linear+Algebra+for+Machine+Learning)|0|
|[Graph Machine Learning Meets Multi-Table Relational Data](https://doi.org/10.1145/3637528.3671471)|Quan Gan, Minjie Wang, David Wipf, Christos Faloutsos|CMU & Amazon, Pittsburgh, PA, USA; Amazon, Shanghai, China|While graph machine learning, and notably graph neural networks (GNNs), have gained immense traction in recent years, application is predicated on access to a known input graph upon which predictive models can be trained. And indeed, within the most widely-studied public evaluation benchmarks such graphs are provided, with performance comparisons conditioned on curated data explicitly adhering to this graph. However, in real-world industrial applications, the situation is often quite different. Instead of a known graph, data are originally collected and stored across multiple tables in a repository, at times with ambiguous or incomplete relational structure. As such, to leverage the latest GNN architectures it is then up to a skilled data scientist to first manually construct a graph using intuition and domain knowledge, a laborious process that may discourage adoption in the first place. To narrow this gap and broaden the applicability of graph ML, we survey existing tools and strategies that can be combined to address the more fundamental problem of predictive tabular modeling over data native to multiple tables, with no explicit relational structure assumed a priori. This involves tracing a comprehensive path through related table join discovery and fuzzy table joining, column alignment, automated relational database (RDB) construction, extracting graphs from RDBs, graph sampling, and finally, graph-centric trainable predictive architectures. Although efforts to build deployable systems that integrate all of these components while minimizing manual effort remain in their infancy, this survey will nonetheless reduce barriers to entry and help steer the graph ML community towards promising research directions and wider real-world impact.|虽然近年来图机学习，尤其是图神经网络(GNN)已经获得了巨大的推动力，但是应用的前提是获得一个可以训练预测模型的已知输入图。事实上，在研究最广泛的公共评价基准中，提供了这样的图表，其绩效比较的条件是明确遵守这一图表的精选数据。然而，在实际的工业应用中，情况往往大不相同。数据最初是通过存储库中的多个表收集和存储的，而不是已知的图形，有时关系结构不明确或不完整。因此，要利用最新的 GNN 架构，就需要一位技术娴熟的数据科学家首先利用直觉和领域知识手动构建一个图表，这是一个费力的过程，可能首先会阻碍采用。为了缩小这个差距和扩大图形 ML 的适用性，我们调查了现有的工具和策略，这些工具和策略可以结合起来解决更基本的问题，即对多个表本身的数据进行预测性表格建模，没有明确的关系结构假设先验。这包括通过相关的表连接发现和模糊表连接、列对齐、自动化关系数据库(RDB)构建、从关系数据库中提取图形、图形采样，以及以图形为中心的可训练预测架构来跟踪一个全面的路径。尽管建立集成所有这些组件同时尽量减少人工操作的可部署系统的努力仍然处于起步阶段，但这项调查将减少进入的障碍，并有助于引导机器学习图形社区朝着有前途的研究方向和更广泛的现实世界影响。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Machine+Learning+Meets+Multi-Table+Relational+Data)|0|
|[Systems for Scalable Graph Analytics and Machine Learning: Trends and Methods](https://doi.org/10.1145/3637528.3671472)|Da Yan, Lyuheng Yuan, Akhlaque Ahmad, Chenguang Zheng, Hongzhi Chen, James Cheng|; Kasma Pte. Ltd., Singapore, Singapore; Department of Computer Science, Indiana University Bloomington, Bloomington, Indiana, USA|Graph-theoretic algorithms and graph machine learning models are essential tools for addressing many real-life problems, such as social network analysis and bioinformatics. To support large-scale graph analytics, graph-parallel systems have been actively developed for over one decade, such as Google's Pregel and Spark's GraphX, which (i) promote a think-like-a-vertex computing model and target (ii) iterative algorithms and (iii) those problems that output a value for each vertex. However, this model is too restricted for supporting the rich set of heterogeneous operations for graph analytics and machine learning that many real applications demand. In recent years, two new trends emerge in graph-parallel systems research: (1) a novel think-like-a-task computing model that can efficiently support the various computationally expensive problems of subgraph search; and (2) scalable systems for learning graph neural networks. These systems effectively complement the diversity needs of graph-parallel tools that can flexibly work together in a comprehensive graph processing pipeline for real applications, with the capability of capturing structural features. This tutorial will provide an effective categorization of the recent systems in these two directions based on their computing models and adopted techniques, and will review the key design ideas of these systems.|图论算法和图机学习模型是解决社会网络分析和生物信息学等现实问题的重要工具。为了支持大规模的图形分析，图形并行系统已经积极开发了10多年，例如谷歌的 Pregel 和 Spark 的 GraphX，它们(i)推广像顶点一样思考的计算模型和目标(ii)迭代算法，以及(iii)那些为每个顶点输出一个值的问题。然而，这个模型对于支持图分析和机器学习所需的丰富的异构操作集是非常有限的，这是许多实际应用所需要的。近年来，图并行系统的研究出现了两个新的趋势: (1)一种新的类任务思维计算模型，能够有效地支持子图搜索的各种计算昂贵的问题; (2)学习图神经网络的可扩展系统。这些系统有效地补充了图形并行工具的多样性需求，能够在面向实际应用的综合图形处理流水线中灵活地协同工作，并具有捕获结构特征的能力。本教程将根据计算模型和采用的技术对这两个方向的最新系统进行有效的分类，并将回顾这些系统的关键设计思想。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Systems+for+Scalable+Graph+Analytics+and+Machine+Learning:+Trends+and+Methods)|0|
|[Machine Learning in Finance](https://doi.org/10.1145/3637528.3671488)|Leman Akoglu, Nitesh V. Chawla, Josep DomingoFerrer, Eren Kurshan, Senthil Kumar, Vidyut M. Naware, José A. RodríguezSerrano, Isha Chaturvedi, Saurabh Nagrecha, Mahashweta Das, Tanveer A. Faruquie|Rennes Sch Business, 2 Rue Robert Arbrissel, F-35065 Rennes, France; Rennes Sch Business, Dept Finance & Accounting, Rennes, France; Dublin City Univ, DCU Business Sch, Financial & Operat Performance Grp, Dublin, Ireland; Rennes Sch Business, Dept Strategy & Innovat, Rennes, France|We provide a first comprehensive structuring of the literature applying machine learning to finance. We use a probabilistic topic modeling approach to make sense of this diverse body of research spanning across the disciplines of finance, economics, computer sciences, and decision sciences. Through the topic modelling approach, a Latent Dirichlet Allocation technique, we are able to extract the 14 coherent research topics that are the focus of the 5,204 academic articles we analyze from the years 1990 to 2018. We first describe and structure these topics, and then further show how the topic focus has evolved over the last two decades. Our study thus provides a structured topography for finance researchers seeking to integrate machine learning research approaches in their exploration of finance phenomena. We also showcase the benefits to finance researchers of the method of probabilistic modeling of topics for deep comprehension of a body of literature, especially when that literature has diverse multi-disciplinary actors.|我们提供了第一个综合结构的文献应用机器学习金融。我们使用概率主题建模方法来理解这种跨越金融、经济、计算机科学和决策科学学科的多样化研究体系。通过主题建模(一种隐含狄利克雷分布技术) ，我们能够提取出14个连贯的研究主题，这些主题是我们从1990年到2018年分析的5204篇学术论文的重点。我们首先描述和构建这些主题，然后进一步说明主题焦点在过去二十年中是如何演变的。因此，我们的研究提供了一个结构化的地形金融研究人员寻求整合机器学习研究方法在他们的金融现象的探索。我们还展示了金融研究人员的好处，为深入理解文献主体的主题的概率建模方法，特别是当文献有不同的多学科行为者。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Machine+Learning+in+Finance)|0|
|[From Word-prediction to Complex Skills: Compositional Thinking and Metacognition in LLMs](https://doi.org/10.1145/3637528.3672193)|Sanjeev Arora|Princeton University, Princeton, NJ, USA|The talk will present evidence that today's large language models (LLMs) display somewhat deeper "understanding'' than one would naively expect.1. When asked to solve a task by combining a set of k simpler skills ("test of compositional capability"), they are able to do so despite not having seen the same combination of skills during their training.2. They demonstrate ability to reason about of their own learning processes, which is analogous to "metacognitive knowledge"[Flavel'76] in humans. For instance, given examples of an evaluation task, they can produce a catalog of suitably named skills that are relevant for solving each example of that task. Furthermore, this catalog of skills is meaningful, in the sense that incorporating it into training pipelines improves performance (including of other unrelated LLMs) on that task.We discuss mechanisms by which such complex understanding could arise (including a theory by [Arora,Goyal'23] that tries to explain (a)) and also give examples of how to leverage LLM meta knowledge to improve LLM training pipelines as well as evaluations. 1. When asked to solve a task by combining a set of k simpler skills ("test of compositional capability"), they are able to do so despite not having seen the same combination of skills during their training. 2. They demonstrate ability to reason about of their own learning processes, which is analogous to "metacognitive knowledge"[Flavel'76] in humans. For instance, given examples of an evaluation task, they can produce a catalog of suitably named skills that are relevant for solving each example of that task. Furthermore, this catalog of skills is meaningful, in the sense that incorporating it into training pipelines improves performance (including of other unrelated LLMs) on that task.|这次演讲将提供证据，证明今天的大型语言模型(LLM)所表现出来的“理解”比人们天真地期望的要深刻得多。1。当被要求结合一系列简单的技能(“组合能力测试”)来解决一个任务时，他们能够做到这一点，尽管在他们的训练中没有看到相同的技能组合。2。他们展示了对自己的学习过程进行推理的能力，这类似于人类的“元认知知识”[ Flavel’76]。例如，给定一个评估任务的示例，他们可以生成一个适当命名的技能目录，这些技能与解决该任务的每个示例相关。此外，这个技能目录是有意义的，因为将其纳入培训管道可以提高该任务的表现(包括其他不相关的 LLM)。我们讨论了这种复杂的理解可能产生的机制(包括[ Arora，Goyal’23]试图解释(a)的理论) ，还举例说明如何利用 LLM 元知识来改善 LLM 培训管道以及评估。1.当被要求结合一系列简单的技能(“组合能力测试”)来解决一个任务时，他们能够做到这一点，尽管在他们的训练中没有看到相同的技能组合。2.他们展示了对自己的学习过程进行推理的能力，这类似于人类的“元认知知识”[ Flavel’76]。例如，给定一个评估任务的示例，他们可以生成一个适当命名的技能目录，这些技能与解决该任务的每个示例相关。此外，这种技能目录是有意义的，因为将其纳入培训管道可以提高该任务的性能(包括其他不相关的 LLM)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Word-prediction+to+Complex+Skills:+Compositional+Thinking+and+Metacognition+in+LLMs)|0|
|[GEO: Generative Engine Optimization](https://doi.org/10.1145/3637528.3671900)|Pranjal Aggarwal, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan, Ameet Deshpande|Indian Institute of Technology Delhi, New Delhi, India; Independent, Seattle, USA; Princeton University, Princeton, USA|The advent of large language models (LLMs) has ushered in a new paradigm of search engines that use generative models to gather and summarize information to answer user queries. This emerging technology, which we formalize under the unified framework of generative engines (GEs), can generate accurate and personalized responses, rapidly replacing traditional search engines like Google and Bing. Generative Engines typically satisfy queries by synthesizing information from multiple sources and summarizing them using LLMs. While this shift significantly improvesuser utility and generative search engine traffic, it poses a huge challenge for the third stakeholder -- website and content creators. Given the black-box and fast-moving nature of generative engines, content creators have little to no control over when and how their content is displayed. With generative engines here to stay, we must ensure the creator economy is not disadvantaged. To address this, we introduce Generative Engine Optimization (GEO), the first novel paradigm to aid content creators in improving their content visibility in generative engine responses through a flexible black-box optimization framework for optimizing and defining visibility metrics. We facilitate systematic evaluation by introducing GEO-bench, a large-scale benchmark of diverse user queries across multiple domains, along with relevant web sources to answer these queries. Through rigorous evaluation, we demonstrate that GEO can boost visibility by up to 40% in generative engine responses. Moreover, we show the efficacy of these strategies varies across domains, underscoring the need for domain-specific optimization methods. Our work opens a new frontier in information discovery systems, with profound implications for both developers of generative engines and content creators.|大型语言模型(LLM)的出现开创了搜索引擎的新范式，这种搜索引擎使用生成模型来收集和总结信息，以回答用户的查询。这种新兴技术，我们在生成引擎(GEs)的统一框架下形式化，可以产生准确和个性化的响应，迅速取代传统的搜索引擎，如谷歌和必应。生成引擎通常通过合成来自多个源的信息并使用 LLM 对其进行汇总来满足查询。虽然这种转变显著地提高了用户效用和生成性搜索引擎流量，但是它对第三方利益相关者——网站和内容创建者——提出了巨大的挑战。考虑到生成引擎的黑盒子和快速移动的特性，内容创建者几乎不能控制何时以及如何显示他们的内容。随着生产引擎的存在，我们必须确保创造者经济不会处于不利地位。为了解决这个问题，我们引入了生成引擎优化(GEO) ，这是第一个通过灵活的黑盒优化框架来优化和定义可见性指标，从而帮助内容创建者在生成引擎响应中提高内容可见性的新范例。我们通过引入 GEO-bench 来促进系统评估，GEO-bench 是跨多个领域的不同用户查询的大规模基准，以及用于回答这些查询的相关网络资源。通过严格的评估，我们证明 GEO 可以提高可见性高达40% 的生成引擎响应。此外，我们显示这些策略的功效不同领域，强调需要领域特定的优化方法。我们的工作开辟了信息发现系统的新前沿，对生成引擎的开发人员和内容创建人员都具有深远的意义。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GEO:+Generative+Engine+Optimization)|0|
|[AI for Nature: From Science to Impact](https://doi.org/10.1145/3637528.3672192)|Tanya Y. BergerWolf|The Ohio State University, Columbus, OH, USA|Computation has fundamentally changed the way we study nature. New data collection technologies, such as GPS, high-definition cameras, autonomous vehicles under water, on the ground, and in the air, genotyping, acoustic sensors, and crowdsourcing, are generating data about life on the planet that are orders of magnitude richer than any previously collected. Yet, our ability to extract insight from this data lags substantially behind our ability to collect it. The need for understanding is more urgent than ever and the challenges are great. We are in the middle of the 6th extinction, losing the planet's biodiversity at an unprecedented rate and scale. In many cases, we do not even have the basic numbers of what species we are losing, which impacts our ability to understand biodiversity loss drivers, predict the impact on ecosystems, and implement policy. From the basic science perspective, the new data opens the possibility of understanding function of traits of organisms and ecosystems, which is critical for biologists to predict effects of environmental change or genetic manipulation and to understand the significance of patterns in the four-billion-year evolutionary history of life. The key to unlocking the potential of this data are machine learning (ML) and artificial intelligence (AI) methods, which are already beginning to have significant impacts on research across ecology and conservation. AI can turn data into high resolution information source about living organisms, enabling scientific inquiry, conservation, and policy decisions. The talk introduces a new field of science, imageomics, and presents a vision and examples of AI as a trustworthy partner both in science and biodiversity conservation, discussing opportunities and challenges.|计算从根本上改变了我们研究自然的方式。新的数据收集技术，如全球定位系统、高清摄像机、水下、地面和空中的自动驾驶汽车、基因分型、声学传感器和众包，正在生成有关地球上生命的数据，这些数据比以前收集到的任何数量级都要丰富。然而，我们从这些数据中提取洞察力的能力远远落后于我们收集数据的能力。对理解的需求比以往任何时候都更加迫切，挑战也是巨大的。我们正处于第六次物种灭绝的过程中，以前所未有的速度和规模丧失着地球上的生物多样性。在许多情况下，我们甚至不知道我们正在失去的物种的基本数量，这影响了我们了解生物多样性丧失驱动因素、预测对生态系统的影响以及执行政策的能力。从基础科学的角度来看，这些新数据为理解生物体和生态系统特征的功能开辟了可能性，这对于生物学家预测环境变化或基因操纵的影响以及理解40亿年生命进化史中模式的重要性至关重要。释放这些数据潜力的关键是机器学习(ML)和人工智能(AI)方法，它们已经开始对生态学和自然保护研究产生重大影响。人工智能可以将数据转化为有关生物体的高分辨率信息源，从而实现科学调查、保护和政策决策。演讲介绍了一个新的科学领域，图像组学，并提出了一个愿景和例子，人工智能作为一个值得信赖的合作伙伴，在科学和生物多样性保护，讨论机会和挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AI+for+Nature:+From+Science+to+Impact)|0|
|[Statistical Models of Top-k Partial Orders](https://doi.org/10.1145/3637528.3672014)|Amel Awadelkarim, Johan Ugander|Stanford University, Stanford, CA, USA|In many contexts involving ranked preferences, agents submit partial orders over available alternatives. Statistical models often treat these as marginal in the space of total orders, but this approach overlooks information contained in the list length itself. In this work, we introduce and taxonomize approaches for jointly modeling distributions over top-k partial orders and list lengths k, considering two classes of approaches: composite models that view a partial order as a truncation of a total order, and augmented ranking models that model the construction of the list as a sequence of choice decisions, including the decision to stop. For composite models, we consider three dependency structures for joint modeling of order and truncation length. For augmented ranking models, we consider different assumptions on how the stop-token choice is modeled. Using data consisting of partial rankings from San Francisco school choice and San Francisco ranked choice elections, we evaluate how well the models predict observed data and generate realistic synthetic datasets. We find that composite models, explicitly modeling length as a categorical variable, produce synthetic datasets with accurate length distributions, and an augmented model with position-dependent item utilities jointly models length and preferences in the training data best, as measured by negative log loss. Methods from this work have significant implications on the simulation and evaluation of real-world social systems that solicit ranked preferences.|在许多涉及排序偏好的上下文中，代理提交的部分订单优于可用的替代方案。统计模型通常将这些信息视为总订单空间中的边际信息，但这种方法忽略了列表长度本身所包含的信息。在这项工作中，我们介绍和分类的方法联合建模分布在最高 k 偏序和列表长度 k，考虑到两类方法: 组合模型，视为一个总序的截断部分的偏序，和扩展排名模型，模型列表的结构作为一系列的选择决定，包括停止的决定。对于复合模型，我们考虑了三种依赖结构，用于顺序和截断长度的联合建模。对于扩展排序模型，我们考虑了关于如何建模停止令牌选择的不同假设。利用来自旧金山学校选择和旧金山排名选择的部分排名数据，我们评估模型预测观测数据和生成现实的合成数据集的效果。我们发现，明确地将长度建模为分类变量的复合模型产生具有精确长度分布的合成数据集，并且具有位置依赖性项目实用程序的增强模型联合模拟训练数据中的长度和偏好，如通过负对数损失测量的。从这项工作的方法有重大意义的模拟和评估现实世界的社会系统，征求排名的偏好。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Statistical+Models+of+Top-k+Partial+Orders)|0|
|[Resilient k-Clustering](https://doi.org/10.1145/3637528.3671888)|Sara Ahmadian, MohammadHossein Bateni, Hossein Esfandiari, Silvio Lattanzi, Morteza Monemizadeh, Ashkan NorouziFard|Department of Mathematics and Computer Science, TU Eindhoven, Eindhoven, Netherlands; Google, New york, USA; Google, New York, USA; Google, Zurich, Switzerland; Google, Barcelona, USA; Google, Seattle, USA|We study the problem of resilient clustering in the metric setting where one is interested in designing algorithms that return high quality solutions that preserve the clustering structure under perturbations of the input points. Our first contribution is to introduce a formal notion of algorithmic resiliency for clustering problems that, roughly speaking, requires an algorithm to have similar outputs on close inputs. Then, we notice that classic algorithms have weak resiliency guarantees and develop new algorithms for fundamental clustering problems such as k-center, k-median, and k-means. Finally, we complement our results with an experimental analysis showing the effectiveness of our techniques on real-world instances.|我们研究了度量设置中的弹性聚类问题，其中一个人感兴趣的是设计算法，返回高质量的解决方案，保留了在输入点扰动下的聚类结构。我们的第一个贡献是为聚类问题引入了算法弹性的形式化概念，粗略地说，这需要一个算法在相近的输入上具有相似的输出。然后，我们注意到经典算法具有较弱的弹性保证，并且针对基本聚类问题，如 k- 中心、 k- 中值和 k- 均值，提出了新的聚类算法。最后，我们用一个实验分析来补充我们的结果，该实验分析显示了我们的技术在真实世界实例中的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Resilient+k-Clustering)|0|
|[A Learned Generalized Geodesic Distance Function-Based Approach for Node Feature Augmentation on Graphs](https://doi.org/10.1145/3637528.3671858)|Amitoz Azad, Yuan Fang|Singapore Management University, Singapore, Singapore|Geodesic distances on manifolds have numerous applications in image processing, computer graphics and computer vision. In this work, we introduce an approach called `LGGD' (Learned Generalized Geodesic Distances). This method involves generating node features by learning a generalized geodesic distance function through a training pipeline that incorporates training data, graph topology and the node content features. The strength of this method lies in the proven robustness of the generalized geodesic distances to noise and outliers. Our contributions encompass improved performance in node classification tasks, competitive results with state-of-the-art methods on real-world graph datasets, the demonstration of the learnability of parameters within the generalized geodesic equation on graph, and dynamic inclusion of new labels.|流形上的测地距离在图像处理、计算机图形学和计算机视觉等领域有着广泛的应用。在这项工作中，我们介绍了一种方法称为“ LGGD”(学习广义测地距离)。该方法通过训练流水线学习广义测地距离函数生成节点特征，该流水线包括训练数据、图拓扑结构和节点内容特征。该方法的优点在于证明了广义测地距离对噪声和离群点的鲁棒性。我们的贡献包括提高节点分类任务的性能，在真实世界图形数据集上使用最先进的方法获得竞争结果，在图形上展示广义测地方程中参数的可学性，以及动态包含新标签。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Learned+Generalized+Geodesic+Distance+Function-Based+Approach+for+Node+Feature+Augmentation+on+Graphs)|0|
|[Improved Active Covering via Density-Based Space Transformation](https://doi.org/10.1145/3637528.3671794)|MohammadHossein Bateni, Hossein Esfandiari, Samira HosseinGhorban, Alipasha Montaseri|Sharif University of Technology, Tehran, Iran; Google Research, New York City, New York, USA; Google Research, London, United Kingdom; School of Computer Science, Institute for Research in Fundamental Sciences, Tehran, Iran|In this work, we study active covering, a variant of the active-learning problem that involves labeling (or identifying) all of the examples with a positive label. We propose a couple of algorithms, namely Density-Adjusted Non-Adaptive (DANA) learner and Density-Adjusted Adaptive (DAA) learner, that query the labels according to a distance function that is adjusted by the density function. Under mild assumptions, we prove that our algorithms discover all of the positive labels while querying only a sublinear number of examples from the support of negative labels for constant-dimensional spaces (see Theorems 5 and 6). Our experiments show that our champion algorithm DAA consistently improves over the prior work on some standard benchmark datasets, including those used by the previous work, as well as a couple of data sets on credit card fraud. For instance, when measuring performance using AUC, our algorithm is the best in 25 out of 27 experiments over 7 different datasets.|在这项工作中，我们研究主动覆盖，一个变种的主动学习问题，涉及标记(或识别)所有的例子与一个积极的标签。提出了密度调整非自适应(DANA)学习算法和密度调整自适应(DAA)学习算法，它们根据密度函数调整的距离函数对标签进行查询。在温和的假设下，我们证明了我们的算法发现所有的正标签，而查询只有一个次线性数量的例子从负标签的支持常量维空间(见定理5和6)。我们的实验表明，我们的冠军算法 DAA 在一些标准基准数据集(包括前面的工作所使用的数据集)以及一些信用卡欺诈数据集上的改进是一致的。例如，当使用 AUC 测量性能时，我们的算法在7个不同数据集的27个实验中的25个中是最好的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improved+Active+Covering+via+Density-Based+Space+Transformation)|0|
|[Towards Robust Information Extraction via Binomial Distribution Guided Counterpart Sequence](https://doi.org/10.1145/3637528.3672067)|Yinhao Bai, Yuhua Zhao, Zhixin Han, Hang Gao, Chao Xue, Mengting Hu|; JD Explore Academy, Beijing, Chile; College of Artificial Intelligence, Tianjin University of Science and Technology, Tianjin, China; College of Software, Nankai University, Tianjin, China|Information extraction (IE) aims to extract meaningful structured tuples from unstructured text. Existing studies usually utilize a pre-trained generative language model that rephrases the original sentence into a target sequence, which can be easily decoded as tuples. However, traditional evaluation metrics treat a slight error within the tuple as an entire prediction failure, which is unable to perceive the correctness extent of a tuple. For this reason, we first propose a novel IE evaluation metric called Matching Score to evaluate the correctness of the predicted tuples in more detail. Moreover, previous works have ignored the effects of semantic uncertainty when focusing on the generation of the target sequence. We argue that leveraging the built-in semantic uncertainty of language models is beneficial for improving its robustness. In this work, we propose Binomial distribution guided counterpart sequence (BCS) method, which is a model-agnostic approach. Specifically, we propose to quantify the built-in semantic uncertainty of the language model by bridging all local uncertainties with the whole sequence. Subsequently, with the semantic uncertainty and Matching Score, we formulate a unique binomial distribution for each local decoding step. By sampling from this distribution, a counterpart sequence is obtained, which can be regarded as a semantic complement to the target sequence. Finally, we employ the Kullback-Leibler divergence to align the semantics of the target sequence and its counterpart. Extensive experiments on 14 public datasets over 5 information extraction tasks demonstrate the effectiveness of our approach on various methods. Our code and dataset are available at https://github.com/byinhao/BCS.|信息抽取(IE)旨在从非结构化文本中提取有意义的结构化元组。现有的研究通常利用预先训练好的生成语言模型将原始句子重新组合成一个目标序列，这个目标序列可以很容易地被解码为元组。但是，传统的评估指标将元组中的一个轻微错误视为整个预测失败，无法察觉元组的正确程度。出于这个原因，我们首先提出了一种新的 IE 评估度量，称为匹配得分，以评估正确性的预测元组更详细。此外，以往的研究忽略了语义不确定性对目标语序列生成的影响。我们认为，利用语言模型内在的语义不确定性有利于提高其鲁棒性。在这项工作中，我们提出了二项分布引导的对应序列(BCS)方法，这是一种模型无关的方法。具体来说，我们提出通过将所有局部不确定性与整个序列连接起来，来量化语言模型内在的语义不确定性。随后，利用语义不确定性和匹配得分，我们为每个局部解码步骤制定一个独特的二项分布。从这个分布中抽样得到一个对应序列，可以看作是对目标序列的语义补充。最后，利用 Kullback-Leibler 散度对目标序列及其对应序列进行语义对齐。通过对14个公共数据集超过5个信息抽取任务的大量实验，证明了我们的方法在各种方法上的有效性。我们的代码和数据集可在 https://github.com/byinhao/bcs 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Robust+Information+Extraction+via+Binomial+Distribution+Guided+Counterpart+Sequence)|0|
|[Graph Mamba: Towards Learning on Graphs with State Space Models](https://doi.org/10.1145/3637528.3672044)|Ali Behrouz, Farnoosh Hashemi|Cornell University, Ithaca, NY, USA|Graph Neural Networks (GNNs) have shown promising potential in graph representation learning. The majority of GNNs define a local message-passing mechanism, propagating information over the graph by stacking multiple layers. These methods, however, are known to suffer from two major limitations: over-squashing and poor capturing of long-range dependencies. Recently, Graph Transformers (GTs) emerged as a powerful alternative to Message-Passing Neural Networks (MPNNs). GTs, however, have quadratic computational cost, lack inductive biases on graph structures, and rely on complex Positional Encodings (PE). In this paper, we show that while Transformers, complex message-passing, and PE are sufficient for good performance in practice, neither is necessary. Motivated by the recent success of State Space Models (SSMs), we present Graph Mamba Networks (GMNs), a framework for a new class of GNNs based on selective SSMs. We discuss the new challenges when adapting SSMs to graph-structured data, and present four required steps to design GMNs, where we choose (1) Neighborhood Tokenization, (2) Token Ordering, (3) Architecture of SSM Encoder, and (4) Local Encoding. We provide theoretical justification for the power of GMNs, and experimentally show that GMNs attain an outstanding performance in various benchmark datasets. The code is available in this link.|图形神经网络(GNN)在图形表示学习中显示出了巨大的潜力。大多数 GNN 定义了一种本地消息传递机制，通过叠加多个层在图上传播信息。然而，已知这些方法存在两个主要的局限性: 过度压缩和不能很好地捕获远程依赖关系。最近，图形变换器(GTs)作为消息传递神经网络(MPNN)的一个强有力的替代品出现了。然而，GT 具有二次计算开销，缺乏对图结构的归纳偏差，并且依赖于复杂的位置编码(PE)。在本文中，我们表明，虽然变压器，复杂的消息传递和 PE 足以在实践中取得良好的性能，但这两者都不是必要的。受最近国家空间模型(SSM)的成功的启发，我们提出了图曼巴网络(GMNs) ，一个基于选择性 SSM 的新类 GNNs 的框架。我们讨论了使 SSM 适应图形结构数据的新挑战，并提出了设计 GMN 所需的四个步骤，其中我们选择(1)邻域标记化，(2)标记排序，(3) SSM 编码器的体系结构，和(4)本地编码。我们从理论上证明了 GMN 的强大功能，并通过实验证明了 GMN 在各种基准数据集中都能获得优异的性能。该代码在此链接中可用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Mamba:+Towards+Learning+on+Graphs+with+State+Space+Models)|0|
|[FaultInsight: Interpreting Hyperscale Data Center Host Faults](https://doi.org/10.1145/3637528.3672051)|Tingzhu Bi, Yang Zhang, Yicheng Pan, Yu Zhang, Meng Ma, Xinrui Jiang, Linlin Han, Feng Wang, Xian Liu, Ping Wang|Peking University & Shuanghu Laboratory, Beijing, China; ByteDance Inc., Beijing, China; Peking University, Beijing, China|Operating and maintaining hyperscale data centers involving millions of service hosts has been an extremely intricate task to tackle for top Internet companies. Incessant system failures cost operators countless hours of browsing through performance metrics to diagnose the underlying root cause to prevent the recurrence. Although many state-of-the-art (SOTA) methods have used time-series causal discovery to construct causal relationships among anomalous metrics, they only focus on homogeneous service-level performance metrics and fail to yield useful insights on heterogeneous host-level metrics. To address the challenge, this study presents FaultInsight, a highly interpretable deep causal host fault diagnosing framework that offers diagnostic insights from various perspectives to reduce human effort in troubleshooting. We evaluate FaultInsight using dozens of incidents collected from our production environment. FaultInsight provides markedly better root cause identification accuracy than SOTA baselines in our incident dataset. It also shows outstanding advantages in terms of deployability in real production systems. Our engineers are deeply impressed by FaultInsight's ability to interpret incidents from multiple perspectives, helping them quickly understand the mechanism behind the faults.|操作和维护涉及数百万服务主机的超大规模数据中心对于顶级互联网公司来说是一项极其复杂的任务。不断的系统故障使操作员花费无数小时浏览性能指标，以诊断潜在的根本原因，防止再次发生故障。虽然许多最先进的(SOTA)方法已经使用时间序列因果发现来构建异常指标之间的因果关系，但它们只关注同质服务水平的性能指标，并且不能产生关于异质主机水平指标的有用见解。为了应对这一挑战，本研究提出了 FaultInsight，一个高度可解释的深层因果主机故障诊断框架，从不同角度提供诊断见解，以减少人们在故障排除方面的努力。我们使用从生产环境中收集的几十个事件来评估 FaultInsight。在我们的事件数据集中，FaultInsight 比 SOTA 基线提供了明显更好的根本原因识别准确性。它还显示了在实际生产系统中可部署性方面的显著优势。我们的工程师对 FaultInsight 从多个角度解释事件的能力印象深刻，它帮助他们快速理解错误背后的机制。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FaultInsight:+Interpreting+Hyperscale+Data+Center+Host+Faults)|0|
|[Making Temporal Betweenness Computation Faster and Restless](https://doi.org/10.1145/3637528.3671825)|Filippo Brunelli, Pierluigi Crescenzi, Laurent Viennot|Inria, DI ENS, Paris, France; Gran Sasso Science Institute, L'Aquila, Italy; European Commission -- JRC, Seville, Spain|Buss et al [KDD 2020] recently proved that the problem of computing the betweenness of all nodes of a temporal graph is computationally hard in the case of foremost and fastest paths, while it is solvable in time O(n3T2) in the case of shortest and shortest foremost paths, where n is the number of nodes and T is the number of distinct time steps. A new algorithm for temporal betweenness computation is introduced in this paper. In the case of shortest and shortest foremost paths, it requires O(n + M) space and runs in time O(nM)=O(n3T), where M is the number of temporal edges, thus significantly improving the algorithm of Buss et al in terms of time complexity (note that T is usually large). Experimental evidence is provided that our algorithm performs between twice and almost 250 times better than the algorithm of Buss et al. Moreover, we were able to compute the exact temporal betweenness values of several large temporal graphs with over a million of temporal edges. For such size, only approximate computation was possible by using the algorithm of Santoro and Sarpe [WWW 2022]. Maybe more importantly, our algorithm extends to the case of restless walks (that is, walks with waiting constraints in each node), thus providing a polynomial-time algorithm (with complexity O(nM)) for computing the temporal betweenness in the case of several different optimality criteria. Such restless computation was known only for the shortest criterion (Rymar et al [JGAA 2023]), with complexity O(n2MT2). We performed an extensive experimental validation by comparing different waiting constraints and different optimisation criteria. Moreover, as a case study, we investigate six public transit networks including Berlin, Rome, and Paris. Overall we find a general consistency between the different variants of betweenness centrality. However, we do measure a sensible influence of waiting constraints, and note some cases of low correlation for certain pairs of criteria in some networks.|Buss 等[ KDD 2020]最近证明，对于最短和最快路径，计算时间图中所有节点之间的间隔问题是计算困难的，而对于最短和最短的最优路径，n 是节点数，T 是不同时间步长的数目，则可以在时间 O (n3T2)内求解。本文介绍了一种新的时间间隔计算算法。在最短和最短前向路径的情况下，它需要 O (n + M)空间并且在时间 O (nM) = O (n3T)中运行，其中 M 是时间边的个数，从而在时间复杂度方面显著改进了 Buss 等人的算法(注意 T 通常很大)。实验结果表明，该算法的性能是 Buss 等算法的两倍至250倍。此外，我们能够精确地计算几个大型时间图的时间间隔值与超过一百万的时间边缘。对于这样的大小，只有近似计算是可能的使用 Santoro 和 Sarpe 的算法[ WWW 2022]。也许更重要的是，我们的算法扩展到不宁行走(即在每个节点中具有等待约束的行走)的情况，从而提供了一个多项式时间算法(具有复杂度 O (nM))来计算在几个不同的最优性准则的情况下的时间间隔。这种无休止的计算仅以最短的标准(Rymar 等[ JGAA 2023])而闻名，其复杂度为 O (n2MT2)。我们通过比较不同的等待约束和不同的优化标准进行了广泛的实验验证。此外，作为一个案例研究，我们调查了六个公共交通网络，包括柏林，罗马和巴黎。总的来说，我们发现了中间性中心性的不同变体之间的一般一致性。然而，我们确实测量了等待约束的明显影响，并注意到在一些网络中某些标准对的低相关性的情况。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Making+Temporal+Betweenness+Computation+Faster+and+Restless)|0|
|[Tackling Instance-Dependent Label Noise with Class Rebalance and Geometric Regularization](https://doi.org/10.1145/3637528.3671707)|Shuzhi Cao, Jianfei Ruan, Bo Dong, Bin Shi||In label-noise learning, accurately identifying the transition matrix is crucial for developing statistically consistent classifiers. This task is complicated by instance-dependent noise, which introduces identifiability challenges in the absence of stringent assumptions. Existing methods use neural networks to estimate the transition matrix by initially extracting confident clean instances. However, this extraction process is hindered by severe inter-class imbalance and a bias toward selecting unambiguous intra-class instances, leading to a distorted understanding of noise patterns. To tackle these challenges, our paper introduces a Class Rebalance and Geometric Regularization-based Framework (CRGR). CRGR employs a smoothed, noise-tolerant reweighting mechanism to equilibrate inter-class representation, thereby mitigating the risk of model overfitting to dominant classes. Additionally, recognizing that instances with similar characteristics often exhibit parallel noise patterns, we propose that the transition matrix should mirror the similarity of the feature space. This insight promotes the inclusion of ambiguous instances in training, serving as a form of geometric regularization. Such a strategy enhances the model's ability to navigate diverse noise patterns and strengthens its generalization capabilities. By addressing both inter-class and intra-class biases, CRGR offers a more balanced and robust classification model. Extensive experiments on both synthetic and real-world datasets demonstrate CRGR's superiority over existing state-of-the-art methods, significantly boosting classification accuracy and showcasing its effectiveness in handling instance-dependent noise.|在标签噪声学习中，准确识别转移矩阵对于开发统计一致的分类器至关重要。由于实例相关噪声的存在，这项任务变得更加复杂，在缺乏严格假设的情况下，噪声引入了可识别性的挑战。现有的方法使用神经网络通过最初提取可信的干净实例来估计转移矩阵。然而，这个提取过程受到严重的类间不平衡和偏向于选择明确的类内实例的阻碍，导致对噪声模式的理解扭曲。为了应对这些挑战，本文提出了一种基于类再平衡和几何正则化的框架(CRGR)。CRGR 采用了一种平滑的、容忍噪声的重新加权机制来平衡类间表示，从而减少了模型过度拟合到显性类的风险。此外，我们认识到具有相似特征的实例通常表现出平行的噪声模式，我们建议转移矩阵应该反映特征空间的相似性。这种洞察力促进了训练中模糊实例的包含，作为几何正则化的一种形式。这种策略增强了模型导航不同噪声模式的能力，并增强了其泛化能力。通过解决类间和类内偏差，CRGR 提供了一个更加平衡和健壮的分类模型。在合成和真实世界数据集上的大量实验证明了 CRGR 相对于现有最先进的方法的优越性，显著提高了分类的准确性，并展示了其在处理实例相关噪声方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tackling+Instance-Dependent+Label+Noise+with+Class+Rebalance+and+Geometric+Regularization)|0|
|[DiffusionE: Reasoning on Knowledge Graphs via Diffusion-based Graph Neural Networks](https://doi.org/10.1145/3637528.3671997)|Zongsheng Cao, Jing Li, Zigan Wang, Jinliang Li|School of Economics and Management, Tsinghua University, Haidian, Beijing, China; ; University of Chinese Academy of Sciences, Beijing, China; School of Economics and Management, Tsinghua University, Beijing, China|Graph Neural Networks (GNNs) have demonstrated powerful capabilities in reasoning within Knowledge Graphs (KGs), gathering increasing attention. Our idea stems from the observation that the prior work typically employs hand-designed or sample-designed paradigms in the process of message propagation, engaging a set of adjacent entities at each step of propagation. As a result, such methods struggle with the increasing number of entities involved as propagation steps extend. Moreover, they neglect the message interactions between adjacent entities and propagation relations in KG reasoning, leading to semantic inconsistency during the message aggregation phase. To address these issues, we introduce a novel knowledge graph embedding method through a diffusion process, termed DiffusionE. Specifically, we reformulate the message propagation in knowledge reasoning as a diffusion process, regarding the message semantics as the diffusion signal. In this sense, guided by semantic information, messages can be transmitted between nodes effectively and adaptively. Furthermore, the theoretical analysis suggests our method can leverage an optimal diffusivity for message propagation in the semantic interactions of KGs. It shows that DiffusionE effectively leverages message interactions between entities and propagation relations, ensuring semantic consistency in KG reasoning. Comprehensive experiments reveal that our method attains state-of-the-art performance compared to prior work on several well-established benchmarks.|图形神经网络(GNN)在知识图(KGs)中展示了强大的推理能力，引起了越来越多的关注。我们的想法来源于这样的观察: 在消息传播的过程中，先前的工作通常使用手工设计或样本设计的范例，在传播的每个步骤中使用一组相邻的实体。因此，随着传播步骤的扩展，此类方法会遇到实体数量不断增加的问题。此外，在 KG 推理中忽略了相邻实体之间的消息交互和传播关系，导致了在消息聚合阶段的语义不一致。为了解决这些问题，我们介绍了一种新的知识图嵌入方法，通过扩散过程，称为扩散 E。具体地说，我们将知识推理中的消息传播重新表述为一个扩散过程，将消息语义视为扩散信号。从这个意义上说，在语义信息的引导下，信息可以在节点之间有效和自适应地传输。此外，理论分析表明，我们的方法可以利用最佳扩散率的信息传播的语义交互的幼稚园。实验结果表明，在 KG 推理中，扩散 E 有效地利用了实体间的消息交互和传播关系，保证了语义的一致性。综合实验表明，我们的方法达到了最先进的性能相比，以前的工作在几个良好的基准。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DiffusionE:+Reasoning+on+Knowledge+Graphs+via+Diffusion-based+Graph+Neural+Networks)|0|
|[Path-based Explanation for Knowledge Graph Completion](https://doi.org/10.1145/3637528.3671683)|Heng Chang, Jiangnan Ye, Alejo LopezAvila, Jinhua Du, Jia Li|Hong Kong University of Science and Technology, Guangzhou, China; Huawei Technologies Co., Ltd., London, United Kingdom; Huawei Technologies Co., Ltd., Beijing, China|Graph Neural Networks (GNNs) have achieved great success in Knowledge Graph Completion (KGC) by modelling how entities and relations interact in recent years. However, the explanation of the predicted facts has not caught the necessary attention. Proper explanations for the results of GNN-based KGC models increase model transparency and help researchers develop more reliable models. Existing practices for explaining KGC tasks rely on instance/subgraph-based approaches, while in some scenarios, paths can provide more user-friendly and interpretable explanations. Nonetheless, the methods for generating path-based explanations for KGs have not been well-explored. To address this gap, we propose Power-Link, the first path-based KGC explainer that explores GNN-based models. We design a novel simplified graph-powering technique, which enables the generation of path-based explanations with a fully parallelisable and memory-efficient training scheme. We further introduce three new metrics for quantitative evaluation of the explanations, together with a qualitative human evaluation. Extensive experiments demonstrate that Power-Link outperforms the SOTA baselines in interpretability, efficiency, and scalability. The code is available at https://github.com/OUTHIM/power-link|近年来，图神经网络(GNN)通过对实体和关系之间的相互作用进行建模，在知识图完成(KGC)方面取得了巨大的成功。然而，对预测事实的解释并没有引起必要的注意。对基于 GNN 的 KGC 模型结果的合理解释增加了模型的透明度，有助于研究人员开发更可靠的模型。解释 KGC 任务的现有实践依赖于基于实例/子图的方法，而在某些场景中，路径可以提供更加用户友好和可解释的解释。尽管如此，为幼儿园生成基于路径的解释的方法还没有得到很好的探索。为了解决这个差距，我们提出 Power-Link，第一个基于路径的 KGC 解释器，探索基于 GNN 的模型。我们设计了一种新颖的简化图形驱动技术，该技术能够生成基于路径的解释，并且具有完全并行和高效的记忆训练方案。我们进一步介绍了三个新的指标来定量评估的解释，以及定性的人类评价。大量的实验表明，Power-Link 在可解释性、效率和可扩展性方面优于 SOTA 基准。密码可在 https://github.com/outhim/power-link 查阅|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Path-based+Explanation+for+Knowledge+Graph+Completion)|0|
|[Cluster-Wide Task Slowdown Detection in Cloud System](https://doi.org/10.1145/3637528.3671936)|Feiyi Chen, Yingying Zhang, Lunting Fan, Yuxuan Liang, Guansong Pang, Qingsong Wen, Shuiguang Deng|Squirrel AI, Bellevue, USA; Alibaba Group, Hangzhou, China; The Hong Kong University of Science and Technology (Guangzhou), Hong Kong, China; Zhejiang University, Hangzhou, China; Singapore Management University, Singapore, Singapore; Zhejiang University & Alibaba Group, Hangzhou, China|Slow task detection is a critical problem in cloud operation and maintenance since it is highly related to user experience and can bring substantial liquidated damages. Most anomaly detection methods detect it from a single-task aspect. However, considering millions of concurrent tasks in large-scale cloud computing clusters, it becomes impractical and inefficient. Moreover, single-task slowdowns are very common and do not necessarily indicate a malfunction of a cluster due to its violent fluctuation nature in a virtual environment. Thus, we shift our attention to cluster-wide task slowdowns by utilizing the duration time distribution of tasks across a cluster, so that the computation complexity is not relevant to the number of tasks. The task duration time distribution often exhibits compound periodicity and local exceptional fluctuations over time. Though transformer-based methods are one of the most powerful methods to capture these time series normal variation patterns, we empirically find and theoretically explain the flaw of the standard attention mechanism in reconstructing subperiods with low amplitude when dealing with compound periodicity. To tackle these challenges, we propose SORN (i.e., Skimming Off subperiods in descending amplitude order and Reconstructing Non-slowing fluctuation), which consists of a Skimming Attention mechanism to reconstruct the compound periodicity and a Neural Optimal Transport module to distinguish cluster-wide slowdowns from other exceptional fluctuations. Furthermore, since anomalies in the training set are inevitable in a practical scenario, we propose a picky loss function, which adaptively assigns higher weights to reliable time slots in the training set. Extensive experiments demonstrate that SORN outperforms state-of-the-art methods on multiple real-world industrial datasets.|慢速任务检测是云操作和维护中的一个关键问题，因为它与用户体验密切相关，可以带来大量的 liquidated damage。大多数异常检测方法从单任务方面检测它。然而，考虑到大规模云计算集群中有数以百万计的并发任务，这就变得不切实际且效率低下。此外，单任务速度减慢非常普遍，并不一定表明由于集群在虚拟环境中的剧烈波动性而出现故障。因此，我们将注意力转移到集群范围的任务减速上，利用集群中任务的持续时间分布，因此计算复杂度与任务数量无关。任务持续时间分布通常表现为复合周期性和局部异常波动。虽然基于变压器的方法是获取这些时间序列正态变化模式的最有力的方法之一，但我们在处理复合周期时，从实证上发现并从理论上解释了标准注意机制在重构低振幅子周期时的缺陷。为了应对这些挑战，我们提出了 SORN (即，按降幅顺序略去子周期和重构非减速波动) ，它由略去注意机制重构复合周期和神经最优传输模块区分簇范围的减速和其他异常波动组成。此外，由于训练集中的异常在实际场景中是不可避免的，我们提出了一个挑剔的损失函数，它自适应地分配更高的权重给训练集中的可靠时隙。大量的实验表明，SORN 在多个真实世界的工业数据集上优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cluster-Wide+Task+Slowdown+Detection+in+Cloud+System)|0|
|[Scalable Algorithm for Finding Balanced Subgraphs with Tolerance in Signed Networks](https://doi.org/10.1145/3637528.3671674)|Jingbang Chen, Qiuyang Mang, Hangrui Zhou, Richard Peng, Yu Gao, Chenhao Ma|Computer Science Department, Carnegie Mellon University, Pittsburgh, PA, USA; School of Data Science, The Chinese University of Hong Kong, Shenzhen, Shenzhen, Guangdong, China; Institute for Interdisciplinary Information Sciences (IIIS), Tsinghua University, Beijing, China; David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Ontario, Canada; Independent, Beijing, China|Signed networks, characterized by edges labeled as either positive or negative, offer nuanced insights into interaction dynamics beyond the capabilities of unsigned graphs. Central to this is the task of identifying the maximum balanced subgraph, crucial for applications like polarized community detection in social networks and portfolio analysis in finance. Traditional models, however, are limited by an assumption of perfect partitioning, which fails to mirror the complexities of real-world data. Addressing this gap, we introduce an innovative generalized balanced subgraph model that incorporates tolerance for imbalance. Our proposed region-based heuristic algorithm, tailored for this NP -hard problem, strikes a balance between low time complexity and high-quality outcomes. Comparative experiments validate its superior performance against leading solutions, delivering enhanced effectiveness (notably larger subgraph sizes) and efficiency (achieving up to 100× speedup) in both traditional and generalized contexts.|有符号的网络，拥有属性边缘标记为正面或负面，提供了对互动动力学的细致洞察，超越了无符号图形的能力。其中心任务是确定最大平衡子图，这对于社交网络中的极化社区检测和金融中的投资组合分析等应用至关重要。然而，传统模型受到完美分区假设的限制，无法反映真实世界数据的复杂性。针对这一差距，我们引入了一个创新的广义平衡子图模型，其中包含了对不平衡的容忍度。我们提出的基于区域的启发式算法，针对这个 NP 难题，取得了低时间复杂度和高质量的结果之间的平衡。比较实验验证了其对领先解决方案的优越性能，在传统和一般情况下都提供了增强的有效性(尤其是更大的子图大小)和效率(达到100倍加速)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scalable+Algorithm+for+Finding+Balanced+Subgraphs+with+Tolerance+in+Signed+Networks)|0|
|[QGRL: Quaternion Graph Representation Learning for Heterogeneous Feature Data Clustering](https://doi.org/10.1145/3637528.3671839)|Junyang Chen, Yuzhu Ji, Rong Zou, Yiqun Zhang, Yiuming Cheung|School of Computer Science and Technology, Guangdong University of Technology, Guangzhou, China; Department of Computer Science, Hong Kong Baptist University, Hong Kong SAR, China|Clustering is one of the most commonly used techniques for unsupervised data analysis. As real data sets are usually composed of numerical and categorical features that are heterogeneous in nature, the heterogeneity in the distance metric and feature coupling prevents deep representation learning from achieving satisfactory clustering accuracy. Currently, supervised Quaternion Representation Learning (QRL) has achieved remarkable success in efficiently learning informative representations of coupled features from multiple views derived endogenously from the original data. To inherit the advantages of QRL for unsupervised heterogeneous feature representation learning, we propose a deep QRL model that works in an encoder-decoder manner. To ensure that the implicit couplings of heterogeneous feature data can be well characterized by representation learning, a hierarchical coupling encoding strategy is designed to convert the data set into an attributed graph to be the input of QRL. We also integrate the clustering objective into the model training to facilitate a joint optimization of the representation and clustering. Extensive experimental evaluations illustrate the superiority of the proposed Quaternion Graph Representation Learning (QGRL) method in terms of clustering accuracy and robustness to various data sets composed of arbitrary combinations of numerical and categorical features. The source code is opened at https://github.com/Juny-Chen/QGRL.git.|聚类是无监督数据分析中最常用的技术之一。由于实际数据集通常由数值特征和分类特征组成，这些特征本质上是异构的，距离度量和特征耦合的异构性使得深度表示学习无法获得满意的聚类精度。目前，有监督的四元数表示学习(QRL)在有效地学习耦合特征的信息表示方面取得了显著的成功。为了继承 QRL 在非监督异构特征表示学习中的优势，本文提出了一种基于编译码方式的深层 QRL 模型。为了确保异构特征数据的隐式耦合能够很好地进行拥有属性表示学习，设计了一种分层耦合编码策略，将数据集转换为一个属性图作为 QRL 的输入。将聚类目标集成到模型训练中，实现了表示和聚类的联合优化。大量的实验结果表明，所提出的四元数图表示学习(QGRL)方法在聚类精度和对由任意数值和分类特征组合构成的各种数据集的鲁棒性方面具有优越性。源代码在 https://github.com/juny-chen/qgrl.git 打开。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=QGRL:+Quaternion+Graph+Representation+Learning+for+Heterogeneous+Feature+Data+Clustering)|0|
|[Can a Deep Learning Model be a Sure Bet for Tabular Prediction?](https://doi.org/10.1145/3637528.3671893)|Jintai Chen, Jiahuan Yan, Qiyuan Chen, Danny Z. Chen, Jian Wu, Jimeng Sun|University of Notre Dame, South Bend, IN, USA; Zhejiang University, Hangzhou, Zhejiang, China; Univ. of Illinois Urbana-Champaign, Urbana, IL, USA|Data organized in tabular format is ubiquitous in real-world applications, and users often craft tables with biased feature definitions and flexibly set prediction targets of their interests. Thus, a rapid development of a robust, effective, dataset-versatile, user-friendly tabular prediction approach is highly desired. While Gradient Boosting Decision Trees (GBDTs) and existing deep neural networks (DNNs) have been extensively utilized by professional users, they present several challenges for casual users, particularly: (i) the dilemma of model selection due to their different dataset preferences, and (ii) the need for heavy hyperparameter searching, failing which their performances are deemed inadequate. In this paper, we delve into this question: Can we develop a deep learning model that serves as a sure bet solution for a wide range of tabular prediction tasks, while also being user-friendly for casual users? We delve into three key drawbacks of deep tabular models, encompassing: (P1) lack of rotational variance property, (P2) large data demand, and (P3) over-smooth solution. We propose ExcelFormer, addressing these challenges through a semi-permeable attention module that effectively constrains the influence of less informative features to break the DNNs' rotational invariance property (for P1), data augmentation approaches tailored for tabular data (for P2), and attentive feedforward network to boost the model fitting capability (for P3). These designs collectively make ExcelFormer a sure bet solution for diverse tabular datasets. Extensive and stratified experiments conducted on real-world datasets demonstrate that our model outperforms previous approaches across diverse tabular data prediction tasks, and this framework can be friendly to casual users, offering ease of use without the heavy hyperparameter tuning. The codes are available at https://github.com/whatashot/excelformer.|以表格形式组织的数据在实际应用中无处不在，用户经常使用有偏差的特征定义和灵活设置他们感兴趣的预测目标来编制表格。因此，一个健壮的，有效的，数据集通用的，用户友好的表格预测方法的快速发展是迫切需要的。虽然梯度提升决策树(GBDTs)和现有的深度神经网络(DNN)已被专业用户广泛使用，但它们对普通用户提出了一些挑战，特别是: (i)由于他们不同的数据集偏好，模型选择的困境，以及(ii)需要大量的超参数搜索，否则他们的性能被认为是不够的。在本文中，我们深入研究这个问题: 我们能否开发一个深度学习模型，作为一个可靠的解决方案，为广泛的表格预测任务，同时也是用户友好的临时用户？我们深入研究了深表模型的三个主要缺点，包括: (P1)缺乏旋转方差性质，(P2)大数据需求，(P3)过于光滑的解决方案。我们建议使用 ExcelForm，通过一个半渗透注意模块来应对这些挑战，该模块有效地限制了信息量较小的特征对打破 DNN 的旋转不变性属性(对于 P1)的影响，为表格数据量身定制的数据增强方法(对于 P2) ，以及专注的前馈网络来提高模型拟合能力(对于 P3)。这些设计共同使得 ExcelForm 成为不同表格数据集的一个确定的解决方案。在真实世界的数据集上进行的广泛和分层的实验表明，我们的模型在不同的表格数据预测任务中优于以前的方法，并且该框架可以友好地对临时用户，提供易于使用而不需要大量的超参数调整。密码可以在 https://github.com/whatashot/excelformer 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+a+Deep+Learning+Model+be+a+Sure+Bet+for+Tabular+Prediction?)|0|
|[Profiling Urban Streets: A Semi-Supervised Prediction Model Based on Street View Imagery and Spatial Topology](https://doi.org/10.1145/3637528.3671918)|Meng Chen, Zechen Li, Weiming Huang, Yongshun Gong, Yilong Yin|School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore; School of Software, Shandong University, Jinan, Shandong, China|With the expansion and growth of cities, profiling urban areas with the advent of multi-modal urban datasets (e.g., points-of-interest and street view imagery) has become increasingly important in urban planing and management. Particularly, street view images have gained popularity for understanding the characteristics of urban areas due to its abundant visual information and inherent correlations with human activities. In this study, we define a street segment represented by multiple street view images as the minimum spatial unit for analysis and predict its functional and socioeconomic indicators, which presents several challenges in modeling spatial distributions of images on a street and the spatial topology (adjacency) of streets. Meanwhile, Large Language Models are capable of understanding imagery data based on its extraordinary knowledge base and unveil a remarkable opportunity for profiling streets with images. In view of the challenges and opportunity, we present a semi-supervised Urban Street Profiling Model (USPM) based on street view imagery and spatial adjacency of urban streets. Specifically, given a street with multiple images, we first employ a newly designed spatial context-based contrastive learning method to generate feature vectors of images and then apply the LSTM-based fusion method to encode multiple images on a street to yield the street visual representation; we then create the descriptions of street scenes for street view images based on the SPHINX (a large language model) and produce the street textual representation; finally, we build an urban street graph based on spatial topology (adjacency) and employ a semi-supervised graph learning algorithm to further encode the street representations for prediction. We conduct thorough experiments with real-world datasets to assess the proposed USPM. The experimental results demonstrate that USPM considerably outperforms baseline methods in two urban prediction tasks.|随着城市的扩张和发展，随着多模式城市数据集(如兴趣点和街景图像)的出现，对城市地区进行剖析已经成为城市规划和管理中越来越重要的内容。特别是街景图像，由于其丰富的视觉信息和与人类活动的内在联系，在理解城市地域特征方面得到了广泛的应用。在这项研究中，我们定义了一个由多个街景图像表示的街段作为分析和预测其功能和社会经济指标的最小空间单元，这在建模街道上的图像的空间分布和街道的空间拓扑(邻接)方面提出了几个挑战。与此同时，大语言模型能够理解图像数据的基础上，其非凡的知识库，揭示了一个显着的机会，以图像剖面街道。针对城市街道面临的挑战和机遇，提出了一种基于街景图像和城市街道空间邻接的半监督城市街道剖面模型(USPM)。具体来说，我们首先采用一种新设计的基于空间上下文的对比学习方法生成图像的特征向量，然后应用基于 LSTM 的融合方法对街道上的多幅图像进行编码，得到街道的视觉表示; 然后基于 SPHINX (一种大语言模型)生成街道图像的街道场景描述并生成街道文本表示; 最后，我们构建了一个基于空间拓扑(邻接)的城市街道图，并采用半监督图学习算法进一步编码街道表示进行预测。我们进行彻底的实验与现实世界的数据集，以评估提出的 USPM。实验结果表明，在两种城市预测任务中，USPM 方法的预测效果明显优于基线方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Profiling+Urban+Streets:+A+Semi-Supervised+Prediction+Model+Based+on+Street+View+Imagery+and+Spatial+Topology)|0|
|[Large Language Model-driven Meta-structure Discovery in Heterogeneous Information Network](https://doi.org/10.1145/3637528.3671965)|Lin Chen, Fengli Xu, Nian Li, Zhenyu Han, Meng Wang, Yong Li, Pan Hui|; Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Hong Kong University of Science and Technology, Hong Kong, China; BNRist, Department of Electronic Engineering, Tsinghua University, Beijing, China; Hefei University of Technology, Hefei, China|Heterogeneous information networks (HIN) have gained increasing popularity in recent years for capturing complex relations between diverse types of nodes. Meta-structures are proposed as a useful tool to identify the important patterns in HINs, but hand-crafted meta-structures pose significant challenges for scaling up, drawing wide research attention towards developing automatic search algorithms. Previous efforts primarily focused on searching for meta-structures with good empirical performance, overlooking the importance of human comprehensibility and generalizability. To address this challenge, we draw inspiration from the emergent reasoning abilities of large language models (LLMs). We propose ReStruct, a meta-structure search framework that integrates LLM reasoning into the evolutionary procedure. ReStruct uses a grammar translator to encode the meta-structures into natural language sentences, and leverages the reasoning power of LLMs to evaluate their semantic feasibility. Besides, ReStruct also employs performance-oriented evolutionary operations. These two competing forces allow ReStruct to jointly optimize the semantic explainability and empirical performance of meta-structures. Furthermore, ReStruct contains a differential LLM explainer to generate and refine natural language explanations for the discovered meta-structures by reasoning through the search history. Experiments on eight representative HIN datasets demonstrate that ReStruct achieves state-of-the-art performance in both recommendation and node classification tasks. Moreover, a survey study involving 73 graduate students shows that the discovered meta-structures and generated explanations by ReStruct are substantially more comprehensible. Our code and questionnaire are available at https://github.com/LinChen-65/ReStruct.|异构信息网络(HIN)近年来由于捕获不同类型节点之间的复杂关系而越来越受到人们的欢迎。元结构被认为是识别 HIN 中重要模式的有用工具，但手工制作的元结构对扩展提出了重大挑战，引起了研究人员对开发自动搜索算法的广泛关注。以往的研究主要集中在寻找具有良好经验性能的元结构，忽视了人类可理解性和普遍性的重要性。为了应对这一挑战，我们从大型语言模型(LLM)的突发推理能力中获得灵感。我们提出了一个元结构搜索框架 ReStruct，它将 LLM 推理集成到进化过程中。ReStruct 使用语法翻译器将元结构编码成自然语言句子，并利用 LLM 的推理能力来评估其语义可行性。此外，ReStruct 还采用了面向性能的进化操作。这两种相互竞争的力量使得重构能够共同优化元结构的语义可解释性和经验性能。此外，ReStruct 还包含一个差分 LLM 解释器，通过搜索历史推理为已发现的元结构生成和细化自然语言解释。在八个典型的 HIN 数据集上的实验表明，ReStruct 在推荐和节点分类任务上都取得了最好的性能。此外，一项涉及73名研究生的调查研究表明，ReStruct 发现的元结构和生成的解释更容易理解。我们的代码和问卷可在 https://github.com/linchen-65/restruct 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Model-driven+Meta-structure+Discovery+in+Heterogeneous+Information+Network)|0|
|[Hate Speech Detection with Generalizable Target-aware Fairness](https://doi.org/10.1145/3637528.3671821)|Tong Chen, Danny Wang, Xurong Liang, Marten Risius, Gianluca Demartini, Hongzhi Yin|The University of Queensland, Brisbane, Australia|To counter the side effect brought by the proliferation of social media platforms, hate speech detection (HSD) plays a vital role in halting the dissemination of toxic online posts at an early stage. However, given the ubiquitous topical communities on social media, a trained HSD classifier can easily become biased towards specific targeted groups (e.g.,female andblack people), where a high rate of either false positive or false negative results can significantly impair public trust in the fairness of content moderation mechanisms, and eventually harm the diversity of online society. Although existing fairness-aware HSD methods can smooth out some discrepancies across targeted groups, they are mostly specific to a narrow selection of targets that are assumed to be known and fixed. This inevitably prevents those methods from generalizing to real-world use cases where new targeted groups constantly emerge (e.g., new forums created on Reddit) over time. To tackle the defects of existing HSD practices, we propose Generalizable target-aware Fairness (GetFair), a new method for fairly classifying each post that contains diverse and even unseen targets during inference. To remove the HSD classifier's spurious dependence on target-related features, GetFair trains a series of filter functions in an adversarial pipeline, so as to deceive the discriminator that recovers the targeted group from filtered post embeddings. To maintain scalability and generalizability, we innovatively parameterize all filter functions via a hypernetwork. Taking a target's pretrained word embedding as input, the hypernetwork generates the weights used by each target-specific filter on-the-fly without storing dedicated filter parameters. In addition, a novel semantic gap alignment scheme is imposed on the generation process, such that the produced filter function for an unseen target is rectified by its semantic affinity with existing targets used for training. Finally, experiments are conducted on two benchmark HSD datasets, showing advantageous performance of GetFair on out-of-sample targets among baselines.|为了应对社交媒体平台扩散带来的副作用，仇恨言论检测(HSD)在及早阻止有毒网络帖子的传播方面发挥着至关重要的作用。然而，鉴于社交媒体上无处不在的话题社区，经过训练的 HSD 分类器很容易偏向特定的目标群体(例如，女性和黑人) ，其中高比率的假阳性或假阴性结果可以显着损害公众对内容调节机制的公平性的信任，并最终损害网络社会的多样性。虽然现有的具有公平意识的 HSD 方法可以消除目标群体之间的一些差异，但它们大多特定于假定已知和固定的狭窄目标选择。这不可避免地阻止了这些方法随着时间的推移将新的目标群体(例如，在 Reddit 上创建的新论坛)推广到现实世界中的用例。为了解决现有 HSD 实践的缺陷，我们提出了一种可概化的目标感知公平(Generalable target-aware Fairness，GetFair) ，这是一种在推理过程中对包含不同甚至不可见目标的每篇文章进行公平分类的新方法。为了消除 HSD 分类器对目标相关特征的虚假依赖，GetFair 在对抗流水线中训练一系列过滤函数，以欺骗从过滤后嵌入中恢复目标群的鉴别器。为了保持可扩展性和通用性，我们创新地通过一个超网络参数化所有的过滤函数。该超网络以目标预先训练好的嵌入词作为输入，在不存储专用滤波器参数的情况下，实时生成每个目标特定滤波器所使用的权值。此外，在生成过程中加入了一种新的语义间隙对齐方案，使得生成的未知目标的过滤函数通过其与用于训练的现有目标的语义亲和性进行校正。最后，在两个基准 HSD 数据集上进行了实验，结果表明 GetFair 对基准之间的样本外目标具有优越的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hate+Speech+Detection+with+Generalizable+Target-aware+Fairness)|0|
|[GraphWiz: An Instruction-Following Language Model for Graph Computational Problems](https://doi.org/10.1145/3637528.3672010)|Nuo Chen, Yuhan Li, Jianheng Tang, Jia Li||Large language models (LLMs) have achieved impressive success across various domains, but their capability in understanding and resolving complex graph problems is less explored. To bridge this gap, we introduce GraphInstruct, a novel instruction-tuning dataset aimed at enabling language models to tackle a broad spectrum of graph problems through explicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an open-source language model capable of solving various graph computational problems while generating clear reasoning processes. To further enhance the model's performance and reliability, we integrate the Direct Preference Optimization (DPO) framework within the graph problem-solving context. The improved model, GraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with different complexity levels, surpassing GPT-4 which has an average accuracy of 43.8%. Our study also investigates the relationship between training data volume and model performance, emphasizing the risk of overfitting as data volume increases. Additionally, we explore the transferability of the proposed model across different tasks and datasets, demonstrating its robust zero-shot generalization capability. GraphWiz offers a new blueprint and valuable insights for developing LLMs specialized in graph reasoning and problem-solving.|大型语言模型(LLM)已经在各个领域取得了令人印象深刻的成功，但是它们在理解和解决复杂图形问题方面的能力却很少被研究。为了弥补这一差距，我们引入了 GraphDirect，这是一个新颖的指令调优数据集，旨在使语言模型能够通过显式推理路径解决广泛的图形问题。我们利用图形指令构建了 GraphWiz，这是一个开源的语言模型，能够在生成清晰的推理过程的同时解决各种图形计算问题。为了进一步提高模型的性能和可靠性，我们将直接偏好优化(DPO)框架集成到图问题求解环境中。改进后的模型 GraphWiz-DPO 在9个不同复杂度级别的任务中达到了65% 的平均准确率，超过了平均准确率为43.8% 的 GPT-4。我们的研究还调查了训练数据量和模型性能之间的关系，强调了随着数据量的增加过度拟合的风险。此外，我们还研究了该模型在不同任务和数据集之间的可转移性，证明了其鲁棒的零镜头泛化能力。GraphWiz 为开发专门用于图形推理和问题解决的 LLM 提供了一个新的蓝图和有价值的见解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphWiz:+An+Instruction-Following+Language+Model+for+Graph+Computational+Problems)|0|
|[Calibration of Time-Series Forecasting: Detecting and Adapting Context-Driven Distribution Shift](https://doi.org/10.1145/3637528.3671926)|Mouxiang Chen, Lefei Shen, Han Fu, Zhuo Li, Jianling Sun, Chenghao Liu|State Street Technology (Zhejiang) Ltd., Hangzhou, China; Salesforce Research Asia, Singapore, Singapore; Zhejiang University, Hangzhou, China|Recent years have witnessed the success of introducing deep learning modelsto time series forecasting. From a data generation perspective, we illustratethat existing models are susceptible to distribution shifts driven by temporalcontexts, whether observed or unobserved. Such context-driven distributionshift (CDS) introduces biases in predictions within specific contexts and poseschallenges for conventional training paradigms. In this paper, we introduce auniversal calibration methodology for the detection and adaptation of CDS witha trained model. To this end, we propose a novel CDS detector, termed the"residual-based CDS detector" or "Reconditionor", which quantifies the model'svulnerability to CDS by evaluating the mutual information between predictionresiduals and their corresponding contexts. A high Reconditionor scoreindicates a severe susceptibility, thereby necessitating model adaptation. Inthis circumstance, we put forth a straightforward yet potent adapter frameworkfor model calibration, termed the "sample-level contextualized adapter" or"SOLID". This framework involves the curation of a contextually similar datasetto the provided test sample and the subsequent fine-tuning of the model'sprediction layer with a limited number of steps. Our theoretical analysisdemonstrates that this adaptation strategy can achieve an optimal bias-variancetrade-off. Notably, our proposed Reconditionor and SOLID are model-agnostic andreadily adaptable to a wide range of models. Extensive experiments show thatSOLID consistently enhances the performance of current forecasting models onreal-world datasets, especially on cases with substantial CDS detected by theproposed Reconditionor, thus validating the effectiveness of the calibrationapproach.|近年来，深度学习模型在时间序列预测中的应用取得了成功。从数据生成的角度，我们说明了现有的模型容易受到时间背景驱动的分布变化的影响，无论是观察到的还是未观察到的。这种上下文驱动的分布转移(CDS)在特定的上下文中引入了预测的偏差，并对传统的训练范式提出了挑战。本文介绍了一种基于训练模型的 CDS 检测与自适应通用标定方法。为此，我们提出了一种新的 CDS 检测器，称为“基于残差的 CDS 检测器”或“重构器”，它通过评估预测残差与其相应上下文之间的互信息来量化模型对 CDS 的脆弱性。一个高分表明一个严重的易感性，从而需要模型适应。在这种情况下，我们提出了一个简单而有效的适配器模型校准框架，称为“样本级上下文适配器”或“ SOLID”。这个框架涉及到与所提供的测试样本上下文相似的数据集的管理，以及随后用有限的步骤对模型的预测层进行微调。我们的理论分析表明，这种适应策略可以实现最佳的偏差-方差权衡。值得注意的是，我们提出的 Reconditionor 和 SOLID 是模型无关的，可以很容易地适应广泛的模型。大量的实验表明，SOLID 一致地提高了现有预测模型在真实世界数据集上的性能，特别是在具有大量 CDS 的情况下，由提出的重调器检测，从而验证了校准方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Calibration+of+Time-Series+Forecasting:+Detecting+and+Adapting+Context-Driven+Distribution+Shift)|0|
|[Co-Neighbor Encoding Schema: A Light-cost Structure Encoding Method for Dynamic Link Prediction](https://doi.org/10.1145/3637528.3671770)|Ke Cheng, Linzhi Peng, Junchen Ye, Leilei Sun, Bowen Du|; CCSE Lab, Beihang University, Beijing, China; School of Transportation Science and Engineering, Beihang University, Beijing, China|Structure encoding has proven to be the key feature to distinguishing links in a graph. However, Structure encoding in the temporal graph keeps changing as the graph evolves, repeatedly computing such features can be time-consuming due to the high-order subgraph construction. We develop the Co-Neighbor Encoding Schema (CNES) to address this issue. Instead of recomputing the feature by the link, CNES stores information in the memory to avoid redundant calculations. Besides, unlike the existing memory-based dynamic graph learning method that stores node hidden states, we introduce a hashtable-based memory to compress the adjacency matrix for efficient structure feature construction and updating with vector computation in parallel. Furthermore, CNES introduces a Temporal-Diverse Memory to generate long-term and short-term structure encoding for neighbors with different structural information. A dynamic graph learning framework, Co-Neighbor Encoding Network (CNE-N), is proposed using the aforementioned techniques. Extensive experiments on thirteen public datasets verify the effectiveness and efficiency of the proposed method.|结构编码已被证明是区分图中链接的关键特征。然而，时态图中的结构编码随着图的演化而不断变化，由于子图结构的高阶性，重复计算这些特征可能会耗费大量的时间。为了解决这个问题，我们开发了协同邻居编码模式(CNES)。CNES 不通过链路重新计算特性，而是将信息存储在内存中以避免冗余计算。此外，不像现有的基于内存的动态图学习方法存储节点隐藏状态，我们引入了一个基于哈希表的内存来压缩邻接矩阵，以便有效地结构特征构建和并行矢量计算更新。此外，CNES 还引入了时间多样性存储器，为具有不同结构信息的邻居生成长期和短期的结构编码。利用上述技术，提出了一种动态图学习框架——协同邻域编码网络(CNE-N)。在十三个公共数据集上的大量实验验证了该方法的有效性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Co-Neighbor+Encoding+Schema:+A+Light-cost+Structure+Encoding+Method+for+Dynamic+Link+Prediction)|0|
|[Resurrecting Label Propagation for Graphs with Heterophily and Label Noise](https://doi.org/10.1145/3637528.3671774)|Yao Cheng, Caihua Shan, Yifei Shen, Xiang Li, Siqiang Luo, Dongsheng Li|Microsoft Research Asia, Shanghai, China; Nanyang Technological University, Singapore, Singapore; East China Normal University, Shanghai, China|Label noise is a common challenge in large datasets, as it can significantly degrade the generalization ability of deep neural networks. Most existing studies focus on noisy labels in computer vision; however, graph models encompass both node features and graph topology as input, and become more susceptible to label noise through message-passing mechanisms. Recently, only a few works have been proposed to tackle the label noise on graphs. One significant limitation is that they operate under the assumption that the graph exhibits homophily and that the labels are distributed smoothly. However, real-world graphs can exhibit varying degrees of heterophily, or even be dominated by heterophily, which results in the inadequacy of the current methods. In this paper, we study graph label noise in the context of arbitrary heterophily, with the aim of rectifying noisy labels and assigning labels to previously unlabeled nodes. We begin by conducting two empirical analyses to explore the impact of graph homophily on graph label noise. Following observations, we propose a efficient algorithm, denoted as R2LP. Specifically, R2LP is an iterative algorithm with three steps: (1) reconstruct the graph to recover the homophily property, (2) utilize label propagation to rectify the noisy labels, (3) select high-confidence labels to retain for the next iteration. By iterating these steps, we obtain a set of ''correct'' labels, ultimately achieving high accuracy in the node classification task. The theoretical analysis is also provided to demonstrate its remarkable denoising effect. Finally, we perform experiments on ten benchmark datasets with different levels of graph heterophily and various types of noise. In these experiments, we compare the performance of R2LP against ten typical baseline methods. Our results illustrate the superior performance of the proposed øurs. The code and data of this paper can be accessed at: https://github.com/cy623/R2LP.git.|在大型数据集中，标记噪声是一个常见的问题，因为它会显著降低深层神经网络的泛化能力。现有的研究大多集中在计算机视觉中的噪声标签，然而，图模型既包含节点特征又包含图拓扑作为输入，并且通过消息传递机制更容易受到标签噪声的影响。近年来，针对图的标签噪声问题的研究很少。一个重要的限制是，它们是在假设图表现出同质性和标签平滑分布的情况下运行的。然而，现实世界中的图可能表现出不同程度的异质性，甚至被异质性所支配，这导致了现有方法的不足。本文研究了任意异质性情况下的图标签噪声，目的是校正噪声标签，并将标签分配给以前未标记的节点。我们首先通过两个实证分析来探讨图同调性对图标噪声的影响。经过观察，我们提出了一种有效的算法，称为 R2LP。具体来说，R2LP 算法是一种迭代算法，它包括三个步骤: (1)重构图来恢复同质性; (2)利用标签传播来校正噪声标签; (3)选择高置信度标签来保留下一次迭代。通过迭代这些步骤，我们得到一组“正确”的标签，最终实现节点分类任务的高精度。理论分析也证明了其显著的去噪效果。最后，我们在十个基准数据集上进行了实验，这些数据集具有不同层次的图异质性和不同类型的噪声。在这些实验中，我们比较了 R2LP 和10种典型的基线方法的性能。我们的结果说明了提议的 øurs 的优越性能。本文件的编码及资料可浏览以下 https://github.com/cy623/r2lp.git。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Resurrecting+Label+Propagation+for+Graphs+with+Heterophily+and+Label+Noise)|0|
|[DyGKT: Dynamic Graph Learning for Knowledge Tracing](https://doi.org/10.1145/3637528.3671773)|Ke Cheng, Linzhi Peng, Pengyang Wang, Junchen Ye, Leilei Sun, Bowen Du|SKL-IOTSC, Department of Computer and Information Science, University of Macau, Macau, China; School of Transportation Science and Engineering, Beihang University, Beijing, China; SKLSDE Lab, Beihang University, Beijing, China|Knowledge Tracing aims to assess student learning states by predicting their performance in answering questions. Different from the existing research which utilizes fixed-length learning sequence to obtain the student states and regards KT as a static problem, this work is motivated by three dynamical characteristics: 1) The scales of students answering records are constantly growing; 2) The semantics of time intervals between the records vary; 3) The relationships between students, questions and concepts are evolving. The three dynamical characteristics above contain the great potential to revolutionize the existing knowledge tracing methods. Along this line, we propose a Dynamic Graph-based Knowledge Tracing model, namely DyGKT. In particular, a continuous-time dynamic question-answering graph for knowledge tracing is constructed to deal with the infinitely growing answering behaviors, and it is worth mentioning that it is the first time dynamic graph learning technology is used in this field. Then, a dual time encoder is proposed to capture long-term and short-term semantics among the different time intervals. Finally, a multiset indicator is utilized to model the evolving relationships between students, questions, and concepts via the graph structural feature. Numerous experiments are conducted on five real-world datasets, and the results demonstrate the superiority of our model. All the used resources are publicly available at https://github.com/PengLinzhi/DyGKT.|知识追踪旨在通过预测学生在回答问题时的表现来评估学生的学习状态。与现有的利用固定长度的学习序列获取学生状态并将 KT 视为静态问题的研究不同，本研究的动力来源于三个动态特征: 1)学生回答记录的规模在不断增长; 2)记录之间的时间间隔语义在不断变化; 3)学生、问题和概念之间的关系在不断演化。上述三个动态特征具有革新现有知识追踪方法的巨大潜力。在此基础上，我们提出了一个基于动态图的知识跟踪模型，即 DyGKT。特别是构造了一个用于知识跟踪的连续时间动态问答图来处理无限增长的应答行为，值得一提的是，这是动态图学习技术首次应用于该领域。然后，提出了一种双时间编码器来捕获不同时间间隔之间的长期和短期语义。最后，利用多集指标模型，通过图形结构特征对学生、问题和概念之间的演化关系进行建模。在五个实际数据集上进行了大量的实验，实验结果表明了该模型的优越性。所有使用过的资源都可以在 https://github.com/penglinzhi/dygkt 上公开获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DyGKT:+Dynamic+Graph+Learning+for+Knowledge+Tracing)|0|
|[Conformal Counterfactual Inference under Hidden Confounding](https://doi.org/10.1145/3637528.3671976)|Zonghao Chen, Ruocheng Guo, JeanFrancois Ton, Yang Liu|University College London, London, United Kingdom; Bytedance Research, San Jose, USA; Bytedance Research, London, United Kingdom|Personalized decision making requires the knowledge of potential outcomes under different treatments, and confidence intervals about the potential outcomes further enrich this decision-making process and improve its reliability in high-stakes scenarios. Predicting potential outcomes along with its uncertainty in a counterfactual world poses the foundamental challenge in causal inference. Existing methods that construct confidence intervals for counterfactuals either rely on the assumption of strong ignorability that completely ignores hidden confounders, or need access to un-identifiable lower and upper bounds that characterize the difference between observational and interventional distributions. In this paper, to overcome these limitations, we first propose a novel approach wTCP-DR based on transductive weighted conformal prediction, which provides confidence intervals for counterfactual outcomes with marginal converage guarantees, even under hidden confounding. With less restrictive assumptions, our approach requires access to a fraction of interventional data (from randomized controlled trials) to account for the covariate shift from observational distributoin to interventional distribution. Theoretical results explicitly demonstrate the conditions under which our algorithm is strictly advantageous to the naive method that only uses interventional data. Since transductive conformal prediction is notoriously costly, we propose wSCP-DR, a two-stage variant of wTCP-DR, based on split conformal prediction with same marginal coverage guarantees but at a significantly lower computational cost. After ensuring valid intervals on counterfactuals, it is straightforward to construct intervals for individual treatment effects (ITEs). We demonstrate our method across synthetic and real-world data, including recommendation systems, to verify the superiority of our methods compared against state-of-the-art baselines in terms of both coverage and efficiency. Our code can be found at https://github.com/rguo12/KDD24-Conformal.|个性化决策需要了解不同治疗方案下的潜在结果，关于潜在结果的置信区间进一步丰富了这一决策过程，并提高了其在高风险情景下的可靠性。在一个反事实的世界中，预测潜在的结果及其不确定性构成了因果推理的基本挑战。为反事实构建置信区间的现有方法要么依赖于完全忽略隐藏混杂因素的强烈可忽略性的假设，要么需要访问表征观察和干预分布之间差异的不可识别的下限和上限。为了克服这些局限性，本文首先提出了一种基于传导加权保角预测的新方法 wTCP-DR，该方法为具有边际收敛保证的反事实结果提供置信区间，即使在隐藏混杂情况下也是如此。在限制性较少的假设下，我们的方法需要获得一小部分介入数据(来自随机对照试验) ，以解释从观察分布到介入分布的协变量转移。理论分析结果明确地说明了在什么条件下我们的算法比只使用介入数据的朴素方法具有严格的优势。由于传导共形预测的代价是众所周知的，我们提出了 wSCP-DR，一个两阶段的 wTCP-DR 的变体，基于分裂共形预测具有相同的边际覆盖保证，但计算成本显著降低。在确保反事实的有效间隔之后，就可以直接构建单个治疗效果(ITE)的间隔。我们通过合成和现实世界的数据(包括推荐系统)演示我们的方法，以验证我们的方法在覆盖面和效率方面与最先进的基线相比的优越性。我们的代码可以在 https://github.com/rguo12/kdd24-conformal 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Conformal+Counterfactual+Inference+under+Hidden+Confounding)|0|
|[Leveraging Pedagogical Theories to Understand Student Learning Process with Graph-based Reasonable Knowledge Tracing](https://doi.org/10.1145/3637528.3671853)|Jiajun Cui, Hong Qian, Bo Jiang, Wei Zhang|East China Normal University, Shanghai, China|Knowledge tracing (KT) is a crucial task in intelligent education, focusingon predicting students' performance on given questions to trace their evolvingknowledge. The advancement of deep learning in this field has led todeep-learning knowledge tracing (DLKT) models that prioritize high predictiveaccuracy. However, many existing DLKT methods overlook the fundamental goal oftracking students' dynamical knowledge mastery. These models do not explicitlymodel knowledge mastery tracing processes or yield unreasonable results thateducators find difficulty to comprehend and apply in real teaching scenarios.In response, our research conducts a preliminary analysis of mainstream KTapproaches to highlight and explain such unreasonableness. We introduce GRKT, agraph-based reasonable knowledge tracing method to address these issues. Byleveraging graph neural networks, our approach delves into the mutualinfluences of knowledge concepts, offering a more accurate representation ofhow the knowledge mastery evolves throughout the learning process.Additionally, we propose a fine-grained and psychological three-stage modelingprocess as knowledge retrieval, memory strengthening, and knowledgelearning/forgetting, to conduct a more reasonable knowledge tracing process.Comprehensive experiments demonstrate that GRKT outperforms eleven baselinesacross three datasets, not only enhancing predictive accuracy but alsogenerating more reasonable knowledge tracing results. This makes our model apromising advancement for practical implementation in educational settings. Thesource code is available at https://github.com/JJCui96/GRKT.|知识追踪(KT)是智力教育中的一项重要任务，其重点是预测学生在给定问题上的表现，从而追踪他们的知识演化过程。深度学习在这个领域的进步导致了深度学习知识跟踪(DLKT)模型优先考虑高预测精度。然而，现有的 DLKT 方法忽视了跟踪学生动态知识掌握的基本目标。这些模型没有明确地模拟知识掌握追踪过程，也没有产生教育者难以理解和应用于真实教学情景的不合理结果。作为回应，我们的研究对主流 KT 方法进行了初步分析，以突出和解释这种不合理性。为了解决这些问题，我们引入了基于 GRKT、基于图的合理知识追踪方法。通过利用图形神经网络，我们的方法深入研究了知识概念的相互影响，提供了知识掌握如何在整个学习过程中发展的更准确的表示。此外，我们提出了一个细粒度的心理三阶段建模过程，即知识检索、记忆增强和知识学习/遗忘，以进行更合理的知识追踪过程。综合实验表明，GRKT 在三个数据集中优于十一个基线，不仅提高了预测的准确性，而且产生了更合理的知识跟踪结果。这使得我们的模型在教育环境中的实际应用有了很大的进步。源代码可在 https://github.com/jjcui96/grkt 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leveraging+Pedagogical+Theories+to+Understand+Student+Learning+Process+with+Graph-based+Reasonable+Knowledge+Tracing)|0|
|[Iterative Weak Learnability and Multiclass AdaBoost](https://doi.org/10.1145/3637528.3671842)|InKoo Cho, Jonathan A. Libgober, Cheng Ding|University of Southern California, Los Angeles, CA, USA; Emory University, ATLANTA, GA, USA; Emory University & Hanyang University, Atlanta, GA, USA|We propose an efficient boosting algorithm for multiclass classification, called AdaBoost.Iter, that extends SAMME and AdaBoost. The algorithm iteratively applies the weak learnability condition of SAMME to eliminate classes to find the correct classificiation. The iterative weak learnability is a sufficient and necessary condition for boostability, but it is also easier to validate than the EOR criterion of AdaBoost.MM \citeMukherjeeSchapire2013. We show that the training error of AdaBoost.Iter vanishes at the exponential rate, while the generalization error converges to zero at the same rate as AdaBoost. AdaBoost.Iter numerically outperforms SAMME and achieves performance comparable to AdaBoost.MM on benchmark datasets.|我们提出了一个有效的多元分类增强算法，叫做 AdaBoost。它扩展了 SAMME 和 AdaBoost。该算法迭代地利用 SAMME 的弱可学性条件来消除类，从而找到正确的分类。迭代的弱可学性是 Booability 的充分和必要条件，但是它也比 AdaBoost.MM citeMukherjeeSchapire2013的 EOR 标准更容易验证。我们展示了 AdaBoost 的训练错误。Iter 以指数级的速度消失，而泛化误差收敛到零的速度与 AdaBoost 相同。AdaBoost.Iter 在数值上优于 SAMME，并且在基准数据集上实现了与 AdaBoost.MM 相当的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Iterative+Weak+Learnability+and+Multiclass+AdaBoost)|0|
|[Divide and Denoise: Empowering Simple Models for Robust Semi-Supervised Node Classification against Label Noise](https://doi.org/10.1145/3637528.3671798)|Kaize Ding, Xiaoxiao Ma, Yixin Liu, Shirui Pan|Griffith University, Gold Coast, Australia; Northwestern University, Evanston, USA; Macquarie University, Sydney, Australia; Monash University, Melbourne, Australia|Graph neural networks (GNNs) based on message passing have achieved remarkable performance in graph machine learning. By combining it with the power of pseudo labeling, one can further push forward the performance on the task of semi-supervised node classification. However, most existing works assume that the training node labels are purely noise-free, while this strong assumption usually does not hold in practice. GNNs will overfit the noisy training labels and the adverse effects of mislabeled nodes can be exaggerated by being propagated to the remaining nodes through the graph structure, exacerbating the model failure. Worse still, the noisy pseudo labels could also largely undermine the model's reliability without special treatment. In this paper, we revisit the role of (1) message passing and (2) pseudo labels in the studied problem and try to address two denoising subproblems from the model architecture and algorithm perspective, respectively. Specifically, we first develop a label-noise robust GNN that discards the coupled message-passing scheme. Despite its simple architecture, this learning backbone prevents overfitting to noisy labels and also inherently avoids the noise propagation issue. Moreover, we propose a novel reliable graph pseudo labeling algorithm that can effectively leverage the knowledge of unlabeled nodes while mitigating the adverse effects of noisy pseudo labels. Based on those novel designs, we can attain exceptional effectiveness and efficiency in solving the studied problem. We conduct extensive experiments on benchmark datasets for semi-supervised node classification with different levels of label noise and show new state-of-the-art performance. The code is available at https://github.com/DND-NET/DND-NET.|基于消息传递的图神经网络在图机学习中取得了显著的效果。结合伪标记的能力，可以进一步提高半监督节点分类的性能。然而，现有的大多数工作假设训练节点标签是纯粹无噪声的，而这种强烈的假设通常不适用于实践。该算法通过图结构将误标节点传播到剩余的节点，从而加剧了模型的失效。更糟糕的是，嘈杂的伪标签也可能在很大程度上破坏模型的可靠性，而无需特殊处理。本文重新审视了(1)消息传递和(2)伪标签在所研究问题中的作用，并试图从模型结构和算法角度分别解决两个去噪子问题。具体来说，我们首先开发了一个标签噪声鲁棒 GNN，它丢弃了耦合消息传递方案。尽管它的结构简单，这种学习骨干防止过度拟合噪声标签，也内在地避免噪声传播的问题。此外，我们提出了一种新的可靠的图伪标记算法，可以有效地利用未标记节点的知识，同时减轻噪声伪标记的不利影响。基于这些新颖的设计，我们可以在解决所研究的问题中获得非常有效和高效的结果。针对不同标签噪声水平的半监督节点分类问题，我们对基准数据集进行了广泛的实验研究，并展示了新的性能。密码可在 https://github.com/dnd-net/dnd-net 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Divide+and+Denoise:+Empowering+Simple+Models+for+Robust+Semi-Supervised+Node+Classification+against+Label+Noise)|0|
|[Unraveling Block Maxima Forecasting Models with Counterfactual Explanation](https://doi.org/10.1145/3637528.3671923)|Yue Deng, Asadullah Hill Galib, PangNing Tan, Lifeng Luo|Michigan State University, East Lansing, MI, USA|Disease surveillance, traffic management, and weather forecasting are some of the key applications that could benefit from block maxima forecasting of a time series as the extreme block maxima values often signify events of critical importance such as disease outbreaks, traffic gridlock, and severe weather conditions. As the use of deep neural network models for block maxima forecasting increases, so does the need for explainable AI methods that could unravel the inner workings of such black box models. To fill this need, this paper presents a novel counterfactual explanation framework for block maxima forecasting models. Unlike existing methods, our proposed framework, DiffusionCF, combines deep anomaly detection with a conditional diffusion model to identify unusual patterns in the time series that could help explain the forecasted extreme block maxima. Experimental results on several real-world datasets demonstrate the superiority of DiffusionCF over other baseline methods when evaluated according to various metrics, particularly their informativeness and closeness. Our data and codes are available at https://github.com/yue2023cs/DiffusionCF.|疾病监测、交通管理和天气预报是一些可以从时间序列的块极大值预测中受益的关键应用程序，因为块极大值通常意味着疾病爆发、交通堵塞和恶劣天气状况等至关重要的事件。随着深度神经网络模型在块极大值预测中的应用越来越多，解释人工智能方法的需求也越来越大，这些方法可以揭示这些黑箱模型的内部工作原理。为了满足这一需求，本文提出了一种新的块极大值预测模型的反事实解释框架。与现有方法不同的是，我们提出的扩散异常检测框架结合了深度扩散模型和条件扩散模型来识别时间序列中不寻常的模式，这有助于解释预测的极端块极大值。在几个实际数据集上的实验结果表明，当根据各种指标，特别是它们的信息量和接近度进行评估时，弥散 CF 优于其他基线方法。我们的数据和代码可以在 https://github.com/yue2023cs/diffusioncf 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unraveling+Block+Maxima+Forecasting+Models+with+Counterfactual+Explanation)|0|
|[Explanatory Model Monitoring to Understand the Effects of Feature Shifts on Performance](https://doi.org/10.1145/3637528.3671959)|Thomas Decker, Alexander Koebler, Michael Lebacher, Ingo Thon, Volker Tresp, Florian Buettner|Siemens AG, Munich, Germany; Goethe University Frankfurt & Siemens AG, Frankfurt, Germany; Ludwig-Maximilians-Universität & Siemens AG, Munich, Germany; Ludwig-Maximilians-Universität & Munich Center for Machine Learning, Munich, Germany|Monitoring and maintaining machine learning models are among the most critical challenges in translating recent advances in the field into real-world applications. However, current monitoring methods lack the capability of provide actionable insights answering the question of why the performance of a particular model really degraded. In this work, we propose a novel approach to explain the behavior of a black-box model under feature shifts by attributing an estimated performance change to interpretable input characteristics. We refer to our method that combines concepts from Optimal Transport and Shapley Values as Explanatory Performance Estimation (XPE). We analyze the underlying assumptions and demonstrate the superiority of our approach over several baselines on different data sets across various data modalities such as images, audio, and tabular data. We also indicate how the generated results can lead to valuable insights, enabling explanatory model monitoring by revealing potential root causes for model deterioration and guiding toward actionable countermeasures.|监测和维护机器学习模型是将该领域最新进展转化为现实应用的最关键挑战之一。然而，当前的监控方法缺乏提供可操作的洞察力来回答为什么特定模型的性能真正下降的问题。在这项工作中，我们提出了一个新颖的方法来解释黑盒模型的行为在特征移位，通过归因于一个估计的性能变化对可解释的输入特征。我们将结合最优运输和 Shapley 值的概念的方法称为解释性能估计(XPE)。我们分析了基本的假设，并证明了我们的方法在不同数据集的基线上的优越性，这些基线跨越了不同的数据模式，如图像、音频和表格数据。我们还指出，生成的结果如何能够产生有价值的见解，通过揭示模型恶化的潜在根源和指导可行的对策来实现解释性模型监测。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explanatory+Model+Monitoring+to+Understand+the+Effects+of+Feature+Shifts+on+Performance)|0|
|[Fast Unsupervised Deep Outlier Model Selection with Hypernetworks](https://doi.org/10.1145/3637528.3672003)|Xueying Ding, Yue Zhao, Leman Akoglu|University of Southern California, Los Angeles, CA, USA; Carnegie Mellon University, Pittsburgh, PA, USA|Deep neural network based Outlier Detection (DOD) has seen a recent surge of attention thanks to the many advances in deep learning. In this paper, we consider a critical-yet-understudied challenge with unsupervised DOD, that is, effective hyperparameter (HP) tuning/model selection. While several prior work report the sensitivity of OD models to HP settings, the issue is ever so critical for the modern DOD models that exhibit a long list of HPs. We introduce HYPER for tuning DOD models, tackling two fundamental challenges: (1) validation without supervision (due to lack of labeled outliers), and (2) efficient search of the HP/model space (due to exponential growth in the number of HPs). A key idea is to design and train a novel hypernetwork (HN) that maps HPs onto optimal weights of the main DOD model. In turn, HYPER capitalizes on a single HN that can dynamically generate weights for many DOD models (corresponding to varying HPs), which offers significant speed-up. In addition, it employs meta-learning on historical OD tasks with labels to train a proxy validation function, likewise trained with our proposed HN efficiently. Extensive experiments on different OD tasks show that HYPER achieves competitive performance against 8 baselines with significant efficiency gains.|由于深度学习的许多进步，基于深度神经网络的异常检测(DOD)最近受到了广泛的关注。在本文中，我们考虑了一个非监督 DOD 的关键但未被充分研究的挑战，即有效的超参数(HP)调整/模型选择。虽然之前的一些工作报告了 OD 模型对 HP 设置的敏感性，但是这个问题对于现代 DOD 模型来说是非常关键的，因为它展示了一长串 HP 的列表。我们引入 HYPER 来调整国防部模型，解决两个基本的挑战: (1)没有监督的验证(由于缺乏标记的异常值)和(2)高效搜索惠普/模型空间(由于惠普数量的指数增长)。其中的一个关键思想是设计和训练一种新的超网络(HN) ，将 HP 映射到主要 DOD 模型的最优权值上。反过来，HYPER 利用一个单一的 HN，可以动态地为许多 DOD 模型生成权重(对应于不同的 HP) ，从而提供显著的加速。此外，它使用元学习的历史 OD 任务与标签训练的代理验证功能，同样训练了我们提出的 HN 有效。对不同 OD 任务的大量实验表明，HYPER 在8个基线上达到了具有竞争力的性能，并且有显著的效率提高。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+Unsupervised+Deep+Outlier+Model+Selection+with+Hypernetworks)|0|
|[Enhancing On-Device LLM Inference with Historical Cloud-Based LLM Interactions](https://doi.org/10.1145/3637528.3671679)|Yucheng Ding, Chaoyue Niu, Fan Wu, Shaojie Tang, Chengfei Lyu, Guihai Chen|Alibaba Group, Hangzhou, China; University of Texas at Dallas, Richardson, Texas, USA; Shanghai Jiao Tong University, Shanghai, China|Many billion-scale large language models (LLMs) have been released for resource-constraint mobile devices to provide local LLM inference service when cloud-based powerful LLMs are not available. However, the capabilities of current on-device LLMs still lag behind those of cloud-based LLMs, and how to effectively and efficiently enhance on-device LLM inference becomes a practical requirement. We thus propose to collect the user's historical interactions with the cloud-based LLM and build an external datastore on the mobile device for enhancement using nearest neighbors search. Nevertheless, the full datastore improves the quality of token generation at the unacceptable expense of much slower generation speed. To balance performance and efficiency, we propose to select an optimal subset of the full datastore within the given size limit, the optimization objective of which is proven to be submodular. We further design an offline algorithm, which selects the subset after the construction of the full datastore, as well as an online algorithm, which performs selection over the stream and can be flexibly scheduled. We theoretically analyze the performance guarantee and the time complexity of the offline and the online designs to demonstrate effectiveness and scalability. We finally take three ChatGPT related dialogue datasets and four different on-device LLMs for evaluation. Evaluation results show that the proposed designs significantly enhance LLM performance in terms of perplexity while maintaining fast token generation speed. Practical overhead testing on the smartphone reveal the efficiency of on-device datastore subset selection from memory usage and computation overhead.|当基于云的强大的 LLM 不可用时，为资源受限的移动设备发布了许多数十亿级的大语言模型(LLM) ，以提供本地 LLM 推理服务。然而，当前设备上 LLM 的性能仍然落后于基于云的 LLM，如何有效地增强设备上 LLM 的推理成为一个实际的需求。因此，我们建议收集用户与基于云的 LLM 的历史交互，并在移动设备上建立一个外部数据存储，使用最近邻搜索进行增强。然而，完整的数据存储提高了令牌生成的质量，代价是生成速度大大降低，这是不可接受的。为了平衡性能和效率，我们提出在给定的大小限制内选择一个完整数据存储的最优子集，其优化目标被证明是子模块化的。我们进一步设计了一个离线算法，在构造完整的数据存储之后选择子集，以及一个在线算法，在流上执行选择，可以灵活调度。我们从理论上分析了离线和在线设计的性能保证和时间复杂度，以验证设计的有效性和可扩展性。最后，我们采用三个 ChatGPT 相关的对话数据集和四个不同的设备上 LLM 进行评估。评估结果表明，该设计在保持快速令牌生成速度的同时，显著提高了 LLM 的性能。智能手机上的实际开销测试揭示了从内存使用和计算开销中选择设备上数据存储子集的效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+On-Device+LLM+Inference+with+Historical+Cloud-Based+LLM+Interactions)|0|
|[IDEA: A Flexible Framework of Certified Unlearning for Graph Neural Networks](https://doi.org/10.1145/3637528.3671744)|Yushun Dong, Binchi Zhang, Zhenyu Lei, Na Zou, Jundong Li|The University of Houston, Houston, TX, USA; The University of Virginia, Charlottesville, VA, USA; The University of Virginia, Charlottesville, USA|Graph Neural Networks (GNNs) have been increasingly deployed in a plethora of applications. However, the graph data used for training may contain sensitive personal information of the involved individuals. Once trained, GNNs typically encode such information in their learnable parameters. As a consequence, privacy leakage may happen when the trained GNNs are deployed and exposed to potential attackers. Facing such a threat, machine unlearning for GNNs has become an emerging technique that aims to remove certain personal information from a trained GNN. Among these techniques, certified unlearning stands out, as it provides a solid theoretical guarantee of the information removal effectiveness. Nevertheless, most of the existing certified unlearning methods for GNNs are only designed to handle node and edge unlearning requests. Meanwhile, these approaches are usually tailored for either a specific design of GNN or a specially designed training objective. These disadvantages significantly jeopardize their flexibility. In this paper, we propose a principled framework named IDEA to achieve flexible and certified unlearning for GNNs. Specifically, we first instantiate four types of unlearning requests on graphs, and then we propose an approximation approach to flexibly handle these unlearning requests over diverse GNNs. We further provide theoretical guarantee of the effectiveness for the proposed approach as a certification. Different from existing alternatives, IDEA is not designed for any specific GNNs or optimization objectives to perform certified unlearning, and thus can be easily generalized. Extensive experiments on real-world datasets demonstrate the superiority of IDEA in multiple key perspectives.|图形神经网络(GNN)已经越来越多地部署在过多的应用。然而，用于培训的图形数据可能包含涉及个人的敏感个人信息。一旦经过训练，GNN 通常会在其可学习的参数中对这些信息进行编码。因此，当经过训练的 GNN 被部署并暴露给潜在的攻击者时，隐私泄漏就可能发生。面对这样的威胁，GNN 的机器学习已经成为一种新兴的技术，旨在删除某些个人信息从训练的 GNN。在这些技术中，认证忘却技术脱颖而出，因为它为信息去除的有效性提供了坚实的理论保证。然而，大多数现有的认证的 GNN 去学习方法仅用于处理节点和边的去学习请求。同时，这些方法通常是为特定的 GNN 设计或特定的培训目标而量身定制的。这些缺点严重损害了它们的灵活性。在本文中，我们提出了一个原则性的框架 IDEA 来实现灵活和认证的 GNN 去学习。具体来说，我们首先在图上实例化了四种类型的忘记请求，然后我们提出了一种近似方法来灵活地处理不同 GNN 上的这些忘记请求。我们进一步提供理论保证的有效性，为提出的方法作为一种证明。与现有的替代方案不同，IDEA 不是为任何特定的 GNN 或优化目标设计的，以执行认证的无学习，因此可以很容易地推广。在实际数据集上的大量实验证明了 IDEA 在多个关键方面的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IDEA:+A+Flexible+Framework+of+Certified+Unlearning+for+Graph+Neural+Networks)|0|
|[Unsupervised Alignment of Hypergraphs with Different Scales](https://doi.org/10.1145/3637528.3671955)|Manh Tuan Do, Kijung Shin|KAIST, Seoul, Republic of Korea|People usually interact in groups, and such groups may appear on different platforms. For instance, people often create various group chats on messaging apps (e.g., Facebook Messenger and WhatsApp) to communicate with families, friends, or colleagues. How do we identify the same people across the two platforms based on the information about the groups? This gives rise to the hypergraph alignment problem, whose objective is to find the correspondences between the sets of nodes of two hypergraphs. In a hypergraph, a node represents a person, and each hyperedge represents a group of several people. In addition, the two sets of hyperedges in the two hypergraphs can vary significantly in scales as people may use different apps at different time periods. In this work, we propose and tackle the problem of unsupervised hypergraph alignment. Given two hypergraphs with potentially different scales and without any side information or prior ground-truth correspondences, we develop ØurMethod, a learning framework, to find node correspondences across the two hypergraphs. ØurMethod directly addresses each challenge of the problem. In particular, it (a) extracts node features from the hypergraph topology, (b) employs contrastive learning, as a "supervised pseudo-alignment'' task to pre-train the learning model (c) applies topological augmentation to help a generative adversarial network to align the two embedding spaces from the two hypergraphs. The purpose of augmentation is to add virtual hyperedges from one hypergraph in order to the other to resolve the scale difference and share information across the two hypergraphs. Our extensive experiments on 12 real-world datasets demonstrate the significant and consistent superiority of ØurMethod over the baseline approaches.|人们通常以群体的形式互动，这样的群体可能出现在不同的平台上。例如，人们经常在消息应用程序(如 Facebook Messenger 和 WhatsApp)上创建各种群聊，以便与家人、朋友或同事交流。我们如何根据组的信息在两个平台上识别相同的人？这就产生了超图对齐问题，其目标是找到两个超图的节点集之间的对应关系。在超图中，一个节点代表一个人，每个超边代表一组几个人。此外，两个超图中的两组超边缘可能在尺度上有显著差异，因为人们可能在不同的时间段使用不同的应用程序。本文提出并解决了无监督超图对齐问题。给定两个具有潜在不同尺度的超图，在没有任何侧面信息或先验地面真相对应的情况下，我们开发了一个学习框架 ØurMethod，用于在两个超图之间寻找节点对应。ØurMethod 直接解决问题的每一个挑战。特别地，它(a)从超图拓扑中提取节点特征，(b)使用对比学习，作为一个“监督伪对齐”任务来预训练学习模型(c)应用拓扑增强来帮助一个生成的对抗网络来对齐两个超图的嵌入空间。增广的目的是从一个超图中添加虚拟超边，以便解决两个超图之间的尺度差异和共享信息。我们在12个真实世界数据集上的广泛实验证明了 ØurMethod 相对于基线方法的显著和一致的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unsupervised+Alignment+of+Hypergraphs+with+Different+Scales)|0|
|[Heterogeneity-Informed Meta-Parameter Learning for Spatiotemporal Time Series Forecasting](https://doi.org/10.1145/3637528.3671961)|Zheng Dong, Renhe Jiang, Haotian Gao, Hangchen Liu, Jinliang Deng, Qingsong Wen, Xuan Song|Squirrel AI, Seattle, USA; The University of Tokyo, Tokyo, Japan; Jilin University & Southern University of Science and Technology, Changchun, China; Southern University of Science and Technology, Shenzhen, China; Hong Kong University of Science and Technology, Hong Kong, China|Spatiotemporal time series forecasting plays a key role in a wide range of real-world applications. While significant progress has been made in this area, fully capturing and leveraging spatiotemporal heterogeneity remains a fundamental challenge. Therefore, we propose a novel Heterogeneity-Informed Meta-Parameter Learning scheme. Specifically, our approach implicitly captures spatiotemporal heterogeneity through learning spatial and temporal embeddings, which can be viewed as a clustering process. Then, a novel spatiotemporal meta-parameter learning paradigm is proposed to learn spatiotemporal-specific parameters from meta-parameter pools, which is informed by the captured heterogeneity. Based on these ideas, we develop a Heterogeneity-Informed Spatiotemporal Meta-Network (HimNet) for spatiotemporal time series forecasting. Extensive experiments on five widely-used benchmarks demonstrate our method achieves state-of-the-art performance while exhibiting superior interpretability. Our code is available at https://github.com/XDZhelheim/HimNet.|时空时间序列预测在广泛的实际应用中起着关键的作用。虽然在这一领域取得了重大进展，但充分捕捉和利用时空异质性仍然是一个根本性挑战。因此，我们提出了一种新的异质性信息元参数学习方案。具体来说，我们的方法通过学习空间和时间嵌入隐含地捕获时空异质性，这可以看作是一个聚类过程。然后，提出了一种新的时空元参数学习范式，用于从元参数池中学习时空特定的参数。基于这些思想，我们开发了一个异质性信息时空元网络(HimNet)用于时空时间序列预测。在五个广泛使用的基准测试上的大量实验表明，我们的方法实现了最先进的性能，同时显示出优越的可解释性。我们的代码可以在 https://github.com/xdzhelheim/himnet 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Heterogeneity-Informed+Meta-Parameter+Learning+for+Spatiotemporal+Time+Series+Forecasting)|0|
|[Representation Learning of Temporal Graphs with Structural Roles](https://doi.org/10.1145/3637528.3671854)|Huaming Du, Long Shi, Xingyan Chen, Yu Zhao, Hegui Zhang, Carl Yang, Fuzhen Zhuang, Gang Kou|; Department of Computer Science, Emory University, Atlanta, Georgia, USA|Temporal graph representation learning has drawn considerable attention in recent years. Most existing works mainly focus on modeling local structural dependencies of temporal graphs. However, underestimating the inherent global structural role information in many real-world temporal graphs inevitably leads to sub-optimal graph representations. To overcome this shortcoming, we propose a novel Role-based Temporal Graph Convolution Network (RTGCN) that fully leverages the global structural role information in temporal graphs. Specifically, RTGCN can effectively capture the static global structural roles by using hypergraph convolution neural networks. To capture the evolution of nodes' structural roles, we further design structural role-based gated recurrent units. Finally, we integrate structural role proximity in our objective function to preserve global structural similarity, further promoting temporal graph representation learning. Experimental results on multiple real-world datasets demonstrate that RTGCN consistently outperforms state-of-the-art temporal graph representation learning methods by significant margins in various temporal link prediction and node classification tasks. Specifically, RTGCN achieves AUC improvement of up to 5.1% for link prediction and F1 improvement of up to 6.2% for new link prediction. In addition, RTGCN achieves AUC improvement up to 4.6% for node classification and 2.7% for structural role classification.|近年来，时间图表示学习引起了人们的广泛关注。现有的工作主要集中在时态图的局部结构依赖建模上。然而，在许多现实世界的时间图中，低估固有的全局结构角色信息不可避免地导致图表示的次优化。为了克服这一缺点，我们提出了一种新的基于角色的时态图卷积网络(RTGCN) ，充分利用时态图中的全局结构角色信息。具体来说，RTGCN 可以利用超图卷积神经网络有效地捕获静态全局结构角色。为了捕捉节点结构角色的演化过程，我们进一步设计了基于结构角色的门控递归单元。最后，我们在目标函数中整合结构角色接近性，以保持全局结构相似性，进一步促进时间图表示学习。在多个实际数据集上的实验结果表明，在各种时间链路预测和节点分类任务中，RTGCN 的表现优于最新的时间图表示学习方法。具体来说，RTGCN 在链路预测方面实现了高达5.1% 的 AUC 改进，在新链路预测方面实现了高达6.2% 的 F1改进。此外，RTGCN 在节点分类和结构角色分类方面的 AUC 改进分别达到4.6% 和2.7% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Representation+Learning+of+Temporal+Graphs+with+Structural+Roles)|0|
|[Reserving-Masking-Reconstruction Model for Self-Supervised Heterogeneous Graph Representation](https://doi.org/10.1145/3637528.3671719)|Haoran Duan, Cheng Xie, Linyu Li|Yunnan University, Kunming, China|Self-supervised Heterogeneous Graph Representation (SSHGRL) learning is widely used in data mining. The latest SSHGRL methods normally use metapaths to describe the heterogeneous information (multiple relations and node types) to learn the heterogeneous graph representation and achieve impressive results. However, establishing metapaths requires lofty computational costs that are too high for the medium and large graphs. To this end, this paper proposes a Reserving-Masking-Reconstruction (RMR) model that can fully consider heterogeneous information without relying on the metapaths. In detail, we propose a reserving method to reserve to-be-masked nodes' (target nodes) information before graph masking. Second, we split the reserved graph into relation subgraphs according to the type of relations that require much less computational overheads than metapath. Then, the target nodes in each relation subgraph are randomly masked with minimal topology information loss. After, a novel reconstruction method is proposed to reconstruct the masked nodes on different relation subgraphs to establish the self-supervised signal. The proposed method requires low computational complexity and can establish a self-supervised signal without deeply changing the graph topology. Experimental results show the proposed method achieves state-of-the-art records on medium and large-scale heterogeneous graphs and competitive records on small-scale heterogeneous graphs. The code is available at https://github.com/DuanhaoranCC/RMR.|自监督异构图表示(SSHGRL)学习在数据挖掘中有着广泛的应用。最新的 SSHGRL 方法通常使用元路径来描述异构信息(多个关系和节点类型) ，以学习异构图表示并取得令人印象深刻的结果。然而，建立元路径需要高昂的计算成本，这对于中型和大型图来说太高了。为此，本文提出了一种不依赖元路径而能够充分考虑异构信息的保留-掩蔽-重构(RMR)模型。详细地，我们提出了一种在图掩蔽之前保留待掩蔽节点(目标节点)信息的方法。其次，我们根据比元路径需要更少的计算开销的关系类型将保留图划分为关系子图。然后对每个关系子图中的目标节点进行最小拓扑信息丢失的随机掩蔽。然后，提出了一种新的重构方法来重构不同关系子图上的掩蔽节点，以建立自监督信号。该方法计算复杂度低，能够在不改变图形拓扑结构的情况下建立自监督信号。实验结果表明，该方法实现了中大规模异构图的最新记录和小规模异构图的竞争记录。密码可在 https://github.com/duanhaorancc/rmr 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reserving-Masking-Reconstruction+Model+for+Self-Supervised+Heterogeneous+Graph+Representation)|0|
|[Pre-Training Identification of Graph Winning Tickets in Adaptive Spatial-Temporal Graph Neural Networks](https://doi.org/10.1145/3637528.3671912)|Wenying Duan, Tianxiang Fang, Hong Rao, Xiaoxi He|University of Macau; Nanchang University|In this paper, we present a novel method to significantly enhance the computational efficiency of Adaptive Spatial-Temporal Graph Neural Networks (ASTGNNs) by introducing the concept of the Graph Winning Ticket (GWT), derived from the Lottery Ticket Hypothesis (LTH). By adopting a pre-determined star topology as a GWT prior to training, we balance edge reduction with efficient information propagation, reducing computational demands while maintaining high model performance. Both the time and memory computational complexity of generating adaptive spatial-temporal graphs is significantly reduced from O(N2) to O(N). Our approach streamlines the ASTGNN deployment by eliminating the need for exhaustive training, pruning, and retraining cycles, and demonstrates empirically across various datasets that it is possible to achieve comparable performance to full models with substantially lower computational costs. Specifically, our approach enables training ASTGNNs on the largest scale spatial-temporal dataset using a single A6000 equipped with 48 GB of memory, overcoming the out-of-memory issue encountered during original training and even achieving state-of-the-art performance. Furthermore, we delve into the effectiveness of the GWT from the perspective of spectral graph theory, providing substantial theoretical support. This advancement not only proves the existence of efficient sub-networks within ASTGNNs but also broadens the applicability of the LTH in resource-constrained settings, marking a significant step forward in the field of graph neural networks. Code is available at https://anonymous.4open.science/r/paper-1430.|本文提出了一种新的方法来显著提高自适应时空图神经网络(ASTGNN)的计算效率，该方法引入了由彩票假说(LTH)衍生而来的图中奖票(GWT)概念。通过采用预先确定的星型拓扑作为训练前的 GWT，我们平衡了边缘约简和有效的信息传播，减少了计算需求，同时保持了高模型性能。生成自适应时空图的时间和内存计算复杂度从 O (N2)显著降低到 O (N)。我们的方法通过消除详尽的训练，修剪和再训练周期的需要来简化 ASTGNN 的部署，并且通过各种数据集经验证明，可以以大大降低的计算成本实现与全模型相当的性能。具体而言，我们的方法使得能够使用配备48GB 内存的单个 A6000在最大规模的空间-时间数据集上训练 ASTGNN，克服在原始训练期间遇到的内存不足问题，甚至实现最先进的性能。此外，本文还从谱图理论的角度对 GWT 的有效性进行了深入的研究，提供了实质性的理论支持。这一进展不仅证明了 ASTGNN 中存在有效的子网络，而且拓宽了 LTH 在资源受限环境中的适用性，标志着图神经网络领域向前迈进了一大步。密码可于 https://anonymous.4open.science/r/paper-1430索取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pre-Training+Identification+of+Graph+Winning+Tickets+in+Adaptive+Spatial-Temporal+Graph+Neural+Networks)|0|
|[Auctions with LLM Summaries](https://doi.org/10.1145/3637528.3672022)|Avinava Dubey, Zhe Feng, Rahul Kidambi, Aranyak Mehta, Di Wang|Google Research, Mountain View, CA, USA|We study an auction setting in which bidders bid for placement of theircontent within a summary generated by a large language model (LLM), e.g., an adauction in which the display is a summary paragraph of multiple ads. Thisgeneralizes the classic ad settings such as position auctions to an LLMgenerated setting, which allows us to handle general display formats. Wepropose a novel factorized framework in which an auction module and an LLMmodule work together via a prediction model to provide welfare maximizingsummary outputs in an incentive compatible manner. We provide a theoreticalanalysis of this framework and synthetic experiments to demonstrate thefeasibility and validity of the system together with welfare comparisons.|我们研究了一个拍卖设置，其中投标人在一个大型语言模型(LLM)生成的摘要中投标放置他们的内容，例如，一个广告拍卖，其中的显示是多个广告的摘要段落。这将经典的广告设置(如位置拍卖)推广到 LLM 生成设置，这使我们能够处理一般的显示格式。我们提出了一个新的因子分解框架，其中拍卖模块和 LLM 模块通过预测模型一起工作，以激励相容的方式提供福利最大化的总结输出。通过理论分析和综合实验验证了该系统的可行性和有效性，并进行了福利比较。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Auctions+with+LLM+Summaries)|0|
|[GAugLLM: Improving Graph Contrastive Learning for Text-Attributed Graphs with Large Language Models](https://doi.org/10.1145/3637528.3672035)|Yi Fang, Dongzhe Fan, Daochen Zha, Qiaoyu Tan|SFSC of AI and DL, New York University (Shanghai), Shanghai, China; Department of Computer Science, Rice University, Huston, USA|This work studies self-supervised graph learning for text-attributed graphs (TAGs) where nodes are represented by textual attributes. Unlike traditional graph contrastive methods that perturb the numerical feature space and alter the graph's topological structure, we aim to improve view generation through language supervision. This is driven by the prevalence of textual attributes in real applications, which complement graph structures with rich semantic information. However, this presents challenges because of two major reasons. First, text attributes often vary in length and quality, making it difficulty to perturb raw text descriptions without altering their original semantic meanings. Second, although text attributes complement graph structures, they are not inherently well-aligned. To bridge the gap, we introduce GAugLLM, a novel framework for augmenting TAGs. It leverages advanced large language models like Mistral to enhance self-supervised graph learning. Specifically, we introduce a mixture-of-prompt-expert technique to generate augmented node features. This approach adaptively maps multiple prompt experts, each of which modifies raw text attributes using prompt engineering, into numerical feature space. Additionally, we devise a collaborative edge modifier to leverage structural and textual commonalities, enhancing edge augmentation by examining or building connections between nodes. Empirical results across five benchmark datasets spanning various domains underscore our framework's ability to enhance the performance of leading contrastive methods (e.g., BGRL, GraphCL, and GBT) as a plug-in tool. Notably, we observe that the augmented features and graph structure can also enhance the performance of standard generative methods (e.g., GraphMAE and S2GAE), as well as popular graph neural networks (e.g., GCN and GAT). The open-sourced implementation of our GAugLLM is available at https://github.com/NYUSHCS/GAugLLM.|研究了文本属性图(TAGs)的自监督图学习问题，其中节点由文本属性表示。与传统的图形对比方法不同，我们的目标是通过语言监督来改进视图的生成，从而扰乱图形的数值特征空间，改变图形的拓扑结构。这是由于在实际应用中文本属性的流行，这种补图结构具有丰富的语义信息。然而，由于两个主要原因，这带来了挑战。首先，文本属性通常在长度和质量上有所不同，因此很难在不改变原始语义含义的情况下扰乱原始文本描述。其次，尽管文本属性具有补图结构，但它们在本质上并不一致。为了弥补这一差距，我们引入了 GAugLLM，一种用于增强标签的新框架。它利用先进的大型语言模型，如 Mistro，以增强自监督图形学习。具体来说，我们引入了一种混合提示专家技术来生成增强的节点特征。该方法自适应地将多个提示专家映射到数值特征空间，每个提示专家使用提示工程修改原始文本属性。此外，我们还设计了一个协作式的边缘修饰符来利用结构和文本的共性，通过检查或建立节点之间的连接来增强边缘增强。跨越不同领域的五个基准数据集的实证结果强调了我们的框架作为插件工具提高领先对比方法(例如 BGRL、 GraphCL 和 GBT)性能的能力。值得注意的是，我们观察到增强的特征和图结构也可以提高标准生成方法(例如 GraphMAE 和 S2GAE)以及流行的图神经网络(例如 GCN 和 GAT)的性能。我们的 gaugLLM 的开源实现可以在 https://github.com/nyushcs/GAugLLM 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GAugLLM:+Improving+Graph+Contrastive+Learning+for+Text-Attributed+Graphs+with+Large+Language+Models)|0|
|[CAT: Interpretable Concept-based Taylor Additive Models](https://doi.org/10.1145/3637528.3672020)|Viet Duong, Qiong Wu, Zhengyi Zhou, Hongjue Zhao, Chenxiang Luo, Eric Zavesky, Huaxiu Yao, Huajie Shao|AT&T Labs, Austin, TX, USA; William & Mary, Williamsburg, VA, USA; AT&T Labs, Bedminster, NJ, USA; University of Illinois at Urbana-Champaign, Champaign, IL, USA; The University of North Carolina at Chapel Hill, Chapel Hill, NC, USA|As an emerging interpretable technique, Generalized Additive Models (GAMs) adopt neural networks to individually learn non-linear functions for each feature, which are then combined through a linear model for final predictions. Although GAMs can explain deep neural networks (DNNs) at the feature level, they require large numbers of model parameters and are prone to overfitting, making them hard to train and scale. Additionally, in real-world datasets with many features, the interpretability of feature-based explanations diminishes for humans. To tackle these issues, recent research has shifted towards concept-based interpretable methods. These approaches try to integrate concept learning as an intermediate step before making predictions, explaining the predictions in terms of human-understandable concepts. However, these methods require domain experts to extensively label concepts with relevant names and their ground-truth values. In response, we propose CAT, a novel interpretable Concept-bAsed Taylor additive model to simplify this process. CAT does not require domain experts to annotate concepts and their ground-truth values. Instead, it only requires users to simply categorize input features into broad groups, which can be easily accomplished through a quick metadata review. Specifically, CAT first embeds each group of input features into one-dimensional high-level concept representation, and then feeds the concept representations into a new white-box Taylor Neural Network (TaylorNet). The TaylorNet aims to learn the non-linear relationship between the inputs and outputs using polynomials. Evaluation results across multiple benchmarks demonstrate that CAT can outperform or compete with the baselines while reducing the need of extensive model parameters. Importantly, it can effectively explain model predictions through high-level concepts. Source code is available at github.com/vduong143/CAT-KDD-2024.|作为一种新兴的可解释技术，广义可加模型(GAM)采用神经网络来单独学习每个特征的非线性函数，然后通过一个线性模型组合起来进行最终的预测。尽管 GAM 可以在特征层次上解释深度神经网络(DNN) ，但是它们需要大量的模型参数，并且容易过度拟合，使得它们难以训练和扩展。此外，在具有许多特征的真实世界数据集中，基于特征的解释对人类的可解释性降低。为了解决这些问题，最近的研究已经转向基于概念的可解释方法。这些方法试图将概念学习作为预测之前的一个中间步骤，用人类可理解的概念来解释预测。然而，这些方法需要领域专家用相关的名称和它们的地面真实值来广泛地标记概念。为了简化这一过程，我们提出了一种新的基于概念的可解释的泰勒可加模型 CAT。CAT 不要求领域专家对概念及其基本真理价值进行注释。相反，它只要求用户将输入特性简单地分为广泛的组，这可以通过快速的元数据审查轻松实现。具体来说，CAT 首先将每组输入特征嵌入到一维高层概念表示中，然后将概念表示输入到一个新的白盒泰勒神经网络(TaylorNet)中。泰勒网的目的是利用多项式学习输入和输出之间的非线性关系。跨多个基准的评价结果表明，计算机辅助测试的表现优于基准或与基准竞争，同时减少了对广泛模型参数的需求。重要的是，它可以通过高级概念有效地解释模型预测。源代码可在 github.com/vduong143/cat-kdd-2024下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CAT:+Interpretable+Concept-based+Taylor+Additive+Models)|0|
|[SensitiveHUE: Multivariate Time Series Anomaly Detection by Enhancing the Sensitivity to Normal Patterns](https://doi.org/10.1145/3637528.3671919)|Yuye Feng, Wei Zhang, Yao Fu, Weihao Jiang, Jiang Zhu, Wenqi Ren|Hikvision Research Institute, Hangzhou, China|Unsupervised anomaly detection in multivariate time series (MTS) has always been a challenging problem, and the modeling based on reconstruction has garnered significant attention. The insensitivity of these methods towards normal patterns poses challenges in distinguishing between normal and abnormal points. Firstly, the general reconstruction strategies may exhibit limited sensitivity to spatio-temporal dependencies, and their performance remains largely unaffected by such dependencies. Secondly, most methods fail to model the heteroscedastic uncertainty in MTS, hindering their abilities to derive a distinguishable criterion. For instance, normal data with high noise levels may lead to detection failure due to excessively high reconstruction errors. In this work, we emphasize the necessity of sensitivity to normal patterns, which could improve the discrimination between normal and abnormal points remarkably. To this end, we propose SensitiveHUE, a probabilistic network by implementing both reconstruction and heteroscedastic uncertainty estimation. Its core includes a statistical feature removal strategy to ensure the dependency sensitive property, and a novel MTS-NLL loss for modeling the normal patterns in important regions. Experimental results demonstrate that SensitiveHUE exhibits nontrivial sensitivity to normal patterns and outperforms the existing state-of-the-art alternatives by a large margin. Code is publicly available at this URL\footnotehttp://github.com/yuesuoqingqiu/SensitiveHUE.|多变量时间序列(mTS)中的无监督异常检测一直是一个具有挑战性的问题，基于重构的建模已经引起了人们的重视。这些方法对正常模式的不敏感性对区分正常点和异常点提出了挑战。首先，一般的重构策略对时空依赖的敏感性有限，其性能基本上不受这种依赖的影响。其次，大多数方法无法对多输入多输出系统的异方差不确定性进行建模，妨碍了它们推导判别准则的能力。例如，正常数据的高噪声水平可能导致检测失败，由于过高的重建误差。在这项工作中，我们强调对正常模式敏感的必要性，这可以显著提高正常和异常点之间的识别。为此，我们提出了灵敏度人工智能，一个概率网络通过实现重构和异方差不确定性估计。其核心包括统计特征去除策略以保证依赖敏感性，以及一种新的 MTS-NLL 损失模型在重要区域的正常模式。实验结果表明，该算法对正常模式具有非平凡的敏感性，并且性能优于现有的最先进的替代方案。代码可以在这个网址的脚注 http:// github.com/yuesuoqingqiu/sensitivehue 上公开获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SensitiveHUE:+Multivariate+Time+Series+Anomaly+Detection+by+Enhancing+the+Sensitivity+to+Normal+Patterns)|0|
|[Communication-efficient Multi-service Mobile Traffic Prediction by Leveraging Cross-service Correlations](https://doi.org/10.1145/3637528.3671730)|Zhiying Feng, Qiong Wu, Xu Chen|; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China|Mobile traffic prediction plays a crucial role in enabling efficient network management and service provisioning. Traditional prediction approaches treat different mobile application services (such as Uber, Facebook, Twitter, etc) as isolated entities, neglecting potential correlation among them. Moreover, such isolated prediction methods necessitate the uploading of historical traffic data from all regions to forecast city-wide traffic, resulting in consuming substantial bandwidth resources and risking prediction failure in the event of data loss in specific regions. To address these challenges, we propose a novel Cross-service Attention-based Spatial-Temporal Graph Convolutional Network (CsASTGCN) for precise and communication-efficient multi-service mobile traffic prediction. Our methodology allows each mobile service to transmit the traffic data of only a fraction of regions for city-wide traffic prediction of all mobile services, which reduces the resource consumption caused by data transmission. Specifically, the sparse traffic data are initially transmitted to the cloud server and the masked graph autoencoder is utilized to roughly reconstruct the traffic volume for regions with missing data. Subsequently, a cross-service attention-based predictor is designed to calculate the data correlation among different mobile services within the same region. Considering the constantly emerging mobile services, we incorporate a novel model-based adaptive transfer learning scheme to extract valuable knowledge from the existing models and expedite the training of a new model for a new service without training from scratch, thereby enhancing the scalability of our framework. Extensive experiments conducted on a large-scale real-world mobile traffic dataset demonstrate that our model greatly outperforms the existing schemes, enhancing both the communication-efficiency and robustness of large-scale multi-service traffic prediction.|移动流量预测在实现有效的网络管理和服务供应方面起着至关重要的作用。传统的预测方法将不同的移动应用服务(如 Uber、 Facebook、 Twitter 等)视为孤立的实体，而忽略了它们之间潜在的相关性。此外，这种孤立的预测方法需要上传来自所有区域的历史交通数据来预测全市的交通，这会消耗大量的带宽资源，并且在特定区域发生数据丢失时可能导致预测失败。为了应对这些挑战，我们提出了一种新颖的基于跨服务注意力的时空图卷积网络(CsASTGCN) ，用于精确高效的多服务移动流量预测。我们的方法允许每个移动服务只传输一小部分地区的交通数据，以便对所有移动服务进行全市范围的交通预测，从而减少数据传输造成的资源消耗。具体来说，稀疏的流量数据首先传输到云服务器，然后利用掩码图形自动编码器对缺少数据的区域进行粗略的流量重构。然后，设计了一个基于注意力的跨业务预测器，计算同一区域内不同移动业务之间的数据相关性。考虑到不断出现的移动服务，我们采用了一种新的基于模型的自适应转移学习方案，从现有模型中提取有价值的知识，并加快新模型的培训，而不需要从头开始培训新服务，从而提高了我们的框架的可扩展性。在大规模实际移动业务数据集上进行的大量实验表明，该模型的性能明显优于现有方案，提高了大规模多业务流量预测的通信效率和鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Communication-efficient+Multi-service+Mobile+Traffic+Prediction+by+Leveraging+Cross-service+Correlations)|0|
|[Federated Graph Learning with Structure Proxy Alignment](https://doi.org/10.1145/3637528.3671717)|Xingbo Fu, Zihan Chen, Binchi Zhang, Chen Chen, Jundong Li|University of Virginia, Charlottesville, Virginia, USA|Federated Graph Learning (FGL) aims to learn graph learning models over graph data distributed in multiple data owners, which has been applied in various applications such as social recommendation and financial fraud detection. Inherited from generic Federated Learning (FL), FGL similarly has the data heterogeneity issue where the label distribution may vary significantly for distributed graph data across clients. For instance, a client can have the majority of nodes from a class, while another client may have only a few nodes from the same class. This issue results in divergent local objectives and impairs FGL convergence for node-level tasks, especially for node classification. Moreover, FGL also encounters a unique challenge for the node classification task: the nodes from a minority class in a client are more likely to have biased neighboring information, which prevents FGL from learning expressive node embeddings with Graph Neural Networks (GNNs). To grapple with the challenge, we propose FedSpray, a novel FGL framework that learns local class-wise structure proxies in the latent space and aligns them to obtain global structure proxies in the server. Our goal is to obtain the aligned structure proxies that can serve as reliable, unbiased neighboring information for node classification. To achieve this, FedSpray trains a global feature-structure encoder and generates unbiased soft targets with structure proxies to regularize local training of GNN models in a personalized way. We conduct extensive experiments over four datasets, and experiment results validate the superiority of FedSpray compared with other baselines. Our code is available at https://github.com/xbfu/FedSpray.|联邦图学习(Federated Graph Learning，FGL)旨在学习分布在多个数据所有者中的图形数据的图形学习模型，该模型已被广泛应用于社会推荐和金融欺诈检测等领域。FGL 继承自一般的联邦学习(Federated Learning，FL) ，同样也存在数据异构性问题，对于跨客户端的分布式图形数据，标签分布可能会有很大的不同。例如，一个客户端可以拥有一个类的大部分节点，而另一个客户端可能只拥有同一个类的几个节点。这个问题导致了局部目标的分歧，并且损害了节点级任务的 FGL 收敛性，特别是对于节点分类。此外，FGL 还面临着节点分类任务的独特挑战: 来自客户端少数类的节点更可能具有偏向的邻近信息，这阻碍了 FGL 利用图神经网络(GNN)学习表达式节点嵌入。为了应对这一挑战，我们提出了 FedSpray 框架，这是一种新的 FGL 框架，它在潜在空间中学习局部类结构代理，并将它们对齐以获得服务器中的全局结构代理。我们的目标是获得一致的结构代理，可以作为可靠的，无偏的相邻信息的节点分类。为了实现这一点，FedSpray 训练了一个全局特征结构编码器，并用结构代理生成无偏软目标，以个性化的方式规范 GNN 模型的局部训练。我们在四个数据集上进行了广泛的实验，实验结果验证了 FedSpray 相对于其他基线的优越性。我们的代码可以在 https://github.com/xbfu/fedspray 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Federated+Graph+Learning+with+Structure+Proxy+Alignment)|0|
|[Policy-Based Bayesian Active Causal Discovery with Deep Reinforcement Learning](https://doi.org/10.1145/3637528.3671705)|Heyang Gao, Zexu Sun, Hao Yang, Xu Chen|Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China|Causal discovery with observational and interventional data plays an important role in numerous fields. Due to the costly and potentially risky nature of intervention experiments, selecting informative interventions is critical in real-world situations. Several recent works introduce Bayesian active learning to select interventions that maximize the expected information gain about the underlying causal relationship at each optimization step. However, there are still some limitations within these methods: (1) Local optimality. With multiple intervention experiments, selecting optimal intervention myopically at each step may drop into the local optimal point. (2) Expensive time cost. Optimizing the most informative intervention at each step is time-consuming and not suitable for adaptive experiments with strict inference speed requirements. In this study, we propose a novel method called Reinforcement Learning-based Causal Bayesian Experimental Design (RL-CBED) to reduce the risk of local optimality and accelerate intervention selection inference. Specifically, we formulate the active causal discovery problem as a partially observable Markov decision process (POMDP). We design an information gain-based sparse reward function and then improve it to a dense reward function, providing fine-grained feedback to help the RL policy learn more quickly in complex environments. Moreover, we theoretically prove that the Q-function estimator can be learned using only trajectories sampled from the prior, which can significantly reduce the time cost of training process, enabling the real-world application of our method. Extensive experiments on both synthetic and real world-inspired semi-synthetic datasets demonstrate the effectiveness of our proposed method.|利用观测和介入数据进行因果发现在许多领域都发挥着重要作用。由于干预实验的成本高昂且具有潜在风险，因此在现实情况下选择信息丰富的干预措施至关重要。最近的一些工作介绍了贝叶斯主动学习，以选择干预措施，最大限度地获得关于每个优化步骤的潜在因果关系的预期信息。但是，这些方法仍然存在一些局限性: (1)局部最优性。通过多次干预实验，在每一步选择最佳干预措施可能会落入局部最佳点。(2)昂贵的时间成本。在每个步骤中优化信息量最大的干预是耗时的，不适合具有严格推理速度要求的自适应实验。在这项研究中，我们提出了一种新的方法，称为强化学习为基础的因果贝叶斯实验设计(RL-CBED) ，以降低局部最优的风险，加速干预选择推理。具体来说，我们将主动因果发现问题(active cause Discovery problem，简称 POMDP)表述为一个部分可观察马可夫决策过程。我们设计了一个基于信息增益的稀疏奖励函数，然后将其改进为密集奖励函数，提供细粒度的反馈，以帮助 RL 策略在复杂环境中更快地学习。此外，从理论上证明了 Q 函数估计器只需利用先验轨迹进行学习，可以显著降低训练过程的时间成本，使我们的方法能够在现实中应用。在合成和真实世界启发的半合成数据集上的大量实验证明了我们提出的方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Policy-Based+Bayesian+Active+Causal+Discovery+with+Deep+Reinforcement+Learning)|0|
|[Graph Condensation for Open-World Graph Learning](https://doi.org/10.1145/3637528.3671917)|Xinyi Gao, Tong Chen, Wentao Zhang, Yayong Li, Xiangguo Sun, Hongzhi Yin|The Chinese University of Hong Kong, Hong Kong, China; The University of Queensland, Brisbane, Australia; Peking University, Beijing, China; Data 61, CSIRO, Brisbane, Australia|The burgeoning volume of graph data presents significant computational challenges in training graph neural networks (GNNs), critically impeding their efficiency in various applications. To tackle this challenge, graph condensation (GC) has emerged as a promising acceleration solution, focusing on the synthesis of a compact yet representative graph for efficiently training GNNs while retaining performance. Despite the potential to promote scalable use of GNNs, existing GC methods are limited to aligning the condensed graph with merely the observed static graph distribution. This limitation significantly restricts the generalization capacity of condensed graphs, particularly in adapting to dynamic distribution changes. In real-world scenarios, however, graphs are dynamic and constantly evolving, with new nodes and edges being continually integrated. Consequently, due to the limited generalization capacity of condensed graphs, applications that employ GC for efficient GNN training end up with sub-optimal GNNs when confronted with evolving graph structures and distributions in dynamic real-world situations. To overcome this issue, we propose open-world graph condensation (OpenGC), a robust GC framework that integrates structure-aware distribution shift to simulate evolving graph patterns and exploit the temporal environments for invariance condensation. This approach is designed to extract temporal invariant patterns from the original graph, thereby enhancing the generalization capabilities of the condensed graph and, subsequently, the GNNs trained on it. Furthermore, to support the periodic re-condensation and expedite condensed graph updating in life-long graph learning, OpenGC reconstructs the sophisticated optimization scheme with kernel ridge regression and non-parametric graph convolution, significantly accelerating the condensation process while ensuring the exact solutions. Extensive experiments on both real-world and synthetic evolving graphs demonstrate that OpenGC outperforms state-of-the-art (SOTA) GC methods in adapting to dynamic changes in open-world graph environments.|图形数据量的迅速增长给训练图形神经网络(GNN)带来了巨大的计算挑战，严重影响了它们在各种应用中的效率。为了解决这个问题，图的压缩(GC)已经成为一个有前途的加速解决方案，集中在一个紧凑但有代表性的图的合成，有效地训练 GNN，同时保持性能。尽管有可能促进 GNN 的可扩展使用，但现有的 GC 方法仅限于将压缩图与观察到的静态图分布对齐。这种局限性极大地限制了压缩图的泛化能力，尤其是在适应动态分布变化方面。然而，在现实世界的场景中，图是动态的并且不断发展的，新的节点和边不断地被集成。因此，由于压缩图的泛化能力有限，使用 GC 进行高效 GNN 训练的应用程序在面对动态现实世界中不断演化的图结构和分布时，最终得到的 GNN 是次优的。为了克服这个问题，我们提出了开放世界图的压缩(OpenGC) ，一个健壮的 GC 框架，集成了结构感知的分布移位，以模拟演化的图模式，并利用不变压缩的时间环境。该方法从原始图中提取出时间不变的模式，从而提高了压缩图的泛化能力，进而提高了对压缩图进行训练的 GNN 的泛化能力。此外，为了支持终身图学习中的周期性重新压缩和加速压缩图的更新，OpenGC 利用核岭回归和非参数图的卷积重构了复杂的优化方案，在保证精确解的同时显著加快了压缩过程。在真实世界和合成演化图表上的大量实验表明，OpenGC 在适应开放世界图表环境中的动态变化方面优于最先进的(SOTA) GC 方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Condensation+for+Open-World+Graph+Learning)|0|
|[PATE: Proximity-Aware Time Series Anomaly Evaluation](https://doi.org/10.1145/3637528.3671971)|Ramin Ghorbani, Marcel J. T. Reinders, David M. J. Tax|Delft University of Technology, Delft, Netherlands|Evaluating anomaly detection algorithms in time series data is critical asinaccuracies can lead to flawed decision-making in various domains wherereal-time analytics and data-driven strategies are essential. Traditionalperformance metrics assume iid data and fail to capture the complex temporaldynamics and specific characteristics of time series anomalies, such as earlyand delayed detections. We introduce Proximity-Aware Time series anomalyEvaluation (PATE), a novel evaluation metric that incorporates the temporalrelationship between prediction and anomaly intervals. PATE usesproximity-based weighting considering buffer zones around anomaly intervals,enabling a more detailed and informed assessment of a detection. Using theseweights, PATE computes a weighted version of the area under the Precision andRecall curve. Our experiments with synthetic and real-world datasets show thesuperiority of PATE in providing more sensible and accurate evaluations thanother evaluation metrics. We also tested several state-of-the-art anomalydetectors across various benchmark datasets using the PATE evaluation scheme.The results show that a common metric like Point-Adjusted F1 Score fails tocharacterize the detection performances well, and that PATE is able to providea more fair model comparison. By introducing PATE, we redefine theunderstanding of model efficacy that steers future studies toward developingmore effective and accurate detection models.|对时间序列数据中的异常检测算法进行评估是至关重要的，因为不准确会导致不同领域的决策失误，而实时分析和数据驱动策略是必不可少的。传统的性能度量方法假定 id 数据，无法捕获复杂的时间动态和时间序列异常的具体特征，如早期和延迟检测。我们介绍了接近感知时间序列异常评估(PATE) ，一种新的评估度量，其中包含了预测和异常间隔之间的时间关系。PATE 使用基于接近度的加权方法，考虑到异常间隔周围的缓冲区，从而能够对检测进行更详细和知情的评估。使用这些权重，PATE 计算精度和召回曲线下面积的加权版本。我们对合成和真实世界数据集的实验显示了 PATE 在提供比其他评估指标更合理和准确的评估方面的优越性。我们还使用 PATE 评估方案在不同的基准数据集上测试了几个最先进的异常检测器。结果表明，点调整 F1得分这一常用指标不能很好地表征检测性能，PATE 能够提供更公平的模型比较。通过介绍 PATE，我们重新定义了模型功效的理解，引导未来的研究发展更有效和准确的检测模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PATE:+Proximity-Aware+Time+Series+Anomaly+Evaluation)|0|
|[Hierarchical Neural Constructive Solver for Real-world TSP Scenarios](https://doi.org/10.1145/3637528.3672053)|Yong Liang Goh, Zhiguang Cao, Yining Ma, Yanfei Dong, Mohammed Haroon Dupty, Wee Sun Lee|Singapore Management University, Singapore, Singapore; National University of Singapore, Singapore, Singapore; Grabtaxi Holdings Pte Ltd & National University of Singapore, Singapore, Singapore|Existing neural constructive solvers for routing problems have predominantly employed transformer architectures, conceptualizing the route construction as a set-to-sequence learning task. However, their efficacy has primarily been demonstrated on entirely random problem instances that inadequately capture real-world scenarios. In this paper, we introduce realistic Traveling Salesman Problem (TSP) scenarios relevant to industrial settings and derive the following insights: (1) The optimal next node (or city) to visit often lies within proximity to the current node, suggesting the potential benefits of biasing choices based on current locations. (2) Effectively solving the TSP requires robust tracking of unvisited nodes and warrants succinct grouping strategies. Building upon these insights, we propose integrating a learnable choice layer inspired by Hypernetworks to prioritize choices based on the current location, and a learnable approximate clustering algorithm inspired by the Expectation-Maximization algorithm to facilitate grouping the unvisited cities. Together, these two contributions form a hierarchical approach towards solving the realistic TSP by considering both immediate local neighbourhoods and learning an intermediate set of node representations. Our hierarchical approach yields superior performance compared to both classical and recent transformer models, showcasing the efficacy of the key designs.|现有的路由问题的神经构造解决方案主要采用变压器结构，将路由构造概念化为一个集合到序列的学习任务。然而，它们的有效性主要表现在完全随机的问题实例上，这些实例没有充分捕捉到真实世界的场景。在本文中，我们介绍了与工业环境相关的现实的旅行商问题(TSP)场景，并得出以下见解: (1)最佳的下一个节点(或城市)访问往往位于当前节点附近，提出了基于当前位置的偏差选择的潜在好处。(2)有效解决 TSP 问题需要对未访问节点进行鲁棒跟踪，并保证简洁的分组策略。基于这些见解，我们建议整合一个受超级网络启发的可学习的选择层，根据当前位置对选择进行优先排序，以及受期望最大化算法启发的可学习的近似聚类算法，以促进对未访问的城市进行分组。同时，这两个贡献形成了一个分层的方法来解决现实的 TSP 通过考虑直接的局部邻域和学习一个中间集的节点表示。我们的分层方法产生优越的性能相比，经典和最近的变压器模型，展示了关键设计的功效。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Neural+Constructive+Solver+for+Real-world+TSP+Scenarios)|0|
|[An Energy-centric Framework for Category-free Out-of-distribution Node Detection in Graphs](https://doi.org/10.1145/3637528.3671939)|Zheng Gong, Ying Sun||Graph neural networks have garnered notable attention for effectively processing graph-structured data. Prevalent models prioritize improving in-distribution (IND) data performance, frequently overlooking the risks from potential out-of-distribution (OOD) nodes during training and inference. In real-world graphs, the automated network construction can introduce noisy nodes from unknown distributions. Previous research into OOD node detection, typically referred to as entropy-based methods, calculates OOD measurements from the prediction entropy alongside category classification training. However, the nodes in the graph might not be pre-labeled with specific categories, rendering entropy-based OOD detectors inapplicable in such category-free situations. To tackle this issue, we propose an energy-centric density estimation framework for OOD node detection, referred to as EnergyDef. Within this framework, we introduce an energy-based GNN to compute node energies that act as indicators of node density and reveal the OOD uncertainty of nodes. Importantly, EnergyDef can efficiently identify OOD nodes with low-resource OOD node annotations, achieved by sampling hallucinated nodes via Langevin Dynamics and structure estimation, along with training through Contrastive Divergence. Our comprehensive experiments on real-world datasets substantiate that our framework markedly surpasses state-of-the-art methods in terms of detection quality, even under conditions of scarce or entirely absent OOD node annotations.|图形神经网络在有效处理图形结构数据方面受到了广泛的关注。流行的模型优先考虑改善内部分发(IND)数据性能，在训练和推断期间经常忽略潜在分发外(OOD)节点的风险。在实际图形中，自动化网络构造可以引入来自未知分布的噪声节点。以往对面向对象的节点检测的研究，通常被称为基于熵的方法，在分类训练的同时，根据预测熵计算面向对象的测量值。然而，图中的节点可能没有预先标记特定的类别，使得基于熵的 OOD 检测器不适用于这种无类别的情况。为了解决这个问题，我们提出了一个以能量为中心的 OOD 节点检测密度估计框架，称为 EnergyDef。在这个框架内，我们引入了一个基于能量的 GNN 来计算节点能量，它作为节点密度的指标，揭示了节点面向对象的不确定性。重要的是，EnergyDef 能够有效地识别具有低资源 OOD 节点注释的 OOD 节点，通过 Langevin Dynamics 和结构估计对幻觉节点进行采样，并通过对比发散进行训练。我们在真实世界数据集上的全面实验证实，我们的框架在检测质量方面明显超过了最先进的方法，即使在缺乏或完全没有 OOD 节点注释的情况下也是如此。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Energy-centric+Framework+for+Category-free+Out-of-distribution+Node+Detection+in+Graphs)|0|
|[Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective](https://doi.org/10.1145/3637528.3671792)|Kai Guo, Hongzhi Wen, Wei Jin, Yaming Guo, Jiliang Tang, Yi Chang|School of Artificial Intelligence, Jilin University, Changchun, Jilan, China; Department of Computer Science, Emory University, Atlanta, GA, USA; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, USA|Graph neural networks (GNNs) have exhibited remarkable performance under the assumption that test data comes from the same distribution of training data. However, in real-world scenarios, this assumption may not always be valid. Consequently, there is a growing focus on exploring the Out-of-Distribution (OOD) problem in the context of graphs. Most existing efforts have primarily concentrated on improving graph OOD generalization from two model-agnostic perspectives: data-driven methods and strategy-based learning. However, there has been limited attention dedicated to investigating the impact of well-known GNN model architectures on graph OOD generalization, which is orthogonal to existing research. In this work, we provide the first comprehensive investigation of OOD generalization on graphs from an architecture perspective, by examining the common building blocks of modern GNNs. Through extensive experiments, we reveal that both the graph self-attention mechanism and the decoupled architecture contribute positively to graph OOD generalization. In contrast, we observe that the linear classification layer tends to compromise graph OOD generalization capability. Furthermore, we provide in-depth theoretical insights and discussions to underpin these discoveries. These insights have empowered us to develop a novel GNN backbone model, DGat, designed to harness the robust properties of both graph self-attention mechanism and the decoupled architecture. Extensive experimental results demonstrate the effectiveness of our model under graph OOD, exhibiting substantial and consistent enhancements across various training strategies. Our codes are available at https://github.com/KaiGuo20/DGAT **REMOVE 2nd URL**://github.com/KaiGuo20/DGAT.|在假设测试数据来自同一分布的训练数据的前提下，图神经网络(GNN)表现出了显著的性能。然而，在现实世界的场景中，这个假设并不总是有效的。因此，在图的背景下探索分布外(OOD)问题越来越受到人们的关注。大多数现有的努力主要集中在从两个模型无关的角度改进图形 OOD 泛化: 数据驱动的方法和基于策略的学习。然而，目前研究 GNN 模型体系结构对图形 OOD 泛化的影响与现有研究是正交的，这方面的研究受到了一定的限制。在这项工作中，我们提供了第一个全面的研究面向对象设计概括图从体系结构的角度，通过检查现代 GNN 的共同构建模块。通过大量的实验，我们发现图的自注意机制和解耦结构都对图的面向对象的泛化起到了积极的作用。相比之下，我们观察到线性分类层往往会影响图的 OOD 泛化能力。此外，我们提供深入的理论见解和讨论，以支持这些发现。这些见解使我们有能力开发一个新的 GNN 骨干网模型，DGat，旨在利用图形自注意机制和解耦体系结构的鲁棒性。广泛的实验结果证明了我们的模型在面向对象的图形下的有效性，显示了实质性的和一致的增强跨各种训练策略。我们的代码可在 https://github.com/kaiguo20/dgat  * * 删除第二个网址 * * :// github.com/kaiguo20/dgat。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Investigating+Out-of-Distribution+Generalization+of+GNNs:+An+Architecture+Perspective)|0|
|[HiFGL: A Hierarchical Framework for Cross-silo Cross-device Federated Graph Learning](https://doi.org/10.1145/3637528.3671660)|Zhuoning Guo, Duanyi Yao, Qiang Yang, Hao Liu|; The Hong Kong University of Science and Technology, Hong Kong, Hong Kong|Federated Graph Learning (FGL) has emerged as a promising way to learnhigh-quality representations from distributed graph data with privacypreservation. Despite considerable efforts have been made for FGL under eithercross-device or cross-silo paradigm, how to effectively capture graph knowledgein a more complicated cross-silo cross-device environment remains anunder-explored problem. However, this task is challenging because of theinherent hierarchy and heterogeneity of decentralized clients, diversifiedprivacy constraints in different clients, and the cross-client graph integrityrequirement. To this end, in this paper, we propose a Hierarchical FederatedGraph Learning (HiFGL) framework for cross-silo cross-device FGL. Specifically,we devise a unified hierarchical architecture to safeguard federated GNNtraining on heterogeneous clients while ensuring graph integrity. Moreover, wepropose a Secret Message Passing (SecMP) scheme to shield unauthorized accessto subgraph-level and node-level sensitive information simultaneously.Theoretical analysis proves that HiFGL achieves multi-level privacypreservation with complexity guarantees. Extensive experiments on real-worlddatasets validate the superiority of the proposed framework against severalbaselines. Furthermore, HiFGL's versatile nature allows for its application ineither solely cross-silo or cross-device settings, further broadening itsutility in real-world FGL applications.|联邦图学习(FGL)已经成为一种有前途的从分布式图形数据中学习高质量表示并保护隐私的方法。尽管在跨设备或跨竖井范式下已经为 FGL 做出了相当大的努力，但如何在更复杂的跨竖井跨设备环境中有效地获取图形知识仍然是一个尚未探索的问题。然而，由于分散客户端的内在层次性和异构性、不同客户端的多样化隐私约束以及跨客户端图形完整性要求，这项任务具有挑战性。为此，本文提出了一种基于层次联邦图学习(HiFGL)的跨筒仓跨设备 FGL 框架。具体来说，我们设计了一个统一的层次结构来保护异构客户端上的联邦 GNN 训练，同时保证图的完整性。此外，我们提出了一个秘密消息传递(SecMP)方案来同时屏蔽未经授权访问的子图级和节点级敏感信息。理论分析表明，HiFGL 在保证复杂度的前提下实现了多级隐私保护。在真实世界数据集上的大量实验验证了该框架对多个基线的优越性。此外，HiFGL 的多功能性质允许其应用在单独的跨筒仓或跨设备设置，进一步扩大其在现实世界的 FGL 应用的适用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HiFGL:+A+Hierarchical+Framework+for+Cross-silo+Cross-device+Federated+Graph+Learning)|0|
|[AnyLoss: Transforming Classification Metrics into Loss Functions](https://doi.org/10.1145/3637528.3672017)|Do Heon Han, Nuno Moniz, Nitesh V. Chawla|; Lucy Family Institute for Data & Society, University of Notre Dame, Notre Dame, IN, USA|Many evaluation metrics can be used to assess the performance of models inbinary classification tasks. However, most of them are derived from a confusionmatrix in a non-differentiable form, making it very difficult to generate adifferentiable loss function that could directly optimize them. The lack ofsolutions to bridge this challenge not only hinders our ability to solvedifficult tasks, such as imbalanced learning, but also requires the deploymentof computationally expensive hyperparameter search processes in modelselection. In this paper, we propose a general-purpose approach that transformsany confusion matrix-based metric into a loss function, AnyLoss, thatis available in optimization processes. To this end, we use an approximationfunction to make a confusion matrix represented in a differentiable form, andthis approach enables any confusion matrix-based metric to be directly used asa loss function. The mechanism of the approximation function is provided toensure its operability and the differentiability of our loss functions isproved by suggesting their derivatives. We conduct extensive experiments underdiverse neural networks with many datasets, and we demonstrate their generalavailability to target any confusion matrix-based metrics. Our method,especially, shows outstanding achievements in dealing with imbalanced datasets,and its competitive learning speed, compared to multiple baseline models,underscores its efficiency.|许多评价指标可以用来评价模型在二进制分类任务中的性能。然而，它们中的大多数都是由不可微形式的混淆矩阵导出的，因此很难产生可微损失函数来直接优化它们。缺乏解决这一挑战的方案不仅阻碍了我们解决困难任务的能力，例如不平衡的学习，而且还需要在模型选择中部署计算昂贵的超参数搜索过程。在本文中，我们提出了一个通用的方法，转换任何混淆矩阵为基础的度量损失函数，AnyLoss，这是可用于优化过程。为了达到这个目的，我们使用一个近似函数来表示一个可微形式的混淆矩阵，这种方法使任何基于混淆矩阵的度量可以直接用作损失函数。给出了近似函数的机理，通过建立近似函数的导数，保证了近似函数的可操作性和损失函数的可微性。我们进行了广泛的实验在多样化的神经网络与许多数据集，并证明了它们的一般可用性的目标任何混淆矩阵为基础的度量。该方法在处理不平衡数据集方面取得了显著的成绩，与多个基线模型相比，该方法的学习速度具有竞争力，突出了该方法的效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AnyLoss:+Transforming+Classification+Metrics+into+Loss+Functions)|0|
|[Expander Hierarchies for Normalized Cuts on Graphs](https://doi.org/10.1145/3637528.3671978)|Kathrin Hanauer, Monika Henzinger, Robin Münk, Harald Räcke, Maximilian Vötsch|Institute of Science and Technology Austria (ISTA), Klosterneuburg, Austria; ; Technical University of Munich, Munich, Germany; Faculty of Computer Science, University of Vienna, Vienna, Austria|Expander decompositions of graphs have significantly advanced theunderstanding of many classical graph problems and led to numerous fundamentaltheoretical results. However, their adoption in practice has been hindered dueto their inherent intricacies and large hidden factors in their asymptoticrunning times. Here, we introduce the first practically efficient algorithm forcomputing expander decompositions and their hierarchies and demonstrate itseffectiveness and utility by incorporating it as the core component in a novelsolver for the normalized cut graph clustering objective. Our extensive experiments on a variety of large graphs show that ourexpander-based algorithm outperforms state-of-the-art solvers for normalizedcut with respect to solution quality by a large margin on a variety of graphclasses such as citation, e-mail, and social networks or web graphs whileremaining competitive in running time.|图的扩展分解极大地提高了对许多经典图问题的理解，并产生了许多基本的理论结果。然而，由于其固有的复杂性和大的隐藏因素在其渐近运行时间，它们在实践中的采用受到了阻碍。在这里，我们介绍了第一个实用有效的扩展器分解算法及其层次结构，并证明了其有效性和实用性，将其作为一个核心组件的规范化割图聚类目标规划求解器。我们在各种大型图表上的广泛实验表明，在各种图表类(如引用、电子邮件、社交网络或网络图表)上，我们基于 expander 的算法在规范化切割方面优于最先进的求解器，同时在运行时间上保持竞争力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Expander+Hierarchies+for+Normalized+Cuts+on+Graphs)|0|
|[Model-Agnostic Random Weighting for Out-of-Distribution Generalization](https://doi.org/10.1145/3637528.3671762)|Yue He, Pengfei Tian, Renzhe Xu, Xinwei Shen, Xingxuan Zhang, Peng Cui|ETH Zürich, Zürich, Switzerland; Tsinghua University, Beijing, China|Despite the encouraging successes in numerous applications, machine learning methods grounded on the i.i.d. assumption often experience performance deterioration when confronted with the distribution shift between training and test data. This challenge has instigated recent research endeavors focusing on out-of-distribution (OOD) generalization. A particularly pervasive and intricate OOD problem is to enhance the model's generalization ability by training it on samples drawn from a single environment. In response to the problem, we propose a simple model-agnostic method tailored for a practical OOD scenario in this paper. Our approach centers on pursuing robust weighted empirical risks, utilizing randomly shifted training distributions derived through a specific sample-based weighting strategy. Furthermore, we theoretically establish that the expected risk of the shifted training distribution can bound the expected risk of the test distribution. This theoretical foundation ensures the improved prediction performance of our method when employed in uncertain test distributions. Extensive experiments conducted on diverse real-world datasets affirm the effectiveness of our method, highlighting its potential to address the distribution shifts in machine learning applications.|尽管在许多应用中取得了令人鼓舞的成功，但是当面对训练数据和测试数据之间的分布变化时，基于 ID 假设的机器学习方法往往会出现性能下降。这一挑战促使最近的研究致力于分布外(OOD)概括。一个特别普遍和复杂的面向对象设计(OOD)问题是通过对从单一环境中提取的样本进行训练来提高模型的泛化能力。针对这一问题，本文提出了一种适用于实际面向对象设计的简单模型无关方法。我们的方法集中在追求稳健的加权经验风险，利用随机移动的训练分布通过一个特定的样本为基础的加权策略。进一步，我们从理论上证明了偏移训练分布的期望风险可以约束测试分布的期望风险。这一理论基础保证了该方法在测试分布不确定时具有更好的预测性能。在不同的真实世界数据集上进行的大量实验证实了我们的方法的有效性，突出了其解决机器学习应用中的分布变化的潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Model-Agnostic+Random+Weighting+for+Out-of-Distribution+Generalization)|0|
|[RoutePlacer: An End-to-End Routability-Aware Placer with Graph Neural Network](https://doi.org/10.1145/3637528.3671895)|Yunbo Hou, Haoran Ye, Yingxue Zhang, Siyuan Xu, Guojie Song|; School of Software and Microelectronics, Peking University, Beijing, China; Huawei Noah's Ark Lab, Shenzhen, China; Huawei Noah's Ark Lab, Markham, Canada|Placement is a critical and challenging step of modern chip design, with routability being an essential indicator of placement quality. Current routability-oriented placers typically apply an iterative two-stage approach, wherein the first stage generates a placement solution, and the second stage provides non-differentiable routing results to heuristically improve the solution quality. This method hinders jointly optimizing the routability aspect during placement. To address this problem, this work introduces RoutePlacer, an end-to-end routability-aware placement method. It trains RouteGNN, a customized graph neural network, to efficiently and accurately predict routability by capturing and fusing geometric and topological representations of placements. Well-trained RouteGNN then serves as a differentiable approximation of routability, enabling end-to-end gradient-based routability optimization. In addition, RouteGNN can improve two-stage placers as a plug-and-play alternative to external routers. Our experiments on DREAMPlace, an open-source AI4EDA platform, show that RoutePlacer can reduce Total Overflow by up to 16% while maintaining routed wirelength, compared to the state-of-the-art; integrating RouteGNN within two-stage placers leads to a 44% reduction in Total Overflow without compromising wirelength.|布局是现代芯片设计的一个关键和具有挑战性的步骤，可布局性是布局质量的重要指标。当前面向路由性的放置器通常应用迭代的两阶段方法，其中第一阶段生成放置解决方案，第二阶段提供不可微路由结果以启发性地提高解决方案的质量。这种方法阻碍了布局过程中路由性方面的联合优化。为了解决这个问题，本文介绍了 RoutePlacer，一种端到端的可路由性感知布局方法。它训练定制图形神经网络 RouteGNN，通过捕获和融合位置的几何和拓扑表示，有效和准确地预测路由性。训练有素的 RouteGNN 可以作为路由性的可微近似值，从而实现基于端到端梯度的路由性优化。此外，RouteGNN 可以改进两阶段占位器，作为外部路由器的即插即用替代方案。我们在开源 AI4EDA 平台 DREAMPlace 上的实验表明，与最先进的技术相比，RoutePlacer 可以在保持路由线长的同时减少总溢出达16% ; 在两阶段放置器中整合 RouteGNN 可以在不损害线长的情况下减少总溢出44% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RoutePlacer:+An+End-to-End+Routability-Aware+Placer+with+Graph+Neural+Network)|0|
|[Is Aggregation the Only Choice? Federated Learning via Layer-wise Model Recombination](https://doi.org/10.1145/3637528.3671722)|Ming Hu, Zhihao Yue, Xiaofei Xie, Cheng Chen, Yihao Huang, Xian Wei, Xiang Lian, Yang Liu, Mingsong Chen|Nanyang Technological University, Singapore, Singapore; Kent State University, Kent, OH, USA; Chinese Academy of Sciences, Shanghai, China; East China Normal University, Shanghai, China; Singapore Management University, Singapore, Singapore|Although Federated Learning (FL) enables global model training across clients without compromising their raw data, due to the unevenly distributed data among clients, existing Federated Averaging (FedAvg)-based methods suffer from the problem of low inference performance. Specifically, different data distributions among clients lead to various optimization directions of local models. Aggregating local models usually results in a low-generalized global model, which performs worse on most of the clients. To address the above issue, inspired by the observation from a geometric perspective that a well-generalized solution is located in a flat area rather than a sharp area, we propose a novel and heuristic FL paradigm named FedMR (Federated Model Recombination). The goal of FedMR is to guide the recombined models to be trained towards a flat area. Unlike conventional FedAvg-based methods, in FedMR, the cloud server recombines collected local models by shuffling each layer of them to generate multiple recombined models for local training on clients rather than an aggregated global model. Since the area of the flat area is larger than the sharp area, when local models are located in different areas, recombined models have a higher probability of locating in a flat area. When all recombined models are located in the same flat area, they are optimized towards the same direction. We theoretically analyze the convergence of model recombination. Experimental results show that, compared with state-of-the-art FL methods, FedMR can significantly improve the inference accuracy without exposing the privacy of each client.|虽然联邦学习(FL)能够在不损害客户原始数据的情况下进行跨客户的全局模型训练，但由于客户之间数据分布不均匀，现有的基于联邦平均(FedAvg)的方法存在推理性能低的问题。具体来说，客户端之间的不同数据分布导致局部模型的不同优化方向。聚合局部模型通常会导致低通用性的全局模型，这种模型在大多数客户端上表现更差。为了解决上述问题，从几何角度观察，一个良好的广义解决方案位于一个平坦的区域，而不是一个尖锐的区域，我们提出了一个新颖的启发式 FL 范式称为 FedMR (联邦模型重组)。FedMR 的目标是引导被训练的重组模型朝向一个平坦的区域。与传统的基于 FedAvg 的方法不同，在 FedMR 中，云服务器重新组合收集的本地模型，通过重新组合每一层的模型，生成多个重新组合的模型，用于客户端的本地培训，而不是聚合的全局模型。由于平坦区域面积大于尖锐区域面积，当局部模型位于不同区域时，重组模型位于平坦区域的概率较高。当所有的重组模型位于相同的平面区域，他们朝着相同的方向优化。从理论上分析了模型重组的收敛性。实验结果表明，与现有的 FL 方法相比，FedMR 能够在不暴露每个客户端隐私的情况下显著提高推理精度。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Is+Aggregation+the+Only+Choice?+Federated+Learning+via+Layer-wise+Model+Recombination)|0|
|[Privacy-Preserved Neural Graph Databases](https://doi.org/10.1145/3637528.3671678)|Qi Hu, Haoran Li, Jiaxin Bai, Zihao Wang, Yangqiu Song|Department of CSE, Hong Kong University of Science and Technology, Hong Kong, China|In the era of large language models (LLMs), efficient and accurate data retrieval has become increasingly crucial for the use of domain-specific or private data in the retrieval augmented generation (RAG). Neural graph databases (NGDBs) have emerged as a powerful paradigm that combines the strengths of graph databases (GDBs) and neural networks to enable efficient storage, retrieval, and analysis of graph-structured data which can be adaptively trained with LLMs. The usage of neural embedding storage and Complex neural logical Query Answering (CQA) provides NGDBs with generalization ability. When the graph is incomplete, by extracting latent patterns and representations, neural graph databases can fill gaps in the graph structure, revealing hidden relationships and enabling accurate query answering. Nevertheless, this capability comes with inherent trade-offs, as it introduces additional privacy risks to the domain-specific or private databases. Malicious attackers can infer more sensitive information in the database using well-designed queries such as from the answer sets of where Turing Award winners born before 1950 and after 1940 lived, the living places of Turing Award winner Hinton are probably exposed, although the living places may have been deleted in the training stage due to the privacy concerns. In this work, we propose a privacy-preserved neural graph database (P-NGDB) framework to alleviate the risks of privacy leakage in NGDBs. We introduce adversarial training techniques in the training stage to enforce the NGDBs to generate indistinguishable answers when queried with private information, enhancing the difficulty of inferring sensitive information through combinations of multiple innocuous queries. Extensive experimental results on three datasets show that our framework can effectively protect private information in the graph database while delivering high-quality public answers responses to queries. The code is available at https://github.com/HKUST-KnowComp/PrivateNGDB.|在大语言模型(LLM)时代，高效、准确的数据检索对于特定领域或私有数据在检索增强生成(RAG)中的应用越来越重要。神经图形数据库(NGDB)已经成为一种强大的范式，它结合了图形数据库(GDB)和神经网络的优势，能够有效地存储、检索和分析图形结构数据，这些数据可以用 LLM 自适应地训练。神经嵌入存储和复杂神经逻辑查询回答(CQA)的使用为 NGDB 提供了泛化能力。当图不完整时，神经图数据库通过提取潜在模式和表征，可以填补图结构中的空白，揭示隐藏的关系，实现准确的查询回答。然而，这种能力带来了内在的权衡，因为它给特定领域或私有数据库带来了额外的隐私风险。恶意攻击者可以通过精心设计的查询在数据库中推断出更敏感的信息，例如从图灵奖获得者1950年前和1940年后出生的地方的答案集中，图灵奖获得者 Hinton 的生活场所可能被暴露，尽管生活场所可能在训练阶段由于隐私问题而被删除。在这项工作中，我们提出了一个隐私保护的神经图数据库(P-NGDB)框架，以减少隐私泄露的风险在 NGDB。我们在训练阶段引入对抗性训练技术，强制 NGDB 在用私人信息查询时生成不可区分的答案，通过多个无害查询的组合增加推断敏感信息的难度。对三个数据集的大量实验结果表明，我们的框架能够有效地保护图形数据库中的私有信息，同时提供高质量的公开答复查询。密码可在 https://github.com/hkust-knowcomp/privatengdb 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Privacy-Preserved+Neural+Graph+Databases)|0|
|[EntropyStop: Unsupervised Deep Outlier Detection with Loss Entropy](https://doi.org/10.1145/3637528.3671943)|Yihong Huang, Yuang Zhang, Liping Wang, Fan Zhang, Xuemin Lin|Guangzhou University, Guangzhou, China; Shanghai Jiao Tong University, Shanghai, China; East China Normal University, Shanghai, China|Unsupervised Outlier Detection (UOD) is an important data mining task. With the advance of deep learning, deep Outlier Detection (OD) has received broad interest. Most deep UOD models are trained exclusively on clean datasets to learn the distribution of the normal data, which requires huge manual efforts to clean the real-world data if possible. Instead of relying on clean datasets, some approaches directly train and detect on unlabeled contaminated datasets, leading to the need for methods that are robust to such challenging conditions. Ensemble methods emerged as a superior solution to enhance model robustness against contaminated training sets. However, the training time is greatly increased by the ensemble mechanism. In this study, we investigate the impact of outliers on training, aiming to halt training on unlabeled contaminated datasets before performance degradation. Initially, we noted that blending normal and anomalous data causes AUC fluctuations-a label-dependent measure of detection accuracy. To circumvent the need for labels, we propose a zero-label entropy metric named Loss Entropy for loss distribution, enabling us to infer optimal stopping points for training without labels. Meanwhile, a negative correlation between entropy metric and the label-based AUC score is demonstrated by theoretical proofs. Based on this, an automated early-stopping algorithm called EntropyStop is designed to halt training when loss entropy suggests the maximum model detection capability. We conduct extensive experiments on ADBench (including 47 real datasets), and the overall results indicate that AutoEncoder (AE) enhanced by our approach not only achieves better performance than ensemble AEs but also requires under 2% of training time. Lastly, loss entropy and EntropyStop are evaluated on other deep OD models, exhibiting their broad potential applicability.|无监督异常检测(UOD)是一项重要的数据挖掘任务。随着深度学习的发展，深度异常检测已受到广泛关注。大多数深度 UOD 模型都是专门针对清洁数据集进行训练，以了解正常数据的分布情况，如果可能的话，这需要大量的人工努力来清洁现实世界中的数据。有些方法不依赖于干净的数据集，而是直接对未标记的污染数据集进行训练和检测，因此需要对这种具有挑战性的条件具有鲁棒性的方法。集成方法作为一种优越的解决方案出现，以增强模型对污染的训练集的鲁棒性。然而，集成机制大大增加了训练时间。在这项研究中，我们调查了异常值对训练的影响，目的是在性能下降之前停止对未标记污染数据集的训练。最初，我们注意到混合正常和异常数据会导致 AUC 波动-一个依赖于标签的检测准确性度量。为了规避标签的需要，我们提出了一个零标签熵度量，称为损失熵的损失分布，使我们能够推断最佳停止点的训练没有标签。同时，从理论上证明了熵度量与基于标签的 AUC 得分之间存在负相关关系。在此基础上，设计了一种自动提前停止算法熵停止训练时，损失熵建议的最大模型检测能力。我们在 ADBench (包括47个实际数据集)上进行了广泛的实验，总体结果表明，该方法增强的 AutoEncoder (AE)不仅比集成 AE 具有更好的性能，而且需要不到2% 的训练时间。最后，在其他深度 OD 模型上对损失熵和熵止进行了评价，展示了其广泛的适用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EntropyStop:+Unsupervised+Deep+Outlier+Detection+with+Loss+Entropy)|0|
|[RC-Mixup: A Data Augmentation Strategy against Noisy Data for Regression Tasks](https://doi.org/10.1145/3637528.3671993)|Seonghyeon Hwang, Minsu Kim, Steven Euijong Whang|KAIST, Daejeon, Republic of Korea|We study the problem of robust data augmentation for regression tasks in the presence of noisy data. Data augmentation is essential for generalizing deep learning models, but most of the techniques like the popular Mixup are primarily designed for classification tasks on image data. Recently, there are also Mixup techniques that are specialized to regression tasks like C-Mixup. In comparison to Mixup, which takes linear interpolations of pairs of samples, C-Mixup is more selective in which samples to mix based on their label distances for better regression performance. However, C-Mixup does not distinguish noisy versus clean samples, which can be problematic when mixing and lead to suboptimal model performance. At the same time, robust training has been heavily studied where the goal is to train accurate models against noisy data through multiple rounds of model training. We thus propose our data augmentation strategy RC-Mixup, which tightly integrates C-Mixup with multi-round robust training methods for a synergistic effect. In particular, C-Mixup improves robust training in identifying clean data, while robust training provides cleaner data to C-Mixup for it to perform better. A key advantage of RC-Mixup is that it is data-centric where the robust model training algorithm itself does not need to be modified, but can simply benefit from data mixing. We show in our experiments that RC-Mixup significantly outperforms C-Mixup and robust training baselines on noisy data benchmarks and can be integrated with various robust training methods.|我们研究了回归任务在有噪声数据存在的情况下的鲁棒数据增强问题。数据增强对于推广深度学习模型至关重要，但是大多数技术，如流行的 Mixup，主要是为图像数据的分类任务而设计的。最近，还出现了专门用于回归任务(如 C-Mixup)的 Mixup 技术。与对样本进行线性插值的 Mixup 相比，C-Mixup 更有选择性地根据样本的标记距离进行混合，以获得更好的回归性能。然而，C-Mixup 不能区分噪声样本和干净样本，这在混合时可能会产生问题，并导致次优模型性能。与此同时，鲁棒训练也得到了广泛的研究，其目标是通过多轮模型训练对噪声数据进行精确的模型训练。因此，我们提出了我们的数据增强策略 RC-Mixup，它紧密集成了 C-Mixup 和多轮鲁棒训练方法的协同效应。特别是，C-Mixup 改进了识别干净数据的健壮训练，而健壮训练为 C-Mixup 提供了更干净的数据，使其表现得更好。RC-Mixup 的一个关键优点是它是以数据为中心的，不需要修改鲁棒模型训练算法本身，但是可以简单地从数据混合中受益。实验结果表明，RC-Mixup 算法在噪声数据基准上的性能明显优于 C-Mixup 算法和鲁棒训练基线，并且可以与各种鲁棒训练方法相结合。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RC-Mixup:+A+Data+Augmentation+Strategy+against+Noisy+Data+for+Regression+Tasks)|0|
|[Learn Together Stop Apart: An Inclusive Approach to Ensemble Pruning](https://doi.org/10.1145/3637528.3672018)|Bulat Ibragimov, Gleb Gusev|; Sber AI Lab, Moscow, Russian Federation|Gradient Boosting is a leading learning method that builds ensembles and adapts their sizes to particular tasks, consistently delivering top-tier results across various applications. However, determining the optimal number of models in the ensemble remains a critical yet underexplored aspect. Traditional approaches assume a universal ensemble size effective for all data points, which may not always hold true due to data heterogeneity. This paper introduces an adaptive approach to early stopping in Gradient Boosting, addressing data heterogeneity by assigning different stop moments to different data regions at inference time while still training a common ensemble on the entire dataset. We propose two methods: Direct Supervised Partition (DSP) and Indirect Supervised Partition (ISP). The DSP method uses a decision tree to partition the data based on learning curves, while ISP leverages the dataset's geometric and target distribution characteristics. An effective validation protocol is developed to determine the optimal number of early stopping regions or detect when the heterogeneity assumption does not hold. Experiments using state-of-the-art implementations of Gradient Boosting, LightGBM, and CatBoost, on standard benchmarks demonstrate that our methods enhance model precision by up to 2%, underscoring the significance of this research direction. This approach does not increase computational complexity and can be easily integrated into existing learning pipelines.|梯度提升是一种领先的学习方法，它可以构建集合，并根据特定任务调整其大小，从而在不同的应用程序中始终保持顶级结果。然而，确定最佳数量的模型在集合仍然是一个关键的，但未充分开发的方面。传统方法假设对所有数据点都有效的通用集合大小，但由于数据异构性，这种假设可能并不总是成立。本文介绍了一种自适应方法来提早停止在梯度提升，解决数据异质性分配不同的停止时刻到不同的数据区域在推断时间，同时仍然训练一个共同的集合在整个数据集。我们提出了两种方法: 直接监督分区(DSP)和间接监督分区(ISP)。DSP 方法利用决策树根据学习曲线对数据进行划分，ISP 则利用数据集的几何特征和目标分布特征。提出了一种有效的验证协议，以确定最佳数目的早期停止区域或检测时，异质性假设不成立。在标准基准上使用最先进的梯度提升、 LightGBM 和 CatBoost 实现的实验表明，我们的方法提高了模型精度2% ，强调了这一研究方向的重要性。这种方法不会增加计算的复杂性，并且可以很容易地集成到现有的学习管道中。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learn+Together+Stop+Apart:+An+Inclusive+Approach+to+Ensemble+Pruning)|0|
|[Efficient Discovery of Time Series Motifs under both Length Differences and Warping](https://doi.org/10.1145/3637528.3671726)|Makoto Imamura, Takaaki Nakamura|Tokai University, Minato-ku, Tokyo, Japan; Mitsubishi Electric Corporation, Kamakura, Japan|Over the past two decades, time series motif discovery has become a crucial subroutine for many time series data mining tasks; concurrently, it has been established that Dynamic Time Warping (DTW) outperforms other similarity measures like Euclidean Distance in most scenarios. Against this backdrop, a DTW motif discovery algorithm was recently developed; however, it is confined to working with fixed-length subsequences. In this work, we propose a novel approach that allows us to find motifs under both length differences and warping. Our algorithm exploits a promising time series representation called Spikelets and introduces the first lower bound for DTW in the Spikelet space. Extensive empirical studies demonstrate that our method scales effectively across various real-world datasets and efficiently identifies DTW motif pairs of different lengths.|在过去的二十年里，时间序列主题发现已经成为许多时间序列数据挖掘任务的一个重要子程序，同时，在大多数情况下，动态时间规整(dtW)的表现优于其他相似性度量，比如欧几里得度量。在这种背景下，一个 DTW 模式发现算法最近被开发出来，但是它仅限于处理固定长度的子序列。在这项工作中，我们提出了一个新颖的方法，让我们找到图案的长度差异和翘曲。我们的算法利用了一个很有前途的时间序列表示叫小穗，并引入了 DTW 在小穗空间的第一个下界。大量的实证研究表明，我们的方法可以有效地跨越各种真实世界的数据集，并有效地识别不同长度的 DTW 主题对。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Discovery+of+Time+Series+Motifs+under+both+Length+Differences+and+Warping)|0|
|[Promoting Fairness and Priority in Selecting k-Winners Using IRV](https://doi.org/10.1145/3637528.3671735)|Md Mouinul Islam, Soroush Vahidi, Baruch Schieber, Senjuti Basu Roy|CS, NJIT, Newark, NJ, USA|We investigate the problem of finding winner(s) given a large number of users' (voters') preferences casted as ballots, one from each of the m users, where each ballot is a ranked order of preference of up to ℓ out of n items (candidates). Given a group protected attribute with k different values and a priority that imposes a selection order among these groups, the goal is to satisfy the priority order and select a winner per group that is most representative. It is imperative that at times the original users' preferences may require further manipulation to meet these fairness and priority requirement. We consider manipulation by modifications and formalize the margin finding problem under modification problem. We study the suitability of Instant Run-off Voting (IRV) as a preference aggregation method and demonstrate its advantages over positional methods. We present a suite of technical results on the hardness of the problem, design algorithms with theoretical guarantees and further investigate efficiency opportunities. We present exhaustive experimental evaluations using multiple applications and large-scale datasets to demonstrate the effectiveness of IRV, and efficacy of our designed solutions qualitatively and scalability-wise.|我们研究的问题，找到获胜者给出了大量的用户的(选民的)偏好投下的选票，一个从每个 m 用户，其中每张选票是一个排名的优先顺序多达1个项目(候选人)。给定一个具有 k 不同值的群体保护属性和一个在这些群体之间强加选择顺序的优先级，目标是满足优先级顺序并为每个群体选择最具代表性的赢家。当务之急是，有时原始用户的首选项可能需要进一步的操作，以满足这些公平性和优先级要求。我们考虑修改操作，并将修改问题下的边界寻找问题形式化。研究了即时决选投票(IRV)作为偏好聚合方法的适用性，并论证了其相对于位置方法的优越性。我们提出了一套技术结果的问题的难度，设计算法与理论保证，并进一步研究效率的机会。我们使用多个应用程序和大规模数据集进行详尽的实验评估，以证明 IRV 的有效性，以及我们设计的解决方案在定性和可扩展性方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Promoting+Fairness+and+Priority+in+Selecting+k-Winners+Using+IRV)|0|
|[FreQuant: A Reinforcement-Learning based Adaptive Portfolio Optimization with Multi-frequency Decomposition](https://doi.org/10.1145/3637528.3671668)|Jihyeong Jeon, Jiwon Park, Chanhee Park, U Kang|Seoul National University & DeepTrade Technologies Inc., Seoul, Republic of Korea; Seoul National University, Seoul, Republic of Korea|How can we leverage inherent frequency features of stock signals for effective portfolio optimization? Portfolio optimization in the domain of finance revolves around strategically allocating assets to maximize returns. Recent advancements highlight the efficacy of deep learning and reinforcement learning (RL) in capturing temporal asset patterns for portfolio optimization. However, previous methodologies focusing on time-domain often fail to detect sudden market shifts and abrupt events because their models are overly tailored to prevalent patterns, resulting in significant losses. In this paper, we propose FreQuant (Adaptive Portfolio Optimization via Multi-Frequency Quantitative Analysis), an effective deep RL framework for portfolio optimization that fully operates in the frequency domain, tackling the limitations of time domain-focused models. By bringing the analysis into the frequency domain with the Discrete Fourier Transform, our framework captures both prominent and subtle market frequencies, enhancing its adaptability and stability in response to market shifts. This approach allows FreQuant to adeptly identify primary asset patterns while also effectively responding to less common and abrupt market events, providing a more accurate and comprehensive asset representation. Empirical validation on diverse real-world trading datasets underscores the remarkable performance of FreQuant, showing its superiority in terms of profitability. Notably, FreQuant achieves up to 2.1x higher Annualized Rate of Return and 2.9x higher Portfolio Value than the best-performing competitors.|我们如何利用股票信号固有的频率特征进行有效的投资组合优化？金融领域的投资组合优化围绕着战略性地配置资产以实现收益最大化。最近的进展突出了深度学习和强化学习(RL)在捕捉投资组合优化的时间资产模式方面的功效。然而，以往侧重于时域的方法往往无法发现突发市场变化和突发事件，因为他们的模型过于适应流行的模式，导致重大损失。本文提出了一种基于频域的自适应投资组合优化框架 FreQuant，该框架能够有效地解决以时域为中心的投资组合优化模型的局限性。透过把分析结果引入频率范畴，我们的架构能捕捉到市场显著和微妙的频率离散傅里叶变换，加强其应变能力和稳定性，以应付市场转变。这种方法使 FreQuant 能够熟练地识别主要的资产模式，同时也能有效地应对不太常见和突然的市场事件，提供更准确和全面的资产表示。对多样化的实际交易数据集的实证验证强调了 FreQuant 的显著性能，显示了其在盈利能力方面的优势。值得注意的是，与业绩最好的竞争对手相比，FreQuant 的年收益率高出2.1倍，投资组合价值高出2.9倍。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FreQuant:+A+Reinforcement-Learning+based+Adaptive+Portfolio+Optimization+with+Multi-frequency+Decomposition)|0|
|[Addressing Prediction Delays in Time Series Forecasting: A Continuous GRU Approach with Derivative Regularization](https://doi.org/10.1145/3637528.3671969)|Sheo Yon Jhin, Seojin Kim, Noseong Park|KAIST, Daejeon, Republic of Korea; Yonsei University, Seoul, Seodaemun-gu, Republic of Korea|Time series forecasting has been an essential field in many different application areas, including economic analysis, meteorology, and so forth. The majority of time series forecasting models are trained using the mean squared error (MSE). However, this training based on MSE causes a limitation known as prediction delay. The prediction delay, which implies the ground-truth precedes the prediction, can cause serious problems in a variety of fields, e.g., finance and weather forecasting --- as a matter of fact, predictions succeeding ground-truth observations are not practically meaningful although their MSEs can be low. This paper proposes a new perspective on traditional time series forecasting tasks and introduces a new solution to mitigate the prediction delay. We introduce a continuous-time gated recurrent unit (GRU) based on the neural ordinary differential equation (NODE) which can supervise explicit time-derivatives. We generalize the GRU architecture in a continuous-time manner and minimize the prediction delay through our time-derivative regularization. Our method outperforms in metrics such as MSE, Dynamic Time Warping (DTW) and Time Distortion Index (TDI). In addition, we demonstrate the low prediction delay of our method in a variety of datasets.|时间序列预测已成为经济分析、气象学等许多不同应用领域的重要研究内容。大多数时间序列预测模型都是使用均方差(MSE)进行训练的。然而，这种基于最小均方误差(MSE)的训练导致了一种称为预测延迟的限制。预测延迟意味着地面事实先于预测，可能会在各个领域引起严重问题，例如金融和天气预报——事实上，接着地面事实观测的预测实际上没有意义，尽管它们的微观经济指标可能较低。本文对传统的时间序列预测任务提出了一个新的视角，并介绍了一种减小预测延迟的新方法。我们引入了一个基于神经元常微分方程的连续时间门控递归单元(GRU) ，它可以监控显式的时间导数。我们以连续时间方式推广 GRU 结构，并通过时间导数正则化使预测延迟最小化。我们的方法在 MSE、动态时间规整(DTW)和时间扭曲指数(tDI)等度量指标上表现优异。此外，我们在各种数据集中展示了我们的方法的低预测延迟。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Addressing+Prediction+Delays+in+Time+Series+Forecasting:+A+Continuous+GRU+Approach+with+Derivative+Regularization)|0|
|[MemMap: An Adaptive and Latent Memory Structure for Dynamic Graph Learning](https://doi.org/10.1145/3637528.3672060)|Shuo Ji, Mingzhe Liu, Leilei Sun, Chuanren Liu, Tongyu Zhu|; The University of Tennessee, Knoxville, TN, USA; CCSE Lab, Beihang University, Beijing, China|Dynamic graph learning has attracted much attention in recent years due to the fact that most of the real-world graphs are dynamic and evolutionary. As a result, many dynamic learning methods have been proposed to cope with the changes of node states over time. Among these studies, a critical issue is how to update the representations of nodes when new temporal events are observed. In this paper, we provide a novel memory structure - Memory Map (MemMap) for this problem. MemMap is an adaptive and evolutionary latent memory space, where each cell corresponds to an evolving "topic" of the dynamic graph. Moreover, the representation of a node is generated from its semantically correlated memory cells, rather than linked neighbors of the node. We have conducted experiments on real-world datasets and compared our method with the SOTA ones. It can be concluded that: 1) By constructing an adaptive and evolving memory structure during the dynamic learning process, our method can capture the dynamic graph changes, and the learned MemMap is actually a compact evolving structure organized according to the latent "topics" of the graph nodes. 2) Our research suggests that it is a more effective and efficient way to generate node representations from a latent semantic space (like MemMap in our method) than from directly connected neighbors (like most of the previous graph learning methods). The reason is that the number of memory cells in latent space could be much smaller than the number of nodes in a real-world graph, and the representation learning process could well balance the global and local message passing by leveraging the semantic similarity of graph nodes via the correlated memory cells.|由于现实世界中的大多数图都是动态演化的，因此动态图学习近年来引起了人们的广泛关注。因此，人们提出了许多动态学习方法来应对节点状态随时间的变化。在这些研究中，一个关键问题是如何在观察到新的时间事件时更新节点的表示。本文针对这一问题提出了一种新的内存结构——内存映射(MemMap)。MemMap 是一个自适应和进化的潜在记忆空间，其中每个单元对应于动态图的一个进化的“主题”。此外，节点的表示是由其语义相关的存储单元生成的，而不是节点的链接邻居。在实际数据集上进行了实验，并与 SOTA 方法进行了比较。研究结果表明: 1)在动态学习过程中，通过构造一个自适应的演化记忆结构，我们的方法可以捕捉到动态图形的变化，而所学习的 MemMap 实际上是根据图形节点的潜在“主题”组织起来的一个紧凑的演化结构。2)我们的研究表明，从潜在的语义空间(如 MemMap 在我们的方法)生成节点表示比从直接相连的邻居(如大多数以前的图学习方法)更有效和有效。其原因是潜在空间中的记忆单元数量可能远小于现实世界图中的节点数量，表示学习过程通过相关记忆单元利用图中节点的语义相似性，可以很好地平衡全局和局部信息的传递。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MemMap:+An+Adaptive+and+Latent+Memory+Structure+for+Dynamic+Graph+Learning)|0|
|[Tensorized Unaligned Multi-view Clustering with Multi-scale Representation Learning](https://doi.org/10.1145/3637528.3671689)|Jintian Ji, Songhe Feng, Yidong Li||The Unaligned Multi-view Clustering (UMC) problem is currently receiving widespread attention, focusing on clustering unaligned multi-view data generated in real-world applications. Although some algorithms have emerged to address this issue, there still exist the following drawbacks: 1) The fully unknown correspondence of samples across views can significantly limit the exploration of consistent clustering structure. 2) The fixed representation space makes it difficult to mine the comprehensive information in the original data. 3) Unbiased tensor rank approximation is desired to capture the high-order correlation among different views. To address these issues, we proposed a novel UMC framework termed Tensorized Unaligned Multi-view Clustering with Multi-scale Representation Learning (TUMCR). Specifically, TUMCR designs a multi-scale representation learning and alignment framework, which constructs multi-scale representation spaces to comprehensively explore the unknown correspondence across views. Then, a tensorial multi-scale fusion module is proposed to fuse multi-scale representations and explore the high-order correlation hidden in different views, which utilizes the Enhanced Tensor Rank (ETR) to learn the low-rank structure. Furthermore, TUMCR is solved by an efficient algorithm with good convergence. Extensive experiments on different types of datasets demonstrate the effectiveness and superiority of our TUMCR compared with state-of-the-art methods. Our code is publicly available at: https://github.com/jijintian/TUMCR.|不对齐多视图聚类(UMC)问题目前受到广泛关注，主要集中在对现实应用程序中生成的不对齐多视图数据进行聚类。尽管已经出现了一些算法来解决这个问题，但是仍然存在以下缺陷: 1)视图间样本的完全未知对应性会严重限制对一致性聚类结构的探索。2)固定的表示空间使得对原始数据中的综合信息进行挖掘变得困难。3)无偏张量秩近似用于捕捉不同视图之间的高阶相关性。为了解决这些问题，我们提出了一个新的 UMC 框架，称为张量不对齐多视图聚类与多尺度表示学习(TUMCR)。具体来说，TUMCR 设计了一个多尺度表示学习和对齐框架，该框架构造了多尺度表示空间来全面探索视图之间的未知对应关系。然后，提出一种张量多尺度融合模块，利用增强张量秩(ETR)学习低阶结构，融合多尺度表示，探索隐藏在不同视图中的高阶相关性。此外，本文还提出了一种收敛性好的 TUMCR 算法。在不同类型的数据集上进行的大量实验表明，与最先进的方法相比，我们的 TUMCR 方法具有有效性和优越性。我们的代码可以在以下 https://github.com/jijintian/tumcr 公开获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tensorized+Unaligned+Multi-view+Clustering+with+Multi-scale+Representation+Learning)|0|
|[Killing Two Birds with One Stone: Cross-modal Reinforced Prompting for Graph and Language Tasks](https://doi.org/10.1145/3637528.3671742)|Wenyuan Jiang, Wenwei Wu, Le Zhang, Zixuan Yuan, Jian Xiang, Jingbo Zhou, Hui Xiong|; Baidu Research, Baidu Inc., Beijing, China|In recent years, Graph Neural Networks (GNNs) and Large Language Models (LLMs) have exhibited remarkable capability in addressing different graph learning and natural language tasks, respectively. Motivated by this, integrating LLMs with GNNs has been increasingly studied to acquire transferable knowledge across modalities, which leads to improved empirical performance in language and graph domains. However, existing studies mainly focused on a single-domain scenario by designing complicated integration techniques to manage multimodal data effectively. Therefore, a concise and generic learning framework for multi-domain tasks, i.e., graph and language domains, is highly desired yet remains under-exploited due to two major challenges. First, the language corpus of downstream tasks differs significantly from graph data, making it hard to bridge the knowledge gap between modalities. Second, not all knowledge demonstrates immediate benefits for downstream tasks, potentially introducing disruptive noise to context-sensitive models like LLMs. To tackle these challenges, we propose a novel plug-and-play framework for incorporating a lightweight cross-domain prompting method into both language and graph learning tasks. Specifically, we first convert the textual input into a domain-scalable prompt, which not only preserves the semantic and logical contents of the textual input, but also highlights related graph information as external knowledge for different domains. Then, we develop a reinforcement learning-based method to learn the optimal edge selection strategy for useful knowledge extraction, which profoundly sharpens the multi-domain model capabilities. In addition, we introduce a joint multi-view optimization module to regularize agent-level collaborative learning across two domains. Finally, extensive empirical justifications over 23 public and synthetic datasets demonstrate that our approach can be applied to diverse multi-domain tasks more accurately, robustly, and reasonably, and improve the performances of the state-of-the-art graph and language models in different learning paradigms.|近年来，图形神经网络(GNN)和大语言模型(LLM)分别在处理不同的图形学习和自然语言任务方面表现出了显著的能力。基于此，人们越来越多地研究将 LLM 与 GNN 相结合，以获得跨模式的可转移知识，从而提高语言和图形领域的经验性能。然而，现有的研究主要集中在单一领域的情况下，通过设计复杂的集成技术来有效地管理多模态数据。因此，一个简洁和通用的多领域任务学习框架，即图形和语言领域，是非常理想的，但仍然没有得到充分利用，由于两个主要的挑战。首先，下游任务的语言语料与图形数据存在显著差异，使得模式之间的知识差距难以弥合。其次，并非所有的知识都能为下游任务带来立竿见影的好处，这可能会给 LLM 等上下文敏感的模型带来干扰噪声。为了应对这些挑战，我们提出了一种新的即插即用框架，将一种轻量级的跨领域提示方法融入到语言和图形学习任务中。具体来说，我们首先将文本输入转换成一个域可伸缩的提示，它不仅保留了文本输入的语义和逻辑内容，而且突出了相关的图形信息作为不同领域的外部知识。然后，我们提出了一种基于强化学习的方法来学习有用知识提取的最优边缘选择策略，从而深刻地提高了多领域模型的能力。此外，我们还引入了一个联合的多视图优化模块来规范代理级别的合作学习。最后，通过对23个公共和合成数据集的大量实证分析表明，我们的方法可以更加准确、稳健和合理地应用于不同的多领域任务，并在不同的学习范式下提高最先进的图形和语言模型的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Killing+Two+Birds+with+One+Stone:+Cross-modal+Reinforced+Prompting+for+Graph+and+Language+Tasks)|0|
|[Sketch-Based Replay Projection for Continual Learning](https://doi.org/10.1145/3637528.3671714)|Jack Julian, Yun Sing Koh, Albert Bifet|AI Institute, The University of Waikato & LTCI, Télécom Paris, IP Paris, Hamilton, New Zealand; School of Computer Science, University of Auckland, Auckland, New Zealand|Continual learning closely emulates human learning, which allows a model to learn from a stream of tasks sequentially without forgetting previously learned knowledge. Replay-based continual learning methods mitigate forgetting and improve performance by reintroducing data belonging to old tasks, however a replay method's performance may deteriorate when the reintroduced data does not effectively represent all experienced data. To address this concern, we propose the Sketch-based Replay Projection (SRP) method to capture and retain the original data stream's distribution within stored memory. SRP augments existing replay frameworks and introduces a two-fold approach. First, we develop a sketch-based sample selection technique to approximate feature distributions within distinct tasks, thereby capturing a wide distribution of examples for subsequent replay. Second, we propose a data compression method which projects examples into a reduced-dimensional space while preserving inter-example relationships and emphasizing inter-class disparities, encouraging diverse representations of each class while maintaining memory requirements similar to existing replay methodologies. Our experimental results demonstrate that SRP enhances replay diversity and improves the performance of existing replay models.|持续学习非常接近人类学习，它允许模型从一系列任务中顺序学习，而不会忘记先前学过的知识。基于重放的连续学习方法通过重新引入属于旧任务的数据来缓解遗忘和提高性能，但是当重新引入的数据不能有效地表示所有经验数据时，重放方法的性能可能会恶化。为了解决这个问题，我们提出了基于草图的重放投影(SRP)方法来捕获和保留原始数据流在存储器中的分布。SRP 扩展了现有的重播框架，并引入了双重方法。首先，我们开发了一个基于草图的样本选择技术，以近似特征分布在不同的任务，从而捕获广泛分布的例子，以便随后的重播。其次，我们提出了一种数据压缩方法，它将例子投射到一个简化的维度空间中，同时保持例子间的关系，强调类间的差异，鼓励每个类的不同表示，同时保持与现有重播方法类似的记忆需求。实验结果表明，SRP 增强了重放分集，提高了现有重放模型的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sketch-Based+Replay+Projection+for+Continual+Learning)|0|
|[RCTD: Reputation-Constrained Truth Discovery in Sybil Attack Crowdsourcing Environment](https://doi.org/10.1145/3637528.3671803)|Xing Jin, Zhihai Gong, Jiuchuan Jiang, Chao Wang, Jian Zhang, Zhen Wang|; School of Cyberspace, Hangzhou Dianzi University, Hangzhou, Zhejiang, China; Research Center for Data Hub and Security, Zhejiang Lab, Hangzhou, Zhejiang, China|Sybil attacks are a prevalent concern within the realm of crowdsourcing, underscoring the significance of quality control in this domain. Truth discovery has been extensively studied to deduce the most trustworthy information from conflicting data based on the principle that reliable workers yield reliable answers. However, existing truth discovery approaches overlook the metric of workers' reputations, e.g., workers' historical approval rates on crowdsourcing platforms, despite being inflated and noisy, they offer a rough indication of workers' ability. In this paper, we first refine the approval rate using Wilson Lower Bound to enhance its confidence, and then mitigate its noise and inflation through a method based on ranking similarity. Specifically, we propose a method called RCTD (Reputation-Constrained Truth Discovery), which introduces a similarity metric between the rankings of workers' weights and the refined approval rates. This metric serves as a penalizing factor in the objective function of the truth discovery, restricting workers' weights to avoid excessively deviating from their historical reputation during the weight estimation process. We solve the objective function by introducing the block coordinate descent coupled with heuristics approach method. Experimental results on real-world datasets demonstrate that our approach achieves more accurate inference of true results in the Sybil attack environment compared to the state-of-the-art methods.|Sybil 攻击是众包领域的一个普遍关注的问题，强调了质量控制在这个领域的重要性。真相发现已被广泛研究，以从相互矛盾的数据中推断出最可信的信息，其基本原理是可靠的工作人员提供可靠的答案。然而，现有的真相发现方法忽视了员工声誉的度量，例如，员工在众包平台上的历史认可率，尽管夸大和喧闹，他们提供了员工能力的粗略指标。本文首先利用威尔逊下界细化批准率，以提高其置信度，然后通过一种基于排序相似度的方法来降低其噪声和通货膨胀。具体来说，我们提出了一种名为声誉约束真相发现(RCTD)的方法，该方法引入了工人权重排名和精确批准率之间的相似度量。这个度量标准在真相发现的客观功能中起着惩罚性因素的作用，它限制了工人的权重，以避免在权重估计过程中过度偏离他们的历史声誉。我们通过引入块坐标下降法和启发式方法来解决目标函数。在真实数据集上的实验结果表明，与现有的方法相比，该方法在 Sybil 攻击环境下能够更准确地推断出真实结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RCTD:+Reputation-Constrained+Truth+Discovery+in+Sybil+Attack+Crowdsourcing+Environment)|0|
|[Bivariate Decision Trees: Smaller, Interpretable, More Accurate](https://doi.org/10.1145/3637528.3671903)|Rasul Kairgeldin, Miguel Á. CarreiraPerpiñán|University of California, Merced, Merced, CA, USA|Univariate decision trees, commonly used since the 1950s, predict by asking questions about a single feature in each decision node. While they are interpretable, they often lack competitive predictive accuracy due to their inability to model feature correlations. Multivariate (oblique) trees use multiple features in each node, capturing high-dimensional correlations better, but sometimes they can be difficult to interpret. We advocate for a model that strikes a useful middle ground: bivariate decision trees, which use two features in each node. This typically produces trees that not only are more accurate than univariate trees, but much smaller, which offsets the small increase in node complexity and keeps them interpretable. They also help data mining by constructing new features that are useful for discrimination, and by providing a form of supervised, hierarchical 2D visualization that reveals patterns such as clusters or linear structure. We give two new algorithms to learn bivariate trees: a fast one based on CART; and a slower one based on alternating optimization with a feature regularization term, which produces the best trees while still scaling to large datasets.|单变量决策树，自20世纪50年代以来被广泛使用，通过询问每个决策节点中单个特征的问题来进行预测。虽然它们是可解释的，但由于它们无法建立特征相关性模型，它们往往缺乏具有竞争力的预测准确性。多变量(斜)树在每个节点中使用多个特征，可以更好地捕获高维相关性，但有时它们可能很难解释。我们提倡一个有用的中间地带的模型: 二元决策树，它在每个节点中使用两个特征。这通常产生的树不仅比单变量树更精确，而且要小得多，这抵消了节点复杂度的小幅增加，并保持了它们的可解释性。它们还有助于数据挖掘，因为它们构建了有助于区分的新特征，并提供了一种受监督的、分层的二维可视化形式，揭示了诸如聚类或线性结构之类的模式。本文提出了两种新的二元树学习算法: 基于 CART 的快速算法和基于特征正则项交替优化的慢速算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bivariate+Decision+Trees:+Smaller,+Interpretable,+More+Accurate)|0|
|[CAFO: Feature-Centric Explanation on Time Series Classification](https://doi.org/10.1145/3637528.3671724)|Jaeho Kim, SeokJu Hahn, Yoontae Hwang, Junghye Lee, Seulki Lee||In multivariate time series (MTS) classification, finding the important features (e.g., sensors) for model performance is crucial yet challenging due to the complex, high-dimensional nature of MTS data, intricate temporal dynamics, and the necessity for domain-specific interpretations. Current explanation methods for MTS mostly focus on time-centric explanations, apt for pinpointing important time periods but less effective in identifying key features. This limitation underscores the pressing need for a feature-centric approach, a vital yet often overlooked perspective that complements time-centric analysis. To bridge this gap, our study introduces a novel feature-centric explanation and evaluation framework for MTS, named CAFO (Channel Attention and Feature Orthgonalization). CAFO employs a convolution-based approach with channel attention mechanisms, incorporating a depth-wise separable channel attention module (DepCA) and a QR decomposition-based loss for promoting feature-wise orthogonality. We demonstrate that this orthogonalization enhances the separability of attention distributions, thereby refining and stabilizing the ranking of feature importance. This improvement in feature-wise ranking enhances our understanding of feature explainability in MTS. Furthermore, we develop metrics to evaluate global and class-specific feature importance. Our framework's efficacy is validated through extensive empirical analyses on two major public benchmarks and real-world datasets, both synthetic and self-collected, specifically designed to highlight class-wise discriminative features. The results confirm CAFO's robustness and informative capacity in assessing feature importance in MTS classification tasks. This study not only advances the understanding of feature-centric explanations in MTS but also sets a foundation for future explorations in feature-centric explanations. The codes are available at https://github.com/eai-lab/CAFO.|在多变量时间序列(MTS)分类中，由于 MTS 数据的复杂，高维性质，复杂的时间动态以及特定领域解释的必要性，寻找模型性能的重要特征(例如传感器)是至关重要的，但具有挑战性。目前 MTS 的解释方法主要集中在以时间为中心的解释上，易于确定重要的时间段，但在识别关键特征方面效率较低。这种局限性强调了以功能为中心的方法的迫切需要，这是一个重要但经常被忽视的视角，补充了以时间为中心的分析。为了弥补这一差距，我们的研究引入了一个新的以特征为中心的 MTS 解释和评估框架，称为 CAFO (渠道注意力和特征正交化)。CAFO 采用基于卷积的方法和信道注意机制，结合基于深度的可分离信道注意模块(DepCA)和基于 QR 分解的损失来提高特征方向的正交性。我们证明这种正交化增强了注意力分布的可分性，从而改善和稳定了特征重要性的排名。这种特征方面排名的改进增强了我们对 MTS 中特征可解释性的理解。此外，我们开发度量来评估全局和类别特定的特征重要性。我们的框架的有效性是通过对两个主要的公共基准和真实世界的数据集进行广泛的实证分析来验证的，这些数据集包括合成的和自我收集的，专门设计用来突出类别的区分特征。结果证实了 CAFO 在 MTS 分类任务中评估特征重要性的鲁棒性和信息能力。本研究不仅提高了对 MTS 中以特征为中心的解释的理解，而且为今后进一步探索以特征为中心的解释奠定了基础。密码可以在 https://github.com/eai-lab/cafo 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CAFO:+Feature-Centric+Explanation+on+Time+Series+Classification)|0|
|[Gandalf: Learning Label-label Correlations in Extreme Multi-label Classification via Label Features](https://doi.org/10.1145/3637528.3672063)|Siddhant Kharbanda, Devaansh Gupta, Erik Schultheis, Atmadeep Banerjee, ChoJui Hsieh, Rohit Babbar|University of California, Los Angeles, Los Angeles, USA; Aalto University, Espoo, Finland; Aalto University & University of Bath, Espoo, Finland|Extreme Multi-label Text Classification (XMC) involves learning a classifier that can assign an input with a subset of most relevant labels from millions of label choices. Recent works in this domain have increasingly focused on a symmetric problem setting where both input instances and label features are short-text in nature. Short-text XMC with label features has found numerous applications in areas such as query-to-ad-phrase matching in search ads, title-based product recommendation, prediction of related searches. In this paper, we propose Gandalf, a novel approach which makes use of a label co-occurrence graph to leverage label features as additional data points to supplement the training distribution. By exploiting the characteristics of the short-text XMC problem, it leverages the label features to construct valid training instances, and uses the label graph for generating the corresponding soft-label targets, hence effectively capturing the label-label correlations. Surprisingly, models trained on these new training instances, although being less than half of the original dataset, can outperform models trained on the original dataset, particularly on the PSP@k metric for tail labels. With this insight, we aim to train existing XMC algorithms on both, the original and new training instances, leading to an average 5% relative improvements for 6 state-of-the-art algorithms across 4 benchmark datasets consisting of up to 1.3M labels. Gandalf can be applied in a plug-and-play manner to various methods and thus forwards the state-of-the-art in the domain, without incurring any additional computational overheads. Code has been open-sourced at www.github.com/xmc-aalto/InceptionXML.|极限多标签文本分类(XMC)涉及到学习一个分类器，该分类器可以从数百万个标签选择中分配一个输入，其中包含最相关标签的子集。最近在这个领域的工作越来越集中在一个对称的问题设置，其中输入实例和标签功能都是短文本性质。具有标签功能的短文 XMC 在搜索广告中的查询到广告短语匹配、基于标题的产品推荐、相关搜索预测等领域有着广泛的应用。在本文中，我们提出了一种新的方法，使用标签共现图利用标签特征作为额外的数据点，以补充训练分布。利用短文本 XMC 问题的特点，利用标签特征构造有效的训练实例，并利用标签图生成相应的软标签目标，从而有效地捕获标签-标签的相关性。令人惊讶的是，在这些新的训练实例上训练的模型，尽管不到原始数据集的一半，可以胜过在原始数据集上训练的模型，特别是在用于尾部标签的 PSP@k 度量上。有了这种洞察力，我们的目标是在原始和新的训练实例上对现有的 XMC 算法进行训练，从而使4个基准数据集(包括多达130万个标签)中的6个最先进的算法平均相对改进5% 。甘道夫可以以即插即用的方式应用于各种方法，从而在不引起任何额外计算开销的情况下，转发该领域的最新技术。代码已经在 www.github.com/xmc-aalto/inceptionxml 上开源。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Gandalf:+Learning+Label-label+Correlations+in+Extreme+Multi-label+Classification+via+Label+Features)|0|
|[OntoType: Ontology-Guided and Pre-Trained Language Model Assisted Fine-Grained Entity Typing](https://doi.org/10.1145/3637528.3671745)|Tanay Komarlu, Minhao Jiang, Xuan Wang, Jiawei Han|University of Illinois Urbana-Champaign, Urbana, IL, USA; Virginia Tech, Blacksburg, VA, USA|Fine-grained entity typing (FET), which assigns entities in text withcontext-sensitive, fine-grained semantic types, is a basic but important taskfor knowledge extraction from unstructured text. FET has been studiedextensively in natural language processing and typically relies onhuman-annotated corpora for training, which is costly and difficult to scale.Recent studies explore the utilization of pre-trained language models (PLMs) asa knowledge base to generate rich and context-aware weak supervision for FET.However, a PLM still requires direction and guidance to serve as a knowledgebase as they often generate a mixture of rough and fine-grained types, ortokens unsuitable for typing. In this study, we vision that an ontologyprovides a semantics-rich, hierarchical structure, which will help select thebest results generated by multiple PLM models and head words. Specifically, wepropose a novel annotation-free, ontology-guided FET method, OntoType, whichfollows a type ontological structure, from coarse to fine, ensembles multiplePLM prompting results to generate a set of type candidates, and refines itstype resolution, under the local context with a natural language inferencemodel. Our experiments on the Ontonotes, FIGER, and NYT datasets using theirassociated ontological structures demonstrate that our method outperforms thestate-of-the-art zero-shot fine-grained entity typing methods as well as atypical LLM method, ChatGPT. Our error analysis shows that refinement of theexisting ontology structures will further improve fine-grained entity typing.|细粒度实体分类(FET)是从非结构化文本中提取知识的一项基本而重要的任务，它为文本中的实体分配上下文敏感的细粒度语义类型。FET 在自然语言处理领域得到了广泛的研究，它通常依赖于人工注释的语料库进行训练，这种训练成本高昂，而且难以扩展。最近的研究探索利用预先训练的语言模型(PLM)作为知识库，为 FEM 产生丰富的和上下文感知的弱监督。然而，PLM 仍然需要方向和指导作为知识库，因为它们经常产生粗糙和细粒度类型的混合物，或者不适合打字的标记。在这项研究中，我们设想一个本体提供了一个语义丰富的层次结构，这将有助于选择由多个 PLM 模型和头文字产生的最佳结果。具体而言，我们提出了一种新的无注释，本体引导的 FET 方法 OntoType，它遵循从粗到细的类型本体结构，将 multiplePLM 集成在一起，提示结果以生成一组类型候选者，并使用自然语言推理模型在局部上下文中改进其类型分辨率。我们在 Ontonote，FIGER 和 NYT 数据集上使用其相关的本体结构的实验表明，我们的方法优于最先进的零拍细粒度实体分类方法以及非典型 LLM 方法 ChatGPT。我们的误差分析表明，细化现有的本体结构将进一步改善细粒度的实体分类。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OntoType:+Ontology-Guided+and+Pre-Trained+Language+Model+Assisted+Fine-Grained+Entity+Typing)|0|
|[LeMon: Automating Portrait Generation for Zero-Shot Story Visualization with Multi-Character Interactions](https://doi.org/10.1145/3637528.3671850)|Ziyi Kou, Shichao Pei, Xiangliang Zhang|Department of Computer Science, University of Massachusetts Boston, Boston, MA, USA; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA|Zero-Shot Story Visualization (ZSV) seeks to depict textual narratives through a sequence of images without relying on pre-existing text-image pairs for training. In this paper, we address the challenge of automated multi-character ZSV, aiming to create distinctive yet compatible character portraits for high-quality story visualization without the need of manual human interventions. Our study is motivated by the limitation of current ZSV approaches that necessitate inefficient manual collection of external images as initial character portraits and suffer from low-quality story visualization, especially with multi-character interactions, when the portraits are not well initiated. To overcome these issues, we develop LeMon, an LLM enhanced Multi-Character Zero-Shot Visualization framework that automates character portrait initialization and supports iterative portrait refinement by exploring the semantic content of the story. In particular, we design an LLM-based portrait generation strategy that matches the story characters with external movie characters, and leverage the matched resources as in-context learning (ICL) samples for LLMs to accurately initialize the character portraits. We then propose a graph-based Text2Image diffusion model that constructs a character interaction graph from the story to iteratively refine the character portraits by maximizing the distinctness of different characters while minimizing their incompatibility in the multi-character story visualization. Our evaluation results show that LeMon outperforms existing ZSV approaches in generating high-quality visualizations for stories across various types with multiple interacted characters. Our code is available at https://github.com/arxrean/LLM-LeMon.|零镜头故事可视化(Zero-Shot Story Visualization，ZSV)旨在通过一系列图像描述文本叙事，而不依赖于已有的文本-图像对进行训练。在本文中，我们解决的挑战，自动多字符 ZSV，旨在创建独特而兼容的人物肖像高质量的故事可视化不需要人工干预。我们的研究是受到当前 ZSV 方法的限制，这种方法需要低效的手工收集外部图像作为初始人物肖像，并且受到低质量故事可视化的影响，特别是在多人物交互的情况下，当肖像没有很好地启动时。为了克服这些问题，我们开发了 LeMon，一个 LLM 增强的多字符零拍摄可视化框架，它自动化人物肖像的初始化，并通过探索故事的语义内容支持迭代肖像细化。特别地，我们设计了一个基于 LLM 的肖像生成策略，将故事人物与外部电影人物进行匹配，并利用匹配的资源作为上下文学习(In-context learning，ICL)样本对 LLM 进行准确的人物肖像初始化。然后我们提出了一个基于图形的 Text2Image 扩散模型，该模型从故事中构建一个人物交互图，通过在多人物故事可视化中最大化不同人物的差异性，同时最小化他们的不兼容性，迭代地细化人物肖像。我们的评估结果表明，LeMon 优于现有的 ZSV 方法，可以为不同类型、多个交互角色的故事生成高质量的可视化效果。我们的代码可以在 https://github.com/arxrean/llm-lemon 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LeMon:+Automating+Portrait+Generation+for+Zero-Shot+Story+Visualization+with+Multi-Character+Interactions)|0|
|[Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Leman Go Indifferent](https://doi.org/10.1145/3637528.3671890)|Lorenz Kummer, Samir Moustafa, Sebastian Schrittwieser, Wilfried N. Gansterer, Nils M. Kriege|; Faculty of Computer Science, University of Vienna, Vienna, Austria|Prior attacks on graph neural networks have focused on graph poisoning and evasion, neglecting the network's weights and biases. For convolutional neural networks, however, the risk arising from bit flip attacks is well recognized. We show that the direct application of a traditional bit flip attack to graph neural networks is of limited effectivity. Hence, we discuss the Injectivity Bit Flip Attack, the first bit flip attack designed specifically for graph neural networks. Our attack targets the learnable neighborhood aggregation functions in quantized message passing neural networks, degrading their ability to distinguish graph structures and impairing the expressivity of the Weisfeiler-Leman test. We find that exploiting mathematical properties specific to certain graph neural networks significantly increases their vulnerability to bit flip attacks. The Injectivity Bit Flip Attack can degrade the maximal expressive Graph Isomorphism Networks trained on graph property prediction datasets to random output by flipping only a small fraction of the network's bits, demonstrating its higher destructive power compared to traditional bit flip attacks transferred from convolutional neural networks. Our attack is transparent, motivated by theoretical insights and confirmed by extensive empirical results.|先前对图神经网络的攻击主要集中在图的中毒和规避上，忽略了网络的权重和偏差。然而，对于卷积神经网络来说，由比特翻转攻击引起的风险是公认的。研究表明，将传统的比特翻转攻击直接应用于图形神经网络是有效的。因此，我们讨论的注入性位翻转攻击，第一位翻转攻击专门设计的图神经网络。我们的攻击目标是量化信息传递神经网络中可学习的邻域聚集函数，降低了它们区分图结构的能力，损害了 Weisfeiler-Leman 检验的表达能力。我们发现，利用特定图形神经网络的数学特性显著增加了它们对位翻转攻击的脆弱性。注入性比特翻转攻击通过只翻转网络比特的一小部分，就可以将基于图性质预测数据集的最大表达式图同构网络降级为随机输出，与传统的卷积神经网络比特翻转攻击相比，显示出更高的破坏力。我们的攻击是透明的，受到理论洞察力的激励，并得到广泛的实证结果的证实。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Attacking+Graph+Neural+Networks+with+Bit+Flips:+Weisfeiler+and+Leman+Go+Indifferent)|0|
|[Max-Min Diversification with Asymmetric Distances](https://doi.org/10.1145/3637528.3671757)|Iiro Kumpulainen, Florian Adriaens, Nikolaj Tatti|University of Helsinki & HIIT, Helsinki, Finland; University of Helsinki, Helsinki, Finland|One of the most well-known and simplest models for diversity maximization is the Max-Min Diversification (MMD) model, which has been extensively studied in the data mining and database literature. In this paper, we initiate the study of the Asymmetric Max-Min Diversification (AMMD) problem. The input is a positive integer k and a complete digraph over n vertices, together with a nonnegative distance function over the edges obeying the directed triangle inequality. The objective is to select a set of k vertices, which maximizes the smallest pairwise distance between them. AMMD reduces to the well-studied MMD problem in case the distances are symmetric, and has natural applications to query result diversification, web search, and facility location problems. Although the MMD problem admits a simple 1/2-approximation by greedily selecting the next-furthest point, this strategy fails for AMMD and it remained unclear how to design good approximation algorithms for AMMD. We propose a combinatorial 1/(6k)-approximation algorithm for AMMD by leveraging connections with the Maximum Antichain problem. We discuss several ways of speeding up the algorithm and compare its performance against heuristic baselines on real-life and synthetic datasets.|最著名和最简单的多样性最大化模型之一是最大-最小多样化(MMD)模型，它在数据挖掘和数据库文献中得到了广泛的研究。本文首先研究了非对称最大-最小多样化(AMMD)问题。输入是一个正整数 k，n 个顶点上的完全有向图，以及服从有向三角不等式的边上的非负距离函数。目标是选择一组 k 顶点，使它们之间的最小成对距离最大化。在距离对称的情况下，AMMD 可以简化为已经深入研究过的 MMD 问题，并且具有查询结果多样化、网络搜索和设施位置问题的自然应用。虽然 MMD 问题通过贪婪地选择下一个最远点来承认一个简单的1/2近似，但这种策略对 AMMD 来说是失败的，而且目前还不清楚如何为 AMMD 设计好的近似算法。我们提出了一个组合1/(6k)-近似演算法的 AMMD 通过利用与最大反链问题的联系。我们讨论了几种提高算法速度的方法，并比较了在实际数据集和合成数据集上的启发式基线算法的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Max-Min+Diversification+with+Asymmetric+Distances)|0|
|[Efficient Topology-aware Data Augmentation for High-Degree Graph Neural Networks](https://doi.org/10.1145/3637528.3671765)|Yurui Lai, Xiaoyang Lin, Renchi Yang, Hongtao Wang|Hong Kong Baptist University, Hong Kong, China|In recent years, graph neural networks (GNNs) have emerged as a potent tool for learning on graph-structured data and won fruitful successes in varied fields. The majority of GNNs follow the message-passing paradigm, where representations of each node are learned by recursively aggregating features of its neighbors. However, this mechanism brings severe over-smoothing and efficiency issues over high-degree graphs (HDGs), wherein most nodes have dozens (or even hundreds) of neighbors, such as social networks, transaction graphs, power grids, etc. Additionally, such graphs usually encompass rich and complex structure semantics, which are hard to capture merely by feature aggregations in GNNs. Motivated by the above limitations, we propose TADA, an efficient and effective front-mounted data augmentation framework for GNNs on HDGs. Under the hood, TADA includes two key modules: (i) feature expansion with structure embeddings, and (ii) topology- and attribute-aware graph sparsification. The former obtains augmented node features and enhanced model capacity by encoding the graph structure into high-quality structure embeddings with our highly-efficient sketching method. Further, by exploiting task-relevant features extracted from graph structures and attributes, the second module enables the accurate identification and reduction of numerous redundant/noisy edges from the input graph, thereby alleviating over-smoothing and facilitating faster feature aggregations over HDGs. Empirically, \algo considerably improves the predictive performance of mainstream GNN models on 8 real homophilic/heterophilic HDGs in terms of node classification, while achieving efficient training and inference processes.|近年来，图神经网络(GNN)已经成为学习图结构数据的有力工具，并在各个领域取得了丰硕的成果。大多数 GNN 遵循消息传递范式，通过递归聚合其邻居的特征来学习每个节点的表示。然而，这种机制给高度图(HDG)带来了严重的过度平滑和效率问题，其中大多数节点有几十个(甚至几百个)邻居，如社交网络、交易图、电网等。此外，这样的图通常包含丰富而复杂的结构语义，仅仅通过 GNN 中的特征聚合很难捕获这些语义。基于上述局限性，我们提出了 TADA，一个有效的面向 HDG 上 GNN 的前置数据增强框架。TADA 包括两个关键模块: (1)结构嵌入的特征扩展和(2)拓扑和属性感知的图稀疏化。前者采用高效的草图绘制方法，将图结构编码为高质量的结构嵌入，从而获得增强的节点特征和增强的模型容量。此外，通过利用从图结构和属性中提取的与任务相关的特征，第二个模块能够准确识别和减少来自输入图的许多冗余/噪声边缘，从而减轻过度平滑并促进 HDG 上更快的特征聚合。从经验上看，算法在节点分类方面显著提高了主流 GNN 模型对8个实际同亲/异亲 HDG 的预测性能，同时实现了有效的训练和推理过程。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Topology-aware+Data+Augmentation+for+High-Degree+Graph+Neural+Networks)|0|
|[ShapeFormer: Shapelet Transformer for Multivariate Time Series Classification](https://doi.org/10.1145/3637528.3671862)|XuanMay Thi Le, Ling Luo, Uwe Aickelin, MinhTuan Tran|Monash University, Melbourne, VIC, Australia; The University of Melbourne, Melbourne, VIC, Australia|Multivariate time series classification (MTSC) has attracted significantresearch attention due to its diverse real-world applications. Recently,exploiting transformers for MTSC has achieved state-of-the-art performance.However, existing methods focus on generic features, providing a comprehensiveunderstanding of data, but they ignore class-specific features crucial forlearning the representative characteristics of each class. This leads to poorperformance in the case of imbalanced datasets or datasets with similar overallpatterns but differing in minor class-specific details. In this paper, wepropose a novel Shapelet Transformer (ShapeFormer), which comprisesclass-specific and generic transformer modules to capture both of thesefeatures. In the class-specific module, we introduce the discovery method toextract the discriminative subsequences of each class (i.e. shapelets) from thetraining set. We then propose a Shapelet Filter to learn the differencefeatures between these shapelets and the input time series. We found that thedifference feature for each shapelet contains important class-specificfeatures, as it shows a significant distinction between its class and others.In the generic module, convolution filters are used to extract generic featuresthat contain information to distinguish among all classes. For each module, weemploy the transformer encoder to capture the correlation between theirfeatures. As a result, the combination of two transformer modules allows ourmodel to exploit the power of both types of features, thereby enhancing theclassification performance. Our experiments on 30 UEA MTSC datasets demonstratethat ShapeFormer has achieved the highest accuracy ranking compared tostate-of-the-art methods. The code is available athttps://github.com/xuanmay2701/shapeformer.|多变量时间序列分类(MTSC)因其在现实世界中的广泛应用而引起了人们的广泛关注。最近，开发变压器的 MTSC 已经取得了最先进的性能。然而，现有的方法侧重于通用特征，提供了对数据的全面理解，但它们忽略了对学习每个类的代表性特征至关重要的类特定特征。这导致在不平衡的数据集或具有相似总体模式但在次要类特定细节上不同的数据集的情况下性能较差。在本文中，我们提出了一个新颖的小波变换器(ShapeForm) ，它包括类特定的和通用的变换器模块来捕获这两个特征。在类特定模块中，我们引入发现方法从训练集中提取每个类的判别子序列(即形状)。然后，我们提出了一个小波滤波器来学习这些形状和输入时间序列之间的差异特征。我们发现每个小波的差异特征包含重要的类别特征，因为它显示了其类别和其他类别之间的显着区别。在通用模块中，卷积过滤器用于提取包含信息以区分所有类别的通用特征。对于每个模块，我们使用变压器编码器来捕获它们之间的相关性。因此，两个变压器模块的结合使我们的模型能够利用两种类型的功能，从而提高分类性能。我们在30个 UEA MTSC 数据集上的实验表明，与最先进的方法相比，ShapeFormer 获得了最高的准确度排名。该代码可以在 https:// github.com/xuanmay2701/shapeformer 上获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ShapeFormer:+Shapelet+Transformer+for+Multivariate+Time+Series+Classification)|0|
|[ReCTSi: Resource-efficient Correlated Time Series Imputation via Decoupled Pattern Learning and Completeness-aware Attentions](https://doi.org/10.1145/3637528.3671816)|Zhichen Lai, Dalin Zhang, Huan Li, Dongxiang Zhang, Hua Lu, Christian S. Jensen|Department of People and Technology, Roskilde University, Roskilde, Denmark; Department of Computer Science, Aalborg University, Aalborg, Denmark; The State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China|Imputation of Correlated Time Series (CTS) is essential in data preprocessing for many tasks, particularly when sensor data is often incomplete. Deep learning has enabled sophisticated models that improve CTS imputation by capturing temporal and spatial patterns. However, deep models often incur considerable consumption of computational resources and thus cannot be deployed in resource-limited settings. This paper presents ReCTSi (Resource-efficient CTS imputation), a method that adopts a new architecture for decoupled pattern learning in two phases: (1) the Persistent Pattern Extraction phase utilizes a multi-view learnable codebook mechanism to identify and archive persistent patterns common across different time series, enabling rapid pattern retrieval during inference. (2) the Transient Pattern Adaptation phase introduces completeness-aware attention modules that allocate attention to the complete and hence more reliable data segments. Extensive experimental results show that ReCTSi achieves state-of-the-art imputation accuracy while consuming much fewer computational resources than the leading existing model, consuming only 0.004% of the FLOPs for inference compared to its closest competitor. The blend of high accuracy and very low resource consumption makes ReCTSi the currently best method for resource-limited scenarios. The related code is available at https://github.com/ryanlaics/RECTSI.|相关时间序列(CTS)的插补是许多任务的数据预处理中必不可少的，特别是当传感器数据往往是不完整的。深度学习使得复杂的模型能够通过捕捉时间和空间模式来改进 CTS 估算。然而，深度模型通常会消耗大量的计算资源，因此无法在资源有限的环境中部署。本文提出了一种新的解耦模式学习方法 ReCTSi，该方法分两个阶段实现: (1)持久模式提取阶段利用多视图可学习的码本机制识别和归档不同时间序列中常见的持久模式，从而实现推理过程中的快速模式检索。(2)瞬态模式适应阶段引入完整性意识注意模块，将注意力分配给完整的、因此更可靠的数据段。大量的实验结果表明，ReCTSi 实现了最先进的插补精度，同时比现有的主流模型消耗更少的计算资源，与其最接近的竞争对手相比，只消耗 FLOP 的0.004% 。高精度和极低资源消耗的混合使 ReCTSi 成为当前资源有限场景的最佳方法。有关密码可于 https://github.com/ryanlaics/rectsi 索取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ReCTSi:+Resource-efficient+Correlated+Time+Series+Imputation+via+Decoupled+Pattern+Learning+and+Completeness-aware+Attentions)|0|
|[Layer-Wise Adaptive Gradient Norm Penalizing Method for Efficient and Accurate Deep Learning](https://doi.org/10.1145/3637528.3671728)|Sunwoo Lee|Department of Computer Engineering, Inha University, Incheon, Republic of Korea|Sharpness-aware minimization (SAM) is known to improve the generalization performance of neural networks. However, it is not widely used in real-world applications yet due to its expensive model perturbation cost. A few variants of SAM have been proposed to tackle such an issue, but they commonly do not alleviate the cost noticeably. In this paper, we propose a lightweight layer-wise gradient norm penalizing method that tackles the expensive computational cost of SAM while maintaining its superior generalization performance. Our study empirically proves that the gradient norm of the whole model can be effectively suppressed by penalizing the gradient norm of only a few critical layers. We also theoretically show that such a partial model perturbation does not harm the convergence rate of SAM, allowing them to be safely adapted in real-world applications. To demonstrate the efficacy of the proposed method, we perform extensive experiments comparing the proposed method to mini-batch SGD and the conventional SAM using representative computer vision and language modeling benchmarks.|锐度感知最小化(SAM)已知可以提高神经网络的泛化性能。然而，由于其昂贵的模型摄动成本，在实际应用中还没有得到广泛的应用。已经提出了 SAM 的一些变体来解决这个问题，但是它们通常不能显著地减少成本。本文提出了一种轻量级分层梯度范数惩罚方法，在保持 SAM 优越泛化性能的同时，解决了 SAM 计算量大的问题。我们的研究经验证明，整个模型的梯度范数可以有效地抑制惩罚只有少数临界层的梯度范数。理论上还证明了这种局部模型扰动不会影响 SAM 的收敛速度，使其能够安全地适应实际应用。为了验证该方法的有效性，我们利用具有代表性的计算机视觉和语言建模基准，对该方法与小批量 SGD 和常规 SAM 进行了广泛的实验比较。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Layer-Wise+Adaptive+Gradient+Norm+Penalizing+Method+for+Efficient+and+Accurate+Deep+Learning)|0|
|[Label Learning Method Based on Tensor Projection](https://doi.org/10.1145/3637528.3671671)|Jing Li, Quanxue Gao, Qianqian Wang, Cheng Deng, DeYan Xie|School of Electronic Engineering, Xidian University, Xi'an, Shaanxi, China; School of Science and Information Science, Qingdao Agricultural University, Qingdao, Shandong, China; School of Telecommunications Engineering, Xidian University, Xi'an, Shaanxi, China|Multi-view clustering method based on anchor graph has been widely concerneddue to its high efficiency and effectiveness. In order to avoidpost-processing, most of the existing anchor graph-based methods learnbipartite graphs with connected components. However, such methods have highrequirements on parameters, and in some cases it may not be possible to obtainbipartite graphs with clear connected components. To end this, we propose alabel learning method based on tensor projection (LLMTP). Specifically, weproject anchor graph into the label space through an orthogonal projectionmatrix to obtain cluster labels directly. Considering that the spatialstructure information of multi-view data may be ignored to a certain extentwhen projected in different views separately, we extend the matrix projectiontransformation to tensor projection, so that the spatial structure informationbetween views can be fully utilized. In addition, we introduce the tensorSchatten p-norm regularization to make the clustering label matrices ofdifferent views as consistent as possible. Extensive experiments have provedthe effectiveness of the proposed method.|基于锚图的多视图聚类方法因其高效性和有效性而受到广泛关注。为了避免后处理，现有的基于锚图的方法大多学习具有连通分量的二部图。然而，这些方法对参数有很高的要求，在某些情况下，可能不可能获得具有清晰连通分量的二部图。为此，我们提出了基于张量投影(LLMTP)的标记学习方法。具体地说，我们通过一个正交投影矩阵将锚图投影到标签空间中，直接得到聚类标签。考虑到多视图数据的空间结构信息在分别投影于不同视图时可能在一定程度上被忽略，将矩阵投影变换推广到张量投影，以充分利用视图间的空间结构信息。另外，我们引入了张量 Schatten p- 范数正则化，使得不同视图的聚类标记矩阵尽可能的一致。大量的实验证明了该方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Label+Learning+Method+Based+on+Tensor+Projection)|0|
|[Physics-informed Neural ODE for Post-disaster Mobility Recovery](https://doi.org/10.1145/3637528.3672027)|Jiahao Li, Huandong Wang, Xinlei Chen|Department of Electronic Engineering, Tsinghua University, Beijing, China; Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Shenzhen International Graduate School, Tsinghua University & Pengcheng Laboratory, Shenzhen, China|Urban mobility undergoes a profound decline in the aftermath of a disaster, subsequently exhibiting a complex recovery trajectory. Effectively capturing and predicting this dynamic recovery process holds paramount importance for devising more efficient post-disaster recovery strategies, such as resource allocation to areas with protracted recovery periods. Existing models for post-disaster mobility recovery predominantly employ basic mathematical methods, which are strongly based on simplifying assumptions, and their limited parameters restrict their capacity to fully capture the mobility recovery patterns. In response to this gap, we introduce the Coupled Dynamic Graph ODE Network (CDGON) to model the intricate dynamics of post-disaster mobility recovery. Our model seamlessly integrates existing physical knowledge pertaining to post-disaster mobility recovery and incorporates the nuanced interactions between intra-regional and inter-regional population flows. Extensive experimental results demonstrate the efficiency of our model in capturing the dynamic recovery patterns of urban population mobility in post-disaster scenarios, surpassing the capabilities of current dynamic graph prediction models.|灾后城市流动性大幅下降，随后呈现出复杂的恢复轨迹。有效捕捉和预测这一动态恢复过程对于制定更有效的灾后恢复战略至关重要，例如向恢复期较长的地区分配资源。现有的灾后机动性恢复模型主要采用基本的数学方法，这些方法强烈地基于简化的假设，其有限的参数限制了它们充分捕捉机动性恢复模式的能力。针对这一差距，我们引入耦合动态图 ODE 网络(CDON)来建立灾后移动性恢复的复杂动态模型。我们的模型无缝地整合了与灾后流动性恢复有关的现有物理知识，并纳入了区域内和区域间人口流动之间微妙的相互作用。大量的实验结果表明，我们的模型在捕捉灾后情景下城市人口流动动态恢复模式方面的效率超过了现有动态图预测模型的能力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Physics-informed+Neural+ODE+for+Post-disaster+Mobility+Recovery)|0|
|[Causal Subgraph Learning for Generalizable Inductive Relation Prediction](https://doi.org/10.1145/3637528.3671972)|Mei Li, Xiaoguang Liu, Hua Ji, Shuangjia Zheng|; College of Computer Science, TMCC, SysNet, DISSec, GTIISC, Nankai University, Tianjin, China; Department of Computer Science, Civil Aviation University of China, Tianjin, China; Global Institute of Future Technology, Shanghai Jiao Tong University, Shanghai, China|Inductive relation reasoning in knowledge graphs aims at predicting missing triplets involving unseen entities and/or unseen relations. While subgraph-based methods that reason about the local structure surrounding a candidate triplet have shown promise, they often fall short in accurately modeling the causal dependence between a triplet's subgraph and its ground-truth label. This limitation typically results in a susceptibility to spurious correlations caused by confounders, adversely affecting generalization capabilities. Herein, we introduce a novel front-door adjustment-based approach designed to learn the causal relationship between subgraphs and their ground-truth labels, specifically for inductive relation prediction. We conceptualize the semantic information of subgraphs as a mediator and employ a graph data augmentation mechanism to create augmented subgraphs. Furthermore, we integrate a fusion module and a decoder within the front-door adjustment framework, enabling the estimation of the mediator's combination with augmented subgraphs. We also introduce the reparameterization trick in the fusion model to enhance model robustness. Extensive experiments on widely recognized benchmark datasets demonstrate the proposed method's superiority in inductive relation prediction, particularly for tasks involving unseen entities and unseen relations. Additionally, the subgraphs reconstructed by our decoder offer valuable insights into the model's decision-making process, enhancing transparency and interpretability.|知识图中归纳关系推理的目的是预测缺失的涉及看不见的实体和/或看不见的关系的三联体。虽然基于子图的方法，推理一个候选三元组周围的局部结构已经显示了希望，他们往往不能准确地建模一个三元组的子图和它的地面真理标签之间的因果关系。这种限制通常导致容易受到混杂因素引起的虚假相关性的影响，从而对泛化能力产生不利影响。在这里，我们介绍了一种新的前门调整为基础的方法，旨在了解因果关系之间的子图和他们的地面真理标签，特别是归纳关系预测。我们将子图的语义信息概念化为中介，并使用图形数据增强机制来创建增强子图。此外，我们在前门调整框架内集成了融合模块和解码器，使得能够估计中介者与增强子图的组合。为了增强模型的鲁棒性，我们还在融合模型中引入了重参数化技巧。通过对已被广泛认可的基准数据集的大量实验证明了该方法在归纳关系预测中的优越性，特别是对于涉及不可见实体和不可见关系的任务。此外，我们的解码器重建的子图提供了有价值的洞察力模型的决策过程，提高透明度和可解释性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causal+Subgraph+Learning+for+Generalizable+Inductive+Relation+Prediction)|0|
|[SimDiff: Simple Denoising Probabilistic Latent Diffusion Model for Data Augmentation on Multi-modal Knowledge Graph](https://doi.org/10.1145/3637528.3671769)|Ran Li, Shimin Di, Lei Chen, Xiaofang Zhou|HKUST(GZ) & HKUST, Guangzhou, China; HKUST, Hong Kong SAR, China|In this paper, we address the challenges of data augmentation in Multi-Modal Knowledge Graphs (MMKGs), a relatively under-explored area. We propose a novel diffusion-based generative model, the Simple Denoising Probabilistic Latent Diffusion Model (SimDiff). SimDiff is capable of handling different data modalities including the graph topology in a unified manner by the same diffusion model in the latent space. It enhances the utilization of multi-modal data and encourage the multi-modal fusion and reduces the dependency on limited training data. We validate our method in downstream Entity Alignment (EA) tasks in MMKGs, demonstrating that even when using only half of the seed entities in training, our methods can still achieve superior performance. This work contributes to the field by providing a new data generation or augmentation method for MMKGs, potentially paving the way for more effective use of MMKGs in various applications. Code is made available at https://github.com/ranlislz/SimDiff.|在本文中，我们讨论了多模态知识图(MMKGs)中数据增强的挑战，这是一个相对欠缺探索的领域。我们提出了一种新的基于扩散的生成模型，简单去噪概率潜在扩散模型(simDiff)。SimDiff 能够通过潜空间中相同的扩散模型统一处理包括图拓扑在内的不同数据模式。它提高了多模态数据的利用率，鼓励了多模态融合，减少了对有限训练数据的依赖。我们验证了我们的方法在 MMKG 的下游实体对齐(EA)任务，证明即使只使用一半的种子实体在训练，我们的方法仍然可以取得优越的性能。这项工作为 MMKG 提供了一种新的数据生成或增强方法，为 MMKG 在各种应用中更有效地使用铺平了道路，从而为该领域做出了贡献。代码可在 https://github.com/ranlislz/simdiff 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SimDiff:+Simple+Denoising+Probabilistic+Latent+Diffusion+Model+for+Data+Augmentation+on+Multi-modal+Knowledge+Graph)|0|
|[ITPNet: Towards Instantaneous Trajectory Prediction for Autonomous Driving](https://doi.org/10.1145/3637528.3671681)|Rongqing Li, Changsheng Li, Yuhang Li, Hanjie Li, Yi Chen, Ye Yuan, Guoren Wang|Beijing Institute of Technology, Beijing, China|Trajectory prediction of moving traffic agents is crucial for the safety of autonomous vehicles, whereas previous approaches usually rely on sufficiently long-observed trajectory (e.g., 2 seconds) to predict the future trajectory of the agents. However, in many real-world scenarios, it is not realistic to collect adequate observed locations for moving agents, leading to the collapse of most prediction models. For instance, when a moving car suddenly appears and is very close to an autonomous vehicle because of the obstruction, it is quite necessary for the autonomous vehicle to quickly and accurately predict the future trajectories of the car with limited observed trajectory locations. In light of this, we focus on investigating the task of instantaneous trajectory prediction, i.e., two observed locations are available during inference. To this end, we put forward a general and plug-and-play instantaneous trajectory prediction approach, called ITPNet. Specifically, we propose a backward forecasting mechanism to reversely predict the latent feature representations of unobserved historical trajectories of the agent based on its two observed locations and then leverage them as complementary information for future trajectory prediction. Meanwhile, due to the inevitable existence of noise and redundancy in the predicted latent feature representations, we further devise a Noise Redundancy Reduction Former (NRRFormer) module, which aims to filter out noise and redundancy from unobserved trajectories and integrate the filtered features and observed features into a compact query representation for future trajectory predictions. In essence, ITPNet can be naturally compatible with existing trajectory prediction models, enabling them to gracefully handle the case of instantaneous trajectory prediction. Extensive experiments on the Argoverse and nuScenes datasets demonstrate ITPNet outperforms the baselines by a large margin and shows its efficacy with different trajectory prediction models.|移动交通主体的轨迹预测对自主车辆的安全至关重要，而以前的方法通常依赖于足够长的观测轨迹(例如，2秒)来预测主体的未来轨迹。然而，在许多实际场景中，为移动代理收集足够的观察位置是不现实的，这会导致大多数预测模型的崩溃。例如，当一辆行驶中的汽车突然出现，并且因为障碍物而非常接近无人机时，无人机必须在有限的轨迹位置上快速而准确地预测汽车的未来轨迹。鉴于此，我们着重研究瞬时轨迹预测的任务，即在推断过程中有两个可观测的位置。为此，我们提出了一种通用的即插即用的瞬时轨迹预测方法 ITPNet。具体来说，我们提出了一种逆向预测机制，该机制根据 Agent 的两个观测位置反向预测未观测历史轨迹的潜在特征表示，然后利用它们作为未来轨迹预测的补充信息。同时，由于预测的潜在特征表示中不可避免地存在噪声和冗余，我们进一步设计了噪声冗余降低前(NRRForm)模块，旨在从未观测轨迹中过滤掉噪声和冗余，并将过滤后的特征和观测特征整合到一个紧凑的查询表示中，用于未来的轨迹预测。从本质上说，ITPNet 可以自然地与现有的轨道预测模型兼容，使它们能够优雅地处理瞬时轨道预测的情况。在 Argoverse 和 nuScenes 数据集上进行的大量实验表明，ITPNet 的性能大大优于基线，并且在不同的轨迹预测模型上显示了其有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ITPNet:+Towards+Instantaneous+Trajectory+Prediction+for+Autonomous+Driving)|0|
|[InLN: Knowledge-aware Incremental Leveling Network for Dynamic Advertising](https://doi.org/10.1145/3637528.3672032)|Xujia Li, Jingshu Peng, Lei Chen|; Hong Kong University of Science and Technology, Hong Kong SAR, China|In today's fast-paced world, advertisers are increasingly demanding real-time and accurate personalized ad delivery based on dynamic preference modeling, which emphasizes the temporality existing in both user preference and product characteristics. Meanwhile, with the development of graph neural networks (GNNs), E-commerce knowledge graphs (KG) with rich semantic relatedness are invoked to improve accuracy and provide appropriate explanations to encourage advertisers' willingness to invest in ad expenses. However, it is still challenging for existing methods to comprehensively consider both time-series interactions and graph-structured knowledge triples in a unified model, i.e., the case in knowledge-aware dynamic advertising. The interaction graph between users and products changes rapidly over time, while the knowledge in KG remains relatively stable. This results in an uneven distribution of temporal and semantic information, causing existing GNNs to fail in this scenario. In this work, we quantitatively define the above phenomenon as temporal unevenness and introduce the Incremental Leveling Network (InLN) with three novel techniques: the periodic-focusing window for node-level dynamic modeling, the biased temporal walk for subgraph-level dynamic modeling and the incremental leveling mechanism for KG updating. Verified by comprehensive and intensive experiments, InLN outperforms nine baseline models in three tasks by substantial margins, reaching up to a 9.9% improvement and averaging a 5.7% increase.|在当今快节奏的世界中，广告主对基于动态偏好建模的实时、准确的个性化广告投放提出了越来越高的要求。同时，随着图神经网络(GNN)的发展，电子商务知识图(KG)具有丰富的语义相关性，以提高准确性，并提供适当的解释，以鼓励广告商的意愿投资广告费用。然而，在统一的模型中，即知识感知的动态广告中，综合考虑时间序列交互作用和图形结构知识三元组仍然是现有方法面临的挑战。用户与产品之间的交互图随着时间的推移迅速变化，而 KG 中的知识保持相对稳定。这导致了时间和语义信息的不均衡分布，导致现有的 GNN 在这种情况下失败。本文将上述现象定量地定义为时间不均匀性，并引入了增量水准网络(InLN)的三种新技术: 用于节点级动态建模的周期聚焦窗口、用于子图级动态建模的有偏时间步行以及用于 KG 更新的增量水准机制。经过全面和深入的实验验证，InLN 在三个任务中的表现大大优于9个基线模型，提高了9.9% ，平均提高了5.7% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=InLN:+Knowledge-aware+Incremental+Leveling+Network+for+Dynamic+Advertising)|0|
|[Bi-Objective Contract Allocation for Guaranteed Delivery Advertising](https://doi.org/10.1145/3637528.3671752)|Yan Li, Yundu Huang, Wuyang Mao, Furong Ye, Xiang He, Zhonglin Zu, Shaowei Cai|; Alibaba Group, Beijing, China; Alibaba Group, Hangzhou, China|Contemporary systems of Guaranteed Delivery (GD) advertising work with two different stages, namely, the offline selling stage and the online serving stage. The former deals with contract allocation, and the latter fulfills the impression allocation of signed contracts. Existing work usually handles these two stages separately. For example, contracts are formulated offline without concerning practical situations in the online serving stage. Therefore, we address in this paper a bi-objective contract allocation for GD advertising, which maximizes the impressions, i.e., Ad resource assignments, allocated for the new incoming advertising orders, and at the same time, controls the balance in the inventories. Since the proposed problem is high dimensional and heavily constrained, we design an efficient local search that focuses on the two objectives alternatively. The experimental results indicate that our algorithm outperforms multi-objective evolutionary algorithms and Gurobi, the former of which is commonly applied for multi-objective optimization and the latter of which is a well-known competitive commercial tool.|现代广告传播保证系统分为线下销售阶段和线上服务阶段。前者处理合同分配问题，后者实现已签订合同的印象分配。现有的工作通常分别处理这两个阶段。例如，合同是在线下制定的，与在线服务阶段的实际情况无关。因此，本文提出了一种双目标广告合同分配模型，即广告资源分配模型，该模型可以最大限度地提高广告客户对广告的印象，同时控制广告库存的平衡。由于提出的问题是高维和严重约束，我们设计了一个有效的局部搜索，重点是两个目标交替。实验结果表明，该算法的性能优于多目标进化算法和 Gurobi 算法，前者是多目标优化的常用算法，后者是众所周知的竞争性商业工具。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bi-Objective+Contract+Allocation+for+Guaranteed+Delivery+Advertising)|0|
|[Improving Robustness of Hyperbolic Neural Networks by Lipschitz Analysis](https://doi.org/10.1145/3637528.3671875)|Yuekang Li, Yidan Mao, Yifei Yang, Dongmian Zou|Zu Chongzhi Center and Data Science Research Center, DNAS, Duke Kunshan University, Kunshan, China; Applied Mathematics and Computational Sciences, DNAS, Duke Kunshan University, Kunshan, China; Electronic Information School, Wuhan University, Wuhan, China|Hyperbolic neural networks (HNNs) are emerging as a promising tool for representing data embedded in non-Euclidean geometries, yet their adoption has been hindered by challenges related to stability and robustness. In this work, we conduct a rigorous Lipschitz analysis for HNNs and propose using Lipschitz regularization as a novel strategy to enhance their robustness. Our comprehensive investigation spans both the Poincaré ball model and the hyperboloid model, establishing Lipschitz bounds for HNN layers. Importantly, our analysis provides detailed insights into the behavior of the Lipschitz bounds as they relate to feature norms, particularly distinguishing between scenarios where features have unit norms and those with large norms. Further, we study regularization using the derived Lipschitz bounds. Our empirical validations demonstrate consistent improvements in HNN robustness against noisy perturbations.|双曲神经网络(HNN)正在成为表示非欧几里德几何中嵌入的数据的一种有前途的工具，然而它们的采用受到与稳定性和鲁棒性有关的挑战的阻碍。本文对 HNN 进行了严格的 Lipschitz 分析，提出了利用 Lipschitz 正则化作为一种新的增强鲁棒性的策略。我们的全面调查跨越了庞加莱圆盘模型和双曲面模型，确定了 HNN 层的 Lipschitz 界限。重要的是，我们的分析提供了关于 Lipschitz 界限与特征规范相关的行为的详细见解，特别是区分特征具有单位规范和具有大规范的情景。进一步，我们利用导出的 Lipschitz 界研究正则化。我们的经验验证表明，一致的改进 HNN 对噪声扰动的鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Robustness+of+Hyperbolic+Neural+Networks+by+Lipschitz+Analysis)|0|
|[ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs](https://doi.org/10.1145/3637528.3671982)|Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, Jia Li|CUHK, Hong Kong SAR, China; HKUST (GZ), Guangzhou, China; THU, Shenzhen, China|With the development of foundation models such as large language models, zero-shot transfer learning has become increasingly significant. This is highlighted by the generative capabilities of NLP models like GPT-4, and the retrieval-based approaches of CV models like CLIP, both of which effectively bridge the gap between seen and unseen data. In the realm of graph learning, the continuous emergence of new graphs and the challenges of human labeling also amplify the necessity for zero-shot transfer learning, driving the exploration of approaches that can generalize across diverse graph data without necessitating dataset-specific and label-specific fine-tuning. In this study, we extend such paradigms to Zero-shot transferability in Graphs by introducing ZeroG, a new framework tailored to enable cross-dataset generalization. Addressing the inherent challenges such as feature misalignment, mismatched label spaces, and negative transfer, we leverage a language model to encode both node attributes and class semantics, ensuring consistent feature dimensions across datasets. We also propose a prompt-based subgraph sampling module that enriches the semantic information and structure information of extracted subgraphs using prompting nodes and neighborhood aggregation, respectively. We further adopt a lightweight fine-tuning strategy that reduces the risk of overfitting and maintains the zero-shot learning efficacy of the language model. The results underscore the effectiveness of our model in achieving significant cross-dataset zero-shot transferability, opening pathways for the development of graph foundation models.|随着大型语言模型等基础模型的不断发展，零冲击迁移学习变得越来越重要。GPT-4等自然语言处理模型的生成能力和 CLIP 等 CV 模型的基于检索的方法都突出了这一点，这两种方法都有效地弥合了可见数据和不可见数据之间的差距。在图形学习领域，新图形的不断出现和人类标记的挑战也放大了零传递学习的必要性，推动了对可以在不同图形数据之间推广的方法的探索，而不需要数据集特定和标签特定的微调。在这项研究中，我们通过引入 ZeroG，将这些范例扩展到图中的零点可转移性，ZeroG 是一个新的框架，可以实现跨数据集的泛化。针对特征错位、标签空间不匹配和负迁移等内在挑战，我们利用语言模型对节点属性和类语义进行编码，确保跨数据集的特征维一致。我们还提出了一个基于提示的子图抽样模块，分别使用提示节点和邻域聚合来丰富提取子图的语义信息和结构信息。我们进一步采用了轻量级的微调策略，降低了过度拟合的风险，并保持了语言模型的零点学习效率。这些结果强调了我们的模型在实现显著的跨数据集零拍可转移性方面的有效性，为图形基础模型的开发打开了通路。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ZeroG:+Investigating+Cross-dataset+Zero-shot+Transferability+in+Graphs)|0|
|[Rethinking Fair Graph Neural Networks from Re-balancing](https://doi.org/10.1145/3637528.3671826)|Zhixun Li, Yushun Dong, Qiang Liu, Jeffrey Xu Yu|The Chinese University of Hong Kong, Hong Kong, Hong Kong; University of Virginia, Charlottesville, USA; Institute of Automation, Chinese Academy of Sciences, Beijing, China|Driven by the powerful representation ability of Graph Neural Networks (GNNs), plentiful GNN models have been widely deployed in many real-world applications. Nevertheless, due to distribution disparities between different demographic groups, fairness in high-stake decision-making systems is receiving increasing attention. Although lots of recent works devoted to improving the fairness of GNNs and achieved considerable success, they all require significant architectural changes or additional loss functions requiring more hyper-parameter tuning. Surprisingly, we find that simple re-balancing methods can easily match or surpass existing fair GNN methods. We claim that the imbalance across different demographic groups is a significant source of unfairness, resulting in imbalanced contributions from each group to the parameters updating. However, these simple re-balancing methods have their own shortcomings during training. In this paper, we propose FairGB, Fair Graph Neural Network via re-Balancing, which mitigates the unfairness of GNNs by group balancing. Technically, FairGB consists of two modules: counterfactual node mixup and contribution alignment loss. Firstly, we select counterfactual pairs across inter-domain and inter-class, and interpolate the ego-networks to generate new samples. Guided by analysis, we can reveal the debiasing mechanism of our model by the causal view and prove that our strategy can make sensitive attributes statistically independent from target labels. Secondly, we reweigh the contribution of each group according to gradients. By combining these two modules, they can mutually promote each other. Experimental results on benchmark datasets show that our method can achieve state-of-the-art results concerning both utility and fairness metrics. Code is available at https://github.com/ZhixunLEE/FairGB.|由于图形神经网络(GNN)强大的表示能力，大量的 GNN 模型被广泛应用于实际应用中。然而，由于不同人口群体之间的分布差异，高风险决策系统的公平性越来越受到重视。尽管最近许多致力于提高 GNN 公平性的工作取得了相当大的成功，但它们都需要重大的体系结构改变或额外的损失函数，需要更多的超参数调整。令人惊讶的是，我们发现简单的重新平衡方法可以很容易地匹配或超过现有的公平 GNN 方法。我们认为，不同人口群体之间的不平衡是不公平的一个重要来源，导致每个群体对参数更新的贡献不平衡。然而，这些简单的再平衡方法在训练过程中也有自己的缺点。本文通过重新平衡提出了 FairGB，即公平图神经网络，通过组平衡来缓解 GNN 的不公平性。从技术上讲，FairGB 包括两个模块: 反事实节点混合和贡献对齐损失。首先，我们选择跨域和跨类的反事实对，并插值自我网络生成新的样本。在分析的指导下，我们可以通过因果观点揭示模型的去偏机制，并证明我们的策略可以使敏感属性在统计上独立于目标标签。其次，我们根据梯度重新衡量每组的贡献。通过这两个模块的结合，它们可以相互促进。在基准数据集上的实验结果表明，该方法在效用度量和公平度量方面都取得了较好的效果。密码可于 https://github.com/zhixunlee/fairgb 索取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rethinking+Fair+Graph+Neural+Networks+from+Re-balancing)|0|
|[MulSTE: A Multi-view Spatio-temporal Learning Framework with Heterogeneous Event Fusion for Demand-supply Prediction](https://doi.org/10.1145/3637528.3672030)|Li Lin, Zhiqiang Lu, Shuai Wang, Yunhuai Liu, Zhiqing Hong, Haotian Wang, Shuai Wang|JD Logistics, Beijing, China; Rutgers University, Piscataway, USA; Southeast University, Nanjing, Jiangsu, China; Peking University, Beijing, China; Southeast University, Nanjing, China|Recently, integrated warehouse and distribution logistics systems are widely used in E-commerce industries to adjust to constantly changing customer demands. It makes the prediction of purchase demand and delivery supply capacity a crucial problem to streamline operations and improve efficiency. The interaction between such demand and supply not only relies on their economic relationships but also on consumer psychology caused by daily events, such as epidemics, promotions, and festivals. Although existing studies have made great efforts in the joint prediction of demand and supply considering modeling the demand-supply interactions, they seldom refer to the impacts of diverse events. In this work, we propose MulSTE, a Multi-view Spatio-Temporal learning framework with heterogeneous Event fusion. Firstly, an Event Fusion Representation (EFR) module is designed to fuse the textual, numerical, and categorical heterogeneous information for emergent and periodic events. Secondly, a Multi-graph Adaptive Convolution Recurrent Network (MGACRN) is developed as the spatio-temporal encoder (ST-Encoder) to capture the evolutional features of demand, supply, and events. Thirdly, the Event Gated Demand-Supply Interaction Attention (EGIA) module is designed to model the demand-supply interactions during events. The evaluations are conducted on two real-world datasets collected from JD Logistics and public websites. The experimental results show that our method outperforms state-of-the-art baselines in various metrics.|近年来，仓储与配送一体化物流系统广泛应用于电子商务行业，以适应不断变化的客户需求。它使得采购需求和供应能力的预测成为精简业务和提高效率的关键问题。这种需求和供给之间的相互作用不仅依赖于它们之间的经济关系，而且依赖于日常事件(如流行病、促销和节日)引起的消费者心理。虽然现有的研究在考虑需求-供给相互作用模型的供需联合预测方面做出了很大的努力，但很少涉及到不同事件的影响。在这项工作中，我们提出了一个多视图的时空异质事件融合学习框架 MulSTE。首先，设计了事件融合表示(EFR)模块，对突发事件和周期性事件的文本、数字和分类异构信息进行融合。其次，提出了一种多图自适应卷积回归网络(MGACRN)作为时空编码器(ST-Encoder)来捕捉需求、供给和事件的演化特征。第三，设计了事件门限需求-供给交互注意(EGIA)模块，对事件期间的需求-供给交互进行建模。这些评估是在从 JD Logistics 和公共网站收集的两个真实世界数据集上进行的。实验结果表明，我们的方法在各种度量方面都优于最先进的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MulSTE:+A+Multi-view+Spatio-temporal+Learning+Framework+with+Heterogeneous+Event+Fusion+for+Demand-supply+Prediction)|0|
|[PSMC: Provable and Scalable Algorithms for Motif Conductance Based Graph Clustering](https://doi.org/10.1145/3637528.3671666)|Longlong Lin, Tao Jia, Zeli Wang, Jin Zhao, RongHua Li|Chongqing University of Post and Telecommunications, Chongqing, China; ; College of Computer and Information Science, Southwest University, Chongqing, China; Shenzhen Institute of Technology & Beijing Institute of Technology, Shenzhen, China|Higher-order graph clustering aims to partition the graph using frequently occurring subgraphs (i.e., motifs), instead of the lower-order edges, as the atomic clustering unit, which has been recognized as the state-of-the-art solution in ground truth community detection and knowledge discovery. Motif conductance is one of the most promising higher-order graph clustering models due to its strong interpretability. However, existing motif conductance based graph clustering algorithms are mainly limited by a seminal two-stage reweighting computing framework, needing to enumerate all motif instances to obtain an edge-weighted graph for partitioning. However, such a framework has two-fold vital defects: (1) It can only provide a quadratic bound for the motif with three vertices, and whether there is provable clustering quality for other motifs is still an open question. (2) The enumeration procedure of motif instances incurs prohibitively high costs against large motifs or large dense graphs due to combinatorial explosions. Besides, expensive spectral clustering or local graph diffusion on the edge-weighted graph also makes existing methods unable to handle massive graphs with millions of nodes. To overcome these dilemmas, we propose a Provable and Scalable Motif Conductance algorithm PSMC, which has a fixed and motif-independent approximation ratio for any motif. Specifically, PSMC first defines a new vertex metric Motif Resident based on the given motif, which can be computed locally. Then, it iteratively deletes the vertex with the smallest motif resident value very efficiently using novel dynamic update technologies. Finally, it outputs the locally optimal result during the above iterative process. To further boost efficiency, we propose several effective bounds to estimate the motif resident value of each vertex, which can greatly reduce computational costs. Empirical results on real-life and synthetic demonstrate that our proposed algorithms achieve 3.2-32 times speedup and improve the quality by at least 12 times than the state-of-the art baselines.|高阶图聚类的目的是使用频繁出现的子图(即图案)来划分图，而不是使用低阶边作为原子聚类单元，这已经被认为是地面真相社区检测和知识发现的最先进的解决方案。基序电导是最有前途的高阶图聚类模型之一，因为它具有很强的可解释性。然而，现有的基于基序电导的图聚类算法主要受限于一个开创性的两阶段重权计算框架，需要枚举所有的基序实例才能得到用于划分的边加权图。然而，这种框架有两个致命的缺陷: (1)它只能为三个顶点的图案提供一个二次界，其他图案是否具有可证明的聚类质量仍然是一个悬而未决的问题。(2)由于组合爆炸的原因，对于大图案或大密度图，主题实例的计数过程会产生高得令人望而却步的代价。此外，边加权图上昂贵的 SVD 或局部图扩散也使得现有的方法无法处理具有数百万个节点的海量图。为了克服这些困难，我们提出了一个可证明的可扩展的基元电导算法 PSMC，它对任何基元都有一个固定的和基元无关的近似比率。具体来说，PSMC 首先定义一个新的基于给定主题的顶点度量主题驻留，它可以在局部计算。然后，利用新的动态更新技术，迭代地删除基元驻留值最小的顶点。最后，在上述迭代过程中输出局部最优结果。为了进一步提高效率，我们提出了几个有效的界来估计每个顶点的模驻留值，这可以大大降低计算量。实际和合成的实验结果表明，我们提出的算法比最先进的基线算法提高了3.2 -32倍的速度，至少提高了12倍的质量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PSMC:+Provable+and+Scalable+Algorithms+for+Motif+Conductance+Based+Graph+Clustering)|0|
|[CONFIDE: Contextual Finite Difference Modelling of PDEs](https://doi.org/10.1145/3637528.3671676)|Ori Linial, Orly Avner, Dotan Di Castro||We introduce a method for inferring an explicit PDE from a data sample generated by previously unseen dynamics, based on a learned context. The training phase integrates knowledge of the form of the equation with a differential scheme, while the inference phase yields a PDE that fits the data sample and enables both signal prediction and data explanation. We include results of extensive experimentation, comparing our method to SOTA approaches, together with ablation studies that examine different flavors of our solution in terms of prediction error and explainability.|我们介绍了一种方法来推断一个明确的偏微分方程从数据样本生成以前看不见的动力学，基于学习上下文。训练阶段将方程形式的知识与微分格式结合起来，而推断阶段产生一个符合数据样本的偏微分方程，并且能够进行信号预测和数据解释。我们包括广泛的实验结果，比较我们的方法与 SOTA 方法，以及消融研究，检查我们的解决方案在预测误差和可解释性方面的不同口味。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CONFIDE:+Contextual+Finite+Difference+Modelling+of+PDEs)|0|
|[CASA: Clustered Federated Learning with Asynchronous Clients](https://doi.org/10.1145/3637528.3671979)|Boyi Liu, Yiming Ma, Zimu Zhou, Yexuan Shi, Shuyuan Li, Yongxin Tong|SKLCCSE Lab, Beihang University, Beijing, China; School of Data Science, City University of Hong Kong, Hong Kong, China|Clustered Federated Learning (CFL) is an emerging paradigm to extract insights from data on IoT devices. Through iterative client clustering and model aggregation, CFL adeptly manages data heterogeneity, ensures privacy, and delivers personalized models to heterogeneous devices. Traditional CFL approaches, which operate synchronously, suffer from prolonged latency for waiting slow devices during clustering and aggregation. This paper advocates a shift to asynchronous CFL, allowing the server to process client updates as they arrive. This shift enhances training efficiency yet introduces complexities to the iterative training cycle. To this end, we present CASA, a novel CFL scheme for Clustering-Aggregation Synergy under Asynchrony. Built upon a holistic theoretical understanding of asynchrony's impact on CFL, CASA adopts a bi-level asynchronous aggregation method and a buffer-aided dynamic clustering strategy to harmonize between clustering and aggregation. Extensive evaluations on standard benchmarks show that CASA outperforms representative baselines in model accuracy and achieves 2.28-6.49× higher convergence speed.|集群联邦学习(Clustered Federated Learning，CFL)是一种从物联网设备的数据中提取见解的新兴范式。通过迭代的客户端集群和模型聚合，CFL 能够很好地管理数据异构性，确保隐私，并向异构设备提供个性化的模型。传统的 CFL 方法是同步操作的，在集群和聚合过程中，由于等待速度较慢的设备而延迟较长。本文主张转向异步 CFL，允许服务器在客户端更新到达时进行处理。这种转变提高了训练效率，但同时也给迭代训练周期带来了复杂性。为此，我们提出了 CASA，这是一种新颖的非同步聚类-聚集协同 CFL 方案。基于对异步对 CFL 影响的全面理论认识，CASA 采用了双层异步聚合方法和缓冲区辅助的动态聚类策略来协调聚类和聚合。对标准基准的广泛评估表明，CASA 在模型精度方面优于典型基准，收敛速度达到2.28 -6.49倍。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CASA:+Clustered+Federated+Learning+with+Asynchronous+Clients)|0|
|[FAST: An Optimization Framework for Fast Additive Segmentation in Transparent ML](https://doi.org/10.1145/3637528.3671996)|Brian Liu, Rahul Mazumder|Massachusetts Institute of Technology, Cambridge, MA, USA|We present FAST, an optimization framework for fast additive segmentation. FAST segments piecewise constant shape functions for each feature in a dataset to produce transparent additive models. The framework leverages a novel optimization procedure to fit these models ~2 orders of magnitude faster than existing state-of-the-art methods, such as explainable boosting machines[20]. We also develop new feature selection algorithms in the FAST framework to fit parsimonious models that perform well. Through experiments and case studies, we show that FAST improves the computational efficiency and interpretability of additive models.|提出了一种快速加性分割的优化框架 FAST。FAST 分割数据集中每个特征的分段常数形状函数，以生成透明的可加模型。该框架利用一种新颖的优化程序来适应这些模型，比现有的最先进的方法(如可解释的增压机)数量级更快。我们还在 FAST 框架中开发了新的特征选择算法，以适应性能良好的简约模型。通过实验和案例研究表明，FAST 提高了可加模型的计算效率和可解释性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FAST:+An+Optimization+Framework+for+Fast+Additive+Segmentation+in+Transparent+ML)|0|
|[Asymmetric Beta Loss for Evidence-Based Safe Semi-Supervised Multi-Label Learning](https://doi.org/10.1145/3637528.3671756)|HaoZhe Liu, MingKun Xie, ChenChen Zong, ShengJun Huang|Nanjing University of Aeronautics and Astronautics, Nanjing, China|The goal of semi-supervised multi-label learning (SSMLL) is to improve model performance by leveraging the information of unlabeled data. Recent studies usually adopt the pseudo-labeling strategy to tackle unlabeled data based on the assumption that labeled and unlabeled data share the same distribution. However, in realistic scenarios, unlabeled examples are often collected through cost-effective methods, inevitably introducing out-of-distribution (OOD) data, leading to a significant decline in model performance. In this paper, we propose a safe semi-supervised multi-label learning framework based on the theory of evidential deep learning (EDL), with the goal of achieving robust and effective unlabeled data exploitation. On one hand, we propose the asymmetric beta loss to not only compensate for the lack of robustness in common MLL losses, but also to solve the inherent positive-negative imbalance problem faced by the EDL losses in MLL. On the other hand, to construct a robust SSMLL framework, we adopt a dual-head structure to generate class probabilities and instance uncertainties. The former are used to generate pseudo-labels, while the latter are utilized to filter OOD examples. To avoid the need for threshold estimation, we develop a dual-measurement weighted loss function to safely perform unlabeled training. Extensive experiments on multiple benchmark datasets verify the effectiveness of the proposed method in both OOD detection and SSMLL tasks.|半监督多标记学习(SSMLL)的目标是通过利用未标记数据的信息来提高模型的性能。最近的研究通常采用伪标记策略来处理未标记数据，这是基于标记数据和未标记数据分布相同的假设。然而，在现实的场景中，未标记的例子通常是通过具有成本效益的方法收集的，这不可避免地会引入分布外(OOD)数据，从而导致模型性能的显著下降。本文基于证据深度学习(EDL)理论，提出了一种安全的半监督多标记学习框架，目的是实现鲁棒有效的未标记数据开发。一方面，我们提出非对称贝塔损耗不仅可以弥补一般 MLL 损耗鲁棒性的不足，而且可以解决 MLL 中 EDL 损耗固有的正负不平衡问题。另一方面，为了构造一个健壮的 SSMLL 框架，我们采用双头结构来产生类概率和实例不确定性。前者用于生成伪标签，后者用于过滤 OOD 示例。为了避免阈值估计的需要，我们开发了一个双测量加权损失函数来安全地执行未标记训练。在多个基准数据集上的大量实验验证了该方法在面向对象检测和 SSMLL 任务中的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Asymmetric+Beta+Loss+for+Evidence-Based+Safe+Semi-Supervised+Multi-Label+Learning)|0|
|[An Unsupervised Learning Framework Combined with Heuristics for the Maximum Minimal Cut Problem](https://doi.org/10.1145/3637528.3671704)|Huaiyuan Liu, Xianzhang Liu, Donghua Yang, Hongzhi Wang, Yingchi Long, Mengtong Ji, Dongjing Miao, Zhiyu Liang|Harbin Institute of Technology, Harbin, China|The Maximum Minimal Cut Problem (MMCP), a NP-hard combinatorial optimization (CO) problem, has not received much attention due to the demanding and challenging bi-connectivity constraint. Moreover, as a CO problem, it is also a daunting task for machine learning, especially without labeled instances. To deal with these problems, this work proposes an unsupervised learning framework combined with heuristics for MMCP that can provide valid and high-quality solutions. As far as we know, this is the first work that explores machine learning and heuristics to solve MMCP. The unsupervised solver is inspired by a relaxation-plus-rounding approach, the relaxed solution is parameterized by graph neural networks, and the cost and penalty of MMCP are explicitly written out, which can train the model end-to-end. A crucial observation is that each solution corresponds to at least one spanning tree. Based on this finding, a heuristic solver that implements tree transformations by adding vertices is utilized to repair and improve the solution quality of the unsupervised solver. Alternatively, the graph is simplified while guaranteeing solution consistency, which reduces the running time. We conduct extensive experiments to evaluate our framework and give a specific application. The results demonstrate the superiority of our method against two techniques designed.|最大最小切入问题(MMCP)是一个 NP 组合优化问题，由于双连通性约束的要求和挑战，它并没有得到很多关注。此外，作为一个 CO 问题，它也是一个令人畏缩的机器学习任务，特别是没有标记的实例。为了解决这些问题，这项工作提出了一个非监督式学习框架结合启发式的 MMCP，可以提供有效和高质量的解决方案。据我们所知，这是第一个研究机器学习和启发式算法来解决 MMCP 的工作。无监督求解器采用松弛-舍入法，松弛解用图形神经网络参数化，明确写出 MMCP 的代价和代价，可以对模型进行端到端的训练。一个重要的观察结果是，每个解对应至少一个生成树。在此基础上，提出了一种通过增加顶点实现树变换的启发式求解器，用于修复和提高无监督求解器的求解质量。或者，在保证解的一致性的同时简化图形，从而减少运行时间。我们进行了广泛的实验，以评估我们的框架，并给出了具体的应用。实验结果证明了该方法对两种设计技术的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Unsupervised+Learning+Framework+Combined+with+Heuristics+for+the+Maximum+Minimal+Cut+Problem)|0|
|[ACER: Accelerating Complex Event Recognition via Two-Phase Filtering under Range Bitmap-Based Indexes](https://doi.org/10.1145/3637528.3671814)|Shizhe Liu, Haipeng Dai, Shaoxu Song, Meng Li, Jingsong Dai, Rong Gu, Guihai Chen|BNRist, Tsinghua University, Beijing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China|Complex event recognition (CER) refers to identifying specific patterns composed of several primitive events in event stores. Since full-scanning event stores to identify primitive events holding query constraint conditions will incur costly I/O overhead, a mainstream and practical approach is using index techniques to obtain these events. However, prior index-based approaches suffer from significant I/O and sorting overhead when dealing with high predicate selectivity or long query window (common in real-world applications), which leads to high query latency. To address this issue, we propose ACER, a Range Bitmap-based index, to accelerate CER. Firstly, ACER achieves a low index space overhead by grouping the events with the same type into a cluster and compressing the cluster data, alleviating the I/O overhead of reading indexes. Secondly, ACER builds Range Bitmaps in batch (block) for queried attributes and ensures that the events of each cluster in the index block are chronologically ordered. Then, ACER can always obtain ordered query results for a specific event type through merge operations, avoiding sorting overhead. Most importantly, ACER avoids unnecessary disk access in indexes and events via two-phase filtering based on the window condition, thus alleviating the I/O overhead further. Our experiments on six real-world and synthetic datasets demonstrate that ACER reduces the query latency by up to one order of magnitude compared with SOTA techniques.|复杂事件识别(CER)是指识别由事件存储中的几个基本事件组成的特定模式。由于全面扫描事件存储以识别持有查询约束条件的基本事件将产生高昂的 I/O 开销，因此主流和实用的方法是使用索引技术来获取这些事件。然而，以前的基于索引的方法在处理高谓词选择性或长查询窗口(在现实世界的应用程序中很常见)时会遇到严重的 I/O 和排序开销，从而导致高查询延迟。为了解决这个问题，我们提出 ACER，一个基于范围位图的索引，以加速 CER。首先，ACER 通过将相同类型的事件分组到一个集群中并压缩集群数据，降低了读取索引的 I/O 开销，从而实现了较低的索引空间开销。其次，ACER 以批处理(块)方式为查询属性构建 Range Bitmap，并确保索引块中每个集群的事件按时间顺序排序。然后，ACER 总是可以通过合并操作获得特定事件类型的有序查询结果，从而避免排序开销。最重要的是，ACER 通过基于窗口条件的两阶段过滤，避免了索引和事件中不必要的磁盘访问，从而进一步减少了 I/O 开销。我们在六个真实世界和合成数据集上的实验表明，与 SOTA 技术相比，ACER 可以减少多达一个数量级的查询延迟。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ACER:+Accelerating+Complex+Event+Recognition+via+Two-Phase+Filtering+under+Range+Bitmap-Based+Indexes)|0|
|[Revisiting Modularity Maximization for Graph Clustering: A Contrastive Learning Perspective](https://doi.org/10.1145/3637528.3671967)|Yunfei Liu, Jintang Li, Yuehe Chen, Ruofan Wu, Ericbk Wang, Jing Zhou, Sheng Tian, Shuheng Shen, Xing Fu, Changhua Meng, Weiqiang Wang, Liang Chen|Ant Group, Hangzhou, China; Unaffiliated, Guangzhou, China; Ant Group, Hangzhou, Zhejiang, China|Graph clustering, a fundamental and challenging task in graph mining, aims to classify nodes in a graph into several disjoint clusters. In recent years, graph contrastive learning (GCL) has emerged as a dominant line of research in graph clustering and advances the new state-of-the-art. However, GCL-based methods heavily rely on graph augmentations and contrastive schemes, which may potentially introduce challenges such as semantic drift and scalability issues. Another promising line of research involves the adoption of modularity maximization, a popular and effective measure for community detection, as the guiding principle for clustering tasks. Despite the recent progress, the underlying mechanism of modularity maximization is still not well understood. In this work, we dig into the hidden success of modularity maximization for graph clustering. Our analysis reveals the strong connections between modularity maximization and graph contrastive learning, where positive and negative examples are naturally defined by modularity. In light of our results, we propose a community-aware graph clustering framework, coined øurs, which leverages modularity maximization as a contrastive pretext task to effectively uncover the underlying information of communities in graphs, while avoiding the problem of semantic drift. Extensive experiments on multiple graph datasets verify the effectiveness of øurs in terms of scalability and clustering performance compared to state-of-the-art graph clustering methods. Notably, øurs easily scales a sufficiently large graph with 100M nodes while outperforming strong baselines.|图聚类是图挖掘中的一项基础性和挑战性工作，其目的是将一个图中的节点划分为若干个不相交的聚类。近年来，图形对比学习(GCL)已经成为图形聚类的主流研究方向，并取得了新的进展。然而，基于 GCL 的方法在很大程度上依赖于图增强和对比方案，这可能会带来诸如语义漂移和可伸缩性问题等挑战。另一个很有前途的研究方向是采用模块化最大化作为聚类任务的指导原则。模块化最大化是一种流行的、有效的社区检测方法。尽管最近取得了一些进展，但是模块化最大化的潜在机制仍然没有得到很好的理解。在本文中，我们深入研究了图聚类中模块化最大化的隐藏成功。我们的分析揭示了模块化最大化和图的对比学习之间的紧密联系，其中积极和消极的例子是由模块化自然定义的。根据我们的研究结果，我们提出了一个社区感知的图聚类框架，称为 øurs，它利用模块化最大化作为一个对比的借口任务来有效地揭示图中社区的潜在信息，同时避免了语义漂移的问题。通过对多个图形数据集的大量实验，验证了 øurs 在可伸缩性和聚类性能方面的有效性，并与现有的图形聚类方法进行了比较。值得注意的是，øurs 可以轻松地扩展拥有100m 节点的足够大图，同时表现优于强基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+Modularity+Maximization+for+Graph+Clustering:+A+Contrastive+Learning+Perspective)|0|
|[Graph Data Condensation via Self-expressive Graph Structure Reconstruction](https://doi.org/10.1145/3637528.3671710)|Zhanyu Liu, Chaolv Zeng, Guanjie Zheng|Shanghai Jiao Tong University, Shanghai, China|With the increasing demands of training graph neural networks (GNNs) on large-scale graphs, graph data condensation has emerged as a critical technique to relieve the storage and time costs during the training phase. It aims to condense the original large-scale graph to a much smaller synthetic graph while preserving the essential information necessary for efficiently training a downstream GNN. However, existing methods concentrate either on optimizing node features exclusively or endeavor to independently learn node features and the graph structure generator. They could not explicitly leverage the information of the original graph structure and failed to construct an interpretable graph structure for the synthetic dataset. To address these issues, we introduce a novel framework named Graph Data Condensation via Self-expressive Graph Structure Reconstruction (GCSR). Our method stands out by (1) explicitly incorporating the original graph structure into the condensing process and (2) capturing the nuanced interdependencies between the condensed nodes by reconstructing an interpretable self-expressive graph structure. Extensive experiments and comprehensive analysis validate the efficacy of the proposed method across diverse GNN models and datasets. Our code is available at https://github.com/zclzcl0223/GCSR.|随着训练图神经网络对大规模图形需求的不断增加，图形数据压缩已经成为降低训练阶段存储量和时间成本的关键技术。它的目的是将原始的大规模图压缩成一个小得多的合成图，同时保留有效训练下游 GNN 所需的必要信息。然而，现有的方法要么专门优化节点特征，要么致力于独立学习节点特征和图结构生成器。它们无法显式地利用原始图结构的信息，也无法为合成数据集构造可解释的图结构。为了解决这些问题，我们提出了一种基于自表达图结构重构(GCSR)的图数据压缩框架。该方法的突出之处在于: (1)将原始的图结构明确地融入到压缩过程中; (2)通过重构一个可解释的自我表达图结构来捕捉压缩节点之间微妙的相互依赖关系。大量的实验和综合分析验证了该方法在不同 GNN 模型和数据集上的有效性。我们的代码可以在 https://github.com/zclzcl0223/gcsr 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Data+Condensation+via+Self-expressive+Graph+Structure+Reconstruction)|0|
|[Generative Pretrained Hierarchical Transformer for Time Series Forecasting](https://doi.org/10.1145/3637528.3671855)|Zhiding Liu, Jiqian Yang, Mingyue Cheng, Yucong Luo, Zhi Li|; Shenzhen International Graduate School, Tsinghua University, Shenzhen, Guangdong, China|Recent efforts have been dedicated to enhancing time series forecasting accuracy by introducing advanced network architectures and self-supervised pretraining strategies. Nevertheless, existing approaches still exhibit two critical drawbacks. Firstly, these methods often rely on a single dataset for training, limiting the model's generalizability due to the restricted scale of the training data. Secondly, the one-step generation schema is widely followed, which necessitates a customized forecasting head and overlooks the temporal dependencies in the output series, and also leads to increased training costs under different horizon length settings. To address these issues, we propose a novel generative pretrained hierarchical transformer architecture for forecasting, named GPHT. There are two aspects of key designs in GPHT. On the one hand, we advocate for constructing a mixed dataset under the channel-independent assumption for pretraining our model, comprising various datasets from diverse data scenarios. This approach significantly expands the scale of training data, allowing our model to uncover commonalities in time series data and facilitating improved transfer to specific datasets. On the other hand, GPHT employs an auto-regressive forecasting approach, effectively modeling temporal dependencies in the output series. Importantly, no customized forecasting head is required, enablinga single model to forecast at arbitrary horizon settings. We conduct sufficient experiments on eight datasets with mainstream self-supervised pretraining models and supervised models. The results demonstrated that GPHT surpasses the baseline models across various fine-tuning and zero/few-shot learning settings in the traditional long-term forecasting task, providing support for verifying the feasibility of pretraining time series large models. We make our codes publicly available\footnotehttps://github.com/icantnamemyself/GPHT.|通过引入先进的网络结构和自我监督预训练策略，最近致力于提高时间序列预测的准确性。尽管如此，现有的方法仍然表现出两个关键的缺点。首先，这些方法往往依赖于一个单一的数据集进行训练，由于训练数据的规模有限，限制了模型的通用性。其次，一步发电模式得到了广泛的遵循，它需要一个定制的预测头，忽略了输出序列中的时间依赖性，并导致在不同的时域长度设置下培训成本的增加。为了解决这些问题，我们提出了一种新的预生成预训练分层变压器结构，称为 GPHT。GPHT 的关键设计包括两个方面。一方面，我们提倡在通道无关的假设下构建一个混合数据集来预训练我们的模型，包括来自不同数据场景的各种数据集。这种方法极大地扩展了训练数据的规模，使我们的模型能够发现时间序列数据中的共性，并促进改进对特定数据集的传输。另一方面，GPHT 采用自回归预测方法，有效地建立了输出序列中的时间依赖关系。重要的是，没有定制的预测头是必需的，使一个单一的模型能够预测在任意的水平设置。我们使用主流的自监督预训练模型和监督模型对八个数据集进行了充分的实验。结果表明，在传统的长期预测任务中，GPHT 超越了各种微调和零/少镜头学习环境下的基线模型，为验证预训练时间序列大模型的可行性提供了支持。我们使我们的代码公开脚注 https:// github.com/icantnamemyself/gpht。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+Pretrained+Hierarchical+Transformer+for+Time+Series+Forecasting)|0|
|[AIM: Attributing, Interpreting, Mitigating Data Unfairness](https://doi.org/10.1145/3637528.3671797)|Zhining Liu, Ruizhong Qiu, Zhichen Zeng, Yada Zhu, Hendrik F. Hamann, Hanghang Tong|University of Illinois Urbana-Champaign, Urbana, IL, USA; IBM Research, Yorktown Heights, NY, USA|Data collected in the real world often encapsulates historical discriminationagainst disadvantaged groups and individuals. Existing fair machine learning(FairML) research has predominantly focused on mitigating discriminative biasin the model prediction, with far less effort dedicated towards exploring howto trace biases present in the data, despite its importance for thetransparency and interpretability of FairML. To fill this gap, we investigate anovel research problem: discovering samples that reflect biases/prejudices fromthe training data. Grounding on the existing fairness notions, we lay out asample bias criterion and propose practical algorithms for measuring andcountering sample bias. The derived bias score provides intuitive sample-levelattribution and explanation of historical bias in data. On this basis, wefurther design two FairML strategies via sample-bias-informed minimal dataediting. They can mitigate both group and individual unfairness at the cost ofminimal or zero predictive utility loss. Extensive experiments and analyses onmultiple real-world datasets demonstrate the effectiveness of our methods inexplaining and mitigating unfairness. Code is available athttps://github.com/ZhiningLiu1998/AIM.|在现实世界中收集的数据往往概括了历史上对弱势群体和个人的歧视。现有的公平机器学习(FairML)研究主要集中在减轻模型预测中的歧视性偏差，而很少致力于探索如何跟踪数据中存在的偏差，尽管它对于 FairML 的透明度和可解释性非常重要。为了填补这个空白，我们调查了一个新的研究问题: 发现样本反映训练数据的偏见/偏见。在现有公平性概念的基础上，给出了样本偏差的判定准则，并提出了测量和抵消样本偏差的实用算法。导出的偏差评分提供了直观的样本水平归因和历史偏差的数据解释。在此基础上，进一步设计了两种基于样本偏差信息的最小数据编辑的 FairML 策略。它们可以以最小或零预测效用损失为代价来缓解群体和个人的不公平。对多个真实世界数据集的大量实验和分析证明了我们的方法在解释和减少不公平方面的有效性。代码可以通过 https:// github.com/zhiningliu1998/aim 获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AIM:+Attributing,+Interpreting,+Mitigating+Data+Unfairness)|0|
|[High-Dimensional Distributed Sparse Classification with Scalable Communication-Efficient Global Updates](https://doi.org/10.1145/3637528.3672038)|Fred Lu, Ryan R. Curtin, Edward Raff, Francis Ferraro, James Holt|Booz Allen Hamilton, McLean, USA; University of Maryland, Baltimore County, Baltimore, USA; Booz Allen Hamilton & University of Maryland, Baltimore County, McLean, USA; Laboratory for Physical Sciences, College Park, USA|As the size of datasets used in statistical learning continues to grow, distributed training of models has attracted increasing attention. These methods partition the data and exploit parallelism to reduce memory and runtime, but suffer increasingly from communication costs as the data size or the number of iterations grows. Recent work on linear models has shown that a surrogate likelihood can be optimized locally to iteratively improve on an initial solution in a communication-efficient manner. However, existing versions of these methods experience multiple shortcomings as the data size becomes massive, including diverging updates and efficiently handling sparsity. In this work we develop solutions to these problems which enable us to learn a communication-efficient distributed logistic regression model even beyond millions of features. In our experiments we demonstrate a large improvement in accuracy over distributed algorithms with only a few distributed update steps needed, and similar or faster runtimes. Our code is available at https://github.com/FutureComputing4AI/ProxCSL.|随着用于统计学习的数据集规模的不断扩大，模型的分布式训练越来越受到人们的关注。这些方法对数据进行分区并利用并行性来减少内存和运行时，但是随着数据大小或迭代次数的增加，通信成本越来越高。最近对线性模型的研究表明，代理似然可以局部优化，以便以通信有效的方式迭代地改进初始解。然而，这些方法的现有版本在数据量变得巨大时会遇到多重缺陷，包括不同的更新和有效地处理稀疏性。在这项工作中，我们开发这些问题的解决方案，使我们能够学习一个通信效率高的分布式 Logit模型模型，甚至超过数百万个功能。在我们的实验中，我们证明了分布式算法在精度上有了很大的提高，只需要少量的分布式更新步骤，以及类似或更快的运行时。我们的代码可以在 https://github.com/futurecomputing4ai/proxcsl 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=High-Dimensional+Distributed+Sparse+Classification+with+Scalable+Communication-Efficient+Global+Updates)|0|
|[FUGNN: Harmonizing Fairness and Utility in Graph Neural Networks](https://doi.org/10.1145/3637528.3671834)|Renqiang Luo, Huafei Huang, Shuo Yu, Zhuoyang Han, Estrid He, Xiuzhen Zhang, Feng Xia|Dalian University of Technology, Dalian, Liaoning, China; RMIT University, Melbourne, Victoria, Australia|Fairness-aware Graph Neural Networks (GNNs) often face a challenging trade-off, where prioritizing fairness may require compromising utility. In this work, we re-examine fairness through the lens of spectral graph theory, aiming to reconcile fairness and utility within the framework of spectral graph learning. We explore the correlation between sensitive features and spectrum in GNNs, using theoretical analysis to delineate the similarity between original sensitive features and those after convolution under different spectra. Our analysis reveals a reduction in the impact of similarity when the eigenvectors associated with the largest magnitude eigenvalue exhibit directional similarity. Based on these theoretical insights, we propose FUGNN, a novel spectral graph learning approach that harmonizes the conflict between fairness and utility. FUGNN ensures algorithmic fairness and utility by truncating the spectrum and optimizing eigenvector distribution during the encoding process. The fairness-aware eigenvector selection reduces the impact of convolution on sensitive features while concurrently minimizing the sacrifice of utility. FUGNN further optimizes the distribution of eigenvectors through a transformer architecture. By incorporating the optimized spectrum into the graph convolution network, FUGNN effectively learns node representations. Experiments on six real-world datasets demonstrate the superiority of FUGNN over baseline methods. The codes are available at https://github.com/yushuowiki/FUGNN.|公平感知图形神经网络(GNN)经常面临一个具有挑战性的权衡，其中优先考虑公平可能需要妥协的效用。本文从光谱图理论的角度重新审视公平性问题，力求在光谱图学习的框架内协调公平性和效用性。通过理论分析，揭示了不同光谱下原始敏感特征与卷积后特征之间的相似性，探讨了不同光谱下原始敏感特征与卷积后特征之间的相似性。我们的分析表明，当与最大幅度特征值相关的特征向量表现出方向相似性时，相似性的影响减小。基于这些理论见解，我们提出了 FUGNN，一种新的光谱图学习方法，协调公平和效用之间的冲突。FUGNN 通过在编码过程中截断频谱和优化特征向量分布来保证算法的公平性和实用性。公平感知的特征向量选择降低了卷积对敏感特征的影响，同时最小化了效用的牺牲。FUGNN 通过变压器结构进一步优化了特征向量的分布。通过将优化后的频谱融入到图卷积网络中，FUGNN 有效地学习节点表示。在六个实际数据集上的实验表明了 FUGNN 方法优于基线方法。密码可以在 https://github.com/yushuowiki/fugnn 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FUGNN:+Harmonizing+Fairness+and+Utility+in+Graph+Neural+Networks)|0|
|[Learning Multi-view Molecular Representations with Structured and Unstructured Knowledge](https://doi.org/10.1145/3637528.3672043)|Yizhen Luo, Kai Yang, Massimo Hong, Xing Yi Liu, Zikun Nie, Hao Zhou, Zaiqing Nie|Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China; Institute for AI Industry Research (AIR), Tsinghua University & Pharmolix Inc., Beijing, China|Capturing molecular knowledge with representation learning approaches holdssignificant potential in vast scientific fields such as chemistry and lifescience. An effective and generalizable molecular representation is expected tocapture the consensus and complementary molecular expertise from diverse viewsand perspectives. However, existing works fall short in learning multi-viewmolecular representations, due to challenges in explicitly incorporating viewinformation and handling molecular knowledge from heterogeneous sources. Toaddress these issues, we present MV-Mol, a molecular representation learningmodel that harvests multi-view molecular expertise from chemical structures,unstructured knowledge from biomedical texts, and structured knowledge fromknowledge graphs. We utilize text prompts to model view information and designa fusion architecture to extract view-based molecular representations. Wedevelop a two-stage pre-training procedure, exploiting heterogeneous data ofvarying quality and quantity. Through extensive experiments, we show thatMV-Mol provides improved representations that substantially benefit molecularproperty prediction. Additionally, MV-Mol exhibits state-of-the-art performancein multi-modal comprehension of molecular structures and texts. Code and dataare available at https://github.com/PharMolix/OpenBioMed.|利用表征学习方法获取分子知识在化学和生命科学等广阔的科学领域具有巨大的潜力。一个有效的和可推广的分子代表性预计将捕获共识和互补的分子专业知识从不同的观点和角度。然而，现有的工作在学习多视角分子表征方面存在不足，这是由于在明确合并视角信息和处理来自异质来源的分子知识方面存在挑战。为了解决这些问题，我们提出了 MV-Mol 分子表示学习模型，它从化学结构中收集多视角的分子专业知识，从生物医学文本中收集非结构化知识，从知识图表中收集结构化知识。利用文本提示对视图信息进行建模，设计融合结构提取基于视图的分子表示。我们开发了一个两阶段的预训练过程，利用不同质量和数量的异构数据。通过广泛的实验，我们表明，MV-Mol 提供了改进的表示，大大有利于分子性质预测。此外，MV-Mol 在分子结构和文本的多模态理解方面表现出最先进的性能。代码和数据可在 https://github.com/pharmolix/openbiomed 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Multi-view+Molecular+Representations+with+Structured+and+Unstructured+Knowledge)|0|
|[Cross-Context Backdoor Attacks against Graph Prompt Learning](https://doi.org/10.1145/3637528.3671956)|Xiaoting Lyu, Yufei Han, Wei Wang, Hangwei Qian, Ivor Tsang, Xiangliang Zhang|CFAR, ASTAR, Singapore, Singapore; University of Notre Dame, Notre Dame, USA; Beijing Jiaotong University & Xi'an Jiaotong University, Beijing, China; School of Computer Science and Technology, Beijing Jiaotong University, Beijing, China; Inria, Univ. Rennes, IRISA, Rennes, France|Graph Prompt Learning (GPL) bridges significant disparities betweenpretraining and downstream applications to alleviate the knowledge transferbottleneck in real-world graph learning. While GPL offers superioreffectiveness in graph knowledge transfer and computational efficiency, thesecurity risks posed by backdoor poisoning effects embedded in pretrainedmodels remain largely unexplored. Our study provides a comprehensive analysisof GPL's vulnerability to backdoor attacks. We introduce CrossBA, thefirst cross-context backdoor attack against GPL, which manipulates only thepretraining phase without requiring knowledge of downstream applications. Ourinvestigation reveals both theoretically and empirically that tuning triggergraphs, combined with prompt transformations, can seamlessly transfer thebackdoor threat from pretrained encoders to downstream applications. Throughextensive experiments involving 3 representative GPL methods across 5 distinctcross-context scenarios and 5 benchmark datasets of node and graphclassification tasks, we demonstrate that CrossBA consistentlyachieves high attack success rates while preserving the functionality ofdownstream applications over clean input. We also explore potentialcountermeasures against CrossBA and conclude that current defenses areinsufficient to mitigate CrossBA. Our study highlights the persistentbackdoor threats to GPL systems, raising trustworthiness concerns in thepractices of GPL techniques.|图形提示学习(Graph Prompt Learning，GPL)弥合了预训练和下游应用之间的显著差异，缓解了现实世界图形学习中的知识转移瓶颈。虽然 GPL 在图形知识转移和计算效率方面提供了优越的有效性，但是嵌入在预先训练的模型中的后门中毒效应所带来的安全风险仍然在很大程度上没有被探索。我们的研究提供了一个全面的分析 GPL 的脆弱性对后门攻击。我们引入了 CrossBA，第一个针对 GPL 的跨上下文后门攻击，它只操纵预训练阶段，而不需要了解下游应用程序。我们的研究从理论和经验上揭示了调优触发器，结合及时的转换，可以无缝地将后门威胁从预先训练的编码器转移到下游应用程序。通过涉及5个不同跨上下文场景和5个节点和图形分类任务的基准数据集的3种代表性 GPL 方法的广泛实验，我们证明 CrossBA 一致地实现高攻击成功率，同时保留下游应用程序的功能而不是干净的输入。我们还探讨了针对 CrossBA 的潜在对策，并得出结论: 目前的防御不足以减轻 CrossBA。我们的研究强调了对 GPL 系统持续的后门威胁，提高了 GPL 技术实践中的可信度问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cross-Context+Backdoor+Attacks+against+Graph+Prompt+Learning)|0|
|[PolyFormer: Scalable Node-wise Filters via Polynomial Graph Transformer](https://doi.org/10.1145/3637528.3671849)|Jiahong Ma, Mingguo He, Zhewei Wei|Renmin University of China, Beijing, China|Spectral Graph Neural Networks have demonstrated superior performance in graph representation learning. However, many current methods focus on employing shared polynomial coefficients for all nodes, i.e., learning node-unified filters, which limits the filters' flexibility for node-level tasks. The recent DSF attempts to overcome this limitation by learning node-wise coefficients based on positional encoding. However, the initialization and updating process of the positional encoding are burdensome, hindering scalability on large-scale graphs. In this work, we propose a scalable node-wise filter, PolyAttn. Leveraging the attention mechanism, PolyAttn can directly learn node-wise filters in an efficient manner, offering powerful representation capabilities. Building on PolyAttn, we introduce the whole model, named PolyFormer. In the lens of Graph Transformer models, PolyFormer, which calculates attention scores within nodes, shows great scalability. Moreover, the model captures spectral information, enhancing expressiveness while maintaining efficiency. With these advantages, PolyFormer offers a desirable balance between scalability and expressiveness for node-level tasks. Extensive experiments demonstrate that our proposed methods excel at learning arbitrary node-wise filters, showing superior performance on both homophilic and heterophilic graphs, and handling graphs containing up to 100 million nodes. The code is available at https://github.com/air029/PolyFormer.|谱图神经网络在图表示学习中表现出了优越的性能。然而，目前的方法主要集中在对所有节点采用共享多项式系数，即学习节点统一过滤器，这限制了过滤器对节点级任务的灵活性。最近的 DSF 试图通过基于位置编码的节点系数学习来克服这一限制。然而，位置编码的初始化和更新过程非常繁琐，阻碍了大规模图的可扩展性。在这项工作中，我们提出了一个可伸缩的节点明智的过滤器，PolyAttn。利用注意力机制，PolyAttn 可以以一种有效的方式直接学习节点过滤器，提供强大的表示能力。在 PolyAttn 的基础上，我们引入了一个完整的模型，命名为 PolyForm。在图形变换模型的透镜中，计算节点内注意力分数的 PolyForm 具有很强的可扩展性。此外，该模型捕获光谱信息，提高了表现力，同时保持了效率。利用这些优势，PolyForm 为节点级任务提供了可伸缩性和表达性之间的理想平衡。广泛的实验表明，我们提出的方法擅长学习任意节点明确的过滤器，显示出优越的性能在同亲和异亲图，并处理图包含多达1亿个节点。密码可在 https://github.com/air029/polyformer 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PolyFormer:+Scalable+Node-wise+Filters+via+Polynomial+Graph+Transformer)|0|
|[Scalable Differentiable Causal Discovery in the Presence of Latent Confounders with Skeleton Posterior](https://doi.org/10.1145/3637528.3672031)|Pingchuan Ma, Rui Ding, Qiang Fu, Jiaru Zhang, Shuai Wang, Shi Han, Dongmei Zhang|Microsoft Research, Beijing, China; HKUST, Hong Kong, Hong Kong; Shanghai Jiao Tong University, Shanghai, China|Differentiable causal discovery has made significant advancements in the learning of directed acyclic graphs. However, its application to real-world datasets remains restricted due to the ubiquity of latent confounders and the requirement to learn maximal ancestral graphs (MAGs). To date, existing differentiable MAG learning algorithms have been limited to small datasets and failed to scale to larger ones (e.g., with more than 50 variables). The key insight in this paper is that the causal skeleton, which is the undirected version of the causal graph, has potential for improving accuracy and reducing the search space of the optimization procedure, thereby enhancing the performance of differentiable causal discovery. Therefore, we seek to address a two-fold challenge to harness the potential of the causal skeleton for differentiable causal discovery in the presence of latent confounders: (1) scalable and accurate estimation of skeleton and (2) universal integration of skeleton estimation with differentiable causal discovery. To this end, we propose SPOT (Skeleton Posterior-guided OpTimization), a two-phase framework that harnesses skeleton posterior for differentiable causal discovery in the presence of latent confounders. On the contrary to a "point-estimation", SPOT seeks to estimate the posterior distribution of skeletons given the dataset. It first formulates the posterior inference as an instance of amortized inference problem and concretizes it with a supervised causal learning (SCL)-enabled solution to estimate the skeleton posterior. To incorporate the skeleton posterior with differentiable causal discovery, SPOT then features a skeleton posterior-guided stochastic optimization procedure to guide the optimization of MAGs. Extensive experiments on various datasets show that SPOT substantially outperforms SOTA methods for MAG learning. SPOT also demonstrates its effectiveness in the accuracy of skeleton posterior estimation in comparison with non-parametric bootstrap-based, or more recently, variational inference-based methods. Finally, we observe that the adoption of skeleton posterior exhibits strong promise in various causal discovery tasks.|可微因果发现在有向无环图的学习方面取得了显著的进展。然而，由于潜在混杂因素的普遍存在以及学习最大祖先图(MAG)的要求，其在现实世界数据集中的应用仍然受到限制。迄今为止，现有的可微 MAG 学习算法一直局限于小数据集，无法扩展到更大的数据集(例如，有超过50个变量)。本文的核心观点是因果骨架是因果图的无向版本，它有可能提高优化过程的精度，减少搜索空间，从而提高可微因果发现的性能。因此，我们试图解决在潜在混杂因素存在下利用因果骨架潜力进行可微因果发现的双重挑战: (1)骨架的可扩展和准确估计，以及(2)骨架估计与可微因果发现的普遍整合。为此，我们提出了 SPOT (骨架后验引导的优化) ，一个两阶段的框架，利用骨架后验在潜在的混杂因素存在的可微因果发现。与“点估计”相反，SPOT 试图估计给定数据集的骨架后验概率。该方法首先将后验推理作为摊销推理问题的一个实例进行描述，然后将其具体化为一个基于监督因果学习(SCL)的解决方案来估计骨架的后验。为了将骨架后验与可微因果发现结合起来，SPOT 提出了骨架后验引导的随机优化方法，用于指导 MAGs 的优化。在各种数据集上的大量实验表明，SPOT 方法在 MAG 学习方面优于 SOTA 方法。SPOT 算法在骨架后验估计的准确性方面也证明了其有效性，并与非参数自举方法或最近的变分推理方法进行了比较。最后，我们观察到骨架后验在各种因果发现任务中表现出很强的前景。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scalable+Differentiable+Causal+Discovery+in+the+Presence+of+Latent+Confounders+with+Skeleton+Posterior)|0|
|[Graph Anomaly Detection with Few Labels: A Data-Centric Approach](https://doi.org/10.1145/3637528.3671929)|Xiaoxiao Ma, Ruikun Li, Fanzhen Liu, Kaize Ding, Jian Yang, Jia Wu|Business School, The University of Sydney, Sydney, New South Wales, Australia; Department of Statistics and Data Science, Northwestern University, Evanston, Illinois, USA; School of Computing, Macquarie University, Sydney, New South Wales, Australia|Anomalous node detection in a static graph faces significant challenges due to the rarity of anomalies and the substantial cost of labeling their deviant structure and attribute patterns. These challenges give rise to data-centric problems, including extremely imbalanced data distributions and intricate graph learning, which significantly impede machine learning and deep learning methods from discerning the patterns of graph anomalies with few labels. While these issues remain crucial, much of the current research focuses on addressing the induced technical challenges, treating the shortage of labeled data as a given. Distinct from previous efforts, this work focuses on tackling the data-centric problems by generating auxiliary training nodes that conform to the original graph topology and attribute distribution. We categorize this approach as data-centric, aiming to enhance existing anomaly detectors by training them on our synthetic data. However, the methods for generating nodes and the effectiveness of utilizing synthetic data for graph anomaly detection remain unexplored in the realm. To answer these questions, we thoroughly investigate the denoising diffusion model. Drawing from our observations on the diffusion process, we illuminate the shifts in graph energy distribution and establish two principles for designing denoising neural networks tailored to graph anomaly generation. From the insights, we propose a diffusion-based graph generation method to synthesize training nodes, which can be promptly integrated to work with existing anomaly detectors. The empirical results on eight widely-used datasets demonstrate our generated data can effectively enhance the nine state-of-the-art graph detectors' performance.|静态图中的异常节点检测由于异常的罕见性以及标记异常结构和属性模式的巨大成本而面临严峻的挑战。这些挑战引发了以数据为中心的问题，包括极不平衡的数据分布和复杂的图学习，这些问题严重阻碍了机器学习和深度学习方法识别少标签的图形异常模式。虽然这些问题仍然是至关重要的，目前的研究大多集中在解决诱发的技术挑战，治疗短缺的标记数据作为一个给定。与以往的工作不同，这项工作的重点是通过生成符合原始图拓扑和属性分布的辅助训练节点来解决以数据为中心的问题。我们把这种方法归类为以数据为中心，目的是通过在我们的合成数据上训练现有的异常检测器来增强它们。然而，生成节点的方法以及利用合成数据进行图形异常检测的有效性在该领域仍未得到探索。为了回答这些问题，我们深入研究了去噪扩散模型。根据我们对扩散过程的观察，我们阐明了图能量分布的变化，并建立了适合于图异常生成的去噪神经网络设计的两个原则。基于此，我们提出了一种基于扩散的图生成方法来合成训练节点，该方法可以及时地与现有的异常检测器集成。对八个广泛使用的数据集的实验结果表明，我们生成的数据可以有效地提高九个最先进的图形检测器的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Anomaly+Detection+with+Few+Labels:+A+Data-Centric+Approach)|0|
|[A Uniformly Bounded Correlation Function for Spatial Point Patterns](https://doi.org/10.1145/3637528.3671891)|Evgenia Martynova, Johannes Textor|Radboud University, Nijmegen, Netherlands; Radboud University Medical Center, Nijmegen, Netherlands|A point pattern is a dataset of coordinates, typically in 2D or 3D space. Point patterns are ubiquitous in diverse applications including Geographic Information Systems, Astronomy, Ecology, Biology and Medicine. Among the statistics used to quantify point patterns, most are based on Ripley's K-function, which measures the deviation of the observed pattern from a completely random arrangement of points. This approach is useful for constructing null hypothesis tests, but Ripley's K and its variants are less suitable as quantitative effect sizes because their ranges and expected values generally depend on the scale or the size of the region in which the pattern is observed. To address this, we propose a new function that behaves like a correlation coefficient for point patterns: it is tightly bounded by -1 and 1, with a value of -1 corresponding to a maximally dispersed arrangement of points, 0 indicating complete spatial randomness, and 1 representing maximal clustering. These properties are independent of scale and observation window size assuming appropriate edge correction. Evaluating our function on simulated data, we show that it has comparable statistical calibration and power to K-based baselines. We hope that the ease of interpretation of our bounded function will facilitate the analysis of spatial data across multiple fields.|点模式是坐标的数据集，通常在2D 或3D 空间中。点模式在包括地理信息系统、天文学、生态学、生物学和医学在内的各种应用中无处不在。在用于量化点模式的统计数据中，大多数是基于 Ripley 的 K- 函数，它测量观察到的模式与完全随机的点排列的偏差。这种方法对于构建零假设检验是有用的，但是 Ripley’s K 及其变体不太适合作为定量效应大小，因为它们的范围和预期值通常取决于观察模式的区域的尺度或大小。为了解决这个问题，我们提出了一个新的函数，它的行为类似于点模式的相关系数: 它被 -1和1紧密约束，值为 -1对应于最大分散的点排列，0表示完全空间随机性，1表示最大聚类。假设适当的边缘校正，这些特性与尺度和观测窗口大小无关。在模拟数据上评估我们的函数，我们表明它具有可比较的统计校准和功能的 K 为基础的基线。我们希望我们有界函数的简易解释将有助于跨多个领域的空间数据分析。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Uniformly+Bounded+Correlation+Function+for+Spatial+Point+Patterns)|0|
|[Fair Column Subset Selection](https://doi.org/10.1145/3637528.3672005)|Antonis Matakos, Bruno Ordozgoiti, Suhas Thejaswi|Unaffiliated, London, United Kingdom; Aalto University, Espoo, Finland; Max Planck Institute for Software Systems, Kaiserslautern, Germany|We consider the problem of fair column subset selection. In particular, we assume that two groups are present in the data, and the chosen column subset must provide a good approximation for both, relative to their respective best rank-k approximations. We show that this fair setting introduces significant challenges: in order to extend known results, one cannot do better than the trivial solution of simply picking twice as many columns as the original methods. We adopt a known approach based on deterministic leverage-score sampling, and show that merely sampling a subset of appropriate size becomes NP-hard in the presence of two groups. Whereas finding a subset of two times the desired size is trivial, we provide an efficient algorithm that achieves the same guarantees with essentially 1.5 times that size. We validate our methods through an extensive set of experiments on real-world data.|我们考虑公平列子集选择问题。特别地，我们假设数据中存在两组，相对于它们各自的最佳秩-k 近似，所选择的列子集必须为两者提供一个很好的近似。我们展示了这种公平的设置带来了重大的挑战: 为了扩展已知的结果，最简单的解决方案就是简单地挑选两倍于原始方法的列。我们采用了一种基于确定性杠杆评分抽样的已知方法，并且证明了在两组情况下，仅仅抽样一个适当大小的子集就变得 NP 难。尽管寻找一个两倍于所需大小的子集是微不足道的，但是我们提供了一个有效的算法，它实现了1.5倍于所需大小的相同保证。我们通过对真实世界数据的大量实验来验证我们的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fair+Column+Subset+Selection)|0|
|[FLAIM: AIM-based Synthetic Data Generation in the Federated Setting](https://doi.org/10.1145/3637528.3671990)|Samuel Maddock, Graham Cormode, Carsten Maple|Meta AI & University of Warwick, Coventry, United Kingdom; University of Warwick, Coventry, United Kingdom|Preserving individual privacy while enabling collaborative data sharing is crucial for organizations. Synthetic data generation is one solution, producing artificial data that mirrors the statistical properties of private data. While numerous techniques have been devised under differential privacy, they predominantly assume data is centralized. However, data is often distributed across multiple clients in a federated manner. In this work, we initiate the study of federated synthetic tabular data generation. Building upon a SOTA central method known as AIM, we present DistAIM and FLAIM. We first show that it is straightforward to distribute AIM, extending a recent approach based on secure multi-party computation which necessitates additional overhead, making it less suited to federated scenarios. We then demonstrate that naively federating AIM can lead to substantial degradation in utility under the presence of heterogeneity. To mitigate both issues, we propose an augmented FLAIM approach that maintains a private proxy of heterogeneity. We simulate our methods across a range of benchmark datasets under different degrees of heterogeneity and show we can improve utility while reducing overhead.|在实现协作数据共享的同时保护个人隐私对组织来说至关重要。合成数据生成是一种解决方案，生成反映私有数据统计特性的人工数据。虽然在差分隐私下设计了许多技术，但它们主要假设数据是集中的。但是，数据通常以联邦的方式分布在多个客户机上。在这项工作中，我们开始了联邦综合表格数据生成的研究。在 SOTA 中心方法 AIM 的基础上，我们提出了 distAIM 和 FLAIM。我们首先展示了分发 AIM 的简单性，扩展了最近的一种基于安全多方计算的方法，这种方法需要额外的开销，因此不太适合联邦场景。然后我们证明，天真的联合 AIM 可以导致实用性在异质性的存在下显著降低。为了缓解这两个问题，我们提出了一个增强的 FLAIM 方法，维护一个私人代理的异构性。在不同程度的异构性下，我们在一系列基准数据集上模拟了我们的方法，并展示了我们可以在降低开销的同时提高效用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FLAIM:+AIM-based+Synthetic+Data+Generation+in+the+Federated+Setting)|0|
|[Interpretable Transformer Hawkes Processes: Unveiling Complex Interactions in Social Networks](https://doi.org/10.1145/3637528.3671720)|Zizhuo Meng, Ke Wan, Yadong Huang, Zhidong Li, Yang Wang, Feng Zhou|University of Illinois at Urbana-Champaign, Urbana, IL, USA; ; Zhoushan Academy of Marine Data Science, Zhoushan, China; The University of Technology Sydney, Sydney, Australia|Social networks represent complex ecosystems where the interactions between users or groups play a pivotal role in information dissemination, opinion formation, and social interactions. Effectively harnessing event sequence data within social networks to unearth interactions among users or groups has persistently posed a challenging frontier within the realm of point processes. Current deep point process models face inherent limitations within the context of social networks, constraining both their interpretability and expressive power. These models encounter challenges in capturing interactions among users or groups and often rely on parameterized extrapolation methods when modeling intensity over non-event intervals, limiting their capacity to capture intricate intensity patterns, particularly beyond observed events. To address these challenges, this study proposes modifications to Transformer Hawkes processes (THP), leading to the development of interpretable Transformer Hawkes processes (ITHP). ITHP inherits the strengths of THP while aligning with statistical nonlinear Hawkes processes, thereby enhancing its interpretability and providing valuable insights into interactions between users or groups. Additionally, ITHP enhances the flexibility of the intensity function over non-event intervals, making it better suited to capture complex event propagation patterns in social networks. Experimental results, both on synthetic and real data, demonstrate the effectiveness of ITHP in overcoming the identified limitations. Moreover, they highlight ITHP's applicability in the context of exploring the complex impact of users or groups within social networks. Our code is available at https://github.com/waystogetthere/Interpretable-Transformer- Hawkes-Process.git.|社交网络代表了复杂的生态系统，其中用户或群体之间的互动在信息传播、舆论形成和社会互动中起着关键作用。有效地利用社交网络中的事件序列数据来挖掘用户或群体之间的互动一直是点过程领域的一个具有挑战性的前沿问题。当前的深点过程模型面临着社会网络背景下固有的局限性，限制了它们的可解释性和表达能力。这些模型在捕获用户或组之间的交互方面遇到挑战，并且在非事件间隔期间建模强度时常常依赖参数化外推方法，从而限制了它们捕获复杂强度模式的能力，特别是在观察到的事件之外。为了应对这些挑战，本研究提出了变压器霍克斯过程(THP)的修改，导致了可解释的变压器霍克斯过程(ITHP)的发展。ITHP 继承了 THP 的优势，同时与统计非线性霍克斯过程保持一致，从而提高了其可解释性，并为用户或群体之间的交互提供了有价值的见解。此外，ITHP 增强了强度函数在非事件间隔上的灵活性，使其更适合于捕获社交网络中复杂的事件传播模式。实验结果，无论是合成的和真实的数据，证明了 ITHP 的有效性，克服了识别的限制。此外，他们还强调了 ITHP 在探索社交网络中用户或群体的复杂影响方面的适用性。我们的代码可以在 Hawkes-Process  https://github.com/waystogetthere/interpretable-transformer- 找到。饭桶。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interpretable+Transformer+Hawkes+Processes:+Unveiling+Complex+Interactions+in+Social+Networks)|0|
|[Scaling Training Data with Lossy Image Compression](https://doi.org/10.1145/3637528.3671904)|Katherine L. Mentzer, Andrea Montanari|Granica, Mountain View, CA, USA|Empirically-determined scaling laws have been broadly successful in predicting the evolution of large machine learning models with training data and number of parameters. As a consequence, they have been useful for optimizing the allocation of limited resources, most notably compute time. In certain applications, storage space is an important constraint, and data format needs to be chosen carefully as a consequence. Computer vision is a prominent example: images are inherently analog, but are always stored in a digital format using a finite number of bits. Given a dataset of digital images, the number of bits L to store each of them can be further reduced using lossy data compression. This, however, can degrade the quality of the model trained on such images, since each example has lower resolution. In order to capture this trade-off and optimize storage of training data, we propose a `storage scaling law' that describes the joint evolution of test error with sample size and number of bits per image. We prove that this law holds within a stylized model for image compression, and verify it empirically on two computer vision tasks, extracting the relevant parameters. We then show that this law can be used to optimize the lossy compression level. At given storage, models trained on optimally compressed images present a significantly smaller test error with respect to models trained on the original data. Finally, we investigate the potential benefits of randomizing the compression level.|经验确定的标度律已广泛成功地预测演化的大型机器学习模型的训练数据和参数数量。因此，它们对于优化有限资源的分配非常有用，尤其是计算时间。在某些应用程序中，存储空间是一个重要的限制，因此需要仔细选择数据格式。计算机视觉是一个突出的例子: 图像本质上是模拟的，但总是以数字格式存储，使用有限的位数。给定一个数字图像数据集，使用有损数据压缩可以进一步减少存储每个图像的比特 L 的数量。然而，由于每个例子都具有较低的分辨率，这会降低在这些图像上训练的模型的质量。为了捕获这种权衡并优化训练数据的存储，我们提出了一种“存储缩放律”，描述了测试误差与每幅图像的样本量和比特数的联合演化。我们证明这一定律适用于图像压缩的程式化模型，并在两个计算机视觉任务中进行实验验证，提取相关参数。然后我们证明这个定律可以用来优化有损数据压缩水平。在给定的存储器中，对最佳压缩图像进行训练的模型与对原始数据进行训练的模型相比，测试误差要小得多。最后，我们研究了随机化压缩级别的潜在好处。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scaling+Training+Data+with+Lossy+Image+Compression)|0|
|[Learning Causal Networks from Episodic Data](https://doi.org/10.1145/3637528.3671999)|Osman Mian, Sarah Mameche, Jilles Vreeken|CISPA Helmholtz Center for Information Security, Saarbruecken, Germany|In numerous real-world domains, spanning from environmental monitoring to long-term medical studies, observations do not arrive in a single batch but rather over time in episodes. This challenges the traditional assumption in causal discovery of a single, observational dataset, not only because each episode may be a biased sample of the population but also because multiple episodes could differ in the causal interactions underlying the observed variables. We address these issues using notions of context switches and episodic selection bias, and introduce a framework for causal modeling of episodic data. We show under which conditions we can apply information-theoretic scoring criteria for causal discovery while preserving consistency. To in practice discover the causal model progressively over time, we propose the CONTINENT algorithm which, taking inspiration from continual learning, discovers the causal model in an online fashion without having to re-learn the model upon arrival of each new episode. Our experiments over a variety of settings including selection bias, unknown interventions, and network changes showcase that CONTINENT works well in practice and outperforms the baselines by a clear margin.|在许多现实世界的领域，从环境监测到长期的医学研究，观察到的不是一批，而是随着时间的推移而发生的事件。这挑战了单个观察性数据集因果发现的传统假设，不仅因为每个事件可能是人群的偏倚样本，而且因为多个事件可能在观察变量的因果相互作用中有所不同。我们使用上下文切换和情节选择偏差的概念来解决这些问题，并介绍了一个情节数据的因果建模框架。我们展示了在哪些条件下，我们可以应用信息论评分标准的因果发现，同时保持一致性。为了在实践中逐步发现因果模型，我们提出 CONTINENT 算法，它从持续学习中获得灵感，以在线方式发现因果模型，而不必在每个新片段到达时重新学习模型。我们在包括选择偏差、未知干预和网络变化在内的各种环境下进行的实验表明，CONTINENT 在实践中运作良好，并且明显优于基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Causal+Networks+from+Episodic+Data)|0|
|[Money Never Sleeps: Maximizing Liquidity Mining Yields in Decentralized Finance](https://doi.org/10.1145/3637528.3671942)|Wangze Ni, Yiwei Zhao, Weijie Sun, Lei Chen, Peng Cheng, Chen Jason Zhang, Xuemin Lin|Hong Kong University of Science and Technology, Hong Kong, China; Hong Kong Polytechnic University, Hong Kong, China; Shanghai Jiao Tong University, Shanghai, China; East China Normal University, Shanghai, China|The popularity of decentralized finance has drawn attention to liquidity mining (LM). In LM, a user deposits her cryptocurrencies into liquidity pools to provide liquidity for exchanges and earn yields. Different liquidity pools offer varying yields and require different pairs of cryptocurrencies. A user can exchange a cryptocurrency for another with some exchange costs. Thus, an LM solution consists of exchange transactions and deposit transactions, guaranteeing (1) each exchange transaction must exchange one cryptocurrency for another at a specific rate (i.e., the exchange constraint); (2) the amounts of cryptocurrencies deposited in a liquidity pool must exceed the required threshold (i.e., the minimum constraint); (3) each deposit transaction must deposit a specific pair of cryptocurrencies at a certain rate in a liquidity pool (i.e., the deposit constraint); and (4) the cryptocurrencies used in the solution do not exceed the cryptocurrencies that the user has (i.e., the budget constraint). Selecting the most profitable LM solution is challenging due to the vast number of candidate solutions. To address this challenge, we define the yield maximization liquidity mining (YMLM) problem. Given a set of liquidity pools, a set of the user's cryptocurrencies, a set of exchange rates, and an evaluation function, YMLM aims to find an LM solution with maximal yields, satisfying the minimum, exchange, deposit, and budget constraints. We prove that YMLM is NP-hard and cannot be solved by algorithms with constant approximation ratios. To tackle YMLM, we propose two algorithms, namely YMLM\_GD and YMLM\_SK, with parameterized approximation ratios. Extensive experiments on both real and synthetic datasets show that our approaches outperform the baselines in yields.|分散式金融的普及引起了人们对流动性挖掘(LM)的关注。在 LM 中，用户将自己的加密货币存入流动性池，为交易所提供流动性，并获得收益率。不同的流动性池提供不同的收益率，需要不同的加密货币对。用户可以用一种加密货币交换另一种加密货币，但需要付出一定的交换成本。因此，LM 解决方案由交易所交易和存款交易组成，保证(1)每笔交易必须以特定的汇率(即交易约束)将一种加密货币兑换成另一种加密货币; (2)存入流动性池的加密货币数量必须超过规定的阈值(即最低限度约束) ; (3)每笔存款交易必须以特定的汇率将一对加密货币存入流动性池(即存款约束) ; 以及(4)解决方案中使用的加密货币不超过用户拥有的加密货币(即预算线)。选择最有利可图的 LM 解决方案是具有挑战性的，因为有大量的候选解决方案。为了解决这个问题，我们定义了收益最大化流动性挖掘(YMLM)问题。给定一组流动性池、一组用户的加密货币、一组汇率和一个评估函数，YMLM 的目标是找到一个收益最大的 LM 解决方案，满足最小值、汇率、存款和预算约束。证明了 YMLM 是 NP 难的，不能用常数逼近比的算法求解。为了解决 YMLM 问题，我们提出了两种具有参数化逼近比的算法: YMLM _ GD 和 YMLM _ SK。对真实和合成数据集的大量实验表明，我们的方法在产量方面优于基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Money+Never+Sleeps:+Maximizing+Liquidity+Mining+Yields+in+Decentralized+Finance)|0|
|[Mining of Switching Sparse Networks for Missing Value Imputation in Multivariate Time Series](https://doi.org/10.1145/3637528.3671760)|Kohei Obata, Koki Kawabata, Yasuko Matsubara, Yasushi Sakurai|SANKEN, Osaka University, Suita, Osaka, Japan|Multivariate time series data suffer from the problem of missing values, which hinders the application of many analytical methods. To achieve the accurate imputation of these missing values, exploiting inter-correlation by employing the relationships between sequences (i.e., a network) is as important as the use of temporal dependency, since a sequence normally correlates with other sequences. Moreover, exploiting an adequate network depending on time is also necessary since the network varies over time. However, in real-world scenarios, we normally know neither the network structure nor when the network changes beforehand. Here, we propose a missing value imputation method for multivariate time series, namely MissNet, that is designed to exploit temporal dependency with a state-space model and inter-correlation by switching sparse networks. The network encodes conditional independence between features, which helps us understand the important relationships for imputation visually. Our algorithm, which scales linearly with reference to the length of the data, alternatively infers networks and fills in missing values using the networks while discovering the switching of the networks. Extensive experiments demonstrate that MissNet outperforms the state-of-the-art algorithms for multivariate time series imputation and provides interpretable results.|多变量时间序列数据存在缺值问题，阻碍了许多分析方法的应用。为了实现这些缺失值的精确估算，利用序列之间的关系(即，网络)的相互关系与使用时间依赖性一样重要，因为序列通常与其他序列相关。此外，由于网络随着时间的变化而变化，因此根据时间利用一个足够的网络也是必要的。然而，在实际场景中，我们通常既不知道网络结构，也不知道网络什么时候事先发生了变化。在这里，我们提出了一种多变量时间序列的缺失值填补方法，即 MissNet，该方法利用状态空间模型的时间相关性和通过切换稀疏网络的互相关性。网络对特征之间的条件独立进行编码，这有助于我们在视觉上理解插补的重要关系。我们的算法可以根据数据的长度进行线性扩展，也可以推断网络并使用网络填充缺失值，同时发现网络的切换。大量实验表明，MissNet 在多变量时间序列插补方面优于最先进的算法，并提供了可解释的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mining+of+Switching+Sparse+Networks+for+Missing+Value+Imputation+in+Multivariate+Time+Series)|0|
|[Ontology Enrichment for Effective Fine-grained Entity Typing](https://doi.org/10.1145/3637528.3671857)|Siru Ouyang, Jiaxin Huang, Pranav Pillai, Yunyi Zhang, Yu Zhang, Jiawei Han|University of Illinois Urbana-Champaign, Urbana, IL, USA; University of Illinois Urbana-Champaign, Urbana, USA; Washington University in Saint Louis, St. Louis, MO, USA|Fine-grained entity typing (FET) is the task of identifying specific entity types at a fine-grained level for entity mentions based on their contextual information. Conventional methods for FET require extensive human annotation, which is time-consuming and costly given the massive scale of data. Recent studies have been developing weakly supervised or zero-shot approaches. We study the setting of zero-shot FET where only an ontology is provided. However, most existing ontology structures lack rich supporting information and even contain ambiguous relations, making them ineffective in guiding FET. Recently developed language models, though promising in various few-shot and zero-shot NLP tasks, may face challenges in zero-shot FET due to their lack of interaction with task-specific ontology. In this study, we propose øurs, where we (1) enrich each node in the ontology structure with two categories of extra information:instance information for training sample augmentation andtopic information to relate types with contexts, and (2) develop a coarse-to-fine typing algorithm that exploits the enriched information by training an entailment model with contrasting topics and instance-based augmented training samples. Our experiments show that øurs achieves high-quality fine-grained entity typing without human annotation, outperforming existing zero-shot methods by a large margin and rivaling supervised methods. øurs also enjoys strong transferability to unseen and finer-grained types. We will open source this work upon acceptance.|细粒度实体类型化(Fine-grainedtity type，FET)是在细粒度级别识别特定实体类型的任务，以便根据实体的上下文信息提及它们。传统的场效应管方法需要大量的人工注释，由于数据规模庞大，这种方法既费时又费钱。最近的研究一直在发展弱监督或零射击方法。本文研究了只提供本体的零激发场效应管的设置问题。然而，现有的大多数本体结构缺乏丰富的支持信息，甚至存在模糊关系，导致本体结构对 FET 的指导无效。最近开发的语言模型，虽然有希望在各种少镜头和零镜头自然语言处理任务，可能面临的挑战，在零镜头场效应管由于缺乏与任务特定的本体交互。在本研究中，我们提出了 øurs，其中我们(1)用两类额外信息来丰富本体结构中的每个节点: 用于训练样本扩充的实例信息和用于将类型与上下文关联的主题信息，以及(2)开发一个粗到精的类型算法，通过训练一个带有对比主题和基于实例的扩充训练样本的蕴涵模型来利用丰富的信息。我们的实验表明，øurs 在无需人工注释的情况下实现了高质量的细粒度实体分类，在很大程度上优于现有的零镜头方法，并且可以与监督方法相媲美。Øurs 还具有很强的可转移性，可以转移到看不见的和更细粒度的类型。我们将在接受后开源这项工作。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ontology+Enrichment+for+Effective+Fine-grained+Entity+Typing)|0|
|[BTTackler: A Diagnosis-based Framework for Efficient Deep Learning Hyperparameter Optimization](https://doi.org/10.1145/3637528.3671933)|Zhongyi Pei, Zhiyao Cen, Yipeng Huang, Chen Wang, Lin Liu, Philip S. Yu, Mingsheng Long, Jianmin Wang|School of Software, BNRist, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China; School of Software, EIRI, Tsinghua University, Beijing, China|Hyperparameter optimization (HPO) is known to be costly in deep learning, especially when leveraging automated approaches. Most of the existing automated HPO methods are accuracy-based, i.e., accuracy metrics are used to guide the trials of different hyperparameter configurations amongst a specific search space. However, many trials may encounter severe training problems, such as vanishing gradients and insufficient convergence, which can hardly be reflected by accuracy metrics in the early stages of the training and often result in poor performance. This leads to an inefficient optimization trajectory because the bad trials occupy considerable computation resources and reduce the probability of finding excellent hyperparameter configurations within a time limitation. In this paper, we propose Bad Trial Tackler (BTTackler), a novel HPO framework that introduces training diagnosis to identify training problems automatically and hence tackles bad trials. BTTackler diagnoses each trial by calculating a set of carefully designed quantified indicators and triggers early termination if any training problems are detected. Evaluations are performed on representative HPO tasks consisting of three classical deep neural networks (DNN) and four widely used HPO methods. To better quantify the effectiveness of an automated HPO method, we propose two new measurements based on accuracy and time consumption. Results show the advantage of BTTackler on two-fold: (1) it reduces 40.33% of time consumption to achieve the same accuracy comparable to baseline methods on average and (2) it conducts 44.5% more top-10 trials than baseline methods on average within a given time budget. We also released an open-source Python library that allows users to easily apply BTTackler to automated HPO processes with minimal code changes\footnotehttps://github.com/thuml/BTTackler.|众所周知，超参数优化(HPO)在深度学习方面代价高昂，特别是在利用自动化方法时。大多数现有的自动化 HPO 方法是基于准确度的，即，准确度指标被用来指导特定搜索空间中不同超参数配置的试验。然而，许多试验可能会遇到严重的训练问题，如梯度消失和收敛不足，这很难反映在训练的早期阶段的准确性度量，往往导致表现不佳。这导致了一个低效的优化轨迹，因为糟糕的试验占用了大量的计算资源，并降低了在一定时间内找到优秀的超参数配置的概率。本文提出了一种新的 HPO 框架 BTTackler，它引入训练诊断，自动识别训练问题，从而处理不良试验。BTTackler 通过计算一组精心设计的量化指标来诊断每个试验，如果发现任何训练问题，就会提前终止试验。对三种经典的深层神经网络(DNN)和四种广泛使用的 HPO 方法组成的代表性 HPO 任务进行了评估。为了更好地量化自动 HPO 方法的有效性，我们提出了两种基于精度和时间消耗的新测量方法。结果表明，BTTackler 的优势有两个方面: (1)在给定的时间预算内，与基线方法平均相比，它减少了40.33% 的时间消耗，以达到相同的准确度; (2)它比基线方法平均多进行了44.5% 的前10项试验。我们还发布了一个开源的 Python 库，允许用户轻松地将 BTtackler 应用于自动化的 HPO 流程，并且代码更改脚注 https:// github.com/thuml/BTTackler。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BTTackler:+A+Diagnosis-based+Framework+for+Efficient+Deep+Learning+Hyperparameter+Optimization)|0|
|[Fast Multidimensional Partial Fourier Transform with Automatic Hyperparameter Selection](https://doi.org/10.1145/3637528.3671667)|Yongchan Park, Jongjin Kim, U Kang|Seoul National University, Seoul, Republic of Korea|Given a multidimensional array, how can we optimize the computation process for a part of Fourier coefficients? Discrete Fourier transform plays an overarching role in various data mining tasks. Recent interest has focused on efficiently calculating a small part of Fourier coefficients, exploiting the energy compaction property of real-world data. Current methods for partial Fourier transform frequently encounter efficiency issues, yet the adoption of pre-computation techniques within the PFT algorithm has shown promising performance. However, PFT still faces limitations in handling multidimensional data efficiently and requires manual hyperparameter tuning, leading to additional costs. In this paper, we propose Auto-MPFT (Automatic Multidimensional Partial Fourier Transform), which efficiently computes a subset of Fourier coefficients in multidimensional data without the need for manual hyperparameter search. Auto-MPFT leverages multivariate polynomial approximation for trigonometric functions, generalizing its domain to multidimensional Euclidean space. Moreover, we present a convex optimization-based algorithm for automatically selecting the optimal hyperparameter of Auto-MPFT. We provide a rigorous proof for the explicit reformulation of the original optimization problem of Auto-MPFT, demonstrating the process that converts it into a well-established unconstrained convex optimization problem. Extensive experiments show that Auto-MPFT surpasses existing partial Fourier transform methods and optimized FFT libraries, achieving up to 7.6x increase in speed without sacrificing accuracy. In addition, our optimization algorithm accurately finds the optimal hyperparameter for Auto-MPFT, significantly reducing the cost associated with hyperparameter search.|给定一个多维数组，我们如何优化一部分傅里叶系数的计算过程？离散傅里叶变换在各种数据挖掘任务中起着至关重要的作用。最近的兴趣集中在有效地计算一小部分傅里叶系数，利用真实世界数据的能量压缩性质。目前用于部分傅里叶变换的方法经常遇到效率问题，然而在 PFT 算法中采用预计算技术已经显示出良好的性能。然而，PFT 仍然面临着有效处理多维数据的局限性，需要手动调整超参数，从而导致额外的成本。在本文中，我们提出了自动多维部分傅里叶变换(auto-MPFT) ，它可以有效地计算多维数据中的傅里叶系数子集，而不需要手动进行超参数搜索。Auto-MPFT 利用多元多项式逼近三角函数，将其域推广到多维欧氏空间。此外，本文还提出了一种基于凸优化的自动选取 Auto-MPFT 最优超参数的算法。我们为 Auto-MPFT 的原始最佳化问题的明确重构提供了严格的证明，展示了将其转化为一个完善的无约束凸最佳化问题的过程。大量的实验表明，Auto-MPFT 超越了现有的部分傅里叶变换方法和优化的 FFT 库，在不牺牲精度的情况下，速度提高了7.6倍。此外，我们的优化算法准确地找到了自动 MPFT 的最优超参数，大大降低了与超参数搜索相关的成本。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+Multidimensional+Partial+Fourier+Transform+with+Automatic+Hyperparameter+Selection)|0|
|[CoMAL: Contrastive Active Learning for Multi-Label Text Classification](https://doi.org/10.1145/3637528.3671754)|Cheng Peng, Haobo Wang, Ke Chen, Lidan Shou, Chang Yao, Runze Wu, Gang Chen|State Key Lab of Blockchain and Data Security, Zhejiang University, Hangzhou, China; Fuxi AI Lab, NetEase Corp., Hangzhou, China; School of Software Technology, Zhejiang University, Ningbo, China|Multi-label text classification (MLTC) allows a given text to be associated with multiple labels, which well suits many real-world data mining scenarios. However, the annotation effort of MLTC is inevitably expensive and time-consuming. Although multi-label active learning provides a cost-effective solution, it still faces two major challenges: (i) constructing decent feature space to distinguish the confusing semantics of different labels; (ii) defining proper sampling criteria to measure a sample's joint effect over the entire label space. To bridge these gaps, we propose a Contrastive Multi-label Active Learning framework (CoMAL) that gives an effective data acquisition strategy. Specifically, a contrastive decoupling mechanism is introduced to fully release the semantic information of multiple labels into the latent space. Then, we devise a hybrid criterion that balances two data value measures: (i) similarity-enhanced label cardinality inconsistency reflects the uncertainty of data predictions. (ii) positive feature diversity evaluates the positive-propensity semantic diversity to handle the label sparsity. Extensive experiments demonstrate that our CoMAL outperforms the current state-of-the-art multi-label active learning approaches. Code for CoMAL is available at https://github.com/chengzju/CoMAL.|多标签文本分类(MLTC)允许将给定的文本与多个标签关联，这非常适合许多现实世界的数据挖掘场景。然而，MLTC 的注释工作不可避免地是昂贵的和耗时的。虽然多标签主动学习提供了一个成本效益的解决方案，它仍然面临两个主要的挑战: (i)构建体面的特征空间，以区分不同标签的混乱语义; (ii)定义适当的抽样标准，以衡量样本的联合效应，在整个标签空间。为了弥补这些差距，我们提出了一个对比多标签主动学习框架(CoMAL) ，提供了一个有效的数据采集策略。具体来说，我们引入了一个对比解耦机制，将多个标签的语义信息完全释放到潜在空间中。然后，我们设计了一个平衡两个数据值度量的混合准则: (i)相似性增强的标签基数不一致性反映了数据预测的不确定性。(ii)正特征多样性评价正倾向语义多样性来处理标签稀疏性。大量的实验表明，我们的 CoMAL 优于目前最先进的多标签主动学习方法。可于 https://github.com/chengzju/CoMAL 索取「协议编码」代码。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CoMAL:+Contrastive+Active+Learning+for+Multi-Label+Text+Classification)|0|
|[TSC: A Simple Two-Sided Constraint against Over-Smoothing](https://doi.org/10.1145/3637528.3671954)|Furong Peng, Kang Liu, Xuan Lu, Yuhua Qian, HongRen Yan, Chao Ma|; College of Physics and Electronic Engineering, Shanxi University, Taiyuan, Shanxi, China; HOPERUN Infomation Technology, Nanjing, Jiangsu, China|Graph Convolutional Neural Network (GCN), a widely adopted method for analyzing relational data, enhances node discriminability through the aggregation of neighboring information. Usually, stacking multiple layers can improve the performance of GCN by leveraging information from high-order neighbors. However, the increase of the network depth will induce the over-smoothing problem, which can be attributed to the quality and quantity of neighbors changing: (a) neighbor quality, node's neighbors become overlapping in high order, leading to aggregated information becoming indistinguishable, (b) neighbor quantity, the exponentially growing aggregated neighbors submerges the node's initial feature by recursively aggregating operations. Current solutions mainly focus on one of the above causes and seldom consider both at once. Aiming at tackling both causes of over-smoothing in one shot, we introduce a simple Two-Sided Constraint (TSC) for GCNs, comprising two straightforward yet potent techniques: random masking and contrastive constraint. The random masking acts on the representation matrix's columns to regulate the degree of information aggregation from neighbors, thus preventing the convergence of node representations. Meanwhile, the contrastive constraint, applied to the representation matrix's rows, enhances the discriminability of the nodes. Designed as a plug-in module, TSC can be easily coupled with GCN or SGC architectures. Experimental analyses on diverse real-world graph datasets verify that our approach markedly reduces the convergence of node's representation and the performance degradation in deeper GCN.|图形卷积神经网络(GCN)是一种广泛采用的分析关系数据的方法，它通过聚合相邻的信息来提高节点的识别能力。通常，多层叠加可以通过利用高阶邻居的信息来提高 GCN 的性能。然而，随着网络深度的增加，过平滑问题会出现，这可以归因于邻居的质量和数量的变化: (a)邻居质量，节点的邻居高度重叠，导致聚合信息难以区分; (b)邻居数量，指数增长的聚合邻居通过递归聚合操作淹没节点的初始特征。目前的解决方案主要集中在上述原因之一，很少同时考虑两者。为了一次性解决导致过度平滑的两个原因，我们引入了一个简单的 GCNs 双边约束(TSC) ，包括两个简单而有效的技术: 随机掩蔽和对比约束。随机掩蔽作用在表示矩阵的列上，调节邻居信息的聚集程度，从而防止节点表示的收敛。同时，将对比约束应用于表示矩阵的行，增强了节点的可识别性。作为一个插件模块设计，TSC 可以很容易地与 GCN 或 SGC 体系结构耦合。通过对不同实际图形数据集的实验分析，证明该方法显著降低了深层 GCN 中节点表示的收敛性和性能下降。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TSC:+A+Simple+Two-Sided+Constraint+against+Over-Smoothing)|0|
|[CASH via Optimal Diversity for Ensemble Learning](https://doi.org/10.1145/3637528.3671894)|Pranav Poduval, Sanjay Kumar Patnala, Gaurav Oberoi, Nitish Srivasatava, Siddhartha Asthana|MasterCard AI Garage, Gurgaon, India; Mastercard AI Garage, Gurgaon, India|The Combined Algorithm Selection and Hyperparameter Optimization (CASH) problem is pivotal in Automatic Machine Learning (AutoML). Most leading approaches combine Bayesian optimization with post-hoc ensemble building to create advanced AutoML systems. Bayesian optimization (BO) typically focuses on identifying a singular algorithm and its hyperparameters that outperform all other configurations. Recent developments have highlighted an oversight in prior CASH methods: the lack of consideration for diversity among the base learners of the ensemble. This oversight was overcome by explicitly injecting the search for diversity into the traditional CASH problem. However, despite recent developments, BO's limitation lies in its inability to directly optimize ensemble generalization error, offering no theoretical assurance that increased diversity correlates with enhanced ensemble performance. Our research addresses this gap by establishing a theoretical foundation that integrates diversity into the core of BO for direct ensemble learning. We explore a theoretically sound framework that describes the relationship between pair-wise diversity and ensemble performance, which allows our Bayesian optimization framework Optimal Diversity Bayesian Optimization (OptDivBO) to directly and efficiently minimize ensemble generalization error. OptDivBO guarantees an optimal balance between pairwise diversity and individual model performance, setting a new precedent in ensemble learning within CASH. Empirical results on 20 public datasets show that OptDivBO achieves the best average test ranks of 1.57 and 1.4 in classification and regression tasks.|组合算法选择和超参数优化(CASH)问题是自动机器学习(AutoML)中的关键问题。大多数领先的方法结合贝叶斯优化和事后集成建设，以创建先进的自动建模系统。贝叶斯优化(BO)通常侧重于识别一个奇异算法及其超参数，其性能优于所有其他配置。最近的事态发展突出表明，以前的 CASH 方法存在一个疏忽: 缺乏对集合基础学习者多样性的考虑。克服这一疏忽的办法是，明确地将寻求多样性的工作纳入传统的现金结算问题。然而，尽管有最近的发展，BO 的局限性在于它不能直接优化集合泛化误差，不能提供增加的多样性与增强的集合性能相关的理论保证。我们的研究通过建立一个理论基础，将多样性融入直接集成学习的 BO 核心，从而解决了这一差距。我们探索了一个理论上合理的框架来描述成对多样性和集合性能之间的关系，这使得我们的贝叶斯优化框架最佳多样性贝叶斯优化(optDivBO)能够直接有效地最小化集合泛化误差。OptDivBO 保证了成对多样性和个体模型性能之间的最佳平衡，在现金集成学习内开创了一个新的先例。对20个公共数据集的实证结果表明，OptDivBO 在分类和回归任务中取得了1.57和1.4的最佳平均测试排名。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CASH+via+Optimal+Diversity+for+Ensemble+Learning)|0|
|[Unifying Evolution, Explanation, and Discernment: A Generative Approach for Dynamic Graph Counterfactuals](https://doi.org/10.1145/3637528.3671831)|Bardh Prenkaj, Mario VillaizánVallelado, Tobias Leemann, Gjergji Kasneci|Technical University of Munich, Munich, Germany; University of Tübingen, Tübingen, Germany; University of Valladolid & Telefónica Research and Development, Valladolid, Spain|We present GRACIE (Graph Recalibration and Adaptive Counterfactual Inspection and Explanation), a novel approach for generative classification and counterfactual explanations of dynamically changing graph data. We study graph classification problems through the lens of generative classifiers. We propose a dynamic, self-supervised latent variable model that updates by identifying plausible counterfactuals for input graphs and recalibrating decision boundaries through contrastive optimization. Unlike prior work, we do not rely on linear separability between the learned graph representations to find plausible counterfactuals. Moreover, GRACIE eliminates the need for stochastic sampling in latent spaces and graph-matching heuristics. Our work distills the implicit link between generative classification and loss functions in the latent space, a key insight to understanding recent successes with this architecture. We further observe the inherent trade-off between validity and pulling explainee instances towards the central region of the latent space, empirically demonstrating our theoretical findings. In extensive experiments on synthetic and real-world graph data, we attain considerable improvements, reaching ~99% validity when sampling sets of counterfactuals even in the challenging setting of dynamic data landscapes.|本文提出了一种动态变化的图形数据生成分类和反事实解释的新方法 GRACIE (图形重校正和自适应反事实检验与解释)。我们从生成分类器的角度研究图的分类问题。我们提出了一个动态的，自我监督的潜变量模型，通过识别合理的反事实输入图和重新校准决策边界通过对比优化更新。与之前的工作不同，我们不依赖于学习图表示之间的线性可分的来找到合理的反事实。此外，GRACIE 消除了在潜空间和图匹配启发式随机抽样的需要。我们的工作提取了潜在空间中生成分类和损失函数之间的隐含联系，这是理解该体系结构最近的成功的关键洞察力。我们进一步观察了有效性与将被解释实例拉向潜在空间中心区域之间的内在权衡，并通过实证证明了我们的理论发现。在对合成和真实世界图形数据的广泛实验中，我们获得了相当大的改进，即使在具有挑战性的动态数据景观环境中，当采样反事实集时，我们也达到了约99% 的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unifying+Evolution,+Explanation,+and+Discernment:+A+Generative+Approach+for+Dynamic+Graph+Counterfactuals)|0|
|[Reimagining Graph Classification from a Prototype View with Optimal Transport: Algorithm and Theorem](https://doi.org/10.1145/3637528.3671696)|Chen Qian, Huayi Tang, Hong Liang, Yong Liu|Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; School of Electronic and Computer Engineering, Peking University, Shenzhen, China|Recently, Graph Neural Networks (GNNs) have achieved inspiring performances in graph classification tasks. However, the message passing mechanism in GNNs implicitly utilizes the topological information of the graph, which may lead to a potential loss of structural information. Furthermore, the graph classification decision process based on GNNs resembles a black box and lacks sufficient transparency. The non-linear classifier following the GNNs also defaults to the assumption that each class is represented by a single vector, thereby limiting the diversity of intra-class representations. To address these issues, we propose a novel prototype-based graph classification framework that introduces the Fused Gromov-Wasserstein (FGW) distance in Optimal Transport (OT) as the similarity measure. In this way, the model explicitly exploits the structural information on the graph through OT while leading to a more transparent and straightforward classification process. The introduction of prototypes also inherently addresses the issue of limited within-class representations. Besides, to alleviate the widely acknowledged computational complexity issue of FGW distance calculation, we devise a simple yet effective NN-based FGW distance approximator, which can enable full GPU training acceleration with a marginal performance loss. In theory, we analyze the generalization performance of the proposed method and derive an O (1 over N) generalization bound, where the proof techniques can be extended to a broader range of prototype-based classification frameworks. Experimental results show that the proposed framework achieves competitive and superior performance on several widely used graph classification benchmark datasets. The code is avaliable at https://github.com/ChnQ/PGOT.|近年来，图神经网络在图分类任务中取得了令人鼓舞的成绩。然而，GNN 中的消息传递机制隐含地利用了图的拓扑信息，这可能导致结构信息的丢失。此外，基于 GNN 的图分类决策过程类似于一个黑盒子，缺乏足够的透明度。GNN 后面的非线性分类器也默认每个类由一个向量表示，从而限制了类内表示的多样性。为了解决这些问题，我们提出了一种新的基于原型的图分类框架，该框架引入了最优运输(OT)中的融合 Gromov-Wasserstein (FGW)距离作为相似性度量。这样，该模型通过 OT 显式地利用图上的结构信息，同时导致一个更加透明和直接的分类过程。原型的引入本质上也解决了类内表示有限的问题。此外，为了解决普遍存在的 FGW 距离计算复杂性问题，我们设计了一个简单而有效的基于神经网络的 FGW 距离近似器，它可以在性能损失较小的情况下实现 GPU 的全训练加速。在理论上，我们分析了该方法的泛化性能，并得到了一个 O (1/N)泛化界，其中证明技术可以扩展到更广泛的基于原型的分类框架。实验结果表明，该框架在几个广泛使用的图分类基准数据集上取得了较好的性能。密码在 https://github.com/chnq/pgot 可用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reimagining+Graph+Classification+from+a+Prototype+View+with+Optimal+Transport:+Algorithm+and+Theorem)|0|
|[Pre-train and Refine: Towards Higher Efficiency in K-Agnostic Community Detection without Quality Degradation](https://doi.org/10.1145/3637528.3671686)|Meng Qin, Chaorui Zhang, Yu Gao, Weixi Zhang, DitYan Yeung|Theory Lab, Huawei, Beijing, China; Department of CSE, HKUST, Hong Kong, Hong Kong; Theory Lab, Huawei, Hong Kong, Hong Kong|Community detection (CD) is a classic graph inference task that partitionsnodes of a graph into densely connected groups. While many CD methods have beenproposed with either impressive quality or efficiency, balancing the twoaspects remains a challenge. This study explores the potential of deep graphlearning to achieve a better trade-off between the quality and efficiency ofK-agnostic CD, where the number of communities K is unknown. We propose PRoCD(Pre-training Refinement fOr Community Detection), a simple yet effectivemethod that reformulates K-agnostic CD as the binary node pair classification.PRoCD follows a pre-training refinement paradigm inspired by recent advancesin pre-training techniques. We first conduct the offline pre-training of PRoCDon small synthetic graphs covering various topology properties. Based on theinductive inference across graphs, we then generalize the pre-trained model(with frozen parameters) to large real graphs and use the derived CD results asthe initialization of an existing efficient CD method (e.g., InfoMap) tofurther refine the quality of CD results. In addition to benefiting from thetransfer ability regarding quality, the online generalization and refinementcan also help achieve high inference efficiency, since there is notime-consuming model optimization. Experiments on public datasets with variousscales demonstrate that PRoCD can ensure higher efficiency in K-agnostic CDwithout significant quality degradation.|社区检测(CD)是一个典型的图推理任务，它将图的节点划分为密集连通的群。虽然许多 CD 方法已经提出或令人印象深刻的质量或效率，平衡这两个方面仍然是一个挑战。这项研究探讨了深度图形学习的潜力，以实现更好的权衡之间的质量和效率的 K 不可知 CD，其中的社区 K 的数量是未知的。我们提出了 PRoCD (用于社区检测的预训练细化) ，一种简单而有效的方法，将 K 不可知 CD 重新制定为二进制节点对分类。 PRoCD 遵循受最近预训练技术进展启发的预训练细化范例。我们首先对覆盖各种拓扑性质的小合成图进行离线预训练。基于图之间的归纳推理，然后将预训练模型(具有冻结参数)推广到大实数图，并使用导出的 CD 结果作为现有有效 CD 方法(例如 InfoMap)的初始化，以进一步细化 CD 结果的质量。除了受益于关于质量的传递能力，在线泛化和细化也可以帮助实现高推理效率，因为没有耗时的模型优化。在不同尺度的公共数据集上进行的实验表明，PRoCD 能够在不显著降低质量的情况下保证 K 无关 CD 的高效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pre-train+and+Refine:+Towards+Higher+Efficiency+in+K-Agnostic+Community+Detection+without+Quality+Degradation)|0|
|[RHiOTS: A Framework for Evaluating Hierarchical Time Series Forecasting Algorithms](https://doi.org/10.1145/3637528.3672062)|Luis Roque, Carlos Soares, Luís Torgo|LIACCFaculty of Engineering, University of Porto, Porto, Portugal; LIACCFaculty of Engineering, University of Porto & Fraunhofer AICOS Portugal, Porto, Portugal; Dalhousie University, Halifax, Canada|We introduce the Robustness of Hierarchically Organized Time Series (RHiOTS) framework, designed to assess the robustness of hierarchical time series forecasting models and algorithms on real-world datasets. Hierarchical time series, where lower-level forecasts must sum to upper-level ones, are prevalent in various contexts, such as retail sales across countries. Current empirical evaluations of forecasting methods are often limited to a small set of benchmark datasets, offering a narrow view of algorithm behavior. RHiOTS addresses this gap by systematically altering existing datasets and modifying the characteristics of individual series and their interrelations. It uses a set of parameterizable transformations to simulate those changes in the data distribution. Additionally, RHiOTS incorporates an innovative visualization component, turning complex, multidimensional robustness evaluation results into intuitive, easily interpretable visuals. This approach allows an in-depth analysis of algorithm and model behavior under diverse conditions. We illustrate the use of RHiOTS by analyzing the predictive performance of several algorithms. Our findings show that traditional statistical methods are more robust than state-of-the-art deep learning algorithms, except when the transformation effect is highly disruptive. Furthermore, we found no significant differences in the robustness of the algorithms when applying specific reconciliation methods, such as MinT. RHiOTS provides researchers with a comprehensive tool for understanding the nuanced behavior of forecasting algorithms, offering a more reliable basis for selecting the most appropriate method for a given problem.|本文介绍了层次组织时间序列(RHiOTS)框架，该框架旨在评估层次时间序列预测模型和算法对实际数据集的鲁棒性。分层时间序列，其中较低水平的预测必须总和较高水平的，普遍存在于各种情况下，如各国的零售销售。目前对预测方法的经验性评估通常局限于一小组基准数据集，从而对算法行为提供了一个狭窄的视角。RHiOTS 通过系统地改变现有数据集和修改单个序列的特征及其相互关系来弥补这一差距。它使用一组可参数化的转换来模拟数据分布中的这些更改。此外，RHiOTS 采用了创新的可视化组件，将复杂的、多维的鲁棒性评估结果转化为直观的、易于解释的视觉效果。这种方法允许在不同的条件下深入分析算法和模型行为。通过分析几种算法的预测性能，说明了 RHiOTS 算法的应用。我们的研究结果表明，传统的统计方法比最先进的深度学习算法更稳健，除非当转换效应是高度破坏性。此外，我们发现在应用特定的协调方法时，算法的鲁棒性没有显著差异，如 MinT。RHiOTS 为研究人员提供了一个全面的工具来理解预测算法的微妙行为，为选择最合适的方法来解决给定的问题提供了更可靠的基础。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RHiOTS:+A+Framework+for+Evaluating+Hierarchical+Time+Series+Forecasting+Algorithms)|0|
|[A Fast Exact Algorithm to Enumerate Maximal Pseudo-cliques in Large Sparse Graphs](https://doi.org/10.1145/3637528.3672066)|Ahsanur Rahman, Kalyan Roy, Ramiza Maliha, Townim Faisal Chowdhury|North South University, Dhaka, Bangladesh; Australian Institute for Machine Learning, University of Adelaide, Adelaide, Australia|Pseudo-cliques (subgraphs with almost all possible edges) have many applications. But they do not satisfy the convertible antimonotone constraint (as we prove here). So, it is hard to reduce the search space of pseudo-cliques and list them efficiently. To our knowledge, only two exact algorithms, namely, ODES and PCE, were proposed for this purpose, but both have high execution times. Here, we present an exact algorithm named Fast Pseudo-Clique Enumerator (FPCE). It employs some pruning techniques we derived to reduce the search space. Our experiment on 15 real and 16 synthetic graphs shows that (i) on real graphs, FPCE is, on average, 38.6 and 6.5 times faster than ODES and PCE, respectively, whereas (ii) on synthetic graphs, FPCE is, on average, 39.7 and 3.1 times faster than ODES and PCE, respectively. We apply FPCE and a popular heuristic method on a PPI network to identify pseudo-cliques. FPCE outputs match with more known protein complexes, are more accurate, and are biologically more significant - suggesting that the exact computation of pseudo-cliques may give better insights. For its speed, FPCE is a suitable choice in such cases.|伪团(几乎具有所有可能边的子图)有许多应用。但是它们不满足可转换反单调约束(正如我们在这里证明的)。因此，很难减少伪链的搜索空间并有效地列出它们。据我们所知，只有两个精确的算法，即 ODES 和 PCE，提出了这一目的，但都有较高的执行时间。在这里，我们提出了一个精确的算法称为快速伪团枚举器(FPCE)。它使用了一些剪枝技术来减少搜索空间。对15个实图和16个合成图的实验结果表明: (1)在实图上，FPCE 的平均速度分别是 ODES 和 PCE 的38.6倍和6.5倍，而在合成图上，FPCE 的平均速度分别是 ODES 和 PCE 的39.7倍和3.1倍。我们应用 FPCE 和一种流行的启发式方法在 PPI 网络上识别伪团体。FPCE 输出与更多已知的蛋白质复合物相匹配，更准确，并且在生物学上更重要——这表明准确计算假团簇可能提供更好的见解。在这种情况下，FPCE 算法的速度是一个合适的选择。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Fast+Exact+Algorithm+to+Enumerate+Maximal+Pseudo-cliques+in+Large+Sparse+Graphs)|0|
|[CoSLight: Co-optimizing Collaborator Selection and Decision-making to Enhance Traffic Signal Control](https://doi.org/10.1145/3637528.3671998)|Jingqing Ruan, Ziyue Li, Hua Wei, Haoyuan Jiang, Jiaming Lu, Xuantang Xiong, Hangyu Mao, Rui Zhao|University of Cologne, EWI gGmbH, Cologne, Germany; Qing Yuan Research Institute, Shanghai Jiao Tong University, Shanghai, China; Arizona State University, Arizona, USA; Baidu Inc., Shenzhen, China; Peking University, Beijing, China; Fudan University, Shanghai, China; Institute of Automation, Chinese Academy of Science, Chinese Academy of Sciences, Beijing, China|Effective multi-intersection collaboration is pivotal for reinforcement-learning-based traffic signal control to alleviate congestion. Existing work mainly chooses neighboring intersections as collaborators. However, quite a lot of congestion, even some wide-range congestion, is caused by non-neighbors failing to collaborate. To address these issues, we propose to separate the collaborator selection as a second policy to be learned, concurrently being updated with the original signal-controlling policy. Specifically, the selection policy in real-time adaptively selects the best teammates according to phase- and intersection-level features. Empirical results on both synthetic and real-world datasets provide robust validation for the superiority of our approach, offering significant improvements over existing state-of-the-art methods. Code is available at https://github.com/bonaldli/CoSLight.|有效的多路口协同是基于强化学习的交通信号控制减缓交通拥堵的关键。现有的工作主要是选择相邻的交叉口作为协作者。然而，相当多的拥塞，甚至一些广泛的拥塞，是由于非邻居未能合作造成的。为了解决这些问题，我们建议将合作者选择作为第二个需要学习的策略，与原始的信号控制策略同时更新。具体来说，实时选择策略根据相位和交叉级特征自适应地选择最佳队友。对合成和真实世界数据集的实证结果为我们的方法的优越性提供了强有力的验证，提供了对现有最先进的方法的显著改进。密码可于 https://github.com/bonaldli/coslight 索取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CoSLight:+Co-optimizing+Collaborator+Selection+and+Decision-making+to+Enhance+Traffic+Signal+Control)|0|
|[A Novel Feature Space Augmentation Method to Improve Classification Performance and Evaluation Reliability](https://doi.org/10.1145/3637528.3671736)|Sakhawat Hossain Saimon, Tanzira Najnin, Jianhua Ruan|Department of Computer Science, The University of Texas at San Antonio, San Antonio, TX, USA|Classification tasks in many real-world domains are exacerbated by class imbalance, relatively small sample sizes compared to high dimensionality, and measurement uncertainty. The problem of class imbalance has been extensively studied, and data augmentation methods based on interpolation of minority class instances have been proposed as a viable solution to mitigate imbalance. It remains to be seen whether augmentation can be applied to improve the overall performance while maintaining stability, especially with a limited number of samples. In this paper, we present a novel feature-space augmentation technique that can be applied to high-dimensional data for classification tasks and address these issues. Our method utilizes uniform random sampling and introduces synthetic instances by taking advantage of the local distributions of individual features in the observed instances. The core augmentation algorithm is class-invariant, which opens up an unexplored avenue of simultaneously improving and stabilizing performance by augmenting unlabeled instances. The proposed method is evaluated using a comprehensive performance analysis involving multiple classifiers and metrics. Comparative analysis with existing feature space augmentation methods strongly suggests that the proposed algorithm can result in improved classification performance while also increasing the overall reliability of the performance evaluation.|在现实世界的许多领域中，分类任务由于类别不平衡、与高维数相比较小的样本量以及测量不确定性而加剧。对类不平衡问题进行了广泛的研究，提出了基于少数类实例插值的数据增强方法，作为一种可行的解决方案。在保持稳定性的同时，特别是在样品数量有限的情况下，是否可以应用增强技术来改善整体性能，仍有待观察。本文提出了一种新的特征空间增强技术，可以应用于高维数据的分类任务，并解决了这些问题。该方法采用均匀随机抽样，利用观测实例中个体特征的局部分布，引入综合实例。核增强算法具有类不变性，为通过增强未标记实例来同时提高和稳定性能开辟了一条尚未探索的途径。提出的方法是评估使用综合性能分析涉及多个分类器和指标。通过与现有特征空间增强方法的对比分析，表明该算法在提高分类性能的同时，也提高了性能评估的整体可靠性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Novel+Feature+Space+Augmentation+Method+to+Improve+Classification+Performance+and+Evaluation+Reliability)|0|
|[DPHGNN: A Dual Perspective Hypergraph Neural Networks](https://doi.org/10.1145/3637528.3672047)|Siddhant Saxena, Shounak Ghatak, Raghu Kolla, Debashis Mukherjee, Tanmoy Chakraborty|Meesho, Bangalore, India; IIT Delhi, New Delhi, India|Message passing on hypergraphs has been a standard framework for learning higher-order correlations between hypernodes. Recently-proposed hypergraph neural networks (HGNNs) can be categorized into spatial and spectral methods based on their design choices. In this work, we analyze the impact of change in hypergraph topology on the suboptimal performance of HGNNs and propose DPHGNN, a novel dual-perspective HGNN that introduces equivariant operator learning to capture lower-order semantics by inducing topology-aware spatial and spectral inductive biases. DPHGNN employs a unified framework to dynamically fuse lower-order explicit feature representations from the underlying graph into the super-imposed hypergraph structure. We benchmark DPHGNN over eight benchmark hypergraph datasets for the semi-supervised hypernode classification task and obtain superior performance compared to seven state-of-the-art baselines. We also provide a theoretical framework and a synthetic hypergraph isomorphism test to express the power of spatial HGNNs and quantify the expressivity of DPHGNN beyond the Generalized Weisfeiler Leman (1-GWL) test. Finally, DPHGNN was deployed by our partner e-commerce company, Meesho for the Return-to-Origin (RTO) prediction task, which shows ~7% higher macro F1-Score than the best baseline.|超图上的消息传递已经成为学习超节点之间高阶相关性的标准框架。最近提出的超图神经网络(HGNN)可以根据其设计选择分为空间方法和谱方法。本文分析了超图拓扑结构的变化对 HGNN 次优性能的影响，提出了一种新的双视角 HGNN，它引入等变算子学习，通过引入拓扑感知的空间和谱归纳偏差来捕获低阶语义。DPHGNN 采用统一的框架，动态地将底层图的低阶显式特征表示融合到叠加超图结构中。我们基准 DPHGNN 超过八个基准超图数据集的半监督超节点分类任务，并获得优于七个国家的最先进的基线性能。我们还提供了一个理论框架和一个综合的超图同构检验来表达空间 HGNN 的能力和量化 DPHGNN 的表达超越广义 Weisfeiler Leman (1-GWL)检验。最后，DPHGNN 由我们的合作伙伴电子商务公司 Meesho 部署，用于返回原产地(RTO)预测任务，其显示比最佳基线高约7% 的宏观 F1-得分。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DPHGNN:+A+Dual+Perspective+Hypergraph+Neural+Networks)|0|
|[Self-Supervised Learning of Time Series Representation via Diffusion Process and Imputation-Interpolation-Forecasting Mask](https://doi.org/10.1145/3637528.3671673)|Zineb Senane, Lele Cao, Valentin Leonhard Buchner, Yusuke Tashiro, Lei You, Pawel Andrzej Herman, Mats Nordahl, Ruibo Tu, Vilhelm von Ehrenheim|Motherbrain, EQT Group & KTH Royal Institute of Technology, Stockholm, Sweden; Motherbrain, EQT Group & QA.tech, Stockholm, Sweden; KTH Royal Institute of Technology, Stockholm, Sweden; Technical University of Denmark, Ballerup, Denmark; Motherbrain, EQT Group, Stockholm, Sweden; Mitsubishi UFJ Trust Investment Technology Institute, Tokyo, Japan|Time Series Representation Learning (TSRL) focuses on generating informativerepresentations for various Time Series (TS) modeling tasks. TraditionalSelf-Supervised Learning (SSL) methods in TSRL fall into four main categories:reconstructive, adversarial, contrastive, and predictive, each with a commonchallenge of sensitivity to noise and intricate data nuances. Recently,diffusion-based methods have shown advanced generative capabilities. However,they primarily target specific application scenarios like imputation andforecasting, leaving a gap in leveraging diffusion models for generic TSRL. Ourwork, Time Series Diffusion Embedding (TSDE), bridges this gap as the firstdiffusion-based SSL TSRL approach. TSDE segments TS data into observed andmasked parts using an Imputation-Interpolation-Forecasting (IIF) mask. Itapplies a trainable embedding function, featuring dual-orthogonal Transformerencoders with a crossover mechanism, to the observed part. We train a reversediffusion process conditioned on the embeddings, designed to predict noiseadded to the masked part. Extensive experiments demonstrate TSDE's superiorityin imputation, interpolation, forecasting, anomaly detection, classification,and clustering. We also conduct an ablation study, present embeddingvisualizations, and compare inference speed, further substantiating TSDE'sefficiency and validity in learning representations of TS data.|时间序列表示学习(TSRL)专注于为各种时间序列(TS)建模任务生成信息表示。TSRL 中的传统自我监督学习(SSL)方法分为四大类: 重建，对抗性，对比性和预测性，每一类都具有对噪音和复杂数据细微差别的敏感性的共同挑战。最近，基于扩散的方法已经显示出先进的生成能力。然而，它们主要针对特定的应用场景，比如插补和预测，在利用通用 TSRL 的扩散模型方面留下了空白。我们的工作，时间序列扩散嵌入(TSDE) ，桥梁这一差距的第一扩散为基础的 SSL TSRL 方法。TSDE 使用插值-插值-预测(IIF)掩模将 TS 数据分割为观测部分和掩模部分。它应用了一个可训练的嵌入功能，具有双正交变换编码器与交叉机制，以观察的部分。我们训练了一个以嵌入为条件的反向扩散过程，用来预测加入到掩蔽部分的噪声。大量的实验证明了 TSDE 在插补、插值、预测、异常检测、分类和聚类方面的优势。我们还进行了消融研究，提出了嵌入可视化，并比较了推理速度，进一步证实了 TSDE 在 TS 数据学习表示中的效率和有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Supervised+Learning+of+Time+Series+Representation+via+Diffusion+Process+and+Imputation-Interpolation-Forecasting+Mask)|0|
|[Self-Explainable Temporal Graph Networks based on Graph Information Bottleneck](https://doi.org/10.1145/3637528.3671962)|Sangwoo Seo, Sungwon Kim, Jihyeong Jung, Yoonho Lee, Chanyoung Park|KAIST, Daejeon, Republic of Korea|Temporal Graph Neural Networks (TGNN) have the ability to capture both the graph topology and dynamic dependencies of interactions within a graph over time. There has been a growing need to explain the predictions of TGNN models due to the difficulty in identifying how past events influence their predictions. Since the explanation model for a static graph cannot be readily applied to temporal graphs due to its inability to capture temporal dependencies, recent studies proposed explanation models for temporal graphs. However, existing explanation models for temporal graphs rely on post-hoc explanations, requiring separate models for prediction and explanation, which is limited in two aspects: efficiency and accuracy of explanation. In this work, we propose a novel built-in explanation framework for temporal graphs, called Self-Explainable Temporal Graph Networks based on Graph Information Bottleneck (TGIB). TGIB provides explanations for event occurrences by introducing stochasticity in each temporal event based on the Information Bottleneck theory. Experimental results demonstrate the superiority of TGIB in terms of both the link prediction performance and explainability compared to state-of-the-art methods. This is the first work that simultaneously performs prediction and explanation for temporal graphs in an end-to-end manner. The source code of TGIB is available at https://github.com/sang-woo-seo/TGIB.|时态图神经网络(TGNN)具有捕获图的拓扑结构和图内交互的动态依赖关系的能力。由于难以确定过去事件如何影响 TGNN 模型的预测，越来越需要解释它们的预测。由于静态图的解释模型不能捕捉时态依赖，因此不能很容易地应用于时态图，最近的研究提出了时态图的解释模型。然而，现有的时间图解释模型依赖于事后解释，预测和解释需要独立的模型，这限制了解释的效率和准确性。在这项工作中，我们提出了一个新的内置的解释框架，时态图，称为自解释时态图网络的图信息瓶颈(TGIB)。TGIB 基于信息瓶颈理论，通过引入时间事件的随机性来解释事件的发生。实验结果表明，与现有的链路预测方法相比，TGIB 在链路预测性能和可解释性方面具有优越性。这是第一个以端到端的方式同时对时间图进行预测和解释的工作。资讯科技总监办公室的源代码可于 https://github.com/sang-woo-seo/TGIB 索取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Explainable+Temporal+Graph+Networks+based+on+Graph+Information+Bottleneck)|0|
|[Offline Imitation Learning with Model-based Reverse Augmentation](https://doi.org/10.1145/3637528.3672059)|JieJing Shao, HaoSen Shi, LanZhe Guo, YuFeng Li|; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China|In offline Imitation Learning (IL), one of the main challenges is the covariate shift between the expert observations and the actual distribution encountered by the agent, because it is difficult to determine what action an agent should take when outside the state distribution of the expert demonstrations. Recently, the model-free solutions introduced supplementary data and identified the latent expert-similar samples to augment the reliable samples during learning. Model-based solutions build forward dynamic models with conservatism quantification and then generate additional trajectories in the neighborhood of expert demonstrations. However, without reward supervision, these methods are often over-conservative in the out-of-expert-support regions, because only in states close to expert-observed states can there be a preferred action enabling policy optimization. To encourage more exploration on expert-unobserved states, we propose a novel model-based framework, called offline Imitation Learning with Self-paced Reverse Augmentation (SRA). Specifically, we build a reverse dynamic model from the offline demonstrations, which can efficiently generate trajectories leading to the expert-observed states in a self-paced style. Then, we use the subsequent reinforcement learning method to learn from the augmented trajectories and transit from expert-unobserved states to expert-observed states. This framework not only explores the expert-unobserved states but also guides maximizing long-term returns on these states, ultimately enabling generalization beyond the expert data. Empirical results show that our proposal could effectively mitigate the covariate shift and achieve the state-of-the-art performance on the offline imitation learning benchmarks. Project website: https://www.lamda.nju.edu.cn/shaojj/KDD24_SRA/.|在离线模仿学习(IL)中，一个主要的挑战是专家观察和代理遇到的实际分布之间的协变量转移，因为很难确定在专家演示的状态分布之外代理应该采取什么行动。最近，无模型解决方案引入了补充数据，并确定了潜在的专家相似样本，以增加学习过程中的可靠样本。基于模型的解决方案构建了具有保守量化的前向动态模型，然后在专家演示的邻近区域生成额外的轨迹。然而，在没有奖励监督的情况下，这些方法在专家支持范围之外的地区往往过于保守，因为只有在接近专家观察状态的状态下，才能有一个优先的行动来实现政策优化。为了鼓励对专家未观测状态的进一步研究，我们提出了一种新的基于模型的框架，称为自适应逆增强离线模仿学习(SRA)。具体来说，我们从离线演示中建立了一个反向动态模型，它可以有效地以自定步调的方式生成导致专家观察状态的轨迹。然后，我们使用后续的强化学习方法来学习增强轨迹，并从专家未观察状态过渡到专家观察状态。这个框架不仅探索了专家未观察到的状态，而且还指导了这些状态的长期回报最大化，最终实现了专家数据之外的泛化。实验结果表明，本文提出的方案能够有效地缓解协变量的偏移，并且在离线模仿学习基准上取得了最好的效果。项目网页:  https://www.lamda.nju.edu.cn/shaojj/kdd24_sra/。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Offline+Imitation+Learning+with+Model-based+Reverse+Augmentation)|0|
|[NeuroCut: A Neural Approach for Robust Graph Partitioning](https://doi.org/10.1145/3637528.3671815)|Rishi Shah, Krishnanshu Jain, Sahil Manchanda, Sourav Medya, Sayan Ranu||Graph partitioning aims to divide a graph into $k$ disjoint subsets while optimizing a specific partitioning objective. The majority of formulations related to graph partitioning exhibit NP-hardness due to their combinatorial nature. As a result, conventional approximation algorithms rely on heuristic methods, sometimes with approximation guarantees and sometimes without. Unfortunately, traditional approaches are tailored for specific partitioning objectives and do not generalize well across other known partitioning objectives from the literature. To overcome this limitation, and learn heuristics from the data directly, neural approaches have emerged, demonstrating promising outcomes. In this study, we extend this line of work through a novel framework, NeuroCut. NeuroCut introduces two key innovations over prevailing methodologies. First, it is inductive to both graph topology and the partition count, which is provided at query time. Second, by leveraging a reinforcement learning based framework over node representations derived from a graph neural network, NeuroCut can accommodate any optimization objective, even those encompassing non-differentiable functions. Through empirical evaluation, we demonstrate that NeuroCut excels in identifying high-quality partitions, showcases strong generalization across a wide spectrum of partitioning objectives, and exhibits resilience to topological modifications.|图划分的目的是将图划分为 $k $不相交子集，同时优化一个特定的划分目标。大多数与图划分有关的公式由于其组合性质而表现出 NP 难度。因此，传统的近似算法依赖于启发式方法，有时有近似保证，有时没有。遗憾的是，传统的方法是为特定的分区目标量身定制的，并且不能很好地推广到文献中其他已知的分区目标。为了克服这个限制，并从数据直接学习启发式，神经方法已经出现，显示了有希望的结果。在这项研究中，我们通过一个新的框架，NeuroCut 扩展了这一工作线。NeuroCut 在流行的方法论上引入了两个关键的创新。首先，它归纳了图的拓扑结构和查询时提供的分区计数。其次，通过利用一个基于强化学习的框架来处理来自图形神经网络的节点表示，NeuroCut 可以适应任何优化目标，即使是那些包含不可微函数的目标。通过实证评估，我们证明 NeuroCut 在识别高质量分区方面表现出色，在广泛的分区目标中表现出强大的泛化能力，并对拓扑修改表现出弹性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NeuroCut:+A+Neural+Approach+for+Robust+Graph+Partitioning)|0|
|[Capturing Homogeneous Influence among Students: Hypergraph Cognitive Diagnosis for Intelligent Education Systems](https://doi.org/10.1145/3637528.3672002)|Junhao Shen, Hong Qian, Shuo Liu, Wei Zhang, Bo Jiang, Aimin Zhou|School of Computer Science and Technology, East China Normal University, Shanghai, China; |Cognitive diagnosis is a vital upstream task in intelligent education systems. It models the student-exercise interaction, aiming to infer the students' proficiency levels on each knowledge concept. This paper observes that most existing methods can hardly effectively capture the homogeneous influence due to its inherent complexity. That is to say, although students exhibit similar performance on given exercises, their proficiency levels inferred by these methods vary significantly, resulting in shortcomings in interpretability and efficacy. Given the complexity of homogeneous influence, a hypergraph could be a choice due to its flexibility and capability of modeling high-order similarity which aligns with the nature of homogeneous influence. However, before incorporating hypergraph, one at first needs to address the challenges of distorted homogeneous influence, sparsity of response logs, and over-smoothing. To this end, this paper proposes a hypergraph cognitive diagnosis model (HyperCDM) to address these challenges and effectively capture the homogeneous influence. Specifically, to avoid distortion, HyperCDM employs a divide-and-conquer strategy to learn student, exercise and knowledge representations in their own hypergraphs respectively, and interconnects them via a feature-based interaction function. To construct hypergraphs based on sparse response logs, the auto-encoder is utilized to preprocess response logs and K-means is applied to cluster students. To mitigate over-smoothing, momentum hypergraph convolution networks are designed to partially keep previous representations during the message propagation. Extensive experiments on both offline and online real-world datasets show that HyperCDM achieves state-of-the-art performance in terms of interpretability and capturing homogeneous influence effectively, and is competitive in generalization. The ablation study verifies the efficacy of each component, and the case study explicitly showcases the homogeneous influence captured by HyperCDM.|认知诊断是智能教育系统的重要上游任务。它模拟学生与练习的互动，旨在推断学生对每个知识概念的熟练程度。本文发现，现有的方法由于其固有的复杂性，很难有效地捕捉同质影响。也就是说，尽管学生在给定的练习中表现出相似的表现，但是这些方法所推断出的水平差异很大，从而导致可解释性和有效性方面的缺陷。由于同质影响的复杂性，超图具有灵活性和建模高阶相似性的能力，符合同质影响的性质，可以作为一种选择。然而，在合并超图之前，首先需要解决同质影响扭曲、响应日志稀疏和过度平滑的挑战。为此，本文提出了一个超图认知诊断模型(HyperCDM) ，以解决这些挑战，并有效地捕捉同质的影响。为了避免失真，HyperCDM 采用了分而治之的策略，分别在各自的超图中学习学生、练习和知识表示，并通过一个基于特征的交互函数将它们互相连接起来。为了构造基于稀疏响应日志的超图，利用自动编码器对响应日志进行预处理，并对聚类学生应用 K 均值。为了减轻过度平滑，动量超图卷积网络被设计为在消息传播过程中部分保持先前的表示。在离线和在线真实世界数据集上的大量实验表明，HyperCDM 在可解释性和有效捕获同质影响方面达到了最先进的性能，并且在泛化方面具有竞争力。消融研究验证了每个组成部分的功效，并且案例研究明确地展示了 HyperCDM 捕获的同质影响。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Capturing+Homogeneous+Influence+among+Students:+Hypergraph+Cognitive+Diagnosis+for+Intelligent+Education+Systems)|0|
|[Optimizing OOD Detection in Molecular Graphs: A Novel Approach with Diffusion Models](https://doi.org/10.1145/3637528.3671785)|Xu Shen, Yili Wang, Kaixiong Zhou, Shirui Pan, Xin Wang|Griffith University, Goldcoast, Australia; Jilin University, Changchun, China; Massachusetts Institute of Technology, Cambridge, MA, USA|Despite the recent progress of molecular representation learning, its effectiveness is assumed on the close-world assumptions that training and testing graphs are from identical distribution. The open-world test dataset is often mixed with out-of-distribution (OOD) samples, where the deployed models will struggle to make accurate predictions. The misleading estimations of molecules' properties in drug screening or design can result in the tremendous waste of wet-lab resources and delay the discovery of novel therapies. Traditional detection methods need to trade off OOD detection and in-distribution (ID) classification performance since they share the same representation learning model. In this work, we propose to detect OOD molecules by adopting an auxiliary diffusion model-based framework, which compares similarities between input molecules and reconstructed graphs. Due to the generative bias towards reconstructing ID training samples, the similarity scores of OOD molecules will be much lower to facilitate detection. Although it is conceptually simple, extending this vanilla framework to practical detection applications is still limited by two significant challenges. First, the popular similarity metrics based on Euclidian distance fail to consider the complex graph structure. Second, the generative model involving iterative denoising steps is notoriously time-consuming especially when it runs on the enormous pool of drugs. To address these challenges, our research pioneers an approach of Prototypical Graph Reconstruction for Molecular OOd Detection, dubbed as PGR-MOOD. Specifically, PGR-MOOD hinges on three innovations: i) An effective metric to comprehensively quantify the matching degree of input and reconstructed molecules according to their discrete edges and continuous node features; ii) A creative graph generator to construct a list of prototypical graphs that are in line with ID distribution but away from OOD one; iii) An efficient and scalable OOD detector to compare the similarity between test samples and pre-constructed prototypical graphs and omit the generative process on every new molecule. Extensive experiments on ten benchmark datasets and six baselines are conducted to demonstrate our superiority: PGR-MOOD achieves more than 8% of average improvement in terms of detection AUC and AUPR accompanied by the reduced cost of testing time and memory consumption. The anonymous code is in: https://github.com/se7esx/PGR-MOOD.|尽管近年来分子表征学习取得了很大的进展，但它的有效性是建立在训练图和测试图来自同一分布的近似世界假设之上的。开放世界的测试数据集常常与超出分布(OOD)的样本混合在一起，在这种情况下，已部署的模型将难以做出准确的预测。在药物筛选或设计中对分子性质的误导性估计会导致实验室资源的巨大浪费，并延迟新疗法的发现。传统的检测方法由于具有相同的表示学习模型，需要在 OOD 检测和分布式(ID)分类性能之间进行权衡。在这项工作中，我们提出采用辅助扩散模型为基础的框架来检测面向对象分子，它比较了输入分子和重构图之间的相似性。由于生成性偏向于重建 ID 训练样本，OOD 分子的相似性得分将大大降低，以利于检测。尽管概念上很简单，但是将这个普通的框架扩展到实际的检测应用程序仍然受到两个重大挑战的限制。首先，目前流行的基于欧氏距离的相似性度量方法没有考虑到复杂的图形结构。其次，包括迭代去噪步骤的生成模型是出了名的耗时，尤其是当它运行在庞大的药物池上时。为了应对这些挑战，我们的研究开创了一种用于分子 OOD 检测的原型图重建方法，称为 PGR-MOOD。具体而言，PGR-MOOD 取决于三个创新: i)根据分子的离散边和连续节点特征全面量化输入和重构分子的匹配程度的有效度量; ii)创造性的图形生成器构建一个与 ID 分布一致但远离 OOD 的原型图列表; iii)高效和可扩展的 OOD 检测器，以比较测试样本和预先构建的原型图之间的相似性，并忽略每个新分子上的生成过程。在10个基准数据集和6个基线上进行了广泛的实验，证明了我们的优越性: PGR-MOOD 在检测 AUC 和 AUPR 方面达到了8% 以上的平均改善，同时降低了测试时间和内存消耗的成本。匿名代码在:  https://github.com/se7esx/pgr-mood。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+OOD+Detection+in+Molecular+Graphs:+A+Novel+Approach+with+Diffusion+Models)|0|
|[Efficient and Long-Tailed Generalization for Pre-trained Vision-Language Model](https://doi.org/10.1145/3637528.3671945)|JiangXin Shi, Chi Zhang, Tong Wei, YuFeng Li|; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China|Pre-trained vision-language models like CLIP have shown powerful zero-shotinference ability via image-text matching and prove to be strong few-shotlearners in various downstream tasks. However, in real-world scenarios,adapting CLIP to downstream tasks may encounter the following challenges: 1)data may exhibit long-tailed data distributions and might not have abundantsamples for all the classes; 2) There might be emerging tasks with new classesthat contain no samples at all. To overcome them, we propose a novel frameworkto achieve efficient and long-tailed generalization, which can be termed asCandle. During the training process, we propose compensating logit-adjustedloss to encourage large margins of prototypes and alleviate imbalance bothwithin the base classes and between the base and new classes. For efficientadaptation, we treat the CLIP model as a black box and leverage the extractedfeatures to obtain visual and textual prototypes for prediction. To make fulluse of multi-modal information, we also propose cross-modal attention to enrichthe features from both modalities. For effective generalization, we introducevirtual prototypes for new classes to make up for their lack of trainingimages. Candle achieves state-of-the-art performance over extensive experimentson 11 diverse datasets while substantially reducing the training time,demonstrating the superiority of our approach. The source code is available athttps://github.com/shijxcs/Candle.|像 CLIP 这样经过预先训练的视觉语言模型已经通过图像-文本匹配显示出强大的零拍摄能力，并且在各种下游任务中被证明是强大的少拍摄学习者。然而，在现实世界的场景中，使 CLIP 适应下游任务可能会遇到以下挑战: 1)数据可能显示长尾数据分布，并且可能没有所有类的大量样本; 2)可能会出现新的任务，其中包含根本不包含样本的新类。为了克服这些问题，我们提出了一个新的框架来实现高效和长尾泛化，这可以被称为蜡烛。在训练过程中，我们提出补偿 logit 调整损失，以鼓励原型的大幅度利润和缓解不平衡的基础类和基础之间的基础和新的类。为了有效地适应，我们将 CLIP 模型视为一个黑盒子，并利用提取的特征来获得用于预测的可视化和文本原型。为了充分利用多模态信息，我们还提出了交叉模态注意来丰富两种模态的特征。为了有效的推广，我们引入了新类的虚拟原型来弥补训练图像的不足。Candle 通过对11个不同数据集的大量实验，实现了最先进的性能，同时大大减少了训练时间，证明了我们方法的优越性。源代码可以在 https:// github.com/shijxcs/candle 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+and+Long-Tailed+Generalization+for+Pre-trained+Vision-Language+Model)|0|
|[MSPipe: Efficient Temporal GNN Training via Staleness-Aware Pipeline](https://doi.org/10.1145/3637528.3671844)|Guangming Sheng, Junwei Su, Chao Huang, Chuan Wu|The University of Hong Kong, Hong Kong, China|Memory-based Temporal Graph Neural Networks (MTGNNs) are a class of temporal graph neural networks that utilize a node memory module to capture and retain long-term temporal dependencies, leading to superior performance compared to memory-less counterparts. However, the iterative reading and updating process of the memory module in MTGNNs to obtain up-to-date information needs to follow the temporal dependencies. This introduces significant overhead and limits training throughput. Existing optimizations for static GNNs are not directly applicable to MTGNNs due to differences in training paradigm, model architecture, and the absence of a memory module. Moreover, these optimizations do not effectively address the challenges posed by temporal dependencies, making them ineffective for MTGNN training. In this paper, we propose MSPipe, a general and efficient framework for memory-based TGNNs that maximizes training throughput while maintaining model accuracy. Our design specifically addresses the unique challenges associated with fetching and updating node memory states in MTGNNs by integrating staleness into the memory module. However, simply introducing a predefined staleness bound in the memory module to break temporal dependencies may lead to suboptimal performance and lack of generalizability across different models and datasets. To overcome this, we introduce an online pipeline scheduling algorithm in MSPipe that strategically breaks temporal dependencies with minimal staleness and delays memory fetching to obtain fresher memory states. This is achieved without stalling the MTGNN training stage or causing resource contention. Additionally, we design a staleness mitigation mechanism to enhance training convergence and model accuracy. Furthermore, we provide convergence analysis and demonstrate that MSPipe maintains the same convergence rate as vanilla sampling-based GNN training. Experimental results show that MSPipe achieves up to 2.45× speed-up without sacrificing accuracy, making it a promising solution for efficient MTGNN training. The implementation of our paper can be found at the following link: https://github.com/PeterSH6/MSPipe.|基于记忆的时间图神经网络(MTGNN)是一类时间图神经网络，它利用节点记忆模块来捕获和保留长期的时间依赖关系，从而获得比无记忆对应物更好的性能。然而，MTGNN 中内存模块的迭代读取和更新过程需要遵循时间依赖关系来获取最新的信息。这引入了大量的开销并限制了培训的吞吐量。由于训练范式、模型结构和内存模块的不同，静态 GNN 的现有优化不能直接应用于 MTGNN。此外，这些优化不能有效地解决时间依赖性带来的挑战，使得它们对 MTGNN 训练无效。在本文中，我们提出了一个通用的和有效的框架，基于内存的 TGNN，最大限度地提高训练吞吐量，同时保持模型的准确性。我们的设计通过将过时性集成到内存模块中，特别解决了与获取和更新 MTGNN 中的节点内存状态相关的独特挑战。然而，简单地在内存模块中引入预定义的过时限制以打破时间依赖性可能会导致性能不够理想，并且在不同的模型和数据集之间缺乏通用性。为了克服这个问题，我们在 MSPipe 中引入了一个在线管道调度算法，该算法以最小的过时性和延迟内存获取策略性地打破了时间依赖，以获得更新的内存状态。这是在不拖延 MTGNN 培训阶段或引起资源争用的情况下实现的。此外，我们设计了一个时滞缓解机制，以提高训练收敛性和模型的准确性。此外，我们还提供了收敛性分析，并证明 MSPipe 保持了与基于香草抽样的 GNN 训练相同的收敛速度。实验结果表明，该算法在不牺牲精度的前提下，提高了2.45倍的速度，为 MTGNN 的有效训练提供了一种有前途的解决方案。有关本文件的执行情况，请浏览以下连结:  https://github.com/petersh6/mspipe。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MSPipe:+Efficient+Temporal+GNN+Training+via+Staleness-Aware+Pipeline)|0|
|[LPFormer: An Adaptive Graph Transformer for Link Prediction](https://doi.org/10.1145/3637528.3672025)|Harry Shomer, Yao Ma, Haitao Mao, Juanhui Li, Bo Wu, Jiliang Tang|Colorado School of Mines, Golden, CO, USA; Michigan State University, East Lansing, MI, USA; Rensselaer Polytechnic Institute, Troy, NY, USA|Link prediction is a common task on graph-structured data that has seen applications in a variety of domains. Classically, hand-crafted heuristics were used for this task. Heuristic measures are chosen such that they correlate well with the underlying factors related to link formation. In recent years, a new class of methods has emerged that combines the advantages of message-passing neural networks (MPNN) and heuristics methods. These methods perform predictions by using the output of an MPNN in conjunction with a "pairwise encoding" that captures the relationship between nodes in the candidate link. They have been shown to achieve strong performance on numerous datasets. However, current pairwise encodings often contain a strong inductive bias, using the same underlying factors to classify all links. This limits the ability of existing methods to learn how to properly classify a variety of different links that may form from different factors. To address this limitation, we propose a new method, LPFormer, which attempts to adaptively learn the pairwise encodings for each link. LPFormer models the link factors via an attention module that learns the pairwise encoding that exists between nodes by modeling multiple factors integral to link prediction. Extensive experiments demonstrate that LPFormer can achieve SOTA performance on numerous datasets while maintaining efficiency. The code is available at The code is available at https://github.com/HarryShomer/LPFormer.|链接预测是图结构化数据的一个常见任务，已经在各种领域得到应用。传统上，手工制作的启发式方法用于此任务。启发式措施的选择，使他们与相关的基本因素链接形成良好的相关性。近年来，出现了一类新的方法，结合了消息传递神经网络(MPNN)和启发式方法的优点。这些方法通过使用 MPNN 的输出结合捕获候选链接中节点之间关系的“成对编码”来执行预测。它们已经被证明可以在大量数据集上实现强大的性能。然而，目前的成对编码往往包含一个强烈的归纳偏差，使用相同的基本因素分类所有链接。这限制了现有方法学习如何正确分类可能由不同因素形成的各种不同链接的能力。为了解决这个问题，我们提出了一种新的方法 LPForm，它尝试自适应地学习每个链路的成对编码。LPForm 通过一个注意模块对链路因子进行建模，该注意模块通过对链路预测的多因子积分建模来学习节点之间存在的成对编码。大量的实验表明，LPForm 可以在保持效率的同时，在大量的数据集上实现 SOTA 性能。代码可在网上查阅代码可在 https://github.com/harryshomer/lpformer 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LPFormer:+An+Adaptive+Graph+Transformer+for+Link+Prediction)|0|
|[Orthogonality Matters: Invariant Time Series Representation for Out-of-distribution Classification](https://doi.org/10.1145/3637528.3671768)|Ruize Shi, Hong Huang, Kehan Yin, Wei Zhou, Hai Jin|Huazhong University of Science and Technology, Wuhan, China|Previous works for time series classification tend to assume that both the training and testing sets originate from the same distribution. This oversimplification deviates from the complexity of reality and makes it challenging to generalize methods to out-of-distribution (OOD) time series data. Currently, there are limited works focusing on time series OOD generalization, and they typically disentangle time series into domain-agnostic and domain-specific features and design tasks to intensify the distinction between the two. However, previous models purportedly yielding domain-agnostic features continue to harbor domain-specific information, thereby diminishing their adaptability to OOD data. To address this gap, we introduce a novel model called Invariant Time Series Representation (ITSR). ITSR achieves a learnable orthogonal decomposition of time series using two sets of orthogonal axes. In detail, ITSR projects time series onto these two sets of axes separately and obtains mutually orthogonal invariant features and relevant features. ITSR theoretically ensures low similarity between these two features and further incorporates various tasks to optimize them. Furthermore, we explore the benefits of preserving orthogonality between invariant and relevant features for OOD time series classification in theory. The results on four real-world datasets underscore the superiority of ITSR over state-of-the-art methods and demonstrate the critical role of maintaining orthogonality between invariant and relevant features. Our code is available at https://github.com/CGCL-codes/ITSR.|前人对时间序列分类的研究往往假设训练集和测试集都来自同一个分布。这种过度简化背离了现实的复杂性，使得将方法推广到分布外(OOD)时间序列数据具有挑战性。目前，关于时间序列面向对象设计方法综合的研究有限，通常将时间序列分解为领域不可知特征和领域特定特征以及设计任务，以强化二者之间的区别。然而，以前的模型据称产生了领域不可知的特征，继续包含领域特定的信息，从而降低了它们对 OOD 数据的适应性。为了解决这个问题，我们引入了一个新的模型，称为不变时间序列表示(ITSR)。ITSR 利用两组正交轴对时间序列进行可学的正交分解。具体来说，ITSR 将时间序列分别投影到这两组轴上，得到相互正交的不变特征和相关特征。ITSR 在理论上确保了这两个特性之间的低相似性，并进一步整合了各种任务来优化它们。此外，我们还从理论上探讨了在面向对象时间序列分类中保持不变量与相关特征之间的正交性的好处。在四个真实世界数据集上的结果强调了 ITSR 相对于最先进的方法的优越性，并证明了保持不变特征和相关特征之间正交性的关键作用。我们的代码可以在 https://github.com/cgcl-codes/itsr 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Orthogonality+Matters:+Invariant+Time+Series+Representation+for+Out-of-distribution+Classification)|0|
|[CoLiDR: Concept Learning using Aggregated Disentangled Representations](https://doi.org/10.1145/3637528.3671938)|Sanchit Sinha, Guangzhi Xiong, Aidong Zhang|University of Virginia, Charlottesville, VA, USA|Interpretability of Deep Neural Networks using concept-based models offers a promising way to explain model behavior through human-understandable concepts. A parallel line of research focuses on disentangling the data distribution into its underlying generative factors, in turn explaining the data generation process. While both directions have received extensive attention, little work has been done on explaining concepts in terms of generative factors to unify mathematically disentangled representations and human-understandable concepts as an explanation for downstream tasks. In this paper, we propose a novel method CoLiDR - which utilizes a disentangled representation learning setup for learning mutually independent generative factors and subsequently learns to aggregate the said representations into human-understandable concepts using a novel aggregation/decomposition module. Experiments are conducted on datasets with both known and unknown latent generative factors. Our method successfully aggregates disentangled generative factors into concepts while maintaining parity with state-of-the-art concept-based approaches. Quantitative and visual analysis of the learned aggregation procedure demonstrates the advantages of our work compared to commonly used concept-based models over four challenging datasets. Lastly, our work is generalizable to an arbitrary number of concepts and generative factors - making it flexible enough to be suitable for various types of data.|基于概念模型的深度神经网络的可解释性为通过人类可理解的概念来解释模型行为提供了一种有前途的方法。一条平行的研究线集中于将数据分布分解为其潜在的生成因素，进而解释数据生成过程。虽然这两个方向都得到了广泛的关注，但是很少有人用生成因素来解释概念，以统一数学上的分离表征和人类可理解的概念，作为对下游任务的解释。在本文中，我们提出了一种新的 CoLiDR 方法-它利用一个离散表示学习设置来学习相互独立的生成因子，然后学习使用一个新的聚集/分解模块将所述表示聚合成人类可理解的概念。实验在具有已知和未知潜在生成因子的数据集上进行。我们的方法成功地将分离的生成因素集成到概念中，同时保持了与最先进的基于概念的方法的一致性。对学习聚合过程的定量和可视化分析表明，与四个具有挑战性的数据集中常用的基于概念的模型相比，我们的工作具有优势。最后，我们的工作可以推广到任意数量的概念和生成因素-使其足够灵活，以适合各种类型的数据。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CoLiDR:+Concept+Learning+using+Aggregated+Disentangled+Representations)|0|
|[On Early Detection of Hallucinations in Factual Question Answering](https://doi.org/10.1145/3637528.3671796)|Ben Snyder, Marius Moisescu, Muhammad Bilal Zafar|; Amazon Web Services, Seattle, WA, USA; Amazon Web Services, Santa Clara, CA, USA|While large language models (LLMs) have taken great strides towards helping humans with a plethora of tasks, hallucinations remain a major impediment towards gaining user trust. The fluency and coherence of model generations even when hallucinating makes detection a difficult task. In this work, we explore if the artifacts associated with the model generations can provide hints that the generation will contain hallucinations. Specifically, we probe LLMs at 1) the inputs via Integrated Gradients based token attribution, 2) the outputs via the Softmax probabilities, and 3) the internal state via self-attention and fully-connected layer activations for signs of hallucinations on open-ended question answering tasks. Our results show that the distributions of these artifacts tend to differ between hallucinated and non-hallucinated generations. Building on this insight, we train binary classifiers that use these artifacts as input features to classify model generations into hallucinations and non-hallucinations. These hallucination classifiers achieve up to 0.80 AUROC. We also show that tokens preceding a hallucination can already predict the subsequent hallucination even before it occurs.|虽然大型语言模型(LLM)已经在帮助人类完成大量任务方面取得了长足的进步，但是幻觉仍然是获得用户信任的主要障碍。模型世代的流畅性和连贯性甚至在产生幻觉时也使得侦测成为一项困难的任务。在这项工作中，我们探索与模型世代相关的人工制品是否可以提供这一世代将包含幻觉的暗示。具体而言，我们探测 LLM: 1)通过基于集成梯度的令牌归属的输入，2)通过 Softmax 概率的输出，以及3)通过自我注意和完全连接层激活的内部状态，以获得开放式问题回答任务上的幻觉迹象。我们的研究结果表明，这些伪影的分布趋向于不同的幻觉和非幻觉世代。在此基础上，我们训练二进制分类器，使用这些工件作为输入特征，将模型代分为幻觉和非幻觉。这些幻觉分类器可以达到0.80 AUROC。我们还发现，幻觉之前的标记甚至在幻觉发生之前就已经能够预测后续的幻觉。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Early+Detection+of+Hallucinations+in+Factual+Question+Answering)|0|
|[MAML-en-LLM: Model Agnostic Meta-Training of LLMs for Improved In-Context Learning](https://doi.org/10.1145/3637528.3671905)|Sanchit Sinha, Yuguang Yue, Victor Soto, Mayank Kulkarni, Jianhua Lu, Aidong Zhang|University of Virginia, Charlottesville, VA, USA; Amazon AGI, Cambridge, MA, USA; Amazon AGI, New York, NY, USA|Adapting large language models (LLMs) to unseen tasks with incontext training samples without fine-tuning remains an important research problem. To learn a robust LLM that adapts well to unseen tasks, multiple meta-training approaches have been proposed such as MetaICL and MetaICT, which involve meta-training pre-trained LLMs on a wide variety of diverse tasks. These meta-training approaches essentially perform in-context multi-task fine-tuning and evaluate on a disjointed test set of tasks. Even though they achieve impressive performance, their goal is never to compute a truly general set of parameters. In this paper, we propose MAML-en-LLM, a novel method for meta-training LLMs, which can learn truly generalizable parameters that not only performs well on disjointed tasks but also adapts to unseen tasks. We see an average increase of 2% on unseen domains in the performance while a massive 4% improvement on adaptation performance. Furthermore, we demonstrate that MAML-en-LLM outperforms baselines in settings with limited amount of training data on both seen and unseen domains by an average of 2%. Finally, we discuss the effects of type of tasks, optimizers and task complexity, an avenue barely explored in metatraining literature. Exhaustive experiments across 7 task settings along with two data settings demonstrate that models trained with MAML-en-LLM outperform SOTA meta-training approaches.|使大语言模型(LLM)适应不需要微调的非上下文训练样本的未知任务仍然是一个重要的研究问题。为了学习能够很好地适应看不见的任务的强大的 LLM，已经提出了多种元培训方法，如 MetaICL 和 MetaICT，其中涉及对各种不同任务进行元培训预先培训的 LLM。这些元训练方法基本上执行上下文中的多任务微调，并对脱节的任务测试集进行评估。尽管它们取得了令人印象深刻的性能，但它们的目标从来不是计算一组真正通用的参数。本文提出了一种新的元训练 LLM 方法 MAML-en-LLM，它能够学习真正可推广的参数，这些参数不仅能够很好地处理不连续的任务，而且能够适应看不见的任务。我们看到在性能上看不见的领域平均增加了2% ，而在适应性能上则大幅提高了4% 。此外，我们证明，MAML-en-LLM 在有限数量的训练数据在可见和不可见领域中的平均表现优于基线2% 。最后，我们讨论了任务类型、优化器和任务复杂度的影响，这是元训练文献中几乎没有涉及到的一个途径。通过7个任务设置和两个数据设置的详尽实验表明，用 MAML-en-LLM 训练的模型优于 SOTA 元训练方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MAML-en-LLM:+Model+Agnostic+Meta-Training+of+LLMs+for+Improved+In-Context+Learning)|0|
|[Fast Computation for the Forest Matrix of an Evolving Graph](https://doi.org/10.1145/3637528.3671822)|Haoxin Sun, Xiaotian Zhou, Zhongzhi Zhang|Fudan University, Shanghai, China|The forest matrix plays a crucial role in network science, opinion dynamics, and machine learning, offering deep insights into the structure of and dynamics on networks. In this paper, we study the problem of querying entries of the forest matrix in evolving graphs, which more accurately represent the dynamic nature of real-world networks compared to static graphs. To address the unique challenges posed by evolving graphs, we first introduce two approximation algorithms, SFQ and SFQPlus, for static graphs. SFQ employs a probabilistic interpretation of the forest matrix, while SFQPlus incorporates a novel variance reduction technique and is theoretically proven to offer enhanced accuracy. Based on these two algorithms, we further devise two dynamic algorithms centered around efficiently maintaining a list of spanning converging forests. This approach ensures O(1) runtime complexity for updates, including edge additions and deletions, as well as for querying matrix elements, and provides an unbiased estimation of forest matrix entries. Finally, through extensive experiments on various real-world networks, we demonstrate the efficiency and effectiveness of our algorithms. Particularly, our algorithms are scalable to massive graphs with more than forty million nodes.|森林矩阵在网络科学、舆论动力学和机器学习中起着至关重要的作用，它提供了对网络结构和动力学的深刻见解。本文研究了进化图中森林矩阵条目的查询问题，与静态图相比，进化图更准确地表示了现实世界网络的动态特性。为了解决进化图所带来的独特挑战，我们首先介绍了静态图的两种近似算法 SFQ 和 SFQPlus。SFQ 采用了森林矩阵的概率解释，而 SFQPlus 采用了一种新的方差减少技术，并在理论上证明了它提供了更高的准确性。基于这两个算法，我们进一步设计了两个动态算法，围绕有效地维护跨越会聚森林的列表。这种方法确保了更新(包括边缘添加和删除)以及查询矩阵元素的 O (1)运行时复杂性，并提供了森林矩阵条目的无偏估计。最后，通过在各种实际网络上的大量实验，验证了算法的有效性。特别是，我们的算法可以扩展到具有超过四千万个节点的海量图形。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+Computation+for+the+Forest+Matrix+of+an+Evolving+Graph)|0|
|[Dual-Assessment Driven Pruning: Iterative Optimizing Layer-wise Sparsity for Large Language Model](https://doi.org/10.1145/3637528.3671780)|Qinghui Sun, Weilun Wang, Yanni Zhu, Shenghuan He, Hao Yi, Zehua Cai, Hong Liu|Alibaba Group, HangZhou, Zhejiang, China; Alibaba Group, Hangzhou, Zhejiang, China|Large Language Models (LLMs) have demonstrated efficacy in various domains, but deploying these models is economically challenging due to extensive parameter counts. Numerous efforts have been dedicated to reducing the parameter count of these models without compromising performance, employing a technique known as model pruning. Conventional pruning methods assess the significance of weights within individual layers and typically apply uniform sparsity levels across all layers, potentially neglecting the varying significance of each layer. To address this oversight, we first propose a dual-assessment driven pruning strategy that employs both intra-layer metric and global performance metric to comprehensively evaluate the impact of pruning. Then our method leverages an iterative optimization algorithm to find the optimal layer-wise sparsity distribution, thereby minimally impacting model performance. Extensive benchmark evaluations on state-of-the-art LLM architectures such as LLaMAv2 and OPT across a variety of NLP tasks demonstrate the effectiveness of our approach. When applied to the LLaMaV2-7B model with an overall pruning sparsity of 80%, our method achieves a 50% reduction in perplexity compared to the benchmark. The results indicate that our method significantly outperforms existing state-of-the-art methods in preserving performance after pruning.|大型语言模型(LLM)已经在不同的领域展示了其有效性，但是由于大量的参数计数，部署这些模型在经济上具有挑战性。许多努力致力于减少这些模型的参数计数而不损害性能，采用一种称为模型修剪的技术。传统的修剪方法评估单个层内权重的重要性，并且通常在所有层之间应用统一的稀疏水平，可能忽略了每一层的不同重要性。为了解决这个疏忽，我们首先提出了一个双重评估驱动的修剪策略，它同时使用层内度量和全球性能度量来全面评估修剪的影响。然后利用迭代优化算法寻找最优的分层稀疏分布，从而最小限度地影响模型的性能。对最先进的 LLM 架构(如 LLaMAv2和 OPT)进行跨多种 NLP 任务的广泛基准评估，证明了我们方法的有效性。当应用于 LLaMaV2-7B 模型时，整体修剪稀疏度为80% ，与基准相比，我们的方法实现了50% 的困惑减少。结果表明，我们的方法在修剪后保持性能方面明显优于现有的最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dual-Assessment+Driven+Pruning:+Iterative+Optimizing+Layer-wise+Sparsity+for+Large+Language+Model)|0|
|[DIVE: Subgraph Disagreement for Graph Out-of-Distribution Generalization](https://doi.org/10.1145/3637528.3671878)|Xin Sun, Liang Wang, Qiang Liu, Shu Wu, Zilei Wang, Liang Wang|; NLPR, MAIS, Institute of Automation, Chinese Academy of Sciences, Beijing, China; University of Science and Technology of China, Hefei, China|This paper addresses the challenge of out-of-distribution (OOD) generalization in graph machine learning, a field rapidly advancing yet grappling with the discrepancy between source and target data distributions. Traditional graph learning algorithms, based on the assumption of uniform distribution between training and test data, falter in real-world scenarios where this assumption fails, resulting in suboptimal performance. A principal factor contributing to this suboptimal performance is the inherent simplicity bias of neural networks trained through Stochastic Gradient Descent (SGD), which prefer simpler features over more complex yet equally or more predictive ones. This bias leads to a reliance on spurious correlations, adversely affecting OOD performance in various tasks such as image recognition, natural language understanding, and graph classification. Current methodologies, including subgraph-mixup and information bottleneck approaches, have achieved partial success but struggle to overcome simplicity bias, often reinforcing spurious correlations. To tackle this, our study introduces a new learning paradigm for graph OOD issue. We propose DIVE, training a collection of models to focus on all label-predictive subgraphs by encouraging the models to foster divergence on the subgraph mask, which circumvents the limitation of a model solely focusing on the subgraph corresponding to simple structural patterns. Specifically, we employs a regularizer to punish overlap in extracted subgraphs across models, thereby encouraging different models to concentrate on distinct structural patterns. Model selection for robust OOD performance is achieved through validation accuracy. Tested across four datasets from GOOD benchmark and one dataset from DrugOOD benchmark, our approach demonstrates significant improvement over existing methods, effectively addressing the simplicity bias and enhancing generalization in graph machine learning.|本文讨论了图形机器学习中的分布外(OOD)泛化问题，这是一个正在迅速发展的领域，同时也正在努力解决源和目标数据分布之间的差异问题。传统的图学习算法基于训练数据和测试数据均匀分布的假设，在这种假设不成立的现实情况下会出现问题，导致性能不理想。导致这种次优性能的一个主要因素是通过随机梯度下降训练的神经网络固有的简单性偏差(SGD) ，它们更喜欢简单的特征，而不是更复杂但同样或更具预测性的特征。这种偏差导致对虚假相关性的依赖，对图像识别、自然语言理解和图形分类等各种任务的面向对象设计(OOD)性能产生不利影响。目前的方法，包括子图混合和信息瓶颈方法，已经取得了部分成功，但努力克服简单性偏差，往往加强虚假的相关性。为了解决这个问题，我们的研究引入了一个新的图形面向对象的学习范式。我们提出 DIVE，通过鼓励模型在子图掩码上促进发散来训练一组模型以关注所有标签预测子图，这绕过了仅关注对应于简单结构模式的子图的模型的限制。具体来说，我们使用一个正则化程序来惩罚模型之间提取子图中的重叠，从而鼓励不同的模型集中于不同的结构模式。通过验证精度实现面向对象的鲁棒性模型选择。通过四个数据集和一个数据集的测试，我们的方法显示了对现有方法的显著改进，有效地解决了图形机器学习中的简单性偏差和提高了泛化能力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DIVE:+Subgraph+Disagreement+for+Graph+Out-of-Distribution+Generalization)|0|
|[Hierarchical Linear Symbolized Tree-Structured Neural Processes](https://doi.org/10.1145/3637528.3671861)|Jin yang Tai, YiKe Guo|; School of Computer Engineering and Science, Shanghai University, Shanghai, China|Traditional Neural Processes (NPs) and their variants aim to learn relationships between context sample points but do not consider multi-level information, resulting in a limited ability to learn complex distributions.This paper draws inspiration from features such as the hierarchical nature and interpretability of tree-like structures. This paper proposes a Hierarchical Linear Symbolized Tree-structured Neural Processes (HLNPs) architecture. This framework utilizes variables to build a top-down hierarchical linear symbolized tree-structured network architecture, enhancing positional representation information in a hierarchical manner along the deterministic path. In the latent distribution, the hierarchical linear symbolized tree-structured network approximates functions discretely through a layered approach. By decomposing the latent complex distribution into several simpler sub-problems using sum and product symbols, the upper bound of optimization is thereby increased. The tree structure discretizes variables to capture model uncertainty in the form of entropy. This approach also imparts a causal effect to the HLNPs model. Finally, we demonstrate the effectiveness of the HLNPs models for 1D data, Bayesian optimization, and 2D data.|传统的神经过程(NPs)及其变体旨在学习上下文样本点之间的关系，但不考虑多层次的信息，导致学习复杂分布的能力有限。本文从树状结构的层次性和可解释性等特征中得到启示。本文提出了一种分层线性符号化树结构神经过程(HLNPs)体系结构。该框架利用变量构建自顶向下的分层线性符号化树状结构网络体系结构，沿着确定性路径以分层方式增强位置表示信息。在潜在分布中，分层线性符号化树结构网络通过分层方法离散逼近函数。通过使用和和和乘积符号将潜在复分布分解为几个较简单的子问题，从而提高了优化的上界。树结构离散变量，以熵的形式捕获模型的不确定性。这种方法也赋予了 HLNPs 模型一种因果效应。最后，我们证明了 HLNPs 模型对一维数据、贝叶斯优化和二维数据的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Linear+Symbolized+Tree-Structured+Neural+Processes)|0|
|[Learning Attributed Graphlets: Predictive Graph Mining by Graphlets with Trainable Attribute](https://doi.org/10.1145/3637528.3671970)|Tajima Shinji, Ren Sugihara, Ryota Kitahara, Masayuki Karasuyama|Nagoya Institute of Technology, Nagoya, Japan|The graph classification problem has been widely studied; however, achieving an interpretable model with high predictive performance remains a challenging issue. This paper proposes an interpretable classification algorithm for attributed graph data, called LAGRA (Learning Attributed GRAphlets). LAGRA learns importance weights for small attributed subgraphs, called attributed graphlets (AGs), while simultaneously optimizing their attribute vectors. This enables us to obtain a combination of subgraph structures and their attribute vectors that strongly contribute to discriminating different classes. A significant characteristics of LAGRA is that all the subgraph structures in the training dataset can be considered as a candidate structures of AGs. This approach can explore all the potentially important subgraphs exhaustively, but obviously, a naïve implementation can require a large amount of computations. To mitigate this issue, we propose an efficient pruning strategy by combining the proximal gradient descent and a graph mining tree search. Our pruning strategy can ensure that the quality of the solution is maintained compared to the result without pruning. We empirically demonstrate that LAGRA has superior or comparable prediction performance to the standard existing algorithms including graph neural networks, while using only a small number of AGs in an interpretable manner.|图形分类问题已经得到了广泛的研究，然而，实现一个具有高预测性能的可解释模型仍然是一个具有挑战性的问题。本文提出了一种可解释的属性图数据分类算法，称为学习属性图。LAGRA 学习小的属性子图的重要性权重，称为属性图(AGs) ，同时优化他们的属性向量。这使我们能够获得子图结构及其属性向量的组合，这对于区分不同的类有很大的帮助。LAGRA 的一个显著特点是训练数据集中的所有子图结构都可以看作是 AGs 的候选结构。这种方法可以详尽地探索所有潜在的重要子图，但是显然，一个幼稚的实现可能需要大量的计算。为了缓解这个问题，我们提出了一个有效的剪枝策略，将近端梯度下降法和图挖掘树搜索相结合。我们的修剪策略可以确保解决方案的质量与不进行修剪的结果相比得以保持。我们的实验证明，LAGRA 具有优越的或可比较的预测性能的标准现有算法，包括图神经网络，而只使用少量的 AGs 在一种可解释的方式。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Attributed+Graphlets:+Predictive+Graph+Mining+by+Graphlets+with+Trainable+Attribute)|0|
|[HiGPT: Heterogeneous Graph Language Model](https://doi.org/10.1145/3637528.3671987)|Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Long Xia, Dawei Yin, Chao Huang|Baidu Inc., Beijing, China; University of Hong Kong, Hong Kong, China|Heterogeneous graph learning aims to capture complex relationships and diverse relational semantics among entities in a heterogeneous graph to obtain meaningful representations for nodes and edges. Recent advancements in heterogeneous graph neural networks (HGNNs) have achieved state-of-the-art performance by considering relation heterogeneity and using specialized message functions and aggregation rules. However, existing frameworks for heterogeneous graph learning have limitations in generalizing across diverse heterogeneous graph datasets. Most of these frameworks follow the "pre-train" and "fine-tune" paradigm on the same dataset, which restricts their capacity to adapt to new and unseen data. This raises the question: "Can we generalize heterogeneous graph models to be well-adapted to diverse downstream learning tasks with distribution shifts in both node token sets and relation type heterogeneity?" To tackle those challenges, we propose HiGPT, a general large graph model with Heterogeneous graph instruction-tuning paradigm. Our framework enables learning from arbitrary heterogeneous graphs without the need for any fine-tuning process from downstream datasets. To handle distribution shifts in heterogeneity, we introduce an in-context heterogeneous graph tokenizer that captures semantic relationships in different heterogeneous graphs, facilitating model adaptation. We incorporate a large corpus of heterogeneity-aware graph instructions into our HiGPT, enabling the model to effectively comprehend complex relation heterogeneity and distinguish between various types of graph tokens. Furthermore, we introduce the Mixture-of-Thought (MoT) instruction augmentation paradigm to mitigate data scarcity by generating diverse and informative instructions. Through comprehensive evaluations conducted in various settings, our proposed framework demonstrates exceptional performance in terms of generalization performance, surpassing current leading benchmarks. We make our model implementation openly available, along with comprehensive details at: https://github.com/HKUDS/HiGPT.|异构图学习旨在捕获异构图中实体之间的复杂关系和多样化关系语义，以获得节点和边的有意义表示。异构图神经网络(HGNN)的最新进展是通过考虑关系异构性和使用专门的消息函数和聚合规则来实现的。然而，现有的异构图学习框架在跨不同的异构图数据集进行泛化方面存在局限性。大多数框架在同一数据集上遵循“预训练”和“微调”范式，这限制了它们适应新的和未见数据的能力。这就提出了一个问题: “我们能否将异构图模型推广到适用于不同的下游学习任务，同时在节点令牌集和关系类型异质性中都有分布变化?”为了应对这些挑战，我们提出了一种基于异构图指令调优的通用大图模型 HiGPT。我们的框架支持从任意异构图学习，而不需要从下游数据集进行任何微调过程。为了处理异构中的分布变化，我们引入了一个上下文异构图标记器，它可以捕获不同异构图中的语义关系，促进模型的自适应。我们在 HiGPT 中加入了大量感知异构性的图形指令，使模型能够有效地理解复杂关系的异构性，并区分不同类型的图形标记。此外，我们引入混合思想(MoT)指令增强范式，通过生成多样化和信息丰富的指令来缓解数据稀缺性。通过在各种环境下进行的全面评估，我们提出的框架在概括性性能方面表现出非凡的业绩，超过了目前的主要基准。我们将我们的模型实现公开化，并在以下 https://github.com/hkuds/higpt 提供全面的细节。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HiGPT:+Heterogeneous+Graph+Language+Model)|0|
|[URRL-IMVC: Unified and Robust Representation Learning for Incomplete Multi-View Clustering](https://doi.org/10.1145/3637528.3671887)|Ge Teng, Ting Mao, Chen Shen, Xiang Tian, Xuesong Liu, Yaowu Chen, Jieping Ye|Alibaba Cloud, Hangzhou, China; ; Zhejiang University, Hangzhou, China|Incomplete multi-view clustering (IMVC) aims to cluster multi-view data that are only partially available. This poses two main challenges: effectively leveraging multi-view information and mitigating the impact of missing views. Prevailing solutions employ cross-view contrastive learning and missing view recovery techniques. However, they either neglect valuable complementary information by focusing only on consensus between views or provide unreliable recovered views due to the absence of supervision. To address these limitations, we propose a novel Unified and Robust Representation Learning for Incomplete Multi-View Clustering (URRL-IMVC). URRL-IMVC directly learns a unified embedding that is robust to view missing conditions by integrating information from multiple views and neighboring samples. Firstly, to overcome the limitations of cross-view contrastive learning, URRL-IMVC incorporates an attention-based auto-encoder framework to fuse multi-view information and generate unified embeddings. Secondly, URRL-IMVC directly enhances the robustness of the unified embedding against view-missing conditions through KNN imputation and data augmentation techniques, eliminating the need for explicit missing view recovery. Finally, incremental improvements are introduced to further enhance the overall performance, such as the Clustering Module and the customization of the Encoder. We extensively evaluate the proposed URRL-IMVC framework on various benchmark datasets, demonstrating its state-of-the-art performance. Furthermore, comprehensive ablation studies are performed to validate the effectiveness of our design.|不完全多视图聚类(IMVC)旨在对只有部分可用的多视图数据进行聚类。这带来了两个主要挑战: 有效地利用多视图信息和减轻缺少视图的影响。目前流行的解决方案采用横向视图对比学习和缺失视图恢复技术。但是，它们要么忽视了有价值的补充信息，只注重意见之间的共识，要么由于缺乏监督而提供不可靠的已恢复的意见。针对这些局限性，我们提出了一种新的不完全多视图聚类统一鲁棒表示学习方法(URL-IMVC)。IMVC 直接学习统一的嵌入，通过整合多个视图和相邻样本的信息，可以鲁棒地查看缺失条件。首先，为了克服跨视图对比学习的局限性，URRL-IMVC 采用基于注意力的自动编码框架融合多视图信息，生成统一的嵌入。其次，URRL-IMVC 通过 KNN 插值和数据增强技术直接增强了统一嵌入对视图缺失条件的鲁棒性，消除了显式缺失视图恢复的需要。最后，为了进一步提高系统的整体性能，引入了增量式改进，如集群模块和编码器的定制。我们在各种基准数据集上广泛评估了提议的 URRL-IMVC 框架，展示了其最先进的性能。此外，进行了全面的消融研究，以验证我们的设计的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=URRL-IMVC:+Unified+and+Robust+Representation+Learning+for+Incomplete+Multi-View+Clustering)|0|
|[Rotative Factorization Machines](https://doi.org/10.1145/3637528.3671740)|Zhen Tian, Yuhong Shi, Xiangkun Wu, Wayne Xin Zhao, JiRong Wen|; Zhejiang University, Hangzhou, China|Feature interaction learning (FIL) focuses on capturing the complex relationships among multiple features for building predictive models, which is widely used in real-world tasks. Despite the research progress, existing FIL methods suffer from two major limitations. Firstly, they mainly model the feature interactions within a bounded order (e.g., small integer order) due to the exponential growth of the interaction terms. Secondly, the interaction order of each feature is often independently learned, which lacks the flexibility to capture the feature dependencies in varying contexts. To address these issues, we present Rotative Factorization Machines (RFM), based on the key idea that represents each feature as a polar angle in the complex plane. As such, the feature interactions are converted into a series of complex rotations, where the orders are cast into the rotation coefficients, thereby allowing for the learning of arbitrarily large order. Further, we propose a novel self-attentive rotation function that models the rotation coefficients through a rotation-based attention mechanism, which can adaptively learn the interaction orders under different interaction contexts. Moreover, it incorporates a modulus amplification network to learn the modulus of the complex features, which further enhances the expressive capacity. Our proposed approach provides a general FIL framework, and many existing models can be instantiated in this framework, e.g., factorization machines. In theory, it possesses more strong capacities to model complex feature relationships, and can learn arbitrary features from varied contexts. Extensive experiments conducted on five widely used datasets have demonstrated the effectiveness of our approach.|特征交互学习(FIL)关注于捕获多个特征之间的复杂关系，用于建立预测模型，在实际任务中得到了广泛的应用。尽管研究取得了进展，但现有的 FIL 方法仍然存在两大局限性。首先，由于交互项的指数增长，他们主要在一个有界顺序(例如小整数顺序)内对特征交互进行建模。其次，每个特征的交互顺序往往是独立学习的，缺乏在不同环境下捕获特征依赖关系的灵活性。为了解决这些问题，我们提出了旋转因子分解机(RFM)的基础上的关键思想，表示每个特征作为一个极角在复杂的平面。因此，特征交互被转换成一系列复杂的旋转，其中的顺序被转换成旋转系数，从而允许学习任意大的顺序。在此基础上，提出了一种新的自注意旋转函数，该函数通过基于旋转的注意机制对旋转系数进行建模，能够自适应地学习不同交互情境下的交互顺序。此外，它还结合了模放大网络来学习复杂特征的模，进一步提高了表达能力。我们提出的方法提供了一个通用的 FIL 框架，并且许多现有的模型可以在这个框架中实例化，例如，因子分解机。理论上，它具有较强的复杂特征关系建模能力，可以从不同的上下文中学习任意特征。在五个广泛使用的数据集上进行的大量实验已经证明了我们方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rotative+Factorization+Machines)|0|
|[Latent Diffusion-based Data Augmentation for Continuous-Time Dynamic Graph Model](https://doi.org/10.1145/3637528.3671863)|Yuxing Tian, Aiwen Jiang, Qi Huang, Jian Guo, Yiyan Qi|Jiangxi Normal University, Nanchang, China; International Digital Economy Academy, IDEA Research, Shenzhen, China|Continuous-Time Dynamic Graph (CTDG) precisely models evolving real-world relationships, drawing heightened interest in dynamic graph learning across academia and industry. However, existing CTDG models encounter challenges stemming from noise and limited historical data. Graph Data Augmentation (GDA) emerges as a critical solution, yet current approaches primarily focus on static graphs and struggle to effectively address the dynamics inherent in CTDGs. Moreover, these methods often demand substantial domain expertise for parameter tuning and lack theoretical guarantees for augmentation efficacy. To address these issues, we propose Conda, a novel latent diffusion-based GDA method tailored for CTDGs. Conda features a sandwich-like architecture, incorporating a Variational Auto-Encoder (VAE) and a conditional diffusion model, aimed at generating enhanced historical neighbor embeddings for target nodes. Unlike conventional diffusion models trained on entire graphs via pre-training, Conda requires historical neighbor sequence embeddings of target nodes for training, thus facilitating more targeted augmentation. We integrate Conda into the CTDG model and adopt an alternating training strategy to optimize performance. Extensive experimentation across six widely used real-world datasets showcases the consistent performance improvement of our approach, particularly in scenarios with limited historical data.|连续时间动态图(CTDG)精确地模拟演化的现实世界的关系，吸引了学术界和工业界对动态图学习的高度兴趣。然而，现有的 CTDG 模型遇到了来自噪音和有限的历史数据的挑战。图形数据增强(Graph Data Augging，GDA)是一个关键的解决方案，然而目前的方法主要集中于静态图形，并努力有效地解决 CTDGs 中固有的动态问题。此外，这些方法往往需要大量的领域专业知识的参数调整和缺乏增强效果的理论保证。为了解决这些问题，我们提出了一种新的基于潜在扩散的 GDA 方法 Conda。Conda 采用了一种类似三明治的体系结构，结合了变分自动编码器(VAE)和条件扩散模型，旨在为目标节点生成增强的历史邻居嵌入。与传统的扩散模型通过预训练对整个图进行训练不同，Conda 需要对目标节点进行历史邻居序列嵌入来进行训练，从而便于进行更有针对性的增强。我们将 Conda 集成到 CTDG 模型中，并采用交替培训策略来优化性能。对六个广泛使用的真实世界数据集进行的广泛实验显示了我们的方法的一致性能改进，特别是在历史数据有限的场景中。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Latent+Diffusion-based+Data+Augmentation+for+Continuous-Time+Dynamic+Graph+Model)|0|
|[Flexible Graph Neural Diffusion with Latent Class Representation Learning](https://doi.org/10.1145/3637528.3671860)|Liangtian Wan, Huijin Han, Lu Sun, Zixun Zhang, Zhaolong Ning, Xiaoran Yan, Feng Xia|; Research Center for Data Hub and Security, Zhejiang Lab, Hangzhou, China; School of Computing Technologies, RMIT University, Melbourne, Australia; School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, Shenzhen, China|In existing graph data, the connection relationships often exhibit uniform weights, leading to the model aggregating neighboring nodes with equal weights across various connection types. However, this uniform aggregation of diverse information diminishes the discriminability of node representations, contributing significantly to the over-smoothing issue in models. In this paper, we propose the Flexible Graph Neural Diffusion (FGND) model, incorporating latent class representation to address the misalignment between graph topology and node features. In particular, we combine latent class representation learning with the inherent graph topology to reconstruct the diffusion matrix during the graph diffusion process. We introduce the sim metric to quantify the degree of mismatch between graph topology and node features. By flexibly adjusting the dependency level on node features through the hyperparameter, we accommodate diverse adjacency relationships. The effective filtering of noise in the topology also allows the model to capture higher order information, significantly alleviating the over-smoothing problem. Meanwhile, we model the graphical diffusion process as a set of differential equations and employ advanced partial differential equation tools to obtain more accurate solutions. Empirical evaluations on five benchmarks reveal that our FGND model outperforms existing popular GNN methods in terms of both overall performance and stability under data perturbations. Meanwhile, our model exhibits superior performance in comparison to models tailored for heterogeneous graphs and those designed to address oversmoothing issues.|在现有的图形数据中，连接关系往往具有统一的权重，导致模型聚集相邻节点，在不同的连接类型中具有相同的权重。然而，这种不同信息的统一聚合降低了节点表示的可分辨性，极大地促进了模型中的过度平滑问题。本文提出了柔性图神经扩散(FGND)模型，结合潜类表示来解决图拓扑结构与节点特征之间的不匹配问题。特别地，我们将潜类表示学习与固有的图拓扑结合起来，在图扩散过程中重构扩散矩阵。引入相似度量来量化图的拓扑结构与节点特征之间的不匹配程度。通过超参数灵活调整节点特征的依赖关系，可以适应不同的邻接关系。在拓扑结构中对噪声进行有效的滤波，使得模型能够捕获更高阶的信息，显著地减轻了过平滑问题。同时，我们将图形扩散过程建模为一组微分方程，并使用先进的偏微分方程工具来获得更精确的解。对五个基准的实证评估表明，我们的 FGND 模型在整体性能和数据扰动下的稳定性方面都优于现有的流行 GNN 方法。与此同时，我们的模型表现出优越的性能相比，为异构图和那些旨在解决过度平滑问题的模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Flexible+Graph+Neural+Diffusion+with+Latent+Class+Representation+Learning)|0|
|[STONE: A Spatio-temporal OOD Learning Framework Kills Both Spatial and Temporal Shifts](https://doi.org/10.1145/3637528.3671680)|Binwu Wang, Jiaming Ma, Pengkun Wang, Xu Wang, Yudong Zhang, Zhengyang Zhou, Yang Wang|University of Science and Technology of China, Hefei, China; Suzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, China|Traffic prediction is a crucial task in the Intelligent Transportation System (ITS), receiving significant attention from both industry and academia. Numerous spatio-temporal graph convolutional networks have emerged for traffic prediction and achieved remarkable success. However, these models have limitations in terms of generalization and scalability when dealing with Out-of-Distribution (OOD) graph data with both structural and temporal shifts. To tackle the challenges of spatio-temporal shift, we propose a framework called STONE by learning invariable node dependencies, which achieve stable performance in variable environments. STONE initially employs gated-transformers to extract spatial and temporal semantic graphs. These two kinds of graphs represent spatial and temporal dependencies, respectively. Then we design three techniques to address spatio-temporal shifts. Firstly, we introduce a Fréchet embedding method that is insensitive to structural shifts, and this embedding space can integrate loose position dependencies of nodes within the graph. Secondly, we propose a graph intervention mechanism to generate multiple variant environments by perturbing two kinds of semantic graphs without any data augmentations, and STONE can explore invariant node representation from environments. Finally, we further introduce an explore-to-extrapolate risk objective to enhance the variety of generated environments. We conduct experiments on multiple traffic datasets, and the results demonstrate that our proposed model exhibits competitive performance in terms of generalization and scalability.|交通预测是智能交通系统(ITS)中的一项重要内容，受到了业界和学术界的广泛关注。许多时空图卷积网络已经应用于交通预测，并取得了显著的成功。然而，这些模型在处理具有结构和时间偏移的 OOD 图形数据时，在泛化和可扩展性方面存在局限性。为了解决时空转移的问题，我们提出了一种基于不变节点依赖性的 STONE 框架，该框架可以在多变环境下获得稳定的性能。STONE 最初使用门控转换器提取空间和时间语义图。这两种图分别表示空间和时间上的依赖关系。然后我们设计了三种技术来解决时空移位问题。首先，我们引入了一种对结构位移不敏感的 Fréchet 嵌入方法，该嵌入空间可以集成图中节点的松散位置依赖关系。其次，提出了一种图干预机制，通过对两种语义图进行干扰，在不增加任何数据的情况下生成多变量环境，并且 STONE 可以从环境中探索不变的节点表示。最后，我们进一步引入了一个探索性外推风险目标，以增强生成环境的多样性。我们在多个交通数据集上进行了实验，结果表明，我们提出的模型在泛化性和可扩展性方面具有竞争性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=STONE:+A+Spatio-temporal+OOD+Learning+Framework+Kills+Both+Spatial+and+Temporal+Shifts)|0|
|[Provable Adaptivity of Adam under Non-uniform Smoothness](https://doi.org/10.1145/3637528.3671718)|Bohan Wang, Yushun Zhang, Huishuai Zhang, Qi Meng, Ruoyu Sun, ZhiMing Ma, TieYan Liu, ZhiQuan Luo, Wei Chen|Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; The Chinese University of Hong Kong, Shenzhen, Shenzhen, Guangdong, China; Chinese Academy of Mathematics and Systems Science, Beijing, China; University of Science and Technology of China & Microsoft Research Asia, Beijing, Haidian, China; The Chinese University of Hong Kong, Shenzhen, Shenzhen, China; Peking University, Beijing, China; Microsoft, Beijing, China|Adam is widely adopted in practical applications due to its fast convergence. However, its theoretical analysis is still far from satisfactory. Existing convergence analyses for Adam rely on the bounded smoothness assumption, referred to as the L-smooth condition. Unfortunately, this assumption does not hold for many deep learning tasks. Moreover, we believe that this assumption obscures the true benefit of Adam, as the algorithm can adapt its update magnitude according to local smoothness. This important feature of Adam becomes irrelevant when assuming globally bounded smoothness. This paper studies the convergence of randomly reshuffled Adam (RR Adam) with diminishing learning rate, which is the major version of Adam adopted in deep learning tasks. We present the first convergence analysis of RR Adam without the bounded smoothness assumption. We demonstrate that RR Adam can maintain its convergence properties when smoothness is linearly bounded by the gradient norm, referred to as the (L0, L1)-smooth condition. We further compare Adam to SGD when both methods use diminishing learning rate. We refine the existing lower bound of SGD and show that SGD can be slower than Adam. To our knowledge, this is the first time that Adam and SGD are rigorously compared in the same setting and the advantage of Adam is revealed.|亚当因其收敛速度快而在实际应用中得到了广泛的应用。然而，其理论分析还远远不能令人满意。现有的亚当收敛性分析依赖于有界光滑假设，称为 L- 光滑条件。不幸的是，这个假设并不适用于许多深度学习任务。此外，我们认为这个假设掩盖了亚当的真正好处，因为该算法可以根据局部平滑度调整其更新幅度。当假设全局有界平滑时，亚当的这个重要特征就变得无关紧要了。本文研究了随机重组亚当(RR 亚当)学习速率递减的收敛性，这是亚当在深度学习任务中采用的主要形式。我们提出了 RR 亚当的第一个收敛性分析，没有有界光滑假设。证明了当光滑度线性有界于梯度范数时(L0，L1)-光滑条件下，RR-Adam 可以保持其收敛性。当两种方法都使用递减学习率时，我们进一步将亚当比作 SGD。我们改进了现有的 SGD 下限，并表明新加坡元可以慢于亚当。据我们所知，这是第一次在同一背景下严格比较亚当和 SGD，并揭示了亚当的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Provable+Adaptivity+of+Adam+under+Non-uniform+Smoothness)|0|
|[Multi-Scale Detection of Anomalous Spatio-Temporal Trajectories in Evolving Trajectory Datasets](https://doi.org/10.1145/3637528.3671874)|Chenhao Wang, Lisi Chen, Shuo Shang, Christian S. Jensen, Panos Kalnis|University of Electronic Science and Technology of China, Chengdu, China; King Abdullah University of Science and Technology, Thuwal, Saudi Arabia; Aalborg University, Aalborg, Denmark|A trajectory is a sequence of timestamped point locations that captures the movement of an object such as a vehicle. Such trajectories encode complex spatial and temporal patterns and provide rich information about object mobility and the underlying infrastructures, typically road networks, within which the movements occur. A trajectory dataset is evolving when new trajectories are included continuously. The ability to detect anomalous trajectories in online fashion in this setting is fundamental and challenging functionality that has many applications, e.g., location-based services. State-of-the-art solutions determine anomalies based on the shapes or routes of trajectories, ignoring potential anomalies caused by different sampling rates or time offsets. We propose a multi-scale model, termed MST-OATD, for anomalous streaming trajectory detection that considers both the spatial and temporal aspects of trajectories. The model's multi-scale capabilities aim to enable extraction of trajectory features at multiple scales. In addition, to improve model evolvability and to contend with changes in trajectory patterns, the model is equipped with a learned ranking model that updates the training set as new trajectories are included. Experiments on real datasets offer evidence that the model can outperform state-of-the-art solutions and is capable of real-time anomaly detection. Further, the learned ranking model achieves promising results when updating the training set with newly arrived trajectories.|轨迹是一系列有时间标记的点位置，用于捕捉物体(如车辆)的运动。这种轨迹编码复杂的空间和时间模式，并提供有关物体移动性和基础设施的丰富信息，通常是道路网络，其中发生的运动。当不断包含新的轨迹时，轨迹数据集就在演变。在这种情况下，以在线方式检测异常轨迹的能力是基本的和具有挑战性的功能，它有许多应用程序，例如，基于位置的服务。最先进的解决方案根据轨迹的形状或路线确定异常，忽略由不同采样率或时间偏移引起的潜在异常。我们提出了一个多尺度模型，称为 MST-OATD，异常流轨迹检测，考虑到空间和时间方面的轨迹。该模型的多尺度能力旨在提取多尺度的弹道特征。此外，为了提高模型的可演化性和应对轨迹模式的变化，该模型配备了一个学习排名模型，该模型在包括新的轨迹时更新训练集。在真实数据集上的实验证明，该模型的性能优于最先进的解决方案，并具有实时异常检测。此外，学习排序模型在用新到达的轨迹更新训练集时取得了令人满意的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Scale+Detection+of+Anomalous+Spatio-Temporal+Trajectories+in+Evolving+Trajectory+Datasets)|0|
|[Global Human-guided Counterfactual Explanations for Molecular Properties via Reinforcement Learning](https://doi.org/10.1145/3637528.3672045)|Danqing Wang, Antonis Antoniades, KhaDinh Luong, Edwin Zhang, Mert Kosan, Jiachen Li, Ambuj Singh, William Yang Wang, Lei Li|Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA; Harvard University & Founding, Cambridge, MA, USA; University of California, Santa Barbara, Santa Barbara, CA, USA; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA; University of California, Santa Barbara, Santa Barbara, USA|Counterfactual explanations of Graph Neural Networks (GNNs) offer a powerful way to understand data that can naturally be represented by a graph structure. Furthermore, in many domains, it is highly desirable to derive data-driven global explanations or rules that can better explain the high-level properties of the models and data in question. However, evaluating global counterfactual explanations is hard in real-world datasets due to a lack of human-annotated ground truth, which limits their use in areas like molecular sciences. Additionally, the increasing scale of these datasets provides a challenge for random search-based methods. In this paper, we develop a novel global explanation model RLHEX for molecular property prediction. It aligns the counterfactual explanations with human-defined principles, making the explanations more interpretable and easy for experts to evaluate. RLHEX includes a VAE-based graph generator to generate global explanations and an adapter to adjust the latent representation space to human-defined principles. Optimized by Proximal Policy Optimization (PPO), the global explanations produced by RLHEX cover 4.12% more input graphs and reduce the distance between the counterfactual explanation set and the input set by 0.47% on average across three molecular datasets. RLHEX provides a flexible framework to incorporate different human-designed principles into the counterfactual explanation generation process, aligning these explanations with domain expertise. The code and data are released at https://github.com/dqwang122/RLHEX.|对图形神经网络(GNN)的反事实解释提供了一种强有力的方法来理解可以自然地用图形结构表示的数据。此外，在许多领域，非常需要推导出数据驱动的全球解释或规则，以便更好地解释有关模型和数据的高级特性。然而，在现实世界的数据集中评估全球反事实解释是困难的，因为缺乏人类注释的地面真相，这限制了它们在分子科学等领域的应用。此外，这些数据集的规模越来越大，对基于随机搜索的方法提出了挑战。本文提出了一种新的分子性质预测的全局解释模型 RLHEX。它将反事实解释与人为原则相结合，使解释更易于理解，更易于专家评价。RLHEX 包括一个基于 VAE 的图形生成器，用于生成全局解释，以及一个适配器，用于根据人定义的原则调整潜在表示空间。通过最近策略优化(PPO)优化，RLHEX 产生的全局解释覆盖了4.12% 以上的输入图，并在三个分子数据集中平均减少了反事实解释集和输入集之间的距离0.47% 。RLHEX 提供了一个灵活的框架，将不同的人类设计的原则纳入反事实解释的生成过程，使这些解释与领域专业知识相一致。代码和数据 https://github.com/dqwang122/rlhex 发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Global+Human-guided+Counterfactual+Explanations+for+Molecular+Properties+via+Reinforcement+Learning)|0|
|[Mastering Long-Tail Complexity on Graphs: Characterization, Learning, and Generalization](https://doi.org/10.1145/3637528.3671880)|Haohui Wang, Baoyu Jing, Kaize Ding, Yada Zhu, Wei Cheng, Si Zhang, Yonghui Fan, Liqing Zhang, Dawei Zhou|Northwestern University, Evanston, USA; Virginia Tech, Blacksburg, USA; IBM Research, Yorktown Heights, USA; NEC Labs America, Princeton, USA; Meta, Sunnyvale, USA; Amazon AGI, Sunnyvale, USA; University of Illinois Urbana-Champaign, Urbana, USA|In the context of long-tail classification on graphs, the vast majority of existing work primarily revolves around the development of model debiasing strategies, intending to mitigate class imbalances and enhance the overall performance. Despite the notable success, there is very limited literature that provides a theoretical tool for characterizing the behaviors of long-tail classes in graphs and gaining insight into generalization performance in real-world scenarios. To bridge this gap, we propose a generalization bound for long-tail classification on graphs by formulating the problem in the fashion of multi-task learning, i.e., each task corresponds to the prediction of one particular class. Our theoretical results show that the generalization performance of long-tail classification is dominated by the overall loss range and the task complexity. Building upon the theoretical findings, we propose a novel generic framework HierTail for long-tail classification on graphs. In particular, we start with a hierarchical task grouping module that allows us to assign related tasks into hypertasks and thus control the complexity of the task space; then, we further design a balanced contrastive learning module to adaptively balance the gradients of both head and tail classes to control the loss range across all tasks in a unified fashion. Extensive experiments demonstrate the effectiveness of HierTail in characterizing long-tail classes on real graphs, which achieves up to 12.9% improvement over the leading baseline method in balanced accuracy.|在图的长尾分类的背景下，绝大多数现有的工作主要围绕模型去偏策略的发展，旨在缓解类的不平衡和提高整体性能。尽管取得了显著的成功，但是提供描述图中长尾类行为的理论工具以及深入了解真实场景中的泛化性能的文献非常有限。为了弥补这一差距，我们提出了一个图的长尾分类的泛化界，即每个任务对应于一个特定类的预测。我们的理论结果表明，长尾分类的泛化性能主要取决于总损失范围和任务复杂度。在理论研究的基础上，我们提出了一个新的通用框架 HierTail 用于图的长尾分类。特别地，我们从一个分层的任务分组模块开始，它允许我们将相关的任务分配到超任务中，从而控制任务空间的复杂性; 然后，我们进一步设计了一个平衡的对比学习模块来自适应地平衡头部和尾部类的梯度，以统一的方式控制所有任务的丢失范围。大量的实验证明了 HierTail 方法在描述真实图上的长尾类时的有效性，在平衡精度方面比主基线方法提高了12.9% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mastering+Long-Tail+Complexity+on+Graphs:+Characterization,+Learning,+and+Generalization)|0|
|[Unsupervised Heterogeneous Graph Rewriting Attack via Node Clustering](https://doi.org/10.1145/3637528.3671716)|Haosen Wang, Can Xu, Chenglong Shi, Pengfei Zheng, Shiming Zhang, Minhao Cheng, Hongyang Chen|Pennsylvania State University, Philadelphia, PA, USA; Southeast University & Zhejiang Lab, Nanjing, China; East China Normal University, Shanghai, China; University of Science and Technology of China, Hefei, China; Southeast University, Nanjing, China; Zhejiang Lab, Hangzhou, China|Self-supervised learning (SSL) has become one of the most popular learning paradigms and has achieved remarkable success in the graph field. Recently, a series of pre-training studies on heterogeneous graphs (HGs) using SSL have been proposed considering the heterogeneity of real-world graph data. However, verification of the robustness of heterogeneous graph pre-training is still a research gap. Most existing researches focus on supervised attacks on graphs, which are limited to a specific scenario and will not work when labels are not available. In this paper, we propose a novel unsupervised heterogeneous graph rewriting attack via node clustering (HGAC) that can effectively attack HG pre-training models without using labels. Specifically, a heterogeneous edge rewriting strategy is designed to ensure the rationality and concealment of the attacks. Then, a tailored heterogeneous graph contrastive learning (HGCL) is used as a surrogate model. Moreover, we leverage node clustering results of the clean HGs as the pseudo-labels to guide the optimization of structural attacks. Extensive experiments exhibit powerful attack performances of our HGAC on various downstream tasks (i.e., node classification, node clustering, metapath prediction, and visualization) under poisoning attack and evasion attack.|自监督学习(SSL)已经成为当前最流行的学习范式之一，并在图论领域取得了显著的成功。近年来，针对实际图形数据的异构性，提出了一系列基于 SSL 的异构图预训练方法。然而，对异构图预训练鲁棒性的验证仍然是一个研究空白。现有的研究大多集中于对图形的监督攻击，这种攻击局限于特定的场景，在没有标签的情况下不起作用。本文提出了一种基于节点聚类(HGAC)的无监督异构图重写攻击方法，该方法可以在不使用标签的情况下有效地攻击 HG 预训练模型。为了保证攻击的合理性和隐蔽性，设计了一种异构边缘重写策略。然后，采用定制的异构图对比学习(HGCL)作为代理模型。此外，我们利用干净 HG 的节点聚类结果作为伪标签来指导结构攻击的优化。广泛的实验表明，我们的 HGAC 在中毒攻击和规避攻击下对各种下游任务(即节点分类、节点聚类、元路径预测和可视化)具有强大的攻击性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unsupervised+Heterogeneous+Graph+Rewriting+Attack+via+Node+Clustering)|0|
|[Effective Edge-wise Representation Learning in Edge-Attributed Bipartite Graphs](https://doi.org/10.1145/3637528.3671805)|Hewen Wang, Renchi Yang, Xiaokui Xiao|Hong Kong Baptist University, Hong Kong, China; National University of Singapore, Singapore, Singapore|Graph representation learning (GRL) is to encode graph elements into informative vector representations, which can be used in downstream tasks for analyzing graph-structured data and has seen extensive applications in various domains. However, the majority of extant studies on GRL are geared towards generating node representations, which cannot be readily employed to perform edge-based analytics tasks in edge-attributed bipartite graphs (EABGs) that pervade the real world, e.g., spam review detection in customer-product reviews and identifying fraudulent transactions in user-merchant networks. Compared to node-wise GRL, learning edge representations (ERL) on such graphs is challenging due to the need to incorporate the structure and attribute semantics from the perspective of edges while considering the separate influence of two heterogeneous node sets U and V in bipartite graphs. To our knowledge, despite its importance, limited research has been devoted to this frontier, and existing workarounds all suffer from sub-par results. Motivated by this, this paper designs EAGLE, an effective ERL method for EABGs. Building on an in-depth and rigorous theoretical analysis, we propose the factorized feature propagation (FFP) scheme for edge representations with adequate incorporation of long-range dependencies of edges/features without incurring tremendous computation overheads. We further ameliorate FFP as a dual-view FFP by taking into account the influences from nodes in U and V severally in ERL. Extensive experiments on 5 real datasets showcase the effectiveness of the proposed EAGLE models in semi-supervised edge classification tasks. In particular, EAGLE can attain a considerable gain of at most 38.11% in AP and 1.86% in AUC when compared to the best baselines.|图表示学习(GRL)是将图元编码成信息向量表示，用于分析图结构化数据的下游任务，在各个领域有着广泛的应用。然而，大多数现存的 GRL 研究都是面向生成节点表示的，这些节点表示不能很容易地被用来执行遍布现实世界的边缘属性二分图(EABG)中的基于边缘的分析任务，例如，在客户产品评论中检测垃圾邮件评论，以及在用户-商家网络中识别欺诈交易。与节点式 GRL 相比，二部图的学习边表示(ERL)具有挑战性，因为在考虑二部图中两个异质节点集 U 和 V 的分离影响时，需要从边的角度结合结构和属性语义。据我们所知，尽管它的重要性，有限的研究已经致力于这一前沿，现有的工作方法都遭受低于标准的结果。基于此，本文设计了一种有效的 EABG ERL 方法—— EAGLE。在深入和严格的理论分析的基础上，我们提出了一种边缘表示的分解特征传播(FFP)方案，该方案充分考虑了边缘/特征的长程依赖关系，并且不会产生巨大的计算开销。我们进一步改进了 FFP 作为一个双视图的 FFP，在 ERL 中分别考虑了 U 和 V 节点的影响。通过对5个实际数据集的大量实验，验证了 EAGLE 模型在半监督边缘分类任务中的有效性。特别是，与最佳基线相比，EAGLE 在 AP 和 AUC 中的最大增益分别为38.11% 和1.86% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Effective+Edge-wise+Representation+Learning+in+Edge-Attributed+Bipartite+Graphs)|0|
|[FedNLR: Federated Learning with Neuron-wise Learning Rates](https://doi.org/10.1145/3637528.3672042)|Haozhao Wang, Peirong Zheng, Xingshuo Han, Wenchao Xu, Ruixuan Li, Tianwei Zhang|; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; Nanyang Technological University, Singapore, Singapore|Federated Learning (FL) suffers from severe performance degradation due to the data heterogeneity among clients. Some existing work suggests that the fundamental reason is that data heterogeneity can cause local model drift, and therefore proposes to calibrate the direction of local updates to solve this problem. Though effective, existing methods generally take the model as a whole, which lacks a deep understanding of how the neurons within deep classification models evolve during local training to form model drift. In this paper, we bridge this gap by performing an intuitive and theoretical analysis of the activation changes of each neuron during local training. Our analysis shows that the high activation of some neurons on the samples of a certain class will be reduced during local training when these samples are not included in the client, which we call neuron drift, thus leading to the performance reduction of this class. Motivated by this, we propose a novel and simple algorithm called FedNLR, which utilizes Neuron-wise Learning Rates during the FL local training process. The principle behind this is to enhance the learning of neurons bound to local classes on local data knowledge while reducing the decay of non-local classes knowledge stored in neurons. Experimental results demonstrate that FedNLR achieves state-of-the-art performance on federated learning with popular deep neural networks.|由于客户端之间的数据异构性，联邦学习(FL)的性能严重下降。现有的一些工作表明，数据的异构性会引起局部模型漂移是造成这一问题的根本原因，因此提出校准局部更新的方向来解决这一问题。现有的方法虽然行之有效，但通常将模型作为一个整体，缺乏对深度分类模型中的神经元在局部训练形成模型漂移过程中如何演化的深入理解。在本文中，我们通过对局部训练过程中每个神经元的激活变化进行直观的理论分析来弥合这一差距。我们的分析表明，当这些样本不包括在客户端中时，在某个类别的样本上的一些神经元的高度激活将在局部训练期间减少，我们称之为神经元漂移，从而导致该类别的性能降低。基于此，我们提出了一种新的简单算法 FedNLR，该算法在 FL 局部训练过程中利用了神经元的学习速率。其背后的原理是增强神经元对局部数据知识的学习，同时减少存储在神经元中的非局部类知识的衰减。实验结果表明，FedNLR 利用流行的深层神经网络实现了最先进的联邦学习性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedNLR:+Federated+Learning+with+Neuron-wise+Learning+Rates)|0|
|[Robust Predictions with Ambiguous Time Delays: A Bootstrap Strategy](https://doi.org/10.1145/3637528.3671920)|Jiajie Wang, Zhiyuan Jerry Lin, Wen Chen|Meta, Menlo Park, USA; Changsha Research Institute of Mining and Metallurgy, Changsha, China|In contemporary data-driven environments, the generation and processing of multivariate time series data is an omnipresent challenge, often complicated by time delays between different time series. These delays, originating from a multitude of sources like varying data transmission dynamics, sensor interferences, and environmental changes, introduce significant complexities. Traditional Time Delay Estimation methods, which typically assume a fixed constant time delay, may not fully capture these variabilities, compromising the precision of predictive models in diverse settings. To address this issue, we introduce the Time Series Model Bootstrap (TSMB), a versatile framework designed to handle potentially varying or even nondeterministic time delays in time series modeling. Contrary to traditional approaches that hinge on the assumption of a single, consistent time delay, TSMB adopts a non-parametric stance, acknowledging and incorporating time delay uncertainties. TSMB significantly bolsters the performance of models that are trained and make predictions using this framework, making it highly suitable for a wide range of dynamic and interconnected data environments. Our comprehensive evaluations, conducted on real-world datasets with different types of time delays, confirm the adaptability and effectiveness of TSMB in multiple contexts. These include, but are not limited to, power and occupancy forecasting in intelligent infrastructures, air quality monitoring, and intricate processes like mineral processing. Further diagnostic analyses strengthen the case for the TSMB estimator's robustness, underlining its significance in scenarios where ambiguity in time delays can have a significant impact on the predictive task.|在当代数据驱动的环境中，多变量时间序列数据的生成和处理是一个无处不在的挑战，往往由于不同时间序列之间的时间延迟而变得复杂。这些延迟来源于各种各样的源头，比如不同的数据传输动力学、传感器干扰和环境变化，它们引入了显著的复杂性。传统的时间延迟估计方法通常假定时间延迟是固定不变的，可能无法完全捕捉到这些变量，从而影响了预测模型在不同环境下的精度。为了解决这个问题，我们引入了时间序列模型引导程序(Time Series Model Bootstrap，TSMB) ，这是一个多功能的框架，旨在处理时间序列建模中可能变化甚至不确定的时间延迟。相对于传统的假设一致的单一时延的方法，TSMB 采取了非参数的立场，承认和合并时延的不确定性。TSMB 显著提高了使用该框架训练和预测的模型的性能，使其非常适合于各种动态和互连的数据环境。我们对不同时延类型的实际数据集进行了全面的评估，证实了 TSMB 在多种情况下的适应性和有效性。这些技术包括但不限于智能基础设施的电力和占用率预测、空气质量监测以及选矿工程等复杂过程。进一步的诊断分析加强了 TSMB 估计器的稳健性，强调了其在时间延迟模糊性可能对预测任务产生显著影响的情况下的重要性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Predictions+with+Ambiguous+Time+Delays:+A+Bootstrap+Strategy)|0|
|[A Novel Prompt Tuning for Graph Transformers: Tailoring Prompts to Graph Topologies](https://doi.org/10.1145/3637528.3671804)|Jingchao Wang, Zhengnan Deng, Tongxu Lin, Wenyuan Li, Shaobin Ling|South China Normal University, Guangzhou, --- Select One ---, China; Beijing University of Posts and Telecommunications, Beijing, China; Guangdong University of Technology, Guangzhou, China; Guangdong University of Technology, Guangzhou, --- Select One ---, China|Deep graph prompt tuning (DeepGPT), which only tunes a set of continuous prompts for graph transformers, significantly decreases the storage usage during training. However, DeepGPT is limited by its uniform prompts to input graphs with various structures. This is because different graph structures dictate various feature interactions between nodes, while the uniform prompts are not dynamic to tailor the feature transformation for the graph topology. In this paper, we propose a Topo-specific Graph Prompt Tuning (TGPT ), which provides topo-specific prompts tailored to the topological structures of input graphs. Specifically, TGPT learns trainable embeddings for graphlets and frequencies, where graphlets are fundamental sub-graphs that describe the structure around specific nodes. Based on the statistic data about graphlets of input graph, topo-specific prompts are generated by graphlet embeddings and frequency embeddings. The topo-specific prompts include node-level topo-specific prompts for specified nodes, a graph-level topo-specific prompt for the entire graph, and a task-specific prompt to learn task-related information. They are all inserted into specific graph nodes to perform feature transformation, providing specified feature transformation for input graphs with different topological structures. Extensive experiments show that our method outperforms existing lightweight fine-tuning methods and DeepGPT in molecular graph classification and regression with comparable parameters.|深度图形提示调优(DeepGPT)只对图形转换器的一组连续提示进行调优，大大减少了培训期间的存储使用。然而，DeepGPT 受到其统一提示输入具有各种结构的图形的限制。这是因为不同的图结构决定了节点之间不同的特征交互，而统一的提示并不能动态地调整图拓扑的特征转换。本文提出了一种特定于拓扑结构的图提示优化算法(TGPT) ，该算法根据输入图的拓扑结构提供特定于拓扑结构的提示。具体来说，TGPT 学习了针对图形和频率的可训练嵌入，其中图形是描述特定节点周围结构的基本子图。基于输入图形的统计数据，通过图形嵌入和频率嵌入生成特定于拓扑的提示。特定于拓扑的提示包括指定节点的节点级特定于拓扑的提示、整个图的图级特定于拓扑的提示和学习任务相关信息的特定于任务的提示。它们都被插入到特定的图节点中进行特征变换，为具有不同拓扑结构的输入图提供特定的特征变换。大量实验表明，该方法在参数可比的分子图分类和回归方面优于现有的轻量级微调方法和 DeepGPT。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Novel+Prompt+Tuning+for+Graph+Transformers:+Tailoring+Prompts+to+Graph+Topologies)|0|
|[DyPS: Dynamic Parameter Sharing in Multi-Agent Reinforcement Learning for Spatio-Temporal Resource Allocation](https://doi.org/10.1145/3637528.3672052)|Jingwei Wang, Qianyue Hao, Wenzhen Huang, Xiaochen Fan, Zhentao Tang, Bin Wang, Jianye Hao, Yong Li|Tianjin University & Huawei Noah's Ark Lab, Beijing, China; Department of EE, BNRist, Tsinghua University, Beijing, China; Huawei Noah's Ark Lab, Beijing, China|In large-scale metropolis, it is critical to efficiently allocate various resources such as electricity, medical care, and transportation to meet the living demands of citizens, according to the spatio-temporal distributions of resources and demands. Previous researchers have done plentiful work on such problems by leveraging Multi-Agent Reinforcement Learning (MARL) methods, where multiple agents cooperatively regulate and allocate the resources to meet the demands. However, facing the great number of agents in large cities, existing MARL methods lack efficient parameter sharing strategies among agents to reduce computational complexity. There remain two primary challenges in efficient parameter sharing: (1) during the RL training process, the behavior of agents changes significantly, limiting the performance of group parameter sharing based on fixed role division decided before training; (2) the behavior of agents forms complicated action trajectories, where their role characteristics are implicit, adding difficulty to dynamically adjusting agent role divisions during the training process. In this paper, we propose Dynamic Parameter Sharing (DyPS) to solve the above challenges. We design self-supervised learning tasks to extract the implicit behavioral characteristics from the action trajectories of agents. Based on the obtained behavioral characteristics, we propose a hierarchical MARL framework capable of dynamically revising the agent role divisions during the training process and thus shares parameters among agents with the same role, reducing computational complexity. In addition, our framework can be combined with various typical MARL algorithms, including IPPO, MAPPO, etc. We conduct 7 experiments in 4 representative resource allocation scenarios, where extensive results demonstrate our method's superior performance, outperforming the state-of-the-art baseline methods by up to 31%. Our source codes are available at https://github.com/tsinghua-fib-lab/DyPS.|在大城市中，根据资源和需求的时空分布，有效地配置电力、医疗、交通等各种资源，以满足市民的生活需求至关重要。先前的研究人员已经通过利用多代理强化学习(Multi-Agent)方法在这类问题上做了大量的工作，其中多个代理合作调节和分配资源以满足需求。然而，面对大城市中大量的智能体，现有的 MARL 方法缺乏有效的智能体间参数共享策略来降低计算复杂度。有效参数共享仍然面临两个主要挑战: (1)在 RL 训练过程中，Agent 的行为发生了显著的变化，限制了基于训练前确定的固定角色划分的群体参数共享性能; (2) Agent 的行为形成了复杂的行为轨迹，其角色特征是隐含的，增加了训练过程中动态调整 Agent 角色划分的难度。本文提出动态参数共享技术(DyPS)来解决上述问题。我们设计自我监督学习任务，从代理的行为轨迹中提取隐含的行为特征。基于所获得的行为特征，提出了一种分层 MARL 框架，该框架能够在训练过程中动态修正代理角色划分，从而在同一角色的代理之间共享参数，降低了计算复杂度。此外，我们的框架可以结合各种典型的 MARL 算法，包括 IPPO，MAPPO 等。我们在4个具有代表性的资源分配场景中进行了7个实验，广泛的结果证明了我们的方法的优越性能，比最先进的基线方法的性能高达31% 。我们的源代码可以在 https://github.com/tsinghua-fib-lab/dyps 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DyPS:+Dynamic+Parameter+Sharing+in+Multi-Agent+Reinforcement+Learning+for+Spatio-Temporal+Resource+Allocation)|0|
|[The Snowflake Hypothesis: Training and Powering GNN with One Node One Receptive Field](https://doi.org/10.1145/3637528.3671766)|Kun Wang, Guohao Li, Shilong Wang, Guibin Zhang, Kai Wang, Yang You, Junfeng Fang, Xiaojiang Peng, Yuxuan Liang, Yang Wang|Shenzhen Technology University, shenzhen, China; Oxford University, London, United Kingdom; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; University of Science and Technology of China (USTC), Hefei, China; International Digital Economy Academy, Shanghai, China; University of Science and Technology of China, Hefei, China; National University of Singapore, Singapore, Singapore|Despite Graph Neural Networks (GNNs) demonstrating considerable promise in graph representation learning tasks, GNNs predominantly face significant issues with overfitting and over-smoothing as they go deeper as models of computer vision (CV) realm. The success of artificial intelligence in computer vision and natural language processing largely stems from its ability to train deep models effectively. We have thus conducted a systematic study on deep GNN models. Our findings indicate that the current success of deep GNNs primarily stems from (I) the adoption of innovations from CNNs, such as residual/skip connections, or (II) the tailor-made aggregation algorithms like DropEdge. However, these algorithms often lack intrinsic interpretability and indiscriminately treat all nodes within a given layer in a similar manner, thereby failing to capture the nuanced differences among various nodes. In this paper, we introduce the Snowflake Hypothesis -- a novel paradigm underpinning the concept of "one node, one receptive field''. The hypothesis draws inspiration from the unique and individualistic patterns of each snowflake, proposing a corresponding uniqueness in the receptive fields of nodes in the GNNs. We employ the simplest gradient and node-level cosine distance as guiding principles to regulate the aggregation depth for each node, and conduct comprehensive experiments including: (1) different training scheme; (2) various shallow and deep GNN backbones; (3) various numbers of layers (8, 16, 32, 64) on multiple benchmarks; (4) compare with different aggregation strategies. The observational results demonstrate that our framework can serve as a universal operator for a range of tasks, and it displays tremendous potential on deep GNNs. Code is available at: https://github.com/CunWang520/Snowhypothe.|尽管图形神经网络(GNNs)在图形表示学习任务中表现出相当大的潜力，但是 GNNs 在计算机视觉(CV)领域中越来越深入，主要面临着过度拟合和过度平滑的重大问题。人工智能在计算机视觉和自然语言处理方面的成功很大程度上源于其有效地训练深度模型的能力。因此，我们对深层 GNN 模型进行了系统的研究。我们的研究结果表明，深度 GNN 目前的成功主要源于(I)采用来自 CNN 的创新，如残留/跳过连接，或(II)定制的聚合算法，如 DropEdge。然而，这些算法往往缺乏内在的可解释性，并且不加区分地以类似的方式处理给定层中的所有节点，从而无法捕捉各个节点之间的细微差异。本文介绍了“雪花假说”——一个支撑“一个节点，一个接受场”概念的新范式。该假设从每片雪花的独特和个性化模式中获得灵感，提出了 GNN 中节点接收域的相应独特性。我们采用最简单的梯度和节点级余弦距离作为指导原则来调节每个节点的聚集深度，并进行了全面的实验，包括: (1)不同的训练方案; (2)各种浅层和深层 GNN 骨干网; (3)多个基准上的不同层数(8,16,32,64) ; (4)比较不同的聚集策略。实验结果表明，我们的框架可以作为一个通用的操作员为一系列的任务，它显示了巨大的潜力深 GNN。密码可于以下 https://github.com/cunwang520/snowhypothe 索取:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Snowflake+Hypothesis:+Training+and+Powering+GNN+with+One+Node+One+Receptive+Field)|0|
|[The Heterophilic Snowflake Hypothesis: Training and Empowering GNNs for Heterophilic Graphs](https://doi.org/10.1145/3637528.3671791)|Kun Wang, Guibin Zhang, Xinnan Zhang, Junfeng Fang, Xun Wu, Guohao Li, Shirui Pan, Wei Huang, Yuxuan Liang|Griffith University, Queensland, Australia; RIKEN AIP, Tokyo, Japan; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Tsinghua University, Beijing, China; University of Science and Technology of China (USTC), Hefei, China; University of Minnesota, Twin Cities, MN, USA; Oxford University, Oxford, United Kingdom; Tongji University, Shanghai, China|Graph Neural Networks (GNNs) have become pivotal tools for a range of graph-based learning tasks. Notably, most current GNN architectures operate under the assumption of homophily, whether explicitly or implicitly. While this underlying assumption is frequently adopted, it is not universally applicable, which can result in potential shortcomings in learning effectiveness. In this paper, or the first time, we transfer the prevailing concept of "one node one receptive field" to the heterophilic graph. By constructing a proxy label predictor, we enable each node to possess a latent prediction distribution, which assists connected nodes in determining whether they should aggregate their associated neighbors. Ultimately, every node can have its own unique aggregation hop and pattern, much like each snowflake is unique and possesses its own characteristics. Based on observations, we innovatively introduce the Heterophily Snowflake Hypothesis and provide an effective solution to guide and facilitate research on heterophilic graphs and beyond. We conduct comprehensive experiments including (1) main results on 10 graphs with varying heterophily ratios across 10 backbones; (2) scalability on various deep GNN backbones (SGC, JKNet, etc.) across various large number of layers (2,4,6,8,16,32 layers); (3) comparison with conventional snowflake hypothesis; (4) efficiency comparison with existing graph pruning algorithms. Our observations show that our framework acts as a versatile operator for diverse tasks. It can be integrated into various GNN frameworks, boosting performance in-depth and offering an explainable approach to choosing the optimal network depth. The source code is available at https://github.com/bingreeky/HeteroSnoH.|图形神经网络(GNN)已经成为一系列基于图形的学习任务的关键工具。值得注意的是，大多数当前的 GNN 体系结构都是在同质的假设下运行的，无论是显式的还是隐式的。虽然这一基本假设经常被采用，但并非普遍适用，这可能导致潜在的缺陷，在学习效果。本文首次将流行的“一个节点一个接受域”的概念转化为异质图。通过构造一个代理标签预测器，我们使每个节点具有一个潜在的预测分布，这有助于连接的节点决定是否应该聚合它们的相关邻居。最终，每个节点都可以有自己独特的聚合跳和模式，就像每片雪花都是独一无二的并且具有自己的特点。在观察的基础上，我们创新性地引入了异质性雪花假说，为指导和促进异质性图及其他图的研究提供了一个有效的解决方案。我们进行了全面的实验，包括(1)10个图的主要结果，在10个主干上具有不同的异质性比率; (2)各种深层 GNN 主干(SGC，JKNet 等)的可伸缩性，跨越各种大量的层(2,4,6,8,16,32层) ; (3)与传统雪花假说的比较; (4)与现有图剪枝算法的效率比较。我们的观察表明，我们的框架充当多种任务的多功能操作员。它可以集成到各种 GNN 框架中，提高性能的深度，并为选择最佳网络深度提供一种可解释的方法。源代码可在 https://github.com/bingreeky/heterosnoh 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Heterophilic+Snowflake+Hypothesis:+Training+and+Empowering+GNNs+for+Heterophilic+Graphs)|0|
|[CutAddPaste: Time Series Anomaly Detection by Exploiting Abnormal Knowledge](https://doi.org/10.1145/3637528.3671739)|Rui Wang, Xudong Mou, Renyu Yang, Kai Gao, Pin Liu, Chongwei Liu, Tianyu Wo, Xudong Liu|; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Software, Beihang University & Zhongguancun Laboratory, Beijing, China; School of Information Engineering, China University of Geosciences Beijing, Beijing, China; Institute of Future Cities, The Chinese University of Hong Kong, Hong Kong, China; School of Software, Beihang University, Beijing, China; Kuaishou Inc., Beijing, China|Detecting time-series anomalies is extremely intricate due to the rarity of anomalies and imbalanced sample categories, which often result in costly and challenging anomaly labeling. Most of the existing approaches largely depend on assumptions of normality, overlooking labeled abnormal samples. While anomaly assumptions based methods can incorporate prior knowledge of anomalies for data augmentation in training classifiers, the adopted random or coarse-grained augmentation approaches solely focus on pointwise anomalies and lack cutting-edge domain knowledge, making them less likely to achieve better performance. This paper introduces CutAddPaste, a novel anomaly assumption-based approach for detecting time-series anomalies. It primarily employs a data augmentation strategy to generate pseudo anomalies, by exploiting prior knowledge of anomalies as much as possible. At the core of CutAddPaste is cutting patches from random positions in temporal subsequence samples, adding linear trend terms, and pasting them into other samples, so that it can well approximate a variety of anomalies, including point and pattern anomalies. Experiments on standard benchmark datasets demonstrate that our method outperforms the state-of-the-art approaches.|由于时间序列异常的罕见性和样本类别的不平衡性，检测时间序列异常极其复杂，往往导致昂贵和具有挑战性的异常标记。大多数现有的方法主要依赖于正常的假设，忽略了标记的异常样本。虽然基于异常假设的方法可以将异常的先验知识引入到训练分类器的数据增强中，但是所采用的随机或粗粒度增强方法只关注点态异常，缺乏尖端领域知识，使得它们不太可能获得更好的性能。本文介绍了一种新的基于异常假设的时间序列异常检测方法 CutAddPaste。它主要采用一种数据增强策略，通过尽可能多地利用异常的先验知识来产生伪异常。CutAddPaste 的核心是从时序子序列样本中的随机位置切割补丁，添加线性趋势项，并将其粘贴到其他样本中，以便能够很好地近似各种异常，包括点和模式异常。在标准基准数据集上的实验表明，我们的方法优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CutAddPaste:+Time+Series+Anomaly+Detection+by+Exploiting+Abnormal+Knowledge)|0|
|[Advancing Molecule Invariant Representation via Privileged Substructure Identification](https://doi.org/10.1145/3637528.3671886)|Ruijia Wang, Haoran Dai, Cheng Yang, Le Song, Chuan Shi|; BioMap Research & MBZUAI, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; Beijing University of Post and Telecommunication, Beijing, China|Graph neural networks (GNNs) have revolutionized molecule representation learning by modeling molecules as graphs, with atoms represented as nodes and chemical bonds as edges. Despite their progress, they struggle with out-of-distribution scenarios, such as changes in size or scaffold of molecules with identical properties. Some studies attempt to mitigate this issue through graph invariant learning, which penalizes prediction variance across environments to learn invariant representations. But in the realm of molecules, core functional groups forming privileged substructures dominate molecular properties and remain invariant across distribution shifts. This highlights the need for integrating this prior knowledge and ensuring the environment split compatible with molecule invariant learning. To bridge this gap, we propose a novel framework named MILI. Specifically, we first formalize molecule invariant learning based on privileged substructure identification and introduce substructure invariance constraint. Building on this foundation, we theoretically establish two criteria for environment splits conducive to molecule invariant learning. Inspired by these criteria, we develop a dual-head graph neural network. A shared identifier identifies privileged substructures, while environment and task heads generate predictions based on variant and privileged substructures. Through the interaction of two heads, the environments are split and optimized to meet our criteria. The unified MILI guarantees that molecule invariant learning and environment split achieve mutual enhancement from theoretical analysis and network design. Extensive experiments across eight benchmarks validate the effectiveness of MILI compared to state-of-the-art baselines.|图形神经网络(GNN)通过将分子建模为图形，将原子表示为节点，化学键表示为边，从而彻底改变了分子表示学习。尽管他们取得了进展，但是他们仍然在分布范围之外的场景中挣扎，比如具有相同性质的分子的大小或支架的变化。一些研究试图通过图不变学习来缓解这个问题，图不变学习通过惩罚环境间的预测方差来学习不变表示。但在分子领域，形成特权子结构的核心官能团支配着分子的性质，并且在分布变化中保持不变。这突出了整合这些先验知识和确保环境分裂与分子不变学习兼容的必要性。为了弥补这一差距，我们提出了一个新的框架 MILI。具体地说，我们首先将基于特权子结构识别的分子不变性学习形式化，并引入子结构不变性约束。在此基础上，我们从理论上建立了两个有利于分子不变学习的环境分裂准则。受这些准则的启发，我们开发了一个双头图神经网络。共享标识符识别特权子结构，而环境和任务负责人根据变体和特权子结构生成预测。通过两个头的相互作用，环境分裂和优化，以满足我们的标准。统一的 MILI 从理论分析和网络设计两个方面保证了分子不变学习和环境分裂的相互增强。通过八个基准的大量实验验证了 MILI 与最先进的基准相比的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Advancing+Molecule+Invariant+Representation+via+Privileged+Substructure+Identification)|0|
|[Optimizing Long-tailed Link Prediction in Graph Neural Networks through Structure Representation Enhancement](https://doi.org/10.1145/3637528.3671864)|Yakun Wang, Daixin Wang, Hongrui Liu, Binbin Hu, Yingcui Yan, Qiyang Zhang, Zhiqiang Zhang|Ant Group, Shanghai, China; Ant Group, Beijing, China; Ant Group, Hangzhou, China|Link prediction, as a fundamental task for graph neural networks (GNNs), has boasted significant progress in varied domains. Its success is typically influenced by the expressive power of node representation, but recent developments reveal the inferior performance of low-degree nodes owing to their sparse neighbor connections, known as the degree-based long-tailed problem. Will the degree-based long-tailed distribution similarly constrain the efficacy of GNNs on link prediction? Unexpectedly, our study reveals that only a mild correlation exists between node degree and predictive accuracy, and more importantly, the number of common neighbors between node pairs exhibits a strong correlation with accuracy. Considering node pairs with less common neighbors, i.e., tail node pairs, make up a substantial fraction of the dataset but achieve worse performance, we propose that link prediction also faces the long-tailed problem. Therefore, link prediction of GNNs is greatly hindered by the tail node pairs. After knowing the weakness of link prediction, a natural question is how can we eliminate the negative effects of the skewed long-tailed distribution on common neighbors so as to improve the performance of link prediction? Towards this end, we introduce our long-tailed framework (LTLP), which is designed to enhance the performance of tail node pairs on link prediction by increasing common neighbors. Two key modules in LTLP respectively supplement high-quality edges for tail node pairs and enforce representational alignment between head and tail node pairs within the same category, thereby improving the performance of tail node pairs. Empirical results across five datasets confirm that our approach not only achieves SOTA performance but also greatly reduces the performance bias between the head and tail. These findings underscore the efficacy and superiority of our framework in addressing the long-tailed problem in link prediction.|链接预测作为图形神经网络的基础性工作，在各个领域都取得了显著的进展。它的成功通常受到节点表示的表达能力的影响，但最近的发展揭示了低度节点由于其稀疏邻居连接性能较差，称为基于度的长尾问题。基于度的长尾分布同样会限制 GNN 链路预测的效率吗？出乎意料的是，我们的研究表明，节点程度与预测精度之间只存在轻微的相关性，更重要的是，节点对之间共同邻居的数量与预测精度有很强的相关性。考虑到具有较少共同邻居的节点对，即尾节点对，在数据集中占很大比例，但性能较差，我们认为链路预测也面临着长尾问题。因此，尾节点对极大地阻碍了 GNN 的链路预测。在认识到链路预测的弱点之后，如何消除偏斜长尾分布对公共邻居的负面影响，从而提高链路预测的性能，是一个很自然的问题？为此，我们引入了长尾结构(LTLP) ，该结构通过增加公共邻居来提高尾节点对的链路预测性能。LTLP 中的两个关键模块分别补充了尾节点对的高质量边，并在同一类别中加强了头尾节点对的表示对齐，从而提高了尾节点对的性能。实验结果表明，该方法不仅能够实现 SOTA 性能，而且能够大大降低 SOTA 的性能偏差。这些发现强调了我们的框架在解决链接预测中的长尾问题的有效性和优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+Long-tailed+Link+Prediction+in+Graph+Neural+Networks+through+Structure+Representation+Enhancement)|0|
|[DiffCrime: A Multimodal Conditional Diffusion Model for Crime Risk Map Inference](https://doi.org/10.1145/3637528.3671843)|Shuliang Wang, Xinyu Pan, Sijie Ruan, Haoyu Han, Ziyu Wang, Hanning Yuan, Jiabao Zhu, Qi Li|Beijing Institute of Technology, Beijing, China|Crime risk map plays a crucial role in urban planning and public security management. Traditionally, it is obtained solely from historical crime incidents or inferred from limited environmental factors, which are not sufficient to accurately model the occurrences of crimes over the geographical space well. Motivated by the impressive and realistic conditional generating power of diffusion models, in this paper, we propose a multimodal conditional diffusion method, namely, DiffCrime, to infer the crime risk map based on datasets in various domains, i.e., historical crime incidents, satellite imagery, and map imagery. It is equipped with a history-gated multimodal denoising network, i.e., HamNet, dedicated to the crime risk map inference. HamNet emphasizes the importance of historical crime data via a Gated-based History Fusion (GHF) module and adaptively controls multimodal conditions to be fused across different diffusion time steps via a Time step-Aware Modality Fusion (TAMF) module. Extensive experiments on two real-world datasets demonstrate the effectiveness of DiffCrime, which outperforms baselines by at least 43% and 31% in terms of RMSE, respectively.|犯罪风险图在城市规划和治安管理中具有重要作用。传统上，它仅仅是从历史犯罪事件中获得的，或者是从有限的环境因素中推断出来的，这些因素不足以很好地准确地模拟地理空间上的犯罪事件。本文基于扩散模型令人印象深刻的现实条件生成能力，提出了一种多模态条件扩散方法，即“区分犯罪”，该方法基于不同领域的数据集，即历史犯罪事件、卫星地图和地图图像，推断犯罪风险图。它配备了一个历史门控的多模式去噪网络，即 HamNet，专门用于犯罪风险图推断。HamNet 通过一个基于门的历史融合(GHF)模块强调历史犯罪数据的重要性，并通过一个时间步感知模态融合(TAMF)模块自适应地控制跨不同扩散时间步骤进行融合的多模态条件。在两个真实世界数据集上的大量实验证明了区分犯罪的有效性，根据 RMSE，区分犯罪的性能至少比基线高出43% 和31% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DiffCrime:+A+Multimodal+Conditional+Diffusion+Model+for+Crime+Risk+Map+Inference)|0|
|[AsyncET: Asynchronous Representation Learning for Knowledge Graph Entity Typing](https://doi.org/10.1145/3637528.3671832)|YunCheng Wang, Xiou Ge, Bin Wang, C.C. Jay Kuo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AsyncET:+Asynchronous+Representation+Learning+for+Knowledge+Graph+Entity+Typing)|0|
|[Unveiling Global Interactive Patterns across Graphs: Towards Interpretable Graph Neural Networks](https://doi.org/10.1145/3637528.3671838)|Yuwen Wang, Shunyu Liu, Tongya Zheng, Kaixuan Chen, Mingli Song||Graph Neural Networks (GNNs) have emerged as a prominent framework for graph mining, leading to significant advances across various domains. Stemmed from the node-wise representations of GNNs, existing explanation studies have embraced the subgraph-specific viewpoint that attributes the decision results to the salient features and local structures of nodes. However, graph-level tasks necessitate long-range dependencies and global interactions for advanced GNNs, deviating significantly from subgraph-specific explanations. To bridge this gap, this paper proposes a novel intrinsically interpretable scheme for graph classification, termed as Global Interactive Pattern (GIP) learning, which introduces learnable global interactive patterns to explicitly interpret decisions. GIP first tackles the complexity of interpretation by clustering numerous nodes using a constrained graph clustering module. Then, it matches the coarsened global interactive instance with a batch of self-interpretable graph prototypes, thereby facilitating a transparent graph-level reasoning process. Extensive experiments conducted on both synthetic and real-world benchmarks demonstrate that the proposed GIP yields significantly superior interpretability and competitive performance to the state-of-the-art counterparts. Our code will be made publicly available¹.|图神经网络(GNN)已经成为图挖掘的一个重要框架，在各个领域都取得了显著的进展。从 GNN 的节点表示出发，现有的解释研究采用子图特定的观点，将决策结果归因于节点的显著特征和局部结构。然而，图级任务需要高级 GNN 的远程依赖和全局交互，这与特定于子图的解释大相径庭。为了弥补这一差距，本文提出了一种新的图分类方法——全局交互模式(GIP)学习方法，该方法引入了可学习的全局交互模式来显式地解释决策。GIP 首先使用一个约束图聚类模块对大量节点进行聚类，从而解决解释的复杂性问题。然后，将粗化的全局交互实例与一批可自解释的图原型进行匹配，从而实现透明的图级推理过程。在合成和现实世界基准上进行的大量实验表明，所提议的 GIP 产生的可解释性和竞争性能明显优于最先进的对应方。我们的代码将公开发布1。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unveiling+Global+Interactive+Patterns+across+Graphs:+Towards+Interpretable+Graph+Neural+Networks)|0|
|[Self-Supervised Learning for Graph Dataset Condensation](https://doi.org/10.1145/3637528.3671682)|Yuxiang Wang, Xiao Yan, Shiyu Jin, Hao Huang, Quanqing Xu, Qingchen Zhang, Bo Du, Jiawei Jiang|School of Computer Science, Wuhan University, Wuhan, China; OceanBase, Ant Group, Hangzhou, China; School of Computer Science and Technology, Hainan University, Haikou, China; Centre for Perceptual and Interactive Intelligence (CPII), Hong Kong, China|Graph dataset condensation (GDC) reduces a dataset with many graphs into a smaller dataset with fewer graphs while maintaining model training accuracy. GDC saves the storage cost and hence accelerates training. Although several GDC methods have been proposed, they are all supervised and require massive labels for the graphs, while graph labels can be scarce in many practical scenarios. To fill this gap, we propose a self-supervised graph dataset condensation method called SGDC, which does not require label information. Our initial design starts with the classical bilevel optimization paradigm for dataset condensation and incorporates contrastive learning techniques. But such a solution yields poor accuracy due to the biased gradient estimation caused by data augmentation. To solve this problem, we introduce representation matching, which conducts training by aligning the representations produced by the condensed graphs with the target representations generated by a pre-trained SSL model. This design eliminates the need for data augmentation and avoids biased gradient. We further propose a graph attention kernel, which not only improves accuracy but also reduces running time when combined with self-supervised kernel ridge regression (KRR). To simplify SGDC and make it more robust, we adopt a adjacency matrix reusing approach, which reuses the topology of the original graphs for the condensed graphs instead of repeatedly learning topology during training. Our evaluations on seven graph datasets find that SGDC improves model accuracy by up to 9.7% compared with 5 state-of-the-art baselines, even if they use label information. Moreover, SGDC is significantly more efficient than the baselines.|图形数据集压缩(GDC)在保持模型训练精度的同时，将多个图形的数据集压缩成较小的图形数据集。GDC 节省了存储成本，因此加速了培训。虽然已经提出了几种 GDC 方法，但它们都是有监督的，需要对图进行大量的标记，而在许多实际场景中，图标记是稀缺的。为了填补这一空白，我们提出了一种称为 SGDC 的自监督图形数据集压缩方法，该方法不需要标签信息。我们最初的设计开始于经典的数据集压缩的双层优化范例，并采用了对比学习技术。但是由于数据增强引起的梯度估计偏差，使得这种解的精度较差。为了解决这个问题，我们引入了表示匹配技术，该技术通过将压缩图生成的表示与预先训练好的 SSL 模型生成的目标表示对齐来进行训练。这种设计消除了数据增强的需要，避免了有偏的梯度。进一步提出了一种图注意核，它与自监督核岭回归(KRR)相结合，不仅提高了精度，而且减少了运行时间。为了简化 SGDC 并使其更加健壮，我们采用了一种邻接矩阵重用的方法，在训练过程中重用原始图的拓扑结构来代替重复学习拓扑结构。我们对七个图形数据集的评估发现，与5个最先进的基线相比，即使使用标签信息，SGDC 也提高了模型的准确性达9.7% 。此外，SGDC 明显比基线更有效。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Supervised+Learning+for+Graph+Dataset+Condensation)|0|
|[From Supervised to Generative: A Novel Paradigm for Tabular Deep Learning with Large Language Models](https://doi.org/10.1145/3637528.3671975)|Xumeng Wen, Han Zhang, Shun Zheng, Wei Xu, Jiang Bian|Tsinghua University, Beijing, China; Microsoft Research Asia, Beijing, China|Tabular data is foundational to predictive modeling in various crucial industries, including healthcare, finance, retail, sustainability, etc. Despite the progress made in specialized models, there is an increasing demand for universal models that can transfer knowledge, generalize from limited data, and follow human instructions. These are challenges that current tabular deep learning approaches have not fully tackled. Here we introduce Generative Tabular Learning (GTL), a novel framework that integrates the advanced functionalities of large language models (LLMs)-such as prompt-based zero-shot generalization and in-context learning-into tabular deep learning. GTL capitalizes on the pre-training of LLMs on diverse tabular data, enhancing their understanding of domain-specific knowledge, numerical sequences, and statistical dependencies critical for accurate predictions. Our empirical study spans 384 public datasets, rigorously analyzing GTL's convergence and scaling behaviors and assessing the impact of varied data templates. The GTL-enhanced LLaMA-2 model demonstrates superior zero-shot and in-context learning capabilities across numerous classification and regression tasks. Notably, it achieves this without fine-tuning, outperforming traditional methods and rivaling state-of-the-art models like GPT-4 in certain cases. Through GTL, we not only foster a deeper integration of LLMs' sophisticated abilities into tabular data comprehension and application but also offer a new training resource and a test bed for LLMs to enhance their ability to comprehend tabular data. To facilitate reproducible research, we release our code, data, and model checkpoints at https://github.com/microsoft/Industrial-Foundation-Models.|表格数据是各种关键行业预测建模的基础，包括医疗、金融、零售、可持续发展等。尽管在专业模型方面取得了进展，但对能够传递知识、从有限的数据中归纳出结论并遵循人类指令的通用模型的需求日益增长。这些是目前表格式深度学习方法尚未完全解决的挑战。在这里，我们介绍了生成表学习(GTL) ，一个新的框架，集成了大型语言模型(LLM)的先进功能-如基于提示的零拍泛化和在上下文学习-到表深度学习。GTL 利用 LLM 对不同表格数据的预训练，增强他们对领域特定知识、数字序列和对准确预测至关重要的统计依赖性的理解。我们的实证研究涵盖了384个公共数据集，严格分析了 GTL 的收敛和缩放行为，并评估了不同数据模板的影响。GTL 增强的 LLaMA-2模型展示了在众多分类和回归任务中优越的零击和上下文学习能力。值得注意的是，它实现这一点没有微调，超过传统的方法和竞争国家的最先进的模型，如 GPT-4在某些情况下。通过 GTL，我们不仅促进了 LLM 复杂能力在表格数据理解和应用中的深入整合，而且为 LLM 提供了一种新的训练资源和测试平台，以提高 LLM 理解表格数据的能力。为了便于重复研究，我们在 https://github.com/microsoft/industrial-foundation-models 发布代码、数据和模型检查点。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Supervised+to+Generative:+A+Novel+Paradigm+for+Tabular+Deep+Learning+with+Large+Language+Models)|0|
|[Dense Subgraph Discovery Meets Strong Triadic Closure](https://doi.org/10.1145/3637528.3671697)|Chamalee Wickrama Arachchi, Iiro Kumpulainen, Nikolaj Tatti|University of Helsinki, Helsinki, Finland; HIIT, University of Helsinki, Helsinki, Finland|Finding dense subgraphs is a core problem with numerous graph mining applications such as community detection in social networks and anomaly detection. However, in many real-world networks connections are not equal. One way to label edges as either strong or weak is to use strong triadic closure~(STC). Here, if one node connects strongly with two other nodes, then those two nodes should be connected at least with a weak edge. STC-labelings are not unique and finding the maximum number of strong edges is NP-hard. In this paper, we apply STC to dense subgraph discovery. More formally, our score for a given subgraph is the ratio between the sum of the number of strong edges and weak edges, weighted by a user parameter λ, and the number of nodes of the subgraph. Our goal is to find a subgraph and an STC-labeling maximizing the score. We show that for λ = 1, our problem is equivalent to finding the densest subgraph, while for λ = 0, our problem is equivalent to finding the largest clique, making our problem NP-hard. We propose an exact algorithm based on integer linear programming and four practical polynomial-time heuristics. We present an extensive experimental study that shows that our algorithms can find the ground truth in synthetic datasets and run efficiently in real-world datasets.|寻找稠密子图是许多图挖掘应用程序的核心问题，比如社交网络和异常检测中的社区检测。然而，在现实世界中，许多网络连接是不相等的。将边标记为强或弱的一种方法是使用强三元闭包 ~ (STC)。在这里，如果一个节点与另外两个节点强烈连接，那么这两个节点至少应该用弱边连接。STC 标记不是唯一的，寻找强边的最大个数是 NP 难的。本文将 STC 应用于稠密子图的发现。更正式地说，我们给定子图的得分是由用户参数 λ 加权的强边数和弱边数之和与子图的节点数之比。我们的目标是找到一个子图和一个 STC 标记最大化得分。证明了对于 λ = 1，我们的问题等价于求最密子图，而对于 λ = 0，我们的问题等价于求最大团，使我们的问题 NP 难。我们提出了一个基于整数线性规划和四个实际的多项式时间启发式的精确算法。我们提出了一个广泛的实验研究表明，我们的算法可以找到地面真理合成数据集和运行在真实世界的数据集有效。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dense+Subgraph+Discovery+Meets+Strong+Triadic+Closure)|0|
|[FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model](https://doi.org/10.1145/3637528.3671897)|Feijie Wu, Zitao Li, Yaliang Li, Bolin Ding, Jing Gao|Purdue University, West Lafayette, USA; Alibaba Group, Bellevue, USA|Large language models (LLMs) show amazing performance on many domain-specific tasks after fine-tuning with some appropriate data. However, many domain-specific data are privately distributed across multiple owners. Thus, this dilemma raises the interest in how to perform LLM fine-tuning in federated learning (FL). However, confronted with limited computation and communication capacities, FL clients struggle to fine-tune an LLM effectively. To this end, we introduce FedBiOT, a resource-efficient LLM fine-tuning approach to FL. Specifically, our method involves the server generating a compressed LLM and aligning its performance with the full model. Subsequently, the clients fine-tune a lightweight yet important part of the compressed model, referred to as an adapter. Notice that as the server has no access to the private data owned by the clients, the data used for alignment by the server has a different distribution from the one used for fine-tuning by clients. We formulate the problem into a bi-level optimization problem to minimize the negative effect of data discrepancy and derive the updating rules for the server and clients. We conduct extensive experiments on LLaMA-2, empirically showing that the adapter has exceptional performance when reintegrated into the global LLM. The results also indicate that the proposed FedBiOT significantly reduces resource consumption compared to existing benchmarks, all while achieving comparable performance levels.|在使用适当的数据进行微调之后，大型语言模型(LLM)在许多特定于领域的任务上显示出惊人的性能。但是，许多特定于域的数据是私有地分布在多个所有者之间的。因此，这种困境引起了人们对如何在联邦学习(FL)中执行 LLM 微调的兴趣。然而，面对有限的计算和通信能力，FL 客户端很难对 LLM 进行有效的微调。为此，我们介绍了 FedBiOT，一种资源有效的 LLM 微调方法。具体来说，我们的方法涉及到服务器生成一个压缩的 LLM 并使其性能与完整模型保持一致。随后，客户机对压缩模型的一个轻量级但重要的部分(称为适配器)进行微调。请注意，由于服务器不能访问客户机拥有的私有数据，因此用于服务器对齐的数据与用于客户机微调的数据具有不同的分布。我们把问题分解成两个最佳化问题，以尽量减少数据差异的负面影响，并推导出服务器和客户端的更新规则。我们在 LLaMA-2上进行了广泛的实验，经验表明，当重新集成到全局 LLM 时，该适配器具有优异的性能。结果还表明，与现有基准相比，提出的 FedBiOT 显著降低了资源消耗，同时达到了可比的性能水平。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedBiOT:+LLM+Local+Fine-tuning+in+Federated+Learning+without+Full+Model)|0|
|[Neural Manifold Operators for Learning the Evolution of Physical Dynamics](https://doi.org/10.1145/3637528.3671779)|Hao Wu, Kangyu Weng, Shuyi Zhou, Xiaomeng Huang, Wei Xiong|; Tencent TEG, Beijing, China; Xingjian College, Tsinghua University, Beijing, China|Modeling the evolution of physical dynamics is a foundational problem in science and engineering, and it is regarded as the modeling of an operator mapping between infinite-dimensional functional spaces. Operator learning methods, learning the underlying infinite-dimensional operator in a high-dimensional latent space, have shown significant potential in modeling physical dynamics. However, there remains insufficient research on how to approximate an infinite-dimensional operator using a finite-dimensional parameter space. Inappropriate dimensionality representation of the underlying operator leads to convergence difficulties, decreasing generalization capability, and violating the physical consistency. To address the problem, we present Neural Manifold Operator (NMO) to learn the invariant subspace with the intrinsic dimension to parameterize infinite-dimensional underlying operators. NMO achieves state-of-the-art performance in statistical and physical metrics and gains 23.35% average improvement on three real-world scenarios and four equation-governed scenarios across a wide range of multi-disciplinary fields. Our paradigm has demonstrated universal effectiveness across various model structure implementations, including Multi-Layer Perceptron, Convolutional Neural Networks, and Transformers. Experimentally, we prove that the intrinsic dimension calculated by our paradigm is the optimal dimensional representation of the underlying operators. We release our code at https://github.com/AI4EarthLab/Neural-Manifold-Operators.|物理动力学演化建模是科学和工程领域的一个基础性问题，它被认为是无限维函数空间之间算子映射的建模。算子学习方法，即在高维潜空间中学习潜在的无穷维算子，在物理动力学建模中显示出巨大的潜力。然而，对于如何利用有限维参数空间逼近无穷维算子，目前还缺乏足够的研究。底层算子的维数表示不当会导致收敛困难、泛化能力下降和物理一致性的破坏。为了解决这个问题，我们提出神经流形算子(NMO)来学习不变子空间的本征维度，以参数化无限维的底层算子。NMO 在统计和物理指标方面实现了最先进的性能，在三个现实世界场景和四个多学科领域的方程式控制场景方面平均提高了23.35% 。我们的范式已经证明了在各种模型结构实现的通用有效性，包括多层感知器，卷积神经网络和变压器。在实验上，我们证明了由我们的范式计算出的本征维度是潜在运算符的最佳维度表示。我们在 https://github.com/ai4earthlab/neural-manifold-operators 发布我们的代码。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Manifold+Operators+for+Learning+the+Evolution+of+Physical+Dynamics)|0|
|[Distributional Network of Networks for Modeling Data Heterogeneity](https://doi.org/10.1145/3637528.3671994)|Jun Wu, Jingrui He, Hanghang Tong|University of Illinois at Urbana-Champaign, Champaign, IL, USA|Heterogeneous data widely exists in various high-impact applications. Domain adaptation and out-of-distribution generalization paradigms have been formulated to handle the data heterogeneity across domains. However, most existing domain adaptation and out-of-distribution generalization algorithms do not explicitly explain how the label information can be adaptively propagated from the source domains to the target domain. Furthermore, little effort has been devoted to theoretically understanding the convergence of existing algorithms based on neural networks. To address these problems, in this paper, we propose a generic distributional network of networks (TENON) framework, where each node of the main network represents an individual domain associated with a domain-specific network. In this case, the edges within the main network indicate the domain similarity, and the edges within each network indicate the sample similarity. The crucial idea of TENON is to characterize the within-domain label smoothness and cross-domain parameter smoothness in a unified framework. The convergence and optimality of TENON are theoretically analyzed. Furthermore, we show that based on the TENON framework, domain adaptation and out-of-distribution generalization can be naturally formulated as transductive and inductive distribution learning problems, respectively. This motivates us to develop two instantiated algorithms (TENON-DA and TENON-OOD) of the proposed TENON framework for domain adaptation and out-of-distribution generalization. The effectiveness and efficiency of TENON-DA and TENON-OOD are verified both theoretically and empirically.|异构数据广泛存在于各种高影响的应用程序中。领域适应和分布外泛化范例已经被制定来处理跨领域的数据异构性。然而，大多数现有的域自适应和分布外泛化算法并没有明确地解释如何将标签信息从源域自适应地传播到目标域。此外，在理论上对现有的基于神经网络的算法的收敛性研究很少。为了解决这些问题，本文提出了一个通用分布式网络(TENON)框架，其中主网络的每个节点代表一个与特定领域网络相关联的独立领域。在这种情况下，主网络中的边表示领域相似性，每个网络中的边表示样本相似性。TENON 的核心思想是在一个统一的框架内描述域内标签平滑性和跨域参数平滑性。从理论上分析了 TENON 的收敛性和最优性。此外，基于 TENON 框架，领域适应和分布外泛化可以分别自然地表述为导性分布学习问题和归纳分布学习问题。这促使我们开发两个实例化算法(TENON-DA 和 TENON-OOD)的提议的 TENON 框架领域适应和分布外泛化。从理论和实验两方面验证了 TENON-DA 和 TENON-OOD 方法的有效性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distributional+Network+of+Networks+for+Modeling+Data+Heterogeneity)|0|
|[Fake News in Sheep's Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks](https://doi.org/10.1145/3637528.3671977)|Jiaying Wu, Jiafeng Guo, Bryan Hooi|University of Chinese Academy of Sciences & Institute of Computing Technology, CAS, Beijing, China; National University of Singapore, Singapore, Singapore|It is commonly perceived that fake news and real news exhibit distinct writing styles, such as the use of sensationalist versus objective language. However, we emphasize that style-related features can also be exploited for style-based attacks. Notably, the advent of powerful Large Language Models (LLMs) has empowered malicious actors to mimic the style of trustworthy news sources, doing so swiftly, cost-effectively, and at scale. Our analysis reveals that LLM-camouflaged fake news content significantly undermines the effectiveness of state-of-the-art text-based detectors (up to 38% decrease in F1 Score), implying a severe vulnerability to stylistic variations. To address this, we introduce SheepDog, a style-robust fake news detector that prioritizes content over style in determining news veracity. SheepDog achieves this resilience through (1) LLM-empowered news reframings that inject style diversity into the training process by customizing articles to match different styles; (2) a style-agnostic training scheme that ensures consistent veracity predictions across style-diverse reframings; and (3) content-focused veracity attributions that distill content-centric guidelines from LLMs for debunking fake news, offering supplementary cues and potential intepretability that assist veracity prediction. Extensive experiments on three real-world benchmarks demonstrate SheepDog's style robustness and adaptability to various backbones.|人们通常认为，假新闻和真新闻表现出不同的写作风格，如使用耸人听闻和客观的语言。然而，我们强调与样式相关的特性也可以用于基于样式的攻击。值得注意的是，强大的大型语言模型(LLM)的出现使恶意的参与者能够模仿可信赖的新闻来源的风格，这样做迅速、具有成本效益并且具有规模效应。我们的分析表明，LLM 伪装的假新闻内容显着破坏了最先进的基于文本的检测器的有效性(F1分数下降了38%) ，这意味着对风格变化的严重脆弱性。为了解决这个问题，我们引入了 SheepDog，一个风格健壮的假新闻检测器，它在确定新闻真实性时优先考虑内容而不是风格。SheepDog 通过(1) LLM 授权的新闻重构，通过定制文章以匹配不同的风格，将风格多样性注入到培训过程中; (2)风格不可知的培训方案，确保在风格多样性的重构过程中保持一致的准确性预测; (3)内容为中心的准确性属性，从 LLM 中提取内容为中心的指导方针，以揭穿假新闻，提供补充线索和潜在的可理解性，协助准确性预测。在三个真实世界基准上的大量实验证明了 SheepDog 的风格健壮性和对各种主干的适应性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fake+News+in+Sheep's+Clothing:+Robust+Fake+News+Detection+Against+LLM-Empowered+Style+Attacks)|0|
|[Counterfactual Generative Models for Time-Varying Treatments](https://doi.org/10.1145/3637528.3671950)|Shenghao Wu, Wenbin Zhou, Minshuo Chen, Shixiang Zhu|Carnegie Mellon University, Pittsburgh, PA, USA; Princeton University, Princeton, NJ, USA|Estimating the counterfactual outcome of treatment is essential for decision-making in public health and clinical science, among others. Often, treatments are administered in a sequential, time-varying manner, leading to an exponentially increased number of possible counterfactual outcomes. Furthermore, in modern applications, the outcomes are high-dimensional and conventional average treatment effect estimation fails to capture disparities in individuals. To tackle these challenges, we propose a novel conditional generative framework capable of producing counterfactual samples under time-varying treatment, without the need for explicit density estimation. Our method carefully addresses the distribution mismatch between the observed and counterfactual distributions via a loss function based on inverse probability re-weighting, and supports integration with state-of-the-art conditional generative models such as the guided diffusion and conditional variational autoencoder. We present a thorough evaluation of our method using both synthetic and real-world data. Our results demonstrate that our method is capable of generating high-quality counterfactual samples and outperforms the state-of-the-art baselines.|估计治疗的反事实结果对于公共卫生和临床科学等领域的决策至关重要。通常，治疗是以连续的、时间变化的方式进行的，导致可能的反事实结果的数量呈指数级增长。此外，在现代应用，结果是高维和传统的平均治疗效果估计不能捕捉个人的差异。为了应对这些挑战，我们提出了一种新的条件生成框架，它能够在时变处理下产生反事实样本，而不需要显式的密度估计。我们的方法通过基于逆概率重新加权的损失函数仔细地处理观测分布和反事实分布之间的分布不匹配，并支持与最先进的条件生成模型(如引导扩散和条件变分自动编码器)的集成。我们提出了一个彻底的评估我们的方法使用合成和真实世界的数据。我们的结果表明，我们的方法能够生成高质量的反事实样本，并优于国家的最先进的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Counterfactual+Generative+Models+for+Time-Varying+Treatments)|0|
|[ProCom: A Few-shot Targeted Community Detection Algorithm](https://doi.org/10.1145/3637528.3671749)|Xixi Wu, Kaiyu Xiong, Yun Xiong, Xiaoxin He, Yao Zhang, Yizhu Jiao, Jiawei Zhang|; University of Illinois at Urbana-Champaign, Champaign, Illinois, USA; National University of Singapore, Singapore, Singapore; IFM Lab, Department of Computer Science, University of California, Davis, Davis, California, USA|Targeted community detection aims to distinguish a particular type of community in the network. This is an important task with a lot of real-world applications, e.g., identifying fraud groups in transaction networks. Traditional community detection methods fail to capture the specific features of the targeted community and detect all types of communities indiscriminately. Semi-supervised community detection algorithms, emerged as a feasible alternative, are inherently constrained by their limited adaptability and substantial reliance on a large amount of labeled data, which demands extensive domain knowledge and manual effort. In this paper, we address the aforementioned weaknesses in targeted community detection by focusing on few-shot scenarios. We propose ProCom, a novel framework that extends the "pre-train, prompt'' paradigm, offering a low-resource, high-efficiency, and transferable solution. Within the framework, we devise a dual-level context-aware pre-training method that fosters a deep understanding of latent communities in the network, establishing a rich knowledge foundation for downstream tasks. In the prompt learning stage, we reformulate the targeted community detection task into pre-training objectives, allowing the extraction of specific knowledge relevant to the targeted community to facilitate effective and efficient inference. By leveraging both the general community knowledge acquired during pre-training and the specific insights gained from the prompt communities, ProCom exhibits remarkable adaptability across different datasets. We conduct extensive experiments on five benchmarks to evaluate the ProCom framework, demonstrating its SOTA performance under few-shot scenarios, strong efficiency, and transferability across diverse datasets.|有针对性的社区检测旨在区分网络中特定类型的社区。对于许多实际应用程序来说，这是一项重要的任务，例如，识别交易网络中的欺诈团伙。传统的社区检测方法不能捕捉目标社区的具体特征，不加区分地检测所有类型的社区。半监督社区检测算法作为一种可行的替代算法，由于其适应性有限和对大量标记数据的严重依赖而受到固有的限制，需要广泛的领域知识和人工操作。在本文中，我们针对上述的弱点，在目标社区检测的重点是少数拍摄场景。我们提出 ProCom，一个新的框架，它扩展了“预训练，及时”范式，提供了一个低资源，高效率，可转移的解决方案。在这个框架内，我们设计了一个双层次上下文感知的预训练方法，促进了对网络中潜在群体的深入理解，为下游任务建立了丰富的知识基础。在即时学习阶段，我们会把目标社群侦测工作重新制订为训练前的目标，以便提取与目标社群有关的特定知识，从而促进有效和高效率的推论。通过利用在培训前获得的一般社区知识和从迅速的社区获得的具体见解，ProCom 在不同的数据集中表现出显著的适应性。我们在五个基准上进行了广泛的实验来评估 ProCom 框架，展示了它在短镜头场景下的 SOTA 性能，强大的效率和跨不同数据集的可转移性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ProCom:+A+Few-shot+Targeted+Community+Detection+Algorithm)|0|
|[Cost-Efficient Fraud Risk Optimization with Submodularity in Insurance Claim](https://doi.org/10.1145/3637528.3672012)|Yupeng Wu, Zhibo Zhu, Chaoyi Ma, Hong Qian, Xingyu Lu, Yangwenhui Zhang, Xiaobo Qin, Binjie Fei, Jun Zhou, Aimin Zhou|School of Computer Science and Technology, East China Normal University, Shanghai, China; AntGroup, Hangzhou, China; Ant Group, Hangzhou, China|The fraudulent insurance claim is critical for the insurance industry. Insurance companies or agency platforms aim to confidently estimate the fraud risk of claims by gathering data from various sources. Although more data sources can improve the estimation accuracy, they inevitably lead to increased costs. Therefore, a great challenge of fraud risk verification lies in well balancing these two aspects. To this end, this paper proposes a framework named cost-efficient fraud risk optimization with submodularity (CEROS) to optimize the process of fraud risk verification. CEROS efficiently allocates investigation resources across multiple information sources, balancing the trade-off between accuracy and cost. CEROS consists of two parts that we propose: a submodular set-wise classification model called SSCM to estimate the submodular objective function, and a primal-dual algorithm with segmentation point called PDA-SP to solve the objective function. Specifically, SSCM models the fraud probability associated with multiple information sources and ensures the properties of submodularity of fraud risk without making independence assumption. The submodularity in SSCM enables PDA-SP to significantly speed up dual optimization. Theoretically, we disclose that when PDA-SP optimizes this dual optimization problem, the process is monotonicity. Finally, the trade-off coefficients output by PDA-SP that balance accuracy and cost in fraud risk verification are applied to online insurance claim decision-making. We conduct experiments on offline trials and online A/B tests in two business areas at Alipay: healthcare insurance recommendation and claim verification. The extensive results indicate that, compared with other methods, CEROS achieves acceleration of 66.9% in convergence speed and meanwhile 18.8% in cost reduction. Currently, CEROS has been successfully deployed in Alipay.|欺诈性保险索赔对保险业至关重要。保险公司或代理平台的目的是通过从各种来源收集数据，有信心地估计索赔的欺诈风险。尽管更多的数据源可以提高估计的准确性，但它们不可避免地会导致成本增加。因此，欺诈风险验证的一个巨大挑战就是如何很好地平衡这两个方面。为此，本文提出了一种基于次模块化的具有成本效益的欺诈风险优化(CEROS)框架来优化欺诈风险验证过程。CEROS 在多个信息来源之间有效地分配调查资源，在准确性和成本之间取得平衡。CEROS 由两部分组成: 子模块集合分类模型 SSCM 估计子模块目标函数，原始-对偶算法 PDA-SP 求解目标函数。具体来说，SSCM 建立了与多个信息源相关的欺诈概率模型，保证了欺诈风险的子模块性，而没有做出独立的假设。SSCM 的次模块性使得 PDA-SP 可以大大加快双重优化的速度。理论上，我们揭示了当 PDA-SP 优化这个双最佳化问题时，过程是单调的。最后，将 PDA-SP 输出的权衡系数应用于在线保险索赔决策中，平衡了欺诈风险验证的准确性和成本。我们在支付宝的两个业务领域进行离线试验和在线 A/B 测试: 医疗保险推荐和理赔验证。广泛的结果表明，与其他方法相比，CEROS 算法在收敛速度上提高了66.9% ，同时降低了成本18.8% 。目前，CEROS 已经成功地部署在支付宝上。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cost-Efficient+Fraud+Risk+Optimization+with+Submodularity+in+Insurance+Claim)|0|
|[A Deep Prediction Framework for Multi-Source Information via Heterogeneous GNN](https://doi.org/10.1145/3637528.3671966)|Zhen Wu, Jingya Zhou, Jinghui Zhang, Ling Liu, Chizhou Huang|; School of Computer Science and Engineering, Southeast University, Nanjing, China; School of Computer Science, Georgia Institute of Technology, Atlanta, USA; School of Computer Science and Technology, Soochow University, Suzhou, China|Predicting information diffusion is a fundamental task in online social networks (OSNs). Recent studies mainly focus on the popularity prediction of specific content but ignore the correlation between multiple pieces of information. The topic is often used to correlate such information and can correspond to multi-source information. The popularity of a topic relies not only on information diffusion time but also on users' followership. Current solutions concentrate on hard time partition, lacking versatility. Meanwhile, the hop-based sampling adopted in state-of-the-art (SOTA) methods encounters redundant user followership. Moreover, many SOTA methods are not designed with good modularity and lack evaluation for each functional module and enlightening discussion. This paper presents a novel extensible framework, coined as HIF, for effective popularity prediction in OSNs with four original contributions. First, HIF adopts a soft partition of users and time intervals to better learn users' behavioral preferences over time. Second, HIF utilizes weighted sampling to optimize the construction of heterogeneous graphs and reduce redundancy. Furthermore, HIF supports multi-task collaborative optimization to improve its learning capability. Finally, as an extensible framework, HIF provides generic module slots to combine different submodules (e.g., RNNs, Transformer encoders). Experiments show that HIF significantly improves performance and interpretability compared to SOTAs.|预测信息扩散是在线社交网络(OSNs)的一项基本任务。目前的研究主要集中在对特定内容的流行程度进行预测，而忽略了多个信息片段之间的相关性。该主题通常用于关联此类信息，并且可以对应于多源信息。一个话题的受欢迎程度不仅取决于信息的传播时间，还取决于用户的追随者。目前的解决方案集中在硬时间划分，缺乏通用性。同时，最先进的 SOTA 方法中采用的基于跳数的抽样方法遇到了冗余的用户跟随问题。此外，许多 SOTA 方法的设计缺乏良好的模块性，缺乏对各功能模块的评价和有启发性的讨论。本文提出了一个新的可扩展框架，称为 HIF，用于有效预测开源网络的流行程度。首先，HIF 采用用户的软分区和时间间隔，以更好地了解用户的行为偏好随着时间的推移。其次，HIF 利用加权抽样优化异构图的构造，减少冗余;。此外，HIF 还支持多任务协同优化，以提高其学习能力。最后，作为一个可扩展的框架，HIF 提供了通用的模块插槽来组合不同的子模块(例如，RNN、 Transformer 编码器)。实验表明，与 SOTA 相比，HIF 显著提高了性能和可解释性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Deep+Prediction+Framework+for+Multi-Source+Information+via+Heterogeneous+GNN)|0|
|[Fast Computation of Kemeny's Constant for Directed Graphs](https://doi.org/10.1145/3637528.3671859)|Haisong Xia, Zhongzhi Zhang|Fudan University, Shanghai, China|Kemeny's constant for random walks on a graph is defined as the mean hitting time from one node to another selected randomly according to the stationary distribution. It has found numerous applications and attracted considerable research interest. However, exact computation of Kemeny's constant requires matrix inversion, which scales poorly for large networks with millions of nodes. Existing approximation algorithms either leverage properties exclusive to undirected graphs or involve inefficient simulation, leaving room for further optimization. To address these limitations for directed graphs, we propose two novel approximation algorithms for estimating Kemeny's constant on directed graphs with theoretical error guarantees. Extensive numerical experiments on real-world networks validate the superiority of our algorithms over baseline methods in terms of efficiency and accuracy.|图上随机游动的 Kemeny 常数定义为根据平稳分布随机选择的从一个节点到另一个节点的平均到达时间。它已经发现了许多应用，并引起了相当大的研究兴趣。然而，精确计算 Kemeny 常数需要矩阵求逆，这对于有数百万个节点的大型网络来说是很困难的。现有的近似算法要么利用无向图的特性，要么涉及低效的仿真，为进一步的优化留下了空间。针对有向图的这些局限性，我们提出了两种新的近似算法来估计有向图上的 Kemeny 常数，并给出了理论误差保证。在实际网络上的大量数值实验验证了我们的算法在效率和准确性方面优于基线方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+Computation+of+Kemeny's+Constant+for+Directed+Graphs)|0|
|[FLea: Addressing Data Scarcity and Label Skew in Federated Learning via Privacy-preserving Feature Augmentation](https://doi.org/10.1145/3637528.3671899)|Tong Xia, Abhirup Ghosh, Xinchi Qiu, Cecilia Mascolo|University of Cambridge, Cambridge, United Kingdom; University of Birmingham & University of Cambridge, Birmingham, United Kingdom|Federated Learning (FL) enables model development by leveraging data distributed across numerous edge devices without transferring local data to a central server. However, existing FL methods still face challenges when dealing with scarce and label-skewed data across devices, resulting in local model overfitting and drift, consequently hindering the performance of the global model. In response to these challenges, we propose a pioneering framework called FLea, incorporating the following key components: i) A global feature buffer that stores activation-target pairs shared from multiple clients to support local training. This design mitigates local model drift caused by the absence of certain classes; ii) A feature augmentation approach based on local and global activation mix-ups for local training. This strategy enlarges the training samples, thereby reducing the risk of local overfitting; iii) An obfuscation method to minimize the correlation between intermediate activations and the source data, enhancing the privacy of shared features. To verify the superiority of FLea, we conduct extensive experiments using a wide range of data modalities, simulating different levels of local data scarcity and label skew. The results demonstrate that FLea consistently outperforms state-of-the-art FL counterparts (among 13 of the experimented 18 settings, the improvement is over 5%) while concurrently mitigating the privacy vulnerabilities associated with shared features. Code is available at https://github.com/XTxiatong/FLea.git|联邦学习(Federated Learning，FL)通过利用分布在众多边缘设备上的数据实现模型开发，而无需将本地数据传输到中央服务器。然而，现有的 FL 方法在跨设备处理稀缺和标签倾斜的数据时仍然面临挑战，导致局部模型过拟合和漂移，从而阻碍了全局模型的性能。为了应对这些挑战，我们提出了一个名为 FLea 的开创性框架，其中包含以下关键组件: i)一个全局功能缓冲区，存储多个客户共享的激活-目标对，以支持本地培训。该设计缓解了由于缺少特定类而引起的局部模型漂移问题; ii)一种基于局部和全局激活混合的局部训练特征增强方法。该策略扩大了训练样本，从而降低了局部过拟合的风险; 三)模糊化方法，最小化中间激活和源数据之间的相关性，增强了共享特征的隐私性。为了验证 FLea 的优越性，我们使用广泛的数据模式进行了广泛的实验，模拟了不同程度的局部数据稀缺性和标签倾斜。结果表明，FLea 始终优于最先进的 FL 同类产品(在试验的18种设置中，13种设置的改进超过5%) ，同时缓解了与共享功能相关的隐私漏洞。密码可于 https://github.com/xtxiatong/flea.git 索取|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FLea:+Addressing+Data+Scarcity+and+Label+Skew+in+Federated+Learning+via+Privacy-preserving+Feature+Augmentation)|0|
|[Motif-Consistent Counterfactuals with Adversarial Refinement for Graph-level Anomaly Detection](https://doi.org/10.1145/3637528.3672050)|Chunjing Xiao, Shikang Pang, Wenxin Tai, Yanlong Huang, Goce Trajcevski, Fan Zhou|Iowa State University, Ames, IA, USA; University of Electronic Science and Technology of China, Chengdu, China; Henan University, Kaifeng, China|Graph-level anomaly detection is significant in diverse domains. To improve detection performance, counterfactual graphs have been exploited to benefit the generalization capacity by learning causal relations. Most existing studies directly introduce perturbations (e.g., flipping edges) to generate counterfactual graphs, which are prone to alter the semantics of generated examples and make them off the data manifold, resulting in sub-optimal performance. To address these issues, we propose a novel approach, Motif-consistent Counterfactuals with Adversarial Refinement (MotifCAR), for graph-level anomaly detection. The model combines the motif of one graph, the core subgraph containing the identification (category) information, and the contextual subgraph (non-motif) of another graph to produce a raw counterfactual graph. However, the produced raw graph might be distorted and cannot satisfy the important counterfactual properties: Realism, Validity, Proximity and Sparsity. Towards that, we present a Generative Adversarial Network (GAN)-based graph optimizer to refine the raw counterfactual graphs. It adopts the discriminator to guide the generator to generate graphs close to realistic data, i.e., meet the property Realism. Further, we design the motif consistency to force the motif of the generated graphs to be consistent with the realistic graphs, meeting the property Validity. Also, we devise the contextual loss and connection loss to control the contextual subgraph and the newly added links to meet the properties Proximity and Sparsity. As a result, the model can generate high-quality counterfactual graphs. Experiments demonstrate the superiority of MotifCAR.|图形级别的异常检测在不同的领域有着重要的意义。为了提高检测性能，通过学习因果关系，利用反事实图来提高泛化能力。大多数现有的研究直接引入扰动(例如，翻转边缘)来生成反事实图，这容易改变生成的例子的语义，使它们脱离数据流形，导致次优性能。为了解决这些问题，我们提出了一种新的方法——基于对抗精炼的主题一致反事实异常检测(motifCAR)。该模型将一个图的主题、包含识别(类别)信息的核心子图和另一个图的上下文子图(非主题)结合起来，生成一个原始的反事实图。然而，生成的原始图可能会失真，不能满足重要的反事实性质: 现实性、有效性、接近性和稀疏性。为此，我们提出了一个基于生成对抗网络(GAN)的图优化器来细化原始的反事实图。它采用鉴别器引导生成器生成接近真实数据的图形，即满足真实感的性质。进一步，我们设计了基元一致性来强制生成的图的基元与现实图的基元一致，满足性质的有效性。此外，我们设计了上下文丢失和连接丢失来控制上下文子图和新增加的链接，以满足接近性和稀疏性的性质。因此，该模型可以生成高质量的反事实图。实验证明了 MotifCAR 的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Motif-Consistent+Counterfactuals+with+Adversarial+Refinement+for+Graph-level+Anomaly+Detection)|0|
|[ReFound: Crafting a Foundation Model for Urban Region Understanding upon Language and Visual Foundations](https://doi.org/10.1145/3637528.3671992)|Congxi Xiao, Jingbo Zhou, Yixiong Xiao, Jizhou Huang, Hui Xiong|; Baidu Inc., Beijing, China; Business Intelligence Lab, Baidu Research, Beijing, China|Understanding urban regional characteristics is pivotal in driving critical insights for urban planning and management. We have witnessed the successful application of pre-trained Foundation Models (FMs) in generating universal representations for various downstream tasks. However, applying this principle to the geospatial domain remains challenging, primarily due to the difficulty of gathering extensive data for developing a dedicated urban foundation model. Though there have been some attempts to empower the existing FMs with urban data, most of them focus on single-modality FMs without considering the multi-modality nature of urban region understanding tasks. To address this gap, we introduce ReFound - a novel framework for Re-training a Foundation model for urban region understanding, harnessing the strengths of both language and visual FMs. In this framework, we first invent a Mixture-of-Geospatial-Expert (MoGE) Transformer, to effectively integrate the embedding of multi-source geospatial data. Building on this, ReFound is enhanced by jointly distilling knowledge from language, visual, and visual-language FMs respectively, thus augmenting its generalization capabilities. Meanwhile, we design a masked geospatial data modeling approach alongside a cross-modal spatial alignment mechanism, to enhance the spatial knowledge of ReFound derived from geospatial data. Extensive experiments conducted on six real-world datasets over three urban region understanding tasks demonstrate the superior performance of our framework.|理解城市区域特征对于推动城市规划和管理的关键见解至关重要。我们已经见证了预先训练的基础模型(FM)在为各种下游任务生成通用表示方面的成功应用。然而，将这一原则应用于地理空间领域仍然具有挑战性，主要是因为难以收集广泛的数据来开发一个专门的城市地基模型。虽然已经有一些尝试将城市数据赋予现有的建筑模型，但大多数侧重于单一模式的建筑模型，而没有考虑到城市区域理解任务的多模式性质。为了解决这一差距，我们引入了 ReFound-一个新的框架，用于再培训城市地区理解的基础模型，同时利用语言和视觉模型的优势。在这个框架中，我们首先发明了一个混合地理空间专家(MoGE)转换器，以有效地整合多源地理空间数据的嵌入。在此基础上，通过分别从语言、可视和可视语言调用表中联合提取知识，增强了 ReFound 的泛化能力。同时，我们设计了一个隐蔽的地理空间数据建模方法和一个跨模态的空间对齐机制，以增强从地理空间数据中获取的 ReFind 的空间知识。在三个城市区域理解任务的六个真实世界数据集上进行的大量实验证明了我们的框架的优越性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ReFound:+Crafting+a+Foundation+Model+for+Urban+Region+Understanding+upon+Language+and+Visual+Foundations)|0|
|[How to Avoid Jumping to Conclusions: Measuring the Robustness of Outstanding Facts in Knowledge Graphs](https://doi.org/10.1145/3637528.3671763)|Hanhua Xiao, Yuchen Li, Yanhao Wang, Panagiotis Karras, Kyriakos Mouratidis, Natalia Rozalia Avlona|Singapore Management University, Singapore, Singapore; University of Copenhagen, Copenhagen, Denmark; East China Normal University, Shanghai, China|An outstanding fact (OF) is a striking claim by which some entities stand out from their peers on some attribute. OFs serve data journalism, fact checking, and recommendation. However, one could jump to conclusions by selecting truthful OFs while intentionally or inadvertently ignoring lateral contexts and data that render them less striking. This jumping conclusion bias from unstable OFs may disorient the public, including voters and consumers, raising concerns about fairness and transparency in political and business competition. It is thus ethically imperative for several stakeholders to measure the robustness of OFs with respect to lateral contexts and data. Unfortunately, a capacity for such inspection of OFs mined from knowledge graphs (KGs) is missing. In this paper, we propose a methodology that inspects the robustness of OFs in KGs by perturbation analysis. We define (1) entity perturbation, which detects outlying contexts by perturbing context entities in the OF; and (2) data perturbation, which considers plausible data that render an OF less striking. We compute the expected strikingness scores of OFs over perturbation relevance distributions and assess an OF as robust if its measured strikingness does not deviate significantly from the expected. We devise a suite of exact and sampling algorithms for perturbation analysis on large KGs. Extensive experiments reveal that our methodology accurately and efficiently detects frail OFs generated by existing mining approaches on KGs. We also show the effectiveness of our approaches through case and user studies.|一个突出的事实(OF)是一个引人注目的主张，其中一些实体脱颖而出，从他们的同行在某些属性。开放式数据库服务于数据新闻、事实核查和推荐。然而，人们可以通过选择真实的 OFs，而有意或无意地忽略使它们不那么引人注目的横向背景和数据，从而直接得出结论。来自不稳定开放式基金的这种跳跃性结论偏见，可能会让包括选民和消费者在内的公众感到迷惑，从而引发人们对政治和商业竞争中的公平性和透明度的担忧。因此，从伦理上讲，若干利益攸关方必须衡量开放式框架在横向背景和数据方面的稳健性。不幸的是，从知识图(KGs)中挖掘的 OFs 缺乏这种检查能力。在本文中，我们提出了一种方法，检查的稳健性开关在 KG 的摄动分析。我们定义了(1)实体扰动，它通过扰动 OF 中的上下文实体来检测外围上下文; (2)数据扰动，它考虑使 OF 不那么引人注目的似是而非的数据。我们计算了扰动相关分布上的运算符的预期显著性分数，并且如果它的测量显著性没有显著偏离预期，那么它就是稳健的。我们设计了一套精确的采样算法用于大型 KG 的摄动分析。广泛的实验表明，我们的方法准确和有效地检测脆弱的开放的现有采矿方法产生的幼稚园。我们还通过案例和用户研究展示了我们的方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+to+Avoid+Jumping+to+Conclusions:+Measuring+the+Robustness+of+Outstanding+Facts+in+Knowledge+Graphs)|0|
|[Temporal Prototype-Aware Learning for Active Voltage Control on Power Distribution Networks](https://doi.org/10.1145/3637528.3671790)|Feiyang Xu, Shunyu Liu, Yunpeng Qing, Yihe Zhou, Yuwen Wang, Mingli Song||Active Voltage Control (AVC) on the Power Distribution Networks (PDNs) aims to stabilize the voltage levels to ensure efficient and reliable operation of power systems. With the increasing integration of distributed energy resources, recent efforts have explored employing multi-agent reinforcement learning (MARL) techniques to realize effective AVC. Existing methods mainly focus on the acquisition of short-term AVC strategies, i.e., only learning AVC within the short-term training trajectories of a singular diurnal cycle. However, due to the dynamic nature of load demands and renewable energy, the operation states of real-world PDNs may exhibit significant distribution shifts across varying timescales (e.g., daily and seasonal changes). This can render those short-term strategies suboptimal or even obsolete when performing continuous AVC over extended periods. In this paper, we propose a novel temporal prototype-aware learning method, abbreviated as TPA, to learn time-adaptive AVC under short-term training trajectories. At the heart of TPA are two complementary components, namely multi-scale dynamic encoder and temporal prototype-aware policy, that can be readily incorporated into various MARL methods. The former component integrates a stacked transformer network to learn underlying temporal dependencies at different timescales of the PDNs, while the latter implements a learnable prototype matching mechanism to construct a dedicated AVC policy that can dynamically adapt to the evolving operation states. Experimental results on the AVC benchmark with different PDN sizes demonstrate that the proposed TPA surpasses the state-of-the-art counterparts not only in terms of control performance but also by offering model transferability. Our code is available at https://github.com/Canyizl/TPA-for-AVC.|配电网有源电压控制(AVC)的目的是稳定电压水平，以确保电力系统的高效和可靠运行。随着分布式能源资源的日益整合，最近人们开始探索使用多代理强化学习技术来实现有效的 AVC。现有的方法主要集中在短期 AVC 策略的获取上，即只学习周期为单日周期的短期 AVC 训练轨迹。然而，由于负荷需求和可再生能源的动态特性，实际 PDN 的运行状态可能会在不同的时间尺度上表现出显著的分布变化(例如，日变化和季节变化)。这可以使这些短期策略次优，甚至过时时，执行连续 AVC 超过延长的时间。本文提出了一种新的时间原型感知学习方法，简称 TPA，以学习短期训练轨迹下的时间自适应 AVC。TPA 的核心是两个互补的组成部分，即多尺度动态编码器和时间原型感知策略，可以很容易地并入各种 MARL 方法。前者集成了一个叠加的变压器网络来学习 PDN 在不同时间尺度上的时间依赖关系，而后者实现了一个可学习的原型匹配机制来构造一个专用的 AVC 策略，该策略能够动态地适应不断变化的运行状态。在不同 PDN 尺寸的 AVC 基准上的实验结果表明，所提出的 TPA 不仅在控制性能方面优于最先进的 AVC 基准，而且提供了模型的可转移性。我们的代码可以在 https://github.com/canyizl/tpa-for-avc 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Temporal+Prototype-Aware+Learning+for+Active+Voltage+Control+on+Power+Distribution+Networks)|0|
|[FlexCare: Leveraging Cross-Task Synergy for Flexible Multimodal Healthcare Prediction](https://doi.org/10.1145/3637528.3671974)|Muhao Xu, Zhenfeng Zhu, Youru Li, Shuai Zheng, Yawei Zhao, Kunlun He, Yao Zhao|; Medical Big Data Research Center, Chinese PLA General Hospital, Beijing, China|Multimodal electronic health record (EHR) data can offer a holistic assessment of a patient's health status, supporting various predictive healthcare tasks. Recently, several studies have embraced the multitask learning approach in the healthcare domain, exploiting the inherent correlations among clinical tasks to predict multiple outcomes simultaneously. However, existing methods necessitate samples to possess complete labels for all tasks, which places heavy demands on the data and restricts the flexibility of the model. Meanwhile, within a multitask framework with multimodal inputs, how to comprehensively consider the information disparity among modalities and among tasks still remains a challenging problem. To tackle these issues, a unified healthcare prediction model, also named by FlexCare, is proposed to flexibly accommodate incomplete multimodal inputs, promoting the adaption to multiple healthcare tasks. The proposed model breaks the conventional paradigm of parallel multitask prediction by decomposing it into a series of asynchronous single-task prediction. Specifically, a task-agnostic multimodal information extraction module is presented to capture decorrelated representations of diverse intra- and inter-modality patterns. Taking full account of the information disparities between different modalities and different tasks, we present a task-guided hierarchical multimodal fusion module that integrates the refined modality-level representations into an individual patient-level representation. Experimental results on multiple tasks from MIMIC-IV/MIMIC-CXR/MIMIC-NOTE datasets demonstrate the effectiveness of the proposed method. Additionally, further analysis underscores the feasibility and potential of employing such a multitask strategy in the healthcare domain. The source code is available at https://github.com/mhxu1998/FlexCare **REMOVE 2nd URL**://github.com/mhxu1998/FlexCare.|多模式电子健康记录(EHR)数据可以提供对患者健康状况的全面评估，支持各种预测性医疗任务。最近，一些研究已经在医疗保健领域采用了多任务学习方法，利用临床任务之间固有的相关性来同时预测多个结果。然而，现有的方法要求样本对所有任务都具有完整的标签，这对数据的要求很高，限制了模型的灵活性。同时，在具有多模式输入的多任务框架内，如何全面考虑模式间和任务间的信息差异仍然是一个具有挑战性的问题。为了解决这些问题，提出了一个统一的医疗保健预测模型，也被称为 FlexCare，以灵活地适应不完整的多模式输入，促进适应多种医疗保健任务。该模型打破了传统的并行多任务预测模型，将其分解为一系列异步单任务预测模型。具体来说，提出了一个任务无关的多模态信息抽取模块，以捕获不同的内部和跨模态模式的去相关表示。充分考虑到不同模式和不同任务之间的信息差异，我们提出了一个任务指导的分层多模式融合模块，该模块将精细的模式级表示集成到个体患者级表示中。对 MIMIC-IV/MIMIC-CXR/MIMIC-NOTE 数据集的多任务实验结果表明了该方法的有效性。此外，进一步的分析强调了在医疗领域采用这种多任务策略的可行性和潜力。源代码可以在 https://github.com/mhxu1998/flexcare  * * 删除第二个网址 * * :// github.com/mhxu1998/flexcare。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FlexCare:+Leveraging+Cross-Task+Synergy+for+Flexible+Multimodal+Healthcare+Prediction)|0|
|[PeFAD: A Parameter-Efficient Federated Framework for Time Series Anomaly Detection](https://doi.org/10.1145/3637528.3671753)|Ronghui Xu, Hao Miao, Senzhang Wang, Philip S. Yu, Jianxin Wang|Central South University, Changsha, China; Aalborg University, Aalborg, Denmark; University of Illinois at Chicago, Chicago, USA|With the proliferation of mobile sensing techniques, huge amounts of timeseries data are generated and accumulated in various domains, fueling plenty ofreal-world applications. In this setting, time series anomaly detection ispractically important. It endeavors to identify deviant samples from the normalsample distribution in time series. Existing approaches generally assume thatall the time series is available at a central location. However, we arewitnessing the decentralized collection of time series due to the deployment ofvarious edge devices. To bridge the gap between the decentralized time seriesdata and the centralized anomaly detection algorithms, we propose aParameter-efficient Federated Anomaly Detection framework named PeFAD with theincreasing privacy concerns. PeFAD for the first time employs the pre-trainedlanguage model (PLM) as the body of the client's local model, which can benefitfrom its cross-modality knowledge transfer capability. To reduce thecommunication overhead and local model adaptation cost, we propose aparameter-efficient federated training module such that clients only need tofine-tune small-scale parameters and transmit them to the server for update.PeFAD utilizes a novel anomaly-driven mask selection strategy to mitigate theimpact of neglected anomalies during training. A knowledge distillationoperation on a synthetic privacy-preserving dataset that is shared by all theclients is also proposed to address the data heterogeneity issue acrossclients. We conduct extensive evaluations on four real datasets, where PeFADoutperforms existing state-of-the-art baselines by up to 28.74%.|随着移动传感技术的迅速发展，大量的时间序列数据在各个领域中产生和积累，推动了大量的现实应用。在这种情况下，时间序列异常检测实际上非常重要。该方法从时间序列的正态样本分布中识别偏差样本。现有的方法通常假定所有的时间序列都可以在一个中心位置获得。然而，由于各种边缘设备的部署，我们正在目睹时间序列的分散收集。为了弥补分散式时间序列数据和集中式异常检测算法之间的差距，我们提出了一个参数高效的联邦异常检测框架，命名为 PeFAD，并考虑到隐私问题。PeFAD 首次采用预训练语言模型(PLM)作为客户端本地模型的主体，利用其跨模态知识转移能力。为了降低通信开销和本地模型自适应成本，我们提出了参数有效的联邦训练模块，客户端只需微调小规模的参数并将其传输到服务器进行更新。 PeFAD 采用了一种新的异常驱动的掩码选择策略，以减轻训练过程中被忽视的异常的影响。针对跨客户端的数据异构问题，提出了一种基于全客户端共享的综合隐私保护数据集的知识提取方法。我们对四个实际数据集进行了广泛的评估，其中 PEFAD 的性能比现有的最先进的基线高出28.74% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PeFAD:+A+Parameter-Efficient+Federated+Framework+for+Time+Series+Anomaly+Detection)|0|
|[ProtoMix: Augmenting Health Status Representation Learning via Prototype-based Mixup](https://doi.org/10.1145/3637528.3671937)|Yongxin Xu, Xinke Jiang, Xu Chu, Yuzhen Xiao, Chaohe Zhang, Hongxin Ding, Junfeng Zhao, Yasha Wang, Bing Xie||With the widespread adoption of electronic health records (EHR) data, deep learning techniques have been broadly utilized for various health prediction tasks. Nevertheless, the labeled data scarcity issue restricts the prediction power of these deep models. To enhance the generalization capability of deep learning models when faced with such situations, a common trend is to train generative adversarial networks (GANs) or diffusion models for data augmentation. However, due to limitations in sample size and potential label imbalance issues, these methods are prone to mode collapse problems. This results in the generation of new samples that fail to preserve the subtype structure within EHR data, thereby limiting their practicality in health prediction tasks that generally require detailed patient phenotyping. Aiming at the above problems, we propose a Prototype-based Mixup method, dubbed ProtoMix, which combines prior knowledge of intrinsic data features from subtype centroids (i.e., prototypes) to guide the synthesis of new samples. Specifically, ProtoMix employs a prototype-guided mixup training task to shift the decision boundary away from the subtypes. Then, ProtoMix optimizes the sampling weights in different areas of the data manifold via a prototype-guided mixup sampling strategy. Throughout the training process, ProtoMix dynamically expands the training distribution using an adaptive mixing coefficient computation method. Experimental evaluations on three real-world datasets demonstrate the efficacy of ProtoMix.|随着电子健康记录(EHR)数据的广泛应用，深度学习技术被广泛应用于各种健康预测任务。然而，标记数据稀缺性问题限制了这些深度模型的预测能力。为了提高深度学习模型在这种情况下的泛化能力，一个共同的趋势是训练生成对抗网络(GAN)或扩散模型的数据增强。然而，由于样本量的限制和潜在的标签不平衡问题，这些方法很容易出现模式崩溃问题。这导致新样本的产生，不能保留 EHR 数据中的亚型结构，从而限制了它们在通常需要详细的患者表型的健康预测任务中的实用性。针对上述问题，本文提出了一种基于原型的混合方法，称之为 ProtoMix，该方法结合子类质心(即原型)内在数据特征的先验知识来指导新样本的合成。具体来说，ProtoMix 采用了一个原型引导的混合训练任务，将决策边界从子类型中转移出来。然后，ProtoMix 通过原型引导的混合采样策略对数据流形中不同区域的采样权重进行优化。在整个训练过程中，ProtoMix 采用自适应混合系数计算方法动态扩展训练分布。通过对三个实际数据集的实验评估，验证了 ProtoMix 的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ProtoMix:+Augmenting+Health+Status+Representation+Learning+via+Prototype-based+Mixup)|0|
|[FedRoLA: Robust Federated Learning Against Model Poisoning via Layer-based Aggregation](https://doi.org/10.1145/3637528.3671906)|Gang Yan, Hao Wang, Xu Yuan, Jian Li|University of Delaware, Newark, DE, USA; Stony Brook University, Stony Brook, NY, USA; Stevens Institute of Technology, Hoboken, NJ, USA; Binghamton University, Binghamton, NY, USA|Federated Learning (FL) is increasingly vulnerable to model poisoning attacks, where malicious clients degrade the global model's accuracy with manipulated updates. Unfortunately, most existing defenses struggle to handle the scenarios when multiple adversaries exist, and often rely on historical or validation data, rendering them ill-suited for the dynamic and diverse nature of real-world FL environments. Exacerbating these limitations is the fact that most existing defenses also fail to account for the distinctive contributions of Deep Neural Network (DNN) layers in detecting malicious activity, leading to the unnecessary rejection of benign updates. To bridge these gaps, we introduce FedRoLa, a cutting-edge similarity-based defense method optimized for FL. Specifically, FedRoLa leverages global model parameters and client updates independently, moving away from reliance on historical or validation data. It features a unique layer-based aggregation with dynamic layer selection, enhancing threat detection, and includes a dynamic probability method for balanced security and model performance. Through comprehensive evaluations using different DNN models and real-world datasets, FedRoLa demonstrates substantial improvements over the status quo approaches in global model accuracy, achieving up to 4% enhancement in terms of accuracy, reducing false positives to 6.4%, and securing an 92.8% true positive rate.|联邦学习(Federated Learning，FL)越来越容易受到模型中毒攻击的影响，在这种攻击中，恶意客户端通过操纵更新降低全局模型的准确性。不幸的是，当存在多个对手时，大多数现有的防御系统难以处理场景，并且常常依赖于历史数据或验证数据，这使得它们不适合真实世界 FL 环境的动态和多样性。加剧这些局限性的事实是，大多数现有的防御也未能解释深度神经网络(DNN)层在检测恶意活动方面的独特贡献，导致对良性更新的不必要拒绝。为了弥补这些差距，我们引入了 FedRoLa，一种针对 FL 优化的基于前沿相似性的防御方法。具体来说，FedRoLa 独立利用全局模型参数和客户端更新，摆脱了对历史数据或验证数据的依赖。它具有独特的基于层次的聚合特性，采用动态层次选择，增强了威胁检测能力，并提出了一种平衡安全性和模型性能的动态概率方法。通过使用不同的 DNN 模型和真实世界数据集的综合评估，FedRoLa 显示了全球模型准确性方面的现状方法的实质性改进，在准确性方面提高了4% ，将假阳性降低到6.4% ，并确保了92.8% 的真阳性率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedRoLA:+Robust+Federated+Learning+Against+Model+Poisoning+via+Layer-based+Aggregation)|0|
|[Team up GBDTs and DNNs: Advancing Efficient and Effective Tabular Prediction with Tree-hybrid MLPs](https://doi.org/10.1145/3637528.3671964)|Jiahuan Yan, Jintai Chen, Qianxing Wang, Danny Z. Chen, Jian Wu|University of Illinois at Urbana-Champaign, Urbana, IL, USA; University of Notre Dame, Notre Dame, IN, USA; Zhejiang University, Hangzhou, China|Tabular datasets play a crucial role in various applications. Thus, developing efficient, effective, and widely compatible prediction algorithms for tabular data is important. Currently, two prominent model types, Gradient Boosted Decision Trees (GBDTs) and Deep Neural Networks (DNNs), have demonstrated performance advantages on distinct tabular prediction tasks. However, selecting an effective model for a specific tabular dataset is challenging, often demanding time-consuming hyperparameter tuning. To address this model selection dilemma, this paper proposes a new framework that amalgamates the advantages of both GBDTs and DNNs, resulting in a DNN algorithm that is as efficient as GBDTs and is competitively effective regardless of dataset preferences for GBDTs or DNNs. Our idea is rooted in an observation that deep learning (DL) offers a larger parameter space that can represent a well-performing GBDT model, yet the current back-propagation optimizer struggles to efficiently discover such optimal functionality. On the other hand, during GBDT development, hard tree pruning, entropy-driven feature gate, and model ensemble have proved to be more adaptable to tabular data. By combining these key components, we present a Tree-hybrid simple MLP (T-MLP). In our framework, a tensorized, rapidly trained GBDT feature gate, a DNN architecture pruning approach, as well as a vanilla back-propagation optimizer collaboratively train a randomly initialized MLP model. Comprehensive experiments show that T-MLP is competitive with extensively tuned DNNs and GBDTs in their dominating tabular benchmarks (88 datasets) respectively, all achieved with compact model storage and significantly reduced training duration. The codes and full experiment results are available at https://github.com/jyansir/tmlp.|表格数据集在各种应用程序中起着至关重要的作用。因此，开发高效、有效和广泛兼容的表格数据预测算法非常重要。目前，两种突出的模型类型，梯度增强决策树(GBDTs)和深度神经网络(DNN) ，已经证明了在不同的表格预测任务的性能优势。然而，为特定的表格数据集选择有效的模型是具有挑战性的，通常需要耗费时间的超参数调优。为了解决这个模型选择困境，本文提出了一个新的框架，融合了 GBDTs 和 DNN 的优势，产生了一个 DNN 算法，它与 GBDTs 一样有效，并且无论 GBDTs 或 DNN 的数据集偏好如何，都具有竞争性。我们的想法植根于一个观察，即深度学习(DL)提供了一个更大的参数空间，可以表示一个性能良好的 GBDT 模型，然而当前的反向传播优化器努力有效地发现这样的最佳功能。另一方面，在 GBDT 开发过程中，硬树剪枝、熵驱动特征门和模型集成等技术对表格数据的适应性更强。通过组合这些关键部分，我们提出了一个树杂交简单 MLP (T-MLP)。在我们的框架中，一个张量化的，快速训练的 GBDT 特征门，一个 DNN 体系结构修剪方法，以及一个普通的反向传播优化器协作训练一个随机初始化的 MLP 模型。综合实验表明，T-MLP 与广泛调整的 DNN 和 GBDTs 分别在其主要的表格基准(88个数据集)上具有竞争力，所有这些都是通过紧凑的模型存储和显著减少训练持续时间来实现的。代码和完整的实验结果可在 https://github.com/jyansir/tmlp 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Team+up+GBDTs+and+DNNs:+Advancing+Efficient+and+Effective+Tabular+Prediction+with+Tree-hybrid+MLPs)|0|
|[Efficient Mixture of Experts based on Large Language Models for Low-Resource Data Preprocessing](https://doi.org/10.1145/3637528.3671873)|Mengyi Yan, Yaoshu Wang, Kehan Pang, Min Xie, Jianxin Li|Beihang University, Beijing, China; Shenzhen Institute of Computing Sciences, Shenzhen, China|Data preprocessing (DP) that transforms erroneous and raw data to a clean version is a cornerstone of the data mining pipeline. Due to the diverse requirements of downstream tasks, data scientists and domain experts have to handcraft domain-specific rules or train ML models with annotated examples, which is costly/time-consuming. In this paper, we present MELD (Mixture of Experts on Large Language Models for Data Preprocessing), a universal solver for low-resource DP. MELD adopts a Mixture-of-Experts (MoE) architecture that enables the amalgamation and enhancement of domain-specific experts trained on limited annotated examples. To fine-tune MELD, we develop a suite of expert-tuning and MoE-tuning techniques, including a retrieval augmented generation (RAG) system, meta-path search for data augmentation, expert refinement and router network training based on information bottleneck. To further verify the effectiveness of MELD, we theoretically prove that MoE in MELD is superior than a single expert and the router network is able to dispatch data to the right experts. Finally, we conducted extensive experiments on 19 datasets over 10 DP tasks to show that MELD outperforms the state-of-the-art methods in both effectiveness and efficiency. More importantly, MELD is able to be fine-tuned in a low-resource environment, e.g. a local, single and low-priced 3090 GPU.|数据预处理(DP)将错误和原始数据转换为干净的版本，这是数据挖掘流水线的基石。由于下游任务的不同需求，数据科学家和领域专家不得不手工制定特定领域的规则或用注释示例训练机器学习模型，这是昂贵的/耗时的。在本文中，我们提出了 MELD (大型语言模型数据预处理混合专家) ，一个低资源 DP 的通用解决方案。MELD 采用了一种专家混合体系结构，这种体系结构能够合并和增强受过有限注释示例培训的特定领域专家。为了对 MELD 进行微调，我们开发了一套专家调优和 MoE 调优技术，包括基于信息瓶颈的检索增强生成(RAG)系统、用于数据增强的元路径搜索、专家调优和路由器网络训练。为了进一步验证 MELD 的有效性，我们从理论上证明了 MELD 中的 MoE 优于单个专家，并且路由器网络能够向合适的专家发送数据。最后，我们在10个 DP 任务的19个数据集上进行了广泛的实验，结果表明 MELD 在有效性和效率方面都优于最先进的方法。更重要的是，MELD 能够在低资源环境中进行微调，例如本地、单一和低价格的3090 GPU。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Mixture+of+Experts+based+on+Large+Language+Models+for+Low-Resource+Data+Preprocessing)|0|
|[An Efficient Subgraph GNN with Provable Substructure Counting Power](https://doi.org/10.1145/3637528.3671731)|Zuoyu Yan, Junru Zhou, Liangcai Gao, Zhi Tang, Muhan Zhang|Institute for Artificial Intelligence, Peking University, Beijing, China; Wangxuan Institute of Computer Technology, Peking University, Beijing, China; Beijing Institute for General Artificial Intelligence, Peking University, Beijing, China|We investigate the enhancement of graph neural networks' (GNNs)representation power through their ability in substructure counting. Recentadvances have seen the adoption of subgraph GNNs, which partition an inputgraph into numerous subgraphs, subsequently applying GNNs to each to augmentthe graph's overall representation. Despite their ability to identify varioussubstructures, subgraph GNNs are hindered by significant computational andmemory costs. In this paper, we tackle a critical question: Is it possible forGNNs to count substructures both efficiently and provably?Our approach begins with a theoretical demonstration that the distance torooted nodes in subgraphs is key to boosting the counting power of subgraphGNNs. To avoid the need for repetitively applying GNN across all subgraphs, weintroduce precomputed structural embeddings that encapsulate this crucialdistance information. Experiments validate that our proposed model retains thecounting power of subgraph GNNs while achieving significantly fasterperformance.|我们研究了通过图神经网络子结构计数的能力来增强其表示能力的问题。最近的进步已经看到了子图 GNN 的采用，它将输入图划分为许多子图，随后将 GNN 应用于每个子图以增强图的整体表示。尽管子图 GNN 具有识别各种子结构的能力，但是它们受到计算和存储成本的限制。在本文中，我们处理了一个关键问题: GNN 是否有可能同时有效和可证明地计算子结构？我们的方法首先从理论上证明了子图中的距离根节点是提高子图 GNN 计数能力的关键。为了避免在所有子图中重复应用 GNN，我们引入了预计算结构嵌入，它封装了这个关键的距离信息。实验验证了该模型在保持子图 GNN 计数能力的同时，显著提高了性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Efficient+Subgraph+GNN+with+Provable+Substructure+Counting+Power)|0|
|[Towards Test Time Adaptation via Calibrated Entropy Minimization](https://doi.org/10.1145/3637528.3671672)|Hao Yang, Min Wang, Jinshen Jiang, Yun Zhou|National University of Defense Technology, ChangSha, China|Robust models must demonstrate strong generalizability, even amid environmental changes. However, the complex variability and noise in real-world data often lead to a pronounced performance gap between the training and testing phases. Researchers have recently introduced test-time-domain adaptation (TTA) to address this challenge. TTA methods primarily adapt source-pretrained models to a target domain using only unlabeled test data. This study found that existing TTA methods consider only the largest logit as a pseudo-label and aim to minimize the entropy of test time predictions. This maximizes the predictive confidence of the model. However, this corresponds to the model being overconfident in the local test scenarios. In response, we introduce a novel confidence-calibration loss function called Calibrated Entropy Test-Time Adaptation (CETA), which considers the model's largest logit and the next-highest-ranked one, aiming to strike a balance between overconfidence and underconfidence. This was achieved by incorporating a sample-wise regularization term. We also provide a theoretical foundation for the proposed loss function. Experimentally, our method outperformed existing strategies on benchmark corruption datasets across multiple models, underscoring the efficacy of our approach.|即使在环境变化的情况下，健壮的模型也必须表现出强大的概括性。然而，真实世界数据中复杂的可变性和噪声常常导致训练和测试阶段之间的性能差距。研究人员最近引入了测试时域适应(TTA)来应对这一挑战。TTA 方法主要使用未标记的测试数据将源预先训练的模型适应目标域。本研究发现现有的 TTA 方法只考虑最大 logit 作为伪标签，目的是使测试时间预测的熵最小化。这使模型的预测置信度最大化。然而，这对应于模型在本地测试场景中过度自信。作为回应，我们引入了一种新的置信度校准损失函数，称为校准熵测试时间适应(CETA) ，它考虑了模型的最大 logit 和排名次高的 logit，旨在在过度自信和不自信之间取得平衡。这是通过合并一个样本明智的正则化术语来实现的。我们还为所提出的损失函数提供了理论基础。在实验上，我们的方法在多个模型的基准腐败数据集上优于现有的策略，突出了我们方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Test+Time+Adaptation+via+Calibrated+Entropy+Minimization)|0|
|[Noisy Label Removal for Partial Multi-Label Learning](https://doi.org/10.1145/3637528.3671677)|Fuchao Yang, Yuheng Jia, Hui Liu, Yongqiang Dong, Junhui Hou|College of Software Engineering, Southeast University, Nanjing, China; School of Computer Science and Engineering, Southeast University, Nanjing, China; Department of Computer Science, City University of Hong Kong, Hong Kong, China; School of Computing & Information Sciences, Saint Francis University, Hong Kong, China|This paper addresses the problem of partial multi-label learning (PML), a challenging weakly supervised learning framework, where each sample is associated with a candidate label set comprising both ground-true labels and noisy labels. We theoretically reveal that an increased number of noisy labels in the candidate label set leads to an enlarged generalization error bound, consequently degrading the classification performance. Accordingly, the key to solving PML lies in accurately removing the noisy labels within the candidate label set. To achieve this objective, we leverage prior knowledge about the noisy labels in PML, which suggests that they only exist within the candidate label set and possess binary values. Specifically, we propose a constrained regression model to learn a PML classifier and select the noisy labels. The constraints of the model strictly enforce the location and value of the noisy labels. Simultaneously, the supervision information provided by the candidate label set is unreliable due to the presence of noisy labels. In contrast, the non-candidate labels of a sample precisely indicate the classes to which the sample does not belong. To aid in the selection of noisy labels, we construct a competitive classifier based on the non-candidate labels. The PML classifier and the competitive classifier form a competitive relationship, encouraging mutual learning. We formulate the proposed model as a discrete optimization problem to effectively remove the noisy labels, and we solve it using an alternative algorithm. Extensive experiments conducted on 6 real-world partial multi-label data sets and 7 synthetic data sets, employing various evaluation metrics, demonstrate that our method significantly outperforms state-of-the-art PML methods. The code implementation is publicly available at https://github.com/Yangfc-ML/NLR.|本文讨论了部分多标签学习(pML)问题，这是一个具有挑战性的弱监督式学习框架，其中每个样本都与一个包含地面真实标签和噪声标签的候选标签集相关联。理论上，我们发现候选标签集中的噪声标签数目增加会导致泛化误差界扩大，从而降低分类性能。因此，解决 PML 的关键在于准确地去除候选标签集中的噪声标签。为了实现这一目标，我们利用了 PML 中关于噪声标签的先验知识，这表明它们只存在于候选标签集中，并且具有二进制值。具体来说，我们提出了一个约束回归模型来学习 PML 分类器和选择有噪声的标签。模型的约束条件严格限制了噪声标签的位置和值。同时，由于标签噪声的存在，候选标签集提供的监督信息是不可靠的。相反，示例的非候选标签精确地指示示例不属于的类。为了帮助噪声标签的选择，我们构造了一个基于非候选标签的竞争分类器。PML 分类器与竞争分类器形成竞争关系，促进相互学习。我们将建议的模型作为一个离散优化问题来制定，以有效地去除噪音标签，并使用另一种算法来解决这个问题。在6个现实世界的部分多标签数据集和7个综合数据集上进行了广泛的实验，采用了各种评估指标，表明我们的方法明显优于最先进的 PML 方法。代码实现可在 https://github.com/yangfc-ml/nlr 公开获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Noisy+Label+Removal+for+Partial+Multi-Label+Learning)|0|
|[Balanced Confidence Calibration for Graph Neural Networks](https://doi.org/10.1145/3637528.3671741)|Hao Yang, Min Wang, Qi Wang, Mingrui Lao, Yun Zhou|National University of Defense Technology, ChangSha, China|This paper delves into the confidence calibration in prediction when using Graph Neural Networks (GNNs), which has emerged as a notable challenge in the field. Despite their remarkable capabilities in processing graph-structured data, GNNs are prone to exhibit lower confidence in their predictions than what the actual accuracy warrants. Recent advances attempt to address this by minimizing prediction entropy to enhance confidence levels. However, this method inadvertently risks leading to over-confidence in model predictions. Our investigation in this work reveals that most existing GNN calibration methods predominantly focus on the highest logit, thereby neglecting the entire spectrum of prediction probabilities. To alleviate this limitation, we introduce a novel framework called Balanced Calibrated Graph Neural Network (BCGNN), specifically designed to establish a balanced calibration between over-confidence and under-confidence in GNNs' prediction. To theoretically support our proposed method, we further demonstrate the mechanism of the BCGNN framework in effective confidence calibration and significant trustworthiness improvement in prediction. We conduct extensive experiments to examine the developed framework. The empirical results show our method's superior performance in predictive confidence and trustworthiness, affirming its practical applicability and effectiveness in real-world scenarios.|本文研究了图形神经网络(GNN)在预测中的置信度校正问题。尽管 GNN 在处理图形结构化数据方面具有显著的能力，但它们在预测方面的可信度往往低于实际的准确度。最近的进展试图通过最小化预测熵来提高置信水平来解决这个问题。然而，这种方法无意中有可能导致对模型预测的过度自信。我们在这项工作中的研究表明，大多数现有的 GNN 校准方法主要集中在最高对数，从而忽略了整个谱的预测概率。为了缓解这一局限性，我们引入了一个新的框架，称为平衡校准图神经网络(BCGNN) ，专门设计来建立一个平衡校准之间的过度置信和不置信的 GNN 的预测。为了从理论上支持我们提出的方法，我们进一步论证了 BCGNN 框架在有效的置信度校正和显著的预测可信度改善方面的机制。我们进行了广泛的实验来检验开发的框架。实证结果表明，该方法具有较好的预测置信度和可信度，验证了该方法在实际场景中的实用性和有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Balanced+Confidence+Calibration+for+Graph+Neural+Networks)|0|
|[Efficient Decision Rule List Learning via Unified Sequence Submodular Optimization](https://doi.org/10.1145/3637528.3671827)|Linxiao Yang, Jingbang Yang, Liang Sun|DAMO Academy, Alibaba Group, Hangzhou, Zhejiang, China|Interpretable models are crucial in many high-stakes decision-making applications. In this paper, we focus on learning a decision rule list for binary and multi-class classification. Different from rule set learning problems, learning an optimal rule list involves not only learning a set of rules, but also their orders. In addition, many existing algorithms rely on rule pre-mining to handle large-scale high-dimensional data, which leads to suboptimal rule list model and degrades its generalization accuracy and interpretablity. In this paper, we learn a rule list from the sequence submodular perspective. We consider the rule list as a sequence and define the cover set for each rule. Then we formulate a sequence function which combines both model complexity and classification accuracy. Based on its appealing sequence submodular property, we propose a general distorted greedy insert algorithm under Minorization-Maximization (MM) framework, which gradually inserts rules with highest inserting gain to the rule list. The rule generation process is treated as a subproblem, allowing our method to learn the rule list through a unified framework which avoids rule pre-mining. We further provide a theoretical lower bound of our greedy insert algorithm in rule list learning. Experimental results show that our algorithm achieves better accuracy and interpretability than the state-of-the-art rule learning methods, and in particular it scales well on large-scale datasets, especially on high-dimensional data.|可解释的模型在许多高风险的决策应用中至关重要。本文主要研究二进制和多类分类决策规则表的学习问题。与规则集学习问题不同，学习最优规则表不仅需要学习一组规则，还需要学习它们的顺序。此外，现有的许多算法都依赖于规则预挖掘来处理大规模的高维数据，这导致了规则列表模型的次优化，降低了其泛化精度和可解释性。在本文中，我们从序列子模的角度学习了一个规则列表。我们将规则列表视为一个序列，并为每个规则定义覆盖集。然后构造了一个模型复杂度和分类精度相结合的序列函数。基于其吸引序列的子模特性，提出了一种通用的少数化-最大化(MM)框架下的扭曲贪婪插入算法，该算法在规则列表中逐步插入插入增益最大的规则。将规则生成过程作为一个子问题来处理，使得我们的方法能够通过一个统一的框架来学习规则列表，避免了规则的预挖掘。我们进一步给出了规则表学习中贪婪插入算法的理论下界。实验结果表明，该算法比现有的规则学习方法具有更好的准确性和可解释性，特别是在大规模数据集上，尤其是在高维数据上具有良好的扩展性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Decision+Rule+List+Learning+via+Unified+Sequence+Submodular+Optimization)|0|
|[Effective Clustering on Large Attributed Bipartite Graphs](https://doi.org/10.1145/3637528.3671764)|Renchi Yang, Yidu Wu, Xiaoyang Lin, Qichen Wang, Tsz Nam Chan, Jieming Shi|Hong Kong Baptist University, Hong Kong, China; Hong Kong Polytechnic University, Hong Kong, China; Chinese University of Hong Kong, Hong Kong, China; Shenzhen University, Shenzhen, China|Attributed bipartite graphs (ABGs) are an expressive data model fordescribing the interactions between two sets of heterogeneous nodes that areassociated with rich attributes, such as customer-product purchase networks andauthor-paper authorship graphs. Partitioning the target node set in such graphsinto k disjoint clusters (referred to as k-ABGC) finds widespread use invarious domains, including social network analysis, recommendation systems,information retrieval, and bioinformatics. However, the majority of existingsolutions towards k-ABGC either overlook attribute information or fail tocapture bipartite graph structures accurately, engendering severely compromisedresult quality. The severity of these issues is accentuated in real ABGs, whichoften encompass millions of nodes and a sheer volume of attribute data,rendering effective k-ABGC over such graphs highly challenging. In this paper, we propose TPO, an effective and efficient approach to k-ABGCthat achieves superb clustering performance on multiple real datasets. TPOobtains high clustering quality through two major contributions: (i) a novelformulation and transformation of the k-ABGC problem based on multi-scaleattribute affinity specialized for capturing attribute affinities between nodeswith the consideration of their multi-hop connections in ABGs, and (ii) ahighly efficient solver that includes a suite of carefully-craftedoptimizations for sidestepping explicit affinity matrix construction andfacilitating faster convergence. Extensive experiments, comparing TPO against19 baselines over 5 real ABGs, showcase the superior clustering quality of TPOmeasured against ground-truth labels. Moreover, compared to the state of thearts, TPO is often more than 40x faster over both small and large ABGs.|属性二部图(ABG)是描述两组异构节点之间相互作用的表达式数据模型，这两组异构节点具有丰富的属性，如客户-产品购买网络和作者-论文作者关系图。在这样的图表中，将目标节点集划分为 k 个不相交的集群(称为 k-ABGC) ，在不同的领域得到了广泛的应用，包括社交网络分析、推荐系统、信息检索和生物信息学。然而，现有的大多数 k-ABGC 解决方案要么忽略了属性信息，要么未能准确地捕获二部图结构，从而严重影响了结果质量。这些问题的严重性在真实的 ABG 中得到了强调，这些 ABG 通常包含数百万个节点和大量的属性数据，使得在这样的图上有效的 k-ABGC 非常具有挑战性。在本文中，我们提出了 TPO，一种有效的 k-ABGA 方法，可以在多个真实数据集上实现卓越的聚类性能。TPOO 通过两个主要贡献获得高聚类质量: (i)基于多尺度属性亲和力的 k-ABGC 问题的新公式和转换，专门用于捕获节点之间的属性亲和力，同时考虑它们在 ABG 中的多跳连接; (ii)包括一套精心设计的优化解决方案，以避免显式亲和力矩阵构造和促进更快的收敛。广泛的实验，比较 TPO 与19个基线超过5个真实的 ABG，展示了对地面真相标签测量的 TPO 的优越聚类质量。此外，与心脏的状态相比，无论是小型还是大型动脉血气分析仪，TPO 的速度通常都要快40倍以上。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Effective+Clustering+on+Large+Attributed+Bipartite+Graphs)|0|
|[ReCDA: Concept Drift Adaptation with Representation Enhancement for Network Intrusion Detection](https://doi.org/10.1145/3637528.3672007)|Shuo Yang, Xinran Zheng, Jinze Li, Jinfeng Xu, Xingjun Wang, Edith C. H. Ngai|The University of Hong Kong, Hong Kong SAR, China; Tsinghua University, Beijing, China|The deployment of learning-based models to detect malicious activities in network traffic flows is significantly challenged by concept drift. With evolving attack technology and dynamic attack behaviors, the underlying data distribution of recently arrived traffic flows deviates from historical empirical distributions over time. Existing approaches depend on a significant amount of labeled drifting samples to facilitate the deep model to handle concept drift, which faces labor-intensive manual labeling and the risk of label noise. In this paper, we propose ReCDA, a Concept Drift Adaptation method with Representation enhancement, which consists of a self-supervised representation enhancement stage and a weakly-supervised classifier tuning stage. Specifically, in the initial stage, ReCDA introduces drift-aware perturbation and representation alignment to facilitate the model in acquiring robust representations from drift-aware and drift-invariant perspectives. Moreover, in the subsequent stage, a meticulously crafted instructive sampling strategy and a robust representation constraint encourage the model to learn discriminative knowledge about benign and malicious activities during fine-tuning, thereby enhancing performance further. We conduct comprehensive evaluations on several benchmark datasets under varying degrees of concept drift. The experiment results demonstrate the superior adaptability and robustness of the proposed method.|概念漂移对基于学习的网络流量恶意行为检测模型的部署提出了严峻的挑战。随着攻击技术的不断发展和攻击行为的动态变化，最近到达的业务流的底层数据分布随时间的推移而偏离历史经验分布。现有的方法依赖于大量的标记漂移样本，以方便深度模型处理概念漂移，面临着劳动密集型人工标记和标记噪声的风险。本文提出了一种具有表示增强的概念漂移自适应方法 ReCDA，该方法由自监督表示增强阶段和弱监督分类器调整阶段组成。具体来说，在初始阶段，ReCDA 引入了漂移感知扰动和表示对齐，以方便模型从漂移感知和漂移不变的角度获得稳健的表示。此外，在随后的阶段，精心制定的指导性抽样策略和强有力的表示约束鼓励模型在微调期间学习关于良性和恶意活动的区分性知识，从而进一步提高性能。我们对不同概念漂移程度下的几个基准数据集进行了综合评价。实验结果表明，该方法具有较好的适应性和鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ReCDA:+Concept+Drift+Adaptation+with+Representation+Enhancement+for+Network+Intrusion+Detection)|0|
|[Your Neighbor Matters: Towards Fair Decisions Under Networked Interference](https://doi.org/10.1145/3637528.3671960)|Wenjing Yang, Haotian Wang, Haoxuan Li, Hao Zou, Ruochun Jin, Kun Kuang, Peng Cui|; Institute of Artificial Intelligence, Zhejiang University, Hangzhou, China; Tsinghua University, Beijing, China; Peking University, Beijing, China; ZGC laboratory, Beijing, China|In the era of big data, decision-making in social networks may introduce bias due to interconnected individuals. For instance, in peer-to-peer loan platforms on the Web, considering an individual's attributes along with those of their interconnected neighbors, including sensitive attributes, is vital for loan approval or rejection downstream. Unfortunately, conventional fairness approaches often assume independent individuals, overlooking the impact of one person's sensitive attribute on others' decisions. To fill this gap, we introduce "Interference-aware Fairness" (IAF) by defining two forms of discrimination as Self-Fairness (SF) and Peer-Fairness (PF), leveraging advances in interference analysis within causal inference. Specifically, SF and PF causally capture and distinguish discrimination stemming from an individual's sensitive attributes (with fixed neighbors' sensitive attributes) and from neighbors' sensitive attributes (with fixed self's sensitive attributes), separately. Hence, a network-informed decision model is fair only when SF and PF are satisfied simultaneously, as interventions in individuals' sensitive attributes or those of their peers both yield equivalent outcomes. To achieve IAF, we develop a deep doubly robust framework to estimate and regularize SF and PF metrics for decision models. Extensive experiments on synthetic and real-world datasets validate our proposed concepts and methods.|在大数据时代，社交网络中的决策可能会因为相互关联的个体而带来偏见。例如，在互联网上的 P2P 贷款平台中，考虑个人的属性以及他们相互关联的邻居的属性，包括敏感属性，对于下游的贷款批准或拒绝是至关重要的。不幸的是，传统的公平方法往往假设独立的个体，忽视了一个人的敏感属性对其他人的决定的影响。为了填补这一空白，我们引入了“干扰意识公平”(IAF) ，将两种形式的歧视定义为自我公平(SF)和同伴公平(PF) ，利用因果推理中干扰分析的进展。具体来说，SF 和 PF 分别捕获和区分个体敏感属性(固定邻居敏感属性)和邻居敏感属性(固定自我敏感属性)所产生的歧视。因此，只有当 SF 和 PF 同时满足时，网络知情决策模型才是公平的，因为对个体敏感属性或同伴敏感属性的干预都会产生等效的结果。为了实现 IAF，我们开发了一个深入的双健壮性框架来估计和规范决策模型的 SF 和 PF 度量。在合成和真实世界数据集上的大量实验验证了我们提出的概念和方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Your+Neighbor+Matters:+Towards+Fair+Decisions+Under+Networked+Interference)|0|
|[SEBot: Structural Entropy Guided Multi-View Contrastive learning for Social Bot Detection](https://doi.org/10.1145/3637528.3671871)|Yingguang Yang, Qi Wu, Buyun He, Hao Peng, Renyu Yang, Zhifeng Hao, Yong Liao|University of Science and Technology of China, Hefei, China; Shantou University, Shantou, China; Beihang University, Beijing, China|Recent advancements in social bot detection have been driven by the adoption of Graph Neural Networks. The social graph, constructed from social network interactions, contains benign and bot accounts that influence each other. However, previous graph-based detection methods that follow the transductive message-passing paradigm may not fully utilize hidden graph information and are vulnerable to adversarial bot behavior. The indiscriminate message passing between nodes from different categories and communities results in excessively homogeneous node representations, ultimately reducing the effectiveness of social bot detectors. In this paper, we propose \SEBot, a novel multi-view graph-based contrastive learning-enabled social bot detector. In particular, we use structural entropy as an uncertainty metric to optimize the entire graph's structure and subgraph-level granularity, revealing the implicitly existing hierarchical community structure. And we design an encoder to enable message passing beyond the homophily assumption, enhancing robustness to adversarial behaviors of social bots. Finally, we employ multi-view contrastive learning to maximize mutual information between different views and enhance the detection performance through multi-task learning. Experimental results demonstrate that our approach significantly improves the performance of social bot detection compared with SOTA methods.|图形神经网络的应用推动了社交机器人检测技术的发展。由社交网络互动构建的社交图包含了互相影响的良性账户和机器人账户。然而，以往的基于图的检测方法遵循传导性消息传递的范式，可能不能充分利用隐藏的图信息，并易受敌对机器人行为。来自不同类别和社区的节点之间不加区分的消息传递导致了过度同质的节点表示，最终降低了社会机器人检测器的有效性。在本文中，我们提出了一种新的基于多视图图形的对比学习社会机器人检测器 SEBot。特别地，我们使用结构熵作为一个不确定性度量来优化整个图的结构和子图级粒度，揭示隐含存在的层次化社区结构。我们设计了一个编码器，使信息传递超越了同质假设，增强了对社会机器人敌对行为的鲁棒性。最后，采用多视图对比学习，最大化不同视图之间的相互信息，通过多任务学习提高检测性能。实验结果表明，与 SOTA 方法相比，该方法显著提高了社会机器人检测的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SEBot:+Structural+Entropy+Guided+Multi-View+Contrastive+learning+for+Social+Bot+Detection)|0|
|[AdaRD: An Adaptive Response Denoising Framework for Robust Learner Modeling](https://doi.org/10.1145/3637528.3671684)|Fangzhou Yao, Qi Liu, Linan Yue, Weibo Gao, Jiatong Li, Xin Li, Yuanjing He|; The Open University of China, Beijing, China|Learner modeling is a crucial task in online learning environments, where Cognitive Diagnosis Models (CDMs) are employed to assess learners' knowledge mastery levels based on recorded response logs. However, the prevalence of noise in recorded response data poses significant challenges, including various behaviors such as guess and slip, casual answers, and system-induced errors. The existence of noise degrades the accuracy of diagnosis results and learner performance predictions. In this work, we propose a general framework, Adaptive Response Denoising (AdaRD), designed to salvage CDMs from the influence of noisy learner-exercise responses. AdaRD extends existing CDMs, incorporating primary training for denoised CDMs and auxiliary training for additional denoising support. The primary training employs binary Generalized Cross Entropy (GCE) loss to slow down the large update of learner knowledge states caused by noisy responses. Simultaneously, we utilize the variance of diagnosed knowledge mastery levels between primary and auxiliary diagnosis modules as a criterion to downweight high-variance responses that are likely to be noisy. In this manner, the proposed framework can prune noisy response learning during training, thereby enhancing the accuracy and robustness of CDMs. Extensive experiments on both real-world and synthetic datasets validate AdaRD's effectiveness in mitigating the impact of noisy learner-exercise responses.|在线学习环境中，学习者建模是一项非常重要的任务，认知诊断模型(CDM)被用来评估学习者的知识掌握水平。然而，在记录的响应数据中，噪音的普遍存在提出了重大的挑战，包括各种行为，如猜测和滑倒，随意的答案，以及系统引起的错误。噪声的存在降低了诊断结果和学习者性能预测的准确性。在这项工作中，我们提出了一个通用的框架，自适应响应去噪(AdaRD) ，旨在挽救清洁发展机制的影响噪声学习者-练习反应。AdaRD 扩展了现有的清洁发展机制，包括降噪清洁发展机制的初级培训和额外降噪支持的辅助培训。初级训练采用二进制广义交叉熵(GCE)损失来减缓由噪声响应引起的学习者知识状态的大幅更新。同时，我们利用主要诊断模块和辅助诊断模块之间诊断知识掌握水平的方差作为减轻可能有噪声的高方差响应的标准。通过这种方式，提出的框架可以在训练过程中去除噪声响应学习，从而提高清晰度模型的准确性和鲁棒性。在真实世界和合成数据集上的大量实验验证了 AdaRD 在减轻噪声学习者练习反应的影响方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AdaRD:+An+Adaptive+Response+Denoising+Framework+for+Robust+Learner+Modeling)|0|
|[RPMixer: Shaking Up Time Series Forecasting with Random Projections for Large Spatial-Temporal Data](https://doi.org/10.1145/3637528.3671881)|ChinChia Michael Yeh, Yujie Fan, Xin Dai, Uday Singh Saini, Vivian Lai, Prince Osei Aboagye, Junpeng Wang, Huiyuan Chen, Yan Zheng, Zhongfang Zhuang, Liang Wang, Wei Zhang|Visa Research, Foster City, CA, USA|Spatial-temporal forecasting systems play a crucial role in addressing numerous real-world challenges. In this paper, we investigate the potential of addressing spatial-temporal forecasting problems using general time series forecasting models, i.e., models that do not leverage the spatial relationships among the nodes. We propose a all-Multi-Layer Perceptron (all-MLP) time series forecasting architecture called RPMixer. The all-MLP architecture was chosen due to its recent success in time series forecasting benchmarks. Furthermore, our method capitalizes on the ensemble-like behavior of deep neural networks, where each individual block within the network behaves like a base learner in an ensemble model, particularly when identity mapping residual connections are incorporated. By integrating random projection layers into our model, we increase the diversity among the blocks' outputs, thereby improving the overall performance of the network. Extensive experiments conducted on the largest spatial-temporal forecasting benchmark datasets demonstrate that the proposed method outperforms 14 alternative methods.|时空预报系统在应对众多现实世界的挑战方面发挥着至关重要的作用。在本文中，我们研究的潜力，解决时空预测问题使用一般的时间序列预测模型，即模型，不利用空间关系的节点。我们提出了一个全多层感知器(全 MLP)时间序列预测体系结构称为 RPMixer。选择全 MLP 架构是因为它最近在时间序列预测基准方面取得了成功。此外，我们的方法利用了深度神经网络的集成行为，其中每个单独的块在网络中的行为就像集成模型中的基础学习者，特别是当身份映射残余连接被合并时。通过将随机投影层集成到我们的模型中，我们增加了块输出的多样性，从而提高了网络的整体性能。在最大的时空预测基准数据集上进行的大量实验表明，该方法优于14种替代方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RPMixer:+Shaking+Up+Time+Series+Forecasting+with+Random+Projections+for+Large+Spatial-Temporal+Data)|0|
|[Using Self-supervised Learning Can Improve Model Fairness](https://doi.org/10.1145/3637528.3671991)|Sofia Yfantidou, Dimitris Spathis, Marios Constantinides, Athena Vakali, Daniele Quercia, Fahim Kawsar|Aristotle University of Thessaloniki, Thessaloniki, Greece; Nokia Bell Labs, Cambridge, United Kingdom|Self-supervised learning (SSL) has become the de facto training paradigm of large models, where pre-training is followed by supervised fine-tuning using domain-specific data and labels. Despite demonstrating comparable performance with supervised methods, comprehensive efforts to assess SSL's impact on machine learning fairness (i.e., performing equally on different demographic breakdowns) are lacking. Hypothesizing that SSL models would learn more generic, hence less biased representations, this study explores the impact of pre-training and fine-tuning strategies on fairness. We introduce a fairness assessment framework for SSL, comprising five stages: defining dataset requirements, pre-training, fine-tuning with gradual unfreezing, assessing representation similarity conditioned on demographics, and establishing domain-specific evaluation processes. We evaluate our method's generalizability on three real-world human-centric datasets (i.e., MIMIC, MESA, and GLOBEM) by systematically comparing hundreds of SSL and fine-tuned models on various dimensions spanning from the intermediate representations to appropriate evaluation metrics. Our findings demonstrate that SSL can significantly improve model fairness, while maintaining performance on par with supervised methods-exhibiting up to a 30% increase in fairness with minimal loss in performance through self-supervision. We posit that such differences can be attributed to representation dissimilarities found between the best- and the worst-performing demographics across models-up to x13 greater for protected attributes with larger performance discrepancies between segments. Code: https://github.com/Nokia-Bell-Labs/SSLfairness|自监督学习(SSL)已经成为大型模型事实上的训练范式，其中预训练之后使用特定领域的数据和标签进行监督微调。尽管证明了与监督方法相当的性能，评估 SSL 对机器学习公平性的影响的全面努力(即，在不同的人口统计学分解上表现相同)是缺乏的。本研究假设 SSL 模型可以学习到更多的通用性，从而减少偏见表征，探讨预训练和微调策略对公平性的影响。本文介绍了 SSL 的公平性评估框架，包括五个阶段: 定义数据集需求、预训练、逐步解冻的微调、基于人口统计学的表示相似性评估和建立特定领域的评估流程。我们通过系统地比较从中间表示到适当的评估指标的各个维度上的数百个 SSL 和微调模型，来评估我们的方法在三个真实世界的以人为中心的数据集(即 MIMIC，MESA 和 GLOBEM)上的普遍性。我们的研究结果表明，SSL 可以显著提高模型的公平性，同时保持与监督方法同等的性能-显示高达30% 的公平性增加，最小的性能损失通过自我监督。我们假设这种差异可以归因于模型中表现最好的和表现最差的人口统计数据之间的代表性差异，对于具有较大性能差异的受保护属性，这种差异高达 x13。密码:  https://github.com/nokia-bell-labs/sslfairness|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Using+Self-supervised+Learning+Can+Improve+Model+Fairness)|0|
|[Self-consistent Deep Geometric Learning for Heterogeneous Multi-source Spatial Point Data Prediction](https://doi.org/10.1145/3637528.3671737)|Dazhou Yu, Xiaoyun Gong, Yun Li, Meikang Qiu, Liang Zhao|Augusta University, Augusta, GA, USA; Emory University, Atlanta, GA, USA|Multi-source spatial point data prediction is crucial in fields like environmental monitoring and natural resource management, where integrating data from various sensors is the key to achieving a holistic environmental understanding. Existing models in this area often fall short due to their domain-specific nature and lack a strategy for integrating information from various sources in the absence of ground truth labels. Key challenges include evaluating the quality of different data sources and modeling spatial relationships among them effectively. Addressing these issues, we introduce an innovative multi-source spatial point data prediction framework that adeptly aligns information from varied sources without relying on ground truth labels. A unique aspect of our method is the 'fidelity score,' a quantitative measure for evaluating the reliability of each data source. Furthermore, we develop a geo-location-aware graph neural network tailored to accurately depict spatial relationships between data points. Our framework has been rigorously tested on two real-world datasets and one synthetic dataset. The results consistently demonstrate its superior performance over existing state-of-the-art methods.|多源空间点数据预测在环境监测和自然资源管理等领域至关重要，在这些领域，整合来自各种传感器的数据是实现全面环境认识的关键。这一领域的现有模型往往由于其特定领域的性质而不足，并且缺乏在没有地面真相标签的情况下整合来自各种来源的信息的战略。主要挑战包括评估不同数据源的质量和有效地建立它们之间的空间关系。针对这些问题，我们引入了一个创新的多源空间点数据预测框架，它能够在不依赖地面真相标签的情况下，灵活地对齐来自不同来源的信息。我们的方法的一个独特方面是“保真度评分”，一种用于评估每个数据源的可靠性的定量度量。此外，我们开发了一个地理位置感知图神经网络，以准确描述数据点之间的空间关系。我们的框架已经在两个真实数据集和一个合成数据集上进行了严格的测试。结果一致表明，其优越的性能超过现有的国家的最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-consistent+Deep+Geometric+Learning+for+Heterogeneous+Multi-source+Spatial+Point+Data+Prediction)|0|
|[PolygonGNN: Representation Learning for Polygonal Geometries with Heterogeneous Visibility Graph](https://doi.org/10.1145/3637528.3671738)|Dazhou Yu, Yuntong Hu, Yun Li, Liang Zhao|Emory University, Atlanta, GA, USA|Polygon representation learning is essential for diverse applications, encompassing tasks such as shape coding, building pattern classification, and geographic question answering. While recent years have seen considerable advancements in this field, much of the focus has been on single polygons, overlooking the intricate inner- and inter-polygonal relationships inherent in multipolygons. To address this gap, our study introduces a comprehensive framework specifically designed for learning representations of polygonal geometries, particularly multipolygons. Central to our approach is the incorporation of a heterogeneous visibility graph, which seamlessly integrates both inner- and inter-polygonal relationships. To enhance computational efficiency and minimize graph redundancy, we implement a heterogeneous spanning tree sampling method. Additionally, we devise a rotation-translation invariant geometric representation, ensuring broader applicability across diverse scenarios. Finally, we introduce Multipolygon-GNN, a novel model tailored to leverage the spatial and semantic heterogeneity inherent in the visibility graph. Experiments on five real-world and synthetic datasets demonstrate its ability to capture informative representations for polygonal geometries.|多边形表示学习对于不同的应用是必不可少的，包括任务，如形状编码，建筑模式分类和地理问题的回答。虽然近年来在这一领域取得了相当大的进展，但大部分的焦点都集中在单个多边形上，忽略了多边形固有的错综复杂的内多边形和内多边形之间的关系。为了解决这一差距，我们的研究引入了一个全面的框架，专门为学习表示多边形几何，特别是多边形。我们的方法的核心是引入一个异构的可见性图，它无缝地集成了内部和内部多边形关系。为了提高计算效率和减少图冗余，我们实现了一种异构生成树抽样方法。此外，我们设计了一个旋转平移不变的几何表示，确保更广泛的适用性跨不同的场景。最后，我们介绍了多边形 GNN，一个新的模型，以利用空间和语义的异质性固有的可见性图。在五个真实世界和合成数据集上的实验证明了它捕获多边形几何信息表示的能力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PolygonGNN:+Representation+Learning+for+Polygonal+Geometries+with+Heterogeneous+Visibility+Graph)|0|
|[GinAR: An End-To-End Multivariate Time Series Forecasting Model Suitable for Variable Missing](https://doi.org/10.1145/3637528.3672055)|Chengqing Yu, Fei Wang, Zezhi Shao, Tangwen Qian, Zhao Zhang, Wei Wei, Yongjun Xu|; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China|Multivariate time series forecasting (MTSF) is crucial for decision-making to precisely forecast the future values/trends, based on the complex relationships identified from historical observations of multiple sequences. Recently, Spatial-Temporal Graph Neural Networks (STGNNs) have gradually become the theme of MTSF model as their powerful capability in mining spatial-temporal dependencies, but almost of them heavily rely on the assumption of historical data integrity. In reality, due to factors such as data collector failures and time-consuming repairment, it is extremely challenging to collect the whole historical observations without missing any variable. In this case, STGNNs can only utilize a subset of normal variables and easily suffer from the incorrect spatial-temporal dependency modeling issue, resulting in the degradation of their forecasting performance. To address the problem, in this paper, we propose a novel Graph Interpolation Attention Recursive Network (named GinAR) to precisely model the spatial-temporal dependencies over the limited collected data for forecasting. In GinAR, it consists of two key components, that is, interpolation attention and adaptive graph convolution to take place of the fully connected layer of simple recursive units, and thus are capable of recovering all missing variables and reconstructing the correct spatial-temporal dependencies for recursively modeling of multivariate time series data, respectively. Extensive experiments conducted on five real-world datasets demonstrate that GinAR outperforms 11 SOTA baselines, and even when 90% of variables are missing, it can still accurately predict the future values of all variables.|多变量时间序列预测(MTSF)是决策的关键，以精确预测未来的价值/趋势，基于从多个序列的历史观察所确定的复杂关系。近年来，时空图形神经网络(STGNN)以其强大的挖掘时空依赖性的能力逐渐成为 MTSF 模型的主题，但是它们大多依赖于历史数据完整性的假设。实际上，由于诸如数据收集器故障和耗时的维修等因素，在不遗漏任何变量的情况下收集整个历史观测数据是极具挑战性的。在这种情况下，STGNN 只能利用正态变量的一个子集，很容易受到不正确的时空依赖性建模问题，导致其预测性能的下降。为了解决这一问题，本文提出了一种新的图插值注意力递归网络(GinAR)来精确模拟有限采集数据的时空依赖关系。在 GinAR 中，它由插值注意和自适应图卷积两个关键部分组成，分别代替简单递归单元的完全连通层，从而能够恢复所有缺失的变量，并为多变量时间序列数据的递归建模重建正确的时空依赖关系。在五个实际数据集上进行的大量实验表明，GinAR 优于11个 SOTA 基线，即使缺少90% 的变量，它仍然可以准确地预测所有变量的未来值。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GinAR:+An+End-To-End+Multivariate+Time+Series+Forecasting+Model+Suitable+for+Variable+Missing)|0|
|[RIGL: A Unified Reciprocal Approach for Tracing the Independent and Group Learning Processes](https://doi.org/10.1145/3637528.3671711)|Xiaoshan Yu, Chuan Qin, Dazhong Shen, Shangshang Yang, Haiping Ma, Hengshu Zhu, Xingyi Zhang|; Shanghai Artificial Intelligence Laboratory, Shanghai, China; School of Computer Science and Technology, Anhui University, Hefei, Anhui, China; School of Artificial Intelligence, Anhui University, Hefei, Anhui, China; Career Science Lab, BOSS Zhipin, Beijing, China; Career Science Lab, BOSS Zhipin & PBC School of Finance, Tsinghua University, Beijing, China|In the realm of education, both independent learning and group learning are esteemed as the most classic paradigms. The former allows learners to self-direct their studies, while the latter is typically characterized by teacher-directed scenarios. Recent studies in the field of intelligent education have leveraged deep temporal models to trace the learning process, capturing the dynamics of students' knowledge states, and have achieved remarkable performance. However, existing approaches have primarily focused on modeling the independent learning process, with the group learning paradigm receiving less attention. Moreover, the reciprocal effect between the two learning processes, especially their combined potential to foster holistic student development, remains inadequately explored. To this end, in this paper, we propose RIGL, a unified Reciprocal model to trace knowledge states at both the individual and group levels, drawing from the Independent and Group Learning processes. Specifically, we first introduce a time frame-aware reciprocal embedding module to concurrently model both student and group response interactions across various time frames. Subsequently, we employ reciprocal enhanced learning modeling to fully exploit the comprehensive and complementary information between the two behaviors. Furthermore, we design a relation-guided temporal attentive network, comprised of dynamic graph modeling coupled with a temporal self-attention mechanism. It is used to delve into the dynamic influence of individual and group interactions throughout the learning processes, which is crafted to explore the dynamic intricacies of both individual and group interactions during the learning sequences. Conclusively, we introduce a bias-aware contrastive learning module to bolster the stability of the model's training. Extensive experiments on four real-world educational datasets clearly demonstrate the effectiveness of the proposed RIGL model. Our codes are available at https://github.com/LabyrinthineLeo/RIGL.|在教育领域，自主学习和小组学习被认为是最经典的范式。前者允许学习者自我指导他们的学习，而后者则是典型的拥有属性教师指导的情景。近年来，智能教育领域的研究利用深度时间模型来跟踪学习过程，捕捉学生知识状态的动态变化，取得了显著的效果。然而，现有的研究方法主要集中在对自主学习过程进行建模，而对小组学习范式的关注较少。此外，这两个学习过程之间的相互作用，特别是它们促进学生整体发展的综合潜力，仍然没有得到充分的探索。为此，在本文中，我们提出了 RIGL，一个统一的互惠模型，以追踪知识状态在个人和团体水平，借鉴独立和团体学习过程。具体来说，我们首先引入一个时间框架感知的互惠嵌入模块来并发模拟跨不同时间框架的学生和团体响应交互。然后，我们采用互惠增强的学习模型来充分利用这两种行为之间的综合信息和互补信息。在此基础上，设计了一个关系引导的时间注意网络，该网络由动态图模型和时间自我注意机制组成。它被用来研究个人和群体互动在整个学习过程中的动态影响，它被精心设计来探索在学习过程中个人和群体互动的动态错综复杂性。最后，我们引入了一个偏差感知的对比学习模块，以增强模型训练的稳定性。在四个真实世界的教育数据集上的大量实验清楚地证明了所提出的 RIGL 模型的有效性。我们的密码可以在 https://github.com/labyrinthineleo/rigl 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RIGL:+A+Unified+Reciprocal+Approach+for+Tracing+the+Independent+and+Group+Learning+Processes)|0|
|[Unveiling Privacy Vulnerabilities: Investigating the Role of Structure in Graph Data](https://doi.org/10.1145/3637528.3672013)|Hanyang Yuan, Jiarong Xu, Cong Wang, Ziqi Yang, Chunping Wang, Keting Yin, Yang Yang|Zhejiang University & Fudan University, Hangzhou, China; Finvolution Group, Shanghai, China; Peking University, Beijing, China; Zhejiang University, Hangzhou, China; Fudan University, Shanghai, China|The public sharing of user information opens the door for adversaries to infer private data, leading to privacy breaches and facilitating malicious activities. While numerous studies have concentrated on privacy leakage via public user attributes, the threats associated with the exposure of user relationships, particularly through network structure, are often neglected. This study aims to fill this critical gap by advancing the understanding and protection against privacy risks emanating from network structure, moving beyond direct connections with neighbors to include the broader implications of indirect network structural patterns. To achieve this, we first investigate the problem of Graph Privacy Leakage via Structure (GPS), and introduce a novel measure, the Generalized Homophily Ratio, to quantify the various mechanisms contributing to privacy breach risks in GPS. Based on this insight, we develop a novel graph private attribute inference attack, which acts as a pivotal tool for evaluating the potential for privacy leakage through network structures under worst-case scenarios. To protect users' private data from such vulnerabilities, we propose a graph data publishing method incorporating a learnable graph sampling technique, effectively transforming the original graph into a privacy-preserving version. Extensive experiments demonstrate that our attack model poses a significant threat to user privacy, and our graph data publishing method successfully achieves the optimal privacy-utility trade-off compared to baselines.|公开分享用户信息为对手推断私人数据打开了大门，导致侵犯隐私和助长恶意活动。虽然大量的研究集中在通过公共用户属性的隐私泄露，与用户关系的暴露相关的威胁，特别是通过网络结构，往往被忽视。本研究旨在通过提高对网络结构引起的隐私风险的认识和保护，超越与邻居的直接联系，将间接网络结构模式的更广泛影响纳入其中，从而填补这一重要空白。为了实现这一目标，我们首先研究了基于结构(GPS)的图形隐私泄漏问题，并引入了一种新的度量方法——广义同伦比，来量化导致 GPS 隐私泄漏风险的各种机制。基于这种认识，我们开发了一种新的图私有属性推理攻击，它可以作为评估最坏情况下通过网络结构发生隐私泄漏的潜在可能性的关键工具。为了保护用户的私有数据不受此类漏洞的影响，提出了一种图形数据发布方法，该方法结合了可学习的图形采样技术，有效地将原始图形转换为保护用户隐私的版本。大量的实验表明，我们的攻击模型对用户隐私构成了严重的威胁，我们的图形数据发布方法成功地实现了与基线相比的最优隐私-效用权衡。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unveiling+Privacy+Vulnerabilities:+Investigating+the+Role+of+Structure+in+Graph+Data)|0|
|[Graph Cross Supervised Learning via Generalized Knowledge](https://doi.org/10.1145/3637528.3671830)|Xiangchi Yuan, Yijun Tian, Chunhui Zhang, Yanfang Ye, Nitesh V. Chawla, Chuxu Zhang|Brandeis University & Georgia Institute of Technology, Waltham, MA, USA; University of Notre Dame, South Bend, IN, USA; Dartmouth College, Hanover, NH, USA; Brandeis University, Waltham, MA, USA|The success of GNNs highly relies on the accurate labeling of data. Existing methods of ensuring accurate labels, such as weakly-supervised learning, mainly focus on the existing nodes in the graphs. However, in reality, new nodes always continuously emerge on dynamic graphs, with different categories and even label noises. To this end, we formulate a new problem, Graph Cross-Supervised Learning, or Graph Weak-Shot Learning, that describes the challenges of modeling new nodes with novel classes and potential label noises. To solve this problem, we propose Lipshitz-regularized Mixture-of-Experts similarity network (LIME), a novel framework to encode new nodes and handle label noises. Specifically, we first design a node similarity network to capture the knowledge from the original classes, aiming to obtain insights for the emerging novel classes. Then, to enhance the similarity network's generalization to new nodes that could have a distribution shift, we employ the Mixture-of-Experts technique to increase the generalization of knowledge learned by the similarity network. To further avoid losing generalization ability during training, we introduce the Lipschitz bound to stabilize model output and alleviate the distribution shift issue. Empirical experiments validate LIME's effectiveness: we observe a substantial enhancement of up to 11.34% in node classification accuracy compared to the backbone model when subjected to the challenges of label noise on novel classes across five benchmark datasets. The code can be accessed through https://github.com/xiangchi-yuan/Graph-Cross-Supervised-Learning.|GNN 的成功在很大程度上依赖于数据的准确标记。现有的确保标签准确性的方法，如弱监督学习，主要集中在图中存在的节点。然而，在现实生活中，动态图上总是不断出现新的节点，它们具有不同的类别，甚至带有标签噪声。为此，我们提出了一个新的问题，图交叉监督学习，或图弱镜头学习，描述了建模新的节点与新的类和潜在的标签噪声的挑战。为了解决这一问题，我们提出了一种基于 Lipshitz 正则化的专家混合相似网络(LIME) ，这是一种新的节点编码和标签噪声处理框架。具体来说，我们首先设计一个节点相似性网络来获取来自原始类的知识，旨在获得对新兴新类的洞察力。然后，为了增强相似网络对可能发生分布偏移的新节点的泛化能力，采用专家混合技术增强相似网络所学知识的泛化能力。为了进一步避免在训练过程中失去泛化能力，我们引入 Lipschitz 界来稳定模型输出，减轻分布移位问题。实验验证了 LIME 算法的有效性: 当受到5个基准数据集上新类别的标签噪声的挑战时，我们观察到与骨干模型相比，LIME 算法在节点分类准确率方面有了大幅度的提高，提高了11.34% 。代码可以通过 https://github.com/xiangchi-yuan/graph-cross-supervised-learning 访问。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Cross+Supervised+Learning+via+Generalized+Knowledge)|0|
|[Effective Generation of Feasible Solutions for Integer Programming via Guided Diffusion](https://doi.org/10.1145/3637528.3671783)|Hao Zeng, Jiaqi Wang, Avirup Das, Junying He, Kunpeng Han, Haoyuan Hu, Mingfei Sun|University of Manchester, Manchester, United Kingdom; Cainiao Network, Hangzhou, China|Feasible solutions are crucial for Integer Programming (IP) since they can substantially speed up the solving process. In many applications, similar IP instances often exhibit similar structures and shared solution distributions, which can be potentially modeled by deep learning methods. Unfortunately, existing deep-learning-based algorithms, such as Neural Diving [21] and Predict-and-search framework [8], are limited to generating only partial feasible solutions, and they must rely on solvers like SCIP and Gurobi to complete the solutions for a given IP problem. In this paper, we propose a novel framework that generates complete feasible solutions end-to-end. Our framework leverages contrastive learning to characterize the relationship between IP instances and solutions, and learns latent embeddings for both IP instances and their solutions. Further, the framework employs diffusion models to learn the distribution of solution embeddings conditioned on IP representations, with a dedicated guided sampling strategy that accounts for both constraints and objectives. We empirically evaluate our framework on four typical datasets of IP problems, and show that it effectively generates complete feasible solutions with a high probability (> 89.7 %) without the reliance of Solvers and the quality of solutions is comparable to the best heuristic solutions from Gurobi. Furthermore, by integrating our method's sampled partial solutions with the CompleteSol heuristic from SCIP [19], the resulting feasible solutions outperform those from state-of-the-art methods across all datasets, exhibiting a 3.7 to 33.7% improvement in the gap to optimal values, and maintaining a feasible ratio of over 99.7% for all datasets.|可行的解决方案对于整数规划来说至关重要，因为它们可以大大加快解决过程。在许多应用中，相似的 IP 实例往往表现出相似的结构和共享的解决方案分布，这可能是由深度学习方法建模。不幸的是，现有的基于深度学习的算法，如神经潜水和预测与搜索框架，仅限于产生部分可行的解决方案，他们必须依靠解决方案，如 SCIP 和 Gurobi，以完成一个给定的知识产权问题的解决方案。在本文中，我们提出了一个新的框架，生成完全可行的解决方案端到端。我们的框架利用对比学习来描述 IP 实例和解决方案之间的关系，并学习 IP 实例及其解决方案的潜在嵌入。此外，该框架使用扩散模型来学习以 IP 表示为条件的解嵌入的分布，并使用一个专门的引导抽样策略来解释约束和目标。我们根据四个典型的知识产权问题数据集对我们的框架进行了实证评估，结果表明，在不依赖 Solvers 的情况下，它有效地产生了高概率(> 89.7%)的完全可行的解决方案，而且解决方案的质量堪比 Gurobi 的最佳启发式解决方案。此外，通过将我们的方法的采样部分解决方案与 SCIP [19]的 CompleteSol 启发式相结合，所得到的可行解决方案在所有数据集中都优于最先进的方法，显示出与最佳值的差距提高了3.7% 至33.7% ，并保持所有数据集的可行比率超过99.7% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Effective+Generation+of+Feasible+Solutions+for+Integer+Programming+via+Guided+Diffusion)|0|
|[Path-Specific Causal Reasoning for Fairness-aware Cognitive Diagnosis](https://doi.org/10.1145/3637528.3672049)|Dacao Zhang, Kun Zhang, Le Wu, Mi Tian, Richang Hong, Meng Wang||Cognitive Diagnosis~(CD), which leverages students and exercise data to predict students' proficiency levels on different knowledge concepts, is one of fundamental components in Intelligent Education. Due to the scarcity of student-exercise interaction data, most existing methods focus on making the best use of available data, such as exercise content and student information~(e.g., educational context). Despite the great progress, the abuse of student sensitive information has not been paid enough attention. Due to the important position of CD in Intelligent Education, employing sensitive information when making diagnosis predictions will cause serious social issues. Moreover, data-driven neural networks are easily misled by the shortcut between input data and output prediction, exacerbating this problem. Therefore, it is crucial to eliminate the negative impact of sensitive information in CD models. In response, we argue that sensitive attributes of students can also provide useful information, and only the shortcuts directly related to the sensitive information should be eliminated from the diagnosis process. Thus, we employ causal reasoning and design a novel Path-Specific Causal Reasoning Framework (PSCRF) to achieve this goal. Specifically, we first leverage an encoder to extract features and generate embeddings for general information and sensitive information of students. Then, we design a novel attribute-oriented predictor to decouple the sensitive attributes, in which fairness-related sensitive features will be eliminated and other useful information will be retained. Finally, we designed a multi-factor constraint to ensure the performance of fairness and diagnosis performance simultaneously. Extensive experiments over real-world datasets (e.g., PISA dataset) demonstrate the effectiveness of our proposed PSCRF.|认知诊断是智能教育的一个基本组成部分，它利用学生和练习数据来预测学生对不同知识概念的熟练程度。由于学生-练习交互数据的缺乏，现有的研究方法大多侧重于充分利用现有的数据，如练习内容和学生信息 ~ (例如，教育背景)。尽管取得了很大的进步，但对学生敏感信息的滥用问题却没有引起足够的重视。由于光盘在智能教育中的重要地位，利用敏感信息进行诊断预测会引起严重的社会问题。此外，数据驱动的神经网络很容易被输入数据和输出预测之间的捷径误导，加剧了这个问题。因此，消除光盘模型中敏感信息的负面影响至关重要。作为回应，我们认为学生的敏感属性也可以提供有用的信息，只有与敏感信息直接相关的快捷方式应该被排除在诊断过程之外。因此，我们采用因果推理并设计了一个新的路径特定因果推理框架(pSCRF)来实现这个目标。具体来说，我们首先利用一个编码器来提取特征，并为学生的一般信息和敏感信息生成嵌入。然后，我们设计了一个新的面向属性的预测器来解耦敏感属性，其中与公平性相关的敏感特征将被消除，其他有用的信息将被保留。最后，我们设计了一个多因素约束来同时保证公平性和诊断性能。对现实世界数据集(例如 PISA 数据集)的大量实验证明了我们提出的 PSCRF 的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Path-Specific+Causal+Reasoning+for+Fairness-aware+Cognitive+Diagnosis)|0|
|[Brant-X: A Unified Physiological Signal Alignment Framework](https://doi.org/10.1145/3637528.3671953)|Daoze Zhang, Zhizhang Yuan, Junru Chen, Kerui Chen, Yang Yang|Zhejiang University, Hangzhou, China|Physiological signals serve as indispensable clues for understanding various physiological states of human bodies. Most existing works have focused on a single type of physiological signals for a range of application scenarios. However, as the body is a holistic biological system, the inherent interconnection among various physiological data should not be neglected. In particular, given the brain's role as the control center for vital activities, electroencephalogram (EEG) exhibits significant correlations with other physiological signals. Therefore, the correlation between EEG and other physiological signals holds potential to improve performance in various scenarios. Nevertheless, achieving this goal is still constrained by several challenges: the scarcity of simultaneously collected physiological data, the differences in correlations between various signals, and the correlation differences between various tasks. To address these issues, we propose a unified physiological signal alignment framework, Brant-X, to model the correlation between EEG and other signals. Our approach (1) employs the EEG foundation model to data-efficiently transfer the rich knowledge in EEG to other physiological signals, and (2) introduces the two-level alignment to fully align the semantics of EEG and other signals from different semantic scales. In the experiments, Brant-X achieves state-of-the-art performance compared with task-agnostic and task-specific baselines on various downstream tasks in diverse scenarios, including sleep stage classification, emotion recognition, freezing of gaits detection, and eye movement communication. Moreover, the analysis on the arrhythmia detection task and the visualization in case study further illustrate the effectiveness of Brant-X in the knowledge transfer from EEG to other physiological signals. The model's homepage is at https://github.com/zjunet/Brant-X/.|生理信号是理解人体各种生理状态不可缺少的线索。大多数现有的工作集中在一个单一类型的生理信号的一系列应用场景。然而，身体是一个整体的生物系统，各种生理数据之间的内在联系不容忽视。特别是，考虑到大脑作为重要活动的控制中心的作用，脑电图(EEG)与其他生理信号显示出显著的相关性。因此，脑电信号与其他生理信号之间的相关性具有改善各种情况下性能的潜力。然而，实现这一目标仍然受到几个挑战的限制: 同时收集的生理数据的稀缺性，不同信号之间相关性的差异，以及不同任务之间的相关性差异。为了解决这些问题，我们提出了一个统一的生理信号对齐框架 Brant-X，来模拟脑电信号与其他信号之间的相关性。该方法(1)利用脑电基础模型将脑电信号中丰富的知识有效地传递给其他生理信号，(2)引入两级对齐，从不同的语义尺度完全对齐脑电信号和其他信号的语义。在实验中，Brant-X 在不同情景下的各种下游任务中，与任务无关和任务特定的基线相比，取得了最好的表现，包括睡眠阶段分类、情绪识别、步态检测冻结和眼球运动交流。此外，对心律失常检测任务的分析和案例研究的可视化进一步说明了 Brant-X 在脑电信号向其他生理信号传递知识方面的有效性。模特的主页已经上了 https://github.com/zjunet/brant-x/。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Brant-X:+A+Unified+Physiological+Signal+Alignment+Framework)|0|
|[Subspace Selection based Prompt Tuning with Nonconvex Nonsmooth Black-Box Optimization](https://doi.org/10.1145/3637528.3671986)|Haozhen Zhang, Hualin Zhang, Bin Gu, Yi Chang|; School of Artificial Intelligence, Jilin University, Changchun, Jilin, China; Mohamed bin Zayed University of Artificial Intelligence, Masdar, United Arab Emirates|In this paper, we introduce a novel framework for black-box prompt tuning with a subspace learning and selection strategy, leveraging derivative-free optimization algorithms. This approach is crucial for scenarios where user interaction with language models is restricted to API usage, without direct access to their internal structures or gradients, a situation typical in Language-Model-as-a-Service (LMaaS). Our framework focuses on exploring the low-dimensional subspace of continuous prompts. Previous work on black-box prompt tuning necessitates a substantial number of API calls due to the random choice of the subspace. To tackle this problem, we propose to use a simple zeroth-order optimization algorithm to tackle nonconvex optimization challenges with nonsmooth nonconvex regularizers: the Zeroth-Order Mini-Batch Stochastic Proximal Gradient method (ZO-MB-SPG). A key innovation is the incorporation of nonsmooth nonconvex regularizers, including the indicator function of the l0 constraint, which enhances our ability to select optimal subspaces for prompt optimization. The experimental results show that our proposed black-box prompt tuning method on a few labeled samples can attain similar performance to the methods applicable to LMaaS with much fewer API calls.|本文利用无导数优化算法，提出了一种基于子空间学习和选择策略的黑盒提示调优框架。这种方法对于那些用户与语言模型的交互仅限于 API 使用，而不能直接访问其内部结构或梯度的场景来说是至关重要的，这种情况在 Language-Model-as-a-Service 很典型。我们的框架侧重于探索连续提示的低维子空间。由于子空间的随机选择，以前关于黑盒提示符调优的工作需要大量的 API 调用。为了解决这个问题，我们提出了一个简单的零阶优化算法来解决非光滑非凸正则化的非凸优化问题: 零阶小批量随机近似梯度法(ZO-MB-SPG)。一个关键的创新是引入了非光滑非凸正则化子，包括 l0约束的指示函数，这增强了我们为及时优化选择最优子空间的能力。实验结果表明，我们提出的针对少量标记样本的黑盒提示调优方法能够以更少的 API 调用获得与 LMaaS 相似的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Subspace+Selection+based+Prompt+Tuning+with+Nonconvex+Nonsmooth+Black-Box+Optimization)|0|
|[Heuristic Learning with Graph Neural Networks: A Unified Framework for Link Prediction](https://doi.org/10.1145/3637528.3671946)|Juzheng Zhang, Lanning Wei, Zhen Xu, Quanming Yao|Department of Electronic Engineering, Tsinghua University, Beijing, China|Link prediction is a fundamental task in graph learning, inherently shaped bythe topology of the graph. While traditional heuristics are grounded in graphtopology, they encounter challenges in generalizing across diverse graphs.Recent research efforts have aimed to leverage the potential of heuristics, yeta unified formulation accommodating both local and global heuristics remainsundiscovered. Drawing insights from the fact that both local and globalheuristics can be represented by adjacency matrix multiplications, we propose aunified matrix formulation to accommodate and generalize various heuristics. Wefurther propose the Heuristic Learning Graph Neural Network (HL-GNN) toefficiently implement the formulation. HL-GNN adopts intra-layer propagationand inter-layer connections, allowing it to reach a depth of around 20 layerswith lower time complexity than GCN. Extensive experiments on the Planetoid,Amazon, and OGB datasets underscore the effectiveness and efficiency of HL-GNN.It outperforms existing methods by a large margin in prediction performance.Additionally, HL-GNN is several orders of magnitude faster thanheuristic-inspired methods while requiring only a few trainable parameters. Thecase study further demonstrates that the generalized heuristics and learnedweights are highly interpretable.|链路预测是图形学习中的一个基本任务，其本质是由图的拓扑结构决定的。虽然传统的启发式算法以图形拓扑为基础，但它们在跨不同图形进行泛化时遇到了挑战。最近的研究工作旨在利用启发式的潜力，Yeta 统一的公式适应本地和全球启发式仍然未被发现。基于局部和全局启发式都可以用邻接矩阵乘法表示的事实，我们提出了一个统一的矩阵公式来适应和推广各种启发式。进一步提出了启发式学习图神经网络(HL-GNN)来有效地实现这一公式。HL-GNN 采用层内传播和层间连接，使其能够以比 GCN 更低的时间复杂度达到20层左右的深度。在行星、亚马逊和 OGB 数据集上的广泛实验强调了 HL-GNN 的有效性和效率。它在预测性能上大大优于现有的方法。此外，HL-GNN 比启发式方法快了几个数量级，同时只需要一些可训练的参数。案例研究进一步表明，广义启发式和学习权重是高度可解释的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Heuristic+Learning+with+Graph+Neural+Networks:+A+Unified+Framework+for+Link+Prediction)|0|
|[Asynchronous Vertical Federated Learning for Kernelized AUC Maximization](https://doi.org/10.1145/3637528.3671930)|Ke Zhang, Ganyu Wang, Han Li, Yulong Wang, Hong Chen, Bin Gu|Department of Computer Science, Western University, London, Ontario, Canada; ; College of Informatics, Huazhong Agricultural University, Wuhan, Hubei, China|Vertical Federated Learning (VFL) has garnered significant attention due to its applicability in multi-party collaborative learning and the increasing demand for privacy-preserving measures. Most existing VFL algorithms primarily focus on accuracy as the training model metric. However, the data we access is often imbalanced in the real world, making it difficult for models based on accuracy to correctly classify minority samples. The Area Under the Curve (AUC) serves as an effective metric to evaluate the performance of a model on imbalanced data. Therefore, optimizing AUC can enhance the model's ability to handle imbalanced data. Besides, computational resources within VFL systems are also imbalanced, which makes synchronous VFL algorithms are difficult to apply in the real world. To address the double imbalance issue, we propose Asynchronous Vertical Federated Kernelized AUC Maximization (AVFKAM). Specifically, AVFKAM asynchronously updates a kernel model based on triply stochastic gradients with respect to (w.r.t.) the pairwise loss and random feature approximation. To facilitate theoretical analysis, we transfer the asynchrony of model coefficients to the functional gradient through a dual relationship between coefficients and objective function. Furthermore, we demonstrate that AVFKAM converges to the optimal solution at a rate of O(1/t), where t represents the global iteration number, and discuss the security of the model. If t is denoted as the global iteration number, we provide that it converges to the optimal solution with the rate of O(1/t). Finally, experimental results on various benchmark datasets demonstrate that AVFKAM maintains high AUC performance and efficiency.|垂直联邦学习(VFL)由于其在多方合作学习中的适用性以及对保护隐私措施的需求日益增长而引起了人们的广泛关注。大多数现有的 VFL 算法主要集中在精度作为训练模型度量。然而，我们访问的数据往往在现实世界中是不平衡的，使得基于准确性的模型难以正确分类少数样本。曲线下面积(AUC)是评价不平衡数据模型性能的有效指标。因此，优化 AUC 可以提高模型处理不平衡数据的能力。此外，VFL 系统中的计算资源也是不平衡的，这使得同步 VFL 算法难以在现实世界中应用。为了解决双不平衡问题，我们提出异步垂直联邦核化 AUC 最大化(AVFKAM)。具体来说，AVFKAM 异步更新基于三重随机梯度的核模型相对于成对损失和随机特征近似。为了便于理论分析，我们通过系数与目标函数之间的对偶关系将模型系数的异步性转化为函数梯度。此外，我们证明了 AVFKAM 以 O (1/t)的速率收敛到最优解，其中 t 表示全局迭代次数，并讨论了模型的安全性。如果 t 表示为全局迭代次数，则证明它以 O (1/t)的速率收敛到最优解。最后，在各种基准数据集上的实验结果表明，AVFKAM 保持了较高的 AUC 性能和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Asynchronous+Vertical+Federated+Learning+for+Kernelized+AUC+Maximization)|0|
|[Multivariate Log-based Anomaly Detection for Distributed Database](https://doi.org/10.1145/3637528.3671725)|Lingzhe Zhang, Tong Jia, Mengxi Jia, Ying Li, Yong Yang, Zhonghai Wu|Peking University, Beijing, China|Distributed databases are fundamental infrastructures of today's large-scalesoftware systems such as cloud systems. Detecting anomalies in distributeddatabases is essential for maintaining software availability. Existingapproaches, predominantly developed using Loghub-a comprehensive collection oflog datasets from various systems-lack datasets specifically tailored todistributed databases, which exhibit unique anomalies. Additionally, there's anotable absence of datasets encompassing multi-anomaly, multi-node logs.Consequently, models built upon these datasets, primarily designed forstandalone systems, are inadequate for distributed databases, and the prevalentmethod of deeming an entire cluster anomalous based on irregularities in asingle node leads to a high false-positive rate. This paper addresses theunique anomalies and multivariate nature of logs in distributed databases. Weexpose the first open-sourced, comprehensive dataset with multivariate logsfrom distributed databases. Utilizing this dataset, we conduct an extensivestudy to identify multiple database anomalies and to assess the effectivenessof state-of-the-art anomaly detection using multivariate log data. Our findingsreveal that relying solely on logs from a single node is insufficient foraccurate anomaly detection on distributed database. Leveraging these insights,we propose MultiLog, an innovative multivariate log-based anomaly detectionapproach tailored for distributed databases. Our experiments, based on thisnovel dataset, demonstrate MultiLog's superiority, outperforming existingstate-of-the-art methods by approximately 12|分布式数据库是当今大规模软件系统(如云系统)的基础设施。检测分布式数据库中的异常对于维护软件可用性至关重要。现有的方法，主要是使用 Loghub 开发的——一个来自不同系统的日志数据集的综合集合——缺乏专门针对分布式数据库的数据集，这些数据集表现出独特的异常。此外，还缺少包含多异常、多节点日志的数据集。因此，建立在这些数据集上的模型，主要是为独立系统设计的，对于分布式数据库来说是不够的，基于单个节点的不规则性来判断整个簇异常的流行方法导致了很高的假阳性率。本文讨论了分布式数据库中日志的独特异常和多变量特性。我们使用分布式数据库中的多变量日志来公开第一个开源的、全面的数据集。利用这个数据集，我们进行了一个广泛的研究，以确定多个数据库的异常，并评估使用多变量日志数据的最新异常检测的有效性。我们的研究结果表明，仅仅依靠单一节点的日志是不足以对异常检测分布式数据库进行精确计算的。利用这些见解，我们提出 MultiLog，一种创新的基于多元日志的异常检测方法，专为分布式数据库而设计。我们的实验，基于这个新颖的数据集，证明了 MultiLog 的优越性，比现有的最先进的方法高出大约12个|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multivariate+Log-based+Anomaly+Detection+for+Distributed+Database)|0|
|[Logical Reasoning with Relation Network for Inductive Knowledge Graph Completion](https://doi.org/10.1145/3637528.3671911)|Qinggang Zhang, Keyu Duan, Junnan Dong, Pai Zheng, Xiao Huang|The Hong Kong Polytechnic University, Hung Hom, Hong Kong; National University of Singapore, Singapore, Singapore; The Hong Kong Polytechnic University, Kowloon, Hong Kong|Inductive knowledge graph completion (KGC) aims to infer the missing relationfor a set of newly-coming entities that never appeared in the training set.Such a setting is more in line with reality, as real-world KGs are constantlyevolving and introducing new knowledge. Recent studies have shown promisingresults using message passing over subgraphs to embed newly-coming entities forinductive KGC. However, the inductive capability of these methods is usuallylimited by two key issues. (i) KGC always suffers from data sparsity, and thesituation is even exacerbated in inductive KGC where new entities often havefew or no connections to the original KG. (ii) Cold-start problem. It is overcoarse-grained for accurate KG reasoning to generate representations for newentities by gathering the local information from few neighbors. To this end, wepropose a novel iNfOmax RelAtion Network, namely NORAN, for inductive KGcompletion. It aims to mine latent relation patterns for inductive KGcompletion. Specifically, by centering on relations, NORAN provides a hyperview towards KG modeling, where the correlations between relations can benaturally captured as entity-independent logical evidence to conduct inductiveKGC. Extensive experiment results on five benchmarks show that our frameworksubstantially outperforms the state-of-the-art KGC methods.|归纳知识图完成(KGC)的目的是推断一组新生实体在训练集中从未出现过的缺失关系。这样的设置更符合现实，因为现实世界的幼儿园不断发展和引入新的知识。最近的研究表明，使用信息传递子图嵌入新的实体归纳 KGC 的结果是有希望的。然而，这些方法的归纳能力通常受到两个关键问题的限制。(i) KGC 总是受到数据稀疏的影响，在归纳 KGC 中，这种情况甚至更加恶化，因为新的实体通常与原始 KG 没有或几乎没有连接。(ii)冷启动问题。对于精确的 KG 推理来说，通过收集少数邻居的局部信息来生成新实体的表示是过于粗粒度的。为此，我们提出了一个新颖的 iNfOmax 关系网络，即 NORAN，用于归纳学习。目的在于挖掘归纳 KG 完成的潜在关系模式。具体来说，以关系为中心，NORAN 提供了一个 KG 建模的全景，其中关系之间的相关性可以自然地被捕获为实体无关的逻辑证据来进行归纳 KGC。对五个基准测试的大量实验结果表明，我们的框架大大优于最先进的 KGC 方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Logical+Reasoning+with+Relation+Network+for+Inductive+Knowledge+Graph+Completion)|0|
|[Towards Adaptive Neighborhood for Advancing Temporal Interaction Graph Modeling](https://doi.org/10.1145/3637528.3671877)|Siwei Zhang, Xi Chen, Yun Xiong, Xixi Wu, Yao Zhang, Yongrui Fu, Yinglong Zhao, Jiawei Zhang|; Ant Group, Shanghai, China; IFM Lab, Department of Computer Science, University of California, Davis, Davis, CA, USA|Temporal Graph Networks (TGNs) have demonstrated their remarkable performance in modeling temporal interaction graphs. These works can generate temporal node representations by encoding the surrounding neighborhoods for the target node. However, an inherent limitation of existing TGNs is their reliance onfixed, hand-crafted rules for neighborhood encoding, overlooking the necessity for an adaptive and learnable neighborhood that can accommodate both personalization and temporal evolution across different timestamps. In this paper, we aim to enhance existing TGNs by introducing anadaptive neighborhood encoding mechanism. We present SEAN (Selective Encoding for Adaptive Neighborhood), a flexible plug-and-play model that can be seamlessly integrated with existing TGNs, effectively boosting their performance. To achieve this, we decompose the adaptive neighborhood encoding process into two phases: (i) representative neighbor selection, and (ii) temporal-aware neighborhood information aggregation. Specifically, we propose the Representative Neighbor Selector component, which automatically pinpoints the most important neighbors for the target node. It offers a tailored understanding of each node's unique surrounding context, facilitating personalization. Subsequently, we propose a Temporal-aware Aggregator, which synthesizes neighborhood aggregation by selectively determining the utilization of aggregation routes and decaying the outdated information, allowing our model to adaptively leverage both the contextually significant and current information during aggregation. We conduct extensive experiments by integrating SEAN into three representative TGNs, evaluating their performance on four public datasets and one financial benchmark dataset introduced in this paper. The results demonstrate that SEAN consistently leads to performance improvements across all models, achieving SOTA performance and exceptional robustness.|时态图网络(TGNs)在时态交互图建模方面表现出了显著的性能。这些工作可以通过编码目标节点的周围邻域来产生时间节点表示。然而，现有 TGN 的一个固有局限性是它们依赖于固定的手工制作的邻域编码规则，忽视了自适应和可学习的邻域的必要性，这种邻域可以容纳跨不同时间戳的个性化和时间演变。本文通过引入一种自适应邻域编码机制来增强现有的 TGNs。我们提出了 SEAN (自适应邻域选择性编码) ，一个灵活的即插即用模型，可以无缝集成到现有的 TGN，有效地提高他们的性能。为了实现这一目标，我们将自适应邻域编码过程分解为两个阶段: (i)代表性邻域选择阶段和(ii)时间感知邻域信息聚合阶段。具体来说，我们提出了代表性邻居选择器组件，它自动精确定位目标节点的最重要的邻居。它提供了对每个节点独特的周围环境的定制理解，促进了个性化。随后，我们提出了一个时间感知聚合器，它通过选择性地确定聚合路径的利用率和衰减过时的信息来综合邻域聚合，允许我们的模型在聚合过程中适应性地利用上下文重要性和当前信息。本文通过将 SEAN 集成到三个具有代表性的 TGNs 中，在四个公共数据集和一个金融基准数据集上进行了广泛的实验，评价了它们的性能。结果表明，SEAN 在所有模型中始终如一地导致性能改进，实现了 SOTA 性能和出色的鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Adaptive+Neighborhood+for+Advancing+Temporal+Interaction+Graph+Modeling)|0|
|[Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional Networks](https://doi.org/10.1145/3637528.3671665)|Weijia Zhang, Le Zhang, Jindong Han, Hao Liu, Yanjie Fu, Jingbo Zhou, Yu Mei, Hui Xiong|Baidu Research, Beijing, China; HKUST(GZ) & HKUST, Guangzhou, China; HKUST(GZ), Guangzhou, China; Arizona State University, Phoenix, USA; Baidu Inc., Beijing, China; HKUST, Hong Kong, China|Accurate traffic forecasting is crucial for the development of Intelligent Transportation Systems (ITS), playing a pivotal role in modern urban traffic management. Traditional forecasting methods, however, struggle with the irregular traffic time series resulting from adaptive traffic signal controls, presenting challenges in asynchronous spatial dependency, irregular temporal dependency, and predicting variable-length sequences. To this end, we propose an Asynchronous Spatio-tEmporal graph convolutional nEtwoRk (ASeer) tailored for irregular traffic time series forecasting. Specifically, we first propose an Asynchronous Graph Diffusion Network to capture the spatial dependency between asynchronously measured traffic states regulated by adaptive traffic signals. After that, to capture the temporal dependency within irregular traffic state sequences, a personalized time encoding is devised to embed the continuous time signals. Then, we propose a Transformable Time-aware Convolution Network, which adapts meta-filters for time-aware convolution on the sequences with inconsistent temporal flow. Additionally, a Semi-Autoregressive Prediction Network, comprising a state evolution unit and a semiautoregressive predictor, is designed to predict variable-length traffic sequences effectively and efficiently. Extensive experiments on a newly established benchmark demonstrate the superiority of ASeer compared with twelve competitive baselines across six metrics.|准确的交通量预测是智能交通系统(ITS)发展的关键，在现代城市交通管理中起着举足轻重的作用。然而，传统的交通流预测方法很难克服自适应交通信号控制产生的不规则交通流时间序列，在异步空间依赖性、不规则时间依赖性以及预测变长序列等方面都面临着挑战。为此，我们提出了一种适用于不规则交通时间序列预测的异步时空图卷积网络(ASeer)。具体来说，我们首先提出一个异步图扩散网络来捕获由自适应交通信号调节的异步测量交通状态之间的空间依赖关系。然后，为了捕获不规则交通状态序列中的时间相关性，设计了一种个性化的时间编码方法来嵌入连续的时间信号。然后，提出了一种可变时间感知卷积网络，该网络对时间流不一致的序列采用元滤波器进行时间感知卷积。此外，设计了一个由状态演化单元和半自回归预测器组成的半自回归预测网络，以有效地预测变长业务序列。在一个新建立的基准上进行的大量实验表明，与横跨6个指标的十二个具有竞争力的基准相比，ASeer 具有优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Irregular+Traffic+Time+Series+Forecasting+Based+on+Asynchronous+Spatio-Temporal+Graph+Convolutional+Networks)|0|
|[A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist](https://doi.org/10.1145/3637528.3671801)|Wentao Zhang, Lingxuan Zhao, Haochong Xia, Shuo Sun, Jiaze Sun, Molei Qin, Xinyi Li, Yuqing Zhao, Yilei Zhao, Xinyu Cai, Longtao Zheng, Xinrun Wang, Bo An|Nanyang Technological University, Singapore, Singapore; National Technological University, Singapore, Singapore; Nanyang Technological University & Skywork AI, Singapore, Singapore; Zhejiang University, Hangzhou, China; National University of Singapore, Singapore, Singapore|Financial trading is a crucial component of the markets, informed by a multimodal information landscape encompassing news, prices, and Kline charts, and encompasses diverse tasks such as quantitative trading and high-frequency trading with various assets. While advanced AI techniques like deep learning and reinforcement learning are extensively utilized in finance, their application in financial trading tasks often faces challenges due to inadequate handling of multimodal data and limited generalizability across various tasks. To address these challenges, we present FinAgent, a multimodal foundational agent with tool augmentation for financial trading. FinAgent's market intelligence module processes a diverse range of data-numerical, textual, and visual-to accurately analyze the financial market. Its unique dual-level reflection module not only enables rapid adaptation to market dynamics but also incorporates a diversified memory retrieval system, enhancing the agent's ability to learn from historical data and improve decision-making processes. The agent's emphasis on reasoning for actions fosters trust in its financial decisions. Moreover, FinAgent integrates established trading strategies and expert insights, ensuring that its trading approaches are both data-driven and rooted in sound financial principles. With comprehensive experiments on 6 financial datasets, including stocks and Crypto, FinAgent significantly outperforms 12 state-of-the-art baselines in terms of 6 financial metrics with over 36% average improvement on profit. Specifically, a 92.27% return (a 84.39% relative improvement) is achieved on one dataset. Notably, FinAgent is the first advanced multimodal foundation agent designed for financial trading tasks.|金融交易是市场的一个重要组成部分，由包括新闻、价格和克莱恩图表在内的多模式信息环境提供信息，并包括各种各样的任务，如定量交易和各种资产的高频交易。尽管深度学习和强化学习等先进的人工智能技术在金融领域得到广泛应用，但由于多模式数据处理不足以及各种任务的普遍性有限，它们在金融交易任务中的应用往往面临挑战。为了应对这些挑战，我们介绍了 FinAgent，一个多模式的基础代理，为金融交易提供了工具增强。FinAgent 的市场情报模块处理各种各样的数据——数字、文本和可视化——以准确地分析金融市场。其独特的双层反射模块不仅能够快速适应市场动态，而且还包括一个多样化的记忆检索系统，增强代理人从历史数据中学习和改进决策过程的能力。代理人强调对行为的推理，从而培养了对其财务决策的信任。此外，FinAgent 整合了既定的交易战略和专家见解，确保其交易方法既是数据驱动的，又植根于健全的财务原则。通过对包括股票和 Crypto 在内的6个财务数据集的全面实验，FinAgent 在6个财务指标方面显著优于12个最先进的基线，平均利润提高超过36% 。具体来说，在一个数据集上实现了92.27% 的回报率(相对提高了84.39%)。值得注意的是，FinAgent 是第一个为金融交易任务设计的高级多模式基础代理。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Multimodal+Foundation+Agent+for+Financial+Trading:+Tool-Augmented,+Diversified,+and+Generalist)|0|
|[Geometric View of Soft Decorrelation in Self-Supervised Learning](https://doi.org/10.1145/3637528.3671914)|Yifei Zhang, Hao Zhu, Zixing Song, Yankai Chen, Xinyu Fu, Ziqiao Meng, Piotr Koniusz, Irwin King|; Data61, CSIRO, Canberra, Australia; CSIRO, Sydney, Australia; The Chinese University of Hong Kong, Hong Kong, China|Contrastive learning, a form of Self-Supervised Learning (SSL), typically consists of an alignment term and a regularization term. The alignment term minimizes the distance between the embeddings of a positive pair, while the regularization term prevents trivial solutions and expresses prior beliefs about the embeddings. As a widely used regularization technique, soft decorrelation has been employed by several non-contrastive SSL methods to avoid trivial solutions. While the decorrelation term is designed to address the issue of dimensional collapse, we find that it fails to achieve this goal theoretically and experimentally. Based on such a finding, we extend the soft decorrelation regularization to minimize the distance between the covariance matrix and an identity matrix. We provide a new perspective on the geometric distance between positive definite matrices to investigate why the soft decorrelation cannot efficiently solve the dimensional collapse. Furthermore, we construct a family of loss functions utilizing the Bregman Matrix Divergence (BMD), with the soft decorrelation representing a specific instance within this family. We prove that a loss function (LogDet) in this family can solve the issue of dimensional collapse. Our novel loss functions based on BMD exhibit superior performance compared to the soft decorrelation and other baseline techniques, as demonstrated by experimental results on graph and image datasets.|对比学习是自我监督学习(SSL)的一种形式，通常由一个校准项和一个正则项组成。对齐项使正对的嵌入之间的距离最小，而正则项防止平凡的解和表达关于嵌入的先验信念。软解相关作为一种广泛应用的正则化技术，已被多种非对比 SSL 方法所采用，以避免平凡的解。去相关项用于解决量纲坍缩问题，但在理论和实验上均未能达到这一目的。基于这一发现，我们扩展了软去相关正则化，以最小化协方差矩阵与恒等矩阵之间的距离。本文从正定矩阵之间的几何距离出发，研究了软去相关不能有效解决维数塌陷问题的原因。此外，我们利用 Bregman 矩阵散度(BMD)构造了一个损失函数族，其中软去相关表示该族中的一个特定实例。证明了这一族中的一个损失函数(LogDet)可以解决维数崩溃问题。我们的新型基于骨密度的损失函数表现出优于软去相关和其他基线技术的性能，如图形和图像数据集的实验结果所证明的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Geometric+View+of+Soft+Decorrelation+in+Self-Supervised+Learning)|0|
|[Representation Learning of Geometric Trees](https://doi.org/10.1145/3637528.3671688)|Zheng Zhang, Allen Zhang, Ruth Nelson, Giorgio Ascoli, Liang Zhao|Emory University, Atlanta, GA, USA; Georgia Institute of Technology, Atlanta, GA, USA; George Mason University, Fairfax, VA, USA; Yale University, New Haven, CT, USA|Geometric trees are characterized by their tree-structured layout and spatially constrained nodes and edges, which significantly impacts their topological attributes. This inherent hierarchical structure plays a crucial role in domains such as neuron morphology and river geomorphology, but traditional graph representation methods often overlook these specific characteristics of tree structures. To address this, we introduce a new representation learning framework tailored for geometric trees. It first features a unique message passing neural network, which is both provably geometrical structure-recoverable and rotation-translation invariant. To address the data label scarcity issue, our approach also includes two innovative training targets that reflect the hierarchical ordering and geometric structure of these geometric trees. This enables fully self-supervised learning without explicit labels. We validate our method's effectiveness on eight real-world datasets, demonstrating its capability to represent geometric trees.|几何树木的拥有属性是树状结构，节点和边缘在空间上受到限制，这对它们的拓扑属性有很大的影响。这种固有的层次结构在神经元形态学和河流地貌学等领域起着至关重要的作用，但传统的图表示方法往往忽视了树状结构的这些特殊性。为了解决这一问题，我们提出了一种适合于几何树的表示学习框架。它首先具有唯一的消息传递神经网络，既是可证明的几何结构可恢复的，又是旋转平移不变的。为了解决数据标签稀缺性问题，我们的方法还包括两个创新的训练目标，反映这些几何树的层次排序和几何结构。这使得完全自我监督的学习没有明确的标签。我们验证了该方法在八个实际数据集上的有效性，证明了其表示几何树的能力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Representation+Learning+of+Geometric+Trees)|0|
|[Rethinking Graph Backdoor Attacks: A Distribution-Preserving Perspective](https://doi.org/10.1145/3637528.3671910)|Zhiwei Zhang, Minhua Lin, Enyan Dai, Suhang Wang|The Pennsylvania State University, State College, PA, USA|Graph Neural Networks (GNNs) have shown remarkable performance in varioustasks. However, recent works reveal that GNNs are vulnerable to backdoorattacks. Generally, backdoor attack poisons the graph by attaching backdoortriggers and the target class label to a set of nodes in the training graph. AGNN trained on the poisoned graph will then be misled to predict test nodesattached with trigger to the target class. Despite their effectiveness, ourempirical analysis shows that triggers generated by existing methods tend to beout-of-distribution (OOD), which significantly differ from the clean data.Hence, these injected triggers can be easily detected and pruned with widelyused outlier detection methods in real-world applications. Therefore, in thispaper, we study a novel problem of unnoticeable graph backdoor attacks within-distribution (ID) triggers. To generate ID triggers, we introduce an OODdetector in conjunction with an adversarial learning strategy to generate theattributes of the triggers within distribution. To ensure a high attack successrate with ID triggers, we introduce novel modules designed to enhance triggermemorization by the victim model trained on poisoned graph. Extensiveexperiments on real-world datasets demonstrate the effectiveness of theproposed method in generating in distribution triggers that can by-pass variousdefense strategies while maintaining a high attack success rate.|图形神经网络(GNN)在各种任务中表现出显著的性能。然而，最近的工作表明，GNN 是脆弱的后门攻击。一般来说，后门攻击通过将后门触发器和目标类标签附加到训练图中的一组节点上来破坏图。对中毒图进行训练的 AGNN 将被误导以预测连接到目标类的触发器的测试节点。尽管有效，但实证分析表明，现有方法产生的触发器倾向于超出分布(OOD) ，这与清洁数据有显著差异。因此，这些注入触发器可以很容易地被检测到，并在现实应用中被广泛使用的异常检测方法进行修剪。因此，本文研究了一个新的不易被察觉的分布式(ID)触发器图后门攻击问题。为了生成 ID 触发器，我们引入了一个面向对象的检测器和一个对抗性学习策略来生成分布中触发器的属性。为了保证 ID 触发器具有较高的攻击成功率，我们引入了新的模块，通过在中毒图上训练的受害者模型来增强触发记忆。在实际数据集上的大量实验证明了该方法在生成分布式触发器方面的有效性，该触发器可以绕过各种防御策略，同时保持较高的攻击成功率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rethinking+Graph+Backdoor+Attacks:+A+Distribution-Preserving+Perspective)|0|
|[Learning Flexible Time-windowed Granger Causality Integrating Heterogeneous Interventional Time Series Data](https://doi.org/10.1145/3637528.3672023)|Ziyi Zhang, Shaogang Ren, Xiaoning Qian, Nick Duffield|Texas A&M University, College Station, Texas, USA; Texas A&M University & Brookhaven National Laboratory, College Station, Texas, USA|Granger causality, commonly used for inferring causal structures from timeseries data, has been adopted in widespread applications across various fieldsdue to its intuitive explainability and high compatibility with emerging deepneural network prediction models. To alleviate challenges in better decipheringcausal structures unambiguously from time series, the use of interventionaldata has become a practical approach. However, existing methods have yet to beexplored in the context of imperfect interventions with unknown targets, whichare more common and often more beneficial in a wide range of real-worldapplications. Additionally, the identifiability issues of Granger causalitywith unknown interventional targets in complex network models remain unsolved.Our work presents a theoretically-grounded method that infers Granger causalstructure and identifies unknown targets by leveraging heterogeneousinterventional time series data. We further illustrate that learning Grangercausal structure and recovering interventional targets can mutually promoteeach other. Comparative experiments demonstrate that our method outperformsseveral robust baseline methods in learning Granger causal structure frominterventional time series data.|格兰杰因果关系，通常用于从时间序列数据推断因果结构，由于其直观的解释性和与新兴的深层神经网络预测模型的高度兼容性，已被广泛应用于各个领域。为了减轻从时间序列中更好地解释因果结构的挑战，使用干预数据已经成为一种实用的方法。然而，现有的方法尚未在不完善的干预措施和未知目标的背景下进行探索，这些干预措施在现实世界的广泛应用中更为常见，而且往往更为有益。此外，在复杂网络模型中，未知干预目标的 Granger 因果关系的可识别性问题仍然没有得到解决。我们的工作提出了一个理论基础的方法，推断格兰杰因果结构和识别未知目标利用异质干预时间序列数据。我们进一步说明了学习格兰杰因果结构和恢复干预目标可以相互促进。对比实验表明，该方法在学习常规时间序列数据的格兰杰因果结构方面优于几种稳健的基线方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Flexible+Time-windowed+Granger+Causality+Integrating+Heterogeneous+Interventional+Time+Series+Data)|0|
|[Algorithmic Fairness Generalization under Covariate and Dependence Shifts Simultaneously](https://doi.org/10.1145/3637528.3671909)|Chen Zhao, Kai Jiang, Xintao Wu, Haoliang Wang, Latifur Khan, Christan Grant, Feng Chen|University of Florida, Gainesville, FL, USA; University of Arkansas, Fayetteville, AR, USA; The University of Texas at Dallas, Richardson, TX, USA; The University of Texas, Dallas, Richardson, TX, USA; Baylor University, Waco, TX, USA|The endeavor to preserve the generalization of a fair and invariantclassifier across domains, especially in the presence of distribution shifts,becomes a significant and intricate challenge in machine learning. In responseto this challenge, numerous effective algorithms have been developed with afocus on addressing the problem of fairness-aware domain generalization. Thesealgorithms are designed to navigate various types of distribution shifts, witha particular emphasis on covariate and dependence shifts. In this context,covariate shift pertains to changes in the marginal distribution of inputfeatures, while dependence shift involves alterations in the joint distributionof the label variable and sensitive attributes. In this paper, we introduce asimple but effective approach that aims to learn a fair and invariantclassifier by simultaneously addressing both covariate and dependence shiftsacross domains. We assert the existence of an underlying transformation modelcan transform data from one domain to another, while preserving the semanticsrelated to non-sensitive attributes and classes. By augmenting varioussynthetic data domains through the model, we learn a fair and invariantclassifier in source domains. This classifier can then be generalized tounknown target domains, maintaining both model prediction and fairnessconcerns. Extensive empirical studies on four benchmark datasets demonstratethat our approach surpasses state-of-the-art methods.|在机器学习中，如何保持分类器的公平性和不变性，特别是在分布偏移的情况下，成为一个重要而复杂的挑战。为了应对这一挑战，许多有效的算法已经开发出来，重点是解决公平意识的领域泛化问题。这些算法被设计用于导航各种类型的分布移位，特别强调协变量和依赖性移位。在这种情况下，协变量变化涉及输入特征边缘分布的变化，而依赖性变化涉及标签变量和敏感属性的联合分布的变化。本文介绍了一种简单而有效的方法，通过同时处理协变量和依赖域间的移位来学习一个公平且不变的分类器。我们断言底层转换模型的存在可以将数据从一个域转换到另一个域，同时保留与非敏感属性和类相关的语义。通过该模型对多个合成数据域进行扩充，在源域中学习了一个公平的、不变的分类器。该分类器可以广义化未知目标域，同时保持模型预测和公平性。对四个基准数据集的大量实证研究表明，我们的方法超越了最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Algorithmic+Fairness+Generalization+under+Covariate+and+Dependence+Shifts+Simultaneously)|0|
|[VertiMRF: Differentially Private Vertical Federated Data Synthesis](https://doi.org/10.1145/3637528.3671771)|Fangyuan Zhao, Zitao Li, Xuebin Ren, Bolin Ding, Shusen Yang, Yaliang Li|Xi'an Jiaotong University, Xi'an, China; Alibaba Group, Bellevue, WA, USA|Data synthesis is a promising solution to share data for various downstream analytic tasks without exposing raw data. However, without a theoretical privacy guarantee, a synthetic dataset would still leak some sensitive information in raw data. As a countermeasure, differential privacy is widely adopted to safeguard data synthesis by strictly limiting the released information. This technique is advantageous yet presents significant challenges in the vertical federated setting, where data attributes are distributed among different data parties. The main challenge lies in maintaining privacy while efficiently and precisely reconstructing the correlation between attributes. In this paper, we propose a novel algorithm called VertiMRF, designed explicitly for generating synthetic data in the vertical setting and providing differential privacy protection for all information shared from data parties. We introduce techniques based on the Flajolet-Martin (FM) sketch for encoding local data satisfying differential privacy and estimating cross-party marginals. We provide theoretical privacy and utility proof for encoding in this multi-attribute data. Collecting the locally generated private Markov Random Field (MRF) and the sketches, a central server can reconstruct a global MRF, maintaining the most useful information. Two critical techniques introduced in our VertiMRF are dimension reduction and consistency enforcement, preventing the noise of FM sketch from overwhelming the information of attributes with large domain sizes when building the global MRF. These two techniques allow flexible and inconsistent binning strategies of local private MRF and the data sketching module, which can preserve information to the greatest extent. We conduct extensive experiments on four real-world datasets to evaluate the effectiveness of VertiMRF. End-to-end comparisons demonstrate the superiority of VertiMRF.|数据合成是一个很有前途的解决方案，可以在不暴露原始数据的情况下共享各种下游分析任务的数据。但是，如果没有理论上的隐私保证，合成数据集仍然会泄露原始数据中的一些敏感信息。作为一种对策，差分隐私被广泛采用，通过严格限制公布的信息来保护数据合成。这种技术很有优势，但是在垂直联邦设置中面临重大挑战，在垂直联邦设置中，数据属性分布在不同的数据方之间。主要的挑战在于维护隐私，同时有效和准确地重建属性之间的关联。在本文中，我们提出了一种名为 VertiMRF 的新算法，该算法明确地设计用于在垂直环境下生成合成数据，并为数据方共享的所有信息提供差分隐私保护。我们引入了基于 Flajolet-Martin (FM)草图的技术，用于编码满足差分隐私和估计跨党派边际的本地数据。我们为这种多属性数据的编码提供了理论上的隐私性和实用性证明。收集本地生成的专用马尔可夫网络(MRF)和草图，中央服务器可以重建全局 MRF，保持最有用的信息。在我们的 VertiMRF 中引入的两个关键技术是维度减化和一致性强制，防止 FM 草图的噪音淹没了大域大小的属性信息，当构建全局 MRF 时。这两种技术允许本地私有 MRF 和数据草图模块采用灵活且不一致的分组策略，最大限度地保留信息。我们在四个真实世界的数据集上进行了广泛的实验来评估 VertiMRF 的有效性。端到端的比较证明了 VertiMRF 的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=VertiMRF:+Differentially+Private+Vertical+Federated+Data+Synthesis)|0|
|[Pre-Training and Prompting for Few-Shot Node Classification on Text-Attributed Graphs](https://doi.org/10.1145/3637528.3671952)|Huanjing Zhao, Beining Yang, Yukuo Cen, Junyu Ren, Chenhui Zhang, Yuxiao Dong, Evgeny Kharlamov, Shu Zhao, Jie Tang|University of Edinburgh, Edinburgh, United Kingdom; Bosch Center for Artifcial Intelligence, Renningen, Germany; Anhui University, Hefei, Anhui, China; Zhipu AI, Beijing, China; Tsinghua University, Beijing, China; Software Engineering, Tsinghua University, Beijing, China|The text-attributed graph (TAG) is one kind of important real-world graph-structured data with each node associated with raw texts. For TAGs, traditional few-shot node classification methods directly conduct training on the pre-processed node features and do not consider the raw texts. The performance is highly dependent on the choice of the feature pre-processing method. In this paper, we propose P2TAG, a framework designed for few-shot node classification on TAGs with graph pre-training and prompting. P2TAG first pre-trains the language model (LM) and graph neural network (GNN) on TAGs with self-supervised loss. To fully utilize the ability of language models, we adapt the masked language modeling objective for our framework. The pre-trained model is then used for the few-shot node classification with a mixed prompt method, which simultaneously considers both text and graph information. We conduct experiments on six real-world TAGs, including paper citation networks and product co-purchasing networks. Experimental results demonstrate that our proposed framework outperforms existing graph few-shot learning methods on these datasets with +18.98% ~ +32.14% improvements.|文本属性图(TAG)是一种重要的现实图形结构数据，其中每个节点都与原始文本相关联。对于标签，传统的少镜头节点分类方法直接对预处理的节点特征进行训练，而不考虑原始文本。特征预处理方法的选择对系统的性能有很大的影响。在本文中，我们提出了 P2TAG，一个基于图预训练和提示的少镜头节点分类框架。P2TAG 首先在带有自监督损失的 TAG 上预训练语言模型(LM)和图神经网络(GNN)。为了充分利用语言模型的能力，我们将掩蔽语言建模目标应用到我们的框架中。然后利用预训练模型，采用混合提示方法对少镜头节点进行分类，该方法同时考虑了文本和图形信息。我们在六个真实世界的标签上进行实验，包括论文引用网络和产品联合采购网络。实验结果表明，我们提出的框架在这些数据集上的性能优于现有的图形少镜头学习方法，提高了 + 18.98% ~ + 32.14% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pre-Training+and+Prompting+for+Few-Shot+Node+Classification+on+Text-Attributed+Graphs)|0|
|[Conformalized Link Prediction on Graph Neural Networks](https://doi.org/10.1145/3637528.3672061)|Tianyi Zhao, Jian Kang, Lu Cheng|University of Southern California, Los Angeles, CA, USA; University of Illinois Chicago, Chicago, IL, USA; University of Rochester, Rochester, NY, USA|Graph Neural Networks (GNNs) excel in diverse tasks, yet their applications in high-stakes domains are often hampered by unreliable predictions. Although numerous uncertainty quantification methods have been proposed to address this limitation, they often lackrigorous uncertainty estimates. This work makes the first attempt to introduce a distribution-free and model-agnostic uncertainty quantification approach to construct a predictive interval with a statistical guarantee for GNN-based link prediction. We term it asconformalized link prediction. Our approach builds upon conformal prediction (CP), a framework that promises to construct statistically robust prediction sets or intervals. There are two primary challenges: first, given dependent data like graphs, it is unclear whether the critical assumption in CP --- exchangeability --- still holds when applied to link prediction. Second, even if the exchangeability assumption is valid for conformalized link prediction, we need to ensure high efficiency, i.e., the resulting prediction set or the interval length is small enough to provide useful information. To tackle these challenges, we first theoretically and empirically establish a permutation invariance condition for the application of CP in link prediction tasks, along with an exact test-time coverage. Leveraging the important structural information in graphs, we then identify a novel and crucial connection between a graph's adherence to the power law distribution and the efficiency of CP. This insight leads to the development of a simple yet effective sampling-based method to align the graph structure with a power law distribution prior to the standard CP procedure. Extensive experiments demonstrate that for conformalized link prediction, our approach achieves the desired marginal coverage while significantly improving the efficiency of CP compared to baseline methods.|图形神经网络(GNN)在不同的任务中表现出色，但它们在高风险领域的应用往往受到不可靠预测的阻碍。尽管已经提出了许多不确定性量化方法来解决这一局限性，但它们往往缺乏严格的不确定性估计。本文首次尝试引入一种无分布和模型无关的不确定性量化方法来构造一个具有统计保证的基于 GNN 的链路预测预测区间。我们称之为共形链路预测。我们的方法建立在保形预测(CP)的基础上，保形预测是一个承诺构建统计稳健预测集或区间的框架。这里有两个主要的挑战: 首先，考虑到依赖数据如图表，目前还不清楚 CP 中的关键假设——可交换性——在应用于链接预测时是否仍然成立。其次，即使可交换性假设对于共形链路预测是有效的，我们也需要确保高效率，即所得到的预测集或区间长度足够小以提供有用的信息。为了解决这些问题，我们首先从理论和经验上建立了 CP 在链路预测任务中应用的置换不变性条件，以及精确的测试时间覆盖率。然后，利用图中的重要结构信息，我们确定了图对幂律分布的依从性和 CP 的效率之间的一个新颖而关键的联系。这种见解导致了一种简单而有效的基于抽样的方法的发展，以便在标准 CP 程序之前将图结构与幂律分布对齐。大量的实验表明，对于共形链路预测，我们的方法达到了预期的边缘覆盖，同时显著提高了 CP 的效率相比，基线方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Conformalized+Link+Prediction+on+Graph+Neural+Networks)|0|
|[GeoMix: Towards Geometry-Aware Data Augmentation](https://doi.org/10.1145/3637528.3671700)|Wentao Zhao, Qitian Wu, Chenxiao Yang, Junchi Yan||Mixup has shown considerable success in mitigating the challenges posed by limited labeled data in image classification. By synthesizing samples through the interpolation of features and labels, Mixup effectively addresses the issue of data scarcity. However, it has rarely been explored in graph learning tasks due to the irregularity and connectivity of graph data. Specifically, in node classification tasks, Mixup presents a challenge in creating connections for synthetic data. In this paper, we propose Geometric Mixup (GeoMix), a simple and interpretable Mixup approach leveraging in-place graph editing. It effectively utilizes geometry information to interpolate features and labels with those from the nearby neighborhood, generating synthetic nodes and establishing connections for them. We conduct theoretical analysis to elucidate the rationale behind employing geometry information for node Mixup, emphasizing the significance of locality enhancement-a critical aspect of our method's design. Extensive experiments demonstrate that our lightweight Geometric Mixup achieves state-of-the-art results on a wide variety of standard datasets with limited labeled data. Furthermore, it significantly improves the generalization capability of underlying GNNs across various challenging out-of-distribution generalization tasks. Our code is available at https://github.com/WtaoZhao/geomix.|在缓解图像分类中有限的标记数据所带来的挑战方面，Mixup 已经取得了相当大的成功。通过特征和标签的插值合成样本，混合有效地解决了数据稀缺的问题。然而，由于图形数据的不规则性和连通性，它在图形学习任务中很少被探讨。具体来说，在节点分类任务中，Mixup 在为合成数据创建连接方面提出了挑战。在本文中，我们提出了几何混合(GeoMix) ，一个简单的和可解释的混合方法，利用就地图编辑。它有效地利用几何信息来插值特征和标签来自附近的邻居，生成合成节点和建立连接。我们进行理论分析，以阐明背后的理论基础使用几何信息的节点混合，强调的意义，局部增强-一个关键的方面，我们的方法的设计。大量的实验表明，我们的轻量级几何混合实现了国家的最先进的结果在各种各样的标准数据集与有限的标签数据。此外，它显著提高了底层 GNN 在各种具有挑战性的分布外泛化任务中的泛化能力。我们的代码可以在 https://github.com/wtaozhao/geomix 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GeoMix:+Towards+Geometry-Aware+Data+Augmentation)|0|
|[Spuriousness-Aware Meta-Learning for Learning Robust Classifiers](https://doi.org/10.1145/3637528.3672006)|Guangtao Zheng, Wenqian Ye, Aidong Zhang|University of Virginia, Charlottesville, VA, USA|Spurious correlations are brittle associations between certain attributes of inputs and target variables, such as the correlation between an image background and an object class. Deep image classifiers often leverage them for predictions, leading to poor generalization on the data where the correlations do not hold. Mitigating the impact of spurious correlations is crucial towards robust model generalization, but it often requires annotations of the spurious correlations in data -- a strong assumption in practice. In this paper, we propose a novel learning framework based on meta-learning, termed SPUME -- SPUriousness-aware MEta-learning, to train an image classifier to be robust to spurious correlations. We design the framework to iteratively detect and mitigate the spurious correlations that the classifier excessively relies on for predictions. To achieve this, we first propose to utilize a pre-trained vision-language model to extract text-format attributes from images. These attributes enable us to curate data with various class-attribute correlations, and we formulate a novel metric to measure the degree of these correlations' spuriousness. Then, to mitigate the reliance on spurious correlations, we propose a meta-learning strategy in which the support (training) sets and query (test) sets in tasks are curated with different spurious correlations that have high degrees of spuriousness. By meta-training the classifier on these spuriousness-aware meta-learning tasks, our classifier can learn to be invariant to the spurious correlations. We demonstrate that our method is robust to spurious correlations without knowing them a priori and achieves the best on five benchmark datasets with different robustness measures. Our code is available at https://github.com/gtzheng/SPUME.|伪相关是指输入的某些属性与目标变量之间的脆弱关联，例如图像背景与目标类之间的相关性。深度图像分类器经常利用它们进行预测，导致在相关性不存在的情况下，对数据的概括性较差。减轻虚假相关性的影响对于健壮的模型泛化至关重要，但是它通常需要对数据中的虚假相关性进行注释——这在实践中是一个强有力的假设。本文提出了一种新的基于元学习的图像分类器学习框架 SPUME —— SPUriness 感知元学习，以训练图像分类器对伪相关的鲁棒性。我们设计这个框架来迭代地检测和减轻分类器过度依赖于预测的伪相关性。为了实现这一目标，我们首先提出利用一个预先训练好的视觉语言模型来从图像中提取文本格式属性。这些属性使我们能够用各种类属性相关性来管理数据，并且我们制定了一个新的度量来衡量这些相关性的虚假程度。然后，为了减轻对伪相关性的依赖，我们提出了一种元学习策略，其中任务中的支持(训练)集和查询(测试)集被策划为具有高度伪相关性的不同伪相关性。通过对分类器进行元训练，我们的分类器可以学会对虚假关联保持不变。我们证明了我们的方法是鲁棒的虚假相关性不知道他们的先验，并取得了最佳的五个基准数据集与不同的鲁棒性措施。我们的代码可以在 https://github.com/gtzheng/spume 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Spuriousness-Aware+Meta-Learning+for+Learning+Robust+Classifiers)|0|
|[SiGeo: Sub-One-Shot NAS via Geometry of Loss Landscape](https://doi.org/10.1145/3637528.3671712)|Hua Zheng, KuangHung Liu, Igor Fedorov, Xin Zhang, WenYen Chen, Wei Wen|Northeastern University, Boston, MA, USA; Meta, Menlo Park, CA, USA|Neural Architecture Search (NAS) has become a widely used tool for automating neural network design. While one-shot NAS methods have successfully reduced computational requirements, they often require extensive training. On the other hand, zero-shot NAS utilizes training-free proxies to evaluate a candidate architecture's test performance but has two limitations: (1) inability to use the information gained as a network improves with training and (2) unreliable performance, particularly in complex domains like RecSys, due to the multi-modal data inputs and complex architecture configurations. To synthesize the benefits of both methods, we introduce a "sub-one-shot" paradigm that serves as a bridge between zero-shot and one-shot NAS. In sub-one-shot NAS, the supernet is trained using only a small subset of the training data, a phase we refer to as "warm-up." Within this framework, we present SiGeo, a proxy founded on a novel theoretical framework that connects the supernet warm-up with the efficacy of the proxy. Extensive experiments have consistently shown that SiGeo, when properly warmed up, surpasses state-of-the-art NAS proxies in many established NAS benchmarks in the computer vision domain. Furthermore, when tested on recommendation system benchmarks, SiGeo demonstrates its ability to match the performance of state-of-the-art weight-sharing one-shot NAS methods while significantly reducing computational costs by approximately 60%.|神经网络结构搜索(NAS)已经成为神经网络自动化设计的一个广泛应用的工具。虽然一次性 NAS 方法成功地减少了计算需求，但是它们通常需要大量的训练。另一方面，零拍 NAS 利用无训练代理来评估候选架构的测试性能，但有两个限制: (1)无法使用随着训练而改善的网络获得的信息; (2)由于多模态数据输入和复杂的架构配置，特别是在诸如 RecSys 这样的复杂领域，性能不可靠。为了综合这两种方法的优点，我们引入了一个“子一击”范例，作为零击和一击 NAS 之间的桥梁。在一次性 NAS 中，超网只使用训练数据的一小部分进行训练，这个阶段我们称之为“预热”在这个框架内，我们提出了 SiGeo，一个建立在一个新的理论框架上的代理，该框架将超级网络的热身与代理的功效联系起来。大量的实验一致表明，SiGeo 在适当预热时，在计算机视觉领域的许多已建立的 NAS 基准中超越了最先进的 NAS 代理。此外，当在推荐系统基准上进行测试时，SiGeo 证明了其能够匹配最先进的权重共享一次性 NAS 方法的性能，同时显著降低大约60% 的计算成本。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SiGeo:+Sub-One-Shot+NAS+via+Geometry+of+Loss+Landscape)|0|
|[Relaxing Continuous Constraints of Equivariant Graph Neural Networks for Broad Physical Dynamics Learning](https://doi.org/10.1145/3637528.3671957)|Zinan Zheng, Yang Liu, Jia Li, Jianhua Yao, Yu Rong|; Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Tencent AI Lab, Shenzhen, China|Incorporating Euclidean symmetries (e.g. rotation equivariance) as inductive biases into graph neural networks has improved their generalization ability and data efficiency in unbounded physical dynamics modeling. However, in various scientific and engineering applications, the symmetries of dynamics are frequently discrete due to the boundary conditions. Thus, existing GNNs either overlook necessary symmetry, resulting in suboptimal representation ability, or impose excessive equivariance, which fails to generalize to unobserved symmetric dynamics. In this work, we propose a general Discrete Equivariant Graph Neural Network (DEGNN) that guarantees equivariance to a given discrete point group. Specifically, we show that such discrete equivariant message passing could be constructed by transforming geometric features into permutation-invariant embeddings. Through relaxing continuous equivariant constraints, DEGNN can employ more geometric feature combinations to approximate unobserved physical object interaction functions. Two implementation approaches of DEGNN are proposed based on ranking or pooling permutation-invariant functions. We apply DEGNN to various physical dynamics, ranging from particle, molecular, crowd to vehicle dynamics. In twenty scenarios, DEGNN significantly outperforms existing state-of-the-art approaches. Moreover, we show that DEGNN is data efficient, learning with less data, and can generalize across scenarios such as unobserved orientation.|将欧氏对称(如旋转等方差)作为归纳偏差引入到图形神经网络中，提高了无界物理动力学建模的泛化能力和数据效率。然而，在各种科学和工程应用中，由于边界条件的限制，动力学的对称性往往是离散的。因此，现有的 GNN 要么忽视必要的对称性，导致次优表示能力，要么施加过多的等方差，这不能推广到未观察到的对称动力学。在这项工作中，我们提出了一个广义的离散等变图神经网络(DEGNN) ，保证等方差给定的离散点群。具体地说，我们证明了这样的离散等变消息传递可以通过将几何特征转换为置换不变嵌入来构造。通过放松连续等变约束，DEGNN 可以采用更多的几何特征组合来逼近未观测到的物理对象相互作用函数。提出了两种基于排序或池置换不变函数的 DEGNN 实现方法。我们将 DEGNN 应用于各种物理动力学，从粒子、分子、群体到车辆动力学。在二十种情况下，DEGNN 显著优于现有的最先进的方法。此外，我们表明，DEGNN 是数据高效率，学习与较少的数据，并可以推广的情况下，如未观察到的方向。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Relaxing+Continuous+Constraints+of+Equivariant+Graph+Neural+Networks+for+Broad+Physical+Dynamics+Learning)|0|
|[LogParser-LLM: Advancing Efficient Log Parsing with Large Language Models](https://doi.org/10.1145/3637528.3671810)|Aoxiao Zhong, Dengyao Mo, Guiyang Liu, Jinbu Liu, Qingda Lu, Qi Zhou, Jiesheng Wu, Quanzheng Li, Qingsong Wen|CAMCA, Harvard Medical School, Massachusetts General Hospital, Boston, MA, USA; Harvard University & Alibaba Group, Cambridge, MA, USA; Alibaba Group, Bellevue, WA, USA; Alibaba Group, Hangzhou, China|Logs are ubiquitous digital footprints, playing an indispensable role in system diagnostics, security analysis, and performance optimization. The extraction of actionable insights from logs is critically dependent on the log parsing process, which converts raw logs into structured formats for downstream analysis. Yet, the complexities of contemporary systems and the dynamic nature of logs pose significant challenges to existing automatic parsing techniques. The emergence of Large Language Models (LLM) offers new horizons. With their expansive knowledge and contextual prowess, LLMs have been transformative across diverse applications. Building on this, we introduce LogParser-LLM, a novel log parser integrated with LLM capabilities. This union seamlessly blends semantic insights with statistical nuances, obviating the need for hyper-parameter tuning and labeled training data, while ensuring rapid adaptability through online parsing. Further deepening our exploration, we address the intricate challenge of parsing granularity, proposing a new metric and integrating human interactions to allow users to calibrate granularity to their specific needs. Our method's efficacy is empirically demonstrated through evaluations on the Loghub-2k and the large-scale LogPub benchmark. In evaluations on the LogPub benchmark, involving an average of 3.6 million logs per dataset across 14 datasets, our LogParser-LLM requires only 272.5 LLM invocations on average, achieving a 90.6% F1 score for grouping accuracy and an 81.1% for parsing accuracy. These results demonstrate the method's high efficiency and accuracy, outperforming current state-of-the-art log parsers, including pattern-based, neural network-based, and existing LLM-enhanced approaches.|日志是无处不在的数字足迹，在系统诊断、安全分析和性能优化中起着不可或缺的作用。从日志中提取可操作的见解严重依赖于日志解析过程，该过程将原始日志转换为结构化格式以进行下游分析。然而，当代系统的复杂性和日志的动态特性对现有的自动解析技术提出了重大挑战。大型语言模型(LLM)的出现提供了新的视野。LLM 凭借其广博的知识和上下文能力，在各种应用中发挥了变革性的作用。在此基础上，我们引入 LogParser-LLM，这是一种集成了 LLM 功能的新型日志解析器。这种结合无缝地融合了语义洞察力和统计细微差别，避免了超参数调整和标记训练数据的需要，同时通过在线解析确保了快速适应性。进一步深化了我们的探索，我们解决了解析粒度的复杂挑战，提出了一个新的度量标准并集成了人类交互，以允许用户根据他们的具体需求校准粒度。通过对 Loghub-2k 和大规模 LogPub 基准的评估，我们的方法的有效性得到了经验证明。在对 LogPub 基准的评估中，涉及14个数据集中每个数据集平均360万个日志，我们的 LogParser-LLM 平均只需要272.5个 LLM 调用，分组准确率达到90.6% 的 F1分数，解析准确率达到81.1% 。这些结果证明了该方法的高效率和准确性，优于目前最先进的日志解析器，包括基于模式、基于神经网络和现有的 LLM 增强方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LogParser-LLM:+Advancing+Efficient+Log+Parsing+with+Large+Language+Models)|0|
|[BitLINK: Temporal Linkage of Address Clusters in Bitcoin Blockchain](https://doi.org/10.1145/3637528.3672037)|Sheng Zhong, Abdullah Mueen|The University of New Mexico, Albuquerque, NM, USA|In the Bitcoin blockchain, an entity (e.g., a gambling service) may control multiple distinct address clusters. Links (i.e., trust relationships) between these disjoint address clusters can be established when one cluster is abandoned, and a new one is formed shortly thereafter. To link the clusters across time, we have developed a deep neural network model that exploits these synchronous actions derived from unlabeled data in a self-supervised manner. This model assesses whether two clusters exhibit synchronous temporal signatures indicative of a shared entity ownership. We validated our model on 26 real-world entities identified by WalletExplorer [36]. In addition to the existing knowledge, our analysis revealed more transaction history by linking address clusters for three major services: HelixMixer, Primedice, and Bitcoin Fog, as well as 60 other services. This enables us to address questions related to the revenue and expenditures of these services and create informative aggregate statistics. Readers can find code and data on our support website: http://www.bitlinkwallet.com.|在比特币区块链中，一个实体(例如，赌博服务)可能控制多个不同的地址集群。这些不相交的地址集群之间的链接(即信任关系)可以在一个集群被放弃时建立起来，不久之后就会形成一个新的集群。为了跨时间链接集群，我们开发了一个深度神经网络模型，利用这些来自未标记数据的同步行动，以自我监督的方式。这个模型评估两个集群是否表现出表明共享实体所有权的同步时间签名。我们在 WalletExplorer 标识的26个实际实体上验证了我们的模型[36]。除了现有的知识，我们的分析揭示了更多的交易历史，通过链接地址集群的三个主要服务: HelixMixer，Primedice，比特币雾，以及其他60个服务。这使我们能够处理与这些服务的收入和支出有关的问题，并创建信息丰富的总体统计数据。读者可以在我们的支援网站找到代码及资料:  http://www.bitlinkwallet.com。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BitLINK:+Temporal+Linkage+of+Address+Clusters+in+Bitcoin+Blockchain)|0|
|[Efficient and Effective Implicit Dynamic Graph Neural Network](https://doi.org/10.1145/3637528.3672026)|Yongjian Zhong, Hieu Vu, Tianbao Yang, Bijaya Adhikari|Department of Computer Science, University of Iowa, Iowa City, IA, USA; Department of Computer Science and Engineering, Texas A&M University, College Station, TX, USA|Implicit graph neural networks have gained popularity in recent years as they capture long-range dependencies while improving predictive performance in static graphs. Despite the tussle between performance degradation due to the oversmoothing of learned embeddings and long-range dependency being more pronounced in dynamic graphs, as features are aggregated both across neighborhood and time, no prior work has proposed an implicit graph neural model in a dynamic setting. In this paper, we present Implicit Dynamic Graph Neural Network (IDGNN) a novel implicit neural network for dynamic graphs which is the first of its kind. A key characteristic of IDGNN is that it demonstrably is well-posed, i.e., it is theoretically guaranteed to have a fixed-point representation. We then demonstrate that the standard iterative algorithm often used to train implicit models is computationally expensive in our dynamic setting as it involves computing gradients, which themselves have to be estimated in an iterative manner. To overcome this, we pose an equivalent bilevel optimization problem and propose an efficient single-loop training algorithm that avoids iterative computation by maintaining moving averages of key components of the gradients. We conduct extensive experiments on real-world datasets on both classification and regression tasks to demonstrate the superiority of our approach over state-of-the-art baselines. We also demonstrate that our bi-level optimization framework maintains the performance of the expensive iterative algorithm while obtaining up to 1600x speed-up.|近年来，隐式图神经网络由于在提高静态图预测性能的同时捕获长期依赖关系而得到了广泛的应用。尽管由于学习嵌入的过度平滑导致的性能下降和长程依赖在动态图中更加明显，因为特征是跨邻域和时间聚合的，但是没有先前的工作在动态环境中提出隐式图神经模型。本文提出了一种新的用于动态图的隐式神经网络——隐式动态图神经网络(IDGNN)。IDGNN 的一个关键特征是它明显是适定的，也就是说，它在理论上保证具有一个不动点表示。然后，我们证明了常用于训练隐式模型的标准迭代算法在我们的动态设置中是计算昂贵的，因为它涉及到计算梯度，它们本身必须以迭代方式估计。为了克服这个问题，我们提出了一个等效的双层最佳化问题，并提出了一个有效的单循环训练算法，通过保持梯度关键部分的移动平均值来避免迭代计算。我们在分类和回归任务上对真实世界的数据集进行了广泛的实验，以证明我们的方法优于最先进的基线。我们还证明，我们的双层优化框架保持了昂贵的迭代算法的性能，同时获得1600倍的加速。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+and+Effective+Implicit+Dynamic+Graph+Neural+Network)|0|
|[CURLS: Causal Rule Learning for Subgroups with Significant Treatment Effect](https://doi.org/10.1145/3637528.3671951)|Jiehui Zhou, Linxiao Yang, Xingyu Liu, Xinyue Gu, Liang Sun, Wei Chen|; State Key Lab of CAD&CG, Zhejiang University, Hangzhou, Zhejiang, China; DAMO Academy, Alibaba Group, Hangzhou, Zhejiang, China|In causal inference, estimating heterogeneous treatment effects (HTE) is critical for identifying how different subgroups respond to interventions, with broad applications in fields such as precision medicine and personalized advertising. Although HTE estimation methods aim to improve accuracy, how to provide explicit subgroup descriptions remains unclear, hindering data interpretation and strategic intervention management. In this paper, we propose CURLS, a novel rule learning method leveraging HTE, which can effectively describe subgroups with significant treatment effects. Specifically, we frame causal rule learning as a discrete optimization problem, finely balancing treatment effect with variance and considering the rule interpretability. We design an iterative procedure based on the minorize-maximization algorithm and solve a submodular lower bound as an approximation for the original. Quantitative experiments and qualitative case studies verify that compared with state-of-the-art methods, CURLS can find subgroups where the estimated and true effects are 16.1% and 13.8% higher and the variance is 12.0% smaller, while maintaining similar or better estimation accuracy and rule interpretability. Code is available at https://osf.io/zwp2k/.|在因果推断中，估计异质治疗效果(HTE)对于确定不同亚组对干预的反应至关重要，在精准医学和个性化广告等领域有着广泛的应用。虽然 HTE 评估方法的目的是提高准确性，但如何提供明确的子群描述仍然不清楚，阻碍了数据解释和战略干预管理。在本文中，我们提出了一种新的规则学习方法 CURLS，利用 HTE，它可以有效地描述具有显著治疗效果的子群。具体来说，我们将因果规则学习视为一个离散优化问题，在处理效果与方差之间进行微妙的平衡，并考虑规则的可解释性。我们设计了一个基于最小化-最大化算法的迭代过程，并且求解了一个次模下界作为原算法的近似。定量实验和定性案例研究证实，与最先进的方法相比，CURLS 可以找到估计效应和真实效应分别高出16.1% 和13.8% ，方差小于12.0% 的亚组，同时保持相似或更好的估计准确性和规则可解释性。密码可于 https://osf.io/zwp2k/索取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CURLS:+Causal+Rule+Learning+for+Subgroups+with+Significant+Treatment+Effect)|0|
|[Neural Collapse Anchored Prompt Tuning for Generalizable Vision-Language Models](https://doi.org/10.1145/3637528.3671690)|Didi Zhu, Zexi Li, Min Zhang, Junkun Yuan, Jiashuo Liu, Kun Kuang, Chao Wu|Tsinghua University, Beijing, China; Zhejiang University, Hangzhou, China|Large-scale vision-language (V-L) models have demonstrated remarkable generalization capabilities for downstream tasks through prompt tuning. However, the mechanisms behind the learned text representations are unknown, limiting further generalization gains, and the limitations are more severe when faced with the prevalent class imbalances seen in web-sourced datasets. Recent advances in the neural collapse (NC) phenomenon of vision-only models suggest that the optimal representation structure is the simplex ETF, which paves the way to study representations in V-L models. In this paper, we make the first attempt to use NC for examining the representations in V-L models via prompt tuning. It is found that NC optimality of text-to-image representations shows a positive correlation with downstream generalizability, which is more severe under class imbalance settings. To improve the representations, we propose Neural-collapse-anchored Prompt Tuning (NPT), a novel method that learns prompts with text and image representations that satisfy the same simplex Equiangular Tight Frame (ETF). NPT incorporates two regularization terms: language-modality collapse and multi-modality isomorphism; and it is compatible with other prompt tuning methods. Extensive experiments show that NPT can consistently help to improve existing prompt tuning techniques across 11 datasets for both balanced and imbalanced settings.|大规模可视化语言(V-L)模型通过及时调优显示了对下游任务的显著泛化能力。然而，学习文本表示背后的机制是未知的，限制了进一步的一般化收益，并且当面对在网络来源数据集中看到的普遍的类不平衡时，局限性更加严重。纯视觉模型的神经崩溃(NC)现象的最新进展表明，最优表示结构是单纯形 ETF，这为研究 V-L 模型中的表示铺平了道路。在本文中，我们首次尝试使用数控技术通过提示调整来检查 V-L 模型中的表示。研究发现，文本-图像表示的 NC 优化与下游泛化能力呈正相关，在类别不平衡设置下，下游泛化能力更强。为了改善表示，我们提出了神经崩溃锚定提示调整(NPT) ，一种新的方法，学习与文本和图像表示提示，满足相同的单纯等角紧框架(ETF)。NPT 包含两个正则化项: 语言-模态崩溃和多模态同构，与其他快速调谐方法兼容。广泛的实验表明，NPT 可以持续地帮助改进现有的11个数据集的平衡和不平衡设置的快速调优技术。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Collapse+Anchored+Prompt+Tuning+for+Generalizable+Vision-Language+Models)|0|
|[Distributed Thresholded Counting with Limited Interaction](https://doi.org/10.1145/3637528.3671868)|Xiaoyi Zhu, Yuxiang Tian, Zengfeng Huang|School of Data Science, Fudan University, Shanghai, China|Problems in the area of distributed computing have been extensively studied. In this paper, we focus on the Distributed Thresholded Counting problem in the coordinator model. In this problem, we have k sites holding their input and communicating with a central coordinator. The coordinator's task is to determine whether the sum of inputs is larger than a threshold. While the communication complexity of this basic problem has been studied for decades, it is still not well understood. Our work considers the worst-case communication cost for an algorithm that uses limited interaction - i.e. a bounded number of rounds of communication. Algorithms in previous research usually need O(łogłog N) or O(k) rounds. In comparison, in the deterministic case, our algorithm achieves optimal communication complexity in only α(k) rounds, where α(k) denotes the inverse Ackermann function and is nearly constant. We also give a randomized algorithm that balances communication, rounds, and error probability.|分布式计算方面的问题已被广泛研究。本文主要研究协调器模型中的分布式阈值计数问题。在这个问题中，我们有 k 个站点保存它们的输入并与一个中心协调器通信。协调器的任务是确定输入的总和是否大于阈值。虽然这个基本问题的通信复杂性已经研究了几十年，但仍然没有得到很好的理解。我们的工作考虑了使用有限交互的算法的最坏情况下的通信成本——也就是有限的几轮通信。在以前的研究中，算法通常需要 O (og og N)或 O (k)轮。相比之下，在确定性情况下，我们的算法只需要 α (k)轮即可达到最佳通信复杂度，其中 α (k)表示反阿克曼函数并且几乎是常数。我们还给出了一个平衡通信、回合和错误概率的随机化算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distributed+Thresholded+Counting+with+Limited+Interaction)|0|
|[Propagation Structure-Aware Graph Transformer for Robust and Interpretable Fake News Detection](https://doi.org/10.1145/3637528.3672024)|Junyou Zhu, Chao Gao, Ze Yin, Xianghua Li, Jürgen Kurths|; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China|The rise of social media has intensified fake news risks, prompting a growing focus on leveraging graph learning methods such as graph neural networks (GNNs) to understand post-spread patterns of news. However, existing methods often produce less robust and interpretable results as they assume that all information within the propagation graph is relevant to the news item, without adequately eliminating noise from engaged users. Furthermore, they inadequately capture intricate patterns inherent in long-sequence dependencies of news propagation due to their use of shallow GNNs aimed at avoiding the over-smoothing issue, consequently diminishing their overall accuracy. In this paper, we address these issues by proposing the Propagation Structure-aware Graph Transformer (PSGT). Specifically, to filter out noise from users within propagation graphs, PSGT first designs a noise-reduction self-attention mechanism based on the information bottleneck principle, aiming to minimize or completely remove the noise attention links among task-irrelevant users. Moreover, to capture multi-scale propagation structures while considering long-sequence features, we present a novel relational propagation graph as a position encoding for the graph Transformer, enabling the model to capture both propagation depth and distance relationships of users. Extensive experiments demonstrate the effectiveness, interpretability, and robustness of our PSGT.|社交媒体的兴起加剧了虚假新闻的风险，促使人们越来越关注利用图形学习方法，如图形神经网络(GNN)来理解新闻传播后的模式。然而，现有的方法往往产生不那么健壮和可解释的结果，因为它们假定传播图中的所有信息都与新闻条目相关，而没有充分消除来自参与用户的噪声。此外，它们没有充分捕捉到新闻传播的长序列依赖性所固有的复杂模式，因为它们使用浅层 GNN，目的是避免过于平滑的问题，从而降低了它们的总体准确性。本文通过提出传播结构感知图形变换(PSGT)来解决这些问题。为了从传播图中过滤出用户的噪声，PSGT 首先设计了一种基于信息瓶颈原理的降噪自注意机制，旨在最小化或完全消除任务无关用户之间的噪声注意链。此外，为了在考虑长序列特征的情况下捕获多尺度传播结构，我们提出了一种新的关系传播图作为图形变压器的位置编码，使模型能够同时捕获用户的传播深度和距离关系。大量的实验证明了我们的 PSGT 的有效性、可解释性和鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Propagation+Structure-Aware+Graph+Transformer+for+Robust+and+Interpretable+Fake+News+Detection)|0|
|[ControlTraj: Controllable Trajectory Generation with Topology-Constrained Diffusion Model](https://doi.org/10.1145/3637528.3671866)|Yuanshao Zhu, James Jian Qiao Yu, Xiangyu Zhao, Qidong Liu, Yongchao Ye, Wei Chen, Zijian Zhang, Xuetao Wei, Yuxuan Liang|Xi'an Jiao Tong University & City University of Hong Kong, Xi'an, China; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; University of York, York, United Kingdom; Jilin University & City University of Hong Kong, Jilin, China; The Hong Kong University of Science and Technology (Guangzhou), Guanzhou, China; Southern University of Science and Technology, Shenzhen, China; City University of Hong Kong, Hong Kong, China; Southern University of Science and Technology & City University of Hong Kong, Shenzhen, China|Generating trajectory data is among promising solutions to addressing privacyconcerns, collection costs, and proprietary restrictions usually associatedwith human mobility analyses. However, existing trajectory generation methodsare still in their infancy due to the inherent diversity and unpredictabilityof human activities, grappling with issues such as fidelity, flexibility, andgeneralizability. To overcome these obstacles, we propose ControlTraj, aControllable Trajectory generation framework with the topology-constraineddiffusion model. Distinct from prior approaches, ControlTraj utilizes adiffusion model to generate high-fidelity trajectories while integrating thestructural constraints of road network topology to guide the geographicaloutcomes. Specifically, we develop a novel road segment autoencoder to extractfine-grained road segment embedding. The encoded features, along with tripattributes, are subsequently merged into the proposed geographic denoising UNetarchitecture, named GeoUNet, to synthesize geographic trajectories from whitenoise. Through experimentation across three real-world data settings,ControlTraj demonstrates its ability to produce human-directed, high-fidelitytrajectory generation with adaptability to unexplored geographical contexts.|生成轨迹数据是解决隐私问题、收集成本和通常与人员流动性分析相关的专利限制的有希望的解决方案之一。然而，由于人类活动固有的多样性和不可预测性，现有的轨迹生成方法仍处于起步阶段，需要解决诸如忠实性、灵活性和普遍性等问题。为了克服这些障碍，我们提出了基于拓扑约束扩散模型的 ControlTraj 可控轨迹生成框架。与以前的方法不同，ControlTraj 利用扩散模型来生成高保真轨迹，同时整合道路网络拓扑的结构约束来指导地理结果。具体来说，我们开发了一种新的路段自动编码器来提取细粒度的路段嵌入。编码后的特征和贡献值随后被合并到提出的地理去噪 UNetArchitecture 中，命名为 GeoUNet，以便从白噪声中合成地理轨迹。通过对三个真实世界数据设置的实验，ControlTraj 展示了其生成人为导向的高保真轨迹生成的能力，并能适应未探索的地理环境。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ControlTraj:+Controllable+Trajectory+Generation+with+Topology-Constrained+Diffusion+Model)|0|
|[One Fits All: Learning Fair Graph Neural Networks for Various Sensitive Attributes](https://doi.org/10.1145/3637528.3672029)|Yuchang Zhu, Jintang Li, Yatao Bian, Zibin Zheng, Liang Chen|Sun Yat-sen University, Guangzhou, China; Tencent AI Lab, Shenzhen, China|Recent studies have highlighted fairness issues in Graph Neural Networks(GNNs), where they produce discriminatory predictions against specificprotected groups categorized by sensitive attributes such as race and age.While various efforts to enhance GNN fairness have made significant progress,these approaches are often tailored to specific sensitive attributes.Consequently, they necessitate retraining the model from scratch to accommodatechanges in the sensitive attribute requirement, resulting in high computationalcosts. To gain deeper insights into this issue, we approach the graph fairnessproblem from a causal modeling perspective, where we identify the confoundingeffect induced by the sensitive attribute as the underlying reason. Motivatedby this observation, we formulate the fairness problem in graphs from aninvariant learning perspective, which aims to learn invariant representationsacross environments. Accordingly, we propose a graph fairness framework basedon invariant learning, namely FairINV, which enables the training of fair GNNsto accommodate various sensitive attributes within a single training session.Specifically, FairINV incorporates sensitive attribute partition and trainsfair GNNs by eliminating spurious correlations between the label and varioussensitive attributes. Experimental results on several real-world datasetsdemonstrate that FairINV significantly outperforms state-of-the-art fairnessapproaches, underscoring its effectiveness. Our code is available via:https://github.com/ZzoomD/FairINV/.|最近的研究强调了图形神经网络(GNNs)中的公平性问题，在这些问题中，它们对按照种族和年龄等敏感属性分类的特定受保护群体产生歧视性预测。尽管增强 GNN 公平性的各种努力取得了重大进展，但这些方法往往是针对特定的敏感属性而量身定制的。因此，他们需要从头开始重新训练模型，以适应敏感属性需求的变化，从而导致高计算成本。为了深入了解这一问题，我们从因果建模的角度探讨了图的公平性问题，其中我们确定了由敏感属性引起的混杂效应作为潜在的原因。基于这一观察，我们从不变学习的角度提出了图的公平性问题，目的是学习环境中的不变表示。因此，我们提出了一个基于不变学习的图公平性框架 FairINV，该框架使公平 GNN 的训练能够在一个训练会话中容纳各种敏感属性。特别地，FairINV 通过消除标签和各种敏感属性之间的伪相关性，整合了敏感属性分区和 train sfair GNN。在几个真实世界数据集上的实验结果表明，FairINV 显著优于最先进的公平性方法，强调了其有效性。我们的代码可以通过以下 https://github.com/zzoomd/fairinv/获得:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=One+Fits+All:+Learning+Fair+Graph+Neural+Networks+for+Various+Sensitive+Attributes)|0|
|[Topology-monitorable Contrastive Learning on Dynamic Graphs](https://doi.org/10.1145/3637528.3671777)|Zulun Zhu, Kai Wang, Haoyu Liu, Jintang Li, Siqiang Luo|Nanyang Technological University, Singapore, Singapore; Sun Yat-Sen University, Guangzhou, China|Graph contrastive learning is a representative self-supervised graph learning that has demonstrated excellent performance in learning node representations. Despite the extensive studies on graph contrastive learning models, most existing models are tailored to static graphs, hindering their application to real-world graphs which are often dynamically evolving. Directly applying these models to dynamic graphs brings in severe efficiency issues in repetitively updating the learned embeddings. To address this challenge, we propose IDOL, a novel contrastive learning framework for dynamic graph representation learning. IDOL conducts the graph propagation process based on a specially designed Personalized PageRank algorithm which can capture the topological changes incrementally. This effectively eliminates heavy recomputation while maintaining high learning quality. Our another main design is a topology-monitorable sampling strategy which lays the foundation of graph contrastive learning. We further show that the design in IDOL achieves a desired performance guarantee. Our experimental results on multiple dynamic graphs show that IDOL outperforms the strongest baselines on node classification tasks in various performance metrics.|图形对比学习是一种有代表性的自监督图形学习，在学习节点表示方面表现出了优异的性能。尽管对图形对比学习模型进行了广泛的研究，但大多数现有的模型都是针对静态图形的，这阻碍了它们在动态演化的现实世界图形中的应用。将这些模型直接应用于动态图中，会带来重复更新学习嵌入的严重效率问题。为了解决这一问题，我们提出了一种新的动态图表示学习对比学习框架 IDOL。IDOL 基于专门设计的个性化 PageRank 算法进行图传播过程，该算法可以逐步捕获拓扑变化。这有效地消除了重复计算，同时保持了高学习质量。我们的另一个主要设计是一个拓扑可监控的抽样策略，它为图的对比学习奠定了基础。我们进一步表明，IDOL 的设计达到了预期的性能保证。我们在多个动态图表上的实验结果表明，IDOL 在各种性能指标上都优于节点分类任务的最强基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Topology-monitorable+Contrastive+Learning+on+Dynamic+Graphs)|0|
|[MacroHFT: Memory Augmented Context-aware Reinforcement Learning On High Frequency Trading](https://doi.org/10.1145/3637528.3672064)|Chuqiao Zong, Chaojie Wang, Molei Qin, Lei Feng, Xinrun Wang, Bo An|Skywork AI, Singapore, Singapore; Singapore University of Technology and Design, Singapore, Singapore; Nanyang Technological University, Singapore, Singapore|High-frequency trading (HFT) that executes algorithmic trading in short time scales, has recently occupied the majority of cryptocurrency market. Besides traditional quantitative trading methods, reinforcement learning (RL) has become another appealing approach for HFT due to its terrific ability of handling high-dimensional financial data and solving sophisticated sequential decision-making problems, e.g., hierarchical reinforcement learning (HRL) has shown its promising performance on second-level HFT by training a router to select only one sub-agent from the agent pool to execute the current transaction. However, existing RL methods for HFT still have some defects: 1) standard RL-based trading agents suffer from the overfitting issue, preventing them from making effective policy adjustments based on financial context; 2) due to the rapid changes in market conditions, investment decisions made by an individual agent are usually one-sided and highly biased, which might lead to significant loss in extreme markets. To tackle these problems, we propose a novel Memory Augmented Context-aware Reinforcement learning method On HFT, a.k.a. MacroHFT, which consists of two training phases: 1) we first train multiple types of sub-agents with the market data decomposed according to various financial indicators, specifically market trend and volatility, where each agent owns a conditional adapter to adjust its trading policy according to market conditions; 2) then we train a hyper-agent to mix the decisions from these sub-agents and output a consistently profitable meta-policy to handle rapid market fluctuations, equipped with a memory mechanism to enhance the capability of decision-making. Extensive experiments on various cryptocurrency markets demonstrate that MacroHFT can achieve state-of-the-art performance on minute-level trading tasks. Code has been released in https://github.com/ZONG0004/MacroHFT.|在短时间内执行算法交易的高频交易(hFT)最近占据了加密货币市场的大部分份额。除了传统的定量交易方法外，强化学习交易(HRL)由于其处理高维金融数据和解决复杂的顺序决策问题的卓越能力，已成为高频交易的另一个有吸引力的方法，例如，分层强化学习(HRL)已经显示出其在二级高频交易中的良好表现，通过训练路由器只从代理池中选择一个子代理来执行当前的交易。然而，现有的高频交易 RL 方法仍然存在一些缺陷: 1)标准的基于 RL 的交易代理受到过度拟合问题的困扰，无法根据金融环境进行有效的政策调整; 2)由于市场环境的迅速变化，单个代理人的投资决策往往是片面的、高度偏颇的，这可能导致极端市场中的重大损失。为了解决这些问题，我们提出了一种新的记忆增强上下文感知强化学习方法在高频交易中，即 MacroHFT，它由两个训练阶段组成: 1)我们首先训练多种类型的子代理，根据不同的财务指标，特别是市场趋势和波动性分解市场数据，其中每个代理拥有一个条件适配器来根据市场条件调整其交易政策; 2)然后我们训练一个超代理混合这些子代理的决定，输出一个持续盈利的元政策来处理快速市场波动，配备一个记忆机制来提高决策。在各种加密货币市场上的大量实验表明，MacroHFT 可以在分钟级交易任务上实现最先进的性能。代码已经在 https://github.com/zong0004/macrohft 中发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MacroHFT:+Memory+Augmented+Context-aware+Reinforcement+Learning+On+High+Frequency+Trading)|0|
|[Lessons Learned while Running ML Models in Harsh Environments](https://doi.org/10.1145/3637528.3672499)|Pedro Bizarro|Research, Feedzai, Lisboa, Portugal|Once a very large payment processor client told us: 'if we are down for 5 minutes, we open the evening news - so don't screw up'. Processing billions of dollars per day, many financial institutions, need to continuously fight organized crime in the form of transaction fraud, stolen cards, anti-money laundering, account opening fraud, impersonations scams, phishing, and many other exotic and ever changing attacks from organized crime groups worldwide. In fact, it is estimated that in 2023 the global losses in fraud scams and bank fraud reached 485.6 billion. However, in addition to having very good detection rates and very low false positive rates, financial institutions also need to maintain very high availability rates, very low latencies, very high throughputs, automatic fault tolerance, auto scale up and down, and more. In this talk we cover some lessons related to running ML models in harsh, mission critical environments. We describe data issues, scale issues, ethical issues, system issues, security issues, compliance issues, business and regulation issues, and some architectural tradeoffs and architectural evolutions.|有一次，一个非常大的支付处理器客户告诉我们: “如果我们停机5分钟，我们就打开晚间新闻——所以不要搞砸了。”。每天处理数十亿美元的资金，许多金融机构，需要不断地打击有组织犯罪，包括交易欺诈、被盗卡、反洗钱、开户欺诈、模仿诈骗、网络钓鱼，以及来自世界各地有组织犯罪集团的许多其他奇异的、不断变化的攻击。事实上，据估计，2023年全球诈骗和银行诈骗造成的损失高达4856亿美元。然而，除了拥有非常好的检测率和非常低的假阳性率之外，金融机构还需要维持非常低的高可用性、非常低的延迟、非常高的吞吐量、自动容错、自动放大和放小等等。在这个演讲中，我们将介绍一些关于在严酷的任务关键环境中运行机器学习模型的经验教训。我们描述了数据问题、规模问题、道德问题、系统问题、安全问题、法规遵循问题、业务和法规问题，以及一些架构折衷和架构演进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Lessons+Learned+while+Running+ML+Models+in+Harsh+Environments)|0|
|[Next-generation Intelligent Assistants for Wearable Devices](https://doi.org/10.1145/3637528.3672500)|Xin Luna Dong|Northwestern Polytech Univ, Sch Chem & Chem Engn, Shaanxi Key Lab Macromol Sci & Technol, Xian 710072, Shaanxi, Peoples R China; Shaanxi Univ Sci & Technol, Coll Chem & Chem Engn, Key Lab Auxiliary Chem & Technol Chem Ind, Minist Educ,Shaanxi Key Lab Chem Addit Ind, Xian 710021, Shaanxi, Peoples R China|Multifunctional wearable electronic devices based on natural materials are highly desirable for versatile applications of energy conversion, electronic skin and artificial intelligence. Herein, multifunctional wearable silver nanowire decorated leather (AgNW/leather) nanocomposites with hierarchical structures for integrated visual Joule heating, electromagnetic interference (EMI) shielding and piezoresistive sensing are fabricated via the facile vacuum-assisted filtration process. The AgNWs penetrate the micro-nanoporous structures in the corium side of leather constructing highly-efficient conductive networks. The resultant flexible and mechanically strong AgNW/leather nanocomposites exhibit extremely low sheet resistance of 0.8 omega/sq, superior visual Joule heating temperatures up to 108 degrees C at low supplied voltage of 2.0 V due to efficient energy conversion, excellent EMI shielding effectiveness (EMI SE) of approximate to 55 dB and outstanding piezoresistive sensing ability in human motion detection. This work demonstrates the fabrication of multifunctional AgNW/leather nanocomposites for next-generation wearable electronic devices in energy conversion, electronic skin and artificial intelligence, etc.|以天然材料为基础的多功能可穿戴电子设备在能量转换、电子皮肤和人工智能的多种应用中都是非常理想的。在这里，多功能可穿戴银纳米线装饰皮革(AgNW/皮革)纳米复合材料具有分层结构的集成视觉焦耳加热，电磁干扰(EMI)屏蔽和压阻传感通过简单的真空辅助过滤过程。AgNWs 穿透皮革真皮侧的微纳米多孔结构，构成高效的导电网络。由此产生的具有柔韧性和机械强度的 AgNW/皮革纳米复合材料表现出极低的片状电阻，为0.8欧米伽/平方米，优越的视觉焦耳加热温度高达108摄氏度，在2.0 V 的低供电电压下，由于有效的能量转换，优异的电磁干扰屏蔽效果(EMI SE)约为55分贝，并在人类动作感应中具有突出的压阻传感能力。这项工作展示了多功能 AgNW/皮革纳米复合材料的制造，用于下一代可穿戴电子设备在能量转换，电子皮肤和人工智能等。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Next-generation+Intelligent+Assistants+for+Wearable+Devices)|0|
|[Scalable Graph Learning for your Enterprise](https://doi.org/10.1145/3637528.3672501)|Hema Raghavan|Kumo.AI., Inc., Mountain View, CA, USA|Much of the world's most valued data is stored in relational databases and data warehouses, where the data is organized into many tables connected by primary-foreign key relations. However, building machine learning models using this data is both challenging and time consuming. The core problem is that no machine learning method is capable of learning on multiple tables interconnected by primary-foreign key relations. Current methods can only learn from a single table, so the data must first be manually joined and aggregated into a single training table, the process known as feature engineering. Feature engineering is slow, error prone and leads to suboptimal models. At Kumo.ai we have worked with researchers worldwide to develop an end-to-end deep representation learning approach to directly learn on data laid out across multiple tables [1]. We name our approach Relational Deep Learning (RDL). The core idea is to view relational databases as a temporal, heterogeneous graph, with a node for each row in each table, and edges specified by primary-foreign key links. Message Passing Graph Neural Networks can then automatically learn across the graph to extract representations that leverage all input data, without any manual feature engineering. Our relational deep learning method to encode graph structure into low-dimensional embeddings brings several benefits: (1) automatic learning from the entire data spread across multiple relational tables (2) no manual feature engineering as the system learns optimal embeddings for a target problem; (3) state-of-the-art predictive performance.|世界上大部分最有价值的数据都存储在关系数据库和数据仓库中，在这些数据中，数据被组织成许多由主键-外键关系连接的表。然而，利用这些数据建立机器学习模型既具有挑战性又耗费时间。其核心问题是没有一种机器学习方法能够在由主-外键关系相互连接的多个表上进行学习。当前的方法只能从单个表中学习，因此必须首先手动将数据合并并聚合到单个训练表中，这个过程称为特征工程。特征工程是缓慢的，容易出错，并导致次优模型。在 Kumo.ai，我们与世界各地的研究人员合作，开发了一种端到端的深度表示学习方法，可以直接学习多个表格中的数据[1]。我们将这种方法命名为关系深度学习(RDL)。其核心思想是将关系数据库视为一个时态的、异构的图形，每个表中的每一行都有一个节点，边由主-外键链接指定。然后，消息传递图形神经网络可以自动学习整个图形，以提取利用所有输入数据的表示，而不需要任何人工特征工程。我们的关系深度学习方法将图形结构编码为低维嵌入带来了几个好处: (1)从跨多个关系表的整个数据中自动学习(2)不需要手动特征工程，因为系统学习了目标问题的最佳嵌入; (3)最先进的预测性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scalable+Graph+Learning+for+your+Enterprise)|0|
|[Dynamic Pricing for Multi-Retailer Delivery Platforms with Additive Deep Learning and Evolutionary Optimization](https://doi.org/10.1145/3637528.3671634)|Ahmed Abdulaal, Ali Polat, Hari Narayan, Wenrong Zeng, Yimin Yi|Shipt, San Francisco, California, USA; Walmart Global Tech, Sunnyvale, California, USA|Dynamic Pricing for online retail has been discussed extensively in literature. However, past solutions fell short of addressing the unique challenges of independent multi-retailer platforms for grocery delivery. From limited visibility of retailers' inventories to diverse demand-side dynamics across retail brands and locations, the highly decentralized nature of multi-retailer platforms deviates from the classical framework of modeling price elasticity and cross-elasticity of demand. In this paper, we present a novel scheme to scalable and practical price adjustment in the highly dynamic multi-retailer context. First, we present a deep learning framework to distinctly model complex cross-elasticity relationships via additive neural networks augmented with adversarial data. Second, we present evolutionary optimization agents for adjusting itemized prices in a location-decentralized manner, while adhering to custom business constraints and objectives. The optimization utilizes the genetic algorithm structure, where we introduce a potential mechanism, inspired by bandit algorithms, in order to improve convergence speed by managing exploitation and exploration trade-offs. Our solution is deployed at Shipt and is extendable to other types of multi-retailer platforms, such as restaurant delivery. Finally, we empirically demonstrate performance using public and industry datasets of hundreds and thousands of diverse products across tens of stores, offering an optimization targets coverage scale in the tens of thousands, far larger than experimental setups in past research.|网上零售的动态定价在文献中得到了广泛的讨论。然而，过去的解决方案未能解决独立的多零售商平台的杂货配送的独特挑战。从零售商库存的有限可见性到零售品牌和零售地点之间多样化的需求侧动态，多零售商平台的高度分散性背离了建模价格弹性和需求交叉弹性的经典框架。本文在高度动态的多零售商环境下，提出了一种新的可扩展的、实用的价格调整方案。首先，我们提出了一个深度学习框架，通过加性神经网络增强敌对数据来清楚地模拟复杂的交叉弹性关系。其次，我们提出的进化优化代理调整逐项价格在位置分散的方式，同时坚持自定义业务的约束和目标。优化采用遗传算法结构，其中引入了一种潜在的机制，灵感来自于匪徒算法，以提高收敛速度，管理开发和勘探的权衡。我们的解决方案部署在 Shipt，并可扩展到其他类型的多零售商平台，如餐厅配送。最后，我们通过使用数十家商店的成百上千种不同产品的公共和行业数据集来实证性地演示性能，提供了数万个优化目标覆盖范围，远远大于过去研究中的实验设置。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamic+Pricing+for+Multi-Retailer+Delivery+Platforms+with+Additive+Deep+Learning+and+Evolutionary+Optimization)|0|
|[Television Discourse Decoded: Comprehensive Multimodal Analytics at Scale](https://doi.org/10.1145/3637528.3671532)|Anmol Agarwal, Pratyush Priyadarshi, Shiven Sinha, Shrey Gupta, Hitkul Jangra, Ponnurangam Kumaraguru, Kiran Garimella|Indraprastha Institute of Information Technology, Delhi, India; Rutgers University, New Brunswick, USA; International Institute of Information Technology, Hyderabad, India|In this paper, we tackle the complex task of analyzing televised debates, with a focus on a prime time news debate show from India. Previous methods, which often relied solely on text, fall short in capturing the multimodal essence of these debates [27]. To address this gap, we introduce a comprehensive automated toolkit that employs advanced computer vision and speech-to-text techniques for large-scale multimedia analysis. Utilizing state-of-the-art computer vision algorithms and speech-to-text methods, we transcribe, diarize, and analyze thousands of YouTube videos of a prime-time television debate show in India. These debates are a central part of Indian media but have been criticized for compromised journalistic integrity and excessive dramatization [18]. Our toolkit provides concrete metrics to assess bias and incivility, capturing a comprehensive multimedia perspective that includes text, audio utterances, and video frames. Our findings reveal significant biases in topic selection and panelist representation, along with alarming levels of incivility. This work offers a scalable, automated approach for future research in multimedia analysis, with profound implications for the quality of public discourse and democratic debate. To catalyze further research in this area, we also release the code, dataset collected and supplemental pdf.1|在本文中，我们将分析电视辩论这一复杂的任务，重点放在来自印度的黄金时段新闻辩论节目上。以前的方法往往只依赖于文本，不能捕捉这些辩论的多模式本质[27]。为了弥补这一差距，我们引入了一个全面的自动化工具包，它采用了先进的计算机视觉和语音到文本的技术进行大规模的多媒体分析。利用最先进的计算机视觉算法和语音文本转换方法，我们转录、日记和分析了印度黄金时段电视辩论节目的数千个 YouTube 视频。这些辩论是印度媒体的一个核心部分，但却因为损害新闻诚信和过度戏剧化而受到批评[18]。我们的工具包提供了评估偏见和不礼貌的具体指标，捕获了一个全面的多媒体视角，包括文本，音频话语和视频帧。我们的研究结果显示，在选题和小组成员的代表性方面存在明显的偏差，同时还存在令人担忧的不礼貌行为。这项工作为未来的多媒体分析研究提供了一种可扩展的、自动化的方法，对公共话语和民主辩论的质量有着深远的影响。为了促进该领域的进一步研究，我们还发布了代码、收集的数据集和补充 pdf|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Television+Discourse+Decoded:+Comprehensive+Multimodal+Analytics+at+Scale)|0|
|[Large Scale Generative AI Text Applied to Sports and Music](https://doi.org/10.1145/3637528.3671542)|Aaron K. Baughman, Eduardo Morales, Rahul Agarwal, Gozde Akay, Rogério Feris, Tony Johnson, Stephen Hammer, Leonid Karlinsky|IBM, Fredericton, NB, Canada; IBM, New York, NY, USA; IBM, RTP, NC, USA; IBM, Boston, MA, USA; IBM, Atlanta, GA, USA|We address the problem of scaling up the production of media content,including commentary and personalized news stories, for large-scale sports andmusic events worldwide. Our approach relies on generative AI models totransform a large volume of multimodal data (e.g., videos, articles, real-timescoring feeds, statistics, and fact sheets) into coherent and fluent text.Based on this approach, we introduce, for the first time, an AI commentarysystem, which was deployed to produce automated narrations for highlightpackages at the 2023 US Open, Wimbledon, and Masters tournaments. In the samevein, our solution was extended to create personalized content for ESPN FantasyFootball and stories about music artists for the Grammy awards. Theseapplications were built using a common software architecture achieved a 15xspeed improvement with an average Rouge-L of 82.00 and perplexity of 6.6. Ourwork was successfully deployed at the aforementioned events, supporting 90million fans around the world with 8 billion page views, continuously pushingthe bounds on what is possible at the intersection of sports, entertainment,and AI.|我们致力于解决为全球大型体育和音乐活动扩大媒体内容生产的问题，包括评论和个性化新闻报道。我们的方法依赖于生成式人工智能模型将大量的多模态数据(例如，视频、文章、实时提要、统计数据和概况介绍)转换为连贯流畅的文本。基于这种方法，我们首次引入了一个人工智能评论系统，该系统被部署用于为2023年美国公开赛、温布尔登和大师赛的亮点包生成自动解说。同时，我们的解决方案被扩展到为 ESPN FantasyFootball 创建个性化内容，以及为格莱美奖创建音乐艺术家的故事。这些应用程序使用一个通用的软件架构构建，实现了15倍的速度改进，平均 Rouge-L 为82.00，困惑度为6.6。我们的工作成功地部署在上述事件，支持世界各地的9000万球迷与80亿页面浏览量，不断推进的界限，在体育，娱乐和人工智能的交叉点可能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Scale+Generative+AI+Text+Applied+to+Sports+and+Music)|0|
|[LiGNN: Graph Neural Networks at LinkedIn](https://doi.org/10.1145/3637528.3671566)|Fedor Borisyuk, Shihai He, Yunbo Ouyang, Morteza Ramezani, Peng Du, Xiaochen Hou, Chengming Jiang, Nitin Pasumarthy, Priya Bannur, Birjodh Tiwana, Ping Liu, Siddharth Dangi, Daqi Sun, Zhoutao Pei, Xiao Shi, Sirou Zhu, Qianqi Shen, KuangHsuan Lee, David Stein, Baolei Li, Haichao Wei, Amol Ghoting, Souvik Ghosh|LinkedIn, Mountain View, CA, USA|In this paper, we present LiGNN, a deployed large-scale Graph Neural Networks (GNNs) Framework. We share our insight on developing and deployment of GNNs at large scale at LinkedIn. We present a set of algorithmic improvements to the quality of GNN representation learning including temporal graph architectures with long term losses, effective cold start solutions via graph densification, ID embeddings and multi-hop neighbor sampling. We explain how we built and sped up by 7x our large-scale training on LinkedIn graphs with adaptive sampling of neighbors, grouping and slicing of training data batches, specialized shared-memory queue and local gradient optimization. We summarize our deployment lessons and learnings gathered from A/B test experiments. The techniques presented in this work have contributed to an approximate relative improvements of 1% of Job application hearing back rate, 2% Ads CTR lift, 0.5% of Feed engaged daily active users, 0.2% session lift and 0.1% weekly active user lift from people recommendation. We believe that this work can provide practical solutions and insights for engineers who are interested in applying Graph neural networks at large scale.|本文提出了一种已部署的大规模图形神经网络(GNN)框架 LiGNN。我们在 LinkedIn 上分享了我们对大规模开发和部署 GNN 的见解。本文提出了一套改进 GNN 表示学习质量的算法，包括具有长期损失的时间图结构、通过图的密集化、 ID 嵌入和多跳邻居采样的有效冷启动解决方案。我们解释了我们是如何通过对 LinkedIn 图的7倍大规模训练来建立和加快训练速度的，这些训练包括邻居的自适应抽样、训练数据批的分组和切片、专门的共享内存队列和局部梯度优化。我们总结了从 A/B 测试实验中获得的部署经验和教训。这项工作中提出的技术有助于近似相对改善1% 的求职申请听力回复率，2% 的广告点击率提升，0.5% 的订阅每日活跃用户，0.2% 的会话提升和0.1% 的每周活跃用户提升从人们的推荐。我们相信，这项工作可以提供实际的解决方案和见解的工程师谁有兴趣应用图形神经网络的大规模。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LiGNN:+Graph+Neural+Networks+at+LinkedIn)|0|
|[Diffusion Model-based Mobile Traffic Generation with Open Data for Network Planning and Optimization](https://doi.org/10.1145/3637528.3671544)|Haoye Chai, Tao Jiang, Li Yu|; Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China; Chinamobile Research Institute, Beijing, China|With the rapid development of the Fifth Generation Mobile Communication Technology (5G) networks, network planning and optimization have become increasingly crucial. Generating high-fidelity network traffic data can preemptively estimate the network demands of mobile users, which holds potential for network operators to improve network performance. However, the data required by existing generation methods is predominantly inaccessible to the public, resulting in a lack of reproducibility for the models and high deployment costs in practice. In this article, we propose an Open data-based Diffusion model for mobile traffic generation (OpenDiff), where a multi-positive contrastive learning algorithm is designed to construct conditional information for the diffusion model using entirely publicly available satellite remote sensing images, Point of Interest (POI), and population data. The conditional information contains relevant human activities in geographical areas, which can effectively guide the generation of network traffic data. We further design an attention-based fusion mechanism to capture the implicit correlations between network traffic and human activity features, enhancing the model's controllable generation capability. We conduct evaluations on three different cities with varying scales, where experimental results verify that our proposed model outperforms existing methods by 14.36% and 13.05% in terms of generation fidelity and controllability. To further validate the effectiveness of the model, we leverage the generated traffic data to assist the operators with network planning on a real-world network optimization platform of China Mobile Communications Corporation. The source code is available online:https://github.com/impchai/OpenDiff-diffusion-model-with-open-data.|随着第五代移动通信技术(5G)网络的迅速发展，网络规划和优化已经变得越来越重要。生成高保真的网络流量数据可以预先估计移动用户的网络需求，为网络运营商提高网络性能提供了可能。然而，现有生成方法所需的数据大多不为公众所获取，导致模型缺乏可重复性，实际部署成本高昂。提出了一种基于开放数据的移动通信扩散模型 OpenDiff，该模型利用完全公开的卫星遥感图像、兴趣点(POI)和人口数据构造扩散模型的条件信息。条件信息包含了地理区域内相关的人类活动，可以有效地指导网络流量数据的生成。进一步设计了基于注意的融合机制，捕捉网络流量与人类活动特征之间的隐式关联，提高了模型的可控生成能力。我们对三个不同规模的城市进行了评估，实验结果表明，我们提出的模型在生成保真度和可控性方面比现有方法分别提高了14.36% 和13.05% 。为了进一步验证该模型的有效性，我们利用生成的流量数据，在中国移动通信公司的一个实际网络优化平台上协助运营商进行网络规划。源代码可在网上获得:  https://github.com/impchai/opendiff-diffusion-model-with-open-data。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Diffusion+Model-based+Mobile+Traffic+Generation+with+Open+Data+for+Network+Planning+and+Optimization)|0|
|[RareBench: Can LLMs Serve as Rare Diseases Specialists?](https://doi.org/10.1145/3637528.3671576)|Xuanzhong Chen, Xiaohao Mao, Qihan Guo, Lun Wang, Shuyang Zhang, Ting Chen|Chinese Academy of Medical Sciences & Peking Union Medical College, Beijing, China; Tsinghua University, Beijing, China|Generalist Large Language Models (LLMs), such as GPT-4, have shownconsiderable promise in various domains, including medical diagnosis. Rarediseases, affecting approximately 300 million people worldwide, often haveunsatisfactory clinical diagnosis rates primarily due to a lack of experiencedphysicians and the complexity of differentiating among many rare diseases. Inthis context, recent news such as "ChatGPT correctly diagnosed a 4-year-old'srare disease after 17 doctors failed" underscore LLMs' potential, yetunderexplored, role in clinically diagnosing rare diseases. To bridge thisresearch gap, we introduce RareBench, a pioneering benchmark designed tosystematically evaluate the capabilities of LLMs on 4 critical dimensionswithin the realm of rare diseases. Meanwhile, we have compiled the largestopen-source dataset on rare disease patients, establishing a benchmark forfuture studies in this domain. To facilitate differential diagnosis of rarediseases, we develop a dynamic few-shot prompt methodology, leveraging acomprehensive rare disease knowledge graph synthesized from multiple knowledgebases, significantly enhancing LLMs' diagnostic performance. Moreover, wepresent an exhaustive comparative study of GPT-4's diagnostic capabilitiesagainst those of specialist physicians. Our experimental findings underscorethe promising potential of integrating LLMs into the clinical diagnosticprocess for rare diseases. This paves the way for exciting possibilities infuture advancements in this field.|通用型大语言模型(LLM) ，如 GPT-4，在各个领域，包括医学诊断方面都显示出相当大的前景。影响全世界大约3亿人的罕见疾病的临床诊断率往往不能令人满意，这主要是由于缺乏经验丰富的医生和区分许多罕见疾病的复杂性。在这种情况下，最近的新闻，如“ ChatGPT 在17名医生失败后正确诊断了一个4岁的‘罕见疾病’”强调了 LLM 的潜力，但尚未充分探讨，在临床诊断罕见疾病中的作用。为了弥补这一研究差距，我们引入了 RareBench，这是一个开创性的基准，旨在系统地评估 LLM 在罕见疾病领域4个关键方面的能力。与此同时，我们已经汇编了最大的开源数据集关于罕见病患者，建立了一个基准在这个领域的未来研究。为了促进稀有疾病的鑑别诊断，我们开发了动态的少数快速方法，利用从多种知识库综合而成的全面的稀有疾病知识图，显著提高 LLM 的诊断性能。此外，我们提出了一个详尽的比较研究的 GPT-4的诊断能力与那些专科医生。我们的实验结果强调了将 LLM 整合到罕见疾病的临床诊断过程中的潜力。这为该领域未来的进步铺平了道路。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RareBench:+Can+LLMs+Serve+as+Rare+Diseases+Specialists?)|0|
|[MARLP: Time-series Forecasting Control for Agricultural Managed Aquifer Recharge](https://doi.org/10.1145/3637528.3671533)|Yuning Chen, Kang Yang, Zhiyu An, Brady Holder, Luke Paloutzian, Khaled M. Bali, Wan Du|University of California, Agriculture and Natural Resources, Parlier, CA, USA; University of California, Merced, Merced, CA, USA|The rapid decline in groundwater around the world poses a significant challenge to sustainable agriculture. To address this issue, agricultural managed aquifer recharge (Ag-MAR) is proposed to recharge the aquifer by artificially flooding agricultural lands using surface water. Ag-MAR requires a carefully selected flooding schedule to avoid affecting the oxygen absorption of crop roots. However, current Ag-MAR scheduling does not take into account complex environmental factors such as weather and soil oxygen, resulting in crop damage and insufficient recharging amounts. This paper proposes MARLP, the first end-to-end data-driven control system for Ag-MAR. We first formulate Ag-MAR as an optimization problem. To that end, we analyze four-year in-field datasets, which reveal the multi-periodicity feature of the soil oxygen level trends and the opportunity to use external weather forecasts and flooding proposals as exogenous clues for soil oxygen prediction. Then, we design a two-stage forecasting framework. In the first stage, it extracts both the cross-variate dependency and the periodic patterns from historical data to conduct preliminary forecasting. In the second stage, it uses weather-soil and flooding-soil causality to facilitate an accurate prediction of soil oxygen levels. Finally, we conduct model predictive control (MPC) for Ag-MAR flooding. To address the challenge of large action spaces, we devise a heuristic planning module to reduce the number of flooding proposals to enable the search for optimal solutions. Real-world experiments show that MARLP reduces the oxygen deficit ratio by 86.8% while improving the recharging amount in unit time by 35.8%, compared with the previous four years.|世界各地地下水的迅速减少对可持续农业构成了重大挑战。为了解决这一问题，农业管理含水层补给(Ag-MAR)被提议通过利用地表水人为地淹没农田来补给含水层。Ag-MAR 需要一个仔细选择的淹水时间表，以避免影响作物根系的氧吸收。然而，目前的 Ag-MAR 调度没有考虑到复杂的环境因素，如天气和土壤氧气，导致作物损害和补给量不足。本文提出了第一个基于数据驱动的 Ag-MAR 端到端控制系统 MARLP。我们首先把银-最佳化问题系数作为一个参数。为此，我们分析了四年的实地数据集，这些数据集揭示了土壤氧含量趋势的多周期性特征，以及利用外部天气预报和洪水预报作为土壤氧含量预测的外部线索的机会。然后，我们设计了一个两阶段的预测框架。第一阶段从历史数据中提取相关性和周期模式进行初步预测;。在第二阶段，它利用气候-土壤和洪水-土壤的因果关系来促进对土壤氧含量的准确预测。最后，我们对 Ag-MAR 洪水进行了模型预估计控制分析。为了解决大行动空间的挑战，我们设计了一个启发式规划模块，以减少洪泛建议的数量，使寻找最佳解决方案。实际试验结果表明，与前四年相比，MARLP 降低了氧缺乏率86.8% ，同时提高了单位时间内的充电量35.8% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MARLP:+Time-series+Forecasting+Control+for+Agricultural+Managed+Aquifer+Recharge)|0|
|[Time-Aware Attention-Based Transformer (TAAT) for Cloud Computing System Failure Prediction](https://doi.org/10.1145/3637528.3671547)|Lingfei Deng, Yunong Wang, Haoran Wang, Xuhua Ma, Xiaoming Du, Xudong Zheng, Dongrui Wu|Alibaba Cloud, Alibaba Group, Hangzhou, China; Huazhong University of Science and Technology, Wuhan, China|Log-based failure prediction helps identify and mitigate system failures ahead of time, increasing the reliability of cloud elastic computing systems. However, most existing log-based failure prediction approaches only focus on semantic information, and do not make full use of the information contained in the timestamps of log messages. This paper proposes time-aware attention-based transformer (TAAT), a failure prediction approach that extracts semantic and temporal information simultaneously from log messages and their timestamps. TAAT first tokenizes raw log messages into specific exceptions, and then performs: 1) exception sequence embedding that reorganizes the exceptions of each node as an ordered sequence and converts them to vectors; 2) time relation estimation that computes time relation matrices from the timestamps; and, 3) time-aware attention that computes semantic correlation matrices from the exception sequences and then combines them with time relation matrices. Experiments on Alibaba Cloud demonstrated that TAAT achieves an approximately 10% performance improvement compared with the state-of-the-art approaches. TAAT is now used in the daily operation of Alibaba Cloud. Moreover, this paper also releases the real-world cloud computing failure prediction dataset used in our study, which consists of about 2.7 billion syslogs from about 300,000 node controllers during a 4-month period. To our knowledge, this is the largest dataset of its kind, and is expected to be very useful to the community.|基于日志的故障预测有助于提前识别和减轻系统故障，提高云弹性计算系统的可靠性。然而，大多数现有的基于日志的故障预测方法只关注语义信息，并没有充分利用日志消息时间戳中包含的信息。提出了一种基于时间感知的注意力转换器(TAAT)的故障预测方法，该方法同时从日志消息及其时间戳中提取语义和时间信息。TAAT 首先将原始日志消息标记为特定的异常，然后执行: 1)异常序列嵌入，将每个节点的异常重组为有序序列，并将其转换为向量; 2)时间关系估计，从时间戳计算时间关系矩阵; 3)时间感知注意，从异常序列计算语义相关矩阵，然后将它们与时间关系矩阵组合。阿里巴巴云的实验表明，与最先进的方法相比，TAAT 的性能提高了大约10% 。TAAT 现在用于阿里巴巴云的日常运作。此外，本文还发布了我们研究中使用的实际云计算失败预测数据集，该数据集包括来自约30万个节点控制器的约27亿个 syslog，时间为4个月。据我们所知，这是同类中最大的数据集，预计将对社区非常有用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Time-Aware+Attention-Based+Transformer+(TAAT)+for+Cloud+Computing+System+Failure+Prediction)|0|
|[FNSPID: A Comprehensive Financial News Dataset in Time Series](https://doi.org/10.1145/3637528.3671629)|Zihan Dong, Xinyu Fan, Zhiyuan Peng|North Carolina State University, Raleigh, NC, USA; SiChuan University, Chengdu, Sichuan Province, China|Financial market predictions utilize historical data to anticipate future stock prices and market trends. Traditionally, these predictions have focused on the statistical analysis of quantitative factors, such as stock prices, trading volumes, inflation rates, and changes in industrial production. Recent advancements in large language models motivate the integrated financial analysis of both sentiment data, particularly market news, and numerical factors. Nonetheless, this methodology frequently encounters constraints due to the paucity of extensive datasets that amalgamate both quantitative and qualitative sentiment analyses. To address this challenge, we introduce a large-scale financial dataset, namely, Financial News and Stock Price Integration Dataset (FNSPID). It comprises 29.7 million stock prices and 15.7 million time-aligned financial news records for 4,775 S&P500 companies, covering the period from 1999 to 2023, sourced from 4 stock market news websites. We demonstrate that FNSPID excels existing stock market datasets in scale and diversity while uniquely incorporating sentiment information. Through financial analysis experiments on FNSPID, we propose: (1) the dataset's size and quality significantly boost market prediction accuracy; (2) adding sentiment scores modestly enhances performance on the transformer-based model; (3) a reproducible procedure that can update the dataset. Completed work, code, documentation, and examples are available at this http URL. FNSPID offers unprecedented opportunities for the financial research community to advance predictive modeling and analysis.|金融市场预测利用历史数据来预测未来的股票价格和市场趋势。传统上，这些预测侧重于对数量因素的统计分析，如股票价格、交易量、通货膨胀率和工业生产的变化。大型语言模型的最新进展推动了情绪数据(尤其是市场新闻)和数字因素的综合财务分析。尽管如此，由于缺乏将定量和定性情绪分析结合起来的大量数据集，这种方法经常遇到限制。为了应对这一挑战，我们引入了一个大规模的金融数据集，即财经新闻和股票价格整合数据集(FNSPID)。该报告包括来自4个股市新闻网站的4775家标准普尔500指数成分股公司1999年至2023年期间的2970万股股价和1570万条与时间一致的财务新闻记录。我们证明，FNSPID 在规模和多样性方面优于现有的股票市场数据集，同时独特地结合了情绪信息。通过在 FNSPID 上的财务分析实验，我们提出: (1)数据集的规模和质量显著提高了市场预测的准确性; (2)增加情绪得分适度提高了基于变压器的模型的性能; (3)一个可重复的过程，可以更新数据集。已完成的工作、代码、文档和示例可在此 http URL 获得。FNSPID 为金融研究界提供了前所未有的机会来推进预测建模和分析。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FNSPID:+A+Comprehensive+Financial+News+Dataset+in+Time+Series)|0|
|[Transportation Marketplace Rate Forecast Using Signature Transform](https://doi.org/10.1145/3637528.3671637)|Haotian Gu, Xin Guo, Timothy L. Jacobs, Philip M. Kaminsky, Xinyu Li|Middle Mile Marketplace Science; University of California Department of Industrial Engineering & Operations Research; Worldwide Operations Research Science, Amazon.com Inc., Bellevue, WA, USA; University of California, Berkeley, Berkeley, CA, USA|Freight transportation marketplace rates are typically challenging to forecast accurately. In this work, we have developed a novel statistical technique based on signature transforms and have built a predictive and adaptive model to forecast these marketplace rates. Our technique is based on two key elements of the signature transform: one being its universal nonlinearity property, which linearizes the feature space and hence translates the forecasting problem into linear regression, and the other being the signature kernel, which allows for comparing computationally efficiently similarities between time series data. Combined, it allows for efficient feature generation and precise identification of seasonality and regime switching in the forecasting process. An algorithm based on our technique has been deployed by Amazon trucking operations, with far superior forecast accuracy and better interpretability versus commercially available industry models, even during the COVID-19 pandemic and the Ukraine conflict. Furthermore, our technique is able to capture the influence of business cycles and the heterogeneity of the marketplace, improving prediction accuracy by more than fivefold, with an estimated annualized saving of \50 million.|货运市场利率通常难以准确预测。在这项工作中，我们开发了一种新的基于签名变换的统计技术，并建立了一个预测和自适应模型来预测这些市场价格。我们的技术基于签名变换的两个关键要素: 一个是它的通用非线性特性，它线性化了特征空间，从而将预测问题转化为线性回归; 另一个是签名核，它允许在计算上有效地比较时间序列数据之间的相似性。结合，它允许有效的特征生成和精确识别季节性和制度转换在预测过程中。基于我们的技术的算法已经在亚马逊的卡车运输业务中使用，即使在2019冠状病毒疾病大流行和乌克兰冲突期间，与商业化的行业模型相比，它也具有更高的预测准确性和更好的可解释性。此外，我们的技术能够捕捉商业周期和市场异质性的影响，提高预测的准确性五倍以上，估计每年节省5000万。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Transportation+Marketplace+Rate+Forecast+Using+Signature+Transform)|0|
|[Intelligent Agents with LLM-based Process Automation](https://doi.org/10.1145/3637528.3671646)|Yanchu Guan, Dong Wang, Zhixuan Chu, Shiyu Wang, Feiyue Ni, Ruihua Song, Chenyi Zhuang|Renmin University of China, Beijing, China; Ant Group, Hangzhou, China; Zhejiang University, Hangzhou, China|While intelligent virtual assistants like Siri, Alexa, and Google Assistant have become ubiquitous in modern life, they still face limitations in their ability to follow multi-step instructions and accomplish complex goals articulated in natural language. However, recent breakthroughs in large language models (LLMs) show promise for overcoming existing barriers by enhancing natural language processing and reasoning capabilities. Though promising, applying LLMs to create more advanced virtual assistants still faces challenges like ensuring robust performance and handling variability in real-world user commands. This paper proposes a novel LLM-based virtual assistant that can automatically perform multi-step operations within mobile apps based on high-level user requests. The system represents an advance in assistants by providing an end-to-end solution for parsing instructions, reasoning about goals, and executing actions. LLM-based Process Automation (LLMPA) has modules for decomposing instructions, generating descriptions, detecting interface elements, predicting next actions, and error checking. Experiments demonstrate the system completing complex mobile operation tasks in Alipay based on natural language instructions. This showcases how large language models can enable automated assistants to accomplish real-world tasks. The main contributions are the novel LLMPA architecture optimized for app process automation, the methodology for applying LLMs to mobile apps, and demonstrations of multi-step task completion in a real-world environment. Notably, this work represents the first real-world deployment and extensive evaluation of a large language model-based virtual assistant in a widely used mobile application with an enormous user base numbering in the hundreds of millions.|尽管像 Siri、 Alexa 和 Google Assistant 这样的智能虚拟助手在现代生活中已经无处不在，但它们在遵循多步指令和完成用自然语言表达的复杂目标方面仍然面临局限。然而，最近在大型语言模型(LLM)方面的突破表明，通过增强自然语言处理和推理能力，有望克服现有的障碍。虽然 LLM 很有前途，但是应用 LLM 来创建更高级的虚拟助理仍然面临诸如确保健壮的性能和处理现实世界中用户命令的可变性等挑战。提出了一种基于 LLM 的虚拟助手，该虚拟助手可以根据用户的高级请求在移动应用中自动执行多步操作。该系统通过提供解析指令、推理目标和执行操作的端到端解决方案来代表助手的进步。基于 LLM 的过程自动化(LLMPA)具有分解指令、生成描述、检测接口元素、预测下一步操作和错误检查的模块。实验表明，该系统基于自然语言指令在支付宝中完成了复杂的移动操作任务。这展示了大型语言模型如何使自动化助手能够完成真实世界的任务。主要贡献包括为应用程序过程自动化而优化的新型 LLMPA 架构，将 LLM 应用于移动应用程序的方法，以及在现实环境中多步骤任务完成的演示。值得注意的是，这项工作代表了第一个真实世界的部署和广泛的评估大型语言模型为基础的虚拟助手在一个广泛使用的移动应用程序与庞大的用户数以亿计。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Intelligent+Agents+with+LLM-based+Process+Automation)|0|
|[SentHYMNent: An Interpretable and Sentiment-Driven Model for Algorithmic Melody Harmonization](https://doi.org/10.1145/3637528.3671626)|Stephen Hahn, Jerry Yin, Rico Zhu, Weihan Xu, Yue Jiang, Simon Mak, Cynthia Rudin|Duke University, Durham, NC, USA|Music composition and analysis is an inherently creative task, involving a combination of heart and mind. However, the vast majority of algorithmic music models completely ignore the "heart" component of music, resulting in output that often lacks the rich emotional direction found in human-composed music. Models that try to incorporate musical sentiment rely on a "valence-arousal" model, which insufficiently characterizes emotion in two dimensions. Furthermore, existing methods typically adopt a black-box, music agnostic approach, treating music-theoretical and sentimental understanding as a by-product that can be inferred given sufficient data. In this study, we introduce two major novel elements: a nuanced mixture-based representation for musical sentiment, including a web tool to gather data, as well as a sentiment- and theory-driven harmonization model, SentHYMNent. SentHYMNent employs a novel Hidden Markov Model based on both key and chord transitions, as well as sentiment mixtures, to provide a probabilistic framework for learning key modulations and chordal progressions from a given melodic line and sentiment. Furthermore, our approach leverages compositional principles, resulting in a simpler model that significantly reduces computational burden and enhances interpretability compared to current state-of-the-art algorithmic harmonization methods. Importantly, as shown in our experiments, these improvements do not come at the expense of harmonization quality. We also provide a web app where users can upload their own melodies for SentHYMNent to harmonize.|音乐创作和分析是一项内在的创造性任务，涉及心灵和思想的结合。然而，绝大多数的算法音乐模型完全忽略了音乐的“心脏”部分，导致输出往往缺乏丰富的情感方向发现人类作曲的音乐。试图融合音乐情感的模型依赖于“价觉-唤醒”模型，该模型没有充分描述情感的两个维度。此外，现有的方法通常采用黑箱，音乐不可知论的方法，把音乐理论和情感的理解作为一个副产品，可以推断给予足够的数据。在这项研究中，我们介绍了两个主要的小说元素: 一个微妙的混合为基础的音乐情感表示，包括一个网络工具收集数据，以及情感和理论驱动的协调模型，SentHYMNent。SentHYMNent 采用了一种新颖的隐马尔可夫模型，基于键和和弦的过渡，以及情绪的混合，提供了一个概率框架来学习关键调制和和弦进展从一个给定的旋律线和情绪。此外，我们的方法利用组合原则，导致一个更简单的模型，大大减少计算负担和提高可解释性相比，目前的国家最先进的算法协调方法。重要的是，正如我们的实验所显示的那样，这些改进并不以牺牲协调质量为代价。我们还提供了一个网络应用程序，用户可以上传自己的旋律来协调 SentHYMNent。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SentHYMNent:+An+Interpretable+and+Sentiment-Driven+Model+for+Algorithmic+Melody+Harmonization)|0|
|[FedSecurity: A Benchmark for Attacks and Defenses in Federated Learning and Federated LLMs](https://doi.org/10.1145/3637528.3671545)|Shanshan Han, Baturalp Buyukates, Zijian Hu, Han Jin, Weizhao Jin, Lichao Sun, Xiaoyang Wang, Wenxuan Wu, Chulin Xie, Yuhang Yao, Kai Zhang, Qifan Zhang, Yuhui Zhang, Carlee JoeWong, Salman Avestimehr, Chaoyang He|University of California, Irvine, Irvine, CA, USA; Zhejiang University, Hangzhou, China; UIUC, Urbana, IL, USA; Texas A&M University, College Station, TX, USA; TensorOpera Inc., Palo Alto, CA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; University of Southern California, Los Angeles, CA, USA; Lehigh University, Bethlehem, PA, USA|This paper introduces FedSecurity, an end-to-end benchmark that serves as asupplementary component of the FedML library for simulating adversarial attacksand corresponding defense mechanisms in Federated Learning (FL). FedSecurityeliminates the need for implementing the fundamental FL procedures, e.g., FLtraining and data loading, from scratch, thus enables users to focus ondeveloping their own attack and defense strategies. It contains two keycomponents, including FedAttacker that conducts a variety of attacks during FLtraining, and FedDefender that implements defensive mechanisms to counteractthese attacks. FedSecurity has the following features: i) It offers extensivecustomization options to accommodate a broad range of machine learning models(e.g., Logistic Regression, ResNet, and GAN) and FL optimizers (e.g., FedAVG,FedOPT, and FedNOVA); ii) it enables exploring the effectiveness of attacks anddefenses across different datasets and models; and iii) it supports flexibleconfiguration and customization through a configuration file and some APIs. Wefurther demonstrate FedSecurity's utility and adaptability through federatedtraining of Large Language Models (LLMs) to showcase its potential on a widerange of complex applications.|本文介绍了 FedSecurity，这是一个端到端的基准，它作为 FedML 库的补充组件，用于模拟 FL 中的敌对攻击和相应的防御机制。FedSecurity 消除了从头开始实现基本的 FL 过程的需要，例如，FLtraining 和数据加载，从而使用户能够专注于开发自己的攻击和防御策略。它包含两个关键组件，包括在 FLtraining 中进行各种攻击的 FedAtacker 和实现防御机制来抵抗这些攻击的 FedDefender。FedSecurity 有以下特点: i)它提供广泛的定制选项，以适应广泛的机器学习模型(例如，Logit模型、 ResNet 和 GAN)和 FL 优化器(例如，FedAVG、 FedOPT 和 FedNOVA) ; ii)它能够探索不同数据集和模型的攻击和防御的有效性; iii)它通过配置文件和一些 API 支持灵活的配置和定制。通过对大型语言模型(LLM)的联合培训，进一步展示了 FedSecurity 的实用性和适应性，以展示其在广泛的复杂应用程序中的潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedSecurity:+A+Benchmark+for+Attacks+and+Defenses+in+Federated+Learning+and+Federated+LLMs)|0|
|[Paths2Pair: Meta-path Based Link Prediction in Billion-Scale Commercial Heterogeneous Graphs](https://doi.org/10.1145/3637528.3671563)|Jinquan Hang, Zhiqing Hong, Xinyue Feng, Guang Wang, Guang Yang, Feng Li, Xining Song, Desheng Zhang|JD Logistics, Beijing, China; Florida State University, Tallahassee, FL, USA; JD Logistics & Rutgers University, Beijing, China; Rutgers University, Piscataway, NJ, USA|Link prediction, determining if a relation exists between two entities, is an essential task in the analysis of heterogeneous graphs with diverse entities and relations. Despite extensive research in link prediction, most existing works focus on predicting the relation type between given pairs of entities. However, it is almost impractical to check every entity pair when trying to find most hidden relations in a billion-scale heterogeneous graph due to the billion squared number of possible pairs. Meanwhile, most methods aggregate information at the node level, potentially leading to the loss of direct connection information between the two nodes. In this paper, we introduce Paths2Pair, a novel framework to address these limitations for link prediction in billion-scale commercial heterogeneous graphs. (i) First, it selects a subset of reliable entity pairs for prediction based on relevant meta-paths. (ii) Then, it utilizes various types of content information from the meta-paths between each selected entity pair to predict whether a target relation exists. We first evaluate our Paths2Pair based on a large-scale dataset, and results show Paths2Pair outperforms state-of-the-art baselines significantly. We then deploy our Paths2Pair on JD Logistics, one of the largest logistics companies in the world, for business expansion. The uncovered relations by Paths2Pair have helped JD Logistics identify 108,709 contacts to attract new company customers, resulting in an 84% increase in the success rate compared to the state-of-the-practice solution, demonstrating the practical value of our framework. We have released the code of our framework at https://github.com/JQHang/Paths2Pair.|链接预测是确定两个实体之间是否存在关系的一个基本问题，是分析具有不同实体和关系的异构图的一个基本问题。尽管在链接预测方面进行了广泛的研究，但现有的工作大多集中于预测给定对实体之间的关系类型。然而，在十亿尺度的异构图中，由于每个实体对的平方数是十亿个，因此在寻找最隐藏的关系时，检查每个实体对几乎是不切实际的。同时，大多数方法在节点级聚合信息，这可能导致两个节点之间直接连接信息的丢失。在本文中，我们介绍 Paths2Pair，一个新的框架，以解决这些限制，链接预测在十亿尺度的商业异构图。(i)首先根据相关元路径选择可靠实体对的子集进行预测。然后，利用每个选定实体对之间的元路径中的各种类型的内容信息来预测目标关系是否存在。我们首先基于大规模数据集评估 Paths2Pair，结果显示 Paths2Pair 的性能明显优于最先进的基线。然后，我们将 Paths2Pair 部署到 JD 物流公司，这是世界上最大的物流公司之一，用于业务扩展。Paths2Pair 发现的关系已经帮助 JD 物流确定了108,709个联系人来吸引新的公司客户，结果与实践中的解决方案相比，成功率提高了84% ，证明了我们框架的实用价值。我们已经在 https://github.com/jqhang/paths2pair 发布了框架代码。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Paths2Pair:+Meta-path+Based+Link+Prediction+in+Billion-Scale+Commercial+Heterogeneous+Graphs)|0|
|[Distributed Harmonization: Federated Clustered Batch Effect Adjustment and Generalization](https://doi.org/10.1145/3637528.3671590)|Bao Hoang, Yijiang Pang, Siqi Liang, Liang Zhan, Paul M. Thompson, Jiayu Zhou|University of Southern California, Los Angeles, California, USA; University of Pittsburgh, Pittsburgh, Pennsylvania, USA; Michigan State University, East Lansing, Michigan, USA|Independent and identically distributed (i.i.d.) data is essential to many data analysis and modeling techniques. In the medical domain, collecting data from multiple sites or institutions is a common strategy that guarantees sufficient clinical diversity, determined by the decentralized nature of medical data. However, data from various sites are easily biased by the local environment or facilities, thereby violating the i.i.d. rule. A common strategy is to harmonize the site bias while retaining important biological information. The ComBat is among the most popular harmonization approaches and has recently been extended to handle distributed sites. However, when faced with situations involving newly joined sites in training or evaluating data from unknown/unseen sites, ComBat lacks compatibility and requires retraining with data from all the sites. The retraining leads to significant computational and logistic overhead that is usually prohibitive. In this work, we develop a novel Cluster ComBat harmonization algorithm, which leverages cluster patterns of the data in different sites and greatly advances the usability of ComBat harmonization. We use extensive simulation and real medical imaging data from ADNI to demonstrate the superiority of the proposed approach. Our codes are provided in https://github.com/illidanlab/distributed-cluster-harmonization.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distributed+Harmonization:+Federated+Clustered+Batch+Effect+Adjustment+and+Generalization)|0|
|[Explainable and Interpretable Forecasts on Non-Smooth Multivariate Time Series for Responsible Gameplay](https://doi.org/10.1145/3637528.3671657)|Hussain Jagirdar, Rukma Talwadker, Aditya Pareek, Pulkit Agrawal, Tridib Mukherjee|Games24x7, Bengaluru, India|Multi-variate Time Series (MTS) forecasting has made large strides (with very negligible errors) through recent advancements in neural networks, e.g., Transformers. However, in critical situations like predicting gaming overindulgence that affects one's mental well-being; an accurate forecast without a contributing evidence (explanation) is irrelevant. Hence, it becomes important that the forecasts are Interpretable - intermediate representation of the forecasted trajectory is comprehensible; as well as Explainable - attentive input features and events are accessible for a personalized and timely intervention of players at risk. While the contributing state of the art research on interpretability primarily focuses on temporally-smooth single-process driven time series data, our online multi-player gameplay data demonstrates intractable temporal randomness due to intrinsic orthogonality between player's game outcome and their intent to engage further. We introduce a novel deep Actionable Forecasting Network (AFN), which addresses the inter-dependent challenges associated with three exclusive objectives - 1) forecasting accuracy; 2) smooth comprehensible trajectory and 3) explanations via multi-dimensional input features while tackling the challenges introduced by our non-smooth temporal data, together in one single solution. AFN establishes a new benchmark via: (i) achieving 25% improvement on the MSE of the forecasts on player data in comparison to the SOM-VAE based SOTA networks; (ii) attributing unfavourable progression of a player's time series to a specific future time step(s), with the premise of eliminating near-future overindulgent player volume by over 18% with player specific actionable inputs feature(s) and (iii) proactively detecting over 23% (100% jump from SOTA) of the to-be overindulgent, players on an average, 4 weeks in advance.|多变量时间序列(MTS)预测通过神经网络(如变压器)的最新进展已经取得了长足的进步(误差非常小)。然而，在一些关键情况下，比如预测游戏过度会影响一个人的心理健康，没有证据(解释)的准确预测是无关紧要的。因此，重要的是，预测是可解释的——预测轨迹的中间表示是可以理解的; 以及可解释的——注意输入特征和事件是可以获得的，以便风险行为者个性化和及时干预。虽然可解释性的最新研究主要集中在时间平稳的单进程驱动的时间序列数据上，但我们的在线多玩家游戏数据表明，由于玩家的游戏结果与其进一步参与的意图之间的内在正交性，难以处理的时间随机性。我们引入了一个新的深度可操作预测网络(AFN) ，它解决了与三个独有的目标相关的相互依赖的挑战: 1)预测精度; 2)平滑可理解的轨迹和3)通过多维输入特征的解释，同时解决我们的非平滑时间数据引入的挑战，在一个单一的解决方案。AFN 通过以下方式建立了一个新的基准: (i)与基于 SOM-VAE 的 SOTA 网络相比，玩家数据预测的 MSE 提高了25% ; (ii)将玩家时间序列的不利进展归因于特定的未来时间步骤，前提是消除近期过度放纵的玩家数量超过18% ，具有玩家特定的可操作输入特征; (iii)主动检测超过23% (从 SOTA 跳跃100%)的未来过度放纵的玩家，平均提前4周。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explainable+and+Interpretable+Forecasts+on+Non-Smooth+Multivariate+Time+Series+for+Responsible+Gameplay)|0|
|[Decomposed Attention Segment Recurrent Neural Network for Orbit Prediction](https://doi.org/10.1145/3637528.3671546)|Seungwon Jeong, Soyeon Woo, Daewon Chung, Simon S. Woo, Youjin Shin|National Satellite Operation Center Korea Aerospace Research Institute, Daejeon, Republic of Korea; The Catholic University of Korea, Bucheon, Republic of Korea; Sejong University, Seoul, Republic of Korea; Computer Science & Engineering Department, Sungkyunkwan University, Suwon, Republic of Korea|As the focus of space exploration shifts from national agencies to private companies, the interest in space industry has been steadily increasing. With the increasing number of satellites, the risk of collisions between satellites and space debris has escalated, potentially leading to significant property and human losses. Therefore, accurately modeling the orbit is critical for satellite operations. In this work, we propose the Decomposed Attention Segment Recurrent Neural Network (DASR) model, adding two key components, Multi-Head Attention and Tensor Train Decomposition, to SegRNN for orbit prediction. The DASR model applies Multi-Head Attention before segmenting at input data and before the input of the GRU layers. In addition, Tensor Train (TT) Decomposition is applied to the weight matrices of the Multi-Head Attention in both the encoder and decoder. For evaluation, we use three real-world satellite datasets from the Korea Aerospace Research Institute (KARI), which are currently operating: KOMPSAT-3, KOMPSAT-3A, and KOMPSAT-5 satellites. Our proposed model demonstrates superior performance compared to other SOTA baseline models. We demonstrate that our approach has 94.13% higher predictive performance than the second-best model in the KOMPSAT-3 dataset, 89.79% higher in the KOMPSAT-3A dataset, and 76.71% higher in the KOMPSAT-5 dataset.|随着空间探索的重点从国家机构转移到私营公司，人们对空间工业的兴趣一直在稳步增加。随着卫星数量的增加，卫星与空间碎片碰撞的风险已经升级，可能导致重大财产和人员损失。因此，精确的轨道建模对卫星运行至关重要。在这项工作中，我们提出了分解注意分段递归神经网络(DASR)模型，在 SegRNN 中加入两个关键组件: 多头注意和张量列分解，用于轨道预测。DASR 模型在分割输入数据和 GRU 层输入数据之前应用多头注意。此外，本文还将张量训练分解应用于编码器和解码器中的多头注意权矩阵。为了进行评估，我们使用了来自韩国航空宇宙研究院(kARI)的三个现实世界卫星数据集，它们目前正在运行: KOMPSAT-3、 KOMPSAT-3a 和 KOMPSAT-5卫星。与其他 SOTA 基线模型相比，我们提出的模型具有更好的性能。我们证明，我们的方法比 KOMPSAT-3数据集中的第二最佳模型具有94.13% 的预测性能，KOMPSAT-3A 数据集中的预测性能高出89.79% ，KOMPSAT-5数据集中的预测性能高出76.71% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Decomposed+Attention+Segment+Recurrent+Neural+Network+for+Orbit+Prediction)|0|
|[RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning](https://doi.org/10.1145/3637528.3671644)|Congyun Jin, Ming Zhang, Weixiao Ma, Yujiao Li, Yingbo Wang, Yabo Jia, Yuliang Du, Tao Sun, Haowen Wang, Cong Fan, Jinjie Gu, Chenfei Chi, Xiangguo Lv, Fangzhou Li, Wei Xue, Yiran Huang|; Ant Group, Hangzhou, China; Ant Group, Shanghai, China; Shanghai Jiao Tong University School of Medicine Affiliated Renji Hospital, Shanghai, China; Shanghai Jiao Tong University School of Medicine Affiliated Renji Hospital, Qingdao, Shandong, China|Recent advancements in Large Language Models (LLMs) and Large Multi-modal Models (LMMs) have shown potential in various medical applications, such as Intelligent Medical Diagnosis. Although impressive results have been achieved, we find that existing benchmarks do not reflect the complexity of real medical reports and specialized in-depth reasoning capabilities. In this work, we establish a comprehensive benchmark in the field of medical specialization and introduced RJUA-MedDQA, which contains 2000 real-world Chinese medical report images poses several challenges: comprehensively interpreting imgage content across a wide variety of challenging layouts, possessing the numerical reasoning ability to identify abnormal indicators and demonstrating robust clinical reasoning ability to provide the statement of disease diagnosis, status and advice based on a collection of medical contexts. We carefully design the data generation pipeline and proposed the Efficient Structural Restoration Annotation (ESRA) Method, aimed at restoring textual and tabular content in medical report images. This method substantially enhances annotation efficiency, doubling the productivity of each annotator, and yields a 26.8% improvement in accuracy. We conduct extensive evaluations, including few-shot assessments of 5 LMMs which are capable of solving Chinese medical QA tasks. To further investigate the limitations and potential of current LMMs, we conduct comparative experiments on a set of strong LLMs by using image-text generated by ESRA method. We report the performance of baselines and offer several observations: (1) The overall performance of existing LMMs is still limited; however LMMs more robust to low-quality and diverse-structured images compared to LLMs. (3) Reasoning across context and image content present significant challenges. We hope this benchmark helps the community make progress on these challenging tasks in multi-modal medical document understanding and facilitate its application in healthcare. Our dataset will be publicly available for noncommercial use at https://github.com/Alipay-Med/medDQA_benchmark.git|大型语言模型(LLM)和大型多模态模型(LMM)的最新进展已经显示出在各种医疗应用中的潜力，例如智能医疗诊断。虽然取得了令人印象深刻的成果，但我们发现，现有的基准并不能反映真实医疗报告的复杂性和专门的深入推理能力。在这项工作中，我们在医学专业领域建立了一个全面的基准，并介绍了 RJUA-MedDQA，其中包含2000个真实世界的中国医疗报告图像提出了几个挑战: 全面解释图像内容在各种具有挑战性的布局，具有数字推理能力来识别异常指标，并表现出强大的临床推理能力，提供疾病诊断，状态和建议的陈述基于医疗背景的收集。我们精心设计了数据生成流水线，提出了一种有效的结构恢复注释(ESRA)方法，旨在恢复医学报告图像中的文本和表格内容。这种方法大大提高了注释效率，使每个注释器的生产率提高了一倍，并使准确率提高了26.8% 。我们进行了广泛的评估，包括对能够解决中医质量保证任务的5个 LMM 进行少量评估。为了进一步研究现有 LMM 的局限性和潜力，我们利用 ESRA 方法生成的图像文本对一组强 LMM 进行了对比实验。我们报告了基线的性能，并提供了几个观察结果: (1)现有的 LMM 的整体性能仍然有限，但是 LMM 对低质量和多样化结构的图像比 LLM 更强大。(3)跨语境和图像内容的推理是一个重大的挑战。我们希望这个基准能够帮助社区在多模式医疗文档理解方面取得进展，并促进其在医疗保健中的应用。我们的数据集将在 https://github.com/alipay-med/meddqa_benchmark.git 公开供非商业使用|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RJUA-MedDQA:+A+Multimodal+Benchmark+for+Medical+Document+Question+Answering+and+Clinical+Reasoning)|0|
|[Large Scale Hierarchical Industrial Demand Time-Series Forecasting incorporating Sparsity](https://doi.org/10.1145/3637528.3671632)|Harshavardhan Kamarthi, Aditya B. Sasanur, Xinjie Tong, Xingyu Zhou, James Peters, Joe Czyzyk, B. Aditya Prakash|The Dow Chemical Company, Midland, MI, USA; Georgia Institute of Technology, Atlanta, GA, USA; The Dow Chemical Company, Houston, TX, USA|Hierarchical time-series forecasting (HTSF) is an important problem for many real-world business applications where the goal is to simultaneously forecast multiple time-series that are related to each other via a hierarchical relation. Recent works, however, do not address two important challenges that are typically observed in many demand forecasting applications at large companies. First, many time-series at lower levels of the hierarchy have high sparsity i.e., they have a significant number of zeros. Most HTSF methods do not address this varying sparsity across the hierarchy. Further, they do not scale well to the large size of the real-world hierarchy typically unseen in benchmarks used in literature. We resolve both these challenges by proposing HAILS, a novel probabilistic hierarchical model that enables accurate and calibrated probabilistic forecasts across the hierarchy by adaptively modeling sparse and dense time-series with different distributional assumptions and reconciling them to adhere to hierarchical constraints. We show the scalability and effectiveness of our methods by evaluating them against real-world demand forecasting datasets. We deploy HAILS at a large chemical manufacturing company for a product demand forecasting application with over ten thousand products and observe a significant 8.5% improvement in forecast accuracy and 23% better improvement for sparse time-series. The enhanced accuracy and scalability make HAILS a valuable tool for improved business planning and customer experience.|分层时间序列预测(HTSF)是许多实际业务应用中的一个重要问题，其目标是通过层次关系同时预测相互关联的多个时间序列。然而，最近的工作并没有解决两个重要的挑战，这两个挑战通常在大公司的许多需求预测应用程序中观察到。首先，层次结构较低层次的许多时间序列具有较高的稀疏性，也就是说，它们有大量的零。大多数 HTSF 方法并不解决这种层次结构间的可变稀疏性。此外，它们不能很好地扩展到文献中使用的基准中通常看不到的现实世界等级的巨大规模。我们通过提出 HAILS 来解决这两个挑战，HAILS 是一种新型的概率层次模型，通过自适应地用不同的分布假设对稀疏和密集的时间序列建模，并协调它们以坚持层次约束，从而实现跨层次的准确和校准的概率预测。我们通过对比真实世界的需求预测数据集来评估我们的方法的可扩展性和有效性。我们在一家大型化工制造公司部署了 HAILS，用于产品需求预测应用程序，产品超过一万种，预测准确率显著提高了8.5% ，稀疏时间序列的预测准确率提高了23% 。增强的准确性和可扩展性使 HAILS 成为改善业务规划和客户体验的宝贵工具。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Scale+Hierarchical+Industrial+Demand+Time-Series+Forecasting+incorporating+Sparsity)|0|
|[Know, Grow, and Protect Net Worth: Using ML for Asset Protection by Preventing Overdraft Fees](https://doi.org/10.1145/3637528.3671628)|Avishek Kumar, Tyson Silver|Lightcast, Moscow, ID, USA; Intuit CreditKarma, Oakland, CA, USA|When a customer overdraws their bank account and their balance is negative they are assessed an overdraft fee. Americans pay approximately $15 billion in unnecessary overdraft fees a year, often in $35 increments; users of the Mint personal finance app pay approximately $250 million in fees a year in particular. These overdraft fees are an excessive financial burden and lead to cascading overdraft fees trapping customers in financial hardship. To address this problem, we have created an ML-driven overdraft early warning system (ODEWS) that assesses a customer's risk of overdrafting within the next week using their banking and transaction data in the Mint app. At-risk customers are sent an alert so they can take steps to avoid the fee, ultimately changing their behavior and financial habits. The system deployed resulted in a $3 million savings in overdraft fees for Mint customers compared to a control group. Moreover, the methodology outlined here is part of a greater effort to provide ML-driven personalized financial advice to help our members know, grow, and protect their net worth, ultimately, achieving their financial goals.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Know,+Grow,+and+Protect+Net+Worth:+Using+ML+for+Asset+Protection+by+Preventing+Overdraft+Fees)|0|
|[AutoWebGLM: A Large Language Model-based Web Navigating Agent](https://doi.org/10.1145/3637528.3671620)|Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, Jie Tang|University of Chinese Academy of Sciences, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; Zhipu AI, Beijing, China; Tsinghua University, Beijing, China; Tsinghua University & Zhipu AI, Beijing, China|Large language models (LLMs) have fueled many intelligent web agents, but most existing ones perform far from satisfying in real-world web navigation tasks due to three factors: (1) the complexity of HTML text data (2) versatility of actions on webpages, and (3) task difficulty due to the open-domain nature of the web. In light of these challenges, we develop the open AutoWebGLM based on ChatGLM3-6B. AutoWebGLM can serve as a powerful automated web navigation agent that outperform GPT-4. Inspired by human browsing patterns, we first design an HTML simplification algorithm to represent webpages with vital information preserved succinctly. We then employ a hybrid human-AI method to build web browsing data for curriculum training. Finally, we bootstrap the model by reinforcement learning and rejection sampling to further facilitate webpage comprehension, browser operations, and efficient task decomposition by itself. For comprehensive evaluation, we establish a bilingual benchmark---AutoWebBench---for real-world web navigation tasks. We evaluate AutoWebGLM across diverse web navigation benchmarks, demonstrating its potential to tackle challenging tasks in real environments. Related code, model, and data are released at https://github.com/THUDM/AutoWebGLM.|大型语言模型(LLM)为许多智能网络代理提供了动力，但大多数现有的 LLM 模型在现实世界的网络导航任务中表现得远远不能令人满意，原因有三: (1) HTML 文本数据的复杂性(2)网页操作的多样性(3)由于网络的开放领域特性，任务难度。针对这些挑战，我们开发了基于 ChatGLM3-6B 的开放式 AutoWebGLM。AutoWebGLM 可以作为一个强大的自动化 Web 导航代理，其性能优于 GPT-4。受到人类浏览模式的启发，我们首先设计了一个 HTML 简化算法来表示保存了重要信息的网页。然后采用人工智能和人工智能相结合的方法建立网页浏览数据，用于课程培训。最后，我们通过强化学习和拒绝抽样来引导模型，以进一步促进网页理解、浏览器操作和有效的任务分解。为了进行全面的评估，我们建立了一个双语基准—— AutoWebBench ——用于真实世界的网页导航任务。我们评估 AutoWebGLM 跨不同的网页导航基准，展示其潜力，以解决具有挑战性的任务在真实的环境。相关的代码、模型和数据在 https://github.com/thudm/autowebglm 发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AutoWebGLM:+A+Large+Language+Model-based+Web+Navigating+Agent)|0|
|[SEFraud: Graph-based Self-Explainable Fraud Detection via Interpretative Mask Learning](https://doi.org/10.1145/3637528.3671534)|Kaidi Li, Tianmeng Yang, Min Zhou, Jiahao Meng, Shendi Wang, Yihui Wu, Boshuai Tan, Hu Song, Lujia Pan, Fan Yu, Zhenli Sheng, Yunhai Tong|Huawei Inc, Shenzhen, China; ICBC Limited, Shanghai, China; Peking University, Beijing, China|Graph-based fraud detection has widespread application in modern industry scenarios, such as spam review and malicious account detection. While considerable efforts have been devoted to designing adequate fraud detectors, the interpretability of their results has often been overlooked. Previous works have attempted to generate explanations for specific instances using post-hoc explaining methods such as a GNNExplainer. However, post-hoc explanations can not facilitate the model predictions and the computational cost of these methods cannot meet practical requirements, thus limiting their application in real-world scenarios. To address these issues, we propose SEFraud, a novel graph-based self-explainable fraud detection framework that simultaneously tackles fraud detection and result in interpretability. Concretely, SEFraud first leverages customized heterogeneous graph transformer networks with learnable feature masks and edge masks to learn expressive representations from the informative heterogeneously typed transactions. A new triplet loss is further designed to enhance the performance of mask learning. Empirical results on various datasets demonstrate the effectiveness of SEFraud as it shows considerable advantages in both the fraud detection performance and interpretability of prediction results. Specifically, SEFraud achieves the most significant improvement with 8.6% on AUC and 8.5% on Recall over the second best on fraud detection, as well as an average of 10x speed-up regarding the inference time. Last but not least, SEFraud has been deployed and offers explainable fraud detection service for the largest bank in China, Industrial and Commercial Bank of China Limited (ICBC). Results collected from the production environment of ICBC show that SEFraud can provide accurate detection results and comprehensive explanations that align with the expert business understanding, confirming its efficiency and applicability in large-scale online services.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SEFraud:+Graph-based+Self-Explainable+Fraud+Detection+via+Interpretative+Mask+Learning)|0|
|[Harvesting Efficient On-Demand Order Pooling from Skilled Couriers: Enhancing Graph Representation Learning for Refining Real-time Many-to-One Assignments](https://doi.org/10.1145/3637528.3671643)|Yile Liang, Jiuxia Zhao, Donghui Li, Jie Feng, Chen Zhang, Xuetao Ding, Jinghua Hao, Renqing He|Tsinghua University, Beijing, China; Meituan, Beijing, China|The recent past has witnessed a notable surge in on-demand food delivery (OFD) services, offering delivery fulfillment within dozens of minutes after an order is placed. In OFD, pooling multiple orders for simultaneous delivery in real-time order assignment is a pivotal efficiency source, which may in turn extend delivery time. Constructing high-quality order pooling to harmonize platform efficiency with the experiences of consumers and couriers, is crucial to OFD platforms. However, the complexity and real-time nature of order assignment, making extensive calculations impractical, significantly limit the potential for order consolidation. Moreover, offline environment is frequently riddled with unknown factors, posing challenges for the platform's perceptibility and pooling decisions. Nevertheless, delivery behaviors of skilled couriers (SCs) who know the environment well, can improve system awareness and effectively inform decisions. Hence a SC delivery network (SCDN) is constructed, based on an enhanced attributed heterogeneous network embedding approach tailored for OFD. It aims to extract features from rich temporal and spatial information, and uncover the latent potential for order combinations embedded within SC trajectories. Accordingly, the vast search space of order assignment can be effectively pruned through scalable similarity calculations of low-dimensional vectors, making comprehensive and high-quality pooling outcomes more easily identified in real time. In addition, the acquired embedding outcomes highlight promising subspaces embedded within this space, i.e., scale-effect hotspot areas, which can offer significant potential for elevating courier efficiency. SCDN has now been deployed in Meituan dispatch system. Online tests reveal that with SCDN, the pooling quality and extent have been greatly improved. And our system can boost couriers' efficiency by 45-55% during noon peak hours, while upholding the timely delivery commitment.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Harvesting+Efficient+On-Demand+Order+Pooling+from+Skilled+Couriers:+Enhancing+Graph+Representation+Learning+for+Refining+Real-time+Many-to-One+Assignments)|0|
|[Hyper-Local Deformable Transformers for Text Spotting on Historical Maps](https://doi.org/10.1145/3637528.3671589)|Yijun Lin, YaoYi Chiang|University of Minnesota, Twin Cities, Minneapolis, MN, USA|Text on historical maps contains valuable information providing georeferenced historical, political, and cultural contexts. However, text extraction from historical maps has been challenging due to the lack of (1) effective methods and (2) training data. Previous approaches use ad-hoc steps tailored to only specific map styles. Recent machine learning-based text spotters (e.g., for scene images) have the potential to solve these challenges because of their flexibility in supporting various types of text instances. However, these methods remain challenges in extracting precise image features for predicting every sub-component (boundary points and characters) in a text instance. This is critical because map text can be lengthy and highly rotated with complex backgrounds, posing difficulties in detecting relevant image features from a rough text region. This paper proposes PALETTE, an end-to-end text spotter for scanned historical maps of a wide variety. PALETTE introduces a novel hyper-local sampling module to explicitly learn localized image features around the target boundary points and characters of a text instance for detection and recognition. PALETTE also enables hyper-local positional embeddings to learn spatial interactions between boundary points and characters within and across text instances. In addition, this paper presents a novel approach to automatically generate synthetic map images, SYNTHMAP+, for training text spotters for historical maps. The experiment shows that PALETTE with SYNTHMAP+ outperforms SOTA text spotters on two new benchmark datasets of historical maps, particularly for long and angled text. We have deployed PALETTE with SYNTHMAP+ to process over 60,000 maps in the David Rumsey Historical Map collection and generated over 100 million text labels to support map searching.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hyper-Local+Deformable+Transformers+for+Text+Spotting+on+Historical+Maps)|0|
|[Source Localization for Cross Network Information Diffusion](https://doi.org/10.1145/3637528.3671624)|Chen Ling, Tanmoy Chowdhury, Jie Ji, Sirui Li, Andreas Züfle, Liang Zhao|Emory University, Atlanta, VA, USA; Emory University, Atlanta, GA, USA|Source localization aims to locate information diffusion sources only given the diffusion observation, which has attracted extensive attention in the past few years. Existing methods are mostly tailored for single networks and may not be generalized to handle more complex networks like cross-networks. Cross-network is defined as two interconnected networks, where one network's functionality depends on the other. Source localization on cross-networks entails locating diffusion sources on the source network by only giving the diffused observation in the target network. The task is challenging due to challenges including: 1) diffusion sources distribution modeling; 2) jointly considering both static and dynamic node features; and 3) heterogeneous diffusion patterns learning. In this work, we propose a novel method, namely CNSL, to handle the three primary challenges. Specifically, we propose to learn the distribution of diffusion sources through Bayesian inference and leverage disentangled encoders to learn static and dynamic node features separately. The learning objective is coupled with the cross-network information propagation estimation model to make the inference of diffusion sources considering the overall diffusion process. Additionally, we also provide two novel cross-network datasets collected by ourselves. Extensive experiments are conducted on both datasets to demonstrate the effectiveness of CNSL in handling the source localization on cross-networks.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Source+Localization+for+Cross+Network+Information+Diffusion)|0|
|[MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning](https://doi.org/10.1145/3637528.3671609)|Bingchang Liu, Chaoyu Chen, Zi Gong, Cong Liao, Huan Wang, Zhichao Lei, Ming Liang, Dajun Chen, Min Shen, Hailian Zhou, Wei Jiang, Hang Yu, Jianguo Li||Code LLMs have emerged as a specialized research field, with remarkable studies dedicated to enhancing model's coding capabilities through fine-tuning on pre-trained models. Previous fine-tuning approaches were typically tailored to specific downstream tasks or scenarios, which meant separate fine-tuning for each task, requiring extensive training resources and posing challenges in terms of deployment and maintenance. Furthermore, these approaches failed to leverage the inherent interconnectedness among different code-related tasks. To overcome these limitations, we present a multi-task fine-tuning framework, MFTcoder, that enables simultaneous and parallel fine-tuning on multiple tasks. By incorporating various loss functions, we effectively address common challenges in multi-task learning, such as data imbalance, varying difficulty levels, and inconsistent convergence speeds. Extensive experiments have conclusively demonstrated that our multi-task fine-tuning approach outperforms both individual fine-tuning on single tasks and fine-tuning on a mixed ensemble of tasks. Moreover, MFTcoder offers efficient training capabilities, including efficient data tokenization modes and PEFT fine-tuning, resulting in significantly improved speed compared to traditional fine-tuning methods. MFTcoder seamlessly integrates with several mainstream open-source LLMs, such as CodeLLama and Qwen. Leveraging the CodeLLama foundation, our MFTcoder fine-tuned model, \textsc{CodeFuse-CodeLLama-34B}, achieves an impressive pass@1 score of 74.4\% on the HumaneEval benchmark, surpassing GPT-4 performance (67\%, zero-shot). MFTCoder is open-sourced at \url{https://github.com/codefuse-ai/MFTCOder}||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MFTCoder:+Boosting+Code+LLMs+with+Multitask+Fine-Tuning)|0|
|[Towards Automatic Evaluation for LLMs' Clinical Capabilities: Metric, Data, and Algorithm](https://doi.org/10.1145/3637528.3671575)|Lei Liu, Xiaoyan Yang, Fangzhou Li, Chenfei Chi, Yue Shen, Shiwei Lyu, Ming Zhang, Xiaowei Ma, Xiangguo Lv, Liya Ma, Zhiqiang Zhang, Wei Xue, Yiran Huang, Jinjie Gu|Renji Hospital, Shanghai, China; Ant Group, Shanghai, China; Ant Group, Hangzhou, China; The Chinese University of Hong Kong, Shenzhen, Shenzhen, China|Large language models (LLMs) are gaining increasing interests to improve clinical efficiency, owing to their unprecedented performance in modelling natural language. Ensuring the reliable clinical applications, the evaluation of LLMs indeed becomes critical for better mitigating the potential risks, e.g., hallucinations. However, current evaluation methods heavily rely on labor-intensive human participation to achieve human-preferred judgements. To overcome this challenge, we propose an automatic evaluation paradigm tailored to assess the LLMs' capabilities in delivering clinical services, e.g., disease diagnosis and treatment. The evaluation paradigm contains three basic elements: metric, data, and algorithm. Specifically, inspired by professional clinical practice pathways, we formulate a LLM-specific clinical pathway (LCP) to define the clinical capabilities that a doctor agent should possess. Then, Standardized Patients (SPs) from the medical education are introduced as the guideline for collecting medical data for evaluation, which can well ensure the completeness of the evaluation procedure. Leveraging these steps, we develop a multi-agent framework to simulate the interactive environment between SPs and a doctor agent, which is equipped with a Retrieval-Augmented Evaluation (RAE) to determine whether the behaviors of a doctor agent are in accordance with LCP. The above paradigm can be extended to any similar clinical scenarios to automatically evaluate the LLMs' medical capabilities. Applying such paradigm, we construct an evaluation benchmark in the field of urology, including a LCP, a SPs dataset, and an automated RAE. Extensive experiments are conducted to demonstrate the effectiveness of the proposed approach, providing more insights for LLMs' safe and reliable deployments in clinical practice.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Automatic+Evaluation+for+LLMs'+Clinical+Capabilities:+Metric,+Data,+and+Algorithm)|0|
|[DAG: Deep Adaptive and Generative K-Free Community Detection on Attributed Graphs](https://doi.org/10.1145/3637528.3671615)|Chang Liu, Yuwen Yang, Yue Ding, Hongtao Lu, Wenqing Lin, Ziming Wu, Wendong Bi|Tencent, Shenzhen, China; Shanghai Jiao Tong University, Shanghai, China|Community detection on attributed graphs with rich semantic and topological information offers great potential for real-world network analysis, especially user matching in online games. Graph Neural Networks (GNNs) have recently enabled Deep Graph Clustering (DGC) methods to learn cluster assignments from semantic and topological information. However, their success depends on the prior knowledge related to the number of communities K, which is unrealistic due to the high costs and privacy issues of acquisition. In this paper, we investigate the community detection problem without prior K, referred to as K-Free Community Detection problem. To address this problem, we propose a novel Deep Adaptive and Generative model~(DAG) for community detection without specifying the prior K. DAG consists of three key components, i.e., a node representation learning module with masked attribute reconstruction, a community affiliation readout module, and a community number search module with group sparsity. These components enable DAG to convert the process of non-differentiable grid search for the community number, i.e., a discrete hyperparameter in existing DGC methods, into a differentiable learning process. In such a way, DAG can simultaneously perform community detection and community number search end-to-end. To alleviate the cost of acquiring community labels in real-world applications, we design a new metric, EDGE, to evaluate community detection methods even when the labels are not feasible. Extensive offline experiments on five public datasets and a real-world online mobile game dataset demonstrate the superiority of our DAG over the existing state-of-the-art (SOTA) methods. DAG has a relative increase of 7.35% in teams in a Tencent online game compared with the best competitor.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DAG:+Deep+Adaptive+and+Generative+K-Free+Community+Detection+on+Attributed+Graphs)|0|
|[EmoLLMs: A Series of Emotional Large Language Models and Annotation Tools for Comprehensive Affective Analysis](https://doi.org/10.1145/3637528.3671552)|Zhiwei Liu, Kailai Yang, Qianqian Xie, Tianlin Zhang, Sophia Ananiadou|The University of Manchester & Artificial Intelligence Research Center, Manchester, United Kingdom; The University of Manchester, Manchester, United Kingdom|Sentiment analysis and emotion detection are important research topics in natural language processing (NLP) and benefit many downstream tasks. With the widespread application of large language models (LLMs), researchers have started exploring the application of LLMs based on instruction-tuning in the field of sentiment analysis. However, these models only focus on single aspects of affective classification tasks (e.g. sentimental polarity or categorical emotions), and overlook the regression tasks (e.g. sentiment strength or emotion intensity), which leads to poor performance in downstream tasks. The main reason is the lack of comprehensive affective instruction tuning datasets and evaluation benchmarks, which cover various affective classification and regression tasks. Moreover, although emotional information is useful for downstream tasks, existing downstream datasets lack high-quality and comprehensive affective annotations. In this paper, we propose EmoLLMs, the first series of open-sourced instruction-following LLMs for comprehensive affective analysis based on fine-tuning various LLMs with instruction data, the first multi-task affective analysis instruction dataset (AAID) with 234K data samples based on 3 classification tasks and 2 regression tasks to support LLM instruction tuning, and a comprehensive affective evaluation benchmark (AEB) with 8 regression tasks and 6 classification tasks from various sources and domains to test the generalization ability of LLMs. We propose a series of EmoLLMs by fine-tuning LLMs with AAID to solve various affective instruction tasks. We compare our models with a variety of LLMs and sentiment analysis tools on AEB, where our models outperform all other open-sourced LLMs and sentiment analysis tools, and surpass ChatGPT and GPT-4 in most tasks, which shows that the series of EmoLLMs achieve the ChatGPT-level and GPT-4-level generalization capabilities on affective analysis tasks, and demonstrates our models can be used as affective annotation tools. This project is available at https://github.com/lzw108/EmoLLMs/.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EmoLLMs:+A+Series+of+Emotional+Large+Language+Models+and+Annotation+Tools+for+Comprehensive+Affective+Analysis)|0|
|[MISP: A Multimodal-based Intelligent Server Failure Prediction Model for Cloud Computing Systems](https://doi.org/10.1145/3637528.3671568)|Xianting Lu, Yunong Wang, Yu Fu, Qi Sun, Xuhua Ma, Xudong Zheng, Cheng Zhuo|Alibaba Cloud, Alibaba Group, Hangzhou, China; Lanzhou University & Zhejiang University, Lanzhou, China; Zhejiang University, Hangzhou, China|Traditional server failure prediction methods predominantly rely on single-modality data such as system logs or system status curves. This reliance may lead to an incomplete understanding of system health and impending issues, proving inadequate for the complex and dynamic landscape of contemporary cloud computing environments. The potential of multimodal data to provide comprehensive insights is widely acknowledged, yet the lack of a holistic dataset and the challenges inherent in integrating features from both structured and unstructured data have impeded the exploration of multimodal-based server failure prediction. Addressing these challenges, this paper presents an industrial-scale, comprehensive dataset for server failure prediction, comprising nearly 80 types of structured and unstructured data sourced from real-world industrial cloud systems 1. Building on this resource, we introduce MISP, a model that leverages multimodal fusion techniques for server failure prediction. MISP transforms multimodal data into multi-dimensional sequences, extracts and encodes features both within and across the modalities, and ultimately computes the failure probability from the synthesized features. Experiments demonstrate that MISP significantly outperforms existing methods, enhancing prediction accuracy by approximately 25% over previous state-of-the-art approaches.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MISP:+A+Multimodal-based+Intelligent+Server+Failure+Prediction+Model+for+Cloud+Computing+Systems)|0|
|[Integrating System State into Spatio Temporal Graph Neural Network for Microservice Workload Prediction](https://doi.org/10.1145/3637528.3671508)|Yang Luo, Mohan Gao, Zhemeng Yu, Haoyuan Ge, Xiaofeng Gao, Tengwei Cai, Guihai Chen|Ant Group, Hangzhou, China; Shanghai Jiao Tong University, Shanghai, China|Microservice architecture has become a driving force in enhancing the modularity and scalability of web applications, as evidenced by the Alipay platform's operational success. However, a prevalent issue within such infrastructures is the suboptimal utilization of CPU resources due to inflexible resource allocation policies. This inefficiency necessitates the development of dynamic, accurate workload prediction methods to improve resource allocation. In response to this challenge, we present STAMP, a Spatio Temporal Graph Network for Microservice Workload Prediction. STAMP is designed to comprehensively address the multifaceted interdependencies between microservices, the temporal variability of workloads, and the critical role of system state in resource utilization. Through a graph-based representation, STAMP effectively maps the intricate network of microservice interactions. It employs time series analysis to capture the dynamic nature of workload changes and integrates system state insights to enhance prediction accuracy. Our empirical analysis, using three distinct real-world datasets, establishes that STAMP exceeds baselines by achieving an average boost of 5.72% in prediction precision, as measured by RMSE. Upon deployment in Alipay's microservice environment, STAMP achieves a 33.10% reduction in resource consumption, significantly outperforming existing online methods. This research solidifies STAMP as a validated framework, offering meaningful contributions to the field of resource management in microservice architecture-based applications.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Integrating+System+State+into+Spatio+Temporal+Graph+Neural+Network+for+Microservice+Workload+Prediction)|0|
|[FusionSF: Fuse Heterogeneous Modalities in a Vector Quantized Framework for Robust Solar Power Forecasting](https://doi.org/10.1145/3637528.3671509)|Ziqing Ma, Wenwei Wang, Tian Zhou, Chao Chen, Bingqing Peng, Liang Sun, Rong Jin||Accurate solar power forecasting is crucial to integrate photovoltaic plants into the electric grid, schedule and secure the power grid safety. This problem becomes more demanding for those newly installed solar plants which lack sufficient data. Current research predominantly relies on historical solar power data or numerical weather prediction in a single-modality format, ignoring the complementary information provided in different modalities. In this paper, we propose a multi-modality fusion framework to integrate historical power data, numerical weather prediction, and satellite images, significantly improving forecast performance. We introduce a vector quantized framework that aligns modalities with varying information densities, striking a balance between integrating sufficient information and averting model overfitting. Our framework demonstrates strong zero-shot forecasting capability, which is especially useful for those newly installed plants. Moreover, we collect and release a multi-modal solar power (MMSP) dataset from real-world plants to further promote the research of multi-modal solar forecasting algorithms. Our extensive experiments show that our model not only operates with robustness but also boosts accuracy in both zero-shot forecasting and scenarios rich with training data, surpassing leading models. We have incorporated it into our eForecaster platform and deployed it for more than 300 solar plants with a capacity of over 15GW.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FusionSF:+Fuse+Heterogeneous+Modalities+in+a+Vector+Quantized+Framework+for+Robust+Solar+Power+Forecasting)|0|
|[Valuing an Engagement Surface using a Large Scale Dynamic Causal Model](https://doi.org/10.1145/3637528.3671604)|Abhimanyu Mukerji, Sushant More, Ashwin Viswanathan Kannan, Lakshmi Ravi, Hua Chen, Naman Kohli, Chris Khawand, Dinesh Mandalapu|Amazon, Sunnyvale, CA, USA; Amazon, Sunnyvale, WA, USA; Amazon, Seattle, WA, USA; Amazon, Vancouver, BC, Canada|With recent rapid growth in online shopping, AI-powered Engagement Surfaces (ES) have become ubiquitous across retail services. These engagement surfaces perform an increasing range of functions, including recommending new products for purchase, reminding customers of their orders and providing delivery notifications. Understanding the causal effect of engagement surfaces on value driven for customers and businesses remains an open scientific question. In this paper, we develop a dynamic causal model at scale to disentangle value attributable to an ES, and to assess its effectiveness. We demonstrate the application of this model to inform business decision-making by understanding returns on investment in the ES, and identifying product lines and features where the ES adds the most value.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Valuing+an+Engagement+Surface+using+a+Large+Scale+Dynamic+Causal+Model)|0|
|[EEG2Rep: Enhancing Self-supervised EEG Representation Through Informative Masked Inputs](https://doi.org/10.1145/3637528.3671600)|Navid Mohammadi Foumani, Geoffrey Mackellar, Soheila Ghane, Saad Irtza, Nam Nguyen, Mahsa Salehi|Emotiv Research, Melbourne, Australia; Emotiv Research, Sydney, Australia; Monash University, Melbourne, Australia|Self-supervised approaches for electroencephalography (EEG) representationlearning face three specific challenges inherent to EEG data: (1) The lowsignal-to-noise ratio which challenges the quality of the representationlearned, (2) The wide range of amplitudes from very small to relatively largedue to factors such as the inter-subject variability, risks the models to bedominated by higher amplitude ranges, and (3) The absence of explicitsegmentation in the continuous-valued sequences which can result in lessinformative representations. To address these challenges, we introduce EEG2Rep,a self-prediction approach for self-supervised representation learning fromEEG. Two core novel components of EEG2Rep are as follows: 1) Instead oflearning to predict the masked input from raw EEG, EEG2Rep learns to predictmasked input in latent representation space, and 2) Instead of conventionalmasking methods, EEG2Rep uses a new semantic subsequence preserving (SSP)method which provides informative masked inputs to guide EEG2Rep to generaterich semantic representations. In experiments on 6 diverse EEG tasks withsubject variability, EEG2Rep significantly outperforms state-of-the-artmethods. We show that our semantic subsequence preserving improves the existingmasking methods in self-prediction literature and find that preserving 50% ofEEG recordings will result in the most accurate results on all 6 tasks onaverage. Finally, we show that EEG2Rep is robust to noise addressing asignificant challenge that exists in EEG data. Models and code are availableat: https://github.com/Navidfoumani/EEG2Rep||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EEG2Rep:+Enhancing+Self-supervised+EEG+Representation+Through+Informative+Masked+Inputs)|0|
|[Detecting Abnormal Operations in Concentrated Solar Power Plants from Irregular Sequences of Thermal Images](https://doi.org/10.1145/3637528.3671623)|Sukanya Patra, Nicolas Sournac, Souhaib Ben Taieb|University of Mons, Mons, Belgium|Concentrated Solar Power (CSP) plants store energy by heating a storage medium with an array of mirrors that focus sunlight onto solar receivers atop a central tower. Operating at high temperatures these receivers face risks such as freezing, deformation, and corrosion, leading to operational failures, downtime, or costly equipment damage. We study the problem of anomaly detection (AD) in sequences of thermal images collected over a year from an operational CSP plant. These images are captured at irregular intervals ranging from one to five minutes throughout the day by infrared cameras mounted on solar receivers. Our goal is to develop a method to extract useful representations from high-dimensional thermal images for AD. It should be able to handle temporal features of the data, which include irregularity, temporal dependency between images and non-stationarity due to a strong daily seasonal pattern. The co-occurrence of low-temperature anomalies that resemble normal images from the start and the end of the operational cycle with high-temperature anomalies poses an additional challenge. We first evaluate state-of-the-art deep image-based AD methods, which have been shown to be effective in deriving meaningful image representations for the detection of anomalies. Then, we introduce a forecasting-based AD method that predicts future thermal images from past sequences and timestamps via a deep sequence model. This method effectively captures specific temporal data features and distinguishes between difficult-to-detect temperature-based anomalies. Our experiments demonstrate the effectiveness of our approach compared to multiple SOTA baselines across multiple evaluation metrics. We have also successfully deployed our solution on five months of unseen data, providing critical insights to our industry partner for the maintenance of the CSP plant. Our code is publicly accessible from https://github.com/sukanyapatra1997/ForecastAD. Additionally, as our dataset is confidential, we release a simulated dataset at https://tinyurl.com/kdd2024Dataset.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+Abnormal+Operations+in+Concentrated+Solar+Power+Plants+from+Irregular+Sequences+of+Thermal+Images)|0|
|[Spatio-Temporal Consistency Enhanced Differential Network for Interpretable Indoor Temperature Prediction](https://doi.org/10.1145/3637528.3671608)|Dekang Qi, Xiuwen Yi, Chengjie Guo, Yanyong Huang, Junbo Zhang, Tianrui Li, Yu Zheng|Southwest Jiaotong University & JD iCity, JD Technology, Chengdu, China; JD iCity, JD Technology & JD Intelligent Cities Research, Beijing, China; Southwest Jiaotong University, Chengdu, China; Xidian University, Xi'an, China; Southwestern University of Finance and Economics, Chengdu, China|Indoor temperature prediction is crucial for decision-making in central heating systems. Beyond accuracy, predictions shall be interpretable, i.e. conform to the laws of physics; otherwise, it may lead to system failures or unsafe conditions. However, deep learning models often face criticism regarding interpretability, which limits their application in such settings. To this end, we propose a Spatio-Temporal Consistency enhanced Differential Network (CONST) for interpretable indoor temperature prediction. Our approach mainly consists of a differential predictive module and a spatio-temporal consistency module. Modeling the influential factors, the first module solves the issue of multicollinearity through the differential operation. Considering the heterogeneity of global and local data distributions, the second module characterizes the temporal and spatial consistency to mine the universal pattern by multi-task learning, thereby improving the prediction interpretability. Besides, we propose a set of interpretability metrics to overcome the drawbacks of partial dependence plot metric, which are more practical, zero-centered, flexible, and numerical. We conclude experiments on a real-world dataset with four heating stations. The results demonstrate the advantages of our approach over various baselines, where the interpretability can be improved by more than 8 times on cRPD while maintaining high accuracy. We developed CONST on the SmartHeat system, providing hourly indoor temperature forecasts for 13 heating stations in northern China.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Spatio-Temporal+Consistency+Enhanced+Differential+Network+for+Interpretable+Indoor+Temperature+Prediction)|0|
|[Addressing Shortcomings in Fair Graph Learning Datasets: Towards a New Benchmark](https://doi.org/10.1145/3637528.3671616)|Xiaowei Qian, Zhimeng Guo, Jialiang Li, Haitao Mao, Bingheng Li, Suhang Wang, Yao Ma|The Pennsylvania State University, University Park, PA, USA; New Jersey Institute of Technology, Newark, NJ, USA; Michigan State University, East Lansing, MI, USA; Rensselaer Polytechnic Institute, Troy, NY, USA|Fair graph learning plays a pivotal role in numerous practical applications.Recently, many fair graph learning methods have been proposed; however, theirevaluation often relies on poorly constructed semi-synthetic datasets orsubstandard real-world datasets. In such cases, even a basic MultilayerPerceptron (MLP) can outperform Graph Neural Networks (GNNs) in both utilityand fairness. In this work, we illustrate that many datasets fail to providemeaningful information in the edges, which may challenge the necessity of usinggraph structures in these problems. To address these issues, we develop andintroduce a collection of synthetic, semi-synthetic, and real-world datasetsthat fulfill a broad spectrum of requirements. These datasets are thoughtfullydesigned to include relevant graph structures and bias information crucial forthe fair evaluation of models. The proposed synthetic and semi-syntheticdatasets offer the flexibility to create data with controllable biasparameters, thereby enabling the generation of desired datasets withuser-defined bias values with ease. Moreover, we conduct systematic evaluationsof these proposed datasets and establish a unified evaluation approach for fairgraph learning models. Our extensive experimental results with fair graphlearning methods across our datasets demonstrate their effectiveness inbenchmarking the performance of these methods. Our datasets and the code forreproducing our experiments are available athttps://github.com/XweiQ/Benchmark-GraphFairness.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Addressing+Shortcomings+in+Fair+Graph+Learning+Datasets:+Towards+a+New+Benchmark)|0|
|[Class-incremental Learning for Time Series: Benchmark and Evaluation](https://doi.org/10.1145/3637528.3671581)|Zhongzheng Qiao, Quang Pham, Zhen Cao, Hoang H. Le, Ponnuthurai N. Suganthan, Xudong Jiang, Savitha Ramasamy|IGP-ERIN, NTU & I2R, ASTAR, Singapore, Singapore; I2R, ASTAR, Singapore, Singapore; Qatar University, Dohar, Qatar; School of Electrical and Electronic Engineering, NTU, Singapore, Singapore; Ho Chi Minh University of Science, Vietnam National University, Ho Chi Minh City, Vietnam; I2R, ASTAR & CNRSCREATE, Singapore, Singapore|Real-world environments are inherently non-stationary, frequently introducingnew classes over time. This is especially common in time series classification,such as the emergence of new disease classification in healthcare or theaddition of new activities in human activity recognition. In such cases, alearning system is required to assimilate novel classes effectively whileavoiding catastrophic forgetting of the old ones, which gives rise to theClass-incremental Learning (CIL) problem. However, despite the encouragingprogress in the image and language domains, CIL for time series data remainsrelatively understudied. Existing studies suffer from inconsistent experimentaldesigns, necessitating a comprehensive evaluation and benchmarking of methodsacross a wide range of datasets. To this end, we first present an overview ofthe Time Series Class-incremental Learning (TSCIL) problem, highlight itsunique challenges, and cover the advanced methodologies. Further, based onstandardized settings, we develop a unified experimental framework thatsupports the rapid development of new algorithms, easy integration of newdatasets, and standardization of the evaluation process. Using this framework,we conduct a comprehensive evaluation of various generic andtime-series-specific CIL methods in both standard and privacy-sensitivescenarios. Our extensive experiments not only provide a standard baseline tosupport future research but also shed light on the impact of various designfactors such as normalization layers or memory budget thresholds. Codes areavailable at https://github.com/zqiao11/TSCIL.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Class-incremental+Learning+for+Time+Series:+Benchmark+and+Evaluation)|0|
|[Leveraging Exposure Networks for Detecting Fake News Sources](https://doi.org/10.1145/3637528.3671539)|Maor Reuben, Lisa Friedland, Rami Puzis, Nir Grinberg|Independent researcher, Boston, MA, USA; Ben-Gurion University of the Negev Software and Information Systems Engineering, Beer Sheva, Israel|The scale and dynamic nature of the Web makes real-time detection of misinformation an extremely difficult task. Prior research mostly focused on offline (retrospective) detection of stories or claims using linguistic features of the content, flagging by users, and crowdsourced labels. Here, we develop a novel machine-learning methodology for detecting fake news sources using active learning, and examine the contribution of network, audience, and text features to the model accuracy. Importantly, we evaluate performance in both offline and online settings, mimicking the strategic choices fact-checkers have to make in practice as news sources emerge over time. We find that exposure networks provide information on considerably more sources than sharing networks (+49.6%), and that the inclusion of exposure features greatly improves classification PR-AUC in both offline (+33%) and online (+69.2%) settings. Textual features perform best in offline settings, but their performance deteriorates by 12.0-18.7% in online settings. Finally, the results show that a few iterations of active learning are sufficient for our model to attain predictive performance to comparable exhaustive labeling while incurring only 24.7% of the labeling costs. These results stress the importance of exposure networks as a source of valuable information for the investigation of information dissemination in social networks and question the robustness of textual features.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leveraging+Exposure+Networks+for+Detecting+Fake+News+Sources)|0|
|[Tackling Concept Shift in Text Classification using Entailment-style Modeling](https://doi.org/10.1145/3637528.3671541)|Sumegh Roychowdhury, Karan Gupta, Siva Rajesh Kasa, Prasanna Srinivasa Murthy|Amazon, Bengaluru, India; Amazon, Bangalore, India|Pre-trained language models (PLMs) have seen tremendous success in text classification (TC) problems in the context of Natural Language Processing (NLP). In many real-world text classification tasks, the class definitions being learned do not remain constant but rather change with time - this is known as concept shift. Most techniques for handling concept shift rely on retraining the old classifiers with the newly labelled data. However, given the amount of training data required to fine-tune large DL models for the new concepts, the associated labelling costs can be prohibitively expensive and time consuming. In this work, we propose a reformulation, converting vanilla classification into an entailment-style problem that requires significantly less data to re-train the text classifier to adapt to new concepts. We demonstrate the effectiveness of our proposed method on both real world & synthetic datasets achieving absolute F1 gains upto ~6% and ~30% respectively in few-shot settings. Further, upon deployment, our solution also helped save 75% direct labeling costs and 40% downstream labeling costs overall in a span of 3 months.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tackling+Concept+Shift+in+Text+Classification+using+Entailment-style+Modeling)|0|
|[Hierarchical Knowledge Guided Fault Intensity Diagnosis of Complex Industrial Systems](https://doi.org/10.1145/3637528.3671610)|Yu Sha, Shuiping Gou, Bo Liu, Johannes Faber, Ningtao Liu, Stefan Schramm, Horst Stoecker, Thomas Steckenreiter, Domagoj Vnucec, Nadine Wetzstein, Andreas Widl, Kai Zhou|FIAS, Goethe Universität and GSI, Frankfurt, Hessian, Germany; FIAS, Frankfurt, Hessian, Germany; SAMSON AG, Frankfurt, Hessian, Germany; CUHK-SZ and FIAS, Shenzhen, Guangdong, China; Xidian University, FIAS and XF-IJRC, Xian, Shaanxi, China; Xidian University, Xian, Shaanxi, China; SAMSON AG, Feankfurt, Hessian, Germany|Fault intensity diagnosis (FID) plays a pivotal role in monitoring and maintaining mechanical devices within complex industrial systems. As current FID methods are based on chain of thought without considering dependencies among target classes. To capture and explore dependencies, we propose a hierarchical knowledge guided fault intensity diagnosis framework (HKG) inspired by the tree of thought, which is amenable to any representation learning methods. The HKG uses graph convolutional networks to map the hierarchical topological graph of class representations into a set of interdependent global hierarchical classifiers, where each node is denoted by word embeddings of a class. These global hierarchical classifiers are applied to learned deep features extracted by representation learning, allowing the entire model to be end-to-end learnable. In addition, we develop a re-weighted hierarchical knowledge correlation matrix (Re-HKCM) scheme by embedding inter-class hierarchical knowledge into a data-driven statistical correlation matrix (SCM) which effectively guides the information sharing of nodes in graphical convolutional neural networks and avoids over-smoothing issues. The Re-HKCM is derived from the SCM through a series of mathematical transformations. Extensive experiments are performed on four real-world datasets from different industrial domains (three cavitation datasets from SAMSON AG and one existing publicly) for FID, all showing superior results and outperform recent state-of-the-art FID methods.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Knowledge+Guided+Fault+Intensity+Diagnosis+of+Complex+Industrial+Systems)|0|
|[Lumos: Empowering Multimodal LLMs with Scene Text Recognition](https://doi.org/10.1145/3637528.3671633)|Ashish Shenoy, Yichao Lu, Srihari Jayakumar, Debojeet Chatterjee, Mohsen Moslehpour, Pierce Chuang, Abhay Harpale, Vikas Bhardwaj, Di Xu, Shicong Zhao, Longfang Zhao, Ankit Ramchandani, Xin Luna Dong, Anuj Kumar|Meta Reality Labs, Menlo Park, CA, USA; Meta Reality Labs, Redmond, WA, USA; Meta, Menlo Park, CA, USA; Reality Labs, Meta, Redmond, WA, USA|We introduce Lumos, the first end-to-end multimodal question-answering systemwith text understanding capabilities. At the core of Lumos is a Scene TextRecognition (STR) component that extracts text from first person point-of-viewimages, the output of which is used to augment input to a Multimodal LargeLanguage Model (MM-LLM). While building Lumos, we encountered numerouschallenges related to STR quality, overall latency, and model inference. Inthis paper, we delve into those challenges, and discuss the systemarchitecture, design choices, and modeling techniques employed to overcomethese obstacles. We also provide a comprehensive evaluation for each component,showcasing high quality and efficiency.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Lumos:+Empowering+Multimodal+LLMs+with+Scene+Text+Recognition)|0|
|[From Variability to Stability: Advancing RecSys Benchmarking Practices](https://doi.org/10.1145/3637528.3671655)|Valeriy Shevchenko, Nikita Belousov, Alexey Vasilev, Vladimir Zholobov, Artyom Sosedka, Natalia Semenova, Anna Volodkevich, Andrey Savchenko, Alexey Zaytsev|AIRI & Sber AI Lab, Moscow, Russian Federation; Skoltech & MIPT, Moscow, Russian Federation; Skoltech, Moscow, Russian Federation; Sber AI Lab, Moscow, Russian Federation; Skoltech & BIMSA, Moscow, Russian Federation|In the rapidly evolving domain of Recommender Systems (RecSys), newalgorithms frequently claim state-of-the-art performance based on evaluationsover a limited set of arbitrarily selected datasets. However, this approach mayfail to holistically reflect their effectiveness due to the significant impactof dataset characteristics on algorithm performance. Addressing thisdeficiency, this paper introduces a novel benchmarking methodology tofacilitate a fair and robust comparison of RecSys algorithms, thereby advancingevaluation practices. By utilizing a diverse set of 30 open datasets,including two introduced in this work, and evaluating 11 collaborativefiltering algorithms across 9 metrics, we critically examine the influence ofdataset characteristics on algorithm performance. We further investigate thefeasibility of aggregating outcomes from multiple datasets into a unifiedranking. Through rigorous experimental analysis, we validate the reliability ofour methodology under the variability of datasets, offering a benchmarkingstrategy that balances quality and computational demands. This methodologyenables a fair yet effective means of evaluating RecSys algorithms, providingvaluable guidance for future research endeavors.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Variability+to+Stability:+Advancing+RecSys+Benchmarking+Practices)|0|
|[Improving Ego-Cluster for Network Effect Measurement](https://doi.org/10.1145/3637528.3671557)|Wentao Su, Weitao Duan|LinkedIn Corporation, Sunnyvale, CA, USA|Network effect is common in social network platforms. Many new features in social networks are designed to specifically create network effect to improve user engagement. For example, content creators tend to produce more when their articles and posts receive more positive feedback from followers. This paper discusses a new cluster-level experimentation methodology to measure the creator-side metrics in the context of A/B experiment. The methodology is designed to address the cases when the experiment randomization unit and the metric measurement unit are not the same, and it is a part of the overall strategy at LinkedIn to promote a robust creator community and ecosystem. The method is developed based on the widely-cited research at LinkedIn, but significantly improves the clustering algorithm efficiency and flexibility, leading to a stronger capability of the creator-side metrics measurement and increasing velocity for creator-related experiments.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Ego-Cluster+for+Network+Effect+Measurement)|0|
|[Beimingwu: A Learnware Dock System](https://doi.org/10.1145/3637528.3671617)|ZhiHao Tan, JianDong Liu, XiaoDong Bi, Peng Tan, QinCheng Zheng, HaiTian Liu, Yi Xie, XiaoChuan Zou, Yang Yu, ZhiHua Zhou|; Nanjing University; Nanjing University School of Artificial Intelligence|The learnware paradigm proposed by Zhou (2016) aims to enable users to leverage numerous existing high-performing models instead of building machine learning models from scratch. This paradigm envisions that: Any developer worldwide can submit their well-trained models spontaneously into a learnware dock system (formerly known as learnware market). The system uniformly generates a specification for each model to form a learnware and accommodates it. As the key component, a specification should represent the capabilities of the model while preserving developer's original data. Based on the specifications, the learnware dock system can identify and assemble existing learnwares for users to solve new machine learning tasks. Recently, based on reduced kernel mean embedding (RKME) specification, a series of studies have shown the effectiveness of the learnware paradigm theoretically and empirically. However, the realization of a learnware dock system is still missing and remains a big challenge. This paper proposes Beimingwu, the first open-source learnware dock system, providing foundational support for future research. The system provides implementations and extensibility for the entire process of learnware paradigm, including the submitting, usability testing, organization, identification, deployment, and reuse of learnwares. Utilizing Beimingwu, the model development for new user tasks can be significantly streamlined, thanks to integrated architecture and engine design, specifying unified learnware structure and scalable APIs, and the integration of various algorithms for learnware identification and reuse. Notably, this is possible even for users with limited data and minimal expertise in machine learning, without compromising the raw data's security. The system facilitates the future research implementations in learnware-related algorithms and systems, and lays the ground for hosting a vast array of learnwares and establishing a learnware ecosystem. The system is fully open-source and we expect the research community to benefit from the system. The system and research toolkit have been released on GitLink and GitHub.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Beimingwu:+A+Learnware+Dock+System)|0|
|[Business Policy Experiments using Fractional Factorial Designs: Consumer Retention on DoorDash](https://doi.org/10.1145/3637528.3671574)|Yixin Tang, Yicong Lin, Navdeep S. Sahni|DoorDash, Inc., San Francisco, CA, USA; Stanford GSB, Stanford, CA, USA|This paper investigates an approach to both speed up business decision-making and lower the cost of learning through experimentation by factorizing business policies and employing fractional factorial experimental designs for their evaluation. We illustrate how this method integrates with advances in the estimation of heterogeneous treatment effects, elaborating on its advantages and foundational assumptions. We empirically demonstrate the implementation and benefits of our approach and assess its validity in evaluating consumer promotion policies at DoorDash, which is one of the largest delivery platforms in the US. Our approach discovers a policy with 5% incremental profit at 67% lower implementation cost.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Business+Policy+Experiments+using+Fractional+Factorial+Designs:+Consumer+Retention+on+DoorDash)|0|
|[TnT-LLM: Text Mining at Scale with Large Language Models](https://doi.org/10.1145/3637528.3671647)|Mengting Wan, Tara Safavi, Sujay Kumar Jauhar, Yujin Kim, Scott Counts, Jennifer Neville, Siddharth Suri, Chirag Shah, Ryen W. White, Longqi Yang, Reid Andersen, Georg Buscher, Dhruv Joshi, Nagu Rangan|Microsoft Corporation, Redmond, WA, USA; University of Washington, Seattle, WA, USA|Transforming unstructured text into structured and meaningful forms, organized by useful category labels, is a fundamental step in text mining for downstream analysis and application. However, most existing methods for producing label taxonomies and building text-based label classifiers still rely heavily on domain expertise and manual curation, making the process expensive and time-consuming. This is particularly challenging when the label space is under-specified and large-scale data annotations are unavailable. In this paper, we address these challenges with Large Language Models (LLMs), whose prompt-based interface facilitates the induction and use of large-scale pseudo labels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate the process of end-to-end label generation and assignment with minimal human effort for any given use-case. In the first phase, we introduce a zero-shot, multi-stage reasoning approach which enables LLMs to produce and refine a label taxonomy iteratively. In the second phase, LLMs are used as data labelers that yield training samples so that lightweight supervised classifiers can be reliably built, deployed, and served at scale. We apply TnT-LLM to the analysis of user intent and conversational domain for Bing Copilot (formerly Bing Chat), an open-domain chat-based search engine. Extensive experiments using both human and automatic evaluation metrics demonstrate that TnT-LLM generates more accurate and relevant label taxonomies when compared against state-of-the-art baselines, and achieves a favorable balance between accuracy and efficiency for classification at scale.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TnT-LLM:+Text+Mining+at+Scale+with+Large+Language+Models)|0|
|[Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs](https://doi.org/10.1145/3637528.3671583)|Junjie Wang, Dan Yang, Binbin Hu, Yue Shen, Wen Zhang, Jinjie Gu|Zhejiang University & Ant Group, Hangzhou, China; Ant Group, Hangzhou, China; Zhejiang University, Hangzhou, China|In this paper, we explore a new way for user targeting, where non-expert marketers could select their target users solely given demands in natural language form. The key to this issue is how to transform natural languages into practical structured logical languages, i.e., the structured understanding of marketer demands. In practical scenarios, the demands of non-expert marketers are often abstract and diverse. Considering the impressive natural language processing ability of large language models (LLMs), we try to leverage LLMs to solve this issue. To stimulate the LLMs' reasoning ability, the chain-of-thought (CoT) prompting method is widely used, but existing methods still have some limitations in our scenario: (1) Previous methods either use simple "Let's think step by step" spells or provide fixed examples in demonstrations without considering compatibility between prompts and concrete questions, making LLMs ineffective when the marketers' demands are abstract and diverse. (2) Previous methods are often implemented in closed-source models or excessively large models, which is not suitable in industrial practical scenarios. Based on these, we propose ARALLM (i.e., Analogical Reasoning Augmented Large Language Models) consisting of two modules: Analogical Reasoning based Prompting and Reasoning-Augmented Multi-Task Model Distillation. Then, we adopt a retrieval-based method to conduct analogical reasoning with the help of the reasoning library. The experimental results show that this prompting strategy achieves better performance than the ordinary prompting method. Beyond that, we distill knowledge from super LLMs (GPT-3.5) to fine-tune smaller student LLMs in a multi-task training paradigm, enabling the models to be easily deployed in practical environments. Part of our data and code can be found at https://github.com/alipay/Analogic-Reasoning-Augmented-Large-Language-Model.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Know+Your+Needs+Better:+Towards+Structured+Understanding+of+Marketer+Demands+with+Analogical+Reasoning+Augmented+LLMs)|0|
|[COMET: NFT Price Prediction with Wallet Profiling](https://doi.org/10.1145/3637528.3671621)|Tianfu Wang, Liwei Deng, Chao Wang, Jianxun Lian, Yue Yan, Nicholas Jing Yuan, Qi Zhang, Hui Xiong|; Microsoft Inc., Beijing, China; Microsoft Inc., Suzhou, China; Microsoft Research Asia, Beijing, China|As the non-fungible token (NFT) market flourishes, price prediction emergesas a pivotal direction for investors gaining valuable insight to maximizereturns. However, existing works suffer from a lack of practical definitionsand standardized evaluations, limiting their practical application. Moreover,the influence of users' multi-behaviour transactions that are publiclyaccessible on NFT price is still not explored and exhibits challenges. In thispaper, we address these gaps by presenting a practical and hierarchical problemdefinition. This approach unifies both collection-level and token-level taskand evaluation methods, which cater to varied practical requirements ofinvestors. To further understand the impact of user behaviours on the variationof NFT price, we propose a general wallet profiling framework and develop aCOmmunity enhanced Multi-bEhavior Transaction graph model, named COMET. COMETprofiles wallets with a comprehensive view and considers the impact of diverserelations and interactions within the NFT ecosystem on NFT price variations,thereby improving prediction performance. Extensive experiments conducted inour deployed system demonstrate the superiority of COMET, underscoring itspotential in the insight toolkit for NFT investors.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=COMET:+NFT+Price+Prediction+with+Wallet+Profiling)|0|
|[Neural Optimization with Adaptive Heuristics for Intelligent Marketing System](https://doi.org/10.1145/3637528.3671591)|Changshuai Wei, Benjamin Zelditch, Joyce Chen, Andre Assuncao Silva T. Ribeiro, Jingyi Kenneth Tay, Borja Ocejo Elizondo, Sathiya Keerthi Selvaraj, Aman Gupta, Licurgo Benemann De Almeida|LinkedIn Corporation, Seattle, USA; LinkedIn Corporation, New York, USA; LinkedIn Corporation, Sunnyvale, USA; LinkedIn Corporation, New York, NY, USA|Computational marketing has become increasingly important in today's digitalworld, facing challenges such as massive heterogeneous data, multi-channelcustomer journeys, and limited marketing budgets. In this paper, we propose ageneral framework for marketing AI systems, the Neural Optimization withAdaptive Heuristics (NOAH) framework. NOAH is the first general framework formarketing optimization that considers both to-business (2B) and to-consumer(2C) products, as well as both owned and paid channels. We describe key modulesof the NOAH framework, including prediction, optimization, and adaptiveheuristics, providing examples for bidding and content optimization. We thendetail the successful application of NOAH to LinkedIn's email marketing system,showcasing significant wins over the legacy ranking system. Additionally, weshare details and insights that are broadly useful, particularly on: (i)addressing delayed feedback with lifetime value, (ii) performing large-scalelinear programming with randomization, (iii) improving retrieval with audienceexpansion, (iv) reducing signal dilution in targeting tests, and (v) handlingzero-inflated heavy-tail metrics in statistical testing.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Optimization+with+Adaptive+Heuristics+for+Intelligent+Marketing+System)|0|
|[On Finding Bi-objective Pareto-optimal Fraud Prevention Rule Sets for Fintech Applications](https://doi.org/10.1145/3637528.3671521)|Chengyao Wen, Yin Lou||Rules are widely used in Fintech institutions to make fraud prevention decisions, since rules are highly interpretable thanks to their intuitive if-then structure. In practice, a two-stage framework of fraud prevention decision rule set mining is usually employed in large Fintech institutions. This paper is concerned with finding high-quality rule subsets in a bi-objective space (such as precision and recall) from an initial pool of rules. To this end, we adopt the concept of Pareto optimality and aim to find a set of non-dominated rule subsets, which constitutes a Pareto front. We propose a heuristic-based framework called PORS and we identify that the core of PORS is the problem of solution selection on the front (SSF). We provide a systematic categorization of the SSF problem and a thorough empirical evaluation of various SSF methods on both public and proprietary datasets. We also introduce a novel variant of sequential covering algorithm called SpectralRules to encourage the diversity of the initial rule set and we empirically find that SpectralRules further improves the quality of the found Pareto front. On two real application scenarios within Alipay, we demonstrate the advantages of our proposed methodology compared to existing work.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Finding+Bi-objective+Pareto-optimal+Fraud+Prevention+Rule+Sets+for+Fintech+Applications)|0|
|[Nested Fusion: A Method for Learning High Resolution Latent Structure of Multi-Scale Measurement Data on Mars](https://doi.org/10.1145/3637528.3671596)|Austin P. Wright, Scott Davidoff, Duen Horng Chau|Georgia Tech, Atlanta, GA, USA; California Institute of Technology, Jet Propulsion Laboratory, Pasadena, CA, USA|The Mars Perseverance Rover represents a generational change in the scale of measurements that can be taken on Mars, however this increased resolution introduces new challenges for techniques in exploratory data analysis. The multiple different instruments on the rover each measures specific properties of interest to scientists, so analyzing how underlying phenomena affect multiple different instruments together is important to understand the full picture. However each instrument has a unique resolution, making the mapping between overlapping layers of data non-trivial. In this work, we introduce Nested Fusion, a method to combine arbitrarily layered datasets of different resolutions and produce a latent distribution at the highest possible resolution, encoding complex interrelationships between different measurements and scales. Our method is efficient for large datasets, can perform inference even on unseen data, and outperforms existing methods of dimensionality reduction and latent analysis on real-world Mars rover data. We have deployed our method Nested Fusion within a Mars science team at NASA Jet Propulsion Laboratory (JPL) and through multiple rounds of participatory design enabled greatly enhanced exploratory analysis workflows for real scientists. To ensure the reproducibility of our work we have open sourced our code on GitHub at https://github.com/pixlise/NestedFusion.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Nested+Fusion:+A+Method+for+Learning+High+Resolution+Latent+Structure+of+Multi-Scale+Measurement+Data+on+Mars)|0|
|[TrajRecovery: An Efficient Vehicle Trajectory Recovery Framework based on Urban-Scale Traffic Camera Records](https://doi.org/10.1145/3637528.3671558)|Dongen Wu, Ziquan Fang, Qichen Sun, Lu Chen, Haiyang Hu, Fei Wang, Yunjun Gao|Huawei Cloud Computing Technologies Co., Ltd, Hangzhou, China; Zhejiang University, Hangzhou, China|Accurate vehicle trajectory recovery enables providing indispensable data foundations in intelligent urban transportation. However, existing methods face two challenges: i) the inability to process city-wide vehicle trajectories, and ii) the dependence on a substantial amount of accurate GPS trajectories for model training, leading to poor generalization ability. To address these issues, we propose a novel trajectory recovery system based on vehicle snapshots captured by traffic cameras, named TrajRecovery. TrajRecovery consists of three main components: i) Preprocessor processes traffic cameras and vehicle snapshots to provide necessary data for trajectory recovery; ii) Spatial Transfer Probabilistic Model (STPM) integrates road conditions and driver behavior to compute turning probability at intersections; iii) Trajectory Generator utilizes the output probabilities from STPM to recover a continuous and most likely complete trajectory. We evaluate TrajRecovery on two real datasets from a city in China, demonstrating substantial performance gains compared to state-of-the-art methods. Furthermore, our system is deployed in practical applications at Huawei Company, achieving extraordinary profits in business scenarios.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TrajRecovery:+An+Efficient+Vehicle+Trajectory+Recovery+Framework+based+on+Urban-Scale+Traffic+Camera+Records)|0|
|[LaDe: The First Comprehensive Last-mile Express Dataset from Industry](https://doi.org/10.1145/3637528.3671548)|Lixia Wu, Haomin Wen, Haoyuan Hu, Xiaowei Mao, Yutong Xia, Ergang Shan, Jianbin Zheng, Junhong Lou, Yuxuan Liang, Liuqing Yang, Roger Zimmermann, Youfang Lin, Huaiyu Wan|; Cainiao Network, Hangzhou, China; Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; National University of Singapore, Singapore, Singapore; Jiaotong University|Real-world last-mile express datasets are crucial for research in logistics, supply chain management, and spatio-temporal data mining. Despite a plethora of algorithms developed to date, no widely accepted, publicly available last-mile express dataset exists to support research in this field. In this paper, we introduce LaDe, the first publicly available last-mile express dataset with millions of packages from the industry. LaDe has three unique characteristics: (1)Large-scale. It involves 10,677k packages of 21k couriers over 6 months of real-world operation. (2)Comprehensive information. It offers original package information, task-event information, as well as couriers' detailed trajecotries and road networks. (3)Diversity. The dataset includes data from various scenarios, including package pick-up and delivery, and from multiple cities, each with its unique spatio-temporal patterns due to their distinct characteristics such as populations. We verify LaDe on three tasks by running several classical baseline models per task. We believe that the large-scale, comprehensive, diverse feature of LaDe can offer unparalleled opportunities to researchers in the supply chain community, data mining community, and beyond. The dataset and code is publicly available at https://huggingface.co/datasets/Cainiao-AI/LaDe.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LaDe:+The+First+Comprehensive+Last-mile+Express+Dataset+from+Industry)|0|
|[Xinyu: An Efficient LLM-based System for Commentary Generation](https://doi.org/10.1145/3637528.3671537)|Yiquan Wu, Bo Tang, Chenyang Xi, Yu Yu, Pengyu Wang, Yifei Liu, Kun Kuang, Haiying Deng, Zhiyu Li, Feiyu Xiong, Jie Hu, Peng Cheng, Zhonghao Wang, Yi Wang, Yi Luo, Mingchuan Yang|Institute for Advanced Algorithms Research, Shanghai, China; Research Institute of China Telecom, Beijing, China; Northeastern University, Shenyang, China; Zhejiang University, Hangzhou, China; State Key Laboratory of Media Convergence Production Technology and Systems, Beijing, China|Commentary provides readers with a deep understanding of events by presenting diverse arguments and evidence. However, creating commentary is a time-consuming task, even for skilled commentators. Large language models (LLMs) have simplified the process of natural language generation, but their direct application in commentary creation still faces challenges due to unique task requirements. These requirements can be categorized into two levels: 1) fundamental requirements, which include creating well-structured and logically consistent narratives, and 2) advanced requirements, which involve generating quality arguments and providing convincing evidence. In this paper, we introduce Xinyu, an efficient LLM-based system designed to assist commentators in generating Chinese commentaries. To meet the fundamental requirements, we deconstruct the generation process into sequential steps, proposing targeted strategies and supervised fine-tuning (SFT) for each step. To address the advanced requirements, we present an argument ranking model for arguments and establish a comprehensive evidence database that includes up-to-date events and classic books, thereby strengthening the substantiation of the evidence with retrieval augmented generation (RAG) technology. To evaluate the generated commentaries more fairly, corresponding to the two-level requirements, we introduce a comprehensive evaluation metric that considers five distinct perspectives in commentary generation. Our experiments confirm the effectiveness of our proposed system. We also observe a significant increase in the efficiency of commentators in real-world scenarios, with the average time spent on creating a commentary dropping from 4 hours to 20 minutes. Importantly, such an increase in efficiency does not compromise the quality of the commentaries.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Xinyu:+An+Efficient+LLM-based+System+for+Commentary+Generation)|0|
|[DuMapNet: An End-to-End Vectorization System for City-Scale Lane-Level Map Generation](https://doi.org/10.1145/3637528.3671579)|Deguo Xia, Weiming Zhang, Xiyan Liu, Wei Zhang, Chenting Gong, Jizhou Huang, Mengmeng Yang, Diange Yang|Baidu Inc., Beijing, China; Tsinghua University & Baidu Inc., Beijing, China; Tsinghua University, Beijing, China|Generating city-scale lane-level maps faces significant challenges due to the intricate urban environments, such as blurred or absent lane markings. Additionally, a standard lane-level map requires a comprehensive organization of lane groupings, encompassing lane direction, style, boundary, and topology, yet has not been thoroughly examined in prior research. These obstacles result in labor-intensive human annotation and high maintenance costs. This paper overcomes these limitations and presents an industrial-grade solution named DuMapNet that outputs standardized, vectorized map elements and their topology in an end-to-end paradigm. To this end, we propose a group-wise lane prediction (GLP) system that outputs vectorized results of lane groups by meticulously tailoring a transformer-based network. Meanwhile, to enhance generalization in challenging scenarios, such as road wear and occlusions, as well as to improve global consistency, a contextual prompts encoder (CPE) module is proposed, which leverages the predicted results of spatial neighborhoods as contextual information. Extensive experiments conducted on large-scale real-world datasets demonstrate the superiority and effectiveness of DuMapNet. Additionally, DuMapNet has already been deployed in production at Baidu Maps since June 2023, supporting lane-level map generation tasks for over 360 cities while bringing a 95% reduction in costs. This demonstrates that DuMapNet serves as a practical and cost-effective industrial solution for city-scale lane-level map generation.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DuMapNet:+An+End-to-End+Vectorization+System+for+City-Scale+Lane-Level+Map+Generation)|0|
|[VecAug: Unveiling Camouflaged Frauds with Cohort Augmentation for Enhanced Detection](https://doi.org/10.1145/3637528.3671527)|Fei Xiao, Shaofeng Cai, Gang Chen, H. V. Jagadish, Beng Chin Ooi, Meihui Zhang|Beijing Institute of Technology, Beijing, China; National University of Singapore & Shopee Singapore, Singapore, Singapore; University of Michigan, Ann Arbor, USA; Zhejiang University, Hangzhou, China; National University of Singapore, Singapore, Singapore|Fraud detection presents a challenging task characterized by ever-evolving fraud patterns and scarce labeled data. Existing methods predominantly rely on graph-based or sequence-based approaches. While graph-based approaches connect users through shared entities to capture structural information, they remain vulnerable to fraudsters who can disrupt or manipulate these connections. In contrast, sequence-based approaches analyze users' behavioral patterns, offering robustness against tampering but overlooking the interactions between similar users. Inspired by cohort analysis in retention and healthcare, this paper introduces VecAug, a novel cohort-augmented learning framework that addresses these challenges by enhancing the representation learning of target users with personalized cohort information. To this end, we first propose a vector burn-in technique for automatic cohort identification, which retrieves a task-specific cohort for each target user. Then, to fully exploit the cohort information, we introduce an attentive cohort aggregation technique for augmenting target user representations. To improve the robustness of such cohort augmentation, we also propose a novel label-aware cohort neighbor separation mechanism to distance negative cohort neighbors and calibrate the aggregated cohort information. By integrating this cohort information with target user representations, VecAug enhances the modeling capacity and generalization capabilities of the model to be augmented. Our framework is flexible and can be seamlessly integrated with existing fraud detection models. We deploy our framework on e-commerce platforms and evaluate it on three fraud detection datasets, and results show that VecAug improves the detection performance of base models by up to 2.48% in AUC and 22.5% in R@P_0.9, outperforming state-of-the-art methods significantly.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=VecAug:+Unveiling+Camouflaged+Frauds+with+Cohort+Augmentation+for+Enhanced+Detection)|0|
|[Weather Knows What Will Occur: Urban Public Nuisance Events Prediction and Control with Meteorological Assistance](https://doi.org/10.1145/3637528.3671639)|Yi Xie, Tianyu Qiu, Yun Xiong, Xiuqi Huang, Xiaofeng Gao, Chao Chen, Qiang Wang, Haihong Li|College of Computer Science, Chongqing University, Chongqing, China; Shanghai Key Lab of Data Science, School of Computer Science, Fudan University, Shanghai, China; MoE Key Lab of Artificial Intelligence, Shanghai Jiao Tong University, Shanghai, China; Meteorological Disaster Prevention Centre, Shanghai Meteorological Bureau, Shanghai, China|Urban public nuisance events, like garbage exposure, illegal parking, facilities damage, and etc., impair the quality of life for city residents. Predicting and controlling these nuisances is crucial but complicated due to their ties to subjective and psychological factors. In this study, we reveal a significant correlation between such nuisances and meteorological indicators, influenced by the impact of climate on people's psychological states. We employ meteorology predictions that are integrated in Hawkes processes to enhance the accuracy of predicting the category and timing of these nuisances. To this end, we propose Spatial-Temporal Two-Tower Transformer (ST-T3), which simultaneously considers spatial data and further improves the prediction accuracy. Evaluated by about three-year data from both downtown and suburban Shanghai, our method outperforms both traditional and advanced prediction systems. We share a portion of the de-identified dataset for open research.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Weather+Knows+What+Will+Occur:+Urban+Public+Nuisance+Events+Prediction+and+Control+with+Meteorological+Assistance)|0|
|[Microservice Root Cause Analysis With Limited Observability Through Intervention Recognition in the Latent Space](https://doi.org/10.1145/3637528.3671530)|Zhe Xie, Shenglin Zhang, Yitong Geng, Yao Zhang, Minghua Ma, Xiaohui Nie, Zhenhe Yao, Longlong Xu, Yongqian Sun, Wentao Li, Dan Pei|Nankai University, Tianjin, China; eBay Inc., Shanghai, China; Microsoft, Redmond, USA; BNRist, Tsinghua University, Beijing, China; Computer Network Information Center, Chinese Academy of Sciences, Beijing, China|Many failure root cause analysis (RCA) algorithms for microservices have been proposed with the widespread adoption of microservices systems. Existing algorithms generally focus on RCA with ranking single-level (e.g. metric-level or service-level) root cause candidates (RCCs) with comprehensive monitoring metrics. However, many heterogeneous RCCs exist with limited observability in real-world microservices systems. Further, we find that the limited observability may result in inaccurate RCA through real-world failures in eBay. In this paper, for the first time, we propose to "model RCCs as latent variables". The core idea is to infer the status of RCCs as latent variables with related monitoring metrics instead of directly extracting features from only the observable metrics. Based on this, we propose LatentScope, an unsupervised RCA framework with heterogeneous RCCs under limited observability. A dual-space graph is proposed to model both observable and unobservable variables, with many-to-many relationships between spaces. To achieve fast inference of latent variables and RCA, we propose the LatentRegressor algorithm, which includes Regression-based Latent-space Intervention Recognition (RLIR) to achieve intervention recognition-based RCA in latent space. LatentScope has been deployed in eBay's production environment and evaluated on both eBay's real-world failures and a testbed dataset. The evaluation results show that, compared with baseline algorithms, our model significantly improves the Top-1 recall by 9.7%-57.9%. The source code of LatentScope and the dataset are available at https://github.com/NetManAIOps/LatentScope.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Microservice+Root+Cause+Analysis+With+Limited+Observability+Through+Intervention+Recognition+in+the+Latent+Space)|0|
|[Understanding the Weakness of Large Language Model Agents within a Complex Android Environment](https://doi.org/10.1145/3637528.3671650)|Mingzhe Xing, Rongkai Zhang, Hui Xue, Qi Chen, Fan Yang, Zhen Xiao|Microsoft Research, Beijing, China; Peking University, Beijing, China|Large language models (LLMs) have empowered intelligent agents to execute intricate tasks within domain-specific software such as browsers and games. However, when applied to general-purpose software systems like operating systems, LLM agents face three primary challenges. Firstly, the action space is vast and dynamic, posing difficulties for LLM agents to maintain an up-to-date understanding and deliver accurate responses. Secondly, real-world tasks often require inter-application cooperation, demanding farsighted planning from LLM agents. Thirdly, agents need to identify optimal solutions aligning with user constraints, such as security concerns and preferences. These challenges motivate AndroidArena, an environment and benchmark designed to evaluate LLM agents on a modern operating system. To address high-cost of manpower, we design a scalable and semi-automated method to construct the benchmark. In the task evaluation, AndroidArena incorporates accurate and adaptive metrics to address the issue of non-unique solutions. Our findings reveal that even state-of-the-art LLM agents struggle in cross-APP scenarios and adhering to specific constraints. Additionally, we identify a lack of four key capabilities, i.e. understanding, reasoning, exploration, and reflection, as primary reasons for the failure of LLM agents. Furthermore, we provide empirical analysis on the failure of reflection, and improve the success rate by 27% with our proposed exploration strategy. This work is the first to present valuable insights in understanding fine-grained weakness of LLM agents, and offers a path forward for future research in this area. Environment, benchmark, prompt, and evaluation code for AndroidArena are released at https://github.com/AndroidArenaAgent/AndroidArena.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+the+Weakness+of+Large+Language+Model+Agents+within+a+Complex+Android+Environment)|0|
|[XRL-Bench: A Benchmark for Evaluating and Comparing Explainable Reinforcement Learning Techniques](https://doi.org/10.1145/3637528.3671595)|Yu Xiong, Zhipeng Hu, Ye Huang, Runze Wu, Kai Guan, Xingchen Fang, Ji Jiang, Tianze Zhou, Yujing Hu, Haoyu Liu, Tangjie Lyu, Changjie Fan|Fuxi AI Lab, NetEase Inc., Hangzhou, China; Fuxi AI Lab, NetEase Inc., Hangzhou, Zhejiang, China|Reinforcement Learning (RL) has demonstrated substantial potential acrossdiverse fields, yet understanding its decision-making process, especially inreal-world scenarios where rationality and safety are paramount, is an ongoingchallenge. This paper delves in to Explainable RL (XRL), a subfield ofExplainable AI (XAI) aimed at unravelling the complexities of RL models. Ourfocus rests on state-explaining techniques, a crucial subset within XRLmethods, as they reveal the underlying factors influencing an agent's actionsat any given time. Despite their significant role, the lack of a unifiedevaluation framework hinders assessment of their accuracy and effectiveness. Toaddress this, we introduce XRL-Bench, a unified standardized benchmark tailoredfor the evaluation and comparison of XRL methods, encompassing three mainmodules: standard RL environments, explainers based on state importance, andstandard evaluators. XRL-Bench supports both tabular and image data for stateexplanation. We also propose TabularSHAP, an innovative and competitive XRLmethod. We demonstrate the practical utility of TabularSHAP in real-worldonline gaming services and offer an open-source benchmark platform for thestraightforward implementation and evaluation of XRL methods. Our contributionsfacilitate the continued progression of XRL technology.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=XRL-Bench:+A+Benchmark+for+Evaluating+and+Comparing+Explainable+Reinforcement+Learning+Techniques)|0|
|[FedGTP: Exploiting Inter-Client Spatial Dependency in Federated Graph-based Traffic Prediction](https://doi.org/10.1145/3637528.3671613)|Linghua Yang, Wantong Chen, Xiaoxi He, Shuyue Wei, Yi Xu, Zimu Zhou, Yongxin Tong|SKLCCSE Lab, Beihang University, Beijing, China; Faculty of Science and Technology, University of Macau, Macau, China; School of Data Science, City University of Hong Kong, Hong Kong, China; SKLCCSE Lab, Institute of Artificial Intelligence, Beihang University, Beijing, China|Graph-based methods have witnessed tremendous success in traffic prediction, largely attributed to their superior ability in capturing and modeling spatial dependencies. However, urban-scale traffic data are usually distributed among various owners, limited in sharing due to privacy restrictions. This fragmentation of data severely hinders interaction across clients, impeding the utilization of inter-client spatial dependencies. Existing studies have yet to address this non-trivial issue, thereby leading to sub-optimal performance. To fill this gap, we propose FedGTP, a new federated graph-based traffic prediction framework that promotes adaptive exploitation of inter-client spatial dependencies to recover close-to-optimal performance complying with privacy regulations like GDPR. We validate FedGTP via large-scale application-driven experiments on real-world datasets. Extensive baseline comparison, ablation study and case study demonstrate that FedGTP indeed surpasses existing methods through fully recovering inter-client spatial dependencies, achieving 21.08%, 13.48%, 19.90% decrease on RMSE, MAE and MAPE, respectively. Our code is available at https://github.com/LarryHawkingYoung/KDD2024_FedGTP||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedGTP:+Exploiting+Inter-Client+Spatial+Dependency+in+Federated+Graph-based+Traffic+Prediction)|0|
|[OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning](https://doi.org/10.1145/3637528.3671582)|Rui Ye, Wenhao Wang, Jingyi Chai, Dihan Li, Zexi Li, Yinda Xu, Yaxin Du, Yanfeng Wang, Siheng Chen|Zhejiang University, Zhejiang, China; Shanghai Jiao Tong University & Shanghai AI Laboratory, Shanghai, China; University of Southern California, Los Angeles, USA; Shanghai Jiao Tong University, Shanghai, China|Trained on massive publicly available data, large language models (LLMs) have demonstrated tremendous success across various fields. While more data contributes to better performance, a disconcerting reality is that high-quality public data will be exhausted in a few years. In this paper, we offer a potential next step for contemporary LLMs: collaborative and privacy-preserving LLM training on the underutilized distributed private data via federated learning (FL), where multiple data owners collaboratively train a shared model without transmitting raw data. To achieve this, we build a concise, integrated, and research-friendly framework/codebase, named OpenFedLLM. It covers federated instruction tuning for enhancing instruction-following capability, federated value alignment for aligning with human values, and 7 representative FL algorithms. Besides, OpenFedLLM supports training on diverse domains, where we cover 8 training datasets; and provides comprehensive evaluations, where we cover 30+ evaluation metrics. Through extensive experiments, we observe that all FL algorithms outperform local training on training LLMs, demonstrating a clear performance improvement across a variety of settings. Notably, in a financial benchmark, Llama2-7B fine-tuned by applying any FL algorithm can outperform GPT-4 by a significant margin, while the model obtained through individual training cannot, demonstrating strong motivation for clients to participate in FL. The code is available at https://github.com/rui-ye/OpenFedLLM. The full version of our paper is available at https://arxiv.org/pdf/2402.06954.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OpenFedLLM:+Training+Large+Language+Models+on+Decentralized+Private+Data+via+Federated+Learning)|0|
|[PAIL: Performance based Adversarial Imitation Learning Engine for Carbon Neutral Optimization](https://doi.org/10.1145/3637528.3671611)|Yuyang Ye, LuAn Tang, Haoyu Wang, Runlong Yu, Wenchao Yu, Erhu He, Haifeng Chen, Hui Xiong|; Department of Computer Science, University of Pittsburgh, Pittsburgh, PA, USA; Department of Data Science and System Security, NEC Laboratories, Princeton, NJ, USA|Achieving carbon neutrality within industrial operations has become increasingly imperative for sustainable development. It is both a significant challenge and a key opportunity for operational optimization in industry 4.0. In recent years, Deep Reinforcement Learning (DRL) based methods offer promising enhancements for sequential optimization processes and can be used for reducing carbon emissions. However, existing DRL methods need a pre-defined reward function to assess the impact of each action on the final sustainable development goals (SDG). In many real applications, such a reward function cannot be given in advance. To address the problem, this study proposes a Performance based Adversarial Imitation Learning (PAIL) engine. It is a novel method to acquire optimal operational policies for carbon neutrality without any pre-defined action rewards. Specifically, PAIL employs a Transformer-based policy generator to encode historical information and predict following actions within a multi-dimensional space. The entire action sequence will be iteratively updated by an environmental simulator. Then PAIL uses a discriminator to minimize the discrepancy between generated sequences and real-world samples of high SDG. In parallel, a Q-learning framework based performance estimator is designed to estimate the impact of each action on SDG. Based on these estimations, PAIL refines generated policies with the rewards from both discriminator and performance estimator. PAIL is evaluated on multiple real-world application cases and datasets. The experiment results demonstrate the effectiveness of PAIL comparing to other state-of-the-art baselines. In addition, PAIL offers meaningful interpretability for the optimization in carbon neutrality.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PAIL:+Performance+based+Adversarial+Imitation+Learning+Engine+for+Carbon+Neutral+Optimization)|0|
|[SepsisLab: Early Sepsis Prediction with Uncertainty Quantification and Active Sensing](https://doi.org/10.1145/3637528.3671586)|Changchang Yin, PinYu Chen, Bingsheng Yao, Dakuo Wang, Jeffrey M. Caterino, Ping Zhang|The Ohio State University, Columbus, OH, USA; Northeastern University, Boston, MA, USA; The Ohio State University Wexner Medical Center, Columbus, OH, USA; IBM Research, Yorktown Heights, NY, USA|Sepsis is the leading cause of in-hospital mortality in the USA. Early sepsis onset prediction and diagnosis could significantly improve the survival of sepsis patients. Existing predictive models are usually trained on high-quality data with few missing information, while missing values widely exist in real-world clinical scenarios (especially in the first hours of admissions to the hospital), which causes a significant decrease in accuracy and an increase in uncertainty for the predictive models. The common method to handle missing values is imputation, which replaces the unavailable variables with estimates from the observed data. The uncertainty of imputation results can be propagated to the sepsis prediction outputs, which have not been studied in existing works on either sepsis prediction or uncertainty quantification. In this study, we first define such propagated uncertainty as the variance of prediction output and then introduce uncertainty propagation methods to quantify the propagated uncertainty. Moreover, for the potential high-risk patients with low confidence due to limited observations, we propose a robust active sensing algorithm to increase confidence by actively recommending clinicians to observe the most informative variables. We validate the proposed models in both publicly available data (i.e., MIMIC-III and AmsterdamUMCdb) and proprietary data in The Ohio State University Wexner Medical Center (OSUWMC). The experimental results show that the propagated uncertainty is dominant at the beginning of admissions to hospitals and the proposed algorithm outperforms state-of-the-art active sensing methods. Finally, we implement a SepsisLab system for early sepsis prediction and active sensing based on our pre-trained models. Clinicians and potential sepsis patients can benefit from the system in early prediction and diagnosis of sepsis.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SepsisLab:+Early+Sepsis+Prediction+with+Uncertainty+Quantification+and+Active+Sensing)|0|
|[Pre-trained KPI Anomaly Detection Model Through Disentangled Transformer](https://doi.org/10.1145/3637528.3671522)|Zhaoyang Yu, Changhua Pei, Xin Wang, Minghua Ma, Chetan Bansal, Saravan Rajmohan, Qingwei Lin, Dongmei Zhang, Xidao Wen, Jianhui Li, Gaogang Xie, Dan Pei|Stony Brook University, New York, USA; Microsoft, Redmond, USA; BizSeer Technology, Beijing, China; Tsinghua University & BNRist, Beijing, China; Computer Network Information Center, Chinese Academy of Sciences, Beijing, China; Microsoft, Beijing, China|In large-scale online service systems, numerous Key Performance Indicators (KPIs), such as service response time and error rate, are gathered in a time-series format. KPI Anomaly Detection (KAD) is a critical data mining problem due to its widespread applications in real-world scenarios. However, KAD faces the challenges of dealing with KPI heterogeneity and noisy data. We propose KAD-Disformer, a KPI Anomaly Detection approach through Disentangled Transformer. KAD-Disformer pre-trains a model on existing accessible KPIs, and the pre-trained model can be effectively "fine-tuned" to unseen KPI using only a handful of samples from the unseen KPI. We propose a series of innovative designs, including disentangled projection for transformer, unsupervised few-shot fine-tuning (uTune), and denoising modules, each of which significantly contributes to the overall performance. Our extensive experiments demonstrate that KAD-Disformer surpasses the state-of-the-art universal anomaly detection model by 13% in F1-score and achieves comparable performance using only 1/8 of the finetuning samples saving about 25 hours. KAD-Disformer has been successfully deployed in the real-world cloud system serving millions of users, attesting to its feasibility and robustness. Our code is available at https://github.com/NetManAIOps/KAD-Disformer.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pre-trained+KPI+Anomaly+Detection+Model+Through+Disentangled+Transformer)|0|
|[An Offline Meta Black-box Optimization Framework for Adaptive Design of Urban Traffic Light Management Systems](https://doi.org/10.1145/3637528.3671606)|Taeyoung Yun, Kanghoon Lee, Sujin Yun, Ilmyung Kim, WonWoo Jung, MinCheol Kwon, Kyujin Choi, Yoohyeon Lee, Jinkyoo Park|KAIST, Daejeon, Republic of Korea; Korea Telecom, Seoul, Republic of Korea|Complex urban road networks with high vehicle occupancy frequently face severe traffic congestion. Designing an effective strategy for managing multiple traffic lights plays a crucial role in managing congestion. However, most current traffic light management systems rely on human-crafted decisions, which may not adapt well to diverse traffic patterns. In this paper, we delve into two pivotal design components of the traffic light management system that can be dynamically adjusted to various traffic conditions: phase combination and phase time allocation. While numerous studies have sought an efficient strategy for managing traffic lights, most of these approaches consider a fixed traffic pattern and are limited to relatively small road networks. To overcome these limitations, we introduce a novel and practical framework to formulate the optimization of such design components using an offline meta black-box optimization. We then present a simple yet effective method to efficiently find a solution for the aforementioned problem. In our framework, we first collect an offline meta dataset consisting of pairs of design choices and corresponding congestion measures from various traffic patterns. After collecting the dataset, we employ the Attentive Neural Process (ANP) to predict the impact of the proposed design on congestion across various traffic patterns with well-calibrated uncertainty. Finally, Bayesian optimization, with ANP as a surrogate model, is utilized to find an optimal design for unseen traffic patterns through limited online simulations. Our experiment results show that our method outperforms state-of-the-art baselines on complex road networks in terms of the number of waiting vehicles. Surprisingly, the deployment of our method into a real-world traffic system was able to improve traffic throughput by 4.80% compared to the original strategy.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Offline+Meta+Black-box+Optimization+Framework+for+Adaptive+Design+of+Urban+Traffic+Light+Management+Systems)|0|
|[OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining](https://doi.org/10.1145/3637528.3672354)|Fanjin Zhang, Shijie Shi, Yifan Zhu, Bo Chen, Yukuo Cen, Jifan Yu, Yelin Chen, Lulu Wang, Qingfei Zhao, Yuqing Cheng, Tianyi Han, Yuwei An, Dan Zhang, Weng Lam Tam, Kun Cao, Yunhe Pang, Xinyu Guan, Huihui Yuan, Jian Song, Xiaoyan Li, Yuxiao Dong, Jie Tang|Tsinghua University, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; Zhipu AI, Beijing, China; Biendata, Beijing, China|With the rapid proliferation of scientific literature, versatile academicknowledge services increasingly rely on comprehensive academic graph mining.Despite the availability of public academic graphs, benchmarks, and datasets,these resources often fall short in multi-aspect and fine-grained annotations,are constrained to specific task types and domains, or lack underlying realacademic graphs. In this paper, we present OAG-Bench, a comprehensive,multi-aspect, and fine-grained human-curated benchmark based on the OpenAcademic Graph (OAG). OAG-Bench covers 10 tasks, 20 datasets, 70+ baselines,and 120+ experimental results to date. We propose new data annotationstrategies for certain tasks and offer a suite of data pre-processing codes,algorithm implementations, and standardized evaluation protocols to facilitateacademic graph mining. Extensive experiments reveal that even advancedalgorithms like large language models (LLMs) encounter difficulties inaddressing key challenges in certain tasks, such as paper source tracing andscholar profiling. We also introduce the Open Academic Graph Challenge(OAG-Challenge) to encourage community input and sharing. We envisage thatOAG-Bench can serve as a common ground for the community to evaluate andcompare algorithms in academic graph mining, thereby accelerating algorithmdevelopment and advancement in this field. OAG-Bench is accessible athttps://www.aminer.cn/data/.|随着科学文献的快速增长，多功能学术知识服务越来越依赖于全面的学术图形挖掘。尽管有公开的学术图表、基准和数据集，但这些资源往往缺乏多方面和细粒度的注释，受限于特定的任务类型和领域，或缺乏基础的真实学术图表。本文介绍了 OAG-Bench，它是一个基于开放学术图的全面的、多方面的、细粒度的人类管理基准。OAG-Bench 涵盖了10个任务、20个数据集、70 + 基线和迄今为止的120 + 实验结果。我们针对某些任务提出了新的数据注释/策略，并提供了一套数据预处理代码、算法实现和标准化评估协议，以促进学术图形挖掘。大量的实验表明，即使是像大型语言模型(LLM)这样的高级算法，在处理某些任务中的关键挑战时也会遇到困难，例如文件来源跟踪和学者剖析。我们还推出了开放学术图形挑战(OAG-Challenge) ，以鼓励社区投入和分享。我们设想 OAG-Bench 可以作为社区评估和比较学术图形挖掘算法的共同基础，从而加速算法的发展和进步。OAG-bench 可访问 https:// www.aminer.cn/data/。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OAG-Bench:+A+Human-Curated+Benchmark+for+Academic+Graph+Mining)|0|
|[Dólares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs Between Spanish and English](https://doi.org/10.1145/3637528.3671554)|Xiao Zhang, Ruoyu Xiang, Chenhan Yuan, Duanyu Feng, Weiguang Han, Alejandro LopezLira, XiaoYang Liu, Meikang Qiu, Sophia Ananiadou, Min Peng, Jimin Huang, Qianqian Xie|; The University of Manchester, Manchester, United Kingdom; Sichuan University, Chengdu, Sichuan, China; Columbia University, New York, NY, USA; Augusta University, Augusta, USA; Wuhan University, Wuhan, Hubei, China; University of Florida, Gainesville, USA; The Fin AI, Singapore, Singapore|Despite Spanish's pivotal role in the global finance industry, a pronouncedgap exists in Spanish financial natural language processing (NLP) andapplication studies compared to English, especially in the era of largelanguage models (LLMs). To bridge this gap, we unveil Toisón de Oro, thefirst bilingual framework that establishes instruction datasets, finetunedLLMs, and evaluation benchmark for financial LLMs in Spanish joint withEnglish. We construct a rigorously curated bilingual instruction datasetincluding over 144K Spanish and English samples from 15 datasets covering 7tasks. Harnessing this, we introduce FinMA-ES, an LLM designed for bilingualfinancial applications. We evaluate our model and existing LLMs using FLARE-ES,the first comprehensive bilingual evaluation benchmark with 21 datasetscovering 9 tasks. The FLARE-ES benchmark results reveal a significantmultilingual performance gap and bias in existing LLMs. FinMA-ES models surpassSOTA LLMs such as GPT-4 in Spanish financial tasks, due to strategicinstruction tuning and leveraging data from diverse linguistic resources,highlighting the positive impact of cross-linguistic transfer. All ourdatasets, models, and benchmarks have been released.|尽管西班牙语在全球金融业中扮演着举足轻重的角色，但与英语相比，尤其是在大语言模型时代，西班牙语的金融自然语言处理(NLP)和应用研究方面存在着明显的差距。为了弥补这一差距，我们推出了 Toisón de Oro，这是第一个建立教学数据集，finetunedLLM，以及西班牙语和英语联合金融 LLM 评估基准的双语框架。我们建立了一个严格管理的双语教学数据集，包括超过144K 的西班牙语和英语样本从15个数据集涵盖7个任务。利用这一点，我们介绍了 FinMA-ES，一个为双语金融应用而设计的 LLM。我们使用 FLARE-ES 评估我们的模型和现有的 LLM。 FLARE-ES 是第一个全面的双语评估基准，共有21个数据集，涵盖9个任务。FLARE-ES 基准测试结果显示现有 LLM 存在显著的多语言性能差距和偏差。FinMA-ES 模型在西班牙金融任务方面超过了 GPT-4等 SOTA LLM 模型，这是由于战略指令调整和利用来自不同语言资源的数据，突出了跨语言转换的积极影响。我们所有的数据集、模型和基准已经发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dólares+or+Dollars?+Unraveling+the+Bilingual+Prowess+of+Financial+LLMs+Between+Spanish+and+English)|0|
|[Large Language Model with Curriculum Reasoning for Visual Concept Recognition](https://doi.org/10.1145/3637528.3671653)|Yipeng Zhang, Xin Wang, Hong Chen, Jiapei Fan, Weigao Wen, Hui Xue, Hong Mei, Wenwu Zhu|DCST, BNRist, Tsinghua University, Beijing, China; MoE Lab, Peking University, Beijing, China; Alibaba Group, Hangzhou, China; DCST, Tsinghua University, Beijing, China|Visual concept recognition aims to capture the basic attributes of an image and reason about the relationships among them to determine whether the image satisfies a certain concept, and has been widely used in various tasks such as human action recognition and image risk warning. Most existing works adopt deep neural networks for visual concept recognition, which are black-box and incomprehensible to humans, thus making them unacceptable for sensitive domains such as prohibited event detection and risk early warning etc. To address this issue, we propose to combine large language model (LLM) with explainable symbolic reasoning via curriculum reweighting to increase the interpretability and accuracy of visual concept recognition in this paper. However, realizing this goal is challenging given that i) the performance of symbolic representations are limited by the lack of annotated reasoning symbols and rules for most tasks, and ii) the LLMs may suffer from knowlege hallucination and dynamic open environment. To address these issues, in this paper, we propose CurLLM-Reasoner, a curriculum reasoning method based on symbolic reasoning and large language model for visual concept recognition. Specifically, we propose a novel rule enhancement module with a tool library, which fully leverage the reasoning capability of large language models and can generate human-understandable rules without any annotation. We further propose a curriculum data resampling methodology to help the large language model accurately extract from easy to complex rules at different reasoning stages. Extensive experiments on various datasets demonstrate that CurLLM-Reasoner can achieve the state-of-the-art visual concept recognition results with explainable rules while free of human annotations.|视觉概念识别的目的是获取图像的基本属性以及它们之间关系的原因，以确定图像是否满足一定的概念，已广泛应用于人类行为识别和图像风险预警等各种任务中。现有作品大多采用深层神经网络进行视觉概念识别，这是一个人类难以理解的黑盒子，因此不适用于敏感领域，如违禁事件检测和风险预警等。针对这一问题，本文提出通过课程重新加权，将大语言模型(LLM)与可解释的符号推理相结合，提高视觉概念识别的可解释性和准确性。然而，实现这一目标是具有挑战性的，因为 i)符号表示的性能受到大多数任务缺乏注释推理符号和规则的限制，以及 ii) LLM 可能遭受知识幻觉和动态开放环境。针对这些问题，本文提出了一种基于符号推理和大语言模型的课程推理方法 CurLLM-Requoner，用于视觉概念识别。具体地说，我们提出了一种新的规则增强模块，该模块使用了工具库，充分利用了大型语言模型的推理能力，可以在不需要任何注释的情况下生成人们可以理解的规则。进一步提出了一种课程数据重采样方法，以帮助大语言模型在不同的推理阶段准确地提取易于复杂的规则。在各种数据集上的大量实验表明，CurLLM-Requoner 能够在不需要人工注释的情况下，通过可解释的规则实现最先进的视觉概念识别。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Model+with+Curriculum+Reasoning+for+Visual+Concept+Recognition)|0|
|[GraSS: Combining Graph Neural Networks with Expert Knowledge for SAT Solver Selection](https://doi.org/10.1145/3637528.3671627)|Zhanguang Zhang, Didier Chételat, Joseph Cotnareanu, Amur Ghose, Wenyi Xiao, HuiLing Zhen, Yingxue Zhang, Jianye Hao, Mark Coates, Mingxuan Yuan|Huawei Noah's Ark Lab, Hong Kong, China; Huawei Noah's Ark Lab, Beijing, China; Huawei Noah's Ark Lab, Montreal, Canada; McGill University, Montreal, Canada|Boolean satisfiability (SAT) problems are routinely solved by SAT solvers inreal-life applications, yet solving time can vary drastically between solversfor the same instance. This has motivated research into machine learning modelsthat can predict, for a given SAT instance, which solver to select amongseveral options. Existing SAT solver selection methods all rely on somehand-picked instance features, which are costly to compute and ignore thestructural information in SAT graphs. In this paper we present GraSS, a novelapproach for automatic SAT solver selection based on tripartite graphrepresentations of instances and a heterogeneous graph neural network (GNN)model. While GNNs have been previously adopted in other SAT-related tasks, theydo not incorporate any domain-specific knowledge and ignore the runtimevariation introduced by different clause orders. We enrich the graphrepresentation with domain-specific decisions, such as novel node featuredesign, positional encodings for clauses in the graph, a GNN architecturetailored to our tripartite graphs and a runtime-sensitive loss function.Through extensive experiments, we demonstrate that this combination of rawrepresentations and domain-specific choices leads to improvements in runtimefor a pool of seven state-of-the-art solvers on both an industrial circuitdesign benchmark, and on instances from the 20-year Anniversary Track of the2022 SAT Competition.|布尔可满足性(SAT)问题通常由 SAT 求解器在实际应用中解决，但在同一实例中，求解时间可能会因求解器的不同而有很大差异。这激发了对机器学习模型的研究，该模型可以预测给定的 SAT 实例中哪个求解器可以从多个选项中进行选择。现有的 SAT 求解器选择方法都依赖于精心挑选的实例特征，计算和忽略 SAT 图中的结构信息代价高昂。本文提出了一种基于实例三部图表示和异构图神经网络(GNN)模型的自动 SAT 求解器选择新方法—— GraSS。虽然 GNN 以前已经在其他 SAT 相关任务中采用，但它们没有包含任何特定领域的知识，并且忽略了不同子句顺序引入的运行时变化。我们用特定领域的决策来丰富图表示，例如新的节点特征设计，图中子句的位置编码，根据我们的三部图定制的 GNN 体系结构和运行时敏感的损失函数。通过大量的实验，我们证明了这种原始表示和领域特定选择的结合导致了在工业电路设计基准和2022年 SAT 竞赛20周年纪念赛道上的七个最先进的解决方案池的运行时间的改善。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraSS:+Combining+Graph+Neural+Networks+with+Expert+Knowledge+for+SAT+Solver+Selection)|0|
|[Diet-ODIN: A Novel Framework for Opioid Misuse Detection with Interpretable Dietary Patterns](https://doi.org/10.1145/3637528.3671587)|Zheyuan Zhang, Zehong Wang, Shifu Hou, Evan Hall, Landon Bachman, Jasmine White, Vincent Galassi, Nitesh V. Chawla, Chuxu Zha, Yanfang Ye|University of Notre Dame, Notre Dame, IN, USA; Purdue University, West Lafayette, IN, USA; Brandeis University, Waltham, MA, USA|The opioid crisis has been one of the most critical society concerns in the United States. Although the medication assisted treatment (MAT) is recognized as the most effective treatment for opioid misuse and addiction, the various side effects can trigger opioid relapse. In addition to MAT, the dietary nutrition intervention has been demonstrated its importance in opioid misuse prevention and recovery. However, research on the alarming connections between dietary patterns and opioid misuse remain under-explored. In response to this gap, in this paper, we first establish a large-scale multifaceted dietary benchmark dataset related to opioid users at the first attempt and then develop a novel framework - i.e., namely Opioid Misuse Detection with INterpretable Dietary Patterns (Diet-ODIN) - to bridge heterogeneous graph (HG) and large language model (LLM) for the identification of users with opioid misuse and the interpretation of their associated dietary patterns. Specifically, in Diet-ODIN, we first construct an HG to comprehensively incorporate both dietary and health-related information, and then we devise a holistic graph learning framework with noise reduction to fully capitalize both users' individual dietary habits and shared dietary patterns for the detection of users with opioid misuse. To further delve into the intricate correlations between dietary patterns and opioid misuse, we exploit an LLM by utilizing the knowledge obtained from the graph learning model for interpretation. The extensive experimental results based on our established benchmark with quantitative and qualitative measures demonstrate the outstanding performance of Diet-ODIN on exploring the complex interplay between opioid misuse and dietary patterns, by comparison with state-of-the-art baseline methods. Our code, built benchmark and system demo are available at https://github.com/JasonZhangzy1757/Diet-ODIN.|阿片类药物危机一直是美国社会最关注的问题之一。虽然药物辅助治疗(MAT)被认为是最有效的阿片类药物滥用和成瘾的治疗方法，但各种副作用可能引发阿片类药物复发。除 MAT 外，膳食营养干预在阿片类药物滥用预防和康复中的重要性也得到了证实。然而，关于饮食模式与阿片类药物滥用之间令人担忧的联系的研究仍然不足。针对这一差距，在本文中，我们首先建立了一个与阿片类药物使用者相关的大规模多方面饮食基准数据集，然后开发了一个新的框架，即具有可解释饮食模式(Diet-ODIN)的阿片类误用检测，以桥接异质图(HG)和大语言模型(LLM) ，用于识别滥用阿片类药物的使用者并解释其相关的饮食模式。具体而言，在 Diet-ODIN 中，我们首先构建一个 HG 来全面纳入饮食和健康相关信息，然后我们设计一个具有降噪的整体图形学习框架，以充分利用用户的个人饮食习惯和共享饮食模式来检测阿片类药物滥用的用户。为了进一步研究饮食模式和阿片类药物滥用之间错综复杂的相关性，我们利用从图形学习模型中获得的知识来解释 LLM。基于我们建立的定量和定性测量基准的广泛实验结果表明，与最先进的基线方法相比，Diet-ODIN 在探索阿片类药物滥用和饮食模式之间复杂的相互作用方面表现出色。我们的代码，建立的基准和系统演示可在 https://github.com/jasonzhangzy1757/diet-odin 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Diet-ODIN:+A+Novel+Framework+for+Opioid+Misuse+Detection+with+Interpretable+Dietary+Patterns)|0|
|[TACCO: Task-guided Co-clustering of Clinical Concepts and Patient Visits for Disease Subtyping based on EHR Data](https://doi.org/10.1145/3637528.3671594)|Ziyang Zhang, Hejie Cui, Ran Xu, Yuzhang Xie, Joyce C. Ho, Carl Yang|Emory University, Atlanta, GA, USA|The growing availability of well-organized Electronic Health Records (EHR) data has enabled the development of various machine learning models towards disease risk prediction. However, existing risk prediction methods overlook the heterogeneity of complex diseases, failing to model the potential disease subtypes regarding their corresponding patient visits and clinical concept subgroups. In this work, we introduce TACCO, a novel framework that jointly discovers clusters of clinical concepts and patient visits based on a hypergraph modeling of EHR data. Specifically, we develop a novel self-supervised co-clustering framework that can be guided by the risk prediction task of specific diseases. Furthermore, we enhance the hypergraph model of EHR data with textual embeddings and enforce the alignment between the clusters of clinical concepts and patient visits through a contrastive objective. Comprehensive experiments conducted on the public MIMIC-III dataset and Emory internal CRADLE dataset over the downstream clinical tasks of phenotype classification and cardiovascular risk prediction demonstrate an average 31.25% performance improvement compared to traditional ML baselines and a 5.26% improvement on top of the vanilla hypergraph model without our co-clustering mechanism. In-depth model analysis, clustering results analysis, and clinical case studies further validate the improved utilities and insightful interpretations delivered by TACCO. Code is available at https://github.com/PericlesHat/TACCO.|组织良好的电子健康记录(EHR)数据的不断增加使得各种机器学习模型朝着疾病风险预测的方向发展。然而，现有的风险预测方法忽视了复杂疾病的异质性，未能根据相应的患者就诊和临床概念亚组对潜在的疾病亚型进行建模。在这项工作中，我们介绍了 TACCO，一个新的框架，共同发现临床概念和病人访问的超图建模的基础上的 EHR 数据簇。具体来说，我们开发了一个新的自我监督协聚类框架，可以指导特定疾病的风险预测任务。此外，我们通过文本嵌入增强 EHR 数据的超图模型，并通过对比目标强化临床概念簇和患者访问之间的对齐。在公共 MIMIC-III 数据集和 Emory 内部 CRADLE 数据集上对表型分类和心血管风险预测的下游临床任务进行的综合实验表明，与传统 ML 基线相比，平均性能提高了31.25% ，在香草超图模型的基础上提高了5.26% ，而没有我们的共聚类机制。深入的模型分析、聚类结果分析和临床病例研究进一步验证了 TACCO 改进的实用性和深刻的解释。密码可于 https://github.com/pericleshat/tacco 索取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TACCO:+Task-guided+Co-clustering+of+Clinical+Concepts+and+Patient+Visits+for+Disease+Subtyping+based+on+EHR+Data)|0|
|[DUE: Dynamic Uncertainty-Aware Explanation Supervision via 3D Imputation](https://doi.org/10.1145/3637528.3671641)|Qilong Zhao, Yifei Zhang, Mengdan Zhu, Siyi Gu, Yuyang Gao, Xiaofeng Yang, Liang Zhao|Emory University, Atlanta, GA, USA; The Home Depot, Atlanta, GA, USA; Stanford University, Palo Alto, CA, USA|Explanation supervision aims to enhance deep learning models by integrating additional signals to guide the generation of model explanations, showcasing notable improvements in both the predictability and explainability of the model. However, the application of explanation supervision to higher-dimensional data, such as 3D medical images, remains an under-explored domain. Challenges associated with supervising visual explanations in the presence of an additional dimension include: 1) spatial correlation changed, 2) lack of direct 3D annotations, and 3) uncertainty varies across different parts of the explanation. To address these challenges, we propose a Dynamic Uncertainty-aware Explanation supervision (DUE\footnoteCode available at: https://github.com/AlexQilong/DUE.) framework for 3D explanation supervision that ensures uncertainty-aware explanation guidance when dealing with sparsely annotated 3D data with diffusion-based 3D interpolation. Our proposed framework is validated through comprehensive experiments on diverse real-world medical imaging datasets. The results demonstrate the effectiveness of our framework in enhancing the predictability and explainability of deep learning models in the context of medical imaging diagnosis applications.|解释监督旨在通过整合额外的信号来指导模型解释的生成，从而增强深度学习模型，显示模型在可预测性和可解释性方面的显著改善。然而，解释监督应用于高维数据，如三维医学图像，仍然是一个探索不足的领域。在存在额外维度的情况下，与监督视觉解释相关的挑战包括: 1)空间相关性改变，2)缺乏直接的3D 注释，3)解释的不同部分存在不同的不确定性。为了应对这些挑战，我们提出了一个动态不确定性解释监督(DUE footescode，可在以下 https://github.com/alexqilong/DUE 获得)三维解释监督框架，确保在处理基于扩散的三维插值的稀疏注释的三维数据时，不确定性意识的解释指导。我们提出的框架是通过对不同的现实世界医学影像数据集的综合实验验证。结果表明，我们的框架在提高医学影像诊断应用背景下的深度学习模型的可预测性和可解释性的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DUE:+Dynamic+Uncertainty-Aware+Explanation+Supervision+via+3D+Imputation)|0|
|[Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy](https://doi.org/10.1145/3637528.3671614)|Yao Zhao, Zhitian Xie, Chen Liang, Chenyi Zhuang, Jinjie Gu|Ant Group, Hangzhou, China|As Large Language Models (LLMs) have made significant advancements across various tasks, such as question answering, translation, text summarization, and dialogue systems, the need for accuracy in information becomes crucial, especially for serious financial products serving billions of users like Alipay. However, for a real-world product serving millions of users, the inference speed of LLMs becomes a critical factor compared to a mere experimental model. Hence, this paper presents a generic framework for accelerating the inference process, resulting in a substantial increase in speed and cost reduction for our LLM-based scenarios, with lossless generation accuracy. In the traditional inference process, each token is generated sequentially by the LLM, leading to a time consumption proportional to the number of generated tokens. To enhance this process, our framework, named lookahead, introduces a multi-branch strategy. Instead of generating a single token at a time, we propose a Trie-based retrieval and verification mechanism to be able to accept several tokens at a forward step. Our strategy offers two distinct advantages: (1) it guarantees absolute correctness of the output, avoiding any approximation algorithms, and (2) the worst-case performance of our approach could be comparable with the performance of the conventional process. We conduct extensive experiments to demonstrate the significant improvements achieved by applying our inference acceleration framework. Our framework has been widely deployed in Alipay since April 2023, and obtained remarkable 2.66x to 6.26x speedup. Our code is available at https://github.com/alipay/PainlessInferenceAcceleration.|随着大型语言模型(LLM)在诸如问答、翻译、文本摘要和对话系统等各种任务中取得重大进展，对信息准确性的需求变得至关重要，尤其是对于像支付宝这样为数十亿用户服务的重要金融产品。然而，对于一个服务于数百万用户的实际产品来说，LLM 的推理速度成为一个关键因素，而不仅仅是一个实验模型。因此，本文提出了一个加速推理过程的通用框架，从而大大提高了基于 LLM 的场景的速度和降低了成本，并且具有无损生成精度。在传统的推理过程中，每个令牌都是由 LLM 顺序生成的，从而导致与生成的令牌数量成比例的时间消耗。为了增强这个过程，我们的框架(名为 lookahead)引入了一个多分支策略。我们提出了一种基于 Trie 的检索和验证机制，以便能够在一个转发步骤中接受多个令牌，而不是一次生成一个令牌。我们的策略提供了两个明显的优势: (1)它保证了输出的绝对正确性，避免了任何近似算法，和(2)我们的方法的最坏情况下的性能可以与传统过程的性能相媲美。我们进行了广泛的实验来证明通过应用我们的推理加速框架所取得的重大改进。我们的框架从2023年4月开始在支付宝上广泛应用，并获得了2.66 ~ 6.26倍的显著加速。我们的代码可以在 https://github.com/alipay/painlessinferenceacceleration 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Lookahead:+An+Inference+Acceleration+Framework+for+Large+Language+Model+with+Lossless+Generation+Accuracy)|0|
|[Decision Focused Causal Learning for Direct Counterfactual Marketing Optimization](https://doi.org/10.1145/3637528.3672353)|Hao Zhou, Rongxiao Huang, Shaoming Li, Guibin Jiang, Jiaqi Zheng, Bing Cheng, Wei Lin|State Key Laboratory for Novel Software Technology, Nanjing University & Meituan, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Meituan, Beijing, China|Marketing optimization plays an important role to enhance user engagement in online Internet platforms. Existing studies usually formulate this problem as a budget allocation problem and solve it by utilizing two fully decoupled stages, i.e., machine learning (ML) and operation research (OR). However, the learning objective in ML does not take account of the downstream optimization task in OR, which causes that the prediction accuracy in ML may be not positively related to the decision quality. Decision Focused Learning (DFL) integrates ML and OR into an end-to-end framework, which takes the objective of the downstream task as the decision loss function and guarantees the consistency of the optimization direction between ML and OR. However, deploying DFL in marketing is non-trivial due to multiple technological challenges. Firstly, the budget allocation problem in marketing is a 0-1 integer stochastic programming problem and the budget is uncertain and fluctuates a lot in real-world settings, which is beyond the general problem background in DFL. Secondly, the counterfactual in marketing causes that the decision loss cannot be directly computed and the optimal solution can never be obtained, both of which disable the common gradient-estimation approaches in DFL. Thirdly, the OR solver is called frequently to compute the decision loss during model training in DFL, which produces huge computational cost and cannot support large-scale training data. In this paper, we propose a decision focused causal learning framework (DFCL) for direct counterfactual marketing optimization, which overcomes the above technological challenges. Both offline experiments and online A/B testing demonstrate the effectiveness of DFCL over the state-of-the-art methods. Currently, DFCL has been deployed in several marketing scenarios in Meituan, one of the largest online food delivery platform in the world.|营销优化对于提高用户在互联网平台上的参与度起着重要作用。现有的研究通常将这个问题表述为一个预算分配问题，并利用机器学习(ML)和运筹学(OR)两个完全解耦的阶段来解决这个问题。然而，机器学习中的学习目标没有考虑或者问题中的下游优化任务，导致机器学习中的预测精度可能与决策质量没有正相关关系。决策聚焦学习(DFL)将机器学习和运算符集成到一个端到端的框架中，以下游任务的目标作为决策损失函数，保证机器学习和运算符优化方向的一致性。然而，部署 DFL 在市场营销是不平凡的，由于多种技术挑战。首先，市场营销中的预算分配问题是一个0-1整数随机规划问题，在现实环境中预算具有不确定性且波动较大，超出了 DFL 中的一般问题背景。其次，市场营销中的反事实导致决策损失不能直接计算，最优解永远不能得到，这两个问题都使 DFL 中常用的梯度估计方法失效。第三，在 DFL 模型训练过程中频繁调用 OR 求解器来计算决策损失，计算量大，不能支持大规模的训练数据。本文提出了一个基于决策的因果学习框架(DFCL) ，用于直接反事实营销优化，克服了上述技术难题。离线实验和在线 A/B 测试都证明了 DFCL 相对于最先进的方法的有效性。目前，DfCL 已在世界上最大的在线食品配送平台之一的美团内部署了多个营销场景。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Decision+Focused+Causal+Learning+for+Direct+Counterfactual+Marketing+Optimization)|0|
|[A Hands-on Introduction to Time Series Classification and Regression](https://doi.org/10.1145/3637528.3671443)|Anthony J. Bagnall, Matthew Middlehurst, Germain Forestier, Ali IsmailFawaz, Antoine Guillaume, David GuijoRubio, Chang Wei Tan, Angus Dempster, Geoffrey I. Webb|Novahe & Constellation, Saint-Cloud, France; IRIMAS, Université de Haute-Alsace, Mulhouse, France; Monash University, Melbourne, Australia; Universidad de Córdoba, Córdoba, Spain; University of Southampton, Southampton, United Kingdom|Time series classification and regression are rapidly evolving fields that find areas of application in all domains of machine learning and data science. This hands on tutorial will provide an accessible overview of the recent research in these fields, using code examples to introduce the process of implementing and evaluating an estimator. We will show how to easily reproduce published results and how to compare a new algorithm to state-of-the-art. Finally, we will work through real world examples from the field of Electroencephalogram (EEG) classification and regression. EEG machine learning tasks arise in medicine, brain-computer interface research and psychology. We use these problems to how to compare algorithms on problems from a single domain and how to deal with data with different characteristics, such as missing values, unequal length and high dimensionality. The latest advances in the fields of time series classification and regression are all available through the aeon toolkit, an open source, scikit-learn compatible framework for time series machine learning which we use to provide our code examples.|时间序列分类和回归是一个迅速发展的领域，在机器学习和数据科学的所有领域都有广泛的应用。本教程将提供这些领域的最新研究的一个可访问的概述，使用代码示例来介绍实现和评估估算器的过程。我们将展示如何轻松地复制已发表的结果，以及如何将新算法与最先进的算法进行比较。最后，我们将从脑电图的分类和回归的角度来研究真实世界的例子。脑电图机器学习任务出现在医学、脑机接口研究和心理学领域。我们利用这些问题来比较单一领域问题的算法，以及如何处理具有不同特征的数据，如缺失值、不等长度和高维数等。时间序列分类和回归领域的最新进展都可以通过 aeon 工具包获得，aeon 工具包是一个开源的、与 scikit 学习兼容的时间序列机器学习框架，我们用它来提供我们的代码示例。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Hands-on+Introduction+to+Time+Series+Classification+and+Regression)|0|
|[Multi-modal Data Processing for Foundation Models: Practical Guidances and Use Cases](https://doi.org/10.1145/3637528.3671441)|Daoyuan Chen, Yaliang Li, Bolin Ding|Alibaba Group, Bellevue, USA; Alibaba Group, Hangzhou, China|In the foundation models era, efficiently processing multi-modal data is crucial. This tutorial covers key techniques for multi-modal data processing and introduces the open-source Data-Juicer system, designed to tackle the complexities of data variety, quality, and scale. Participants will learn how to use Data-Juicer's operators and tools for formatting, mapping, filtering, deduplicating, and selecting multi-modal data efficiently and effectively. They will also be familiar with the Data-Juicer Sandbox Lab, where users can easily experiment with diverse data recipes that represent methodical sequences of operators and streamline the creation of scalable data processing pipelines. This experience solidifies the concepts discussed, as well as provides a space for innovation and exploration, highlighting how data recipes can be optimized and deployed in high-performance distributed environments. By the end of this tutorial, attendees will be equipped with the practical knowledge and skills to navigate the multi-modal data processing for foundation models. They will leave with actionable knowledge with an industrial open-source system and an enriched perspective on the importance of high-quality data in AI, poised to implement sustainable and scalable solutions in their projects. The system and related materials are available at https://github.com/modelscope/data-juicer.|在基础模型时代，有效处理多模态数据至关重要。本教程涵盖了多模态数据处理的关键技术，并介绍了开源 Data-Juicer 系统，该系统旨在解决数据多样性、质量和规模的复杂性。参加者将学习如何使用 Data-Juicer 的操作员和工具来有效地格式化、映射、过滤、去重复和选择多模态数据。他们还将熟悉 Data-Juicer Sandbox 实验室，在那里，用户可以轻松地试验各种数据配方，这些配方代表有条不紊的操作员序列，并简化可伸缩数据处理管道的创建。这种体验巩固了所讨论的概念，并为创新和探索提供了空间，突出了如何在高性能分布式环境中优化和部署数据菜谱。在本教程结束时，与会者将获得实际知识和技能，以导航的基础模型的多模态数据处理。他们将带着可操作的知识离开工业开源系统和对人工智能中高质量数据重要性的丰富视角，准备在他们的项目中实施可持续和可扩展的解决方案。有关系统及相关资料可于 https://github.com/modelscope/data-juicer 索取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-modal+Data+Processing+for+Foundation+Models:+Practical+Guidances+and+Use+Cases)|0|
|[DARE to Diversify: DAta Driven and Diverse LLM REd Teaming](https://doi.org/10.1145/3637528.3671444)|Manish Nagireddy, Bernat Guillen Pegueroles, Ioana Baldini|IBM Research, Yorktown Heights, New York, USA; IBM Research, Cambridge, Massachusetts, USA; Google, Zurich, CH|Large language models (LLMs) have been rapidly adopted, as showcased by ChatGPT's overnight popularity, and are integrated in products used by millions of people every day, such as search engines and productivity suites. Yet the societal impact of LLMs, encompassing both benefits and harms, is not well understood. Inspired by cybersecurity practices, red-teaming is emerging as a technique to uncover model vulnerabilities. Despite increasing attention from industry, academia, and government centered around red-teaming LLMs, such efforts are still limited in the diversity of the red-teaming focus, approaches and participants. Importantly, given that LLMs are becoming ubiquitous, it is imperative that red-teaming efforts are scaled out to include large segments of the research, practitioners and the people whom are directly affected by the deployment of these systems. The goal of this tutorial is two fold. First, we introduce the topic of LLM red-teaming by reviewing the state of the art for red-teaming practices, from participatory events to automatic AI-focused approaches, exposing the gaps in both the techniques and coverage of the targeted harms. Second, we plan to engage the audience in a hands-on and interactive exercise in LLM red-teaming to showcase the ease (or difficulty) of exposing model vulnerabilities, contingent on both the targeted harm and model capabilities. We believe that the KDD community of researchers and practitioners are in a unique position to address the existing gaps in red-teaming approaches, given their longstanding research and practice of extracting knowledge from data.|大语言模型(LLM)已经被迅速采用，正如 ChatGPT 一夜成名所展示的那样，并被集成到每天数百万人使用的产品中，例如搜索引擎和生产力套件。然而，LLM 的社会影响，包括利益和危害，还没有得到很好的理解。受网络安全实践的启发，红队正在成为一种发现模型漏洞的技术。尽管业界、学术界和政府越来越多地关注红色团队 LLM，但这种努力在红色团队关注点、方法和参与者的多样性方面仍然有限。重要的是，鉴于 LLM 正变得无处不在，红色团队的努力必须扩大到包括大部分研究、从业人员和直接受到这些系统部署影响的人。本教程的目标是双重的。首先，我们通过回顾红色团队实践的现状，从参与性事件到自动 AI 重点方法，介绍 LLM 红色团队的主题，揭示目标危害的技术和覆盖面的差距。其次，我们计划让受众参与 LLM 红色团队的实践和互动练习，以展示暴露模型漏洞的容易程度(或难度) ，这取决于目标伤害和模型能力。我们认为，KDD 研究人员和从业人员群体具有独特的地位，可以弥补红队方法中现有的差距，因为他们长期从数据中提取知识的研究和实践。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DARE+to+Diversify:+DAta+Driven+and+Diverse+LLM+REd+Teaming)|0|
|[Privacy-Preserving Federated Learning using Flower Framework](https://doi.org/10.1145/3637528.3671447)|Mohammad Naseri, Javier FernándezMarqués, Yan Gao, Heng Pan|Flower Labs, Cambridge, UK|AI projects often face the challenge of limited access to meaningful amounts of training data. In traditional approaches, collecting data in a central location can be problematic, especially in industry settings with sensitive and distributed data. However, there is a solution -"moving the computation to the data" through Federated Learning. Federated Learning, a distributed machine learning approach, offers a promising solution by enabling model training across devices. It is a data minimization approach where direct access to data is not required. Furthermore, federated learning can be combined with techniques like differential privacy, secure aggregation, homomorphic encryption, and others, to further enhance privacy protection. In this hands-on tutorial, we delve into the realm of privacy-preserving machine learning using federated learning, leveraging the Flower framework which is specifically designed to simplify the process of building federated learning systems, as our primary tool. Moreover, we present the foundations of federated learning, explore how different techniques can enhance its privacy aspects, how it is being used in real-world settings today and a series of practical, hands-on code examples that showcase how you can federate any AI project with Flower, an open-source framework for all-this federated.|人工智能项目往往面临获得大量培训数据的机会有限的挑战。在传统方法中，在中心位置收集数据可能会有问题，特别是在拥有敏感和分布式数据的行业环境中。然而，有一个解决方案——通过联邦学习(Federated Learning)“将计算转移到数据”。联邦学习(Federated Learning)是一种分布式机器学习方法，通过支持跨设备的模型培训，提供了一种有前途的解决方案。这是一种不需要直接访问数据的数据最小化方法。此外，联合学习还可以与差分隐私、安全聚合、同态加密等技术相结合，进一步加强隐私保护。在这个实践教程中，我们深入研究了使用联邦学习保护隐私的机器学习领域，利用 Flower 框架，它是专门设计来简化构建联邦学习系统的过程的，作为我们的主要工具。此外，我们还介绍了联合学习的基础，探索了不同的技术如何增强其隐私方面，如何在现实世界的环境中使用它，以及一系列实用的、动手操作的代码示例，展示了如何将任何人工智能项目与 Flower 联合起来，这是一个面向所有人的开源框架。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Privacy-Preserving+Federated+Learning+using+Flower+Framework)|0|
|[Graph Reasoning with LLMs (GReaL)](https://doi.org/10.1145/3637528.3671448)|Anton Tsitsulin, Bryan Perozzi, Bahare Fatemi, Jonathan J. Halcrow|Google Research, Montreal, QC, Canada; Google Research, New York, NY, USA; Google Research, Atlanta, GA, USA|Graphs are a powerful tool for representing and analyzing complex relationships in real-world applications. Large Language Models (LLMs) have demonstrated impressive capabilities by advancing state-of-the-art on many language-based benchmarks. Their ability to process and understand natural language open exciting possibilities in various domains. Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with LLMs remains an understudied problem that has recently gained more attention. This tutorial builds upon recent advances in expressing reasoning problems through the lens of tasks on graph data. The first part of the tutorial will provide an in-depth discussion of techniques for representing graphs as inputs to LLMs. The second, hands-on, portion will demonstrate these techniques in a practical setting. As a learning outcome of participating in the tutorial, participants will be able to analyze graphs either on free-tier Colab or their local machines with the help of LLMs.|图是表示和分析真实世界应用程序中复杂关系的强大工具。大型语言模型(LLM)通过在许多基于语言的基准测试上推进最先进的技术，展示了令人印象深刻的能力。他们处理和理解自然语言的能力在各个领域开辟了令人兴奋的可能性。尽管在自然文本自动推理方面取得了显著的进步，但用 LLM 进行图形推理仍然是一个被忽视的问题，最近得到了更多的关注。本教程建立在通过图形数据上的任务透镜来表达推理问题的最新进展之上。本教程的第一部分将深入讨论将图表示为 LLM 输入的技术。第二部分是实践部分，将在实际环境中演示这些技术。作为参与本教程的学习成果，参与者将能够在自由层 Colab 上或者在 LLM 的帮助下分析本地机器上的图形。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Reasoning+with+LLMs+(GReaL))|0|
|[Breaking Barriers: A Hands-On Tutorial on AI-Enabled Accessibility to Social Media Content](https://doi.org/10.1145/3637528.3671446)|Julio Villena, Rosa Català, Janine García, Concepción Polo, Yessika Labrador, Francisco delValle, Bhargav Ayyagari|Reddit, Inc., Madrid, Spain; Reddit, Inc., Toronto, Canada; Reddit, Inc., San Francisco, USA|Reddit's mission is to bring community, belonging, and empowerment to everyone in the world. This hands-on tutorial explores the immense potential of Artificial Intelligence (AI) to improve accessibility to social media content for individuals with different disabilities, including hearing, visual, and cognitive impairments. We will design and implement a variety of AI-based approaches based on multimodal open-source Large Language Models (LLMs) to bridge the gap between research and real-world applications.|Reddit 的使命是为世界上的每一个人带来社区、归属感和权力。本实践教程探索了人工智能(AI)的巨大潜力，以改善不同残疾人士(包括听力、视力和认知障碍)对社交媒体内容的无障碍访问。我们将设计和实现基于多模态开源大语言模型(LLM)的各种基于人工智能的方法，以弥合研究和现实应用之间的差距。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Breaking+Barriers:+A+Hands-On+Tutorial+on+AI-Enabled+Accessibility+to+Social+Media+Content)|0|
|[Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text](https://doi.org/10.1145/3637528.3671463)|Sara Abdali, Richard Anarfi, C. J. Barberan, Jia He|Microsoft, Cambridge, MA, USA; Microsoft, Redmond, WA, USA|Large Language Models (LLMs) have revolutionized the field of NaturalLanguage Generation (NLG) by demonstrating an impressive ability to generatehuman-like text. However, their widespread usage introduces challenges thatnecessitate thoughtful examination, ethical scrutiny, and responsiblepractices. In this study, we delve into these challenges, explore existingstrategies for mitigating them, with a particular emphasis on identifyingAI-generated text as the ultimate solution. Additionally, we assess thefeasibility of detection from a theoretical perspective and propose novelresearch directions to address the current limitations in this domain.|大语言模型(LLM)通过展示生成类似人类文本的令人印象深刻的能力，彻底改变了自然语言生成(NLG)领域。然而，它们的广泛使用带来了挑战，需要深思熟虑的审查，道德审查和负责任的做法。在这项研究中，我们深入研究这些挑战，探索减轻这些挑战的现有策略，特别强调识别人工智能生成的文本作为最终解决方案。此外，我们从理论角度评估检测的可行性，并提出新的研究方向，以解决目前在这一领域的局限性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Decoding+the+AI+Pen:+Techniques+and+Challenges+in+Detecting+AI-Generated+Text)|0|
|[Advances in Human Event Modeling: From Graph Neural Networks to Language Models](https://doi.org/10.1145/3637528.3671466)|Songgaojun Deng, Maarten de Rijke, Yue Ning|Stevens Institute of Technology, Hoboken, New Jersey, USA; University of Amsterdam, Amsterdam, The Netherlands|Human events such as hospital visits, protests, and epidemic outbreaks directly affect individuals, communities, and societies. These events are often influenced by factors such as economics, politics, and public policies of our society. The abundance of online data sources such as social networks, official news articles, and personal blogs chronicle societal events, facilitating the development of AI models for social science, public health care, and decision making. Human event modeling generally comprises both the forecasting stage, which estimates future events based on historical data, and interpretation, which seeks to identify influential factors of such events to understand their causative attributes. Recent achievements, fueled by deep learning and the availability of public data, have significantly advanced the field of human event modeling. This survey offers a systematic overview of deep learning technologies for forecasting and interpreting human events, with a primary focus on political events. We first introduce the existing challenges and background in this domain. We then present the problem formulation of event forecasting and interpretation. We investigate recent achievements in graph neural networks, owing to the prevalence of relational data and the efficacy of graph learning models. We also discuss the latest studies that utilize large language models for event reasoning. Lastly, we provide summaries of data resources, open challenges, and future research directions in the study of human event modeling.|人类活动，如医院访问、抗议和流行病爆发直接影响个人、社区和社会。这些事件往往受到经济、政治和社会公共政策等因素的影响。丰富的在线数据源，如社交网络、官方新闻文章和个人博客记录了社会事件，促进了人工智能模型在社会科学、公共卫生保健和决策方面的发展。人类事件建模一般包括预测阶段(基于历史数据估计未来事件)和解释阶段(寻求识别此类事件的影响因素以了解其致因属性)。在深度学习和公共数据可用性的推动下，最近的成就显著地推进了人类事件建模领域。这项调查提供了一个系统的深度学习技术预测和解释人类事件的概述，主要侧重于政治事件。我们首先介绍了该领域存在的挑战和背景。然后，我们提出了事件预测和解释的问题表述。由于关系数据的普遍性和图学习模型的有效性，我们研究了图神经网络的最新成果。我们还讨论了利用大型语言模型进行事件推理的最新研究。最后，对人类事件建模的数据资源、开放性挑战以及未来的研究方向进行了总结。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Advances+in+Human+Event+Modeling:+From+Graph+Neural+Networks+to+Language+Models)|0|
|[Reasoning and Planning with Large Language Models in Code Development](https://doi.org/10.1145/3637528.3671452)|Hao Ding, Ziwei Fan, Ingo Gühring, Gaurav Gupta, Wooseok Ha, Jun Huan, Linbo Liu, Behrooz OmidvarTehrani, Shiqi Wang, Hao Zhou|AWS AI Labs, New York, NY, USA; AWS AI Labs, Berlin, Germany; AWS AI Labs, Santa Clara, CA, USA; AWS AI labs, Santa Clara, CA, USA|Large Language Models (LLMs) are revolutionizing the field of code development by leveraging their deep understanding of code patterns, syntax, and semantics to assist developers in various tasks, from code generation and testing to code understanding and documentation. In this survey, accompanying our proposed lecture-style tutorial for KDD 2024, we explore the multifaceted impact of LLMs on the code development, delving into techniques for generating a high-quality code, creating comprehensive test cases, automatically generating documentation, and engaging in an interactive code reasoning. Throughout the survey, we highlight some crucial components surrounding LLMs, including pre-training, fine-tuning, prompt engineering, iterative refinement, agent planning, and hallucination mitigation. We put forward that such ingredients are essential to harness the full potential of these powerful AI models in revolutionizing software engineering and paving the way for a more efficient, effective, and innovative future in code development.|大型语言模型(LLM)通过利用它们对代码模式、语法和语义的深刻理解来帮助开发人员完成各种任务，从代码生成和测试到代码理解和文档编写，从而彻底改变了代码开发领域。在这个调查中，伴随着我们提出的 KDD 2024讲座式教程，我们探索 LLM 对代码开发的多方面影响，深入研究生成高质量代码的技术，创建全面的测试用例，自动生成文档，并参与交互式代码推理。在整个调查中，我们强调了关于 LLM 的一些重要组成部分，包括预训练、微调、快速工程、迭代求精、代理计划和幻觉缓解。我们提出，这些要素对于充分利用这些强大的人工智能模型的潜力是必不可少的，它们将彻底改革软件工程，并为代码开发中更有效、更有效和更具创新性的未来铺平道路。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reasoning+and+Planning+with+Large+Language+Models+in+Code+Development)|0|
|[Sharing is Caring: A Practical Guide to FAIR(ER) Open Data Release](https://doi.org/10.1145/3637528.3671468)|Amelia Henriksen, Miranda Mundt|Sandia National Laboratories, Albuquerque, New Mexico, USA|Findable. Accessible. Interoperable. Reusable. Since their introduction in 2016, the FAIR data principles have defined the standards by which scientific researchers share data. However, modern research in data editing and management consistently shows that while the FAIR data principles are widely accepted in theory, they can be much more difficult to understand and implement in practice. In this tutorial, we explore some of the simple, realistic steps scientists can take to FAIRly release open data. We also explore areas where the current FAIR guidelines fall short and offer practical suggestions for making open data FAIR(ER): more Equitable and Realistic. This first involves ways to make datasets themselves more equitably accessible for researchers with disabilities. While equitably accessible data design has some research overlap with paper, presentation, and website design, we suggest several unique distinctions specific to datasets. The "Realistic'' aspect of FAIR(ER) data facilitates a path to translate open data (and research on that data) back to true applications. Driven by national security applications pipelines, we call out important considerations for balancing data editing against data realism.|找得到。无障碍。可互操作。可重复使用。自2016年引入以来，FAIR 数据原则已经确定了科学研究人员共享数据的标准。然而，现代数据编辑和管理研究一致表明，虽然 FAIR 数据原则在理论上被广泛接受，但在实践中可能更难理解和实施。在本教程中，我们将探索一些科学家可以采取的简单、现实的步骤，以公平地发布开放数据。我们还探讨了当前公平竞争指南的不足之处，并提供了实用的建议，使开放数据公平竞争(ER) : 更加公平和现实。这首先涉及到如何让残疾研究人员更公平地获取数据集。虽然公平可访问的数据设计与论文、演示文稿和网站设计有一些研究重叠，但我们提出了一些特定于数据集的独特区别。FAIR (ER)数据的“现实”方面促进了将开放数据(以及对该数据的研究)转换回真正应用程序的途径。在国家安全应用管道的驱动下，我们呼吁在数据编辑和数据现实主义之间进行平衡的重要考虑。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sharing+is+Caring:+A+Practical+Guide+to+FAIR(ER)+Open+Data+Release)|0|
|[Grounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned (Survey)](https://doi.org/10.1145/3637528.3671467)|Krishnaram Kenthapadi, Mehrnoosh Sameki, Ankur Taly|Oracle Health AI, Redwood City, CA, USA; Google Cloud AI, Sunnyvale, CA, USA; Microsoft Azure AI, Boston, MA, USA|With the ongoing rapid adoption of Artificial Intelligence (AI)-based systems in high-stakes domains, ensuring the trustworthiness, safety, and observability of these systems has become crucial. It is essential to evaluate and monitor AI systems not only for accuracy and quality-related metrics but also for robustness, bias, security, interpretability, and other responsible AI dimensions. We focus on large language models (LLMs) and other generative AI models, which present additional challenges such as hallucinations, harmful and manipulative content, and copyright infringement. In this survey article accompanying our tutorial, we highlight a wide range of harms associated with generative AI systems, and survey state of the art approaches (along with open challenges) to address these harms.|随着基于人工智能(AI)的系统在高风险领域的快速应用，确保这些系统的可靠性、安全性和可观测性已变得至关重要。评估和监控人工智能系统不仅是为了准确性和质量相关指标，而且也是为了鲁棒性、偏差、安全性、可解释性和其他负责任的人工智能维度。我们专注于大型语言模型(LLMs)和其他生成性人工智能模型，这些模型提出了额外的挑战，比如幻觉、有害和操纵性内容以及盗版。在本教程附带的这篇调查文章中，我们强调了与生成性 AI 系统相关的一系列危害，并调查了解决这些危害的最新方法(以及开放式挑战)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Grounding+and+Evaluation+for+Large+Language+Models:+Practical+Challenges+and+Lessons+Learned+(Survey))|0|
|[A Survey on Hypergraph Neural Networks: An In-Depth and Step-By-Step Guide](https://doi.org/10.1145/3637528.3671457)|Sunwoo Kim, Soo Yong Lee, Yue Gao, Alessia Antelmi, Mirko Polato, Kijung Shin|University of Turin, Turin, Italy; KAIST, Seoul, Republic of Korea; Tsinghua University, Beijing, China|Higher-order interactions (HOIs) are ubiquitous in real-world complex systems and applications. Investigation of deep learning for HOIs, thus, has become a valuable agenda for the data mining and machine learning communities. As networks of HOIs are expressed mathematically as hypergraphs, hypergraph neural networks (HNNs) have emerged as a powerful tool for representation learning on hypergraphs. Given the emerging trend, we present the first survey dedicated to HNNs, with an in-depth and step-by-step guide. Broadly, the present survey overviews HNN architectures, training strategies, and applications. First, we break existing HNNs down into four design components: (i) input features, (ii) input structures, (iii) message-passing schemes, and (iv) training strategies. Second, we examine how HNNs address and learn HOIs with each of their components. Third, we overview the recent applications of HNNs in recommendation, bioinformatics and medical science, time series analysis, and computer vision. Lastly, we conclude with a discussion on limitations and future directions.|高阶交互(HOI)在现实世界的复杂系统和应用中无处不在。因此，研究 HOI 的深度学习已经成为数据挖掘和机器学习领域的一个有价值的议题。由于 HOI 网络被数学上表示为超图，超图神经网络(HNN)已经成为超图表示学习的有力工具。鉴于新兴的趋势，我们提出了第一个调查专门针对 HNN，与一个深入和逐步指南。概括地说，本调查综述了 HNN 的体系结构、培训策略和应用。首先，我们将现有的 HNN 分解为四个设计组件: (i)输入特性，(ii)输入结构，(iii)消息传递方案和(iv)训练策略。其次，我们研究了 HNN 如何通过它们的每个组件来寻址和学习 HOI。第三，我们概述了 HNN 在推荐、生物信息学和医学科学、时间序列分析和计算机视觉等方面的最新应用。最后，我们讨论了本文的局限性和未来的发展方向。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Survey+on+Hypergraph+Neural+Networks:+An+In-Depth+and+Step-By-Step+Guide)|0|
|[Graph Intelligence with Large Language Models and Prompt Learning](https://doi.org/10.1145/3637528.3671456)|Jia Li, Xiangguo Sun, Yuhan Li, Zhixun Li, Hong Cheng, Jeffrey Xu Yu|The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; The Chinese University of Hong Kong, HongKong, China|Graph plays a significant role in representing and analyzing complex relationships in real-world applications such as citation networks, social networks, and biological data. Graph intelligence is rapidly becoming a crucial aspect of understanding and exploiting the intricate interconnections within graph data. Recently, large language models (LLMs) and prompt learning techniques have pushed graph intelligence forward, outperforming traditional Graph Neural Network (GNN) pre-training methods and setting new benchmarks for performance. In this tutorial, we begin by offering a comprehensive review and analysis of existing methods that integrate LLMs with graphs. We introduce existing works based on a novel taxonomy that classifies them into three distinct categories according to the roles of LLMs in graph tasks: as enhancers, predictors, or alignment components. Secondly, we introduce a new learning method that utilizes prompting on graphs, offering substantial potential to enhance graph transfer capabilities across diverse tasks and domains. We discuss existing works on graph prompting within a unified framework and introduce our developed tool for executing a variety of graph prompting tasks. Additionally, we discuss the applications of combining Graphs, LLMs, and prompt learning across various tasks, such as urban computing, recommendation systems, and anomaly detection. This lecture-style tutorial is an extension of our original work published in IJCAI 2024[44] and arXiv[77] with the invitation of KDD24.|在引用网络、社会网络和生物数据等现实应用中，图表在表示和分析复杂关系方面发挥着重要作用。图形智能正在迅速成为理解和利用图形数据中错综复杂的相互关系的一个关键方面。近年来，大语言模型(LLM)和快速学习技术推动了图形智能的发展，其性能优于传统的图形神经网络(GNN)预训练方法，并为性能设定了新的基准。在本教程中，我们首先全面回顾和分析集成 LLM 和图形的现有方法。我们介绍现有的工作基于一个新的分类法，根据 LLM 在图形任务中的作用将它们分为三个不同的类别: 作为增强子、预测器或对齐组件。其次，我们介绍了一种新的学习方法，利用图上的提示，提供了大量的潜力，以提高图的传输能力跨不同的任务和领域。在统一的框架下讨论了现有的图形提示工作，并介绍了我们开发的用于执行各种图形提示任务的工具。此外，我们还讨论了结合图形、 LLM 和跨不同任务的快速学习的应用，例如城市计算、推荐系统和异常检测。这个演讲风格的教程是我们的原始工作的延伸，应 KDD24的邀请发表在 IJCAI 2024[44]和 arXiv [77]。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Intelligence+with+Large+Language+Models+and+Prompt+Learning)|0|
|[Foundation Models for Time Series Analysis: A Tutorial and Survey](https://doi.org/10.1145/3637528.3671451)|Yuxuan Liang, Haomin Wen, Yuqi Nie, Yushan Jiang, Ming Jin, Dongjin Song, Shirui Pan, Qingsong Wen|University of Connecticut, Storrs, CT, USA; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Griffith University, Brisbane, Australia; Squirrel AI, Seattle, WA, USA; Monash University, Melbourne, Australia; Princeton University, Princeton, NJ, USA; The Hong Kong University of Science and Technology (Guangzhou|Time series analysis stands as a focal point within the data mining community, serving as a cornerstone for extracting valuable insights crucial to a myriad of real-world applications. Recent advances in Foundation Models (FMs) have fundamentally reshaped the paradigm of model design for time series analysis, boosting various downstream tasks in practice. These innovative approaches often leverage pre-trained or fine-tuned FMs to harness generalized knowledge tailored for time series analysis. This survey aims to furnish a comprehensive and up-to-date overview of FMs for time series analysis. While prior surveys have predominantly focused on either application or pipeline aspects of FMs in time series analysis, they have often lacked an in-depth understanding of the underlying mechanisms that elucidate why and how FMs benefit time series analysis. To address this gap, our survey adopts a methodology-centric classification, delineating various pivotal elements of time-series FMs, including model architectures, pre-training techniques, adaptation methods, and data modalities. Overall, this survey serves to consolidate the latest advancements in FMs pertinent to time series analysis, accentuating their theoretical underpinnings, recent strides in development, and avenues for future exploration.|时间序列分析是数据挖掘领域的焦点，是提取对于大量现实应用程序至关重要的宝贵见解的基石。基础模型的最新进展从根本上重塑了时间序列分析的模型设计范式，推动了实践中各种下游任务的完成。这些创新的方法往往利用预先训练或微调的模拟器来利用为时间序列分析量身定制的通用知识。本调查旨在为时间序列分析提供全面和最新的调查表概述。虽然以前的调查主要集中在时间序列分析中的建模方法的应用或流水线方面，但它们往往缺乏对阐明建模方法有利于时间序列分析的原因和方式的基本机制的深入理解。为了弥补这一差距，我们的调查采用了以方法论为中心的分类，描述了时间序列模型的各种关键要素，包括模型架构、预训练技术、适应方法和数据模式。总体而言，本调查旨在巩固与时间序列分析相关的 FM 的最新进展，强调它们的理论基础、最近的发展进展以及未来探索的途径。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Foundation+Models+for+Time+Series+Analysis:+A+Tutorial+and+Survey)|0|
|[Symbolic Regression: A Pathway to Interpretability Towards Automated Scientific Discovery](https://doi.org/10.1145/3637528.3671464)|Nour Makke, Sanjay Chawla|Qatar Airways (Qatar)|Symbolic regression is a machine learning technique employed for learning mathematical equations directly from data. Mathematical equations capture both functional and causal relationships in the data. In addition, they are simple, compact, generalizable, and interpretable models, making them the best candidates for i) learning inherently transparent models and ii) boosting scientific discovery. Symbolic regression has received a growing interest since the last decade and is tackled using different approaches in supervised and unsupervised deep learning, thanks to the enormous progress achieved in deep learning in the last twenty years. Symbolic regression remains underestimated in conference coverage as a primary form of interpretable AI and a potential candidate for automating scientific discovery. This tutorial overviews symbolic regression: problem definition, approaches, and key limitations, discusses why physical sciences are beneficial to symbolic regression, and explores possible future directions in this research area.|符号回归是一种直接从数据中学习数学方程的机器学习技术。数学方程捕捉数据中的函数关系和因果关系。此外，它们是简单、紧凑、可推广和可解释的模型，使它们成为 i)学习固有的透明模型和 ii)促进科学发现的最佳候选者。自过去十年以来，符号回归得到了越来越多的关注，并且由于过去二十年在深度学习方面取得的巨大进展，在有监督和无监督的深度学习中使用了不同的方法来处理符号回归。象征性回归作为可解释人工智能的主要形式和自动化科学发现的潜在候选者，在会议覆盖面方面仍然被低估。本教程概述了符号回归: 问题的定义，方法和关键的局限性，讨论了为什么物理科学有利于符号回归，并探讨了在这个研究领域可能的未来方向。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Symbolic+Regression:+A+Pathway+to+Interpretability+Towards+Automated+Scientific+Discovery)|0|
|[A Survey of Large Language Models for Graphs](https://doi.org/10.1145/3637528.3671460)|Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh V. Chawla, Chao Huang|University of Hong Kong, Hong Kong, China; Baidu, Beijing, China; University of Notre Dame, Indiana, USA|Graphs are an essential data structure utilized to represent relationships in real-world scenarios. Prior research has established that Graph Neural Networks (GNNs) deliver impressive outcomes in graph-centric tasks, such as link prediction and node classification. Despite these advancements, challenges like data sparsity and limited generalization capabilities continue to persist. Recently, Large Language Models (LLMs) have gained attention in natural language processing. They excel in language comprehension and summarization. Integrating LLMs with graph learning techniques has attracted interest as a way to enhance performance in graph learning tasks. In this survey, we conduct an in-depth review of the latest state-of-the-art LLMs applied in graph learning and introduce a novel taxonomy to categorize existing methods based on their framework design. We detail four unique designs: i) GNNs as Prefix, ii) LLMs as Prefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only, highlighting key methodologies within each category. We explore the strengths and limitations of each framework, and emphasize potential avenues for future research, including overcoming current integration challenges between LLMs and graph learning techniques, and venturing into new application areas. This survey aims to serve as a valuable resource for researchers and practitioners eager to leverage large language models in graph learning, and to inspire continued progress in this dynamic field. We consistently maintain the related open-source materials at https://github.com/HKUDS/Awesome-LLM4Graph-Papers.|图是在现实场景中用于表示关系的基本数据结构。先前的研究已经证实，图形神经网络(GNN)在以图形为中心的任务(如链路预测和节点分类)中能够产生令人印象深刻的结果。尽管取得了这些进步，但数据稀少和泛化能力有限等挑战仍然存在。近年来，大语言模型(LLM)在自然语言处理领域引起了广泛的关注。他们擅长语言理解和总结。将 LLM 与图形学习技术相结合作为一种提高图形学习任务性能的方法引起了人们的兴趣。在这篇综述中，我们对最新的应用于图形学习的最小二乘法进行了深入的回顾，并引入了一种新的分类方法来根据它们的框架设计对现有的方法进行分类。我们详细介绍了四种独特的设计: i) GNN 作为前缀，ii) LLM 作为前缀，iii) LLM-Graphs 集成，以及 iv) LLM-Only，突出每个类别中的关键方法。我们探讨了每个框架的优势和局限性，并强调了未来研究的潜在途径，包括克服当前 LLM 和图形学习技术之间的集成挑战，以及进入新的应用领域。这项调查的目的是作为一个宝贵的资源，为研究人员和从业人员渴望利用大型语言模型在图形学习，并激励在这个动态领域的持续进展。我们一直在 https://github.com/hkuds/awesome-llm4graph-papers 保存相关的开源材料。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Survey+of+Large+Language+Models+for+Graphs)|0|
|[Explainable Artificial Intelligence on Biosignals for Clinical Decision Support](https://doi.org/10.1145/3637528.3671459)|Miriam Cindy Maurer, Jacqueline Michelle Metsch, Philip Hempel, Theresa Bender, Nicolai Spicher, AnneChristin Hauschild|; Department of Medical Informatics, University Medical Center, Göttingen, Germany|Deep learning has proven effective in several areas, including computer vision, natural language processing, and disease prediction, which can support clinicians in making decisions along the clinical pathway. However, in order to successfully integrate these algorithms into clinical practice, it is important that their decision-making processes are transparent, explainable, and interpretable. Firstly, this tutorial will introduce targeted eXplainable Artificial Intelligence (XAI) methods to address the urgent need for explainability of deep learning in healthcare applications. In particular, it focuses on algorithms for raw biosignals without prior feature extraction that enable medical diagnoses, specifically electrocardiograms (ECG) -- stemming from the heart -- and electroencephalograms (EEG) representing the electrical activity of the brain. Secondly, participants are provided with a comprehensive workflow that includes both data processing and an introduction to relevant network architectures. Subsequently, various XAI methods are described and it is shown, how the resulting relevance attributions can be visualized on biosignals. Finally, two compelling real-world use cases are presented that demonstrate the effectiveness of XAI in analyzing ECG and EEG signals for disease prediction and sleep classification, respectively. In summary, the tutorial will provide the skills required for gaining insight into the decision process of deep neural networks processing authentic clinical biosignal data.|深度学习已被证明在多个领域有效，包括计算机视觉、自然语言处理和疾病预测，这可以支持临床医生根据临床路径做出决策。然而，为了成功地将这些算法集成到临床实践中，它们的决策过程是透明的、可解释的和可解释的是非常重要的。首先，本教程将介绍目标可解释的人工智能(XAI)方法，以满足医疗应用程序中对深度学习可解释性的迫切需求。特别是，它专注于原始生物信号的算法，而不需要事先提取特征来进行医学诊断，特别是心电图(ECG)——源自心脏——和脑电图(EEG)代表大脑的电活动。其次，向参与者提供一个全面的工作流程，包括数据处理和相关网络架构的介绍。随后，描述了各种 XAI 方法，并展示了如何在生物信号上可视化得到的相关属性。最后，给出了两个令人信服的现实应用案例，分别证明了 XAI 在分析心电信号和脑电信号用于疾病预测和睡眠分类方面的有效性。总之，本教程将提供深入了解处理真实临床生物信号数据的深层神经网络的决策过程所需的技能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explainable+Artificial+Intelligence+on+Biosignals+for+Clinical+Decision+Support)|0|
|[Urban Foundation Models: A Survey](https://doi.org/10.1145/3637528.3671453)|Weijia Zhang, Jindong Han, Zhao Xu, Hang Ni, Hao Liu, Hui Xiong|HKUST(GZ), Guangzhou, China; HKUST(GZ) & HKUST, Guangzhou, China; HKUST, Hong Kong, China|Machine learning techniques are now integral to the advancement of intelligent urban services, playing a crucial role in elevating the efficiency, sustainability, and livability of urban environments. The recent emergence of foundation models such as ChatGPT marks a revolutionary shift in the fields of machine learning and artificial intelligence. Their unparalleled capabilities in contextual understanding, problem solving, and adaptability across a wide range of tasks suggest that integrating these models into urban domains could have a transformative impact on the development of smart cities. Despite growing interest in Urban Foundation Models (UFMs), this burgeoning field faces challenges such as a lack of clear definitions and systematic reviews. To this end, this paper first introduces the concept of UFMs and discusses the unique challenges involved in building them. We then propose a data-centric taxonomy that categorizes and clarifies current UFM-related works, based on urban data modalities and types. Furthermore, we explore the application landscape of UFMs, detailing their potential impact in various urban contexts. Relevant papers and open-source resources have been collated and are continuously updated at: https://github.com/usail-hkust/Awesome-Urban-Foundation-Models.|机器学习技术现在已经成为智能城市服务的一个组成部分，在提高城市环境的效率、可持续性和宜居性方面发挥着至关重要的作用。ChatGPT 等基础模型的出现标志着机器学习和人工智能领域的革命性转变。它们在背景理解、解决问题和适应各种任务方面的无与伦比的能力表明，将这些模型纳入城市领域可以对智慧城市的发展产生变革性的影响。尽管人们对城市基础模型(UFM)的兴趣日益增长，但这一新兴领域仍面临诸如缺乏清晰定义和系统评价等挑战。为此，本文首先介绍了 UFM 的概念，并讨论了构建它们所涉及的独特挑战。然后，我们提出了一个以数据为中心的分类法，根据城市数据模式和类型对当前 UFM 相关的工作进行分类和澄清。此外，我们探讨了 UFM 的应用景观，详细说明了它们在各种城市背景下的潜在影响。相关文件和开放源码资源已经整理并不断更新， https://github.com/usail-hkust/awesome-urban-foundation-models 如下:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Urban+Foundation+Models:+A+Survey)|0|
|[Inference Optimization of Foundation Models on AI Accelerators](https://doi.org/10.1145/3637528.3671465)|Youngsuk Park, Kailash Budhathoki, Liangfu Chen, Jonas M. Kübler, Jiaji Huang, Matthäus Kleindessner, Jun Huan, Volkan Cevher, Yida Wang, George Karypis|AWS AI, Santa Clara, CA, USA; AWS AI & EPFL, Tübingen, Germany; AWS AI, Tübingen, Germany|Powerful foundation models, including large language models (LLMs), with Transformer architectures have ushered in a new era of Generative AI across various industries. Industry and research community have witnessed a large number of new applications, based on those foundation models. Such applications include question and answer, customer services, image and video generation, and code completions, among others. However, as the number of model parameters reaches to hundreds of billions, their deployment incurs prohibitive inference costs and high latency in real-world scenarios. As a result, the demand for cost-effective and fast inference using AI accelerators is ever more higher. To this end, our tutorial offers a comprehensive discussion on complementary inference optimization techniques using AI accelerators. Beginning with an overview of basic Transformer architectures and deep learning system frameworks, we deep dive into system optimization techniques for fast and memory-efficient attention computations and discuss how they can be implemented efficiently on AI accelerators. Next, we describe architectural elements that are key for fast transformer inference. Finally, we examine various model compression and fast decoding strategies in the same context.|包括大型语言模型(LLM)在内的强大的基础模型，以及变压器架构，已经在各个行业开创了生成式人工智能的新时代。在这些基础模型的基础上，工业界和研究界目睹了大量新的应用。此类应用程序包括问答、客户服务、图像和视频生成以及代码完成等。然而，当模型参数的数量达到数千亿时，它们的部署在实际场景中会产生令人望而却步的推理成本和高延迟。因此，对使用人工智能加速器的高性价比和快速推理的需求越来越高。为此，我们的教程提供了关于使用 AI 加速器的互补推理优化技术的全面讨论。从变压器基本结构和深度学习系统框架的概述开始，我们深入研究了快速和高效的注意力计算系统优化技术，并讨论了如何在 AI 加速器上有效地实现这些技术。接下来，我们将描述对于快速转换器推理非常关键的体系结构元素。最后，我们研究了在相同环境下的各种模型压缩和快速译码策略。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Inference+Optimization+of+Foundation+Models+on+AI+Accelerators)|0|
|[Automated Mining of Structured Knowledge from Text in the Era of Large Language Models](https://doi.org/10.1145/3637528.3671469)|Yunyi Zhang, Ming Zhong, Siru Ouyang, Yizhu Jiao, Sizhe Zhou, Linyi Ding, Jiawei Han|University of Illinois Urbana-Champaign, Urbana, IL, USA|Massive amount of unstructured text data are generated daily, ranging from news articles to scientific papers. How to mine structured knowledge from the text data remains a crucial research question. Recently, large language models (LLMs) have shed light on the text mining field with their superior text understanding and instruction-following ability. There are typically two ways of utilizing LLMs: fine-tune the LLMs with human-annotated training data, which is labor intensive and hard to scale; prompt the LLMs in a zero-shot or few-shot way, which cannot take advantage of the useful information in the massive text data. Therefore, it remains a challenge on automated mining of structured knowledge from massive text data in the era of large language models. In this tutorial, we cover the recent advancements in mining structured knowledge using language models with very weak supervision. We will introduce the following topics in this tutorial: (1) introduction to large language models, which serves as the foundation for recent text mining tasks, (2) ontology construction, which automatically enriches an ontology from a massive corpus, (3) weakly-supervised text classification in flat and hierarchical label space, (4) weakly-supervised information extraction, which extracts entity and relation structures.|从新闻文章到科学论文，每天都会生成大量的非结构化文本数据。如何从文本数据中挖掘出结构化知识仍然是一个关键的研究问题。近年来，大语言模型(LLM)以其优越的文本理解和指令跟踪能力为文本挖掘领域带来了新的发展。通常有两种利用 LLM 的方法: 使用人工注释的训练数据对 LLM 进行微调，这是一种劳动密集型且难以扩展的方法; 以零拍摄或少拍摄的方式提示 LLM，这种方法不能利用海量文本数据中的有用信息。因此，在大语言模型时代，从海量文本数据中自动挖掘结构化知识仍然是一个挑战。在本教程中，我们将介绍在使用监督非常薄弱的语言模型挖掘结构化知识方面的最新进展。在本教程中，我们将介绍以下主题: (1)大型语言模型的介绍，它是最近文本挖掘任务的基础; (2)本体构建，它可以从大量的语料库中自动丰富本体; (3)平面和分层标签空间中的弱监督文本分类; (4)弱监督信息抽取，它可以提取实体和关系结构。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automated+Mining+of+Structured+Knowledge+from+Text+in+the+Era+of+Large+Language+Models)|0|
|[Causal Inference with Latent Variables: Recent Advances and Future Prospectives](https://doi.org/10.1145/3637528.3671450)|Yaochen Zhu, Yinhan He, Jing Ma, Mengxuan Hu, Sheng Li, Jundong Li|Case Western Reserve University, Charlottesville, VA, USA; University of Virginia, Charlottesville, VA, USA|Causality lays the foundation for the trajectory of our world. Causalinference (CI), which aims to infer intrinsic causal relations among variablesof interest, has emerged as a crucial research topic. Nevertheless, the lack ofobservation of important variables (e.g., confounders, mediators, exogenousvariables, etc.) severely compromises the reliability of CI methods. The issuemay arise from the inherent difficulty in measuring the variables.Additionally, in observational studies where variables are passively recorded,certain covariates might be inadvertently omitted by the experimenter.Depending on the type of unobserved variables and the specific CI task, variousconsequences can be incurred if these latent variables are carelessly handled,such as biased estimation of causal effects, incomplete understanding of causalmechanisms, lack of individual-level causal consideration, etc. In this survey,we provide a comprehensive review of recent developments in CI with latentvariables. We start by discussing traditional CI techniques when variables ofinterest are assumed to be fully observed. Afterward, under the taxonomy ofcircumvention and inference-based methods, we provide an in-depth discussion ofvarious CI strategies to handle latent variables, covering the tasks of causaleffect estimation, mediation analysis, counterfactual reasoning, and causaldiscovery. Furthermore, we generalize the discussion to graph data whereinterference among units may exist. Finally, we offer fresh aspects for furtheradvancement of CI with latent variables, especially new opportunities in theera of large language models (LLMs).|因果关系奠定了我们世界轨迹的基础。因果推理(CI)是一个重要的研究课题，其目的是推断感兴趣的变量之间的内在因果关系。然而，缺乏对重要变量(如混杂因素、中介因素、外生变量等)的观察严重影响了 CI 方法的可靠性。这个问题可能源于测量变量的固有困难。此外，在被动记录变量的观察性研究中，某些协变量可能会被实验者无意中忽略。根据未观察到的变量的类型和具体的 CI 任务，如果这些潜在变量处理不当，可能会产生各种后果，例如因果效应的偏倚估计，对因果机制的不完全理解，缺乏个体层面的因果考虑等。在这项调查中，我们提供了一个潜在变量的近期发展综述 CI。我们从讨论传统的 CI 技术开始，当感兴趣的变量被假定为被充分观察时。然后，在规避和推理方法的分类下，我们深入讨论了处理潜在变量的各种 CI 策略，包括因果效应估计，中介分析，反事实推理和因果发现的任务。此外，我们将讨论推广到可能存在单位间干扰的图形数据。最后，我们为潜变量 CI 的进一步发展提供了新的方向，特别是在大语言模型时代提供了新的机遇。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causal+Inference+with+Latent+Variables:+Recent+Advances+and+Future+Prospectives)|0|
|[A Survey on Safe Multi-Modal Learning Systems](https://doi.org/10.1145/3637528.3671462)|Tianyi Zhao, Liangliang Zhang, Yao Ma, Lu Cheng|Rensselaer Polytechnic Institute, Troy, USA; University of Southern California, Los Angeles, USA; University of Illinois Chicago, Chicago, USA|In the rapidly evolving landscape of artificial intelligence, multimodal learning systems (MMLS) have gained traction for their ability to process and integrate information from diverse modality inputs. Their expanding use in vital sectors such as healthcare has made safety assurance a critical concern. However, the absence of systematic research into their safety is a significant barrier to progress in this field. To bridge the gap, we present the first taxonomy that systematically categorizes and assesses MMLS safety. This taxonomy is structured around four fundamental pillars that are critical to ensuring the safety of MMLS: robustness, alignment, monitoring, and controllability. Leveraging this taxonomy, we review existing methodologies, benchmarks, and the current state of research, while also pinpointing the principal limitations and gaps in knowledge. Finally, we discuss unique challenges in MMLS safety. In illuminating these challenges, we aim to pave the way for future research, proposing potential directions that could lead to significant advancements in the safety protocols of MMLS.|在快速发展的人工智能领域，多模态学习系统(MMLS)因其处理和整合来自不同模态输入的信息的能力而受到关注。它们在医疗等关键领域的使用不断扩大，使得安全保障成为一个关键问题。然而，缺乏对其安全性的系统研究是这一领域取得进展的一个重大障碍。为了弥补这一差距，我们提出了第一个系统分类和评估 MMLS 安全性的分类法。这个分类法是围绕四个基本支柱构建的，这四个支柱对于确保 MMLS 的安全性至关重要: 健壮性、对齐性、监视性和可控性。利用这种分类法，我们回顾了现有的方法、基准和当前的研究状况，同时也指出了知识中的主要局限性和差距。最后，我们讨论了 MMLS 安全性的独特挑战。在阐明这些挑战时，我们的目标是为未来的研究铺平道路，提出可能导致 MMLS 安全协议显著进步的潜在方向。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Survey+on+Safe+Multi-Modal+Learning+Systems)|0|
|[Responsible AI Day](https://doi.org/10.1145/3637528.3673867)|Ricardo BaezaYates, Nataly Buslón||We summarize the goals of the Responsible AI day, giving a glimpse on the program as well as a short biography of the organizers.|我们总结了负责任的人工智能日的目标，让一个程序的一瞥，以及一个简短的传记的组织者。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Responsible+AI+Day)|0|
|[Heterogeneous Contrastive Learning for Foundation Models and Beyond](https://doi.org/10.1145/3637528.3671454)|Lecheng Zheng, Baoyu Jing, Zihao Li, Hanghang Tong, Jingrui He|University of Illinois at Urbana-Champaign, Champaign, USA|In the era of big data and Artificial Intelligence, an emerging paradigm isto utilize contrastive self-supervised learning to model large-scaleheterogeneous data. Many existing foundation models benefit from thegeneralization capability of contrastive self-supervised learning by learningcompact and high-quality representations without relying on any labelinformation. Amidst the explosive advancements in foundation models acrossmultiple domains, including natural language processing and computer vision, athorough survey on heterogeneous contrastive learning for the foundation modelis urgently needed. In response, this survey critically evaluates the currentlandscape of heterogeneous contrastive learning for foundation models,highlighting the open challenges and future trends of contrastive learning. Inparticular, we first present how the recent advanced contrastive learning-basedmethods deal with view heterogeneity and how contrastive learning is applied totrain and fine-tune the multi-view foundation models. Then, we move tocontrastive learning methods for task heterogeneity, including pretrainingtasks and downstream tasks, and show how different tasks are combined withcontrastive learning loss for different purposes. Finally, we conclude thissurvey by discussing the open challenges and shedding light on the futuredirections of contrastive learning.|在大数据和人工智能时代，利用对比自监督学习对大规模异构数据进行建模是一种新兴的模式。许多现有的基础模型都受益于对比自监督学习的泛化能力，它不依赖任何标签信息，而是通过学习紧凑而高质量的表示。随着基础模型在自然语言处理和计算机视觉等多个领域的突飞猛进，基础模型的异构对比学习研究迫在眉睫。作为回应，本调查批判性地评价了当前异质对比学习的基础模型，强调了开放的挑战和对比学习的未来趋势。特别是，我们首先介绍了当前先进的基于对比学习的方法是如何处理视图异质性的，以及如何应用对比学习对多视图基础模型进行训练和微调。然后，我们针对任务异质性提出了对比学习方法，包括预训练任务和下游任务，并展示了不同任务如何为不同目的组合对比学习损失。最后，我们通过讨论开放性挑战和对比学习未来发展方向的启示，得出本调查的结论。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Heterogeneous+Contrastive+Learning+for+Foundation+Models+and+Beyond)|0|
|[Equity, Diversity & Inclusion (EDI): Special Day at ACM KDD 2024](https://doi.org/10.1145/3637528.3673870)|Tania Cerquitelli, Amin Mantrach|Amazon, Luxembourg, Luxembourg; Politecnico di Torino, Turin, Piedmont, Italy|The Equity, Diversity & Inclusion event is a special day organized in conjunction with KDD '24, the 30 ^th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, which will take place from Sunday, August 25 to Thursday, August 29, 2024 at the Center de Convencions Internacional de Barcelona in Barcelona, Spain. This special day, scheduled for August 28, 2024, promotes equity, diversity, and inclusion (EDI) in data science, artificial intelligence, and beyond. It will bring together academics, researchers, practitioners, and human resources professionals (i) to present algorithms, techniques, methodologies, and projects in data science that enable responsible data processing and modeling; (ii) to discuss policies, best practices, and guidelines to promote an inclusive work environment and effective collaboration; (iii) to share personal stories to encourage young researchers, including those from groups unrepresented in the research community, to develop strong careers in data science; and (iv) to collaboratively develop and discuss an EDI Manifesto to promote an inclusive workplace environment and guiding principles in the development of research activities.|公平、多样性和包容性活动是与第30届 ACM SIGKDD 知识发现和数据挖掘会议 KDD’24共同组织的一个特殊日子，该会议将于2024年8月25日星期日至8月29日星期四在西班牙巴塞罗那国际会议中心举行。这个特殊的日子定于2024年8月28日，旨在促进数据科学、人工智能等领域的公平、多样性和包容性(EDI)。它将汇集学者、研究人员、从业人员和人力资源专业人员(i)提出数据科学中的算法、技术、方法和项目，使负责任的数据处理和建模成为可能; (ii)讨论促进包容性工作环境和有效合作的政策、最佳实践和指导方针; (iii)分享个人故事，以鼓励年轻研究人员，包括那些来自研究界无代表团体的研究人员，在数据科学领域发展强大的职业生涯; (iv)合作制定和讨论电子数据交换宣言，以促进包容性工。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Equity,+Diversity+&+Inclusion+(EDI):+Special+Day+at+ACM+KDD+2024)|0|
|[Health Day: Building Health AI Ecosystem: From Data Harmonization to Knowledge Discovery](https://doi.org/10.1145/3637528.3673866)|Jake Chen, Peipei Ping|; School of Medicine, University of California, Los Angeles, Los Angeles, CA, USA|The ACM KDD 2024 Health Day theme, "Building Health AI Ecosystem: From Data Harmonization to Knowledge Discovery," highlights the transformative potential of AI-driven ecosystems in healthcare, translational biomedical research, and basic biological research. This extended abstract discusses recent advancements, challenges, and future directions, focusing on integrating AI-ready data sets, interdisciplinary collaborations, and ethical AI practices. It aims to catalyze discussions on the potential of AI ecosystems in revolutionizing healthcare and related fields.|ACM kDD 2024年健康日的主题是“建立健康的人工智能生态系统: 从数据协调到知识发现”，强调了人工智能驱动的生态系统在医疗、转化生物医学研究和基础生物学研究方面的变革潜力。这个扩展的摘要讨论了最近的进步，挑战和未来的方向，重点集成人工智能准备的数据集，跨学科的合作，和道德的人工智能实践。它旨在促进关于人工智能生态系统在医疗保健和相关领域革命化的潜力的讨论。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Health+Day:+Building+Health+AI+Ecosystem:+From+Data+Harmonization+to+Knowledge+Discovery)|0|
|[Overview of ACM SIGKDD 2024 AI4Science4AI Special Day](https://doi.org/10.1145/3637528.3673871)|Wei Ding, Gustau CampsValls|; Image Processing Laboratory (IPL), Universitat de València, Paterna, Spain|This paper provides an overview of the ACM SIGKDD 2024 AI4Science4AI special day. It includes information about the organizers, invited speakers, keynote speakers, the event agenda, and insights from related workshops. The AI4Science4AI special day aims to bring together experts in artificial intelligence (AI) and science to discuss the latest developments, challenges, and future directions.|本文提供了 ACM SIGKDD 2024 AI4Science4AI 特别日的概述。其中包括有关组织者、特邀演讲者、主旨演讲者、活动议程以及相关研讨会的见解的信息。AI4Science4AI 特别日旨在汇集人工智能(AI)和科学方面的专家，讨论最新的发展、挑战和未来的方向。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Overview+of+ACM+SIGKDD+2024+AI4Science4AI+Special+Day)|0|
|[KDD 2024 Special Day - AI for Environment](https://doi.org/10.1145/3637528.3673869)|Karina Gibert, Wee Hyong Tok, Miquel SànchezMarrè|; Microsoft, Redmond, Washington, USA|Environmental problems such as air pollution monitoring and prevention, flood detection and prevention, land use, forest management, river water quality, wastewater treatment supervision, etc. are more complex than typical real-world problems usually AI faces to. This added complexity rises from several aspects, such as the randomness shown by most of environmental processes involved, the 2D/3D nature of involved problems, the temporal aspects, the spatial aspects, the inexactness of the information, etc. In fact, environmental problems belong to the most difficult problems with a lot of inexactness and uncertainty, and possibly conflicting objectives to be solved according to several classifications such as the one by Funtowicz & Ravetz (Funtowicz & Ravetz, 1999), which states that there are 3 kinds of problems. Also, they are non-structured problems in the classification proposed by H. Simon (Simon, 1966). All this complexity means that to effectively solve those problems a lot of knowledge is needed. This knowledge can be theoretical knowledge expressed in mechanistic models, such as the Gravidity Newton's Theory, or it can be empirical knowledge that can be expressed by means of empirical models, originated by some data and observations (data-driven knowledge) or by the expertise gathered by people when coping with such problems (model-driven knowledge, particularly expert-based knowledge). The KDD 2024 Special Day for AI for environment brings together researchers and practitioners to present their perspective on this very timely topic on how AI can be used for good, and improving the environment where we all live in.|大气污染监测与防治、洪水探测与防治、土地利用、森林管理、河流水质、污水处理监督等环境问题比人工智能面临的典型现实问题更为复杂。这种增加的复杂性来自几个方面，例如所涉及的大多数环境过程的随机性、所涉及问题的2D/3D 性质、时间方面、空间方面、信息的不准确性等。事实上，环境问题属于最困难的问题，具有很多不精确性和不确定性，可能存在冲突的目标，需要根据几个分类来解决，如 Funtowicz & Ravetz (Funtowicz & Ravetz，1999)的分类。此外，它们是 H。 Simon (Simon，1966)提出的分类中的非结构化问题。所有这些复杂性意味着，要有效地解决这些问题，需要大量的知识。这种知识可以是在机械模型中表达的理论知识，例如引力牛顿理论，也可以是经验知识，可以通过经验模型表达，起源于一些数据和观察(数据驱动的知识)或由人们在处理这些问题时收集的专业知识(模型驱动的知识，特别是基于专家的知识)。KDD 2024人工智能环境特别日汇集了研究人员和从业人员，就这个非常及时的主题提出他们的观点，如何利用人工智能做好事，并改善我们所有人生活的环境。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=KDD+2024+Special+Day+-+AI+for+Environment)|0|
|[European Data Science Day: KDD-2024 Special Day](https://doi.org/10.1145/3637528.3673868)|Dunja Mladenic, Dumitru Roman|Oslo Metropolitan University, SINTEF AS, Oslo, Norway; Jozef Stefan Institute, Ljubljana, Slovenia|The European Data Science Day offers a full day focused exclusively on innovative KDD-relevant research and development projects from national and regional funding programs, as well as corporate, start-up, and nonprofit channels. The idea is to bring together a diverse community of researchers in Data Science, Machine Learning, Language Technologies, and Knowledge Discovery, as well as partnerships in the social and physical sciences/arts, to showcase the state-of-the-art in research and applications.|欧洲数据科学日是一个专注于从国家及地区资助项目、企业、初创公司及非营利渠道中获取的与KDD相关的创新研究与开发项目的全天活动。其核心理念是将数据科学、机器学习、语言技术和知识发现领域的多样化研究社群，以及社会与自然科学/艺术领域的合作伙伴汇聚一堂，展示当前最前沿的研究成果与应用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=European+Data+Science+Day:+KDD-2024+Special+Day)|0|
|[Generative AI Day](https://doi.org/10.1145/3637528.3673872)|Jie Tang, Yuxiao Dong, Michalis Vazirgiannis|Tsinghua University, Beijing, China; Ecole Polytechnique & Mohamed bin Zayed University of Artificial Intelligence, Paris, France|The Generative AI (AIGC) Day at KDD'24 is a dedicated full-day event for generative AI at KDD. This is an opportunity to bring together researchers, practitioners, and startups to share the insights about the cutting-edge advancements and to discuss the potential societal impacts of LLMs and AIGC. It is exciting that this year, we have invited speakers from both industry (e.g., Amazon, Zhipu AI) and academia (e.g., USC, UCLA). The topics cover various perspectives of generative AI including foundation models, streaming LLMs, LLM training and inference. As demonstrated, data plays a crucial role in developing cutting-edge generative AI models. For example, the Gemini Team has found that "data quality is an important factor for highly-performing models...''. To date, there is still significant room to define design principles and develop methods for improved data collection, selection, and synthetic data generation for the pre-training and alignment of language, vision, and multi-modal models. Therefore, the Day will invite the speakers and KDD audience to discuss the challenges and opportunities for data mining researchers in the era of generative AI.|KDD'24上的生成式AI（AIGC）日是一个专门为KDD举办的生成式AI全天活动。这是一个汇聚研究人员、从业者和初创企业，分享关于生成式AI前沿进展的见解，并讨论大型语言模型（LLMs）和AIGC潜在社会影响的机会。今年，我们邀请了来自工业界（如亚马逊、智谱AI）和学术界（如南加州大学、加州大学洛杉矶分校）的演讲者，令人振奋。话题涵盖了生成式AI的多个视角，包括基础模型、流式LLMs、LLM训练与推理。如所展示的，数据在开发尖端生成式AI模型中起着至关重要的作用。例如，Gemini团队发现“数据质量是高性能模型的重要因素……”。迄今为止，仍有很大的空间来定义设计原则，并开发改进数据收集、选择和合成数据生成的方法，以用于语言、视觉和多模态模型的预训练和对齐。因此，本次活动将邀请演讲者和KDD的观众一起讨论生成式AI时代数据挖掘研究者的挑战与机遇。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+AI+Day)|0|
|[KDD 2024 Finance Day](https://doi.org/10.1145/3637528.3673865)|Guiling Wang, Daniel Borrajo|Universidad Carlos III de Madrid, Madrid, Spain; New Jersey Institute of Technology, Newark, NJ, USA|The Finance Day at KDD 2024 will take place on August 26th in Barcelona, Spain. Following the success of the inaugural event last year, the second edition highlights the significant role of AI in transforming the financial industry. This special day serves as a forum for discussion of innovations at the intersection of AI and finance. An exciting lineup of 12 influential speakers from nine different countries will be featured, representing a mix of government organizations, leading banks, innovative hedge funds, and top academic institutions. These experts will delve into a range of topics, from cutting-edge FinTech innovations to ethical considerations in machine learning, providing a comprehensive overview of the finance and AI. The distinguished speakers include Avanidhar Subrahmanyam from UCLA, Henrike Mueller from the Financial Conduct Authority, Claudia Perlich from Two Sigma, Eyke Hüllermeier from Ludwig-Maximilians-Universität München, Senthil Kumar from Capital One, Stefan Zohren from the University of Oxford, Dumitru Roman from SINTEF ICT, Kubilay Atasu from TU Delft, Xiao-Ming Wu from Hong Kong Polytechnic University, Yongjae Lee from UNIST, Jundong Li from the University of Virginia, and Milos Blagojevic from BlackRock.|2024年KDD金融日将于8月26日在西班牙巴塞罗那举行。继去年首届活动取得成功后，第二届活动强调了人工智能在转变金融行业中的重要作用。这一特别日活动为探讨人工智能与金融交叉领域的创新提供了一个论坛。届时将有一系列由来自九个不同国家的12位有影响力的演讲者组成的精彩阵容，他们代表了政府机构、领先银行、创新对冲基金和顶级学术机构。这些专家将深入探讨从尖端金融科技创新到机器学习中的伦理考量等一系列话题，全面概述金融与人工智能的现状。知名演讲者包括来自加州大学洛杉矶分校的Avanidhar Subrahmanyam、金融行为监管局的Henrike Mueller、Two Sigma的Claudia Perlich、慕尼黑路德维希-马克西米利安大学的Eyke Hüllermeier、Capital One的Senthil Kumar、牛津大学的Stefan Zohren、SINTEF ICT的Dumitru Roman、代尔夫特理工大学的Kubilay Atasu、香港理工大学的Xiao-Ming Wu、UNIST的Yongjae Lee、弗吉尼亚大学的Jundong Li以及BlackRock的Milos Blagojevic。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=KDD+2024+Finance+Day)|0|
|[AdKDD 2024](https://doi.org/10.1145/3637528.3671476)|Abraham Bagherjeiran, Nemanja Djuric, KuangChih Lee, Linsey Pang, Vladan Radosavljevic, Suju Rajan|Aurora Innovation, Inc., Pittsburgh, PA, USA; Spotify, New York, NY, USA; eBay, Inc., San Jose, CA, USA; Walmart, Sunnyvale, CA, United States; Amazon, Palo Alto, CA, USA; Salesforce, San Francisco, CA, USA|The digital advertising field has always had challenging ML problems, learning from petabytes of data that is highly imbalanced, reactivity times in the milliseconds, and more recently compounded with the complex user's path to purchase across devices, across platforms, and even online/real-world behavior. The AdKDD workshop continues to be a forum for researchers in advertising, during and after KDD. Our website which hosts slides and abstracts receives approximately 2,000 monthly visits and 1,800 active users during the KDD 2021. In surveys during AdKDD 2019 and 2020, over 60% agreed that AdKDD is the reason they attended KDD, and over 90% indicated they would attend next year. The 2024 edition is particularly timely because of the increasing application of Graph-based NN and Generative AI models in advertising. Coupled with privacy-preserving initiatives enforced by GDPR, CCPA the future of computational advertising is at an interesting crossroads. For this edition, we plan to solicit papers that span the spectrum of deep user understanding while remaining privacy-preserving. In addition, we will seek papers that discuss fairness in the context of advertising, to what extent does hyper-personalization work, and whether the ad industry as a whole needs to think through more effective business models such as incrementality. We have hosted several academic and industry luminaries as keynote speakers and have found our invited speaker series hosting expert practitioners to be an audience favorite. We will continue fielding a diverse set of keynote speakers and invited talks for this edition as well. As with past editions, we hope to motivate researchers in this space to think not only about the ML aspects but also to spark conversations about the societal impact of online advertising.|数字广告领域一直面临着具有挑战性的机器学习问题，这些问题源自于处理海量且高度不平衡的数据，响应时间以毫秒计，并且近年来由于用户在不同设备、平台间甚至线上线下行为的复杂购买路径而变得更加复杂。AdKDD研讨会继续作为广告领域研究者在KDD期间及之后的交流平台。我们的网站托管了幻灯片和摘要，每月大约有2000次访问，在KDD 2021期间有1800名活跃用户。在AdKDD 2019和2020的调查中，超过60%的人同意AdKDD是他们参加KDD的原因，超过90%的人表示明年还会参加。2024年的研讨会尤为及时，因为基于图的神经网络和生成式AI模型在广告中的应用日益增多。结合GDPR和CCPA实施的隐私保护举措，计算广告的未来正处于一个有趣的十字路口。对于这一期，我们计划征集涵盖深度用户理解同时保持隐私保护的论文。此外，我们还将寻求探讨广告公平性、超个性化在多大程度上有效以及广告行业是否需要思考更有效的商业模式（如增量性）的论文。我们曾邀请过几位学术界和行业的杰出人士作为主讲嘉宾，并发现邀请专家实践者的系列演讲深受观众喜爱。我们也将继续为这一期邀请多样化的主讲嘉宾和特邀演讲。与往届一样，我们希望激励该领域的研究者不仅思考机器学习方面，还能引发关于在线广告社会影响的讨论。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AdKDD+2024)|0|
|[Fragile Earth: Generative and Foundational Models for Sustainable Development](https://doi.org/10.1145/3637528.3671493)|Emre Eftelioglu, Bistra Dilkina, Naoki Abe, Ramakrishnan Kannan, Yuzhou Chen, Yulia R. Gel, Kathleen Buckingham, Auroop R. Ganguly, James Hodson, Jiafu Mao|Oak Ridge National Laboratory, Oak Ridge, TN, USA; Amazon, Bellevue, WA, USA; The University of Texas at Dallas, Richardson, TX, USA; AIForGood, San Francisco, CA, USA; IBM Research, Yorktown Heights, New York, USA; Temple University, Philadelphia, PA, USA; veritree, Vancouver, BC, Canada; University of Southern California, Los Angeles, CA, USA; Northeastern University, Boston, MA, USA|The Fragile Earth Workshop is a recurring event in ACM's KDD Conference on research in knowledge discovery and data mining that gathers the research community to find and explore how data science can measure and progress climate and social issues, following the United Nations Sustainable Development Goals (SDGs) framework.|“脆弱地球”研讨会是ACM知识发现与数据挖掘（KDD）会议的定期活动，旨在召集研究社区，探讨如何利用数据科学来衡量并推动气候和社会问题的发展，这一探讨遵循联合国可持续发展目标（SDGs）框架。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fragile+Earth:+Generative+and+Foundational+Models+for+Sustainable+Development)|0|
|[Artificial Intelligence and Data Science for Healthcare: Bridging Data-Centric AI and People-Centric Healthcare](https://doi.org/10.1145/3637528.3671497)|Shenda Hong, Daoxin Yin, Gongzheng Tang, Tianfan Fu, Liantao Ma, Junyi Gao, Mengling Feng, Mai Wang, Yu Yang, Fei Wang, Hongfang Liu, Luxia Zhang|Health Data Research UK, University of Edinburgh, Edinburgh, United Kingdom; Department of Population Health Sciences, Cornell University, New York, NY, USA; Department of Health Data Science and Artificial Intelligence, UTHealth Houston, Houston, TX, USA; National Institute of Health Data Science, Peking University, Beijing, China; Computer Science Department, Rensselaer Polytechnic Institute, New York, USA; National Engineering Research Center for Software, Peking University, Beijing, China; Saw Swee Hock School of Public Health, National University of Singapore, Singapore, Singapore|KDD AIDSH 2024 aims to foster discussions and developments that push the boundaries of Artificial Intelligence (AI) and Data Science (DS) in healthcare, enhance diagnostic accuracy and promote human-centric approaches to healthcare, thus stimulating future interdisciplinary collaborations. This year's symposium will focus on expanding the application of AI/DS in healthcare/medicine and bridging existing gaps. The workshop invites submissions of full papers as well as work-in-progress on the application of AI/DS in healthcare. The workshop will feature three invited talks from eminent speakers, spanning academia, industry, and clinical researchers. In addition, selected papers will be invited to publish in Health Data Science, a Science Partner Journal. This summary provides a brief description of the half-day workshop to be held on August 26th, 2024. The webpage for the workshop can be found at https://aimel.ai/kdd2024aidsh.|KDD AIDSH 2024旨在推动人工智能（AI）和数据科学（DS）在医疗领域的讨论与发展，提升诊断准确性，并促进以人为本的医疗方法，从而激发未来的跨学科合作。本次研讨会将聚焦于扩大AI/DS在医疗/医学中的应用，并弥合现有差距。工作坊欢迎提交完整论文以及正在进行的研究工作，内容涉及AI/DS在医疗领域的应用。工作坊将邀请三位知名专家进行主题演讲，涵盖学术界、工业界和临床研究领域。此外，精选论文将被邀请发表在《健康数据科学》（Health Data Science）期刊上，该期刊是Science的合作期刊。本摘要简要介绍了将于2024年8月26日举行的半天工作坊。工作坊的网页地址为https://aimel.ai/kdd2024aidsh。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Artificial+Intelligence+and+Data+Science+for+Healthcare:+Bridging+Data-Centric+AI+and+People-Centric+Healthcare)|0|
|[TSMO 2024: Two-sided Marketplace Optimization](https://doi.org/10.1145/3637528.3671484)|Mihajlo Grbovic, Vladan Radosavljevic, Minmin Chen, Katerina IliakopoulouZanos, Thanasis Noulas, Amit Goyal, Fabrizio Silvestri|Spotify, New York City, NY, USA; Sapienza University of Rome, Rome, Italy; Bitvavo, Thessaloniki, Greece; Amazon, San Francisco, CA, USA; Google Deepmind, Mountain View, CA, USA; Meta, New York City, NY, USA; Airbnb, Inc., San Francisco, CA, USA|In recent years, two-sided marketplaces have emerged as viable business models in many real-world applications. In particular, we have moved from the social network paradigm to a network with two distinct types of participants representing the supply and demand of a specific good. Examples of industries include but are not limited to accommodation (Airbnb, Booking.com), video content (YouTube, Instagram, TikTok), ridesharing (Uber, Lyft), online shops (Etsy, Ebay, Facebook Marketplace), music (Spotify, Amazon), app stores (Apple App Store, Google App Store) or job sites (LinkedIn). The traditional research in most of these industries focused on satisfying the demand. OTAs would sell hotel accommodation, TV networks would broadcast their own content, or taxi companies would own their own vehicle fleet. In modern examples like Airbnb, YouTube, Instagram, or Uber, the platforms operate by outsourcing the service they provide to their users, whether they are hosts, content creators or drivers, and have to develop their models considering their needs and goals.|近年来，双边市场已成为许多实际应用中可行的商业模式。特别是，我们从社交网络模式转变为一个包含两种不同类型参与者的网络，这些参与者分别代表特定商品的供需双方。此类行业的例子包括但不限于住宿服务（如Airbnb、Booking.com）、视频内容（如YouTube、Instagram、TikTok）、拼车服务（如Uber、Lyft）、在线商店（如Etsy、Ebay、Facebook Marketplace）、音乐平台（如Spotify、Amazon）、应用商店（如Apple App Store、Google App Store）或求职网站（如LinkedIn）。在大多数这些行业中，传统研究主要集中在满足需求上。例如，在线旅行社（OTAs）会销售酒店住宿，电视网络会播放自有内容，出租车公司会拥有自己的车队。而在现代平台如Airbnb、YouTube、Instagram或Uber中，这些平台通过将其提供的服务外包给用户（无论是房东、内容创作者还是司机）来运作，并需要在其模型开发中考虑到这些用户的需求和目标。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TSMO+2024:+Two-sided+Marketplace+Optimization)|0|
|[KDD workshop on Evaluation and Trustworthiness of Generative AI Models](https://doi.org/10.1145/3637528.3671481)|Yuan Ling, Shujing Dong, Yarong Feng, Zongyi Joe Liu, George Karypis, Chandan K. Reddy|Amazon, Irvine, WA, USA; Virginia Tech  Amazon, Arlington, VA, USA; Univ. of Minnesota  Amazon, Santa Clara, CA, USA; Amazon, Seattle, WA, USA|The KDD workshop on Evaluation and Trustworthiness of Generative AI Models aims to address the critical need for reliable generative AI technologies by exploring comprehensive evaluation strategies. This workshop will delve into various aspects of assessing generative AI models, including Large Language Models (LLMs) and diffusion models, focusing on trustworthiness, safety, bias, fairness, and ethical considerations. With an emphasis on interdisciplinary collaboration, the workshop will feature invited talks, peer-reviewed paper presentations, and panel discussions to advance the state of the art in generative AI evaluation.|KDD生成式AI模型评估与可信度研讨会旨在通过探索全面的评估策略，解决生成式AI技术可靠性这一关键需求。本次研讨会将深入探讨评估生成式AI模型的各个方面，包括大型语言模型（LLMs）和扩散模型，重点关注可信度、安全性、偏见、公平性及伦理考量。研讨会强调跨学科合作，将通过邀请演讲、同行评审论文展示及小组讨论等形式，推动生成式AI评估领域的技术进步。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=KDD+workshop+on+Evaluation+and+Trustworthiness+of+Generative+AI+Models)|0|
|[NL2Code-Reasoning and Planning with LLMs for Code Development](https://doi.org/10.1145/3637528.3671505)|Ye Xing, Jun Huan, Wee Hyong Tok, Cong Shen, Johannes Gehrke, Katherine Lin, Arjun Guha, Omer Tripp, Murali Krishna Ramanathan|Microsoft, Redmond, USA; Northeastern University, Boston, USA; University of Virginia, Charlottesville, USA; Amazon, Seattle, USA; Microsoft, Boston, USA; Microsoft, Richmond, USA|There is huge value in making software development more productive with AI. An important component of this vision is the capability to translate natural language to a programming language ("NL2Code") and thus to significantly accelerate the speed at which code is written. This workshop gathers researchers, practitioners, and users from industry and academia that are working on NL2Code, specifically on the problem of using large language models to convert statements posed in a human language to a formal programming language.|利用人工智能提升软件开发效率具有巨大的价值。这一愿景的重要组成部分是实现将自然语言转化为编程语言（即“NL2Code”）的能力，从而显著加快代码编写速度。本次研讨会汇聚了来自学术界和工业界的研究人员、从业者及用户，他们专注于NL2Code领域，特别是利用大型语言模型将人类语言表述的语句转化为正式编程语言的问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NL2Code-Reasoning+and+Planning+with+LLMs+for+Code+Development)|0|
